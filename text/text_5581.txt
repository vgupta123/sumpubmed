BACKGROUND
sequencing technology has made great advances over the last  <dig> years since the development of chain-terminating inhibitor-based technologies  <cit> . traditional sequencing approaches require cloning of dna fragments into bacterial vectors for amplification and sequencing of individual templates using vector-based primers. this approach was adapted for cdna libraries  <cit>  and, with the advent of capillary sequencing, became suitable for high-throughput sequencing of large samples of transcripts, termed expressed sequence tags . ests have become an invaluable resource for gene discovery, genome annotation, alternative splicing, snp discovery, molecular markers for population analysis, and expression analysis in animal, plant, and microbial species  <cit> . other approaches for analyzing transcriptomes include serial analysis of gene expression   <cit> , massively parallel signature sequencing   <cit> , and microarrays  <cit> . these approaches, which involve the sequencing or hybridizing of small concatamers of cdna derived from mrna by reverse transcription, have been used successfully in analyzing the expression of genomes  at a very large scale, usually from species with a sequenced genome or an existing and extensive est data set. although several alternatives have been described since the emergence of est sequencing projects, none has yet totally supplanted the use of bacterial vectors and sanger sequencing.

in  <dig>  two new sequencing technologies were introduced both based on sequencing by synthesis, which promised to replace or enhance traditional sequencing methods. the  <dig> system http://www. <dig> com, using pyrosequencing technology  <cit> , and the solexa system http://www.illumina.com, which detects fluorescence signals  <cit> . both execute millions of sequencing reactions in parallel, producing data at ultrahigh rates  <cit> . although read lengths are much shorter with these new methods than with capillary sequencing , respectively, both platforms generate sufficient data to completely re-sequence bacterial genomes in a single run  <cit> . in the past year, applied biosystems has introduced their solid sequencer http://www <dig> appliedbiosystems.com, another short-read 20â€“ <dig> bp platform, with read lengths anticipated to be  <dig> bp in the upcoming solid <dig> release. the three platforms offer a variety of experimental approaches for characterizing a transcriptome, including single-end and paired-end cdna sequencing, tag profiling , methylation assays, small rna sequencing, sample tagging  to permit small subsample identification, and splice variant analyses. several challenges face investigators hoping to use these methods, including the relatively large cost of most ng experiments and intense demands for data storage and analysis on the scale required for ng datasets, and rapidly evolving technologies. initial studies reported success with  <dig> sequencing of chloroplast genomes  <cit> , small rnas  <cit> , and transcriptomes of organisms with  <cit>  or without  <cit>  extensive genomic sequence information.

these next generation  sequencing methods promise a cost-effective means of either deeply sampling or fully sequencing an organism's transcriptome, with even small experiments tagging a very large number of expressed genes. however, prior transcriptome sequencing studies have been largely exploratory, only hinting at the potential for ng transcriptome sequencing at different scales. there is a great need for quantitative studies and analysis tools that help investigators optimally design ng sequencing experiments to address specific goals.

a complete solution to this problem would involve realistic models for each technology, accounting for the cost of library generation and data collection, the characteristics of cdna libraries, transcript abundance distributions, read length distributions, and the error rates in sequence generation and assembly. the present study focuses on the first four of these issues to provide estimates of theoretical coverage of complex transcriptomes with varying scales and types of dna sequencing experiments. in earlier publications  <cit> , we developed a robust simulation approach to model traditional capillary transcriptome sequencing, which incorporates distributions of the relative start site of cdna sequences as a function of cdna length, the read length distribution, and the transcript abundance distribution. we have now adapted this simulation approach to model the specific characteristics of ng sequencing. the results from this study should help researchers working with these new and exciting technologies.

the present study has several goals. first, we report empirical comparisons of  <dig> pyrosequencing and capillary-based transcriptome sequencing from the model plant, arabidopsis thaliana, and two non-model plant species, the basal eudicot eschscholzia californica  and the magnoliid persea americana . we use these results to examine the effects of library preparation procedures, specifically, normalized versus non-normalized and random versus oligo-dt primed libraries. we then introduce a simulation approach, based on the gs <dig> sequencing results, to predict the outcome of additional gs <dig> transcriptome sequencing experiments while accounting for critical features in cdna library construction. we then use the gs <dig> simulation results to extrapolate results for 454flx and solexa platforms, in order to estimate technology-specific sequencing characteristics. finally, we report on simulated experiments aimed at characterizing the optimal mixture of methods for most complete and cost-effective transcriptome sequencing with one or more sequencing technologies.

RESULTS
next generation transcriptome sequencing of arabidopsis floral tissue
a half plate of gs <dig> sequencing from an arabidopsis random-primed cdna library generated  <dig>  reads totalling  <dig>  mb with an average length of  <dig>  bp. the reads were assembled into  <dig>  unigenes, which included  <dig>  contigs with an average length of  <dig> bp and  <dig>  singleton reads . we mapped  <dig>   reads to the tair  <dig> arabidopsis genome annotation . of the total mapped reads,  <dig> % were located within  <dig>  genic regions and  <dig> % were located in intergenic regions. within the genic regions,  <dig>   reads mapped exactly to known exons, while  <dig>   and  <dig>   reads mapped to introns and intron/exon boundaries, respectively. also,  <dig>   of the reads included in the genic regions extended current boundaries of known genes while  <dig> reads combined two annotated genes or marked areas of the genome with overlapping genes. there were  <dig>   reads that did not have a significant blastn match to any location within the genome. there were  <dig>  genes that had more than  <dig> reads per locus, and the  <dig> most highly expressed genes , included two subunits of the photosynthetic protein rubisco, as well as tasty, tgg <dig>  and pdf <dig>  these "top ten" transcripts had read counts ranging from  <dig> to  <dig> reads with the rubisco small subunit 1a being most highly represented. at this shallow sequencing depth,  <dig> non-overlapping contigs, with lengths of  <dig> and  <dig> bp, mapped to the rubisco small subunit 1a gene.

read, contig, singleton, and unigene counts , mean sequence lengths (), and total amount of sequence data  for  <dig> gs <dig> libraries analyzed. species codes are ath , pam , eca . cdna library production method indicated in parentheses. read lengths based on number of q <dig> equivalent bases produced, after trimming and cleaning with the program seqclean http://compbio.dfci.harvard.edu/tgi/software/; normalized library original read mean length was  <dig>  prior to trimming normalization adapter.

all  <dig> reads were mapped  to the genome. tair xml files were parsed to obtain exon structure and location within the genome. percentages were calculated for each class of sequence type. the number of genes does not equal the summation of gene components because there are some genes that are hit by multiple reads in different sections of the gene. the percent for each gene component is the percent of total reads.

unigenes from each library, arabidopsis flower bud random-primed , eschscholzia flower bud oligo-dt  and random-primed , and persea americana normalized flower bud , were mapped to the annotated tair cdna and protein datasets using blastx . column headers are contig name , contig length , number of reads per contig , percent coverage , arabidopsis best hit gene identifier , annotation , and e-value. ribosomal rna and contaminants such as putative endophytes removed from this list. refer to additional file  <dig> for detailed blast results.

despite low overall transcriptome coverage, one-half plate of arabidopsis gs <dig> sequence data returned  <dig> fully sequenced cdnas, as well as  <dig>   <dig>  and  <dig> genes at 90%, 80%, and 70% coverage, respectively. these results demonstrate that nominal amounts of  <dig> sequencing can generate complete or nearly complete sequences for an appreciable number of genes, especially those that are small and highly expressed. another very promising result is the improved annotation of genes for both model and non-model species. for example, although the arabidopsis genome has been largely sequenced since  <dig>  <cit> , the half plate of gs <dig> extended the untranslated regions  of roughly  <dig>  genes and mapped new transcript boundaries of  <dig>  genic regions. these regions are possibly new splice variants of previously annotated genes. finally,  <dig>  transcripts were mapped to  <dig>  unique intergenic regions. these transcripts might represent un-annotated protein-coding genes or non-coding rna sequences that have not previously been sampled in traditional cdna libaries.

transcriptome sequencing of eschscholzia californica using oligo-dt and random-primed libraries
two full plates  of gs <dig> sequencing was performed on the emerging model basal eudicot, eschscholzia californica  <cit> , including one plate from a  <dig> library of oligo-dt primed cdna and one plate from a  <dig> library of random hexamer-primed cdna. the library of oligo-dt primed cdna generated  <dig>  reads totalling  <dig>  mb with an average length of  <dig>  bp. the reads assembled into  <dig>  unigenes, including  <dig>  contigs with an average length of  <dig>  bp and  <dig>  singletons . the library of random-primed cdna generated  <dig>  reads totalling  <dig>  mb with an average length of  <dig>  bp. the reads assembled into  <dig>  unigenes, including  <dig>  contigs with an average length of  <dig>  bp and  <dig>  singleton reads . finally, we assembled both plates, which resulted in  <dig>  unigenes, including  <dig>  contigs with an average length of  <dig>  bp and  <dig>  singleton reads .

as expected, the most obvious difference between the oligo-dt and random-primed cdna sequences was the representation of rrna genes. additional rounds of mrna purification, however, could have reduced the level of rrna "contamination". we also examined the relative start positions of the reads from each library by mapping the reads to the proteome of arabidopsis . the relative start positions are defined as the start position of the best arabidopsis hsp divided by the length of the best protein match. as expected, the oligo-dt library had a greater 3' bias than the random primed library. the unigenes from both libraries mapped to  <dig>  unique arabidopsis genes, with  <dig>  of the transcripts found in both. the level of redundancy observed between these two plates  suggests that many more genes would be discovered with additional sequencing.

transcriptome sequencing in a normalized library of persea americana
one plate of gs <dig> sequencing was performed on a normalized library for persea americana, an emerging model for the magnoliids  <cit> . the plate generated  <dig>  reads totalling  <dig>  mb with an average length of  <dig>  bp. we then trimmed the adaptors used in the normalization step, which reduced the total number of reads to  <dig>  with an average sequence length of  <dig>  bp. trimming the adaptors reduced the total amount of sequence by more than  <dig> mb, bringing the total to  <dig>  mb. the reads assembled into  <dig>  unigenes, including  <dig>  contigs with an average length of  <dig>  bp and  <dig>  singleton reads .

to determine the success of the normalization step, we plotted the relative frequency of the number of reads per gene, using arabidopsis as a reference . compared to the other library methods used in this study, the normalized persea library  contained the largest number of genes with fewer than five reads per gene and the fewest number of genes with more than  <dig> reads per gene. the gene with the highest number of mapped reads was a protein phosphatase with  <dig> reads. in contrast, the most highly represented genes in the poppy non-normalized libraries had over  <dig> reads mapping to specific arabidopsis genes. hence, the normalization step was successful. note that the persea library, constructed using the trimmer-direct kit  with amplification of full-length cdnas , also has the least amount of 3' bias in read start positions .

correlation of observed arabidopsis transcript frequencies with microarray data
of the  <dig>  genes included on the arabidopsis affymetrix  microarray,  <dig>  had at least one read mapped to its cdna sequence. for these genes, we used affy microarray expression values generated from inflorescence tissue in the same a. thaliana ecotype  <cit>  to compare with the number of  <dig> reads for each gene. the comparison revealed that  <dig>  genes that were detected above normalized expression level  <dig> with the affy chip were not detected in the  <dig> sequences, while  <dig>  genes were detected in  <dig> reads, but were below expression level  <dig> with affy data . an additional  <dig>  genes detected by  <dig> reads were not included as probes on the affy gene chip. a moderate correlation was observed between microarray expression values and number of  <dig> reads .

next generation transcriptome simulation study
a primary goal of large-scale transcriptome sequencing is to identify and obtain full-length sequences of all of the expressed genes in an organism or tissue. a researcher will typically begin with rnas isolated from a tissue of interest or a collection of tissues from the entire organism. the researcher may use tissue from a particular developmental stage or assay gene expression under a range of experimental conditions . each of the new ng technologies  produces data with characteristics that can be evaluated and compared to each other and traditional capillary sequencing.

in order to predict the expected outcomes of varied amounts of sequencing effort using a blend of technologies, we developed a predictive model based on the simulation engine of eststat  <cit> . inputs to the model include four distribution profiles that reflect information about the cdna library or sequencing technology: 1) the transcript abundance profile, a transcriptome-specific frequency distribution of the number of tags of different genes in the entire transcriptome, 2) the distribution of cdna lengths 3) the distribution of sequencing start sites, and 4) the distribution of read lengths after removal of vector and low quality data. the first three of these reflect library specific features, while the fourth is mostly dependent upon the sequencing technology. the eststat simulation model has been tested under a variety of situations and found to robustly predict the outcomes of future sequencing experiments. although eststat can estimate and correct assembly errors in silico without reference to a known genome sequence, we were able to map each read to its known location on the arabidopsis genome to assess and correct assembly error.

we used the results from our gs <dig> sequencing to simulate different levels of sequencing coverage for each of the ng and capillary technologies. for each technology, we considered both non-normalized and perfectly normalized libraries, in which the expression level of every gene is made identical. actual normalization experiments should therefore fall somewhere between non-normalized and perfectly normalized, depending on the normalization method, rna quality, and success of the normalization procedure . we used the following parameters to help evaluate the different sequencing platforms: transcriptome coverage, percentage of all expressed genes that were tagged, percentage of singletons, number of unigenes, mean unigene length, and the percentage of all expressed genes that were sequenced completely .

transcriptome coverage  is a direct indicator of the sequencing depth and breadth of sequence data relative to the sample transcriptome. we define the transcriptome coverage as the total non-redundant number of bases from sampled genes that are included in at least one est, divided by the sum of cdna lengths for all expressed genes . in this study, the  <dig>  detected genes and randomly sampled  <dig>  undetected genes  sum to  <dig>  genes, with an expected total cdna length of  <dig>  mb. the transcriptome coverage, as a function of the total number of sequenced bases , differs only slightly for all technologies. however, when the amount of sequence is low , the transcriptome coverage is greater in the normalized libraries  compared to the non-normalized libraries  for each technology. theoretically, perfect normalization will equalize the level of expression for all genes, without any other impact on library quality, and thus will increase the coverage of genes that are randomly sampled. using the distributions of cdna length, read length, and sequencing start sites obtained in these experiments, we estimate that traditional 5' capillary sequencing of a non-normalized library will cover approximately 14%, 52%, and 82% of the transcriptome with  <dig> ,  <dig>  and  <dig> mb of sequencing, respectively. for a normalized library, the percentage increases to 18%, 69%, and 95% with the same amounts of sequence. the same pattern was observed for the ng technologies but with higher levels of transcriptome coverage. for example, the gs <dig> technology is estimated to cover 15%, 54%, and 88% of the transcriptome for a non-normalized library and  <dig> %, 72%, and 98% of the transcriptome for a normalized library at  <dig> ,  <dig>  and  <dig> mb of sequencing. the lower coverage of capillary-based est sequencing given the same number of sequenced bases is attributed to biases implicit in the cdna cloning process. the flx is estimated to cover 15%, 54%, and 88% for the non-normalized library and 18%, 72%, 98% for a normalized library at the same intervals. finally, the solexa platform is estimated to cover 55% and 87% for the non-normalized library and 75% and 98% for the normalized library for  <dig> and  <dig> mb, respectively. given that one plate of sequence data from the solexa platform is estimated at  <dig>  mb, we chose  <dig> mb  as the first interval to be simulated, and we excluded all intervals less than  <dig> mb.

transcriptome coverage differs substantially among the various technologies at the same cost. however, the cost used in this analysis refers only to the actual sequencing costs and not the pre-processing costs such as library preparation and normalization. the solexa platform rapidly approaches 100% coverage primarily because the cost of sequencing is substantially smaller per mb . solexa is followed by gs <dig>  flx, and conventional est sequences. it is estimated that traditional capillary sequencing would reach 100% transcriptome coverage at more than  <dig> mb and at a cost of over $ <dig> . while solexa sequencing is the most economical technology for deep coverage of transcriptomes, de novo assembly of short solexa sequences for non-model species remains an unresolved challenge.

a second indicator of the depth of transcriptome sequencing is the percentage of genes tagged . a gene is considered tagged if it has been sampled with at least one read. the percentage of genes tagged increases with both amount of sequencing and price. for a non-normalized traditional library, we estimate that 27%, 75%, and 96% of the genes will be tagged in our sample transcriptome with  <dig> ,  <dig>  and  <dig> mb of sequencing. for a normalized library, the percentage increases to 39%, 98%, and 100% with the same amounts of sequence. as expected, this percentage increases when the sequencing is done with any of the ng technologies. the cost of gene tagging also differs substantially among the various sequencing technologies. the solexa platform tags essentially 100% of the expressed genes with less than one plate of sequence . solexa is followed by gs <dig>  flx, and conventional est sequences. capillary sequencing would approach 100% genes tagged at more than  <dig> mb and over $ <dig> .

the number of unigenes  â€“ including singletons and contigs â€“ has typically been used to estimate the number of transcribed genes in a tissue. with small amounts of sequencing, the number of unigenes is similar to the number of sequences, but with more sequencing multiple reads are observed for each gene , and the rate of discovery for new genes falls off. at a particular point in the sequencing process , the number of unigenes will begin to decrease as disconnected reads coalesce into contigs covering entire genes, and eventually the unigene number approaches the number of genes expressed in the library. the rate at which multiple reads for a gene coalesce into a single contig is a function of read length. with the capillary technology, each read is large compared to the ng reads. with a non-normalized library similar to the model library, we will reach the peak unigene number at more than  <dig> mb of sequencing. with a normalized library, we reach the peak at approximately  <dig> mb and decrease gradually with an additional  <dig> mb of sequence. however, we still do not reach the estimated  <dig>  genes expressed in the arabidopsis floral library. for the flx technology, the maximum number of unigenes occurs at roughly  <dig> mb and  <dig> mb for the non-normalized and normalized libraries, respectively. however, because the flx sequences are two to three times shorter than the traditional sequences, the peak is reached with roughly double the number of unigenes . for the gs <dig> platform, the peaks occur at nearly the same levels  as the flx platform, but since these reads are half as long as flx reads, the gs <dig> produces more than twice the number of unigenes  for both library types. the solexa platform produces many more unigenes at all levels of sequencing and the peak occurs at approximately  <dig> mb for both library types .

the mean unigene length  is an important statistic if the goal of the transcriptome sequences is to perform multi-gene phylogenetic or molecular evolutionary analyses. in this case, researchers would like full-length sequences for many expressed genes, not just small fragments of expressed genes. in the arabidopsis genome, the average transcript length is approximately  <dig>  bp . therefore, a researcher would like to sequence enough of a library to produce contiguous sequences with average lengths of all genes in the library. we calculated the unigene length in two different ways. first, we used the mean length of all unigenes, although this estimate lowers the mean length for the shorter sequences in the ng technologies. second, we calculated the mean length of only the longest unigenes for each gene . all ng technology and library type combinations require greater depth of sequencing to reach the same level as its traditional counterpart. when we examine the mean unigene length in relation to price, the traditional sequencing produces the longest unigenes until approximately $ <dig>  worth of sequencing. this is approximately 4â€“ <dig> mb of capillary sequencing and  <dig> â€“ <dig>  reads. at this point, the ng technologies begin to generate enough sequences to assemble longer unigenes at a lower cost.

the percentage of singleton reads  reflects sequencing depth and the likelihood that a given read will assemble to form a contig with other reads. a singleton is defined as a single read that does not contain enough overlap in length to be combined with other reads from the same transcribed gene. the percentage of singletons is also inversely proportional to the levels of redundancy in the library. therefore, additional sequencing usually reduces the percentage of singletons. this is the case for capillary sequencing, where the percentages of singletons are 73%, 40%, and 16% for non-normalized and 81%, 23%, and 4% for normalized libraries at the  <dig> ,  <dig>  and  <dig> mb levels, respectively. for the gs <dig>  these values change to 76%, 48%, and 25% for non-normalized libraries and 80%, 34%, and 7% for normalized libraries at the same levels. for the flx, the percentage of singletons changes to 74%, 44%, and 22% for non-normalized and to 78%, 29%, and 5% for normalized libraries at the same levels. finally, for solexa, the percentage of singletons is predicted to be around 68%, 47%, and 25% for non-normalized and 67%, 32%, and 7% for normalized libraries at the  <dig>   <dig>  and  <dig> mb sequence intervals, respectively.

the final parameter used to evaluate and compare the technologies is the percentage of genes with 100% coverage . as with mean unigene length, gene coverage can be calculated using all of the unigenes per gene, or by using only the longest unigene. the smaller reads from the ng technologies might cover all the regions within a gene. however, many of the reads for a gene will not have sufficient overlap to assemble into a contiguous sequence. although we calculated both estimates, we use the percentage of gene coverage based on the longest unigene for comparisons to other platforms. in relation to amount of sequencing , the capillary, gs <dig>  and flx technologies have similar percentages. the solexa platform requires more data  to fully sequence a similar number of genes. for example, the flx generates unigenes that completely cover roughly 18% and 58% of the total genes with  <dig> mb and  <dig> mb of sequence data. the same amounts of solexa sequencing would fully sequence 4% and 25% of the genes. however, the flx experiment would cost approximately $ <dig>  and $ <dig> , whereas the solexa data could be generated for roughly $ <dig> and $ <dig> . finally, with capillary sequencing,  <dig> mb would need to be sequenced at $250k to fully cover 25% of the genes.

combinations of traditional and ng sequencing
analyses of genome sequencing projects suggest that optimal genome assemblies can be obtained through a combination of traditional and ng technologies  <cit> . in order to investigate the combination of these new technologies for transcriptome sequencing, we examined the addition of ng sequences to traditional capillary sequences  and the combinations of ng sequences alone . all of the indicators from the previous section dramatically improved with the addition of small amounts of ng sequences. among the various combinations of technologies, there is little difference in most of the indicators used in the previous section. for example, the percentage of genes tagged approaches 100% with very small amounts of ng sequences. therefore, to evaluate the various combinations of technologies, we compared three of the statistics described above: mean unigene length, transcriptome coverage, and percent of genes 100% covered.

the addition of ng sequences to traditional capillary sequences increased each of these three indicators at most sequence increments . only the addition of one plate of solexa and all gs <dig> plate increments decreased the mean unigene length . the addition of four plates of flx increased the mean unigene length to  <dig> and  <dig> bp with  <dig>  and  <dig> mb and of traditional sequences, respectively. at these same increments, transcriptome coverage would increase from 94% to 95% , while the percent of genes 100% covered would increase from 33% to 38% . the addition of this amount of flx would increase the total cost of sequencing from $40k to $ <dig> . however, sequencing only four plates of flx, assuming perfect assembly, could in theory generate 1323-bp unigenes at under $ <dig> , with approximately 94% transcriptome coverage and covering 37% of the genes 100% covered. adding four plates of solexa to four plates of flx would generate  <dig> bp unigenes at just over $ <dig>  . this amount of sequencing would cover 100% of the transcriptome  and fully sequence 84% of the genes . under these conditions, the primary advantage of including sanger sequences would be the improvement of assembly through the inclusion of long individual reads, and simplification of downstream experiments with physical clones.

estcalc: a simulation calculator for ng transcriptome sequencing experiments http://fgp.huck.psu.edu/ng_sims/ngsim.pl
with sequencing technologies rapidly advancing, researchers will wish to predict the cost and potential outcomes of diverse transcriptome sequencing projects under a wide range of initial assumptions. we have constructed estcalc, an online webtool, which allows users to explore the results of this study by specifying individualized costs and sequencing characteristics. users can choose a single sequencing method , perfectly normalized or non-normalized libraries, and varied amounts of sequencing and read lengths to predict many of the same parameters used in this study. user-defined costs can include both fixed  and per unit sequencing costs, with default costs the same as used in our study. estcalc will extrapolate from the closest treatments examined in our simulation study, and give outcomes such as the project cost, predicted number of unigenes, unigene length, transcriptome coverage, and related statistics presented earlier in this study. combinations of sequencing strategies, such as normalized plus non-normalized libraries, or combinations of different technologies, can also be examined under the same range of combinations used in our study. additional combinations, including parameter sets for solid sequencing, will be added to estcalc as they are obtained in ongoing data analyses.

discussion
next generation transcriptome sequencing
next generation sequencing has great potential for accurate transcriptome characterization because of the large amount of data obtained at considerably lower costs compared to traditional methods. although the cost of traditional sequencing  has continued to decrease over the last decade, the lower cost of ng sequencing  will dramatically improve transcriptome sequencing in future research. the overall yield and value of ng sequencing is evident in the amount of sequence data obtained in each run. we identified a large number of uniquely tagged gene sequences in each of our three cdna libraries . with only a small amount of sequencing  in arabidopsis, we tagged more than  <dig>  genes and completely or nearly completely sequenced several hundred of the highly expressed genes. even with a very modest amount of data by ng sequencing standards, many of these sequences extended the annotated untranslated regions  and redefined intron/exon boundaries, including evidence of alternative splicing. we also identified more than  <dig>  transcripts that were not previously annotated in the arabidopsis genome. these may define new genes or transcribed non-coding regions such as mirna or other small rna. in any event, these results illustrate the utility of ng transcriptome sequencing for genome annotation.

our data were limited to arabidopsis inflorescence, and there are likely to be differences in experimental outcomes using different organisms and tissue. to assess the similarity of the arabidopsis inflorescence transcriptome with other transcriptomes, we considered the distribution of intensities of the perfect match probes from several affymetrix experiments involving various tissues and organisms. we examined data from arabidopsis inflorescence, leaf, and root on ath <dig> arrays  <cit> , human skeletal muscle  <cit>  on the hgu95av <dig> array, caenorhabditis elegans   <cit>  on the c. elegans array, drosophila melanogaster   <cit> , and saccharomyces cerevisiae  on ammonium sulfite nitrogen source  <cit> . small differences are observed in the expression profiles, consistent with some samples having different proportions of genes expressed more or less highly, but overall, the distribution of expression intensities is very similar for all of the samples . these differences among samples are on the same scale, and sometimes smaller than, the variations seen among replicate samples from arabidopsis inflorescence. because the tissue-specific expression profile is the one method-independent input to the simulation model, we can expect similar predictions for transcriptomes from different sources.

ng sequencing simulation studies and comparisons of platforms
simulation studies help researchers predict outcomes of expensive or time consuming experiments that cannot be readily performed in the near term. for transcriptome sequencing, simulation studies have allowed researchers to conduct in silico experiments of systems that would be costly and time-consuming to do in the lab. we have developed a simulation approach to understand the advantages of each of the ng technologies in comparison with traditional capillary sequencing. although all technologies eventually converge at similar points with regard to unigene length, transcriptome coverage, and percentage of genes fully sequenced , the ng technologies offer huge advances, most notably in the amount of sequence generated at considerably lower costs. even though small ng experiments will tag a very large fraction of the transcriptome, it will commonly be in the form of thousands of disconnected fragments of genes, with relatively few full-length cdnas. thus, ng technologies are very effective for tagging sequences from fully sequenced species. however, researchers sequencing transcripts from novel species with few genomic resources or from species that are evolutionarily distant from a sequenced model organism might face several challenges when evaluating the data. the problems might become amplified with 25â€“ <dig> bp reads generated by the solexa system or other short-read platforms. the benefits of normalization are most evident in traditional sequencing, although some benefits, which include longer unigenes, are apparent in the ng technologies. however, the cost of normalization, and the potential for loss of closely related genes from the dataset, might outweigh the potential benefits.

although ng sequencing does outperform traditional sequencing in many areas, the problems in assembly cannot be underestimated. solexa and solid sequences, currently less than  <dig> bp, will pose problems in assembly of unigenes, especially for short segments of genes that may be present in several genes. for example, only  <dig> % of 15-mers are unique in the populus transcriptome . this leaves nearly  <dig> million 15-mers not unique to the populus transcriptome, including  <dig>  15-mers that are present in at least  <dig> different genes . until methods are developed to deal with this large fraction of sequence fragments that might lead to mis-assembled unigenes, researchers will not be able to use the solexa or solid technologies alone for transcriptome sequencing in non-model species. research into genome assembly strategies with these short sequences with and without a reference genome is currently under investigation  <cit>  and will hopefully become part of transcriptome assembly. the addition of solexa sequences to longer ng sequences or traditional capillary sequences should help assemble larger unigenes. these longer sequences will have a much higher confidence in their uniqueness. the combination of technologies for transcriptome sequencing is analogous to genome shotgun sequencing which uses varying sizes of clones.

in order to evaluate the robustness of our simulator to both different organisms and tissues, we compared our simulated results against the poppy and avocado transcriptomes generated for this study and against two recently published  <dig> transcriptomes  <cit> . vera et al.  performed de novo assembly of 454-transcriptome sequences in the butterfly melitaea cinxia. the rnas were isolated from a genetically diverse pool of larvae, pupae, and adults. the authors sequenced two plates of gs <dig>  and after trimming and cleaning, there were  <dig>  reads  with an average length of  <dig> bp. the reads assembled into  <dig>  unigenes that included  <dig>  singleton reads . the mean unigene length of  <dig> bp is the summation of  <dig> bp for all contigs plus the  <dig> bp average for the singleton reads. from our simulation results, we would predict roughly  <dig>  unigenes, 48% singletons, and an average unigene length of  <dig> bp. weber et. al sequenced  <dig> gs <dig> plates of cdna derived from aboveground tissues of 8-day old light-grown arabidopsis seedlings. the reads tagged an estimated  <dig>  cdnas, which is nearly identical to what our simulations would predict for that amount of gs <dig> data. for poppy, each of the observed assembly characteristics  are very close to the predicted values for this amount of gs <dig> sequence. for example, the average unigene numbers for each of the poppy libraries are  <dig>  and  <dig> , which are very close to the estimated  <dig>  unigenes from the simulation. for avocado, the observed number of unigenes and percent singletons  is considerably larger than predicted for a normalized library sequenced to this depth. we do not know if this unexpected large number of unigenes in avocado is due to a larger underlying transcriptome size, sequence error causing false misassembly  <cit> , or some other unknown factor. for each of these comparisons, we applied the distributions of read lengths, sequence start sites, and transcript abundance frequencies previously observed from arabidopsis. therefore, although we have not actually fine-tuned these specific outcomes with the true  transcript distribution profiles for each of species and tissue, the observed outcomes are very close to the model predictions. this is particularly true when considering the above uncertainties associated with de novo assembly, differences in tissue sources, and technical variation that may be expected from run to run.

analysis of gene expression by ng sequencing
ng sequencing is potentially a direct and cost-effective way to obtain genome scale expression information from organisms that lack a genome sequence and comprehensive microarray platform. digital expression data obtained by direct sequencing is not dependent on gene models, comprehensive genome data, or understanding of alternative splice forms. the half plate of gs <dig> sequencing in arabidopsis showed a moderate correlation  between the number of reads and microarray expression values generated from the same tissue and ecotype. solexa and solid have the potential to increase this correlation, since with millions of sequence reads per experiment, they will ultimately have a large dynamic range similar to traditional microarray experiments. correct mapping to specific genes may be problematic for short solexa or solid reads , when no genome sequence is available. enriching for 3' utrs, however, should improve assignment accuracy and increase efficiency of massively parallel sequencing for assessing gene expression levels  <cit> . in research on an organism without a sequenced genome, the value-added expression evidence could be very important in the early stages of developing its transcriptome, an advantage for gs <dig> or flx ng technologies. even in organisms that have comprehensive microarrays, the probe designs are usually dependent on and built with information from early genome assemblies. for example, the current arabidopsis affymetrix ath <dig> array  <cit>  contains probes for approximately  <dig>  genes, approximately 5000â€“ <dig> genes fewer than the current annotation.

ng sequencing can be scaled to suit different project goals
ng sequencing technologies are a highly flexible set of platforms that can be used alone or in combination to best suit the research at hand. small-scale experiments  provide a wealth of information including the tagging of many or most expressed genes, microsatellite markers, full-length sequencing of highly expressed genes, and modest expression level information. since there can be multiple lanes in ng plates , and multiple bar-coded libraries can be sequenced on a single plate, researchers might fully sequence a small number of highly expressed genes with very little cost or time investment. low-copy, highly expressed genes might be quite useful for phylogenetic analysis or markers for population level studies. even small-scale experiments will tag a large fraction of genes in a transcriptome. these tags can be used for building microarray probes  <cit>  and enhancing microarray design. small studies might sequence the highly expressed genes from many different tissues in the same sequencing runs without the need for bar-coding. small experiments might also be sufficient to provide rich information for genome annotations in pre-draft and early draft forms. for example, a single run plate will tag nearly every transcribed gene and help identify utr and intron/exon boundaries. however, experiments on this scale sample only a small fraction of the actual transcriptome and the assembly is often in many small pieces. moderate transcriptome studies  have the potential to sequence more than 50% of the transcriptome. they will provide small annotation datasets, identify new genes in an organism, further extend genic regions, and help with alternative splicing, especially in sequenced genomes. with deeper sequencing , researchers attain a level of transcriptome that has never been possible before due to the higher cost of earlier technologies. not only will these studies sequence more than 90% of the transcriptome, the coverage per gene will approach traditional sequencing. this should allow researchers to use these genes to identify pathways, determine tissue-specific expression for lowly expressed genes, and will be critical for genome annotation.

CONCLUSIONS
ng technologies are revolutionizing est sequencing and applications that revolve around gene expression. another important consideration in ng transcriptome sequencing is the efficiency and flexibility in library construction. ng library construction costs less, takes less time, and does not produce physical clones that must be stored. if the goal is sequencing full-length genes, non-normalized libraries will yield a larger number of full-length sequences in small sequencing experiments compared to normalized libraries. as sequencing quantities increase, this relationship reverses, and normalized libraries will capture more full-length cdna sequences. there is also a trade-off in the cost of normalization versus the cost of sequencing. we have shown the feasibility of using a simulation approach to quantitatively evaluate the different platforms and the various combinations of platforms. currently, the low per-base cost of solexa sequencing suggests that it may be the most efficient method of transcriptome characterization for sequenced genomes , but in the absence of a reference genome, the problem of de novo assembly of the short solexa reads has not yet been resolved. under these circumstances, a blend of solexa and gs-flx sequencing may be optimal .

this is the first simulation study to address some of the technology-specific characteristics found in several ng sequencing technologies. our approach focuses on the critical questions of data production and coverage, which differ dramatically between methods and experimental scales. by extrapolating the results of the gs <dig> simulations, we are able to predict outcomes with various ng methods and combinations. estcalc allows a variety of assumptions to be explored that will be relevant to different experimental designs with current and future transcriptome sequencing technologies. although estcalc and the underlying simulations do not currently incorporate explicit models of sequencing and assembly errors, the results provide a null hypothesis of predicted outcomes with theoretically perfect sequence data and no assembly error. deviations from these values in real transcriptome datasets and assemblies will reflect the magnitude of these errors and potential contamination of transcriptome libraries with genomic dna. these factors will tend to inflate the number of singleton reads relative to the predicted numbers without such errors; their evaluation will aid in sequence cleaning and assembly experiments. a next step will be to develop realistic models of error in sequencing and assembly, and to provide tools to allow any sets of assumptions about read length and cost to be examined. future studies should be able to build upon this first simulation study, while accounting for additional issues in transcriptome sequencing and assembly experiments.

