BACKGROUND
several studies have demonstrated the potential for reconstructing genomes from viral metagenomes  <cit> . a major obstacle for metagenomic reconstruction is the existence of highly similar regions between coexisting genomes which can lead to fragmented assemblies, like repeats in single genome assemblies. in this regard, it has become important to analyze the performance of different sequencing platforms and assemblers in a metagenomic context, and assess their strengths and limitations. initially, mavromatis et al.  <cit>  combined sanger reads from sequenced bacterial isolates to form in vitro simulated communities of different complexities and benchmark assembly and other metagenomic processing methods. pignatelli & moya  <cit>  derived short-read in silico simulated metagenomes from this work to explore facets of the assembly of high-throughput sequencing data. charuvaha & rangwala  <cit>  evaluated the effects of k-mer size on the performance of the bruijn-based assemblers. later, mende et al.  <cit>  studied the effect of quality filtering on the assembly of sanger,  <dig> and illumina metagenomic datasets, while luo et al. compared  <cit>  the assembly of  <dig> and illumina datasets from the same metagenome, and evaluated single bacterial genome reconstruction in a metagenomics setting  <cit> .

the above-mentioned studies of metagenomic assembly all used bacterial communities and mainly focused on assessing functional and taxonomic annotations. while many of their results and findings can be applicable to the study of natural viral communities, viruses present unique properties. viral genomes are usually smaller than those of bacteria and it has become affordable to obtain high coverage of viral metagenomes using current high-throughput sequencing platforms and to attempt reconstructing environmental genomes. viral genome reconstruction is an important step in the metagenomic analysis of viral communities. viruses lack a universal phylogenetic marker gene that can be used as the ribosomal genes for cellular organisms. in this sense, both accurate phylogenetic annotation and putative host description rely heavily on the almost complete reconstruction of the viral genome. additionally, the extent of intra-group variability among viruses is greater than in bacteria due to their faster evolution rates, which poses increased difficulties to the assembler. recently, vázquez-castellanos et al.  <cit>  assessed the effects of different overlap-layout-consensus  assemblers for the functional and taxonomic annotation of an in silico simulated  <dig> viral metagenome, and solonenko et al.
 <cit>  commented on how different library preparation choices bias the outcome of virome assembly.

community diversity is an important ecological characteristic of natural communities, and its estimation usually complements taxonomic and functional analyses of viral metagenomes. there are currently three different approaches to estimate viral richness in metagenomic datasets; the use of clustering , phaccs  <cit>  and catchall  <cit> . the latter two represent software tools which rely on assembly results, more precisely contig spectra for the fit of their diversity models. unfortunately, none of these methods have been the subject of a comparative performance evaluation using viromes of known diversity.

in the present study, we investigate the ability of various sequencing platform – assembler – depth  combinations to reconstruct the genomes from a high-throughput in silico simulated virome, and explore how genome relatedness impacts the success of genome reconstruction. furthermore, we evaluate the applicability of three different methods to estimate viral community diversity. collectively, our results should guide researchers undertaking deep viral metagenomic studies to adequate methods for genome reconstruction and diversity estimation, as well as understand their limitations.

RESULTS
metagenomic assembly
using a single virtual viral community, composed of  <dig> genomes with different degrees of relatedness, from both ssdna  and dsdna  viral families, we generated a large number of metagenomic reads mimicking roche’s  <dig> and illumina’s gaiix sequencing platforms. sequencing costs were kept similar for each technology , resulting in different sequencing depths. we also produced lower coverage datasets, containing 10% of the reads of these high coverage datasets, and complemented them with additional illumina miseq and hiseq low coverage metagenomic libraries.

assembly statistics
we assessed assembly of these data using three de bruijn k-mer-based assemblers, chosen either for their widespread use , or for their claimed performance in a metagenomic setting . the various platform-assembler-depth  combinations were evaluated based on contig length statistics, accuracy of the generated contigs, and comprehensiveness of the reconstructed genomes. there were some differences in the assembly statistics of different pads . based on assembly results for the high-coverage datasets , camera performed better than clc with the  <dig> dataset with respect to the maximum contig size and n <dig> parameters. for the illumina gaiix dataset, and compared to the clc assembler, idba_ud also had a much larger n <dig>  even if it produced many more contigs . we compared the effect of scaffolding on these statistics by comparing idba_ud with and without scaffolding. for both high and low coverage datasets, the most striking difference was an increase in n <dig> when scaffolding.table  <dig> 
sequencing and assembly statistics



a ) high-coverage mock

454-camera

454-clc

gaiix-clc

gaiix-idba

gaiix-idba*

b) low-coverage mock

454- camera

gaiix-idba

gaiix- idba*

miseq-idba

hiseq -idba

c) empirical

454- camera

gaiix-idba
*no scaffolding.



we then compared sequencing platforms, focusing on the assembly statistics obtained with the best tested assembler for each platform . for the high coverage datasets , illumina gaiix achieved a higher number of contigs, contig sum and n <dig> than  <dig>  similarly, for the low coverage datasets , illumina gaiix outperformed  <dig>  and even surpassed the high coverage dataset by assembling the same amount of reads in fewer, larger contigs. this outcome indicates that the simulated community may have been oversequenced by the gaiix high coverage dataset, and is consistent with the inability of bruijn k-mer based assemblers to deal with large numbers of sequencing errors, resulting in more fragmented assemblies  <cit> . illumina miseq and hiseq  contigs  were also more numerous and longer than those of  <dig>  and there were no major differences between them, other than hiseq having a slightly larger n <dig> value. all illumina platforms were able to completely recover the longest genome in the dataset .

we also analyzed the behavior of  <dig> and illumina technologies using an empirical metagenomic data derived from a single antarctic freshwater viral community . this dataset contains about 40% of the sequence information of the low coverage  <dig> and illumina gaiix datasets. in this case, it was possible to assemble more reads into contigs with illumina than with  <dig>  leading to much higher maximum contig length and n <dig> values.

contig correctness
contig length statistics alone cannot indicate the degree to which assembly faithfully reconstructed the original community. to this end, we compared the contigs generated by each pad combination to the original genomes, assessing their accuracy and chimericity. contig accuracy represents how well a contig aligns to the genome its represents, while chimericity reflects how many genomes contributed to each contig. overall, the contigs produced by all pad combinations were accurate, with average accuracy and percentage of high-accuracy contigs usually over 98% . the only exception was gaiix-clc with an accuracy of 94 ± 14%, and only 86% of contigs with high accuracy. we also noted that accuracy was more consistent  at higher sequencing coverage and when scaffolding. no notable differences in chimericity could be attributed to the use of different assembly programs . however, there were large differences between sequencing platforms, with roche  <dig> producing both less chimeric contigs and a larger fraction of perfectly non-chimeric contigs. the illumina miseq dataset also had a much larger fraction of perfectly non-chimeric contigs when compared to that of illumina hiseq . these results seem to indicate that longer sequencing reads help prevent the formation of chimeras. another remarkable result is that while the effect of scaffolding on chimericity for the high coverage gaiix-idba dataset seems marginal, scaffolding reduced the fraction of perfectly non-chimeric contigs from  <dig> % to  <dig> % for its equivalent low coverage dataset.table  <dig> 
accuracy and chimericity statistics


a
b

a) high-coverage mock

454- camera

454-clc

gaiix-clc

gaiix-idba

gaiix-idba*

b) low-coverage mock

454– camera

gaiix–idba

gaiix-idba*

miseq-idba

hiseq-idba
*no scaffolding. ± represent sds. apercentage of contigs with accuracy >90%. bpercentage of non-chimeric contigs .



contig coverage
next, we assessed how extensively the pad combinations recovered the information contained within the original genomes by calculating overall contig coverage, i.e. the percentage of genome covered by its contigs, and the maximum contig coverage, i.e. the percentage of genome covered by its longest contig. clc produced significantly lower overall contig coverage than both camera  and idba   . with regards to the sequencing technology, illumina gaiix outperformed  <dig> at high coverage. differences became more pronounced at low coverage, with values of 18 ±  <dig> and 83 ±  <dig> for camera- <dig> and gaiix-idba respectively. scaffolding produced marginal yet significantly higher overall contig coverage for the high but not low coverage dataset. no differences were observed between high and low coverage gaiix values. maximum contig coverage comparisons produced essentially the same results, with few exceptions. mainly, scaffolding had a positive effect on both the high and low coverage datasets, and the low coverage gaiix-idba produced larger maximum contig coverage values than its high coverage counterpart, again suggesting some detrimental effect associated with oversequencing. the number of genomes showing maximum contig coverage above 95%/50% followed a similar pattern than the above results.figure  <dig> 
overall and maximum contig coverage statistics. for each platform-assembler-depth combination black bars represent average overall contig coverage and gray bars average maximum contig coverage, with the error bars representing sds. within the gray bars, numbers represent the number of genomes showing max contig values above 95% and 50% respectively. high; high coverage datasets, low; low coverage datasets. *no scaffolding.



genome and community characteristics effects on genome reconstruction
it is noteworthy that the best pad combination for contig coverage  only yielded maximum contig coverage above 95% for  <dig> out of the  <dig> different genomes in the community, a figure that was not improved by increasing the sequencing effort 10-fold . this inability to obtain contigs spanning a large proportion of many original genomes, despite high sequencing coverage may be due to: i) limits to what the assembly algorithm can achieve given a particular community, either theoretically or due to imperfect design, ii) variable genome coverage along genomes , iii) large genome length, iv) the existence of repeats regions in the genome, and v) high community diversity.

several of these factors were further studied using the illumina hiseq dataset showing best overall performance and likely future use, and while more contigs derived from this dataset were chimeric compared to its miseq counterpart they were still highly accurate. differential coverage along genomes  was not studied as it was not modelled by the chosen metagenomic simulator, although some less versatile simulators include such feature  <cit> . first, we evaluated to which degree the existence of repeats regions within the genomes may have translated into low maximum contig coverage. we then analyzed the genomes in our evolved mock community for long repeats regions and found that the longest repeats region did not span more than 400 bp. hence, the existence of repeats within the original genomes was not likely the cause of low maximum contig coverage.

the initially exploration of the results  indicated that the minimum coverages attained were sufficient to recover both large and small genomes. this result shows that the coverages attained were not a limiting factor for genome reconstruction, in line with recent results showing that a bacterial genome could be recovered from a complex community with as little as 20x coverage  <cit> . moreover, genome length did not seem to have a strong influence on maximum contig coverage. on the other hand, grouping genomes by relatedness  revealed that the number of siblings per group and especially their degree of relatedness likely contributed to obtaining maximum contig coverage well below 95%.figure  <dig> 
initial exploration of maximum contig coverage vs. genomic characteristics. the diameter of each bubble positively correlates with genome length, and its color represent its complexity group; s1a <dig> represents the unmodified genomes, ‘s’ denotes number of siblings and ‘a’ the nucleotide transition rates employed . only genomes longer than 1700 nt are shown . genome coverage; for each complexity group  no apparent trend of increasing maximum contig coverage  with genome coverage  is observed. genome size; for each complexity group  no apparent trend of increasing maximum contig coverage with decreasing bubble size  is observed. number of sibling genomes; for each transition rate  the number of sibling genomes  seems to influence maximum contig coverage values attained . degree of relatedness; transition rates employed had a profound effect on maximum contig coverage values obtained .



next, we studied whether or not we had retrieved at least one almost complete genome for the intra-species groups , which exhibit the lowest maximum contig coverage . for groups of two siblings, the largest maximum contig coverage corresponded to the genome with highest coverage . however, only for one out of ten existing intra-species  sibling pairs did the maximum contig coverage value surpass the 95% threshold, and this group was characterized by the lowest coverage by others values  of the ten groups. on the other hand, the groups with eight siblings showed a strikingly different behavior, with seven out of ten groups having at least one member surpassing the 95% maximum contig coverage threshold. for six of these groups, the genome showing greatest maximum contig coverage was not the most abundant, and maximum contig coverage for the other sibling were very low. the exception as before corresponded to a group with very low coverage by others values, where all  <dig> siblings surpassed the 95% threshold. subsequent principal coordinates analyses based on pairwise nucleotide similarities of the six groups showing a similar behavior revealed that in five out of six cases the reconstructed genome represented a central genome within the group .

we then aimed to evaluate the effect caused by interferences between similar genomes by refining the results obtained in figure  <dig>  studying the genomes’ coverage by others to coverage ratio . this new measure should serve as a proxy of the possible difficulties faced by the assembler to reconstruct each genome due to the existence within the community of other genomes with highly similar regions. thirty eight genomes had no reads by others, and all but one of them had large maximum contig coverage .plotting cbo/c against maximum contig coverage values revealed a particular phenomenon ; while there is still the possibility of maximal reconstruction with high interference from other genomes, it diminished with growing ratio, and it seems to be unrelated from genome length. hence, there is a tendency of diminishing maximum contig coverage with increasing cbo/c, but with noticeable dispersion. we further studied the main outlier group representing genomes with both very large cbo/c and maximum contig coverage . interestingly, these genomes belonged to the groups of  <dig> intra-species siblings  previously shown to produce a single reconstructed genome in detriment of its siblings’ maximum contig coverage. this indicates that the dispersion observed from the prominent tendency of diminishing maximum contig coverage with increasing cbo/c is due to foreign reads being assembled during he reconstruction of particular genomes to the detriment of the reconstruction of their original genomes .figure  <dig> 
bubble chart of maximum contig coverage vs cbo/c ratio. the diameter of each bubble positively correlates with genome length. grey bubbles represent the single reconstructed genomes from the groups of eight highly similar siblings .



estimating community diversity
we used phaccs and catchall to estimate the number of viral species in the evolved simulated community using contig spectra derived from sub-samples of the illumina gaiix dataset. since we observed that the assemblers were not able to resolve all cases between highly similar genomes we expected richness estimates ca.  <dig> –  <dig>  however, both methods over-estimated the number of species , with catchall being always  <dig> to  <dig> orders of magnitudes off compared to phaccs, and richness estimates increased with sequencing depth.

to further investigate the accuracy of viral diversity estimation tools, we tested them on  <dig> simulated communities with different richness and evenness. for comparison, we complemented this analysis with richness estimates based on the number of clusters formed by uclust. the richness estimates of catchall and uclust were orders of magnitude higher than those obtained by phaccs, which was the closest to the expected richness . while not absolutely accurate, the estimates produced by phaccs were consistent with community richness and evenness . when accounting for average genome length in the community , uclust estimates improved dramatically, while catchall estimates were still at least one order of magnitude higher than expected .figure  <dig> 
boxplots of richness estimates. boxplots of richness estimates  generated by uclust, phaccs and catchall for four types of community structure. agl: actual genome length.



discussion
our ability to accurately reconstruct the viral genomes within a deep metagenomic dataset represents a black box. assembly success is usually described in terms of rather subjective proxies such as assembly statistics, or the number of apparently complete genomes recovered. however, as we remain ignorant of the richness, structure, and genetic diversity of the community it is not possible to work with more objective measures of success. this is especially true for viruses that have no ribosomal genes to help us estimate its composition, richness and structure. in this study, we have shed some light on this black box’s internal functions and mechanisms.

the assembly statistics derived from each pad combination revealed that both camera and idba_ud outperformed the popular commercial assembler clc in terms of both total information in contigs  and n <dig>  likely due to the fact that they have been specifically developed for metagenomic studies. for the high coverage datasets, both  <dig> and illumina performed similarly. however, for the low coverage scenarios all illumina platforms greatly outperformed  <dig>  with hiseq performing slightly better than miseq. this result was apparent in the empirical metagenomic datasets, where illumina was able to recover  <dig> times the amount of bases in contigs of  <dig> 

since single point errors in contigs can alter gene calling and predicted translated proteins, contig accuracy is important. the contigs and scaffolds produced by all pad combinations were generally highly accurate, but clc exhibited a lower accuracy that could substantially compromise annotation efforts. concerning chimericity, no major differences between assemblers were observed. however, the  <dig> platform produced less chimeric contigs and a larger fraction of non-chimeric contigs than the illumina platforms. moreover, miseq also produced a larger fraction of perfectly non-chimeric contigs compared to hiseq. both results indicate that read length correlates negatively with chimericity.

overall, scaffolding marginally improved both contig length statistics and accuracy, with the tradeoff of increasing contig chimericity, which is consistent with previous results  <cit> . both overall and maximum contig coverage derived from clc assemblies were lower than for their camera and idba_ud counterparts.  <dig> produced lower yet relatively similar values when compared to illumina gaiix for the high coverage datasets. however, its ability to reconstruct the original genomes  was much reduced in the low coverage dataset, when compared to all illumina platforms.

interestingly, while some large genomes were recovered completely as a single contig, we could not obtain contigs spanning a large proportion of many original genomes despite the high coverage values attained. this issue was further explored using the illumina hiseq dataset, which showed that genome length, sequencing coverage, or the existence of repeat regions had little effect on genome reconstruction. however, the co-existence of highly similar genomes within the community had a strong effect on genome reconstruction. it is noteworthy that usually, for pairs of highly similar genomes, the largest maximum contig coverage corresponded to the most abundant genome. however, in the more complex cases with eight intra-species genomes, a single genome was normally reconstructed. instead of representing the most abundant species, the reconstructed genome tended to be that showing closest similarity to its siblings. in this sense, it seems that it is not possible to ascertain whether a retrieved genome corresponds to a single strain, or rather to a group of highly similar strains. one could take steps to study the intra-population structure of a given genome, e.g. by re-mapping onto it all metagenomic reads and then evaluating its nucleotide diversity, or using more sophisticated software  <cit> .

despite advances in sequencing technologies and bioinformatic tools, the assembly of viral metagenomes thus remains incomplete. since all genomes cannot be reconstructed, even with very high sequencing coverage, methods to estimate viral diversity in deep metagenomes remain highly relevant. phaccs and catchall are sophisticated tools that model community diversity based on the assembly of metagenomes, while clustering is a simple method that provides a proxy for viral richness and has been used to generate rarefaction curves. using  <dig> simulated viromes, we showed that catchall results are orders of magnitude higher than expected, due to its underlying assumption that each contig belongs to a different viral genome. the use of a discounted model was not sufficient to alleviate this fundamental limitation. by its nature, clustering creates many clusters for each genome and its richness estimates can thus be considered upper bounds of viral richness. however, correcting clustering results for genome length dramatically improved richness estimates, which suggest a new direction for modeling viral diversity in metagenomes. phaccs was the most accurate of the tested tools, reaching a  <dig> % ± <dig>  relative error. note that these results were obtained using the more thorough ‘cha’ mode and providing the exact average genome length in the community, which is rarely known with precision. we suggest that assuming an average for the viral genome length limits the absolute accuracy of phaccs. however, phaccs richness and evenness estimates were consistent with community structure, which makes phaccs well suited for the estimation of viral diversity of communities in comparative studies.

CONCLUSIONS
the amount of metagenomic information available for genome reconstruction had a profound effect on assembly success, as evidenced by the low performance demonstrated by the low coverage and empirical  <dig> datasets. however, despite the fact that all illumina combinations tested likely presented per-genome coverage in excess of what seems to be needed for accurate genome reconstruction, they were unable to recover all genomes in the community, because of the presence within the community of genomes bearing highly similar regions. the assemblers were nevertheless generally able to recover at least a single genome from a highly similar group of genomes. overall, we recommend the use of illumina platforms such as hiseq and miseq, bearing in mind that oversequencing may be detrimental,and a metagenomic-aware assembler such as idba-ud for the assembly of viral metagenomes; this pad combinations provide good value for money, and yield long, accurate contigs.

deep metagenomic studies can be complemented by analyses of community diversity, some of which are based on contig assembly. while our simulation results argue against the use of catchall for this purpose, phaccs was shown to be well suited for comparative work. clustering might also prove a worthy alternative in the future, provided average genome length in the community is taken into account.

