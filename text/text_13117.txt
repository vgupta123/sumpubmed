BACKGROUND
modern experimental techniques in genetic research such as microarray experiments or gene association studies produce high dimensional data and often thousands of hypotheses are tested simultaneously to identify genetic markers. due to limited resources, the number of measurements per marker in a conventional single-stage design is often low. two-stage designs have been proposed where in a first stage promising markers are identified from the set of all markers considered initially. thus, hypotheses corresponding to unpromising markers can be dropped in the interim analysis such that the second stage is performed with the reduced set of selected hypotheses. given limited total resources or budgets, this allows the allocation of a larger number of observations to more promising hypotheses. it has been shown that such sequential procedures are typically considerably more powerful than single-stage designs
 <cit> .

an important problem when drawing inference from data produced by such designs is the construction of hypothesis tests that control the false discovery rate . while the construction of such test procedures is straightforward if only the second-stage data is used for testing, tests that make use of the data from both stages need to account for the specific selection rule used to select hypotheses for the second stage.

for two-stage procedures where in an interim analysis all hypotheses with an unadjusted first-stage p-value below a pre-fixed selection boundary γ1are selected for the second stage, hypothesis tests based on the pooled data from both stages have been proposed that control fdr or the familywise error rate
 <cit>  . selecting all hypotheses whose first-stage p-value lies below a fixed threshold has several implications. first, the number of hypotheses selected for the second stage is a random variable unknown a priori, which is an obstacle for researchers if resources are limited and the number of markers for which further measurements can be collected cannot be arbitrarily increased. second, because rather large thresholds need to be applied in the interim analysis in order not to miss alternative hypotheses in spite of the small sample size, the fixed threshold rule will, with a high probability, select some hypotheses even under the global null hypothesis, even though resources could be saved in this scenario by stopping the experiment for futility at the interim analysis.

in this work we propose statistical tests to control the fdr in two-stage designs with selection rules that are not based on a fixed threshold for the first-stage p-values. first, we consider designs where a fixed number of hypotheses is selected for the second stage 
 <cit> . the selected hypotheses are the top-ranked hypotheses according to the first stage p-values. second, we consider two-stage designs where selection is based on a fixed fdr threshold α <dig> in the interim analysis . all hypotheses that can be rejected with a test controlling the fdr at level α <dig> are selected for the second stage
 <cit> . for the fns design the number of continued hypotheses is deterministic and the procedure continues to the second stage, regardless of the actual effect sizes observed. for the fdrs design in contrast, the number of selected hypotheses is a random variable. furthermore, under the global null hypothesis  with probability 1−α <dig> no hypothesis can be rejected with the interim test at fdr level α1and the trial is stopped for futility.

a simple approach to construct hypothesis tests controlling the fdr for two-stage designs is to consider tests based on the second-stage data only. standard multiple testing procedures applied to the second-stage data will control the fdr. for the fdrs design benjamini and yekutieli
 <cit>  showed that the nominal level applied at the second stage may be even increased taking into account the interim selection threshold. however, these approaches do not make full use of the available data because the first-stage observations are used for selection only. we construct tests for the fns and fdrs designs that are based on sufficient test statistics of the data from both stages . these tests are designed in analogy to group sequential tests and appear to control the fdr well if the number of hypotheses tested is large enough.

the paper is structured as follows: in the next section the testing problem and the selection rules are introduced. then the results of a simulation study which investigates the actual fdr and compares the mean number of rejected alternatives of the integrated approach to the pilot approach are presented. finally, a real data example and a short discussion are given.

methods
the test problem
we consider an experiment to test m two-sided null hypotheses h0i: μi= <dig> versus h1i: μi≠ <dig> 
i= <dig> …,m, for the mean of independent, normally distributed observations with variances
σi <dig> 
i= <dig> …,m. more general distributional scenarios are discussed at the end of this section.

to adjust for multiple testing we aim to control the fdr of the experiment. the fdr
 <cit>  is defined as the expectation of the proportion of erroneously rejected hypotheses v  among all rejected hypotheses r,
fdr=e. we apply the step-up benjamini-hochberg  procedure to control the fdr at level α. denote the ordered p-values of the m hypotheses by
p≤p≤⋯≤p and let d = argmaxi { p  ≤ iα / m } denote the index of the largest p-value p smaller than or equal to iα/m. then the bh-procedure rejects all hypotheses h0isuch that p≤dα/m. for single-stage tests it has been shown by benjamini and yekutieli
 <cit>  that the bh-procedure controls the fdr at level Π0α, if the subset of test statistics corresponding to true null hypotheses are independent or positively regression dependent. here, Π <dig> denotes the  proportion of true null hypotheses among the m tested null hypotheses. furthermore, the fdr is asymptotically controlled  if the limiting fraction of true null hypotheses is less than one and the test statistics are weakly dependent such that their empirical distribution functions converge almost surely 
 <cit> . in the following we assume distributional scenarios where the bh-procedures controls the fdr.

the two-stage procedure
in the first-stage for each hypothesis n1observations are collected. then an interim analysis is performed and for each hypothesis a two-sided first-stage p-value
pi=2|),
i= <dig> …,m, is calculated, where
zi denotes the standardized first-stage mean for hypothesis i and Φ the cumulative distribution function of the standard normal distribution. the first-stage p-values are ranked according to their magnitude and the m <dig> hypotheses with the smallest p-value are selected for the second stage. the number of selected hypotheses m <dig> can be either a pre-fixed number or may depend on the first-stage results. below we consider several choices for m <dig>  in a second stage for each selected hypothesis n <dig> observations are collected. n <dig> is assumed to be fixed and does not depend on the number of selected hypotheses. we consider two different approaches to arrive at the final test decision: the “integrated approach”, where the test decision is based on the combined data of both stages and the “pilot approach”, where the test decision is based on the second-stage data only.

in the following we introduce several rules to determine the number of selected hypotheses m <dig> 

selection rules for two-stage designs
selection according to a prefixed selection boundary γ1
two-stage designs have been proposed
 <cit>  where a selection boundary γ <dig> is pre-specified and in the interim analysis all hypotheses with a first-stage p-value smaller than γ <dig> are selected for the second stage. then 

  m2=∑i=1m1{pi≤γ1}, 

 where  <dig> is the indicator function which equals  <dig> if the condition in the parentheses is satisfied and  <dig> otherwise. thus, m <dig> is a random variable.

pre-fixed number of hypotheses selected for the second stage - fns design
with this rule the value of m <dig> is fixed a priori and the m <dig> hypotheses with the smallest first-stage p-values are selected for the second stage. we denote this procedure fns design .

selection based on an fdr threshold - fdrs design
in this approach all hypotheses which are significant according to the bh-procedure at a prefixed level α1>αin the interim analysis are selected for the second stage. thus, the number of selected hypotheses m <dig> is a random variable which depends on the first-stage results. if no hypothesis can be rejected at level α <dig> in the interim analysis and thus be carried over to the second stage, m <dig> is set to zero. in this case the whole experiment is stopped. note that under the global null hypothesis, i.e. in the setting where all null hypotheses are true and thus Π0= <dig>  this occurs with a probability of 1−α <dig> 

fdr control
in the subsections below we review the fdr controlling test procedure for two-stage designs where hypotheses are selected based on a prefixed selection boundary applied to the first-stage p-values. both pilot and integrated designs are considered. we then propose generalizations of these test procedures to fns and fdrs designs, showing that for each of the designs a corresponding data dependent selection boundary for the first-stage p-values can be defined that converges under suitable assumptions almost surely to a fixed value. the results are derived for independent data. however, in genetic data dependence of test statistics is frequently observed and even weak dependence may be a strong assumption. in the simulation study we thus investigate the performances of the procedures for several correlation structures.

selection according to a prefixed selection boundary γ1
pilot approach
in the pilot approach the final test statistics are based on data from the second stage only. the first-stage data are used for selecting promising hypotheses only. to control the fdr of the experiment, at the end of the trial for each hypothesis a two-sided p-value
pi=2)|),
i= <dig> …,m <dig>  is calculated, where
zi denotes the standardized mean of the second-stage observations for hypothesis i. then the bh-procedure is applied to these p-values which are based on the second-stage data only. because the first-stage observations are used only for selection and do not enter the final test statistics, the bh-procedure controlling the fdr at the second stage controls the fdr overall.

integrated approach
if the data from both stages are to be used in the final test decision, one can account for the selection in the interim analysis by calculating sequential p-values pi,
i=1…,m, based on the monotonic ordering of the sample space
 <cit> . if
pi>γ <dig> then the two-sided sequential p-value is defined as 

  pi=pi=pho|zi|≥|zi| 

and if
pi≤γ <dig> it is given by 

  pi=pho{|zi|≥|zi|}∩{|zi|≥c1−γ1/2}, 

where zi denotes the standardized overall mean of the observations from both stages and
zi the standardized mean of the observations in the first stage. furthermore,
zi,zi denote realizations of the random variables
zi,zi and
c1−γ1/ <dig> the -quantile of the standard normal distribution. if the stopping criterion is satisfied the sequential p-value is just the classical fixed sample p-value calculated from the first-stage observations. otherwise, the calculation of the sequential p-value involves the numerical solution of an integral . finally, the bh-procedure is applied to the sequential p-values
p <dig> …,pm.

in a two-stage procedure with fixed per hypothesis sample sizes n <dig>  n <dig> and a fixed selection boundary γ <dig> the sequential p-values are uniformly distributed under the null hypothesis
 <cit> .

if for the subset of true null hypotheses the observations are independent across hypotheses such that the sequential p-values are independent as well, it follows that the fdr is controlled in such a two-stage design. as described above fdr control holds also if the sequential p-values corresponding to true null hypotheses are positive regression dependent
 <cit> .

next we extend the test procedure to the fns and fdrs design.

fns design
pilot approach
for the fns selection rule the fdr control with the pilot approach is straightforward: the bh-procedure can be applied to the second-stage p-values of the m <dig> selected hypotheses. because the first-stage data do not enter the final test statistics, fdr control is guaranteed under the assumption of positive regression dependency.

integrated approach
to utilize the data from both stages for the final test decision, we propose to compute sequential p-values in analogy to , replacing the fixed selection boundary γ <dig>  by the value of the largest first-stage p-value of all selected hypotheses, i.e., setting
γ1=p. thus, the threshold γ <dig> is now data dependent. because this is not accounted for in the calculation of the sequential p-values, they may no longer be uniformly distributed under the null hypothesis and it is not obvious if the fdr is still controlled. however, if the observations are sufficiently independent across hypotheses, such that the empirical distribution functions of the first-stage test statistics converge almost surely as the number of tested hypotheses increases , γ <dig> converges almost surely to a fixed number. thus, asymptotically γ <dig> is deterministic and for large m and m <dig> the procedure becomes similar to the method with a prefixed selection boundary.

note that with the integrated two-stage testing procedure, hypotheses that have not been selected in the interim analysis can in principle be rejected in the final test. especially, if m <dig> is small compared to the number of hypotheses for which the alternative holds and the effect sizes are large, hypotheses that were not selected at the interim analysis can be rejected at the end because for every hypothesis a sequential p-value is calculated . rejection of non-selected hypotheses can occur in an overpowered fns design where only few hypotheses are selected, but even the sequential p-values corresponding to non-selected alternative hypotheses are small enough to lead to a rejection. if such rejections occur, this is an indication that the first-stage sample size has been chosen too large and no second-stage sample would have been needed to reach sufficient power. while the efficiency of such a design can be improved by choosing appropriate first-stage sample sizes and selection rules, the control of the fdr is not affected.

fdrs design
pilot approach
as for the fns selection rule, if the bh-procedure is applied at nominal level α to the second-stage p-values of the m <dig> selected hypotheses , fdr control is guaranteed. however, as benjamini and yekutieli
 <cit>  showed, if, in a first stage, hypotheses are selected that can be rejected with the bh-procedure at nominal level α <dig>  and, in a second stage, the selected hypotheses are tested at nominal level α <dig>  the fdr of the second-stage test is actually controlled at level α1α2Π <dig>  given the test statistics at each stage are positively regression dependent
 <cit> . thus, if in the second stage the nominal level α/α <dig> is applied, the fdr is still controlled at level Π0α. in the following we consider the latter, improved procedure.

integrated approach
similar to the fns rule we propose to compute sequential p-values in analogy to  to utilize the data from both stages for the final test decision.

again, the resulting threshold γ <dig> is data dependent: we set γ1=m2α1/mwhere m <dig> is a random variable. then γ <dig> is approximately equal to the largest first-stage p-value of all selected hypotheses. thus the sequential p-values may no longer be uniformly distributed under the null hypothesis and fdr control is in question. however, the following argument gives a heuristic for fdr control when the number of hypotheses is large. if for a positive proportion of hypotheses the alternative holds the empirical distribution functions of the first-stage test statistics converge almost surely as the number of tested hypotheses increases , it can be shown that γ <dig> converges almost surely. hence, in these settings γ <dig> is asymptotically deterministic.

under the global null hypothesis γ <dig> does not converge and simulations  show that the fdr is actually inflated. therefore, we suggest the following modification of the test procedure. let ms >  <dig> denote a positive constant. in cases where less than ms hypotheses are selected by the fdrs selection rule the threshold γ <dig> used in  is set to the ms-smallest first-stage p-value, thus
γ1=max,p). note that this modification increases the first-stage critical boundary used in the calculation of the sequential p-value.

generalizations to other testing problems
the procedure can be directly generalized to two group comparisons, replacing the standardized means by the standardized mean between group differences. more generally, the sequential p-value can be computed as in  if  the cumulative test statistics follow a multivariate normal distribution with mean zero and variance one. in the actual computation based on  the term n1/ ) is then replaced by the correlation ρ. for example, for testing problems as the comparison of rates, or the analysis of co-variances, the standardized means can be replaced by standardized efficient score statistics that are asymptotically normally distributed. the correlation between these test statistics is determined by the information fractions .

RESULTS
first we investigate the actual fdr of the proposed testing procedures for the fns and fdrs selection rules. additionally, to quantify the advantage in power of the integrated approach compared to the pilot approach, we report the mean number of rejected alternatives under different scenarios. we consider the one-sample z-test for m two-sided null hypotheses h0i: μi= <dig> versus h1i: μi≠ <dig> 
i= <dig> …,m, for the mean of normally distributed observations with nominal significance level α= <dig> . the simulations are performed for a wide range of scenarios. for a detailed description see additional file
 <dig> 

in the following we assume independence of test statistics across hypotheses. however, because this assumption is often not satisfied in genetic data, we also report simulations assuming several correlation structures.

all computations were performed using the statistical language r
 <cit> . r-code to reanalyse the real data application is available for download from the authors’ web page
http://statistics.msi.meduniwien.ac.at/index.php?page=pageszfnr.

simulation results for the fns procedure
control of the error rate
integrated approach: in all simulated scenarios the fdr is well controlled if m <dig> >  <dig> . only if a smaller m <dig> is chosen, the fdr may be inflated up to  <dig> .

a heuristic explanation for this inflation is that for very small m <dig> the p-value threshold corresponding to the fns design,
p, is close to zero. for such low p-value thresholds even small changes may lead to large changes in the sequential p-value  and the approximation based on a fixed threshold is poor. figure
1c illustrates the decrease of the fdr with increasing m <dig> for two particular scenarios.

pilot approach: for the pilot approach the fdr is controlled in all scenarios.

mean number of rejected hypotheses
table
 <dig> shows the mean number of rejected alternatives for selected scenarios for the integrated approach controlling the fdr and the improvement in percent compared to the pilot approach. while the simulation study investigating fdr control covers Π <dig> values from  <dig>  to  <dig> for the investigation of the power we consider settings where alternative hypotheses are sparse. these are settings where the advantage of two-stage designs that select promising hypotheses at interim analysis is expected to be largest.

in all scenarios the integrated approach rejects more or the same number of alternative hypotheses than the pilot approach. the increase in rejections is up to 59%. figure
1d shows the impact of m <dig> on the mean number of rejected alternatives for the integrated  and the pilot approach : for Δ= <dig>  and very small m <dig>  the number of rejected alternatives is very small but it clearly increases with m <dig>  here the difference to the pilot design is more distinct. for Δ= <dig>  the advantage of the integrated approach is only moderate.

simulation results for the fdrs procedure
control of the error rate
integrated approach: for the original procedure  and Π <dig> <  <dig>  the fdr is controlled for all considered values of m, α <dig> and Δ . for larger values of Π <dig> the fdr may be inflated, especially if the effect size under the alternative is low such that the expected number of selected hypotheses for the second stage is very small. the inflation is, however, moderate and the maximal fdr over all simulation scenarios is  <dig>  instead of the nominal  <dig> .

the simulations for the modified procedure show that across all scenarios the fdr is controlled for ms= <dig> . therefore, in the following we only report results of the modified procedure with ms= <dig>  note that for some of the parameter values the modified procedure is strictly conservative.

for the pilot approach fdr control follows by theoretical arguments in
 <cit>  and is confirmed by the simulation study .

mean number of rejected hypotheses
table
 <dig> and figure
1b show that the number of rejected alternatives increases with α <dig> as expected. for small values of α <dig>  the pilot and the integrated approach have similar power values. in some settings for lower m the pilot design even slightly outperforms the integrated design. this is due to the fact that the modified fdrs procedure may be strictly conservative, especially if the number of selected hypotheses is low.

if the first-stage sample size is increased, the advantage of the integrated approach increases: e.g., for n1=n2= <dig> and Π0= <dig> , Δ= <dig>  α1= <dig> , the mean number of rejected hypotheses is 22% larger for the integrated approach than for the pilot approach.

correlated test statistics
test statistics from genetic data are often stochastically dependent across hypotheses. in this section we study the impact of correlation between test statistics on the fdr and consider auto-correlation, block-correlation
 <cit>  and equi-correlation
 <cit> . auto-correlation may occur for example in microarray data because of spatial artefacts on the array or in gene association studies due to correlation between neighbouring markers. a block correlation structure, also called clumpy correlation, may be induced in microarray data for example by pathways of genes that are commonly regulated
 <cit> . finally, equi-correlation can be due to ‘array effects’ in microarray analyses.

for auto-correlation we consider an order among hypotheses and assume an autoregressive correlation structure. here the correlation between the test statistics for hypotheses i and j is given by ρ|i−j|. for block-correlation we assume that the test statistics are correlated in blocks of  <dig> hypotheses where the correlation between the test statistics within one block is ρ <cit> . hypotheses of different blocks are assumed to be independent. for equi-correlation we assume that for all pairs of hypotheses a pairwise correlation of ρholds. for all correlation structures the alternatives are randomly distributed among the sequence of hypotheses. the simulations with correlated data were performed for the scenarios m∈{ <dig> ,100000}, m <dig> ∈{ <dig> m, <dig> m, <dig> m}, Δ∈{ <dig> .6}), and Π <dig> ∈{ <dig> , <dig> ,1} with correlation coefficient ρ= <dig> .

for block-correlation and auto-correlation the results are very similar to the independent case concerning the actual fdr. the mean number of rejected alternatives for the pilot and the integrated design are nearly identical . for equi-correlated data the error rates of both selection procedures are maintained in all scenarios, even under the global null hypothesis. however, for most scenarios the procedure appears to be more conservative compared to the independent case. for scenarios with small Δ the mean number of rejected alternatives and the superiority of the integrated designs increases for the fdrs design . for the fns design the differences between the integrated and the pilot design decrease . note, however, that equi-correlation can be reduced or removed by adequate normalization
 <cit> .

real data application
we reanalysed the microarray data set by tian
 <cit> . in this experiment gene expression data were compared between  <dig> patients where bone lytic lesions could be detected by magnetic resonance imaging and  <dig> controls where such lesions could not be detected for  <dig> probe sets. we used the pre-processed data set by jeffery
 <cit> .

to obtain balanced group sizes for the re-analysis we arbitrarily selected  <dig> patients from the bone lytic lesions group. the samples were arbitrarily allocated to the two stages and the pilot and the integrated approach were applied for the fns and the fdrs procedure and different parameters: n1={ <dig> } , m2={ <dig> , <dig> }, α1={ <dig> , <dig> , <dig> , <dig> }, ms= <dig>  in the first stage for all procedures a two-sided t-test was computed. for the integrated procedures we computed sequential p-values based on  using a normal approximation where the critical values from the model with known variances are applied to the p-values of the t-test.

table
 <dig> shows that in most scenarios the integrated procedure rejects more hypotheses than the corresponding pilot procedure for both selection rules. this difference is larger for larger first stage sample sizes and for increasing parameters m <dig> or α <dig>  respectively. only for small α <dig> and n1= <dig> the integrated and the pilot approach of the fdrs procedure reject approximately the same number of hypotheses. note that no hypothesis was significant at the final test decision which was not considered in the second stage. setting ms= <dig> the results for the integrated fdrs procedure did not change.

discussion and 
CONCLUSIONS
in this paper we discussed several selection rules for two-stage designs, where after an interim analysis only promising hypotheses are considered in the second stage.

for the choice of the selection rule, different criteria may apply. with the fns design, the total number of observations is known in advance, which facilitates the planning of resources. however, this design does not adapt to the number of hypotheses that show an effect in the interim analysis. the latter can be achieved with the fdrs design, where, on the other hand, the total number of observations is random and the planning of resources becomes more difficult. as an extension one can consider an fdrs design where the overall number of observations  is fixed and the observations allocated to the second stage are equally distributed among the selected hypotheses. this comes at the cost of a decreasing per hypothesis power if for a larger number of hypotheses the alternative holds.

for the fns design the testing procedures provided a sound control in the considered scenarios where more than  <dig> hypotheses are selected for the second stage for independent as well as for correlated data. also for the modified fdrs procedure fdr control is given in all scenarios for ms >  <dig>  comparing the integrated approaches for both selection rules with the corresponding pilot approaches showed an advantage of the integrated approach in many scenarios. this holds particularly for the fns design but in many scenarios also for the fdrs design. the advantage of the integrated design increases with the proportion of observations allocated to the first stage. this is in line with earlier findings
 <cit> , where scenarios with small first-stage sample sizes were considered and only small differences between the integrated and the pilot design have been observed. in particular, if the effect sizes in microarray studies are low  and the number of observations in the first stage is sufficiently large compared to the number of observations for the second stage, the integrated design is superior.

on the other hand, using only the second-stage data for testing has the advantage of increased flexibility and simplicity. for example, the pilot fns procedure controls the fdr even if the hypotheses for the second stage are selected in an arbitrary way. furthermore, standard  tests can be applied and fdr control can be shown analytically under suitable assumptions.

in the simulations the bh-procedure was applied to the sequential p-values to control the fdr. as described above, this method is conservative if Π <dig> <  <dig> as it controls the fdr actually at level Π0α. following the suggestion of one of the referees, we additionally considered so called adaptive fdr controlling procedures that are based on an estimate of Π <dig>  under independence these adaptive tests are less conservative then the bh-tests, but did not exceed the nominal level in the considered simulation scenarios. however, as shown earlier  under strong correlation adaptive procedures may inflate the fdr.

it is well known that two-stage designs may lead to a considerable improvement in efficiency compared to single-stage designs
 <cit>  and this applies also to the procedures investigated in this paper . furthermore, the methods can be extended to designs where an explicit early rejection boundary is applied in the interim analysis as in many group-sequential applications. in this case the calculation of the sequential p-values is slightly modified . however, unless the fraction of hypotheses for which the alternative holds is large, it is expected that the addition of an early rejection boundary at the interim analysis has only a marginal impact on the efficiency of the procedure. furthermore, for hypotheses that are rejected in the interim analysis based on few observations, a confirmation with a larger sample size might be important anyway.

appendix
asymptotic considerations
in this section we argue that asymptotically, for increasing number of hypotheses, the fns and the fdrs selection rule are equivalent to a selection rule where hypotheses are selected based on a fixed threshold γ <dig>  let
r=♯{pi≤γ} denote the number of hypotheses with a first-stage p-value not exceeding γ, and v) the respective number of p-values corresponding to true null  hypotheses, respectively. let m <dig> and m <dig> denote the number of true null and alternative hypotheses, respectively. consider the following assumptions: 

the empirical distribution functions of the first-stage p-values corresponding to the null and the alternative hypotheses converge almost surely, i.e.,
limm→∞v/m0=γ and
limm→∞s/m1=f <dig> exist for all γ∈=Π0p + f <dig> denote the limiting distribution function of the first-stage p-values. by the continuity and strict monotonicity of g there is a unique
γ1∞ such that
g=δ. now,
limm→∞p=γ1∞ almost surely. if we additionally assume that for all finite m the distribution of a  p-value is given by g with density g, then γ <dig> is the m2/m quantile of g. thus, its variance is given by
/2) <cit> .

for the fdrs procedure γ1=m2/mα corresponds to the critical value that results from the bh-procedure. if  and  hold and Π0< <dig>  it follows as in
 <cit>   that this critical value converges almost surely.

computation of the two-sided sequential p-value
if the hypothesis hi,i= <dig> …,mis selected for the second stage , the two-sided sequential p-value given by  is calculated by numerical integration: 

  pi=∫c1−γ1/2∞1−Φφdz+∫c1−γ1/2∞Φφdz+∫−∞cγ1/21−Φφdz+∫−∞cγ1/2Φφdz 

with
a=n1n1+n2z and
b=n2n1+n <dig>  here φ, Φ, and
c1−γ1/ <dig> denote the density, the cumulative distribution function, and the -quantile of the standard normal distribution, respectively. if the first-stage p-value is larger than γ <dig> 
pi=pi.

competing interests
both authors have no competing interests.

author’s contributions
both authors contributed equally to the development of the methods, the design of the simulations, and to writing the paper. sz conducted the simulations and data analyses. all authors read and approved the final manuscript.

authors’ information
the views expressed are those of the author  and should not be understood or quoted as being made on behalf of the european medicines agency or its scientific committees.

supplementary material
additional file 1
we report the simulation scenarios and results of the simulation study assessing the fdr of the fns and fdrs design  for the case of independent test statistics as described in the results section of the manuscript. for each scenario at least  <dig> simulation runs were performed. for scenarios with lower m the simulation runs were increased to  <dig> ,  <dig> , and  <dig> , because in these scenarios there is a higher variability of the false discovery proportion such that the estimator of the fdr converges slower. this also holds if m is large but Π <dig> ≈  <dig> or Δ is small. therefore, for these scenarios the number of simulation runs was increased. the resulting fdr values were plotted as a function of α <dig> for the fdrs design  or as a function of m <dig> for the fns design , respectively.

click here for file

 additional file 2
results of a simulation study for two-stage designs where an adaptive test procedures is applied based on an estimator for the proportion of true null hypotheses.

click here for file

 additional file 3
two single-stage designs are compared to the results: for the first single-stage design the sample size for each hypothesis is n <dig>  for the second design the sample size is n <dig> + n <dig>  for the first design we compare the gain in power of the integrated design and for the second design the attention lies on the reduction in costs.

click here for file

 acknowledgements
we would like to thank the two reviewers for helpful suggestions and julia saperia for many helpful comments.

this work was supported by the austrian science fund fwf .
