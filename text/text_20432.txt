BACKGROUND
the key notion of translational research is the flow of information resources  across organizations, domains, and projects that impacts both patient care and  basic research. this necessitates keeping track of the provenance metadata of resources from the point of their creation to intermediate processing, and finally their end use. provenance, derived from the french term provenir meaning "to come from", has traditionally played an important role in keeping track of cultural artifacts, such as paintings and sculpture, but is also rapidly becoming a key component of the high-throughput data generation and computing infrastructure used in translational research. figure  <dig> illustrates the four phases of the provenance life cycle, both before the publication of scientific data and results in literature or submission to data repositories  and the use of the results by data mining or knowledge discovery applications .

during the pre-publication phases , provenance is collected to describe the experiment design, such as details about the biological or technical replication  in microarray experiments, the type of parasite used to create an avirulent strain, or the demographic information used in a clinical trial  <cit> . similarly, provenance information about the experiment platform  and the tools used to process or analyze data  is also collected  <cit> .

in the post-publication phase , data mining and knowledge discovery applications use provenance associated with the data extracted from peer-reviewed literature , public data repositories , and web resources  to guide analysis algorithms and interpretation of results  <cit> . specifically, the provenance information in post-publication phase is used to constrain extraction processes to reputable sources , clustering datasets according to their source, and ranking results based on the timestamp or authorship information  <cit> .

the semantic problem solving environment for t.cruzi project 
t.cruzi is the principal causative agent of the human chagas disease and affects approximately  <dig> million people, predominantly in latin america. about  <dig> percent of these affected persons are predicted to eventually suffer from chagas disease, which is the leading cause of heart disease and sudden death in middle-aged adults in the region. research in t.cruzi has reached a critical juncture with the publication of its genome in  <dig>  <cit>  and can potentially improve human health significantly. but, mirroring the challenges in other translational research projects, current efforts to identify vaccine candidates in t.cruzi and development of diagnostic techniques for identification of best antigens, depend on analysis of vast amounts of information from diverse sources. to address this challenge, the semantic problem solving environment  for t.cruzi project has created an ontology-driven integration environment for multi-modal local and public data along with the provenance metadata to answer biological queries at multiple levels of granularity  <cit> .

reverse genetics is one of the several experiment methods used in the study of the t.cruzi parasite and involves the creation of avirulent  strains of the parasite in the laboratory  <cit> . the process to create a new strain  may take many months involving multiple researchers or experiment techniques, and at each step, provenance information must be collected and stored to allow researchers and administrators to track and manage the experiments. the relevant provenance information includes, samples identifier, names and annotation information for the targeted genes, justification for knockout, plasmid constructs, antibiotic resistance genes, transfection methods , number of transfection attempts, selection antibiotic, period of selection, and the ultimate success of knocking-out the gene from the genome.

traditionally, bench science has used manual techniques or ad-hoc software tools to collect and store provenance information . this approach has several drawbacks, including the difficulty in ensuring adequate collection of provenance, creation of "silos" due to limited or no support for provenance interoperability across projects. further, the use of high-throughput data generation technologies, such as sequencing, microarrays, mass spectrometry , and nuclear magnetic resonance  are introducing additional challenges for the traditional approaches to provenance management. a new approach for provenance management is also required to support the increasing trend of publishing experiment results  to community data repositories .

in the next section, we describe the bkr project corresponding to the post-publication stage.

the biomedical knowledge repository project 
in contrast to the t.cruzi spse project, the biomedical knowledge repository  project at the u.s. national library of medicine is creating a comprehensive repository of integrated biomedical data from a variety of published data sources such as biomedical literature , structured data bases , and terminological knowledge sources )  <cit> . similar to many biomedical data repositories  <cit> , bkr uses w3c recommended resource description framework  format  <cit>  to represent the extracted and integrated information .

in addition to data, bkr project also includes provenance describing the source of an extracted rdf triple, temporal information , version of a data repository, and confidence value associated with the extracted information . for example, the provenance of the rdf statement "lipoprotein→affects→inflammatory_cells", the source article with pubmed identifier pmid:  <dig>  is also stored in the bkr project . the provenance information is used to support the services offered by bkr namely,  enhanced information retrieval service that allows search based on named relationship between two terms,  multi-document summarization,  question answering, and  knowledge discovery service.

the rdf reification vocabulary is often used to represent provenance information in semantic web applications. a variety of practical and theoretical issues have been identified in use of the rdf reification vocabulary  <cit> , including a disproportionate increase in total size of the rdf document without a corresponding increment in the information content of the rdf document. figure  <dig> illustrates this issue, where the reification of a single rdf triple leads to the creation of four extra rdf triples. the extra triples do not model any provenance-related information, but are merely artifacts of the rdf syntax. this adversely affects the scalability of large projects, such as bkr, which track the provenance of hundreds of millions of rdf triples.

challenges to provenance management in translational research
broadly, the challenges to provenance management, in both the pre and post-publication stages, can be divided into four categories:

 collecting provenance information in high throughput environments that is also adequate to support complex queries,

 representing the provenance information using a model that supports interoperability across projects, is expressive enough to capture the complexities of a specific domain , and allows use of reasoning software for automated provenance analysis over large datasets,

 efficiently storing and ensuring seamless propagation of provenance as the data is transferred across the translational research lifecycle,

 a dedicated query infrastructure that allows composition of provenance queries with minimal user effort, addresses the requirements specific to provenance queries , and a highly scalable implementation to support complex user queries over large volumes of data.

this paper extends our previous work  <cit>  that separately addressed some aspects of provenance management in the pre and post-publications phases. in this paper, our contributions go beyond the previous work and can be summarized as follows.

- we introduce a unified provenance management framework called semantic provenance framework based on the provenir upper-level provenance ontology for use in both the pre- and post-publication phases of translational research.

- we introduce a dedicated ontology-driven provenance collection infrastructure called ontology-based annotation tool  that makes it easier for biomedical researchers to create and maintain web forms for use with bench experiments.

- we illustrate the advantage of storing provenance metadata and data as a single rdf graph with significant impact on propagation of provenance.

- we present the architectural details of a provenance query engine that can be deployed over multiple rdf databases and supports a set of dedicated provenance query operators.

in the next section, we describe spf based on the notion of semantic provenance to address the provenance management challenges.

methods
in contrast to traditional database and workflow provenance, semantic provenance incorporates domain-specific terminology represented using a logic-based formal model, which facilitates domain scientists to intuitively query provenance and also automated processing of provenance metadata  <cit> . the semantic provenance framework  uses the provenir upper-level provenance ontology as the core formal model coupled with semantic web technologies, including rdf  <cit> , the web ontology language   <cit> , and the sparql query language  <cit>  for implementing provenance systems.

the approach used for provenance representation has a significant impact on the storage, propagation, and querying phases of the provenance life cycle. in  <cit> , we had introduced the provenir ontology as a reference model for provenance representation, which models a minimum set of provenance terms and relationships that are common across multiple translational research domains. the provenir ontology extends primitive philosophical ontology terms of "continuant" and "occurrent"  <cit>  along with ten fundamental relationships defined in the relation ontology  <cit> . the provenir ontology is composed of three top-level classes, namely data, process, and agent, which are fundamental to provenance modeling . the data class is further specialized into two classes namely, data_collection and parameter. the data_collection class represents entities that participate in an experiment or analysis process, while the parameter class, specialized into three classes along the spatial, temporal, and thematic  dimensions, models parameter values of a process. the provenir ontology classes are linked by ten relationships adapted from the relation ontology  <cit> , which allows provenir to capture and explicitly represent the semantics of the connections between terms that can be used by automated reasoning tools  <cit>  for consistent interpretation of provenance.

the provenir ontology is domain-upper ontology that can be extended, using the standard rdfs: subclassof and rdfs: subpropertyof <cit>  properties, for creating new domain-specific provenance ontologies. this approach of creating a suite of domain-specific ontologies by extending an upper-level ontology  facilitates provenance interoperability by ensuring consistent modeling and uniform use of terms  <cit>  and is a scalable solution. this approach is also consistent with existing ontology engineering practices based on the suggested upper merged ontology   <cit> , basic formal ontology   <cit> , and the descriptive ontology for linguistic and cognitive engineering   <cit> ). the provenir ontology is modeled using the description logic profile of owl   <cit> .

in the following sections, we discuss the use of the provenir ontology to implement the spf for managing the four stages of the provenance metadata in the two translational research exemplar projects.

provenance collection
the first phase of the provenance life cycle begins with the collection of provenance information as data is generated or modified in a project. the challenges in this phase include,  minimizing the disruption to existing research environment,  automating the collection procedure to scale with high-throughput data generation protocols while minimizing the workload for researchers, and  creating a flexible infrastructure that can be easily modified in response to changing user requirements. in the following sections, we describe the provenance collection infrastructure created for the t.cruzi spse and the bkr projects.

collecting provenance in the t.cruzi spse
we used a two-phase approach to implement the provenance collection infrastructure in the t.cruzi spse. in the first stage, existing data stored in a rdb store was converted to rdf using the parasite experiment ontology , which represents the domain-specific provenance information, as reference. the d2rq rdb to rdf tool  <cit>  was used to convert the existing data in rdb to rdf by defining mapping between the data value in a rdb table column to a concept in the ontology . this batch conversion of data from the relational data to rdf was a temporary solution, while we created an integrated infrastructure to collect and directly store provenance information in rdf. to implement this infrastructure, we defined a novel ontology-driven web form generation tool called ontology-based annotation tool . ontoant allows domain scientists to:

 <dig>  dynamically generate web forms for use in research projects to capture provenance information,

 <dig>  allow automatic conversion of the data captured in the web forms to rdf, and

 <dig>  use the built-in automatic validation of the web forms to ensure data quality and consistency with respect to the reference domain-specific provenance ontology 

ontoant has three components ,  a pattern manager,  a form manager, and  rdf manager along with an intuitive web interface to allow domain users to easily manage the provenance collection infrastructure. the pattern manager in turn has two components, namely  a pattern generator, which is a visual interface to assist users in defining a "provenance pattern" to capture the relevant provenance information and is composed of provenance ontology classes and properties. for example, to create the "gene knockout entry form" , the user selects gene_knockout_process, researcher, and priority classes from peo to compose the provenance pattern. the provenance pattern is used as reference by ontoant to generate rdf triples from the data captured in the web form . the pattern validator, which is the second component of the pattern manager, validates the consistency of the provenance pattern with respect to the provenance ontology schema using the pellet reasoning tool  <cit> .

once a valid provenance pattern is created, the form manager is invoked to automatically generate and deploy the web form. the form generator component of the form manager automatically generates the web form as a set of paired entities, namely, a field name and the corresponding text box . each field name in a web form corresponds to the provenance ontology class, while the value in the text box represents the instance of the ontology class . for example, the field "priority"  corresponds to the priority class in peo and the values in the drop-down menu  correspond to the instance values. the form processing engine component of the form manager allows users to modify the automatically generated form. the third component of the form manager is the form validator, which ensures that the data values entered in the web forms are consistent with the provenance ontology. for example, the form validator validates that for a user input value for a web form field is consistent with the ontology class definition or with the property range or domain constraints.

the rdf manager component of ontoant defines a set of application programming interfaces  that can be used by other ontoant components to access, construct queries, and generate as well as validate rdf triples. ontoant is currently being used in the t.cruzi spse project to deploy web forms .

provenance extraction in bkr
bkr collects the provenance information at two levels. at the first level the provenance information associated with an rdf triple is collected, such as the source of the triple , the date of the original publication, and the author list for the source article. at the second level, bkr records the provenance information associated with the extraction process, for example the confidence value associated with the extraction technique . the provenance collection process in bkr is integrated with the rdf generation process, which is described in the next section on provenance representation. provenance representation is a central issue in provenance management and has direct impact on the storage, querying, and analysis of provenance information in translational research.

provenance representation
earlier, we had described the provenir ontology that forms the core model of the spf. in this section, we demonstrate that though the requirements for provenance representation in the pre-publication phase differ from the post-publication phase, the provenir ontology can be extended to model provenance in both the t.cruzi spse  and bkr  projects.

parasite experiment ontology: modeling provenance in the t.cruzi spse project
in the pre-publication phase of translational research, the provenance information often describes the generation, curation, and processing of scientific data. in the t.cruzi spse project, the parasite experiment ontology  was created to model the experiment process used to generate data, the description of the raw material used, instruments, and parameter values that influence the generation or processing of data. in contrast to workflow provenance approaches that often model the "system-level" view of scientific processes  <cit> , peo incorporates domain-specific detail that allows us to comprehensively capture the context of an experiment and also allows researchers to use the domain-specific terminology to access and query the datasets. peo initially modeled the experiment protocols used in reverse genetics  as was reported in our previous work  <cit> . currently, peo has been extended to model microarray as well as mass spectrometry  based proteomics protocols also  and currently has  <dig> classes with  <dig> properties with a dl expressivity of alchq.

peo models the experiment protocols by specializing the provenir ontology classes and properties. in addition, peo re-uses classes and relationships from existing biomedical ontologies, including the sequence ontology  <cit> , the national cancer institute  thesaurus, gene ontology  <cit> , the w3c owl time ontology  <cit> , and the ontology for parasite lifecycle   <cit> . this facilitates interoperability of data modeled using peo with data that conform to other existing ontologies  <cit> . for example, we imported and seamlessly integrated functional gene annotations, using go terms, from the tritrypdb  <cit>  and kegg with existing internal experiment data for a specific list of genes found in t.cruzi and related parasites. hence, peo creates a unified schema for both the domain-specific provenance information and data that can be extended  to adapt to evolving needs of bench scientists in the t.cruzi spse project.

provenance context: representing provenance in the bkr project
in contrast to the t.cruzi spse project, the representation of provenance in the bkr project was more challenging. as we discussed earlier, the traditional rdf reification approach has many limitations that makes it difficult for translational research projects such as bkr to use it for provenance tracking. to address the limitations of the rdf reification approach, we defined a new approach based on the provenir ontology and context theory called provenance context entity   <cit> . the premise for the pace approach is that the provenance associated with rdf triples provides the contextual information necessary to correctly interpret rdf statements. a "provenance context" is defined for a specific application, such as bkr, by either using terms of the provenir ontology or terms defined in a domain-specific provenance ontology that extends the provenir ontology. in the bkr project , the provenance context consists of the unified medical language system  semantic network   <cit>  terms, entrez gene and pubmed identifiers. the terms of the bkr provenance context are defined as subclass of provenir: data class using the rdfs: subclassof property.

once a provenance context has been defined for an application, the contextualized rdf triples can be generated from the information extracted from the original data sources. the pace approach allows an application to decide the appropriate level of granularity with three possible implementation approaches. the first implementation  is an exhaustive approach and explicitly links the s, p, and o to the source journal article. the second implementation is a minimalist approach that links only the s of a rdf triple to the source article. the third implementation takes an intermediate approach that creates two additional provenance-specific triples but requires the application to assume that the source of the o is the same as the s, and p. it is important to note that none of the three variants of the pace approach requires the use of rdf reification vocabulary or the use of blank nodes.

a practical challenge for implementing the pace approach in the bkr is to formulate an appropriate provenance context-based uniform resource identifier  scheme that also conforms to best practices of creating uris for the semantic web, including support for use of http protocol  <cit> . the design principle of urip is to incorporate a "provenance context string" as the identifying reference of an entity and is a variation of the "reference by description" approach that uses a set of description to identify an entity  <cit> . the syntax for urip consists of the <base uri>, the <provenance context string>, and the <entity name>. for example, the urip for the entity lipoprotein is http://mor.nlm.nih.gov/bkr/pubmed_17209178/lipoprotein where the pubmed_ <dig> provenance context string identifies the source of a specific instance of lipoprotein.

this approach to create uris for rdf entities also enables bkr  to group together entities with the same provenance context. for example,


http://mor.nlm.nih.gov/bkr/pubmed_17209178/lipoprotein



http://mor.nlm.nih.gov/bkr/pubmed_17209178/affects



http://mor.nlm.nih.gov/bkr/pubmed_17209178/inflammatory_cells


are entities extracted from the same journal article. the multiple contextualized uris representing a common type of entity, for example "lipoprotein", can be asserted to be instances of a common ontology class by using the rdf: type property. in the next section, we address the issues in provenance storage and propagation stage of the provenance lifecycle.

provenance storage and propagation
the current high-throughput data generation techniques, including gene sequencing and dna microarray, have created very large datasets in biomedical applications  <cit> . though the capture and storage of provenance associated with the above datasets leads to an exponential increase in the total size of the datasets  <cit> , provenance plays an important role in optimizing the access and query of the datasets  <cit> . there are two approaches to store provenance, namely  provenance is stored together with the dataset, and  provenance is stored separately from the data .

the spf uses the first approach by storing both the data and provenance together in a single rdf graph. the primary motivation for selecting the first approach is to allow applications to flexibly categorize an information entity as either data or provenance metadata according to evolving user requirements. for example, the temperature of a gene knockout experiment  is provenance information, which can be used to query for results generated using similar temperature conditions. in contrast, the body temperature of a patient in clinical research scenario is a data value and not provenance information. hence, this application-driven distinction between provenance metadata and data is a critical motivation for storing provenance and data together in the spf. in addition, storing provenance together with the data makes it easier for application to also ensure that updates to data are seamlessly applied to the associated provenance. ensuring synchronization between the data and separately stored provenance is challenging especially in a high-throughput data generation scenarios and the provenance information may become inconsistent with the data.

an essential requirement for provenance storage is ensuring the propagation of provenance as the data traverses the translational research life cycle, for example the provenance of gene expression profiling experiment results is used in a downstream application such as biological pathway research. the integrated approach for provenance storage allows seamless propagation of provenance information with the data. in contrast, it is often difficult to transfer provenance separately from the data across projects, institutions, or applications. further, many applications often query the provenance metadata to identify relevant datasets to be imported and analyzed further, for example identifying a relevant patient cohort for clinical research requires identifying qualifying health care providers, the geographical location of the patients, and related provenance information. hence, if the provenance associated with a patient health record is stored separately and cannot be easily propagated and accessed by the clinical researcher then it adversely affects translational research projects.

though storing provenance and data together has many advantages, one of the challenges that needs to be addressed is the large size of the resulting datasets. cloud-based storage solutions, such as simple storage service  from amazon and azure blob from microsoft have been proposed to effectively address these issues  <cit> .

provenance storage in the t.cruzi spse
the t.cruzi spse project currently stores more than  <dig>  rdf triples corresponding to the data and the associated provenance information for four experiment protocols, namely proteome, microarray, gene knockout, and strain creation . the experiment data and the associated provenance information are stored in a single rdf graph. this allows easy propagation of provenance along with the original experiment data. the rdf triples are stored in an oracle10g  rdf datastore. table  <dig> illustrates that a very large percentage of the total data, between 87%  to 98% , is provenance information. the use of provenance information to query and access specific datasets is discussed later in the provenance query and analysis section.

provenance storage in the bkr project
in the bkr project, the initial data  consisted of  <dig>  million rdf triples. the initial data was augmented with the provenance information and stored using the three pace approaches discussed in the previous section , namely:

a) exhaustive approach : capturing the provenance of the s, p, and o elements of the rdf triple increased the total size of the bkr dataset to  <dig>  million rdf triples

b) minimal approach :  <dig>  million additional rdf triples  were created using this approach

c) intermediate approach : a total of  <dig>  million rdf triples were created using the i_pace approach

provenance query and analysis
the provenance literature discusses a variety of queries that are often executed using generic or project-specific query mechanisms that are difficult to re-use. provenance queries in workflow systems focus on execution of computational process and their input/output values. provenance queries in relational databases trace the history of a tuple or data entity  <cit> . in contrast, scientists formulate provenance queries using domain-specific terminology and follow the course of an experiment protocol  <cit> . in addition, provenance queries over scientific data often exhibit "high expression complexity"  <cit>  reflecting the real world complexity of the scientific domain  <cit> .

the composition of provenance queries using sql or sparql query languages is not intuitive for translational research scientists. hence, we have defined specialized "query operators" for use by domain scientists, which use the specified input value  to automatically compose and execute complex provenance queries:

provenance
 <dig>  provenance
 <dig>  provenance
 <dig>  provenance
a) provenance  query operator - to retrieve provenance information for a given dataset,

b) provenance_context  query operator - to retrieve datasets that satisfy constraints on provenance information,

c) provenance_compare  query operator - given two datasets, this query operator determines if they were generated under equivalent conditions by comparing the associated provenance information, and

d) provenance_merge  query operator - to merge provenance information from different stages of an experiment protocol. in the t.cruzi spse project, provenance information from two consecutive phases, namely gene knockout and strain creation phases, can be merged using this query operator.

the query operators are defined in terms of a "search pattern template" composed of provenir ontology classes and properties . the query operators use the standard rdfs entailment rules  <cit>  to expand the query pattern and can be executed against the instance base of any  domain-specific provenance ontology. the formal definition of these query operators is described in  <cit> . in addition, the query operators can be extended to create new query operators and can be implemented in either sql or sparql.

RESULTS
the spf was implemented as a scalable provenance query engine that can be deployed over any rdf database that supports standard rdfs entailment rules  <cit> .

provenance query engine
the provenance query engine consists of three functional components :

 <dig>  a query composer
the query composer maps the provenance query operators to sparql syntax according to semantics of the query operators.

 <dig>  a function to compute transitive closure over rdf
sparql query language does not support transitive closure for an rdf <node, edge> combination. hence, we have implemented a function to efficiently compute transitive closure using the sparql ask function. the output of this function together with the output of the query composer is used to compose the complete query pattern.

 <dig>  query optimizer using materialized provenance views
using a new class of materialized views based on the provenir ontology schema called materialized provenance views  a query optimizer has been implemented that enables the query engine to scale with very large rdf data sets.

the query operators are implemented taking into account the distinct characteristics of provenance queries as well as existing provenance systems. for example, provenance information represents the complete history of an entity and is defined by the exhaustive set of dependencies among data, process, and agent. however, in real world scenarios the provenance information available can be incomplete due to application-specific or cost-based limitations. hence, a straightforward mapping of provenance query operators to sparql as a basic graph pattern  is not desirable, since the bgp-based query expression pattern may not return a result in the presence of incomplete provenance information  <cit> . hence, the optional function in sparql can be used to specify query expression patterns that can succeed with partial instantiation, yielding maximal "best match" result graph. another challenge in implementation of the query engine was that unlike many graph database query languages such as lorel or graphlog,  <cit> , sparql does not provide an explicit function for transitive closure to answer reachability queries . reachability queries involving computation of transitive closure is an important characteristic of provenance queries to retrieve the history of an entity beginning with its creation. in case of the provenance query engine, the query composer computes the transitive closure over the <process, preceded_by> combination to retrieve all individuals of the process class linked to the input value by the preceded_by property.

transitive closure module
we had two options in implementing the transitive closure function, namely a function that is tightly coupled to a specific rdf database or a generic function. we chose a generic implementation using the sparql ask function that allows the provenance query engine to be used over multiple rdf stores. the sparql ask function allows "application to test whether or not a query pattern has a solution,"  <cit>  without returning a result set or graph. the transitive closure function starts with the process instance  linked to the input value and then recursively expands the sparql query expression using the ask function till a false value is returned, thereby terminating the function . the sparql ask function, in contrast to the select and construct functions, does not bind the results of the query to variables in the query pattern. hence, it is a low-overhead function for computing transitive closure  <cit> .

the evaluation of the provenance query engine followed the standard approach in database systems  <cit>  and was performed for both "expression complexity" - sparql query patterns with varying levels of complexity, and "data complexity" - varying sizes of rdf datasets. the sparql query complexity was measured using the total number of variables, triples, use of optional function, and levels of nesting in the query pattern  <cit> . the most complex query pattern had  <dig> variables,  <dig> triples, and  <dig> levels of nesting using the optional function. further, to evaluate the data complexity, five different sized datasets were used ranging from  <dig>  rdf triples to  <dig> million rdf triples. we found that a straightforward implementation of the query engine was not able to scale with both increasing expression and data complexity  <cit> . hence, the provenance query engine uses a novel materialization strategy based on the provenir ontology schema, called materialized provenance views   <cit> . the use of mpv improved the performance of the query engine by an average of  <dig> % for increasingly complex sparql query patterns and by an average of  <dig> % for increasingly large rdf datasets, thereby validating the scalability of the query engine. we now describe a few example provenance queries in context of the t.cruzi spse project that leverage the spf query infrastructure.

provenance queries in the t.cruzi spse
in the t.cruzi spse project, provenance queries broadly address two types of issues:

 <dig>  retrieving the history of experiment results to ensure quality and reproducibility of data. in addition, the provenance information is used to describe the experiment conditions of results published in literature

 <dig>  keeping track of experiment resources during an ongoing project or auditing of resources used in a completed project. this helps project managers to monitor status of projects and ensure optimal use of lab resources

we consider the following two example provenance queries representing the above two categories of usage:

query 1: find the drug and its concentration that was used during drug selection process to create "cloned_sample <dig> "

query 2: what is the status of knockout plasmid construction step to create ptrex? query  <dig> illustrates the retrieval of provenance information associated with a cloned sample, where the type of drug and concentration of the drug are important for researchers to understand the characteristics of the cloned sample. similarly, query  <dig> describes a provenance query used for project management, where the lead researcher or project manager can keep track of the project status. both these example queries are answered using the provenance () query operator, which takes as input "cloned_sample66" and "ptrex" as input values respectively.

as described earlier, the provenance () query operator, implemented in the provenance query engine, automatically generates a sparql query pattern using the peo schema as reference. this query pattern is executed against the t.cruzi spse rdf instance base and the retrieved results are represented as a rdf graph . similar to our earlier work  <cit> , the results of the above queries were manually validated by domain researchers in the tarleton research group. in the next section, we describe provenance queries used in the bkr project.

provenance query in the bkr project
the provenance queries in the bkr project are used for identifying the source of an extracted rdf triple, retrieving temporal information , version information for a database, and the confidence value associated with a triple . the provenance information is essential in the bkr project to ensure the quality of data and associate trust value with the rdf triple. we discuss the following two example provenance queries used in the bkr project:

query 1: find all documents asserting the triple "il- <dig> → inhibits → cox-2"

query 2: find all triples of the form "il- <dig> → inhibits → gene" where value of gene is not known apriori. the results are filtered based on a set of provenance constraints such that results are only from  journals with impact factor >  <dig>   journal published after the year  <dig>   rdf triples with confidence value >  <dig> 

query  <dig> is used by the enhanced information retrieval service in the bkr project, which supports user query based on not only keyword or concepts, but also relations  <cit> . hence, results from query  <dig> are used to create a basic index, similar to traditional search engines, listing all documents from which a given biomedical assertion is extracted  <cit> . in contrast to query  <dig>  query  <dig> is used by the question answering service of the bkr project to define provenance-based quality constraints to retrieve results from reputable journals that have been published recently and a high confidence value is associated with the extracted rdf triple. both the provenance queries are expressed in sparql and executed against the bkr instance base. in our earlier work, we have discussed the improved performance of provenance queries using the pace approach in comparison to the rdf reification vocabulary  <cit> .

in both the t.cruzi spse and bkr project, the spf provides users with an easy to use, expressive, and scalable provenance query infrastructure that can scale with increasing size of data and complexity of the queries  <cit> .

discussions
we first discuss related work in provenance representation in context of the provenir ontology. next, we discuss the work in database provenance and workflow provenance with respect to provenance query/analysis and compare it with the functionality of the provenance query operators defined in spf.

provenance representation
multiple provenance representation models have been proposed, with the open provenance model   <cit>  and the proof markup language   <cit>  being the two prominent projects. as part of the w3c provenance incubator group, we have defined a lightweight mapping between the opm and other provenance models including the provenir ontology, which demonstrates that all three of them model similar classes, but only the provenir ontology has a comprehensive set of named relationships linking the provenance classes  <cit> . specifically, opm  models only "causal relations" linking provenance entities  <cit> , which makes it difficult for opm to model partonomy, containment, and non-causal participatory provenance properties needed in many translational research applications. provenance representations, in the context of relational databases, extend the relational data model with annotations  <cit> , provenance and uncertainty  <cit> , and semirings of polynomials  <cit> . provenir ontology can be extended to model the provenance of tuple in relational databases, which relies on mappings defined between description logic to relational algebra  <cit> .

database provenance
database provenance or data provenance, often termed as "fine-grained" provenance, has been extensively studied in the database community. early work includes the use of annotations to associate "data source" and "intermediate source" with data  in a federated database environment to resolve conflicts  <cit> , and use of "attribution" for data extracted from web pages  <cit> . more recent work has defined database provenance in terms of "why provenance," "where provenance,"  <cit>  and "how provenance"  <cit> . "why provenance", introduced in  <cit> , describes the reasons for the presence of a value in the result  and "where provenance" describes the source location of a value  <cit> . a restricted view of the "where provenance" identifies each piece of input data that contributes to a given element of the result set returned by each database query. we use the syntactic definition of "why provenance"  <cit>  that defines a "proof" for a data entity. the proof consists of a query, representing a set of constraints, over a data source with "witness" values that result in a particular data output. the semantics of the provenance () query operator closely relates to both "where provenance" and "why provenance"  <cit> .

to address the limitation of "why provenance" that includes "...set of all contributing input tuples" leading to ambiguous provenance,  <cit>  introduced semiring-based "how provenance." the provenance () query operator over a "weighted" provenance model, which reflects the individual contribution of each component , is comparable to "how provenance."

the trio project  <cit>  considers three aspects of lineage information of a given tuple, namely, how was a tuple in the database derived along with a time value  and the data sources used. a subset of queries in trio, "lineage queries", discussed in  <cit> , can be mapped both as provenance () and as provenance_context () query operators depending on the input value.

workflow provenance
the rapid adoption of scientific workflows to automate scientific processes has catalyzed a large body of work in recording provenance information for the generated results. simmhan et al.  <cit>  survey different approaches for collection, representation, and management of workflow provenance. recent work has also recognized the need for inclusion of domain semantics in the form of domain-specific provenance metadata  <cit>  along with workflow provenance  <cit> . the semantics of these projects can be mapped to the provenance () query operator.

discussions
in our previous work  <cit> , we have separately addressed some of the issues in pre- and post-publications phases of translational research applications. here we expand on the challenges in creating a unified framework for provenance management, with a focus on a dedicated infrastructure for effective provenance collection, a flexible provenance model, and a scalable query implementation that can be adopted across translational research projects.

what does it take to build an effective provenance management system for translational research today? it is clear from the work discussed in this paper that creation of a practical and usable provenance management system is not a trivial task. though provenance represents critical information for research projects, the high threshold in terms of resources required deters widespread adoption of a systematic and comprehensive provenance infrastructure. in addition, the lack of provenance-specific standards makes it difficult for developers to implement interoperable provenance systems across projects, applications, and different phases of the translational research lifecycle. this current state of provenance systems forces researchers to create ad-hoc systems that cannot be re-used, extended, or adapted to changing project requirements.

hence, we have deliberately aligned the implementation of the spf components with existing w3c semantic web standards, including rdf, owl, and sparql. though, these standards are not tailored for the specific requirements of provenance systems, we demonstrated that they can be extended and adopted to address some of the challenges. for example, a component of the provenance query engine uses sparql ask function to compute transitive closure over rdf graphs, since sparql does not have explicit support for computing transitive closure. despite some advantages of using existing semantic web standards, provenance management in context of translational research is still in an early phase.

how are things likely to improve in the future? the w3c provenance incubator group   <cit>  has collected an extensive set of use cases and requirements for effective provenance management. this work has led to the creation of the w3c provenance working group, which has been mandated to define a language for exchanging provenance information across applications  <cit> . in addition, the working group will also define a mechanism for querying and accessing the provenance information along with a set of best practices that can be used to guide implementation of provenance systems  <cit> . we are members of the working group and we plan to make the spf compatible with the standards that will be proposed by the working group.

CONCLUSIONS
we described a unified framework based on the upper-level provenir provenance ontology for managing provenance information during generation of data from bench experiments and their subsequent use  by data mining and knowledge discovery applications. in the process, we identified that both the pre and post-publication phases of translational research have a common set of stages associated with the provenance metadata that can be managed by the spf. using two exemplar projects, corresponding to the two translational research phases, we described how the spf could handle provenance collection, representation, storage/propagation, and query/analysis.

as part of our future work, we will implement a "lifting mechanism" between contexts to allow easier transformation of rdf triple between different pace-based applications. in addition, we aim to specialize the existing provenance query operators to interface with distributed sparql end-points, which have been proposed for provenance access and querying by the w3c provenance working group.

authors' contributions
sss defined the framework, created the ontologies, implemented query operators, query engine, and wrote this manuscript. vn designed and implemented the ontoant tool and contributed to writing of the manuscript. ob implemented pace, executed the queries, and validated the results. pp and tm defined the t.cruzi spse queries, contributed to peo, and validated the results. aps contributed to creation of the framework and development of the ontologies. all authors have read and approved the final manuscript.

