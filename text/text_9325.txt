BACKGROUND
named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. previous studies have shown that automatic named entity recognition can be performed with a reasonable level of accuracy by using various machine learning models such as support vector machines  or conditional random fields   <cit> .

however, the lack of annotated corpora, which are indispensable for training machine learning models, makes it difficult to broaden the scope of text mining applications. in the biomedical domain, for example, several annotated corpora such as genia  <cit> , pennbioie  <cit> , and genetag  <cit>  have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to carry out.

active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus  <cit> . in active learning, samples that need to be annotated by the human annotator are picked up from a big pool of samples by a machine learning model in an iterative and interactive manner, considering the informativeness of the samples. it has been shown that, compared to random sampling, active learning can often drastically reduce the amount of training data necessary to achieve the same level of performance. the effectiveness of active learning has been demonstrated in several natural language processing tasks including named entity recognition.

the problem with active learning, however, is that the resulting annotated data is dependent on the machine learning algorithm and the sampling strategy employed, because active learning annotates only a subset of the samples in the given corpus. this sampling bias is not a serious problem if one is to use the annotated corpus only for their own machine learning purpose and with the same machine learning algorithm. however, the existence of bias is not desirable if one wants the corpus to be used by other applications or researchers. for the same reason, active learning approaches cannot be used to enrich an existing linguistic corpus with a new named entity category.

in this paper, we present a framework that enables one to make named entity annotations for a given corpus with a reduced cost. unlike active learning approaches, our framework aims to annotate all named entities of the target category contained in the corpus. obviously, if we were to ensure 100% coverage of annotation, there is no way of reducing the annotation cost, i.e. the human annotator has to go through every sentence in the corpus. however, we show in this paper that it is possible to reduce the cost by slightly relaxing the requirement for the coverage, and the reduction can be drastic when the target named entities are sparse.

we should note here that the purpose of this paper is not to claim that our approach is superior to existing active learning approaches. the goals are different – while active learning aims at optimizing the performance of the resulting machine learning-based tagger, our framework aims to help develop an unbiased named entity-annotated corpus.

methods
annotating named entities by dynamic sentence selection
this overall flow of annotation framework is very similar to that of active learning. in fact, the only differences are the criterion of sentence selection and the fact that our framework uses the estimated coverage as the stopping condition. in active learning, sentences are selected according to their informativeness to the machine learning algorithm. our approach, in contrast, selects sentences that are most likely to contain named entities of the target category. the next section elaborates on how to select such sentences using the output of the crf-based tagger.

the other key in this annotation framework is when to stop the annotation work. if we repeat the process until all sentences are annotated, then obviously there is no merit of using this approach. we show in the next section that we can quite accurately estimate how many of the entities in the corpus are already annotated and use this estimated coverage as the stopping condition.

selecting sentences using the crf tagger
our annotation framework takes advantage of the ability of crfs to output multiple probabilistic hypotheses. this section describes how we obtain named entity candidates and their probabilities from crfs in order to compute the expected number of named entities contained in a sentence.

we should note that one could use other machine learning algorithms for this task as long as they can produce probabilistic output. for example, maximum entropy markov models are a possible alternative. we have chosen the crf model because it is currently one of the best models for named entity recognition and there are efficient algorithms to compute marginal probabilities and n-best sequences in crfs.

the crf tagger
crfs  <cit>  can be used for named entity recognition by representing the spans of named entities using the "bio" tagging scheme, in which 'b' represents the beginning of a named entity, 'i' the inside, and 'o' the outside . this representation converts the task of named entity recognition into a sequence tagging task.

a linear chain crf defines a single log-linear probabilistic distribution over the possible tag sequences y for a sentence x:

 p=1zexp⁡∑t=1t∑k=1kλkfk, 

where fk is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and z is a normalization function:

 z=∑yexp⁡∑t=1t∑k=1kλkfk. 

this modeling allows us to define features on states  and edges  combined with observations  tags).

the weights of the features are determined in such a way that they maximize the conditional log-likelihood of the training data: ℒ=∑i=1nlog⁡pθ|x). in the actual implementation, we also used the l2-norm penalty term to avoid overfitting of the model to the training data. we used the l-bfgs algorithm  <cit>  to compute the parameters.

computing the expected number of named entities
to select sentences that are most likely to contain named entities of the target category, we need to obtain the expected number of named entities contained in each sentence. crfs are well-suited for this task as the output is fully probabilistic – one can easily obtain probabilistic information on possible tag sequences using established algorithms .

suppose, for example, that the sentence is "transcription factor gata- <dig> and the estrogen receptor". table  <dig> shows an example of the 5-best sequences output by the crf tagger. the sequences are represented by the aforementioned "bio" representation. for example, the first sequence indicates that there is one named entity 'transcription factor' in the sequence. by summing up these probabilistic sequences, we can compute the probabilities for possible named entities in a sentence. from the five sequences in table  <dig>  we obtain the following three named entities and their corresponding probabilities.

'transcription factor' 

'estrogen receptor' 

'transcription factor gata-1' 

the expected number of named entities in this sentence can then be calculated as  <dig>  +  <dig>  +  <dig>  =  <dig> .

in this example, we used 5-best sequences as an approximation to all possible sequences needed to compute the exact expected number of entities. one possible way to achieve a good approximation is to use a large n for n-best sequences, but there is a simpler and more efficient way, which directly produces the exact expected number of entities. recall that named entities are represented with the "bio" tags. since one entity always contains one "b" tag, we can compute the number of expected entities by simply summing up the marginal probabilities for the "b" tags on all tokens in the sentence. the marginal probabilities on each token can be computed by the forward-backward algorithm. this is normally more efficient than computing n-best sequences for a large n. for efficient implementation of the forward-backward algorithm, see  <cit> .

once we compute the expected number of entities for every unannotated sentence in the corpus, we sort the sentences in descending order of the expected number of entities and choose the top n sentences to be presented to the human annotator.

coverage estimation
to ensure the quality of the resulting annotated corpus, it is crucial to be able to know the current coverage of annotation at each iteration in the annotation process. to compute the coverage, however, one needs to know the total number of target named entities in the corpus. the problem is that it is not known until all sentences are annotated.

in this paper, we solve this dilemma by using an estimated value for the total number of entities. then, the estimated coverage can be computed as follows:

  =mm+∑i∈uei 

where m is the number of entities actually annotated so far and ei is the expected number of entities in sentence i, and u is the set of unannotated sentences in the corpus. at any iteration, m is always known and ei is obtained from the output of the crf tagger as explained in the previous section.

RESULTS
we carried out experiments to see how our method can improve the efficiency of annotation process for sparse named entities. we evaluate our method by simulating the annotation process using existing named entity corpora. in other words, we use the gold-standard annotations in the corpus as the annotations that would be made by the human annotator during the annotation process.

corpus
we used two named entity corpora for the experiments. one is the training data provided for the conll- <dig> shared task  <cit> , which consists of  <dig>  sentences and includes four named entity categories  for the general domain. the other is the training data provided for the nlpba shared task  <cit> , which consists of  <dig>  sentences and five named entity categories  for the biomedical domain. this corpus is created from the genia corpus  <cit>  by merging the original fine-grained named entity categories.

in the experiments reported in the following sections, we do not use the "protein" category because there is little merit of using our framework when most sentences are relevant to the target category.

accelerated annotation
we carried out eight sets of experiments, each of which corresponds to one of those named entity categories shown in table  <dig> . the number of sentences selected in each iteration  was set to  <dig> throughout all experiments.

figures  <dig> to  <dig> show the results obtained on the conll corpus. the figures show how the coverage increases as the annotation process proceeds. the x-axis shows the number of annotated sentences.

each figure contains three lines. the normal line represents the coverage actually achieved, which is computed as follows:

  =entities_annotatedtotal_number_of_entities. 

the dashed line represents the coverage estimated by using equation  <dig>  for the purpose of comparison, the dotted line shows the coverage achieved by the baseline annotation strategy in which sentences are selected sequentially from the beginning to the end in the corpus.

the figures clearly show that our method can drastically accelerate the annotation process in comparison to the baseline annotation strategy. the improvement is most evident in figure  <dig>  in which named entities of the category "misc" are annotated.

we should also note that coverage estimation was surprisingly accurate. in all experiments, the difference between the estimated coverage and the real coverage was very small. this means that we can safely use the estimated coverage as the stopping condition for the annotation work.

figures  <dig> to  <dig> show the experimental results on the genia data. the figures show the same characteristics observed in the conll data. the acceleration by our framework was most evident for the "rna" category. table  <dig> shows how much we can save the annotation cost if we stop the annotation process when the estimated coverage reaches 99%. the first column shows the coverage actually achieved, and the second and third columns show the number and percentage of the sentences annotated in the corpus. this result shows that, on average, we can achieve a coverage of  <dig> % by annotating  <dig> % of the sentences in the corpus. in other words, we could roughly halve the annotation cost by accepting the missing rate of  <dig> %.

as expected, the cost reduction was most drastic when "rna", which is the most sparse named entity category , was targeted. the cost reduction was more than seven-fold. these experimental results confirm that our annotation framework is particularly useful when applied to sparse named entities.

one of the potential problems with this kind of active learning-like framework is the computation time required to retrain the tagger at each iteration. since the human annotator has to wait while the tagger is being retrained, the computation time required for retraining the tagger should not be very long. table  <dig> shows the time elapsed in the experiments. we used amd opteron  <dig>  ghz servers for the experiments and our crf tagger is implemented in c++. in our experiments, the worst case  required  <dig> seconds for retraining the tagger at the last iteration, but in most cases the training time for each iteration was kept under several minutes. we used the bfgs algorithm for training the crf model in this work, but it is probably possible to further reduce the training time by using more recent parameter estimation algorithms such as exponentiated gradient algorithms  <cit> .

suggesting annotation candidates
in the previous section, we discussed how much we can reduce the number of sentences that need to be examined by the annotator, but we did not discuss the annotation cost for individual sentences presented to the annotator. here we present some experimental results to show that the annotation cost for each sentence could be reduced if we take advantage of the n-best sequences output from the crf tagger by using the idea presented in  <cit> .

the idea is to present the n-best sequences from the crf tagger to the annotator as the annotation candidates, so that the annotator does not have to make the annotation for the sentence from scratch. in other words, all he has to do is selecting the correct annotation for the sentence from the list of likely annotations suggested by the system.

the effectiveness of this approach is almost exclusively dependent on the quality of the annotation candidates generated by the crf tagger. to investigate their quality, we carried out additional experiments using the genia corpus and the rna category.

in this experiment, we assumed that the crf tagger generates  <dig> annotation candidates for each sentence. the fifth column in table  <dig> shows the percentage of the cases where the correct annotation for the sentence was actually included in the candidates. the sixth column shows the average rank of the correct annotation in the candidates. the experimental results were promising – the correct annotation was included in the top  <dig> candidates in most cases and they are usually highly ranked in the list.

enriching an existing corpus
in the experiments presented in the previous sections, we assumed that the annotation work for each named entity category is carried out independently from other categories. however, there are cases where we can take advantage of the information about named entities of other categories.

let us suppose a situation where we want to enrich an existing named entity corpus with a new named entity category. if named entities are not allowed to overlap, we can rule out the text regions that are already covered by the existing named entity categories when computing the expected numbers of target named entities, which should improve the accuracy of estimated coverage and lead to improved efficiency of annotation work.

we carried out another set of experiments to simulate this kind of situation. we used the same eight categories that were used in the previous experiments, but we assumed the existence of the named entities of the other categories. for example, when we ran the experiments for the loc category in the conll corpus, we assumed that the corpus was already annotated with the other three categories  and named entities were not allowed to overlap.

the results are shown in table  <dig>  as expected, the numbers of sentences that needed to be examined were much smaller than those in the previous experiments shown in table  <dig> 

discussion and related work
our annotation framework is, by definition, not something that can ensure a coverage of 100%. the seriousness of a missing rate of, for example, 1% is not entirely clear – it depends on the application and the purpose of annotation. in general, however, it is hard to achieve a coverage of 100% in real annotation work even if the human annotator scans through all sentences, because there is often ambiguity in deciding whether a particular named entity should be annotated or not. previous studies report that inter-annotator agreement rates with regards to gene/protein name annotation are f-scores around 90%  <cit> . we believe that the missing rate of 1% can be an acceptable level of sacrifice, given the cost reduction achieved and the unavoidable discrepancy made by the human annotator.

at the same time, we should also note that our framework could be used in conjunction with existing methods for semi-supervised learning to improve the performance of the crf tagger, which in turn will improve the coverage. it is also possible to improve the performance of the tagger by using external dictionaries or using more sophisticated probabilistic models such as semi-markov crfs  <cit> . these enhancements should further improve the coverage, keeping the same degree of cost reduction.

the idea of improving the efficiency of annotation work by using automatic taggers is certainly not new. tanabe et al.  <cit>  applied a gene/protein name tagger to the target sentences and modified the results manually. culotta and mccallum  <cit>  proposed to have the human annotator select the correct annotation from multiple choices produced by a crf tagger for each sentence. tomanek et al.  <cit>  discuss the reusability of named entity-annotated corpora created by an active learning approach and show that it is possible to build a corpus that is useful to different machine learning algorithms to a certain degree.

the limitation of our framework is that it is useful only when the target named entities are sparse because the upper bound of cost saving is limited by the proportion of the relevant sentences in the corpus. our framework may therefore not be suitable for a situation where one wants to make annotations for named entities of many categories simultaneously . in contrast, our framework should be useful in a situation where one needs to modify or enrich named entity annotations in an existing corpus, because the target named entities are almost always sparse in such cases. we should also note that named entities in full papers, which recently started to attract much attention, tend to be more sparse than those in abstracts.

CONCLUSIONS
we have presented a simple but powerful framework for reducing the human effort for making name entity annotations in a corpus. the proposed framework allows us to annotate almost all named entities of the target category in the given corpus without having to scan through all the sentences. the framework also allows us to know when to stop the annotation process by consulting the estimated coverage of annotation.

experimental results demonstrated that the framework can reduce the number of sentences to be annotated almost by half, achieving a coverage of  <dig> %. our framework was particularly effective when the target named entities were very sparse.

unlike active learning, this work enables us to create a named entity corpus that is free from the sampling bias introduced by the active learning strategy. this work will therefore be especially useful when one needs to enrich an existing linguistic corpus  with named entity annotations for a new semantic category.

competing interests
the authors declare that they have no competing interests.

authors' contributions
yt developed the algorithm, carried out the experiments and drafted the manuscript. jt and sa conceived the study and participated in its design and coordination. all authors read and approved the final manuscript.

