BACKGROUND
high-throughput screening studies in biology and other fields are increasingly popular due to ease of sample tracking and decreasing technology costs. these experimental setups enable researchers to obtain numerous measurements across multiple individuals in parallel  or in series . the large number of measurements collected often comes at the cost of measurement precision or the overall power of detection. for many large-scale studies, the experimental design aims to maximize the number of compounds or individuals tested, resulting in limited replication and few to no controls. in the case of microarray studies, several methods for normalizing arrays have been developed  <cit>  with no universal method adopted as the standard. quantitative pcr faces the same issues as it is used more frequently in high throughput platforms, with analysis methodologies being developed paralleling those for expression arrays  <cit> .

metabolite profiling is a rapidly expanding area of high throughput measurements, where samples having large amounts of biological variability and diverse physical properties makes quantification of large numbers of structurally diverse metabolites challenging  <cit> . few strategies exist for normalization in metabolite analysis to control for run-to-run variance other than to include negative and positive controls. for large-scale screens involving mutagenized populations  or crosses , the goal is to identify putative hits, or individuals that are likely to be different from the bulk of the samples for subsequent follow-up . in these conditions, properties of the sample cohort serve as controls with the measure of differences between an individual and its cohort used to identify samples differentially accumulating a metabolite  <cit> . this strategy can streamline sample processing and maximize throughput when the expected effects are large and easily observable.

for studies where comparisons are sought across an experiment conducted over the course of several months or in different sample batches, normalizing factors are necessary, especially given typically high levels of biological and technical variability  <cit> . ideal experiments include technical and biological replication within each set as well as controls facilitating comparisons between sample batches, but these are often limited or omitted entirely due to likely increases in experimental costs or the negative impacts on throughput. however, absence of these experimental controls limits the ability to handle variability between sample groups  making it a greater challenge to identify individuals within the range between normal and aberrant phenotypes. without the ability to normalize the data provided by experimental controls, some of the benefits of high throughput screens are lost, yet the desire to maximize throughput places constraints on the experimental design.

the motivation for algorithm development came from the arabidopsis thaliana chloroplast  <dig> project large-scale reverse genetic phenotypic screen . this project leverages the collection of t-dna insertion lines and genomic sequence for the plant model species a. thaliana to screen large numbers of putative gene knockouts with the aim of functionally characterizing chloroplast-targeted genes. the presence of a large t-dna insertion can block or reduce expression of the gene it lands in, and altered phenotypes can provide insights into the normal function of the gene and its protein or rna product.

in addition to qualitative and semi-quantitative measures of physiological and morphological characteristics, the pipeline assayed levels of leaf fatty acids and leaf and seed free amino acids, important outputs of chloroplast metabolism. the pipeline assays were preformed on groups of individual plants planted in units of up to thirty-two per tray and three trays of plants per assay group. two assay groups were grown concurrently under controlled environment plant growth conditions. individuals representing t-dna insertion events in different locations within the same gene  are present in the dataset, and it is of interest to compare the assay responses of these individuals as well as to identify other individuals with similar responses. because the experimental design lacked cross-group controls , the ability to make even semi-quantitative cross-dataset comparisons was not possible using existing methodology.

developing phenotypic annotation for un- and under- annotated genes is a primary goal for the chloroplast  <dig> project and identification of individuals with like phenotypes  is a way to achieve that goal. thus, a method that would allow cross-dataset comparisons and identify putative mutants was needed to achieve the goal. the resulting method, mipheno , is aimed at improving first-pass screening capabilities for large datasets in the absence of defined controls. algorithm performance was tested using a synthetic data set and the chloroplast  <dig> high throughput phenotypic dataset. the executable code and data for the chloroplast  <dig> analysis are available as additional file  <dig> and as a cran package .

the following describes a quality control process for identifying aberrant groups followed by a data normalization method, which aims to bring samples into the same distribution allowing for dataset-wide comparisons. additionally, we describe a hit detection function based on the cumulative distribution function  to identify samples with putative, 'non-normal' phenotypes. for clarity, the terms normal and wild type  are used to describe the typical response of the population. generally, this could be the untreated  population or the base level of the system . non-wild type responses, a hit or mutant, refer to a response that is distinct from the normal response distribution, with a putative hit/putative mutant referring to a sample that is predicted to have a response different from the normal response distribution but has yet been confirmed. in high throughput screens, the objective is to identify putative hits balancing the false positive rate , or the number of wt samples that are called hits, with the false non-discovery rate , the number of true hits that are missed. results are presented from analysis of the synthetic dataset and biological data.

RESULTS
input data characteristics and structure
mipheno is specifically designed for the analysis of first pass screening data where the majority of measured responses are from the wt or normal class and the number of responses not in this group  is quite small. examples of experiments yielding appropriate data are non-targeted protein binding/activator assays, reporter gene assays, or population screens, where there are either no defined classes or very unbalanced classes such that a large majority of responses fall in the wt class. data coming from a treatment vs control experiment would not meet the criteria if there were large numbers of 'non-wt' responses expected. additionally, the approach is tolerant to repetition of both individual samples and sample groups across the course of the experiment so long as the portion of individuals showing a wt response in any sample group is over 50%. as the portion of wt individuals in a sample group decreases, there will be a reduction in accuracy and a corresponding increase in false non-discovery rate  due to the assumptions of the algorithm, as demonstrated in the testing section below. additionally, while some measured responses may not be independent , the method treats these attributes  as independent to increase the flexibility of the analysis. for instance, the results for attribute  <dig>  do not impact the results for attribute  <dig>  this is beneficial in post hoc analysis where the individual performing the analysis has limited knowledge of the relationship between measures.

input data for analysis by mipheno assumes that multiple attributes are measured for each individual. the data structure treats each row as an individual sample, whose relationship to other samples can be described by one or multiple factor variables represented in columns . for example, the assay group representing the identification number for a 96-well plate containing up to  <dig> individuals. subsequent columns describing the response of the individual to some assay  are quantitative, continuous values. information must be present that enables association of a grouping factor to the attribute responses, but a single data object may include the responses for different attributes as long as the appropriate grouping factor is present. for example, a 'lc_id' column might provide the grouping factor for ten columns of lc-ms amino acid data, while 'hplc_id' might provide the grouping factor for five columns of hplc-derived responses on the same set of samples. this structure is aimed at simplifying situations where multiple measurements are taken on the same individual.

algorithm
mipheno is based on invariant set normalization with three key assumptions made of the input data. the first is that samples from the same genetic background should have a similar assay response over time. this implies that, given a population p, the distribution of an observed response r from sample set p in set p should have the same distribution as the response r from population p as p approaches p. following this logic, the second assumption of the data is that the observed differences between the distributions r and r are due to technical error as opposed to biological or genetic variance as p approaches p. the last assumption is that there will be limited observable effects of simple genetic manipulations to an organism for any random gene. this is based on empirical evidence from years of published studies  <cit> . specifically, due to genetic redundancy and metabolic flexibility, a given disruption in gene function will likely cause a response outside the wt distribution in only a limited number of measured responses.

these assumptions are similar to those for microarray analysis, specifically that for a random or large grouping of individuals , changes will be observed for a relatively small proportion  <cit> . other assumptions used to normalize the data  have the same effect of forcing the median value of a sample set across several experiments or arrays to be equal. similar assumptions also apply to data from other high throughput screens, e.g. reporter gene-based assays and enzymatic assays.

an overview of the algorithm is presented in figure  <dig>  the algorithm requires that input data have a grouping factor that presents a batch or process group on which the normalization steps can be performed . if multiple grouping factors are present  it is recommended to use the factor representing the highest level of technical  error for normalization. this can be determined by familiarity with the methodology or by checking the grouping factors to see which factor has the largest interquartile range for group medians.

quality control method
in performing post-hoc data analysis it is often unknown if on-line quality control  was conducted or where process changes occurred that could negatively affect the outcome of analysis. to address these issues, a quality control  step prior to analysis was included to identify samples with a high likelihood of assay or group-specific process error. examples of sources of these types of error include instrument malfunction , abnormalities in growth or preparation of material , or improper sample handling affecting a group of samples exposed to the same conditions rather than an individual response. if an on-line qc step was already used to filter the dataset this step can be omitted. thresholds for qc are determined from the overall distribution of the collected data with a user-defined cut off; for example groups with group median >  <dig> median adjusted deviations  from the global median. the amount of data removed will depend on the cut off used and the data distribution. a visual inspection of the data using box and whisker plots is advised to check the data for clear signs of drift or likely changes in protocol that may require manual qc. examples would be group medians steadily increasing or decreasing across dataset or a switch to a new average median response corresponding with sample order, respectively. for post hoc analysis on datasets where the order in which samples were assayed or collected is unknown, it may be advisable to use a cut off of  <dig> mad to permit more data passing on to the next stage.

data quality is assessed on an attribute-by-attribute basis with the assumption that the measured traits are independent; with an attribute being any measured or observed response. thus, if multiple attributes are measured for a group , only attribute data for the trait that shows high deviation would be removed and the rest of the data for the group retained. for example, 'hplc_id' is the grouping factor for the response of metabolites, such as amino acids. the overall response distribution of each metabolite is assumed to be independent of the other metabolites; thus if the measured response of alanine is 10x the response of proline it will not impact the qc step . if the median response for alanine in hplc_id =  <dig> is greater than the qc cut off, all responses for alanine in hplc_id =  <dig> are removed but the other measured responses for hplc_id =  <dig> are retained, provided they too pass qc. while this does not control for drift, it provides a facile qc step for post-hoc data analysis where the order of data generation is unknown.

normalization
the normalization process is done on an attribute-by-attribute basis using a user-defined grouping. a grouping factor should encompass the highest amount of non-biological variation and may be the same factor used in the qc step, but should include as many individuals as possible . a scaling factor is calculated to bring the median of each group to the global median, similar to invariant set normalization  <cit> . the key difference from invariant set or quantile strategies is that just the median value is used, not an explicit individual or multiple quantiles to take into account lack of replication between groups and limited sample size. it is important that groupings represent a selection of individuals where the frequency of non-wt behaviours approaches that of the overall population to avoid bias in cases when a particular group is enriched with non-wt behaviors for a given attribute.

testing
to gauge the performance of the approach, a synthetic dataset was generated emulating characteristics of actual data . this dataset was used initially since the true properties of the individuals could be known, allowing for observation classification  and to evaluate the effect of population distribution on the performance of the method. figure  <dig> illustrates the population distributions used to test the performance of mipheno.

comparison of two different data analysis approaches was used to test 1) if pre-processing steps remove high amounts of real biological variation indicative of a putative hit and 2) whether an increased false non-discovery rate  resulted from using mipheno verses a sample-group based method . the first approach referred to as 'raw', uses the raw, unprocessed data, but followed the same process as in mipheno to identify putative mutants. differences between raw and mipheno aid in illuminating the effectiveness of pre-processing in noise removal. the second approach, referred to as 'z', also utilized the raw data but used a mad score on a sample-group basis to identify putative mutants as described for the chloroplast  <dig> data  <cit> . comparison of mipheno to z aids in determining potential loss of information due to normalizing across the data sets , or if the group-based error was controlled for without negatively impacting hit detection. in a review of performance metrics by ferri et al.  <cit> , accuracy  was found to be a better metric than area under the receiver-operating curve  in the case of unbalanced sample size as well as misclassification noise, which are both properties of the data under analysis. conversely, they found auc outperformed acc in probability and, to a lesser degree, ranking noise. false non-discovery rate is an important metric when considering first-pass screens as one seeks to limit the true positives missed, which is the situation described here.

results of the performance trials using a combination of two population distributions that had a high frequency of wt  =  <dig> ) and low wt frequency  =  <dig> ), drawn from populations of equal standard deviation  or relative standard deviation  , are shown in figures  <dig>   <dig>  and  <dig>  these results suggest that the proportion of true wt in the sample had little effect on the performance of the methods relative to each other, regardless of the metric used; however, the accuracy is decreased and the false non-discovery rate is increased for all methods when the portion of data from the mutant class is increased . mipheno showed a higher accuracy and lower fndr  across a range of fdr cut offs compared to the other methods . furthermore, the auc of both mipheno and z outperformed an analysis of raw , which performed just above what is expected at random, highlighting the importance of controlling for group-based variability. in summary, mipheno outperformed both the raw and z-methods across all three metrics tested.

implementation
results from the chloroplast  <dig> project  <cit>  were used to test the performance of mipheno on experimentally generated high throughput screening data. this dataset includes results for leaf protein amino acids and fatty acid methyl esters as well as seed protein amino acids for plants run through the chloroplast  <dig> pipeline. multiple individuals representing the same seed stock or the same gene are present in the dataset although they were not assayed in the same group. thus, it is of interest to look at the consistency between individuals representing the same gene to identify leaf and seed metabolite data from mutants in the col- <dig>  ecotype genetic background were processed using mipheno and z score methods independently. figure  <dig> outlines the methods for comparison. briefly, both mipheno empirical p-values and z scores were calculated for the two data measurements available in the chloroplast  <dig> dataset . the average score per t-dna insertion line was calculated for each data type to avoid overemphasizing lines that were analyzed multiple times. aracyc  <cit>  and gene ontology   <cit>  information obtained from the arabidopsis information resource   <cit>  were used to generate a list of loci previously demonstrated to have a biological function in arabidopsis. loci with phenotypes predicted by the methods were compared to the list of literature-documented loci. the biological role and/or phenotypes of the genes were compared to the published information to determine the accuracy of the prediction. results are given in table  <dig>  while both methods had a similar frequency of correctly identifying mutant phenotypes at the initial level of z cut off of  <dig> , the z method returned fewer lines than mipheno. it was necessary to adjust the z threshold to  <dig>  to recover these lines, which resulted in no additional mutants but an increase in false positives. overall, there was ~four-fold improvement in the ability to detect previously described or expected phenotypes compared with the z-score.

*aracyc information not updated, manually added

results of the analysis presented in figure  <dig> 

discussion
mipheno offers a way to control for assay variability in high throughout mutant screening studies. it outperformed using raw data or the group-based z method in mutant identification on the synthetic data set . comparison of population parameters including proportion of wt and the distribution shape suggest that the method is tolerant to uneven distributions  and to higher mutant frequencies within the population. when applied to a biological data set, mipheno led to identification of more true mutants than the z method for the chloroplast  <dig> set  based on literature reported phenotypes or pathways. this suggests that mipheno reduces the false positive rate by decreasing the variation due to batch effects but does not directly influence the false non-discovery rate. the method additionally offers the user the ability to utilize any a priori information on the wt population/null distribution available as well as customize a quality control step that is sensitive to the needs of their process.

one drawback of using the normalization strategy described here is that it fails to control for the within-group variance to the degree that a quantile normalization strategy might. quantile normalization makes the assumption that both the median or mean and the standard deviation of the data are all equal and would require sample sizes to be more or less equal as well as large enough to start approximating the normal distribution. this assumption does not always apply to post-hoc analysis; for example, the size of the sample groups in the chloroplast  <dig> data set varied from  <dig> to  <dig>  mipheno aims at addressing this type of use case.

CONCLUSIONS
the strong performance of mipheno on two different data sets and its ability to permit cross-dataset comparisons of individuals without explicit controls makes it an ideal method for processing large datasets prior to meta analyses combining different data sets from high-throughput experiments. because more researchers are making their primary data available and the number of large-scale, high-throughput experiments keeps increasing, mipheno will provide a valuable processing platform that can theoretically be applied to very diverse measurement types .

