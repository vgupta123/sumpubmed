BACKGROUND
the human immune system is a rich source of information about the health and disease status of an individual  <cit> . immunosignaturing is a new technology that may be useful to decode the vast amounts of health information contained in the immune system. an immunosignature is a pattern containing multiplexed signals from chronic or recently matured antibodies. these signals come from a sufficiently diverse set of peptide targets on a microarray. thousands of peptides of random sequence  provide the density and diversity sufficient to discriminate different diseases. an initial question, and the aim of this paper, is how best to analyze and decode the information from immunosignaturing studies. previous reports  <cit>  used frequentist statistics  and cluster analysis  to identify features that classify disease states. we examine other methods that may yield better performance in immunosignature analyses. corrected t-tests as well as logistic and multinomial logistic regression models have demonstrated an ability to differentiate between patients with different disease states even after stringent corrections for running multiple statistical tests . confirmatory factor analysis is an additional method which provides an abundance of information relating to the clustering of samples as well as providing an alternative method for categorizing and determining the disease state of a single sample. descriptive statistics help to paint a better picture of the overall immune system activity. finally, structural equation modeling and mixture models can help explain the underlying structure of an immunosignature.

for these analyses we examined a dataset containing breast cancer samples along with patients who had a second primary tumor . the group with a second primary tumor was included in the analyses because if these patients could be diagnosed as having a high probability of developing a second tumor, they could be more closely monitored.

in an immunosignaturing study, sera samples are collected from participants and the physical information from the immune system is extracted using high density peptide microarrays. each microarray contains a large number of peptides; in this case  <dig>  peptides. the selection of these peptides was designed to give broad spectrum coverage of relevant antigens in the human immune system. the relevant nature of each peptide capitalized on early phage display research  <cit> . the decision was made to use a peptide microarray instead of phage library panning because of the increased speed and efficiency offered by a peptide microarray  <cit> . ideally, if we can better understand the information captured by the peptide microarrays we may be able to develop quick, accurate, unobtrusive and inexpensive screening tests for many types of disease.

classic peptide microarrays are created by spotting overlapping peptides corresponding to linear sequences of proteins known to be involved in an infectious disease. these arrays cannot identify non-linear epitopes. the epitopes are identified when b-cells produce antibodies  specific to 8- <dig> residue peptides that are components of the antigen protein. in contrast, immunosignaturing arrays utilize random-sequence peptides. random sequence peptides have some specific and reproducible affinity to antibodies, and determining the level and pattern of binding is core to determining the difference between patients with different diseases.

although much research has been done on statistical analyses using microarrays, immunosignaturing microarrays pose a number of novel challenges not encountered in traditional microarrays. in nucleic acid microarray technologies, binding is essentially only between two types of molecules of complementary sequence. for example, in a genotype array, genomic dna binds to complementary nucleic acid probes that have either matches  or mismatches  and the signals from the different probes are combined to make homozygous and heterozygous base calls for individual single nucleotide polymorphisms . in a gene expression microarray, only a specific fragment of rna will bind to the oligonucleotide on the array. with modern microarrays, as long as there is a sufficient abundance of rna on the array, it will generally bind only to the specific complementary probe, with very limited non-specific binding.

with immunosignaturing microarrays, the intensity values are a continuous value from 0- <dig>  and binding is not restricted to a single "complementary" molecule. multiple antibodies in igg could bind to the same 20mer peptide on the array. also, although the immunosignaturing arrays are designed to measure igg, there may still be competitive binding from other material in the sera and from other types of immunoglobulin. competitive binding could result in an igg antibody not binding at all or binding with a lower affinity. this could be potentially problematic if the auxiliary particle reducing binding affinity does not differ systematically across groups. furthermore, a single antibody may also bind to multiple peptides on the array; a problem almost non-existent in genotype or gene expression arrays.

with the potential for so many different things to bind to a peptide on the array, it is not immediately clear how accurately traditional and more novel statistical methods would perform. one primary goal of the research reported here was to determine if the proposed statistical methods were capable of effectively analyzing the data and producing a correct pattern of results. for example, with a number of different things binding to a peptide and antibodies binding to multiple peptides it was initially uncertain if this would produce erratic signatures which would lead to incorrect results when certain methods were used.

despite a number of new complexities created by immunosignaturing microarrays, these challenges give us the opportunity to test the performance of classically used methods such as factor analysis models in a different environment while also allowing us to ask new and fundamentally different research questions. in order to answer these new research questions, there is a need to use different statistical models not commonly used to analyze microarray data. this is because more traditional models used to analyze microarrays lack the versatility to adequately capture and explain the complexities of immunosignatures. here, we explore the use of structural equation models in order to try to determine whether the immunosignature formed by the fluorescent values of the  <dig>  peptides is mostly random or if there is a consistent underlying pattern or factor structure to an immunosignature that correlated with disease. this research question is made possible because of the novelty in immunosignaturing arrays that that allow a single antibody to bind to multiple peptides on the array. this research shows that there are complex and consistently reproducible structures underlying peptides which differentiate groups. such patterns can be used as biosignatures for disease as well as provide deep insight into antibodies and immune response to disease. although there are new analytic challenges in immunosignaturing, it is these exact challenges that provide the promise of new discoveries while laying the groundwork for applications in future research and technologies.

in this paper we present a range of statistical methods, their use and demonstrate what type of information they can provide researchers in immunosignaturing studies. we show the ability to classify samples into their respective disease categories and find peptides which significantly predict disease status. this provides a promising method for screening and potentially presymptomatic screening of disease. we also identify a number of latent factors using sem. we hypothesize that the latent factors being modeled may represent specific antibodies that differ among disease classes.

methods
patient samples are analyzed by applying the sera or plasma to the array at a 1: <dig> dilution, detected with an anti-human fluorescent antibody, and the signals are read using an agilent c laser scanner. images are processed using genepix pro  <dig> providing a text file of values for each peptide. binding affinity is a continuous value from 0- <dig>  . genepix software was used to convert the 16-bit tiff images to values, median non-background subtracted values were used and log <dig> transformation was done on the median normalized intensity values. three distinct datasets were used in these analyses. one was a set of samples from a random group of individuals without breast cancer, a second set of samples is from a group with breast cancer and finally the third set of samples is from a group of patients who were diagnosed with a second primary tumor. the normal samples were a convenient sample of individuals without any known breast cancer history. the breast cancer samples were a sample of current breast cancer patients with different levels of disease progression and diverse demographic backgrounds. there were  <dig> samples from normal individuals without cancer,  <dig> samples from cancer patients with a single primary tumor and there were  <dig> samples with second primary tumors. human subjects protection was observed, collaborators ensured all samples were collected under the same protocol. all of the sample came from females between the age of  <dig> and  <dig>  the specific ages for each sample was kept from us because of hippa and patient privacy concerns. all pre-processing was median-normalization per microarray slide, to adjust for global intensity bias. data was also log <dig> transformed. the spot intensity was the median signal  with no local background subtraction. background subtraction was not used because the arrays showed consistent background across the  <dig> empty spots which were spread across the physical surface of the array. technical replicates also showed greater reproducibility without background subtraction than with, indicating that the method for subtracting background was not useful. additionally, the local and global background estimates were, on average, 150- <dig> rfu, which for any microarray is extremely low considering the 3+ logs of dynamic range.

it is common in similar lines of research, such as genotype experimentation to use a pattern matched experimental design. matching participants in an experiment has the effect of increasing homogeneity among groups. as a result, the reduced within class variation which often accompanies matching designs has the effect of reducing the standard error and denominator of common statistical tests. this in turn leads to higher statistical power. additionally, more homogeneous groups often enable easier classification in exploratory models. in the data analyzed here, the normal non-cancer samples were not matched to either the cancer groups, however research has shown that the signature of immune response is far less susceptible to the type of personal factors that genetic studies are - even hla has only a minor effect on the consistency of a disease state immunosignature pattern  <cit> .

given that immunosignaturing is a new technology, early investigations, contrary to initial belief actually capitalize on the lack of rigid experimental designs. this is because additional sources of variance in the data allow us to better understand the robustness of the technology and related statistical analyses. if a method can perform well in a somewhat noisy environment with loose experimental designs, it is highly likely to perform even better when well curated studies  are performed. in many respects, testing immunosignaturing data with loosely structured and curated data provides a much more stringent test of the technology and methods. being able to obtain statistically significant results with the correct patterns of results from such unstructured data illustrates the versatility of immunosignaturing technology and the statistical methods tested here.

understanding the robustness of the technology provide guidance for future experiments using this technology while giving insight into the potential clinical use of immunosignaturing. biologically, it is possible that healthy normal individuals with no active infection are responding immunologically to their environment, and persons with an infection have a focused immune response. it is likely that high variation in immune response to an environment would be present across individuals. therefore, in order to be clinically useful, it is imperative that the technology and methods are robust enough to function accurately outside of precisely controlled laboratory settings; as would be encountered during clinical deployment of the technology.

RESULTS
descriptive statistics
although there are large differences in the ranges, in order to have any predictive validity, the differences in ranges need to be consistent across samples within each group. for example, a high fluorescence value over  <dig> in the second tumor samples needs to occur on a given peptide with regularity to produce a statistically significant result.

classical statistical significance tests
there are a number of statistical tests which could potentially be used to test whether the differences between groups across peptides are significant beyond what would be expected by chance alone. some of these methods include the t-test, corrected t-tests, logistic regression and multinomial logistic regression. the standard t-test divides the mean difference between two groups by a standard error to produce a t-statistic used for null hypothesis significance testing. one problem with the standard t-test is that normal theory underlying the test makes the assumptions that the variances in both groups are equal. the problem of unequal variances in a t-test is commonly known in the statistics literature as the behrens-fisher problem and has been researched for the better part of the last century in various contexts. if the assumption of equal variances is violated, the t-statistic can be either inflated or deflated depending on the samples sizes in each group. as a result, the analyses were conducted using a satterthwaite corrected t-test. the satterthwaite test is one of numerous corrections for unequal variances that have been proposed over the years. the satterthwaite test works by adjusting the degrees of freedom in the test. the resulting correction produces an asymptotically correct t-statistic when groups have unequal variances. the satterthwaite correction works by modifying the degrees of freedom via equation 1:

  df=*2w1*2n2-1+w2*2n2- <dig> 

a satterthwaite corrected t-test and a number of similar test corrections which could have also been used such as a brown-forsythe correction in an anova model tended to produce statistically significant results after a bonferroni correction for multiple testing . a bonferroni correction was used to protect against alpha inflation because with a standard alpha level of . <dig>  purely by chance alone,  <dig> out of  <dig> tests will be significant. the bonferroni correction divides the alpha value by the number of tests run; in this case  <dig> , or one for each peptide on the microarray. this resulted in a corrected p-value threshold of  <dig> *10- <dig>  nonetheless, despite this much lower p-value, highly significant results are still obtained for satterthwaite corrected t-tests comparing normal versus single tumor cancer samples, normal versus second diagnosis samples and single tumor cancer versus second primary tumor cancer samples. table  <dig> shows the top  <dig> significant peptides for a satterthwaite corrected t-test comparing normal samples to cancer samples. logistic and multinomial logistic regression may also be of interest and an alternative method for comparing groups to the tests used here. one place in which logit models may be useful is if a researcher in future studies has a known set of covariates they wish to control for. for example, in the study of diabetes, it may be of interest to control for body mass index or hb1ac test results.

exploratory factor analysis
factor analytic models have previously been used in analyzing immunosignatures and are quite common in analyzing high dimensionality microarray data  <cit> . each of the models explored during this line of research were investigated in order to determine its feasibility for answering a specific research question. exploratory factor analysis  was examined as a method to be able to differentiate samples based on disease states with no prior clinical knowledge of the samples. estimation of efa models was performed using ordinary least squares . efa with promax rotation proved significantly better than chance at classifying samples. efa is a set of procedures that accounts for the relationship among a set of variables in terms of a smaller set of underlying latent constructs or factors.  we specifically use principal axis factoring with iterated communalities. although pca and efa are quite similar, an important difference between the two methods is that pca makes the assumption that all of the variance in an item is a reflection of common variance shared among all items whereas efa posits that each item shares some common variance with all other items but also has its own unique variance. mathematically the difference between pca and efa is the addition of single matrix; d <dig> 

  rzz=a*rf*a’+d <dig> 

in equation  <dig> rzz is the correlation matrix among the observed variables. a is a matrix of factor loadings, rf is the correlation matrix among the factor loadings, the a' denotes the transpose of the a matrix of factor loadings and thus arfa' is the matrix representation of the common factor structure. d <dig> is a diagonal matrix that captures the unique variance weights and distinguishes efa from pca.

varimax and promax rotation methods were explored in depth. this is in part because varimax is often a starting point for a promax rotation. a sample is said to "load on" a given factor when the model suggests a strong fit on the given factor. rotation in efa is a method for making factor loadings more interpretable. rotation methods change the relationship between items and the factors . rotation does not change the relationship among the individual items. since rotation methods only make changes to the axes and not to the communalities , rotation does not mathematically change the initially obtained results. rotation makes the factor loadings more interpretable.

varimax uses a complexity function to maximize the variance of the squared loadings on each factor. this results in loadings with a more even spread across the factors; as opposed to having an overabundance of loadings on a first factor. varimax is an orthogonal rotation that maintains the orthogonal  intersection of the axes. this has the result of keeping the correlation between the factors at zero because the cosine of  <dig> degrees is  <dig> 

promax is an oblique rotation that allows the angle between the axes to vary. in statistics, variance has to be accounted for in some part of the model. allowing the axes to vary and thus a correlation between the factors is another path to account for variance. allowing variance to be expressed in terms of correlations between factors has the result of not forcing variance between factors to be represented as between item variance. this can result in cleaner factor loadings. additionally, the assumption that there is no correlation between factors, or in this analysis, disease states, is unlikely because there will always be some additional common variance and similarities in immune samples due to basic immune responses and structures present across all samples.

unlike varimax, promax does not use a complexity function. rather, promax rotation is a procrustean rotation to a target matrix. in promax, a pattern matrix of loadings  is taken to some power  to form a target matrix. the original loading components are then rotated to get as close as possible to the newly formed target matrix. a number of efa models with promax rotation were run to investigate the utility of this method for differentiating between groups with no prior knowledge of group membership. table  <dig> provides summary results. the number of factors was known to be  <dig> for each comparison. scree plots were used to validate the hypothesis. none of the plots suggested the presence of a strong third factor. a scree plot plots the eigenvalues for each component. the largest components before a leveling off is used to determine the appropriate number of factors. factor loadings greater than . <dig> were said to load on a given factor. if loadings for both factors were less than . <dig> the sample was said to not counted as a correct classification on either match. catell  provides a more detailed description of how to use eigenvalues and scree plots for determining the number of factors  <cit> .

an efa between cancer samples and the samples from patients who had a second primary tumor produced a correct classification for  <dig> % of the cases. of the cases that were miscategorized, all of them except one were cancer cases that loaded more highly with the second primary tumor group. there are a few possible explanations for this. this could simply be model error resulting from the lack of homogeneity among the first time cancer group. however, it is possible that the miscategorized cases may represent individuals who will at some point in the future develop a second primary tumor or are unbeknownst to the researchers already in the process of developing one. all this says is that less than 10% of cancer samples are more closely related to the samples of individuals who had acquired a second primary tumor than the samples with a single primary tumor.

a second efa was run between the normal or noncancerous samples and the samples with a second primary tumor. the overall classification accuracy was  <dig> %. within this model,  <dig> % of the normal samples loaded correctly on the same factor whereas 100% of the second primary tumor samples loaded on the correct and same factor.

a third efa was run exploring the relationship between normal or non-cancerous samples and single tumor cancer samples. using the same model specifications as in the first model, this efa produced a 68% classification accuracy. although this is quite low by traditional model building standards, there are a number of factors relating to the data which may make this a useful starting point. first, the normal patients were taken from a wide range of convenient lab samples. some of the normal samples may have come from individuals outside of the age and traditional demographic background to even be remotely at risk for breast cancer. secondly, the stage and progression of cancer patients was unknown. as a result, an additional possibility for the classification accuracy may be that the cross loadings represent a mixture of early stage cancer patients and those at high risk for or who are developing cancer.

unfortunately, detailed information about the disease state of the samples is unavailable and does make conjectures purely hypothetical. however, in all models, the results are significantly better than chance and illustrate in many ways the performance of the technology and approach under adverse conditions. the three models taken in concert illustrate that the lack of a concrete and well curated control group is likely responsible for the decremented classification accuracy in some models. this can be most clearly seen when considering that the single tumor cancer and second primary tumor cancer samples consistently exhibit stable factor loadings with relatively low cross loadings because the single tumor cancer samples serve as a much cleaner control group for the second primary tumor cancer samples than the normal do for either of the cancer groups. this early research suggests that future studies using more precisely selected control groups and experimental design would have even better ability to classify cancer patents.

beyond classification accuracy, the similarity between different factor based models and rotations is extremely informative from a biological perspective. all combinations of pca and efa with varimax or promax gave highly similar results with respect to overall classification of groups across a number of different analyses. although specific factor loadings certainly had different values, the overall picture and classification accuracy was relatively constant. brief investigations into other rotations such as oblimin were also explored in the context of efa models and produced similar results to varimax and promax.

first, with respect to pca versus efa, the lack of difference suggests that the vast majority of the variance accounting for classification is at the factor level  and not the individual level. this is because as the d <dig> matrix which differentiates the two methods captures the unique variance in an efa model and as the d <dig> matrix approaches zero, an efa model approaches a pca model. therefore, since the d <dig> matrix is the only difference in the equation and an analytic solutions exists due to ordinary least squares estimation, we can conclude that the lack of difference was because there was relatively little unique variance present.

confirmatory factor analysis
since efa models showed the ability to differentiate samples, a logical clinical application of immunosignaturing would be to screen a single sample from an individual to determine his or her disease status. confirmatory factor analysis  was chosen as an ideal method for investigating this question due in part to the similarity with efa and because of the versatility to examine one specific sample in detail. efa is an exploratory method that should be used when the number of groups or structure of the data is not well understood. conversely, cfa is a confirmatory method that can be used when the structure of the data is well understood. as the name implies, exploratory factor analysis, efa models should not be used as confirmatory model or to confirm a hypothesis.

both cfa and efa attempt to explain the underlying structure in a dataset. however, cfa and efa approach the problem from two distinct directions. efa makes almost no prior assumptions about the structure of the data and attempts to sort through the data to help a researcher determine what the underlying structure of the data is. in this research, the general group membership was known and thus the appropriate number of factors was specified apriori. in a cfa model, the researcher explicitly identifies not only the number of factors but which cases load on each factor as well as factor variances, covariances between the factors and disturbances for each item. cfa models are not data mining approaches and require well formulated notions about the underlying structure of the data.

mathematically, the simplest formulation of a cfa model in matrix notation is:

  x=∧*ξ*Δl 

in equation  <dig>  × is a vector of observed variables, Λ is a matrix of factor loadings, ξ is a matrix of scores for each variable on a factor or latent construct and Δ is a vector containing measurement error.

in the cfa models analyzed here, one sample from each factor  was chosen at random as a scaling constraint in order to ensure identification in these models. maximum likelihood estimation with robust standard errors was used to estimate these cfa models. the known disease status was the basis for defining the factor loading for each sample. a sample was allowed to load only on a single factor and fixed to zero on the other. variances and covariances between all factors were estimated. summary results are provided in table  <dig> 

for a cfa comparing single tumor cancer samples and second primary tumor samples,  <dig> % of samples loaded on the specified factor. for a normal versus second primary tumor cfa,  <dig> % of the samples loaded on the specified factor and a normal versus single tumor cfa produced sample loadings on the specified factor  <dig> % of the time. the difference in classification accuracy between the cfa and efa models is due to a number of factors; some of which include model variance and covariance specifications as well as different estimator types.

one primary advantage cfa models have over efa models are fit indices which give some quantitative measure of how accurately the specified model is. although there are a plethora of fit indices that have been proposed within the structural modeling framework that cfa models reside, the chi-square difference test, root mean square error  and standardized root mean error  are among the most common and widely cited.

the chi-square test ostensibly tests how well the specified model reproduces the covariance matrix from the original data. the problem with this test is that it is so sensitive that it is nearly impossible to obtain statistically non-significant results. it is important to note that the null hypothesis of this test is that there is no difference between the specified model's covariance matrix and the covariance model in the actual data, a non-significant p-value is the desired outcome. because it is of interest to find no difference between the specified model and the data, a non-significant p-value is the goal. the chi-square test for all of the cfa models was significant with p < . <dig> suggesting that there is a statistically significant difference between the specified model covariance matrix and the covariance matrix of the original data. however, the chi-square test is extremely sensitive and often detects trivial differences  <cit> . noting the sensitivity of the test is not meant to suggest that in fact the specified cfa models are perfect fits or deny lack of fit. rather, the test is noted because it is among the most common fit indices and the issues with the test are noted as a means of providing appropriate context for the results.

the root mean square error of approximation  and the standardized root mean square residual  are two common fit indices used in the structural equation modeling  framework description; of which cfa is a part of. the basis of the rmsea is a non-centrality parameter. the simplest reduced form of the rmsea equation is:

  rmsea=x2df-1n- <dig> 

in equation  <dig>  x <dig> is the model generated chi-square value, df is the degrees of freedom and n is the sample size. smaller rmsea values suggest better fit. the srmr measures the standardized difference between the observed covariance matrix and the model implied covariance matrix.

for the cfa model for single tumor samples versus second primary tumor samples, the rmesa was . <dig> and the srmr was . <dig>  for the cfa model comparing normal versus second primary tumor samples the rmsea was . <dig> and the srmr was . <dig> while the normal versus single tumor samples produced a rmsea of . <dig> and a srmr of . <dig>  these are marginally significant results because traditional benchmarks cite . <dig> as a cutoff for statistical significance  <cit> . rmsea and srmr values in the .05-. <dig> range are usually regarded as marginally significant. although the results do not meet the rigid . <dig> level, they are actually quite impressive when considering the experimental design and the fact that a portion of the lack of fit may actually be representing natural biological patterns such as the development of a first or second tumor.

perhaps the real utility of a cfa model for immunosignaturing could come in the form of diagnostic testing. given the accuracy of the cfa model with this data, once a well curated set of samples for a certain disease or collection of diseases has been established, a cfa model could be specified where a new unknown sample could be allowed to load on both  factors. by comparing the relative loadings on the factors, it would be possible to determine to which group the sample most likely belongs. for example, there are numerous subtypes of breast cancer and different stages of disease progression. if a collection of samples was available as a concrete reference set, a cfa model could be easily and accurately employed as a new method for aiding in the diagnosis as well as perhaps early detection of breast cancer.

structural equation models
from efa, cfa and descriptive statistics we know that the immunosignatures as a whole are in fact different across groups while corrected t-tests show that there are statistically significant systematic variations. the logical question arising from these findings is how precisely do the immunosignatures differ from one another? is there a clear, consistent and reproducible pattern underlying the differences in immunosignatures across disease states? because a single antibody can bind to multiple peptides and different antibodies can bind to the same peptide, a coherent pattern of peptide fluorescence across an immunosignature is much more informative than the fluorescence of individual peptides on their own. furthermore, being able to identify common relationships and covariances between groups of peptides is of even greater utility. this can be accomplished by modeling latent factors.

on a genotype microarray, the probe is directly measuring an individual's genotype at a specific location. in contrast, the peptide probes on an immunosignature array are indirectly measuring immune response and antibodies present in the sera. when measures are not directly observed they are often referred to in statistical and structural equation modeling literature as latent factors. if there are clear, consistent and reproducible patterns caused by specific antibodies in a sera sample binding to peptides on an immunosignaturing array, it should be possible to model individual antibodies as latent factors. for example, when reading the tick marks on a mercury thermometer, one is not reading a direct measure of temperature but rather displacement of mercury. the latent factor measured by displacement of mercury is temperature because from a purely physics standpoint, temperature is the kinetic energy of an object; usually measured at the molecular level. another example of a latent factor is depression. psychologists cannot directly measure depression but they can ask a series of questions that cumulatively allow them to model the latent construct of depression. each question in a depression inventory gets at one small piece of the latent factor depression in much the same way that peptides on an immunosignaturing array provide an indirect measure of immune response; as measured primarily by igg antibodies.

structural equation modeling  is specifically designed for modeling latent variables. sem models have two parts: a path model comprised of regressing a set of variables on another and a measurement model in which cfa is used to form latent variables. when a set of measured variables is set to load on a given factor, the result is a latent factor. in sem, the resulting latent variables can be treated as either endogenous or exogenous variables; depending on the research question of interest. a full sem model is a collection of equations defining each variable and their relation to one another. since complex models can quickly generate a large number of equations, sem models are often represented graphically for quicker interpretation. since confirmatory factor analysis is a major component in a full latent variable structural equation model, attempting to classify samples with factor analytic methods lent evidence to the feasibility of sem models. these early models also provided a plethora of background information which aided in the testing of full sem models.

initial sem testing
despite evidence from previous factor analytic models that sem models should be feasible, since these are highly complex models, an incremental approach was taken to building and testing large scale sem models. to start with, a measurement model and full structural equation model was run using the top three peptides from the normal versus single tumor cancer samples  to predict disease. the measurement model  in a sem model tests the loadings of individual peptides onto latent variables. in this model one peptide was set as a scaling constraint and the other two were freely estimated. three peptides were chosen because that is the minimum needed for model identification and provides for the simplest model. because of the iterative nature of the maximum likelihood algorithms used in sem models, starting with a simple model reduces computational time and aids in convergence. furthermore, starting with the simplest model and building up is good practice in modeling.

since a measurement model with  <dig> factors is just identified or has no extra degrees of freedom, fit indices cannot be calculated. however, all the variables load strongly on the latent factor with loadings greater than . <dig>  this finding suggests that the top  <dig> peptides are indicative of a single underlying latent factor.

in order to help rule out the possibility that the consistent loadings in the first model were not type  <dig> error or false positive, the same model specification was run in an attempt to see if the top  <dig> peptides differentiating single tumor cancer samples from second primary tumor cancer samples. in this model the top  <dig> peptides also loaded on a single latent variable. like the first model, the second model illustrated the same pattern of results with the top  <dig> peptides all significantly loading on a single latent factor.

the same pattern of results can be replicated with two disease contrasts. replicating the finding with normal versus a single primary tumor cancer and second primary tumor cancer versus single primary tumor dramatically reduces threats to validity against causal conclusions proposed by sem models of immunosignaturing data.

when investigating models that differentiate two distinct groups from a baseline group  there are three potential outcomes. first, a complete lack of model fit and no consistent underlying factor structure. in this case, none of the peptides would load consistently and correctly on either of two specified factors suggesting that peptide florescence is random. the second possibility is that all of the peptides would load on one factor. this result could result from any number of potential biases in the technology itself, printing or processing of the microarrays. another reason all of the peptides might load on one common factor is that they are all part of a single latent factor. however, because the significance of each peptide varies quite precipitously across group contrasts, it seems unlikely that a single underlying latent factor would produce different significance values across disease contrasts. the third possibility is that the peptides significantly load on two separate factors and that the peptides for each contrast exhibit no cross loadings.

a series of analyses was run using significant peptides from normal versus single tumor cancer corrected t-tests as well as second primary tumor samples versus single tumor samples combined into a single model. the first model was a measurement model which added the first two cfa's into one model. the top  <dig> peptides for normal versus single tumor samples and single tumor samples versus second tumor samples each were set to load on a separate latent factor. a covariance between the two latent variables was also estimated. the path diagram in figure  <dig> illustrates this model. in figure 1's path diagram, the square boxes represent measured variables, which, in this case are peptide fluorescent values. the large circles are the unmeasured latent variables. the arrows between the latent factors and measured variables show which measured peptides load on which latent variable. the curved arrow represents an estimated covariance between the two latent variables.

in path diagrams, the arrows represent the causal flow of information. the arrows are pointing from the latent variables to the measured variables because the argument in sem models is that there is some unmeasured and underlying latent construct that is responsible for the observed results of the measured variables. the immune response and antibodies present in the sera samples is the ultimate causal factor of peptide fluorescence.

the model tested in figure  <dig> was estimated using maximum likelihood estimation with robust standard errors . the model exhibits excellent model fit with an rmsea of . <dig> and an srmr of . <dig>  in addition, the chi-square test was not significant, chi-sq =  <dig> , df =  <dig>  p = . <dig>  a non-significant chi-square test is the desired result. again, this is because the null hypothesis of this chi-square test is that there is no difference between the observed covariance matrix  and the covariance matrix implied by the model in figure  <dig>  these results strongly suggest excellent model fit and that the latent factors are unique constructs. biologically, this suggests that a different latent factor is underlying each latent variable.

to further confirm the interpretation that the latent factors are different, one peptide from each factor was switched. v <dig> and v <dig> were set to load on the opposite factor from the first model. in this new model, there was a complete lack of fit. in addition to poor loadings, the fit indices dramatically decreased. the rmsea was . <dig>  the srmr was . <dig> and the chi-square was  <dig> , df =  <dig>  p < . <dig>  thus further suggests two different underlying constructs rather than statistical anomalies.

an additional set of analyses were run using the top  <dig> peptides instead of just the top  <dig>  the first models run in this sequence were varimax and promax exploratory factor analyses. both models gave 100% classification with extremely strong loadings on each factor. table  <dig> is the rotated factor pattern or a two group efa taking the top  <dig> peptides from each disease contrast. this clearly illustrates the top five peptides strongly load on factor one while the last five strongly load on the second factor. the loadings of peptides are consistent with the groups from which each peptide was selected. for example, v <dig>  v <dig>  v <dig>  v <dig> and v <dig> were the top  <dig> most significant peptides differentiating first time cancer samples from second time cancer samples. in combination with earlier results, this very clear and consistent loading pattern strongly suggests that the top peptides for each class form unique latent variables and they are almost irrefutably measuring different constructs. biologically, this suggests that the latent factor which is more active in single tumor cancer samples compared to normal samples is not the same latent factor that appears to be present in second tumor samples.

the same result was also found by running a two group exploratory factor mixture model with geomin rotation. geomin rotation is another oblique rotation method similar to promax. a more complete discussion of the mathematical differences of rotation methods can be found in browne   <cit> . in this data, the observed peptides as a whole form a single distribution. in mixture modeling, the underlying notion is that the distribution formed by all of the observed data is the product of two or more underlying distributions; each of which represents a distinct class. ostensibly, an exploratory factor mixture model is trying to answer the same question as pca and efa, paf/factor analysis but via a different mathematical framework. despite the complexity of mixture modeling, the basis of an exploratory factor mixture model is for a categorical latent class variable c, for a specific class k. the model estimated is:

  yp= vkp+λkp*η*ϵp 

in equation  <dig>  for a variable yp, vkp is an intercept parameter, λkp is a vector of loadings, η is a vector of latent factors and εp is a residual term. in addition, there is a correlation matrix Ψk for the latent factors η of class k along with a distribution for the latent class variable c: pk = p. in this equation, for a dependent variable p, the probability of c is equal to k. also, other constraints are added to this basic framework for purposes of identification but are related to model specific decisions such as orthogonal or oblique rotation.

efa mixture models were estimated using maximum likelihood with robust standard errors  estimation and  <dig> random start values. random starting values were used in part due to the complexity inherent in mixture models and to check for local solutions. by running the analysis with multiple random start values log likelihood  values can be compared. to the extent that different ll values are obtained, the random start values can be directly input into the model and the results can be compared to the best fitting ll model. this is useful because if different start values produce dramatically different results, this might suggest that the algorithm converged at a local maxima instead of global maxima or that the results are unstable.

fit statistics such as the bayesian information criterion  provide a more quantitative analysis of model fit for a series of nested models. efa mixture models were estimated for one, two and three class models. this approach allows us to confirm that a two class model is in fact the best fit for the data.

the series of efa mixture models suggested the same pattern of results as traditional efa models; that there are two distinct and separate underlying classes formed by the top  <dig> peptides for each disease contrast. in addition, mixture models also produce a statistic for the average latent class probability:

  pyp=j|c=k=φ-1t*kpj-φ-1t*kpj- <dig> 

in equation  <dig> t*kpj is a threshold parameter on a standardized correlation metric and φ is a matrix of residuals for the latent factors  <cit> . for both two and three class models, the average latent class probability for the most likely latent class membership was greater than 99% for both class  <dig> and class  <dig>  in other words, for the subgroup of samples classified as being part of class  <dig> by the model, more than 99% of the time, class  <dig> was also their most likely class membership. this further reaffirms the excellent model classification. the three class model produced nearly identical average latent class probability values because the model did not classify any of the peptides as belonging to the third class.

the bic was used to assess the best fitting model. the bic is estimated as follows:

  bic=-2*ll+p*logn 

in equation  <dig> ll is the log likelihood value of the model, p is the number of parameters and n is the number of observations. the lower the value of the bic the better the model fit. often times, bic values or plots are used ostensibly in the same fashion that scree plots and eigenvalues are used in pca or traditional factor models where a researcher looks for the point at which the decrease in values levels off. however, in this analysis, the two class model had the lowest bic and somewhat unexpectedly, the three class model actually saw a slight increase in the bic this result further reaffirms the excellent fit of a two class model.

as is common in model building, a series of full structural equation models  were run in increasing levels of complexity. to start with, the two latent variables were regressed on their respective disease states in individual models. a path diagram for the normal versus single tumor samples is presented in figure  <dig> 

these models were estimated using mlr. the latent variable regression was performed using logistic regression and was significant, p < . <dig>  additionally, the odds ratio was  <dig> . this suggests that having the attributed measured by the latent variable makes an individual  <dig>  times more likely to develop breast cancer. the same model specification for single tumor versus second tumor samples produced similar results with p < . <dig> and an odds ratio of  <dig> . in other words, there appears to be a latent factor that is present in those who have a single tumor that is not present in those samples with a second primary tumor.

furthermore, another sem model was run combining the above two analyses so that the two distinct latent variables were used to predict disease status. the estimation of disease status was done via multinomial logistic regression. this was done because when the models were combined there were three levels of disease. in a multinomial logistic regression model, one level  was set as the reference group. then n- <dig> logistic separate regression equations are run; where n is the number of levels of the dependent variable. therefore, since each latent variable was regressed on disease status, there were two logistic regression equations run. both latent variables predicted their respective disease status with p < . <dig>  again, this suggests that normal, single tumor cancer and second tumor cancer samples are separated by different sets of latent variables.

the first set of sem models provided an initial proof of concept for full sem models. this laid the groundwork for the more interesting question of what the underlying structure looks like for unique parts of the immunosignatures. since further investigations are meant to look at the overall differences in immunosignatures as a whole, it is hypothesized that the latent factors differentiating groups are specific antibodies present in the sera samples; as explained above. two experimental tests were conducted: a series of structural equation models and an examination of the peptide means across groups.

sem models of significant peptides and antibodies
next, all of the peptides that were statistically significant after a bonferroni correction in the normal versus single tumor and second tumor versus single tumor contrasts were selected for further analysis. following the same pattern as before, exploratory factor analysis models were run to determine how many underlying factors appeared to be present. this was done because selecting the top peptides might yield more than one factor; suggesting more than one antibody. for the normal versus single tumor contrast there were  <dig> peptides that were significant and there were  <dig> significant peptides for the second tumor versus single tumor contrast. the eigenvalues and scree plots suggest a three factor solution for the normal versus single tumor contrast and a one factor solution for the second tumor versus single tumor contrast. a scree plot for the second tumor versus single tumor contrast is shown in figure  <dig>  in other words, for the normal versus single tumor, the hypothesis is that there are three antibodies that differentiate the groups while there is only a single antibody differentiating the second tumor versus single tumor groups.

in the second tumor versus single tumor contrast, factor loadings from exploratory factor mixture models and promax efa models confirm an unstable second factor. this is because the loadings on the second factor are generally low and minimally larger than the first factor loading on the same peptide. additionally, a two factor solution produced heywood cases in which there were communality estimates greater than one; suggesting a problem with the two factor model. when single factor models were run, all of the peptides loaded highly on the one factor. as a result of the efa models suggesting a single factor solution, a full sem model was run in which all of the top  <dig> peptides were set to load on a single latent variable which was then regressed on disease status. in this model, the stable latent factor significantly correlated with disease status, p <  <dig> . the odds ratio of  <dig>  suggests that the single hypothesized antibody confers significant risk for acquiring a second tumor. also, the means for all of the peptides in the second tumor samples were lower than the means for the single tumor samples. this suggests immune suppression. in other words, there appears to be an antibody present in samples with a single tumor that is not present in samples with a second tumor.

the normal versus single tumor samples is a bit more complex. a full sem model containing all three hypothesized factors was unable to be estimated because there were more peptides than samples. therefore, there were not enough degrees of freedom to run a full model containing all  <dig> groups. as a result, subsets and individual factors were tested individually. when tested individually, all of the three factors/hypothesized antibodies significantly correlate with disease, p <  <dig> . two of the latent factors positively correlated while the third negatively correlated with disease status.

within the  <dig> significant peptides for normal versus single tumor samples,  <dig> peptides increase or have a higher mean in the cancer samples than in the normal samples increase. conversely,  <dig> decrease or have a higher mean in the normal group than in the cancer group. in other words, there appears to be two new antibodies present in cancer samples not present in normal samples and one antibody present in normal samples that is not present in cancer samples. immunosignatures are unique in analysis of the humoral response in that they can detect decreases in reactivity relative to normal levels.

one finding of particular note is a high covariance between the two positive factors . the high covariance and multicollinearity suggests that the two are very similar. when regressing both of the positive latent variables on disease, in every instance, only one of the latent factors was significant with p < . <dig>  this is likely due to the way in which multiple regression partitions variance. in a multivariate regression model, the effect of one variable  is the unique contribution of that variable with all others held constant. because there is so much common or shared variance, a vast majority of the variance is used up or accounted for by the first factor, not leaving enough unique unexplained variance left for the second factor to be significant as well.

a two level measurement model was run to test to see if the two factors were measuring a similar underlying construct. in this model, the two latent factors were set to load on a third latent variable. the theory behind this test was that if the two latent factors loaded on a single second level latent factor then the two original factors would be measuring the same underlying construct. one way this could occur is if the antibody had a highly complex structure. however, this model was not significant, rmsea = . <dig>  srmr = . <dig>  this suggests that the two factors are unique albeit highly similar.

there are a number of potential interpretations of this result. one of the more plausible biological hypotheses is the presence of subpopulations. among two different cancer subtypes of single tumor breast cancer, there are likely two distinct antibodies; one for each subtype. if subpopulations are present in the data, it seems plausible that these two antibodies are quite highly similar because in the end they are still responding to breast cancer. the variations that lead to different subtypes may in fact be what makes the two positive latent factors separate and distinct from one another. the high covariance and multicollinearity may be a function of the fact that the two different subtypes are still breast cancer. the multicollinearity may be because they vary together, not that they have a similar sequence and see the same antigen. if two different antigens consistently arose in a tumor they would raise antibodies that varied together in samples but would see totally different antigens.

a second possibility is that this is modeling different times in the disease progression. as disease progresses it is likely different antigens are presented by the tumor to the immune system. if so, the relative amount of particular marker antibodies will also change.

discussion
we have explored a number of statistical models for analyzing immunosignatures. each method explored herein helps answer a different research question relating to the analysis of immunosignatures. descriptive statistics about an immunosignature can provide high level information about the general immune response in a signature. exploratory factor analytic models  can be useful for classifying immunosignatures into different disease groups without any clinical information. cfa models can classify samples onto specified factors and could be developed into a useful model for determining the disease status of a single sample. as an extension, sem models find some interesting and robust latent factor structure to immunosignatures which warrant further investigation.

implications of sem models
latent factors can be reliably extracted from immunosignatures. these latent factors are clear, consistent and replicable patterns which differentiate disease state in a statistically significant fashion. at the very minimum, these latent factors can serve as strong biomarkers for disease. given the design of the technology and the fact that antibodies are binding to peptides on immunosignature arrays, it is highly plausible that the latent factors are modeling individual antibodies.

although future research is needed to conclusively confirm the relationship between modeled latent factors and antibodies, the potential of having a high-throughput bioinformatics-driven method for antibody discovery creates countless potential avenues for future applications. the primary benefit of this methodological approach is to reduce the time it takes to identify antibodies associated with various clinical situations. doing so will reduce cost and increase the speed of advancement in biomedicine. additionally, the reduced cost and speed may open doors that were beyond the realm of consideration just a short time ago. for example, a method for quickly and inexpensively detecting an antibody could play a crucial first step in developing personalized vaccines.

below we present a multi-step procedure for detecting latent factors and potentially antibodies in an immunosignaturing study. the first step is to run an exploratory factor analysis on the data with rotation. various rotations can be explored but promax or geomin are recommended. efa models are a useful starting place for multiple reasons. first, it ensures that the groups are different constructs and significantly different from one another. this determination can be made by looking at scree plots and eigenvalues to assess the probable number of groups in the model; which should be equal to the number of known disease states. the samples should load correctly on a given factor with a high classification rate.

at this point, cross loadings in an efa model can be investigated. if clinical data exists, it would be of use to try to assess if there are potential reasons for why a specific sample may be cross loading. for example, is there a history of cancer in a normal sample that cross loads on a cancer sample which might suggest the person is in a transition phase? this may be a way of detecting aberrant cases or outliers. that said, haphazardly removing cases from a dataset is not advocated in any fashion. cross loadings were not analyzed in this paper due to a lack of additional information and clinical data upon which to draw any relevant conclusions.

from here, an appropriate test statistic comparing the groups can be run on the peptides in order to test for statistical significance. t-tests or logistic regression and their multivariate extensions anova and multinomial logistic regression are a few potential methodological tools. the specific test should be picked with respect to the features of the data being analyzed. for example, in this paper, we used a satterthwaite corrected t-test because of unequal samples sizes and variances. a correction should be made to protect against alpha inflation. although a number of tests exist for this purpose, the bonferroni correction is among the most common; even if it may be somewhat conservative.

a traditional efa model or an exploratory factor mixture model can be used to infer the structure of the significant peptides within each group. this information can be used to create a full structural equation model. however, as part of good model building practices, starting with a cfa measurement model is recommended; especially because the iterative nature and complexity of these models may lead to convergence problems. additionally, information from these simpler models can be used to specify starting values in full sem models if convergence problems occur. cfa measurement models specify which peptides load together on a given latent factor. checking the fit of the measurement models can confirm the accuracy of the model. however, given that cfa is so similar to efa methods, it is unlikely that differing results would be obtained.

once a working measurement model has been obtained, a full sem model can be created by regressing the latent factors on disease state. it is important to test a full sem model for a number of reasons. although efa and cfa models may suggest that a group of significant peptides are related in some way, without a full sem model, there is no way of knowing whether the relationship is a significant predictor of a specific disease state. in the absence of predictive validity for a specified disease state, any relationship among the peptides is trivial and would not suggest that it is because of a common antibody. the same conclusion can made if the latent factor is predictive of disease states beyond the hypothesized state.

if a significant sem model can be obtained, wet lab validation can then attempt to determine if the model is correct. one potential way of testing this in the wet lab would be to use the designated peptides to affinity purify the antibody from the sera. the prediction is that the different peptides would purify the same antibody. this could be tested by immunosignaturing the antibodies purified.

screening and presymptomatic screening for disease
the relative ease from which samples can be classified and differentiated with all of the methods explored herein makes this technology an excellent use for disease screening. whether examining the loadings of new samples in a cfa model or as part of a larger sem model, this technology can allow researchers to screen patients in a variety of contexts. this initial research suggests that immunosignaturing could be developed into a quick and inexpensive method of screening for cancer. taking a small sera sample from an individual is much less expensive and intrusive than traditional screening methods such as mammograms. one early potential use for immunosignaturing would be to help follow at risk populations; such as those individuals with a family history of cancer. immunosignatures could be taken at regular intervals between regularly scheduled mammograms. if the generated immunosignature from an interim test started to suggest a closer similarity to cancer, this could prompt physicians to follow the patient more closely or advise additional screening. immunosignatures could be used in the same way for individuals who already have cancer. in this case, if an immunosignature suggested the person was developing an antibody signature indicative of a second tumor , the individual could be followed more closely to detect the presence of a second primary tumor.

screening for a specific disease state is fairly straightforward. a well curated collection of disease samples would form baseline control factors. a sera sample would be taken from an individual and their sample would be allowed to freely load in a cfa model across relevant disease conditions. a significant loading on a disease factor would provide strong evidence for the person having a given disease.

there are a number of ways in which a presymptomatic screening test could be developed from immunosignatures. this could be done by collecting a longitudinal or time series sample of sera from an individual and following the factor loadings on a disease state over time. as the loadings on a disease factor tend to increase the individual could be watched more closely and additional screening for a disease could be recommended by a physician. a number of statistical methods and time series analyses such as latent transition analysis  could be employed to model this.

CONCLUSIONS
immunosignaturing is a novel approach for understanding disease. a number of statistical methods including, exploratory factor analysis, confirmatory factor analysis, descriptive statistics, corrected t-tests, anova, logistic and multinomial logistic regression, mixture models and structural equation modeling have shown promising abilities for analyzing different dimensions of immunosignatures. immunosignaturing in the context of breast cancer has been shown to be a good platform for differentiating groups of samples based on disease status, determining the disease status of specific samples as well as potentially serving a role in the discovery of antibodies for specific diseases.

despite many new challenges posed by immunosignaturing microarrays such as competitive binding and binding to multiple sites, the analyses conducted here clearly illustrate the usefulness of classical analytical methods to produce accurate results. the results are particularly noteworthy because of the lack of structure in the data and lack of a full pattern matched experimental design. the early results of structural equation modeling are very promising. although wet lab validation is needed for the proposed methodology of antibody discovery, even if the latent factors turn out not to be a specific antibody, the model can still serve as an excellent biosignatures for disease screening.

early detection of cancer is among the best predictors of survival. continued development of immunosignaturing into a screening and presymptomatic screening diagnostic tool will aid in early discovery and help turn the corner in the fight against cancer. future research in this field should aim at validating the hypothesis that the latent factors modeled here are in fact antibodies and to develop the technology into a diagnostic screening tool.

competing interests
the authors declare that they have no competing interests.

authors' contributions
jb helped with design of methods, statistical analyses and drafting of the document. ps helped with collection of biological samples, design of methods, statistical analyses and drafting of document. sj helped with collection of biological samples, design of methods and drafting of document. vd helped with design of methods, statistical analyses and drafting of document. all authors read and reviewed the final manuscript.

acknowledgements and sources of funding
thanks to patricia carrigan, research professor, center for innovations in medicine, biodesign institute and hoda anton-culver, chair dept. of epidemiology, school of medicine, university of california, irvine.

justin r. brown - science foundation arizona graduate research fellowship. this source of funding had no role in the design of the research or publication decisions.

ethical approval and informed consent
consent was obtained for every sample in this manuscript and was approved by asu irb according to protocol number  <dig> entitled "profiling human sera for unique antibody signatures". humans were consented by the retrieving institution and a materials transfer agreement was signed between the biodesign institute and the collaborating institute. the collaborating institutes' protocols were current and each human subject signed an approved consent form and released their sera.
