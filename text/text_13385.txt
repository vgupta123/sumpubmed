BACKGROUND
one major challenge in proteomics is the identification of proteins within a specific experimental context. the methods employed in these fields are numerous. although multi-dimensional liquid chromatography  methods coupled to mass spectrometry  are advancing  <cit> , two-dimensional gel electrophoresis combined with ms is still a major method for proteome analysis  <cit> .

ms is currently the tool of choice for peptide and protein identification  <cit> . for this, a bottom-up strategy is most widely employed in ms  <cit> . using this method, proteins are first cleaved into peptides by a protease . these peptides are then analyzed using ms or tandem ms. the resulting tandem mass spectra are typically submitted to computational analysis by algorithms which correlate spectra to entries in multiple amino acid sequence databases. although there are numerous software tools which can perform this mapping, the two most widespread are sequest  <cit>  and mascot  <cit>  which currently represent the industry standard  <cit> . the results of this analysis are amino acid sequences which have been successfully mapped to ms/ms spectra.

the set of resulting peptides from this analysis can be used to identify proteins. a protein with two or more supporting peptides is widely accepted as a confident identification  <cit> . a protein with a single supporting peptide can be accepted as a confident identification when de novo amino acid sequencing and correlation analysis together give supportive evidence  <cit> .

as can be deduced from the simplified view of proteomics above, data from proteomics experiments are extremely heterogeneous. the challenge in proteomics is to integrate all this data into one data warehouse enabling searches and creating relationships across different topics. a part of this enormous task is addressed in this paper. our initial interest was the identification of proteins from experiments which can be represented as pictures containing specific areas of interest  which were examined by ms/ms with subsequent computational analysis. there is, however, no limitation imposed by this and experiments do not need pictorial representation although it enhances their presentation and usability.

it is important to connect areas of interest in a picture  to results from subsequent analysis. to achieve this, it is necessary to define these spots, and for this purpose a software tool which is integrated into the 2db application  is provided, directly allowing definition of areas on the picture while enabling the specification of additional information for each.

the bottom-up strategy employed in mass spectrometry today presents one problem which calls for the use of a database to represent the information gained in this type of experiment. since multiple databases are usually queried for the existence of an amino acid sequence that best explains an ms/ms spectrum, possibly using multiple software tools, the first task that needs to be achieved is the assurance that each mass spectrum is only represented by its best identification while making sure that it is also significantly better than the second best identification, if any. furthermore, it is necessary to evaluate peptides based on the scores from different software tools; see shadforth  <dig> and kapp  <dig> for a review of the vast amount of available software  <cit> . this was achieved by allowing user defined thresholds for any potential software applications which can evaluate tandem mass spectra.

peptides identified by distinct mass spectrometric identification algorithms have to be matched to possible amino acid sequences in order to discover putative proteins. in order to allow this assessment protein sequences need to be stored in the database. any protein sequence available in fasta format  <cit>  can be imported into 2db, which then builds the basis for automatic protein discovery from peptides. interconnection to other databases, as stipulated for federated databases in an article involving swiss-2dpage  <cit> , has been adhered to in the implementation presented here. our implementation does not aim to compete with swiss-2dpage, rather, we aim to provide a solution which can be used to display all lines of evidence which led to protein identification, enabling the user to evaluate the result quality. protein identification can, then, in the spirit of a federated database, be made available on the web and linked to, as well as searched from, swiss-2dpage in the future.

implementation
there are currently two approaches that would allow for efficient representation of the wealth of information from proteomics experiments. one is to represent the data in extensible markup language   <cit>  format and then use native xml database tools to access the information. it is questionable how this information, which is probably extremely heterogeneous, can be evaluated without extensive standardization approaches. furthermore, there seems to be a need to rerun all ms/ms mapping algorithms and rebuild the database if the underlying protein models change. another approach, used in this study, is to model the expected data to a relational database. this was done using a mixture of strict and loose typing in order to allow for the import of data from various sources, especially in regards to the results from computational identification of ms/ms spectra.

the database schema is currently specifically designed for the mysql  <cit>  database management system  since it is powerful and free of charge. furthermore, most web space providers support mysql even in their basic packages. however, throughout the code plain structured query language   <cit>  was used which enables a smooth migration to other or multiple dbms in the future. most code has been written in hypertext preprocessor   <cit>  which is more widespread than mysql and available on virtually any server on the internet. three core applets were programmed in java  <cit> , a programming language executable on almost any computer having a recent version of the java virtual machine installed. the interactive web pages have been designed in hypertext markup language   <cit> , javascript, and cascading style sheets   <cit> , the overall package should, therefore, work on most servers that are commercially available and on those provided in educational institutions.

the application is divided in four logical areas: administration, input, access and output. the database is accessed in a three tier mode. html forms build the front-end while php scripts provide the middleware which retrieves the information from the database and then displays the result to the user. access control is based on users, groups, memberships to the groups, and rights. login information is stored in encrypted cookies, which provides an adequate degree of security considering the sensitivity of the stored data.

the package can be downloaded as a single compressed file. after decompression and uploading to the server, a wizard style installation script will ensure that the application will be properly installed on the server. thereafter, some customization is still necessary, such as importing possible amino acid sequences, entering users, and adding at least one organism before the application can work properly with imported data sets.

RESULTS
many databases that host proteomics data have been proposed, most of which are tailored to a specific purpose, organism, lab, or more generally, an environment  <cit> . this is also partly true for our implementation. it is directed towards proteomics experiments including single or multiple separation steps, followed by mass spectrometry and computational analysis of the ms/ms data. this includes, for example, 1d gels, 2d gels, and multidimensional protein identification technology   <cit> . within this context, however, we aim to represent all necessary lines of evidence leading to distinct protein identifications.

the 2db application consists of four distinct areas which will be discussed in the following individual sections.

installation and administration
the first criterion for any software is that it needs to provide an easy-to-use installation procedure. most current applications, however, expect the user to manually edit scripts, thus creating a large obstacle seriously limiting the software's potential for use. we overcame this by completely hiding this process from the user, only providing a wizard like interface to collect the necessary data for the installation of 2db on a server.

following installation, the administration section of 2db should be the first stop. usually a number of groups and users modelling differential access to the data should be created, which can be achieved in the groups and user-profile areas of the administration area of the user interface of 2db . the next important step is to enter the thresholds for the software tools used for peptide identification from ms/ms spectra which will provide the input to any instance of 2db. this enables the database application to filter input data according to these thresholds.

after successful installation some initial customization is required for the efficient use of the database application. this includes the import of amino acid sequences  specific to the research organism.

import and data description
any database application in proteomics should allow for batch import of data. however, other database applications, such as swiss-2dpage  <cit>  and phproteomicdb  <cit> , however, expect manual data entry for modelling protein identifications. even though the amount of data needing to be entered is significantly lower than the amount of data which would need to be entered into 2db, it is still inconvenient. therefore, we provide complete automatic import of peptide identifications. currently, we support the import of ms/ms identifications from sequest  <cit>  out, html, and xml files. as well as from x!tandem  <cit>  and omssa  <cit>  in csv and xml format. pepxml  <cit>  files can also be imported in addition to automs ams files . these formats can automatically be converted for upload to 2db, using a special data format described in the manual . this format can also be used to import data from various other sources not yet specifically supported. it consists of only  <dig> data points in plain text format, some of which may be left empty. adhering to this format ensures that the correct data items are grouped and related automatically.

in order to enable automatic protein discovery from this data, potential protein sequences have to be available in the database.

as mentioned earlier, the results are first filtered by user thresholds stored in the database. if the thresholds are not passed, the results are discarded. if the thresholds are passed, the result is stored in the database. because different algorithms may perform the analysis and different sequence databases may be used, a mass spectrum can be mapped to different possible peptide sequences. this would lead to the undesirable effect of multiple sequences for a single ms/ms spectrum, and can be avoided by using cosine similarity to determine which sequence prediction is closer to the experimental spectrum. this assessment, while seeming crude, is deemed to be sufficient since results have been pre-filtered by other algorithms. in cases where the scores are identical, both results are discarded; if one scores better than the other, the lower score is discarded. we chose to keep the database free of the results that fail to pass the thresholds or that conflict, since the data inflation caused by this may lead to slow response times of the database management system as was experienced in preliminary trials.

we created a wizard-like interface able to import sequences in fasta format. if the header  information in the fasta file contains protein identifiers that can be used to link to protein details, this information can be used to establish a link from the sequence in the database to the origin of the protein . this is also possible if the underlying sequences come from multiple sources.

each peptide imported after computational analysis of mass spectrometric experiments is automatically compared to the sequences stored in the database. if a match can be established, a link is formed in the database. subsequently, a check is made to see whether, according to the confidence thresholds for protein identification, proteins can be identified from the relationships between the number of peptides identified in respect to a specific protein.

many current databases fail to respond to changes in the underlying protein models since they are built on static models such as xml or html files. in the case of 2db, the peptide protein link is established whenever changes in either the protein models or the stored identifications are made.

the data imported into the database can be complemented with visual information; it is possible to upload a picture of a gel, blot, or similar item and correlate it to the imported data via hot spots on the image. these hotspots can be defined using a java applet included in the 2db package . in addition to this, information concerning the experiment, such as methods employed and the overall aim of the study, can be entered. while considering this approach sufficient, we recognize the importance of standards, and intend to integrate support for hupo's psi-gel  <cit>  standard, which will describe, for example, spots on a gel, upon publication.

one of the most important databases representing results from proteomics experiments is swiss-2dpage. it represents the identified proteins which must be established prior to constructing the 2d reference map. in our application proteins are only presented by sparse information and a hyperlink to their origin, whereas swiss-2dpage gives additional information for each protein. while swiss-2dpage only links ms/ms datasheets statically, our implementation actively uses ms/ms search results to discover proteins , which can subsequently be connected to areas on an image resource using a tool provided with the database . our database can be accessed in the spirit of a federated database, as described in a publication involving swiss-2dpage  <cit> . this means that proteins can be searched for and their location identified as well  as it is possible to directly link to any publicly available section of this package.

although data generated by proteomics experiments is vast and heterogeneous, and calls for a distributed representation, there is at least one approach which attempts to capture a large fraction of the information: this approach, called pedrodb  <cit> , is based on data contained in xml files. this data can be queried with native xml database management systems such as xindice  <cit>  which was used in the above approach. generally large data inflation is caused by capturing all available information, such as the parameters of the mass spectrometer over time, even more so if it involves xml which amends each data point with a data description , sometimes more than doubling the size of the actual data. in certain environments it would be crucial to retain this information; however, for high throughput proteomics studies, some of this information may not be required in the context of peptide and protein identification. as a side effect, the additional information needs to be entered into the database by the experimenter. in order to reduce the workload on researchers as much as possible, we decided to take a lean approach representing all lines of evidence for each protein identification in a consistent manner while retaining only sparse evidence in raw format, such as mass spectra, which are an important means for assessing the identification quality.

pedro, intended as a model for capturing all data in gel-based proteomics, has been adopted by proteomics applications, for example maspectras, and forms the basis of its data model  <cit> , which is similar to other laboratory information systems  <cit> . recently the proteomics standard initiative formalized the minimum information necessary for a proteomics experiment  <cit>  which now supersedes the pedro model.

2db spans several parts of the miape standard such as the gel electrophoresis, the mass spectrometry and the capillary electrophoresis modules.

in the 2db database model, we tried to model information as abstract as possible so that even unexpected data can be captured: the scores provided for the peptide assignment to an ms/ms spectrum, for example, can be stored irrespective of the software used for the analysis and irrespective of their type or range.

we acknowledge the need for standardization and we are working towards compliance. at this point, 2db is mainly designed to discover information from a subset of the captured data, but it also provides functionality to capture a large amount of meta data which can be tailored to compliance with standards upon their publication.

automatic protein detection
peptides that are confidently identified from mass spectrometric data using established algorithms are related to protein sequences present in the database. currently, it is possible to specify thresholds for the number of peptides that support a protein found in a given selection. this, for example, enables distinct views of the results with different levels of confidence. it is also possible to put more weight on peptides which have been identified by trusted applications. combinations of both thresholds are also supported. in the future, we will add support for discrimination using the number of supporting spectra per identified peptide and the number of algorithms supporting the identification per spectrum.

a fraction which will be analyzed for identifiable proteins is currently set to be a subset of the experiment . peptides found in that fraction are pooled and mapped to the sequences in the database. the pooling of specific fractions or experiments is still at an experimental stage, but will be supported in the future. thus, the results of multiple biological samples can then be pooled.

the analysis is done on the fly, so by adding new data to the database the confidently identified proteins are updated if affected by the addition.

access control and search facility
one aim of the software developed in this study is to allow concurrent storage, analysis, and publication of data bundled in one application. therefore, a strict user control, achieved by introducing group level access rights, is essential. thus it is possible to limit the access to data to any number of groups defined in the group section of the database. two groups, administrators and guests, are initially set-up, although other groups can be added by administrators at any time. we currently have four groups, administrators, researchers, collaborators, and guests. administrators can change and modify all parts of the database, while on the next level, the researchers from the host lab are only able to enter and retrieve data. collaborators from other research groups are on the next lower level of access rights; they are only able to access data provided from the host specifically to them. lastly, guest users can access all information which has been published, and is therefore in the public domain.

given that the information sought for is actually accessible, the question remains as to how to retrieve information to which a user is entitled, according to the access rights previously described. we provide a basic search facility which can aid in finding information. specific questions that can be asked are, for example, protein ids which are dependent on the sequences present in the database. we use transcript numbers provided by the joint genome institute  <cit>  as protein identifiers in the case of our working instance of the database, namely wwupepprotdb <cit> .

the database has the advantage that, when a particular protein is searched for, its location is revealed in every experiment in which it was detected. this offers the possibility of finding differential expression of a protein in related experiments. other search modes include sequence search, which not only retrieves proteins but also peptides, and searching for names of experiments or organisms. these are performed as a free text search which returns results from all areas of interest.

other databases such as prodb  <cit>  and uab  <cit>  also provide the possibility for browsing locally produced data in a similar way to 2db, although, there is a separation between the data presented to the research community and the data which is used in-house for data evaluation. 2db offers the possibility of hosting both, the sensitive in-house information and the published data on the same server. it is sufficient to change the access rights in one location in order to open or close public access to an experiment in the database.

data views and output formats
the main entry points for accessing information on the 2db platform are the search facility and the experiment gallery sections. all experiments stored in the database can be accessed via the experiment gallery. access rights are indicated by a green box  or a red frame . clicking on one of the previews, if available, brings up the experiments details page which includes all information about the experiment as provided by the experimenter and a larger view of the image representing the experiment, if available. this picture can serve as an image map and can lead to more detailed information about proteins identified for areas of the image . following such a link will display all proteins on top of the result page that were confidently identified from the recorded mass spectra for that spot. this is followed by proteins which lack enough supporting peptides, and thus do not pass the significance thresholds. finally, peptides which cannot be mapped to protein sequences in the database are listed. the information is presented in such a way that details need to be specifically accessed. on the first level, the identified proteins are displayed. enabling the second layer will provide all supporting peptides for a particular protein. each supporting peptide may be identified by a single, or multiple spectra which can be seen on the following level. each supporting spectrum can be viewed in context with the theoretical fragmentation pattern of the predicted amino acid sequence. furthermore, each spectrum, which led to the same proposed peptide, comes with the scores given by each software tool used for its identification and meta-data  can be generated and downloaded as a table. an extract of an experiment is given as an example  which can be downloaded from the experiment overview. this table contains information about the protein identifications from an experiment, such as the number of spectra and peptides required for confident protein identification.

this table shows a section of meta-data about identifications we were able to make in an experiment. the exported data contains only meta-information about the identified proteins. this information includes the protein identifier, the number of supporting peptides, the sequence coverage and other information. files with comma separated values  can be imported in basically all spreadsheet calculation software such as microsoft excel, systat's sigmaplot and openoffice's calc.

other tools that are available but subject to on going development are, for example, the calculation of differential protein expression in different experiments stored in the database. this calculation is currently based on spectral and peptide count only.

links to other databases are provided through the name of the protein or by plug-ins, which can be used to send the underlying protein sequence to different websites in order to retrieve further information. plug-ins are extremely easy to define, although, the number presently available seems sufficient at this point. other implementations such as proteomeweb  <cit>  provide functionality, for example the calculation of theoretical maps, not currently implemented in 2db due to lack of manpower.

there are more than  <dig> experiments in our local instance of the 2db application  <cit>  four of which are publicly accessible and which were recently published in proteomics  <cit> .

CONCLUSIONS
many databases have been tailored to specific purposes in the past. here we present a database application which can be employed in many contexts, regardless of the software framework currently used. although the database application developed in this study can represent identified proteins in their experimental context, much like swiss-2dpage, it can also store selected mass spectrometric raw data to provide all necessary lines of evidence to back up each of the protein identifications. a mixture of loose and strict typing within a relational database context makes it possible to integrate data from multiple sources which means that virtually any software for identification of ms/ms spectra can be used. furthermore, the move away from a static model to a more dynamic model fits changes in underlying protein models very well. in summary, the database greatly facilitates proteomic data handling as well as analysis from high throughput experiments.

outlook
while the database is fully functional, there are some aspects that can be improved upon as can be seen in one instance wwupepprotdb  <cit> . for example, at present, contradicting information given by different algorithms is resolved via cosine similarity, whereas other algorithms may be more suitable for this purpose. this implementation is at present limited in regards to input formats which can be handled by the file upload software. we will, however, implement converters and import facilities for standards as they become available.

availability and requirements
• the project sources and installation script can be downloaded from our webpage  <cit> . help is available on the google group  <cit> .

• experiments serving as a sample preview in the context of this paper are presented at the web on wwupepprotdb  <cit> .

• operating system: the project was developed on a unix server, but should generally not be sensitive to a specific operating system.

• browser: the database is only accessed via a web browser. it has been tested with internet explorer  <dig> and  <dig>  firefox  <dig> , safari  <dig> , and opera  <dig> . in internet explorer  <dig>  privacy and security settings need to be adjusted to be able to log onto the system.

• implementation details: php was used in combination with html, css, and javascript for most parts of the user interface. java was used for extensive user interaction modules. the database was developed using sql, but was only tested using the mysql database management system.

• requirements: any web server able to host mysql  <dig>  or greater and php  <dig> or greater. we used several versions of apache. jvm  <dig>  or greater, preferably from sun microsystems needs to be installed on the client computers. java, javascript, and cookies need to be enabled in the web browser.

• the server can be run locally on a pc as well. we achieved the best results using wampserver  <dig>  <cit> .

• license: gnu general public license

authors' contributions
ja designed the database schema and programmed significant parts of the scripts. he also wrote the first draft of this publication. sk designed the user interface and programmed significant parts of the scripts. he proofed all versions of this publication. mh supervised and provided constant critiques and insights throughout the development of the application. he significantly revised and enhanced the first draft of this publication.

supplementary material
additional file 1
all files needed to run and further develop the database application as well as the user manual have been bundled into one zip file which can be downloaded from biomedcentral here. due to constant upgrading of the system, it may be beneficial to check for the latest version on our website  <cit> . all the sources and additional installation files.

click here for file

 additional file 2
all files needed to run and further develop the database application as well as the user manual have been bundled into one zip file which can be downloaded from biomedcentral here. due to constant upgrading of the system, it may be beneficial to check for the latest version on our website  <cit> . the user manual for both installation and usage.

click here for file

 acknowledgements
we would like to thank simon mumford who edited the final version of this publication and rainer perske who promptly fixed all problems in our database and web development environment.
