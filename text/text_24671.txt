BACKGROUND
high-throughput technologies, such as dna microarray  <cit>  and rna-seq   <cit> , have made it possible to perform genome-wide profiling of various genomic features, such as, genes, transcripts, exons, dna modifications, and so on. these technologies have been widely adopted to detect genomic features  that are differentially expressed between different conditions . when analyzing genome-wide datasets to detect differentially expressed features , it is important to control the overall false positive rate because thousands of hypotheses are tested simultaneously. controlling the false discovery rate  was first introduced by benjamini and hochberg  <cit>  to large-scale testing problems and has been broadly applied in detecting defs since then. the benjamini-hochberg  approach takes the p-values of all hypothesis tests and uses a sequential method to estimate the rejection region . more recently, researchers formulated fdr estimation in a bayesian fashion , which assumes the distribution of the statistic as a density mixed of nulls and alternatives. the bayesian approaches can be implemented non-parametrically using the test statistics directly rather than their p-values. the calculation of test statistics  can be deemed as a mapping from the original high-dimensional observations to a single index value per feature. the ordinary t-test  <cit>  was one of the most popular mappings for detecting differential expressions measured by dna microarrays. the t-test assumes normality in the target data and can be prone to outliers. in addition, its variance estimation is feature specific and is impacted by great variability when only few samples/replicates are available in dna microarray experiments. to deal with this problem, various versions of moderated t-statistics  were developed to utilize information across features for regularizing variance estimation.

statistics based on the poisson distribution  <cit>  or the negative binomial  distribution  were later proposed specifically for detecting defs using rna-seq data. different from typical dna microarray approaches that rely on hybridization to measure the expression levels of features as continuous values, rna-seq approaches use deep sequencing to produce millions of short reads corresponding to those features. the reads are then mapped onto a reference genome, which makes poisson a natural representation of read counts. it was shown that the poisson distribution was able to effectively characterize technical replicates in rna-seq experiments  <cit> . however, the poisson distribution forced the mean and variance to be the same and predicts a smaller variance than what was observed in biological replicates  <cit> . to deal with this so-called over-dispersion problem, poissonseq  <cit>  applied a power transformation to make the data distribution look more like poisson. auer  <cit>  proposed a two-stage poisson model  to handle features with significant over-dispersion evidence by a quasi-likelihood approach  <cit> . in the meantime, the nb distribution was proposed as an alternative  <cit>  and has been gaining momentum in analyzing rna-seq data. compared to the poisson distribution, the nb distribution allows the modeling of a more general mean-variance relation by taking another dispersion parameter. several nb-based approaches, such as deseq  <cit> , deseq <dig>  <cit> , edger  <cit> , nbpseq  <cit> , ebseq  <cit> , bayseq  <cit> , shrinkseq  <cit> , and so on, have been developed, and they mainly differ in their ways of modeling and estimating the dispersion parameter.

recently, it was demonstrated that the moderated t-statistic, when combined with appropriate data preprocessing methods, could be powerful for detecting defs using rna-seq data. for example, voom  <cit>  extended limma  <cit> , which uses the moderated t-statistic in a pipeline well-established for analyzing dna microarray data, for differential expression analysis using rna-seq data. voom applies a logarithmic transforms to read-counts normalized by the corresponding library size, estimates the mean-variance relationship non-parametrically from the transformed data, uses the estimated relationship to generate a precision weight for each normalized observation, and finally enters them into the limma empirical bayes analysis pipeline for detecting defs. in another example, vst/limma  <cit>  applied the variance-stabilizing transformation  of deseq to rna-seq data before using limma to calculate the moderated t-statistic.

the above test statistics can be viewed as attributes extracted from data to characterize the observed differential expression patterns. most existing attribute extraction methods make specific assumptions about data distributions , and then calculate a statistic  for each feature. although those test statistics are efficient in preserving differential expression information up to certain levels, they leave plenty of room for further improvements. in real applications, the profiles of individual features in the same dataset can be governed by complex distributions, and hence may not be well represented by the assumed distribution  <cit> . we made a similar observation that the distributions could indeed be far more complex than those often assumed . individual attributes based on relatively simple distribution assumptions will have limited capacity in characterizing complex differential expression patterns, and hence can greatly affect def detection results. in theory, we can explicitly make every differential expression test follow a common family of distributions by designing a complex distribution form  to approximate all complex distributions in data. such a complex distribution will have unknown parameters that can be estimated from data by applying the same procedure to all features. however, it can be challenging to design not only a statistic for testing differential expressions based on such a complex distribution but also a parametric def detection approach that uses this test statistic.

there are non-parametric approaches that do not assume data distribution, such as, samseq  <cit>  and noiseq  <cit> . samseq utilizes the ranksum test statistic  <cit>  to characterize differential expressions and uses resampling to adjust for different library sizes. although the ranksum test does not assume any data distribution and is less likely to be affected by outliers, it can sometimes be considerably less capable of preserving information. noiseq uses two simple attributes , and estimates the null as the joint distribution of these two attributes from replicates , which is then used to calculate the odds of an observed statistic pair indicating differential expression. nevertheless, noiseq does not directly estimate fdr. in addition, log fold-changes and absolute expression differences can be prone to outliers and are not powerful enough for characterizing complex differential expression patterns. however, noiseq motivated us to investigate better ways for integrating multiple attributes to detect defs while controlling the fdr.

in this paper, we call the above attributes “basic” because of their relatively simple forms and limited capacity in preserving differential expression information. most of the existing def detection methods rely on one single basic attribute in each analysis run, which can greatly restrict their detection power. since different basic attributes may capture distinct aspects of differential expression patterns, we anticipate that defs can be better differentiated from non-defs using multiple basic attributes, which may be extracted from data using existing tools, such as, deseq <dig>  voom, limma, and so on. this work addresses a general application scenario, in which users set a target fdr and ask a method to detect as many defs as possible. this can be formulated as a constrained optimization problem that tries to learn an optimal decision boundary in a space of multiple basic attributes to differentiate defs from non-defs. an algorithm discriminant-cut has been developed to explore the linear decision boundary family. extensive tests were conducted to test discriminant-cut and compare it with several popular def detection methods. the results demonstrate that it is significantly advantageous to combine multiple basic attributes in detecting defs.

methods
def detection as learning multi-dimensional decision boundary
let g = {g
ij}i = 1 … m,j = 1 … n contain the values of m features in n samples, in which g
ij is the value of the i-th feature in j-th sample. without loss of generality, we assume that samples are randomly selected from a population with two different conditions. let y = {y
j}j =  <dig> …,n, where y
j be the binary condition label of the j-th sample. the goal is to detect features that are differentially expressed between these two conditions. we propose to treat def detection as finding a discriminant function h that specifies the decision boundary between defs and non-defs. let d
i = h be the discriminant value of the i-th feature. the i-th feature is called a def if d
i >  <dig>  the unknown parameters of h should be learned from x = <g, y>. it can be challenging to design a proper h in a top-down way and learn such a function. to circumvent this problem, we can take advantage of previous research achievements in designing and calculating various statistics for testing differential expression . we let h ≜ f, s
i, …, s
i) where s
i, s
i, …, s
i are k different basic attributes  of the i-th feature. this design can be considered as a two-layer data summarization mapping with calculating the basic attributes as the first layer and f as the second layer. the function f should be much less complex than h, and its unknown parameters can be estimated from x more easily. our approach can be geometrically interpreted as treating each feature as a point in the multi-dimensional space of those k basic attributes, and learning f from a given dataset to specify a decision boundary between defs and non-defs in that space. each basic attribute provides a certain point-of-view about being differentially expressed, which is then integrated by f to produce a more comprehensive view. we leave the detailed specification of f to implementation and focus on explaining the idea for now. it will be shown later in our experiments that simple instantiations of f, such as linear functions, can deliver superior performance.

as simple as it sounds, it is in fact quite significant and innovative to explicitly model def detection as learning a decision boundary in a multi-dimensional space. conventional def detection approaches use top-down approaches to design single attributes to characterize differential expression information, and then find decision points in one-dimensional spaces. to accurately deal with complex differential expression patterns in the traditional way, we need to design a complex data distribution and a corresponding statistic for testing differential expression, which can be challenging and often requires performing less tractable computations. our approach is much more simple and practical, and offers a straightforward geometrical interpretation. our novel formulation of def detection opens up a new avenue to advance def detection research by incorporating decision boundary modeling and learning techniques developed in machine learning community. learning f from x is an unsupervised task because no feature is labeled as def or non-def in x. as far as we know, this kind of unsupervised learning problem  has not captured major attentions in machine learning research.

maximizing def detection by constrained optimization
let dxf=di=fsi1si2…siki=1…m be the discriminant value set including the discriminant values of all m features in x. we want to learn f from data so that the number of the detected defs is maximized while the fdr is under controlled by a user-defined threshold Ψ. given a dataset x and a fixed discriminant function f, the def set is indicated as  <dig> Γxf=idi> <dig> di∈dxf 


let fdr denote the corresponding fdr of Γ. this problem of learning f from x to maximize the size of Γ subject to the fdr constraint can be mathematically written as:  <dig> maxf⋅Γxfsatisfyfdrxf<Ψ 


our approach is different from the optimal discovery procedure   <cit>  that tries to optimally capture common differential expression patterns shared among detected defs by rigorously exploring the relevant information across features to rank their significance of being differentially expressed. the current setup of odp only allows one kind of hypothesis test for all features in each analysis run. our approach tries to capture differential expression information of individual features as much as possible by scrutinizing their expression profiles from multiple “view angles” . we aim to maximize the number of detected defs at a given fdr level. it is possible that different numbers of defs can have the same fdr. it can be beneficial to treat up- and down-regulation asymmetrically  because the induced and suppressed features may exhibit different up- and down-tail characteristics in the joint distribution of basic attributes . equation  and the following derivations are general and can be applied to detect both up- and down-regulated features. before we introduce the algorithm to find the parameters of f by trying to solve eq. , we explain how to estimate fdr in the following.fig.  <dig> the up- and down-regulation tails may show different characteristics in the joint distribution of two basic attributes. two typical examples are shown here. a a rna-seq dataset gse <dig> : y-axis – the moderated t-statistic from voom; x-axis – the wald statistic of the nb-based differential expression test from deseq <dig>  b a dna methylation dataset gse <dig> : y-axis – the moderated t-statistic from limma; x-axis – the moderated t-statistic from voom. gse <dig> and gse <dig> were downloaded from the gene expression omnibus database 




fdr estimation
in practice, fdr in eq.  is unknown. to estimate the fdr of an arbitrary f, we implemented the storey framework  <cit>  in a non-parametric fashion  <cit> , which we briefly explain below for completeness. let the null hypothesis of a feature be that it is not a def. assuming there are m independent features. table  <dig> lists the possible results when simultaneously testing m features for calling defs using f, among which r is an observable variable indicating the number of defs detected by f and v is a hidden variable indicating the number of false defs detected by f. let d
f be the variable representing discriminant value calculated by f. we can write down the fdr according to  <cit>  as a function of f:  <dig> fdrf=evfrf|rf>0prf>0=pnull|df> <dig> rf>0⋅prf> <dig> 



equation  can be rewritten using the bayes rule as the following:  <dig> fdrf=pnull⋅pdf>0|null,rf>0pdf>0|rf>0⋅prf>0+pnull⋅pdf>0|null,rf=0pdf>0|rf>0⋅prf=0=pnull⋅pdf>0|nullpdf>0|rf> <dig> 


equation  utilizes the fact that p = 0) =  <dig> because no hypothesis is rejected when r =  <dig>  below we explain non-parametric methods for estimating p, p, and p > 0) given a dataset x and a fixed f.


estimate p. the term p is the probability of d
f >  <dig> when null is true. the distribution of d
f under null condition, depending on both the distributions of basic attributes and f, can be extremely complex. hence it may not feasible to determine this term in an analytical form. we therefore estimate p by adopting the non-parametric method developed in  <cit> , which allows us to better explore the structure of data distribution in a data-dependent manner. this method randomly permutes the original dataset b times to generate the null control, and estimates p as:  <dig> p^df>0|null=e^bdi,b*>0|di,b*∈dxb*fm where d
i,b* is the discriminant value of the i-th feature in the b-th  permutation x
b*. the function e^b⋅ uses all b permutated datasets to estimate the expected number of non-defs that are incorrectly classified as defs. a reasonable choice of e^b⋅ is the median/mean function.


estimate p. it is expected that p ⋅ m features are non-defs . below we use the p-value concept to explain how to estimate p from data although we do not need to estimate p-values. assuming that all features are independent, the p-values of the discriminant values of these p ⋅ m features should be uniformly distributed between  <dig> and  <dig>  therefore, for some chosen p-value cutoff λ ∈ , we should expect that there are  ⋅ p ⋅ m non-defs whose p-values are greater than λ. let d
λ denote the discriminant value whose p-value is λ. since it is possible for some true defs to have p-values greater than λ, it is expected that 1−λ⋅pnull⋅m≤di|di≤dλ,di∈dxf when m is large enough and λ is well-chosen. in practice, d
λ can be estimated as the value smaller than λ percentile of elements in the permutation set dxb*fb=1…b. we can hence have a conservative estimation of p as:  <dig> p^null=di|di≤dλ,di∈dxf1−λ⋅m 


we conservatively set λ = 50% and truncate p^null at  <dig> because a probability should never exceed  <dig> 


estimate p > 0). the probability p > 0) can be naturally estimated as:  <dig> p^df>0|rf>0=di|di> <dig> di∈dxf∨1m=rxf∨1m where rxf=di|di> <dig> di∈dxf is an observed value of the variable r given the dataset x, and r ∨ 1 = r if r >  <dig>  otherwise  <dig>  the term r ∨  <dig> prevents the estimated fdr from being undefined due to having  <dig> as the denominator. plugging eqs.  into eq. , we have the estimated fdr as:  <dig> fdr^λxf=di|di≤dλ,di∈dxf⋅e^bdi,b*>0|di,b*∈dxb*f1−λ⋅rxf∨1⋅m 


if the number of permutation is large enough, r ∨  <dig> will effectively set the estimated fdr as  <dig> when r =  <dig> because, on expectation, the discriminant values of the permuted data are less significant than those of the original data. thus we have e^bdi,b*>0|di,b*∈dxb*f/m≤didi> <dig> di∈dxf/m=rxf/m= <dig>  which makes fdr^λxf= <dig> 

discriminant-cut algorithm
as a simple start to implement eq. , we chose the discriminant function f from the linear function family fs1…sk=∑i=1kwisi−τ, subject to |∑i=1kwi|= <dig>  where {w
i} and τ are the unknown parameters of f to be learned from x. we further require w
i ≥  <dig> when detecting up-regulated defs and w
i ≤  <dig> when detecting down-regulated defs, which effectively make |∑i=1kwi|=∑i=1k|wi|= <dig> a l <dig> regularization that tends to yield sparse models. a simple algorithm, discriminant-cut , was designed and implemented to search for the “ideal” f*. dc performs an exhaustive search at an empirically decided resolution . the algorithm first populates a set of {w
i} candidates, and for each of them, tunes τ to detect as many defs as possible while keeping the estimated fdrs under controlled by a user-desired threshold Ψ. since both finding f* and estimating fdr using the same permutation set, it is possible that the final estimated fdr is biased. to address this, we referred the idea in  <cit> . after choosing f*, we calibrate its cutoff τ using another large independent permutation set, and then apply the recalibrated f* to identify defs. the efficiency of the search was greatly improved by sorting intermediate results to facilitate quick search, binary search, and avoiding unnecessary exploration . the algorithm runs fast in practice. in our experiments, most of the runtime was spent on computing basic attributes, and the remaining computations took almost negligible time.

there are approaches for linearly combining multiple attributes  from either dependent or independent datasets  . some of them mainly explore the covariance between attributes. some aim to minimize the p-values of individual features by allowing each feature to has its own combination setting. our approach does not make any assumption about the joint distribution of the attributes. we try to maximally explore differential expression information in one dataset, and force all features to share the same {w
i}. in addition, our objective function explicitly models the overall goal – maximize detections constrained by a target fdr. in the future, it may worth exploring how minimizing the p-values of individual features can benefit our goal.

RESULTS
rna-seq simulation test
we firstly carried out a series simulation tests, in which the ground truths were known to ensure proper comparison, to assess the advantages of combining multiple basic attributes by dc. we let dc use up to three representative basic attributes:  s
t – the moderated t-statistic from voom,  s
r – the corrected ranksum statistic from sam , and  s
nb – the wald statistic for nb-based differential expression test from deseq <dig>  this produced seven dc configurations: dct , dcr , dcnb , dct+r , dcr+nb , dct+nb , and dct+r+nb . we also compared dc with  <dig> other rna-seq differential expression analysis methods including bayseq, deseq, ebseq, edger, nbpseq, samseq, shrinkseq, tspm, voom, vst/limma, poissonseq, deseq <dig>  and odp.

simulation design
to make the simulation tests as realistic as possible, we simulated the test datasets based on a real rna-seq dataset – the montgomery dataset   <cit> , which contains the transcriptome of  <dig>  genes in  <dig> extended hapmap individuals of european descent. large number of samples in this dataset allows us to reveal that the distributions in real datasets can be indeed much more complex than often assumed. nevertheless, the number of replicates in each simulated dataset is much smaller and is within the range of common practice. we first removed genes with extremely low expression profiles . for each of the remaining  <dig>  genes, we decided whether its read counts could be better modeled by a nb distribution or a gaussian mixture model  in the following way. the nb and gmm distributions were estimated by using deseq <dig> implemented in r and the statistics toolbox of matlab r2013a, respectively. the most proper number of components in a gmm was decided based on the bayesian information criterion. the gmms of ~ <dig>  ~ <dig>  and ~6% genes contained  <dig>   <dig>  and  <dig> components, respectively. figure 2a–c show a few typical examples. then, for each gene, we calculated the correlation between the histograms of its read-counts and the corresponding fitted nb/gmm to decide which distribution was a better fit. the gmms were truncated at zero because read counts should be non-negative. the distributions of about  <dig>  and  <dig> % of genes can be better represented by gmm and nb , respectively. a simple experiment presented in fig. 2d caption validates that the distributions of many genes in this dataset are more complex than what assumed conventionally . our choice of examining correlation between the histogram of data and its fits was based on two considerations:  histogram is commonly used in practice to approximate distributions, and  correlation is a widely adopted distance metric. this method is mainly used to show that features have complex patterns of distributions rather than as a rigorous model selection method for determining the exact ratio of gmm to nb, such as the one  shown above. we consider it sufficient for choosing distributions, which roughly approximate the original ones, for generating data in the following simulation test.fig.  <dig> the montgomery dataset shows that real rna-seq datasets contain complex distributions. , , and  show the distributions of three genes as examples. the white bars represent the histogram of the original data. the solid-red and dashed-blue curves represent the distributions of the fitted gmm and nb, respectively. see main text for the details of fitting nb and gmm to the original data. the number of gaussian mixtures are  <dig>   <dig>  and  <dig> in , , and , respectively. gmm is better than nb at representing distributions with multiple modes.  y-axis: the correlation coefficients between the read-count distributions and the corresponding fitted gmm distribution. x-axis: the correlation coefficients between the read-count distributions and the corresponding fitted nb distribution. each dot represents a gene. the distribution of a gene’s read-counts is approximated by a histogram of  <dig> equal-size bins spanning the read-count value range. the colors of dots indicate the most proper numbers of components in a fitted gmm according to the bayesian information criterion: green , blue , and red  correspond to  <dig>   <dig> and  <dig> components, respectively. about  <dig> % of genes are above the diagonal line indicating their distributions are more gmm-like. the distributions of the remaining ~ <dig> % genes are more nb-like. to further investigate this observation, we calculated n
nbgmm as the number of genes whose advantages of their gmm fits over their nb fits are significant  if the distributions of all genes are nbs. if all genes are indeed governed by nbs, n
nbgmm should be close to the expected number that is  <dig>  ×  <dig>  ≈  <dig>  we sampled  <dig> datasets from the nb fit of each gene, each of which contain  <dig> samples. for each dataset, we fit a gmm and a nb, and calculated the difference between their fitting scores . the score differences across all datasets were collected to approximate the null distribution and calculate the p-value of the score difference between the gmm and nb fits to the original samples. we got n
nbgmm =  <dig> ,  <dig> of which have 2+ components in their gmms. hence we can deduce that the distributions of a substantial number of genes are not nb-like. in a similar way, we calculated n
gmmnb as the number of genes whose advantages of their nb fits over their gmm fits are significant  if the distributions of all genes are gmm. we obtained n
gmmnb =  <dig>  indicating that the distributions of a substantial number of genes are not gmm-like. putting the above together, we conclude that neither nb nor gmm dominates the distributions of genes in the montgomery dataset




in each simulation test run for comparing the chosen rna-seq differential expression analysis methods, we simulated n read-counts for every gene using the distribution  decided to be better in the above way, and randomly divided the simulated read-counts into two equal-size groups to obtain true non-defs. the simulation of a gene was repeated until its logarithmic fold-change was not larger than  <dig> σ
n, where σ
n is the standard deviation of the logarithmic fold-change between two n-sample groups randomly chosen from the montgomery dataset. the  <dig> σ
n fold-change threshold was chosen because we observed in the montgomery dataset that the expected number of fold-changes higher than  <dig> σ
n is below  <dig> . then we randomly made g
ba genes  as true defs in the following way. for each of the chosen genes, we multiplied or divided one of its groups by a factor uniformly sampled between  <dig>  and  <dig>  to provide a reasonable wide range of differences in expression. finally, all simulated values were rounded to their nearest integers.

a series of simulation test runs were conducted under  <dig> different settings:  <dig> different sample sizes  ×  <dig> different true def configurations . at each of the  <dig> simulation settings, we ran the test  <dig> times and recorded the results. our comparisons focus on two key performance factors:  the effectiveness of fdr control, namely whether the real fdr is effectively bounded by the target fdr; and  the detection power, namely the ability to detect as many true defs as possible without violating .

integrating multiple basic attributes helps substantially
comparing the results of different dc configurations shows that the advantage of integrating multiple basic attributes in detecting defs is significant. figure  <dig> shows that dct+r+nb consistently outperformed the three single-attribute dc configurations under all  <dig> simulation test settings , and single-attribute dc methods  significantly underperformed the multi-attribute ones. here we use the results of a typical simulation test setting  as an example. even though some individual attributes alone may be inferior to other attributes in detecting defs, they can indeed provide substantial enhancements to other attributes. for example, in table  <dig>  dcr detected no defs at fdr <  <dig>  or fdr <  <dig> . adding s
r to s
nb significantly improved the results by  <dig> %  at fdr <  <dig>  and by  <dig> %  at fdr <  <dig> . results across different sample sizes  confirm the advantages of integrating multiple basic attributes. grouping the defs detected by dct, dcnb, and dct+r+nb accordingly to their distribution categories , we observe that integrating multiple basic attributes helps to detect defs across the whole distribution spectrum. interestingly, dct on average detected more defs governed by nb distributions than dcnb, which to some extent resonates with the idea of voom, i.e., it is sometimes more important to model the mean-variance relationship correctly than to design the exact distribution of read-counts.fig.  <dig> compares the true defs detected by different dc configurations. four dc configurations  are compared under different sample sizes  and different true def configurations:  g
 <dig>   g
 <dig>   g
 <dig>  and  g
 <dig>  the target fdr cutoff is  <dig> . the y-axis indicates the relative differences  of the average true defs detected by different dc configurations with respect to those detected by dct+r+nb.the plots only display up to 20% relative difference. this figure clearly shows that dct+r+nb outperformed the remaining dc configurations


, sr, and snb
the 1st column lists the single-attribute dc configurations. dcr was not displayed because it failed to detect any true defs under both fdr targets. the 2nd column lists the target fdr levels  at which performances are compared. cells in the 3rd~6th columns show the improvements in percentage of multi-attribute dc configurations  over single-attribute dc configurations . the numbers in parentheses are the paired t-test p-values showing the significance of the improvement. for example, the cell at the 3rd column and 4th row shows that dct+nb outperformed dcnb by  <dig> % with a paired t-test p-value of  <dig> e- <dig> at fdr <  <dig> . although dcr as a single attribute failed detect any defs, adding sr to the other two attributes  significantly improved the performance, as indicated by the 4th column


compares dct+r+nb with three single-attribute dc configurations on simulated datasets of various sample sizes at fdr <  <dig>  and g
 <dig>  each cell shows the average number of true defs detected by a dc configuration under a sample size indicated by the column header. the numbers in parentheses are the paired t-test p-values indicating how significant dct+r+nb outperformed the corresponding single-attribute dc configurations under the same simulation test settings. dcr detected no defs when n <  <dig> 


dct
dcnb
dct+r+nb
compares the average numbers of true defs detected by dct, dcnb, and dct+r+nb in different distribution categories under the simulation test setting:  <dig> vs.  <dig>  g
 <dig>  and the target fdr <  <dig> . dcr is not displayed because it detected no defs. the numbers in the parentheses are the paired t-test p-values indicating how significant dct+r+nb outperformed the corresponding single-attribute dc configurations.




no single basic attribute dominates
we also observed that none of the basic attributes consistently performed better than other basic attributes in our simulation tests, which resonates the idea of utilizing multiple attributes. for example in table  <dig>  under the simulation test setting  <dig> vs.  <dig> and g
 <dig>  dct on average detected more true defs than dcnb  at fdr <  <dig> , but performed worse than dcnb  at fdr <  <dig> . moreover, at fdr <  <dig> , dct outperformed dcnb on datasets when the sample size was relatively small  while dcnb outperformed dct when the sample size was larger . interestingly, although dcr underperformed dct under most test settings, dcr outperformed dct under the setting of  <dig> vs.  <dig>  g
 <dig> and target fdr <  <dig>  .

compare dct+r+nb with other def detection approaches
we compared dct+r+nb and  <dig> other rna-seq differential expression analysis methods including bayseq, deseq, ebseq, edger, nbpseq, samseq, shrinkseq, tspm, voom, vst/limma, poissonseq, deseq <dig>  and odp. figure  <dig> shows the average numbers of the detected true defs at two typical target fdr levels  under a typical simulation test setting  <dig> vs.  <dig> and g
 <dig> . among those able to effectively control the fdr, dct+r+nb in general performed the best. at target fdr <  <dig> , dct+r+nb on average detected  <dig>  true defs, which is significantly better  than the  <dig>  true defs detected by the best non-dc method . at target fdr <  <dig> , dct+r+nb detected  <dig>  true defs, which is significantly better  than the  <dig>  true defs detected by the best non-dc method . figure  <dig> compares the average number of true positives detected by different approaches at different target fdr cutoffs  under a typical simulation test setting of  <dig> vs.  <dig> and g
 <dig> . figure  <dig> and additional file 1: figures s21– <dig> show that dc in general performed the best among those effectively controlled fdr.fig.  <dig> evaluates rna-seq differential expression analysis methods using simulated data . methods are listed along the x-axis. the red bars indicate the average true fdrs . the horizontal dashed line across the figure marks the target fdr. the blue bars indicate the average number of the detected true defs . the 90% confidence intervals of the detected defs are marked except for those whose true fdrs exceed the target fdr by 10%. a target fdr <  <dig> . b target fdr <  <dig> . the true fdrs of dct+r+nb do not exceed the corresponding target fdrs. dct+r+nb was the most powerful among those able to effectively control the fdr 


fig.  <dig> compare the curves of the true positives vs. the target fdr . the x- and y- axes indicate the target fdr cutoff and the average number of true positives, respectively. the solid curve with blue circle markers represents dct+r+nb and other curves represent non-dc methods. the result of a method at a particular target fdr is shown in this plot if  its average true fdr does not exceed the target fdr by 10%; and  its average number of true defs is ≥ <dig>  . dc was able to meet all target fdr cutoffs. the results of voom and vst/limma are almost the same




in some application scenarios other than ours, users may want to choose a fixed number of top defs. to serve this purpose, fig.  <dig> compares the results using fdc . the fdcs of other test settings are provided in additional file 1: figures s41– <dig>  figure  <dig> and additional file 1: figures s41– <dig> show that dc is among the best performers including voom, vst/limma, deseq <dig>  edger, and shrinkseq. here we do not show roc , which is also popular for evaluating machine learning techniques and statistical analysis methods, because fdc and roc deliver the same information from different viewpoints. since true fdr can be estimated but usually unknown, fdc and roc should be used with caution in our application scenario because they do not consider whether a method is able to estimate fdr well. fdc and roc only depend on the ranks of features’ significance scores regardless of their actual values. therefore, it is possible that two def detection methods can produce the same roc/fdc although they have quite different capabilities in estimating fdr. imagining there are two def detection methods. the 1st method is biased towards high p-values  because it imposes some assumptions. calling one single significant feature using the 1st method will lead to an extraordinarily high estimated fdr. on the contrary, the 2nd method is biased towards small p-values  because it imposes other assumptions. given a target fdr, the 2nd method will dramatically underestimate its true fdr and call too many false positives. nevertheless, if the features are ranked in the same order by both methods, they will produce exactly the same roc/fdc.fig.  <dig> the curves of the true fdr vs. the number of detected defs in a typical simulation test . the x- and y- axes indicate the number of detected defs and the average true fdr, respectively. the curve of dct+r+nb  in this figure were converted from the results obtained by setting the target fdr between  <dig>  and  <dig>  with an increasing step of  <dig> . the curves of other methods were obtained by letting them call the same number of defs detected by dct+r+nb at each target fdr




effects of sample size and def configuration
figure  <dig> summarizes the effects of “sample sizes + def configurations” on def detection results at target fdr <  <dig> . the result of a method under a particular setting is included if its true fdr does not exceed the target fdr by 10% and it detects on average at least  <dig>  true defs . under most settings, dct+r+nb was able to effectively control fdr and detect more defs. however, when the sample size is small , the average true fdrs of dct+r+nb were  <dig>  and  <dig>  for g
 <dig> and g
 <dig>  respectively; and odp was the only method able to detect true defs  while meeting the fdr target. when the sample size was decreased, all methods detected less defs, and it was more difficult to control the fdr, especially when a more stringent target fdr was imposed. for example, when n =  <dig> , g
 <dig>  and the target fdr <  <dig> , dct+r+nb on average detected less than  <dig> defs, and one single false positive alone would increase its true fdr by  <dig> , which is much higher than the target fdr. smaller sample sizes  were also tested. however, no method was able to control fdr well  or detect at least  <dig>  defs on average. hence, the results of  <dig> vs.  <dig> and  <dig> vs.  <dig> are not shown in fig.  <dig>  this indicates that it remains challenging to detect defs governed by complex distributions when the sample size is small.fig.  <dig> compares def detection results under different test settings at target fdr <  <dig> . the x- and y- axes indicate the sample size and the number of detected true defs, respectively. plots , , , and  are the results of def configurations g
 <dig>  g
 <dig>  g
 <dig> and g
 <dig>  respectively. a method is not displayed under a test setting if either its corresponding true fdr exceeds the target fdr by 10% or it on average detected less than  <dig>  true def




evaluation using the seqc/maqc-iii dataset
the us food and drug administration has coordinated a large-scale community effort, the sequencing quality control project , to assess the performance of rna-seq across laboratories and to test different sequencing platforms and data analysis pipelines  <cit> . the consortium has generated a rna-seq datasets  from two reference rna samples, the strategene universal human reference rna  and the ambion human brain reference rna . this dataset contains two reference feature subsets:   <dig> synthetic rnas from the external rna control consortium  with four different sample a/sample b ratios ; and  ~ <dig> genes whose sample a/sample b fold-changes were validated using taqman qrt-pcr  <cit> . in the following comparison, we used log <dig> expression change threshold of  <dig>  to select true defs from the ~ <dig> taqman qrt-pcr validated genes, and obtained  <dig> genes denoted as the positive taqman genes below. however, due to the extreme difference between samples a and b  <cit> , the positive taqman genes only represent a small fraction of those differentially expressed between samples a and b. if we let different def detection methods compare the replicates of samples a and those of samples b, their results on the positive ercc spike-ins and the positive taqman genes cannot accurately reflect their overall performances. in addition, all def detection methods will detect too many defs that dwarf the differences between their detection results. thus we designed the following procedure to make the positive ercc spike-ins and the positive taqman genes together as a proper reference feature set for evaluating def detection methods.

we focused on the seqc/maqc-iii rna-seq subset sequenced at the australian genome research facility using the illumina hiseq <dig>  in which each rna sample has  <dig> technical replicates . first, the low-count genes  were removed. after this step,  <dig> negative ercc spike-ins  and  <dig> positive ercc spike-ins  were retained. then we used the state-of-the-art rna-seq normalization tool, ruvseq  <cit> , to normalize all  <dig> replicates using the negative ercc spike-ins and  <dig> least differentially expressed genes  as the in silico empirical negative control genes. in particularly, we used ruvg  and followed the practice of ruvg authors described in the online methods of  <cit>  by dropping the first unwanted factor and retained the next  <dig> factors. after normalizing the replicates, we randomly chose  <dig> replicates from one library from the sample a and divide them into two equal-size groups to form the base of non-defs . occasionally we obtained two very distinct groups because the above normalization procedure could not get rid of all unwanted variations. to avoid this problem, we applied poissonseq to calculate the p-values of the true non-defs being differentially expressed between the chosen groups, and redid grouping if the p-value distribution of the true non-defs was not closed to uniform between  <dig> and  <dig>  poissonseq was used because the poisson distribution was reported to be effective for modelling technical replicates  <cit> . finally, we replaced the values of the positive ercc spike-ins and the positive taqman genes in one of the chosen groups by their values in  <dig> randomly selected replicates of sample b. this arrangement should make the positive taqman genes as the true defs and the remaining genes as the true non-defs.

the data obtained above was then used to benchmark different def detection methods. we repeated the above procedure  <dig> times. the results are summarized in fig.  <dig>  all dc configurations and most non-dc methods were able to effectively control the fdr at both target fdr levels . among those able to effectively control the fdr, dct+r+nb was the most powerful. at target fdr <  <dig> , dct+r+nb on average detected  <dig>  true defs, which is significantly better  than the  <dig>  true defs detected by the best non-dc method . at target fdr <  <dig> , dct+r+nb on average detected  <dig>  true defs, which is significantly better  than the  <dig>  true defs detected by the best non-dc method . the leads of dct+r+nb over non-dc methods are not as large as those in the simulation test because we used technical replicates in this experiment. figures  <dig> and  <dig> compare the curves of “the true positives vs. the target fdr” and fdcs, respectively. the supreme performance of dct+r+nb can be explained by fig.  <dig>  which shows that the normalized-count distributions of some positive taqman genes are complex even within the chosen technical replicate subset.fig.  <dig> evaluates rna-seq differential expression analysis methods using the seqc/maqc-iii dataset. the red bars indicate the average true fdrs . the horizontal dashed line across the figure marks the target fdr. the blue bars indicate the average number of the detected true defs . the 90% confidence intervals of the detected defs are marked except for those whose true fdrs exceed the target fdr by 10%. a target fdr <  <dig> . b target fdr <  <dig> . at both target fdr levels, most methods effectively controlled the fdr. dct+r+nb is the most powerful one 


fig.  <dig> the curves of the true positives vs. the target fdr using the seqc/maqc-iii dataset. the x- and y- axes indicate the target fdr level and the average number of true positives, respectively. the solid curve with blue circle markers represents dct+r+nb, and other curves represent non-dc methods. the result of a method at a particular target fdr is shown if  its true fdr does not exceed the target fdr by 10%; and  it detects on average ≥ <dig>  true defs 


fig.  <dig> the curves of the true fdr vs. the number of the detected defs using the seqc/maqc-iii dataset. the x- and y- axes indicate the number of detected defs and the average true fdr, respectively. the curve of dct+r+nb  in this figure were converted from the results obtained by setting the target fdr between  <dig>  and  <dig>  with an increasing step of  <dig> . the curves of other methods were obtained by letting them call the same number of defs detected by dct+r+nb at each target fdr


fig.  <dig> some positive taqman genes have complex distributions in technical replicates . y-axis: the correlation coefficients between the normalized-count distributions and the corresponding fitted gmm distribution. x-axis: the correlation coefficients between the normalized-count distributions and the corresponding fitted nb distribution. each dot represents a gene. the distribution of a gene’s normalized counts is approximated by a histogram of  <dig> equal-size bins spanning its read-count value range. the colors of dots indicate the most proper numbers of components in a fitted gmm according to the bayesian information criterion: green , blue , and red  correspond to  <dig>   <dig> and  <dig> components, respectively




analyze a methylation dataset
to demonstrate the general applicability of dc, we applied it to analyze a dna methylation dataset generated by aldinger et al.  <cit>  using the illumina humanmethylation <dig> beadchip, which can be downloaded as gse <dig> from geo. this data set contains global dna methylation of  <dig> rett syndrome samples and  <dig> control samples. since the nature of this dataset is quite different from typical rna-seq count data, we did not include methods developed specifically for rna-seq in this comparison. instead, we focused on the applicability of dc and assessing the benefits of using more than one attributes. we selected five basic statistics and let dc use two of them in each run:  s
t.sam – the corrected t-statistic  <cit> ;  s
t.log.sam – the corrected t-statistic with logarithmic transformation;  s
r.sam – the corrected ranksum statistic  <cit> ;  s
t.voom – the moderated t-statistic produced by voom; and  s
t.limma – the moderated t-statistic produced by limma. the original data values were multiplied by  <dig> and then rounded to the nearest integer if an attribute extraction package only accepts integer inputs. the nb-based basic attributes  were not used because the distributions of dna methylation features in this dataset are quite different from the nb distribution.

the results at fdr <  <dig>   show that combining two basic attributes is significantly advantageous over utilizing single ones. for example, dct.limma+t.voom detected  <dig> defs, which is much higher than the  <dig> and  <dig> defs detected by dct.limma and dct.voom, respectively. this result is interesting because both s
t.voom and s
t.limma are moderated t-statistics and voom utilizes limma to calculate its test statistics after applying a log-count per million transformation to the original data. nevertheless, the integration of s
t.voom and s
t.limma by dc can achieve significantly higher detection power than using one of them. the main reason underlying this observation is visualized in fig. 12: the joint distribution of s
t.limma and s
t.voom are quite asymmetric and non-gaussian. it is more advantageous to use s
t.voom and s
t.limma to detect defs in the up- and down-regulated regions, respectively. their advantages can be integrated by dc that rigorously explores the structures in the joint distribution of s
t.voom and s
t.limma to achieve better def detection results.table  <dig> compares the performances of dc using different pairs of basic attributes on gse34099

basic attribute #2
s
t.sam
s
t.log.sam
s
r.sam
s
t.voom
s
t.limma

s
t.sam

s
t.log.sam

s
r.sam

s
t.voom

s
t.limma
the first column and row indicate the basic attributes used by dc. the diagonal cells list the numbers of defs detected by dc using single attributes. the rest of cells list the numbers of defs detected by dc using different combinations of two basic attributes


fig.  <dig> compares the defs detected by dct.voom , dct.limma , and dct.limma+t.voom  in gse <dig>  each dot represents a feature in the dataset. a the x-axis and y-axis indicate the s
t.voom and s
t.limma attributes, respectively. b & c are two blow-outs of the corresponding areas in  for better view. see main text for detailed discussions




discussions
conventional methods for differential expression analysis often use individual basic attributes , which may significantly underestimate the complexity observed in reality. this is partially because the datasets, which were available when those analysis methods were developed, usually contained only a few replicates. it can also be due to underestimation of the underlying biological variations. we have shown in this paper that insufficient characterization of differential expression information could lead to low detection power and/or higher-than-expected fdrs. it is expected that future studies will produce sufficiently large number of replicates because the collaboration scales are quickly growing larger and the rapid advances of high-throughput technologies will bring down the experimental cost dramatically. therefore, it is important to develop novel def detection methods with better capability of dealing with complex differential expression patterns. to this end, we proposed to utilize multiple basic attributes to better capture differential expression information and formulate the problem of detecting defs as optimizing discriminant boundary constrained by a user-defined fdr cutoff in a multi-dimensional space. we have developed the discriminant-cut  algorithm for dealing with a special family of discriminant functions . the comparison of dc with several existing def detection methods using simulated datasets and the seqc/maqc-iii rna-seq dataset confirms the advantages of dc in handling complex differential expression patterns. in addition, we also show an application of dc to analyze microarray datasets, and expect that dc can be used  to analyze many different types of high-throughput datasets. in the future, we will explore our approach for meta-analysis  <cit>  that integrate multiple datasets.

using linear discriminant functions is an effective step forward, but it may not be powerful enough to fully utilize large-scale datasets. more powerful methods can be developed in the future by exploring more sophisticated discriminant function families and learning techniques. discriminant analysis by integrating heterogeneous attributes is popular in many machine-learning research and its applications . it is mostly done in supervised way that can rely on labelled information to perform calibration. our approach is unsupervised and uses the estimated fdr for self-calibration. this kind of machine learning problem has not been widely researched, and hence can be of great interest to future research.

our approach greatly benefits from the attributes designed by previous research on differential analysis . we believe that we are far from fully exploring the potentials of those attributes. on the other hand, it is possible that some attributes may be redundant  or their information cannot be effectively utilized by the chosen discriminant function family. which attributes are effective depends on the characteristics of the dataset under analysis. dc already has certain attribute selection capability because it applies the l <dig> regularization. however, we believe attribute selection remains an open problem and can be domain specific. we will investigate this problem in the context of detecting defs in the future. as far as we know, our work is the first one that formerly introduces unsupervised multi-dimension discriminant analysis to def detection, which can be a new direction to significantly advance the def detection research as supported by our experimental results.

CONCLUSIONS
this paper presents a novel machine learning methodology for robust differential expression analysis, which can be a new avenue to significantly advance research on large-scale differential expression analysis. the corresponding mathematical model was formulated as a constrained optimization problem aiming to maximize discoveries satisfying a user-defined fdr constraint. an effective algorithm, discriminant-cut, was developed to solve an instantiation of this problem. extensive comparisons of discriminant-cut with a couple of cutting edge methods were carried out to demonstrate its robustness and effectiveness.

additional file

additional file 1: additional documentation. 




abbreviations
dcdiscriminant-cut

defdifferential expressed features

fdrfalse discovery rate

