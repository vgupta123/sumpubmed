BACKGROUND
precision medicine requires to tightly couple phenotypic and genotypic patient data  <cit> , thus advocating for the development of it tools that enable deep joint investigations of the two data sources. the informatics for integrating biology and beside  system provides an excellent framework to analyze clinical data for research purposes  <cit>  and facilitates the implementation of precision medicine strategies  <cit> . thanks to its modular architecture based on open-source rest web services and on the design of a simple but effective data warehouse scheme, i2b <dig> is now very popular in academia and industry, and has been used as the basis for many research projects, including for example transmart  <cit> .

built on a “hive” of multiple server-side software modules  that communicate through their integrated xml-based web services, the i2b <dig> platform consists of several core and optional cells. each cell either holds data or a business tier. for example, the i2b <dig> web client interface allows performing ad hoc queries in order to find those patients having particular phenotypes described by an integrated controlled vocabulary. once a patient set has been defined, data can be passed to one of the i2b <dig> plug-ins that implements specific analysis methods.

an interesting extension of the i2b <dig> capabilities is the ability to efficiently handle, together with clinical information, large-scale molecular data, and in particular those produced by next generation sequencing  technologies. ngs technologies, able to read billions of dna fragments at once, cover a broad range of genomic, transcriptomic and epigenomic applications allowing the study of genetic signals underlying phenotypic traits of interests. over the last few years, targeted re-sequencing has become one of the most popular ngs genomic approaches due to its cost affordability  <cit> . in brief, it consists of selectively sequencing genomic regions of interest , mapping the resulting dna sequences to a given genomic reference, and reporting the identified differences, i.e. variants. the most exhaustive and common targeted re-sequencing application is whole-exome, that allows identifying variants over the entire set of known human genes  <cit> .

the increasing availability of ngs facilities and the upcoming use of target re-sequencing technologies in clinical practice will generate large data sets that need to be properly integrated in software architectures able to jointly manage phenotypic and genotypic data for precision medicine purposes. on the one hand, it is important to report the presence of variants that have an established clinical meaning in the patients’ clinical records. on the other hand, it is also crucial to progressively store the variants with unknown meaning for future use and interpretation. a single whole-exome analysis may generate tens of thousands of such variants, which may need to be queried and retrieved, for example, within a large-scale study. storing and retrieving this kind of data present a number of challenges. first, variants need to be annotated using genomic knowledgebases necessary for their interpretation. second, since biomedical knowledge is steadily increasing, the data model used for variant representation should be flexible enough to support frequent updates and the introduction of new sources of biological annotations. finally, variant queries need to be fast and the overall data management process should scale efficiently due to the growing number of experiments conducted.

several approaches and frameworks have been developed with the aim to store, retrieve and analyze genomic variants . among them we can distinguish those based on relational databases  <cit>  and not-only-sql  ones  <cit> .

nosql solutions, in particular, represent a group of very interesting tools to store and retrieve very large data sets  <cit>  and have emerged in recent years due to the rising need to handle “big data”, characterized by properties such high volume, variability and velocity  <cit> .

genomic variants can be rightfully included in this category. volume and velocity are given by the high rate at which variants are generated by the increasingly fast and high throughput sequencing instruments. variability refers to the need to pre-process and evaluate variants accordingly to different variant types , sequencing applications, and diseases under study. indeed, one may wish to use different genomic knowledgebases , or to evaluate specific variant measures . furthermore, because several genomic annotations fit only with a particular set of variant types, this would lead to many missing data in a structured data context .

even though efforts to standardize the way to report genomic variant and related ngs measures have been pursued  <cit> , variant annotation for genomic knowledgebases depends on the specific application and is difficult to standardize. as a consequence, structured and centralized relational databases are not the best choice to deal with increasingly growing, heterogeneously annotated genomic variants.

the flexible and distributed data model behind nosql databases, on the contrary, is suitable to systematically store and retrieve genomic variants and their annotations coming from different and frequently updated ngs analysis workflows. however, there is a tradeoff between data model flexibility and query complexity: nosql databases generally do not have a sql-like query language, and queries have to be pre-computed in order to build the corresponding in-memory index structures that allow for fast searches.

notably, nosql databases have been used to manage genomic data  <cit> , showing better performance than relational databases both in terms of horizontal scalability  and computational time in data retrieval.

in their works, o’connor b. et al.  <cit>  and wang s. et al.  <cit>  adopted hbase, a column-family nosql database. briefly, the underlying key-value data model  <cit>  allows retrieving values  that belong to rows by querying the related row-keys. typically, one has to build a number of column families equal to the number of the desired queries. this leads to writing a significant amount of code to manage the population of column families and to ensure data consistency among them, both in import and update phase.

couchdb  <cit>  is a nosql database that uses semi-structured documents  to handle data, a restful programming interface and javascript to define queries that exploit the mapreduce paradigm. it has been successfully adopted to deal with gene annotations, drug-target interactions, and copy number variants  <cit> .

here we present bigq, an extension to the i2b <dig> framework, implemented to handle genomic variants such as single nucleotide variants  and short insertion and deletions  within their annotations. bigq allows the joint query of phenotype and genotype data by integrating different technological layers including for the first time a nosql component into the i2b <dig> overall architecture.

we chose couchdb among the other nosql databases for several reasons. first, the object-oriented nature of json documents looks particularly appropriate to manage different kinds of genomic variants and their annotations. second, the flexible schema of couchdb allows storing information on variants, that is potentially heterogeneous, as json files. third, queries are designed by writing simple javascript functions using mapreduce operations on json attributes, with the corresponding results indexed for a fast data retrieval. notably, couchdb automatically updates its indexes when json documents are added, deleted or modified. in particular, some features of couchdb made it preferable among the other document-based nosql data stores, such as mongodb: the embedded mapreduce engine, its rest-ful architecture and natively version-aware document management.

the couchdb rest-ful programming interface has been used to enable communication between the variant database and the i2b <dig> framework. we extended i2b <dig> with two main components: i) a new i2b <dig> cell that manages http requests to couchdb and ii) a plug-in based on a visual programming paradigm that allows dynamically performing queries on clinical and genomic data. the i2b <dig> extension is also provided with an extraction, transformation and loading  middleware that, starting from raw variant files in a standard format, uploads genomic variants, including a pre-defined set of annotations, in couchdb. in the following we describe the technical aspects of this extension, and an evaluation of its performance on exome data from the  <dig> genomes projects   <cit> .

implementation
bigq consists of three main components: bigq-etl, to annotate and import variants in couchdb; bigq-cell, that holds the business logic to query couchdb; and bigq-plugin to query genomic data belonging to a patient cohort retrieved from the i2b <dig> data warehouse. figure  <dig> shows the system components and their inter-relationships.fig.  <dig> system components and their interrelationships. bigq-etl requires the user to provide one or more vcf files that are functionally annotated with annovar and used to create one json document for each variant belonging to each patient; these jsons are stored in couchdb to be queried by the biqq-cell. on the client-side, the bigq-plugin allows the user to create a genetic query with drag-and-drop interactions within the i2b <dig> webclient; the plugin then communicates with the cell to run the query and collect the results that are shown to the user



the bigq-etl
accordingly to the i2b <dig> policy, data entry is not demanded to the final user; therefore this module has been developed as a back-end tool.

bigq-etl takes genomic variants in the variant calling format   <cit>  as input, and uses annovar  <cit>  to annotate them. annovar allows to easily annotating a set of variants using several sources of information on transcripts, genes, gene-based functions , evolutionary conservation scores, public variant databases such as dbsnp  <cit>  and many other -omics resources  <cit> .

in particular, bigq-etl implements the table_annovar script, able to annotate variants both on standard and customized genomic tracks.

the data import process only requires providing the vcf files and some basic information, such as the i2b <dig> identification codes of the patients whose variants are contained in the vcf files. once these data are available, the process is completely automatic: the files are sent to the server entrusted with the functional annotation process, where the table_annovar script is executed. variants are enriched with transcripts, gene-based functions, 1kgp variant frequencies and the whole dbnsfp  <cit>  dataset including variant scores given by several prediction tools such as polyphen- <dig>  <cit>  and sift  <cit> .

the output of annovar is then used to create one json document for each variant belonging to a single patient. json is an open standard format used to transmit data objects consisting of attribute-value pairs. in our case, the set of possible attributes consist of data coming from the original vcf file, and of the functional annotations added by annovar . finally, each document is associated with a unique universal identifier  and sent to couchdb. notably, different types of variants may be represented by different data structures. for example, a deep intergenic variant may not have any gene associated with it, or a synonymous coding variant does not hold data about prediction scores such as polyphen- <dig> and sift . we have therefore developed an extensible object-oriented data structure, written in java, able to model different kind of variants and annotations. one may want to add a new genomic track, or may not be interested in using others. therefore, each variant type is modeled and treated individually, and holds only the needed attributes .fig.  <dig> example of annotated variants in json format. two variants, represented in vcf format, are identified by standard attributes . the annotation step finds out that one variant falls in an exon causing an amino acid substitution at the protein level  while the other is located in a transcript splicing site. the two variants generate two different json objects, characterized by different attributes. differences between jsons are highlighted in bold



alongside with the json documents, bigq-etl generates a set of design documents and sends them to couchdb. these documents describe the queries  that can be executed on the database.

views are defined using javascript functions that specify attribute-value constraints corresponding to the query requirements, and that implement the map and reduce functions according to the mapreduce paradigm. map functions are called once on each document: the document can be skipped  or can be transformed  into one or more view rows as key/value pairs. view rows are inserted into a b-tree storage engine and sorted by key : lookups by key or key range are therefore extremely efficient, with o complexity. since map functions are applied to each document in isolation, computation can be highly parallelized within and across nodes where the database is distributed. reduce functions can be optionally used in combination with map functions in order to report data aggregates grouping by row keys, such as counting the number of rows within a view or to calculate averages on related values.

we have chosen to specify a design document, and consequently an index, for all the possible attributes describing variants . this choice allows us to perform any complex query as the combination of simple ones. it has to be noted that couchdb flexibility allows building a view an all json documents despite their heterogeneity.

the whole data import process, which is potentially time and resource consuming, has been designed to be easily run in parallel, by splitting vcf files in batches, on a cloud-based architecture.

interval queries in bigq
a genomic interval identifies a variant by its start and end positions with respect to the reference genome. genome browsers  <cit>  and genomic annotation tools  <cit>  are based on a particular binning scheme  <cit>  in order to index genomic intervals and allow for a fast search of overlapping features given a query interval.

we have implemented a similar approach in couchdb that fits with its query logic.

each chromosome has been divided into a predefined set of hierarchical bins  depending on the specific chromosome length. the value  <dig> or  <dig> has been assigned to each bin, depending on being the left or right child bin within the tree, respectively . a code is then assigned to each genomic feature , corresponding to the ordered series of 0s and 1s given by navigating the binning tree from the root to the smallest bin that entirely contains the variant . the bin code is then represented as a json attribute, whose value is an array of 0s and 1s.fig.  <dig> the simplified binning scheme and search strategy implemented in couchdb. for the genomic feature a, the smallest containing bin is the one reached by navigating the tree in the following way:  <dig> , <dig> , <dig> . for feature b and feature u the smallest bins are  and  respectively. given the interval query q, its smallest containing bin is the one coded by . when searching for genomic features within the corresponding overlapping bins, both for the lower and upper part of the tree, genomic feature u would also be reported: in fact, despite overlapping with one of the searched bins, it does not overlap with q. therefore, two more queries  are performed in order to remove the non-overlapping elements: the first adds the start position of the genomic feature to the view keys  while the second one adds the stop position. in this example, genomic feature u would be removed from the query result set because its start position is greater than the end one of q



we implemented a couchdb view with row keys composed by patient id, chromosome and bin code. given an interval query, the search space is calculated a priori. it consists of the smallest containing bin for the interval query and its overlapping bins both for the upper and lower part of the binning tree. the view is therefore searched for the variants within these bins and two additional views are used to filter out variants that, even if located in the overlapping bins, do not overlap the interval query .

the bigq-cell
one of the most significant features of i2b <dig> is its loosely coupled structure, in which a set of web services  concurs to create the i2b <dig> server side core . the schema of exposed data is defined by the i2b <dig> xml-based messaging standard. this type of architecture lends itself easily to be updated and enhanced with new features  <cit> . the bigq-cell is a novel i2b <dig> cell we developed that enables the communication with couchdb in order to execute queries on genetic data. the cell has been implemented in java and uses the lightcouch library  <cit>  to manage the communication with the database.

the cell extracts all the parameters required to execute a query from the xml file that it takes as its input: the basic object exchanged is a set of variants’ uuids grouped by patient, called datain.

 the logic of the query: “add” or “filter”. if “add” is chosen, the uuids returned by couchdb are added to datain and sent back in the cell response; if “filter” is chosen, only the uuids belonging to both sets are sent back.

 the query type that identifies the variant fields  on which the query should be executed. examples of allowed query types are: gene for gene names, exonicfunc for exonic functions and polyphenscore for the polyphen- <dig> score.

 the query details, the set of values required to perform the specific type of query. for example: the list of gene names for the gene query, the list of exonic functions for the exonicfunc query and the endpoints of the score interval for the polyphenscore query.



once these parameters are extracted, the cell accesses the couchdb view associated with the specific query type according to the query details; this operation is performed for each patient in the datain set. the aggregated results from the database, consisting of a new set of variants’ uuids grouped by patient, is combined with datain according to the query logic to build the output object of the cell, called dataout. finally, the bigq-cell builds the response xml message encoding the dataout object and sends it back to the client.

the bigq-plugin
the bigq plugin for the i2b <dig> webclient has been specifically developed to communicate with the bigq-cell. this plugin allows users to run genetic queries within i2b <dig>  exploiting the patient sets previously extracted with phenotype queries. i2b <dig> 

the main feature of bigq-plugin, when compared to other webclient plugins, is that its interface is based on visual programming. the user graphically builds queries with drag-and-drop interactions; feedback and results are presented within the same workspace in order to provide a more consistent experience. the plugin interface exploits the mxgraph javascript libraries  <cit> .

each query exposed by the bigq-cell is represented by a block that can be dragged into the workspace; the final query is made up by the sequence different blocks connected to each other. besides standard blocks , bigq-plugin also provides the patient result set drop  block, an input block to import a patient set in the workspace, and the patient result set table  block, an output block that shows the patients that have at least one variant that matches the query.

a typical interaction with the plugin starts with the user generating a patient set with the i2b <dig> query tool, using the data stored in the data warehouse; this allows extracting the patients corresponding to the phenotype of interest. afterwards, moving to bigq  the user is presented with a blank workspace, where he/she can define the query as a sequence of blocks. sequences typically start with the prs drop block that imports a patient set  and makes it available to the blocks that are directly connected to it. double-clicking on the query blocks brings up a form to specify their query logic and query details. query blocks receive their datain from the upstream blocks, call the cell to run the query and return the dataout generated by the cell to the downstream blocks. finally the prs table block is added to show the result of the query. an example query is shown in fig.  <dig> fig.  <dig> screenshot of the bigq-plugin with user interactions highlighted.  the user creates a query by dragging and dropping different blocks inside the plugin’s workspace, with each block representing a query on a single attribute that will be performed by the bigq-cell. the query is defined by connecting the blocks to each other.  a patient set, previously created with a standard i2b <dig> query, is dragged and dropped on the patient result set drop  block to define the patients whose exomes will be queried.  by double-clicking the standard query blocks  it is possible to specify their query logic and query parameters.  the query process can start and each block executes its query sequentially, calling the bigq-cell.  when all blocks have performed their query, the user can visualize the results by double-clicking the patient result set table  block



RESULTS
to test our approach for integrating genetic queries within the i2b <dig> framework, we have performed a “stress test” on the system by submitting increasingly large whole exome sequencing  datasets of genomic variants. wes data were retrieved from the  <dig> genomes project phase <dig> integrated release. we have tested our system on variant sets coming from  <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> exomes. the average number of variants per exome, and thereby of the json documents added to the database for each individual, is about  <dig>  for a maximum of  <dig> , <dig> genomic variants in the case of  <dig> exomes.

for the importing phase, bigq-etl has been tested on  <dig> amazon ec <dig>  <cit>  virtual machines, in particular, c <dig> xlarge instances  <cit> , a medium-high level server with  <dig> virtual cpus and 15gb ram. couchdb was initially installed on a single c <dig> xlarge instance. total time to complete the annovar run, generating the json documents and uploading them for  <dig> exomes was approximately one hour and  <dig> min. the most computationally demanding operation in the data import process was indexing all the views in the database, with an average time of  <dig> min and  <dig> s per exome. figure  <dig> reports disk space occupancy and importing time performances for annotation and indexing phase at the different dataset sizes.fig.  <dig> importing time performances and disk space occupancy on a single machine. a time performances for annotation, json conversion and importing of genomic variants belonging to  <dig> , <dig> , <dig>  whole-exome samples into couchdb, installed on a single amazon aws machine. b disk space occupancy in relation to the whole-exome data growth



for each data set we tested three types of queries that are commonly used by researchers to identify patients with possible variants of interest. the first query  searches among a limited patient set  those having a genomic variant with a given dbsnp id. the second query  is similar to q <dig> with the difference that the search for the dbsnp id is performed on the whole patient dataset. the third query  aims at identifying patients that have variants either introducing a premature stop codon or non-synonymous variants with a high damaging score  in a given gene and considering the whole patient dataset.

for each test we measured the average time necessary to run all three queries. figure  <dig> and table  <dig> show the results obtained, indicating that the query time is independent of the size of the database in the case of q <dig>  while it linearly scales with the size of the database in q <dig> and q <dig>  it is interesting to note that with the proposed computational infrastructure the query time is almost instantaneous for the user in the case of q <dig> , while querying more than  <dig> million variants  takes about  <dig> s. since query flexibility is not provided by couchdb, we have implemented a strategy to combine together the results from simple queries. as a consequence, a complex query  involving more than one variant attribute results in a longer query time due to the number of views to be searched  and the add and/or filter operations to be performed in backend.fig.  <dig> query time performances. query times  plotted against the increasing numbers of individuals  in the database



we therefore tried to perform query q <dig> by building up two dedicated views, i.e. by creating javascript map functions that index variants basing on patient id, gene, exonic function and polyphen- <dig> score. in particular, the first view  allows to dynamically set the gene name and the polyphen- <dig> threshold while for the second one  the gene name and the polyphen- <dig> score are set a priori. as expected, query time decreased to about  <dig> and  <dig> s on  <dig> exomes for q3a and q3b respectively .fig.  <dig> query time performances using dedicated views. time performance of the q <dig> query and of those using dedicated views  plotted against the increasing numbers of individuals  in the database



we also tested bigq by using couchdb in a distributed environment on the cloud. in particular, we have installed an elastic cluster version of couchdb  on a  <dig> c <dig> xlarge aws machine. the distributed database was set up as follows:  <dig> shards , no redundant copies , minimum read and write quorum ; see additional file  <dig> for details on bigcouch tuning parameters.

we therefore performed the same operations described above, from data import to the query test. we found out that the computational time during import phase is reduced thanks to horizontal scaling: the view creation phase for the  <dig> exomes decreased from  <dig> h and  <dig> min  to  <dig> h and  <dig> min .fig.  <dig> importing time performances on a distributed environment. time performance for annotation, json conversion and importing of genomic variants belonging to  <dig> , <dig> ,  <dig>   <dig> whole-exome samples into couchdb, installed on a distributed environment consisting of six amazon aws machines 



interestingly, we noted that horizontal scaling degrades performances on data retrieval , in particular for query q <dig> if executed using the flexible schema that combines different queries, while no significant differences were observed when using the corresponding dedicated view .fig.  <dig> query time performances on a distributed environment. time performance of q <dig>  q3a and q3b using couchdb in a distributed environment



this behavior can be explained by the bigcouch data sharding: each query needs to retrieve pieces of data of interest that are distributed among instances, to assemble them together and to return them to the client, resulting in slower performance especially when the number of sequential queries increases. as a consequence, the dedicated views strategy is even more necessary in a distributed couchdb scenario. therefore, we have planned to integrate bigq-plugin with an additional functionality that allows to save a composed query by blocks, create ad hoc views and retrieve results in a faster way, improving system usability. because each research group tends to standardize the way it searches for variants of interest, we believe this approach is valuable.

furthermore we intend to explore the fine-tuning features available for bigcouch. these tunings, together with an optimal number of nodes in the cluster, could bring about a considerable improvement in the indexing/query performance of the system.

CONCLUSIONS
in this paper we have described a system designed to deal with many genomic variants coming from heterogeneous and frequent ngs analysis, performed in a hospital environment where clinical research data are managed by the i2b <dig> framework.

the integration of patient clinical phenotype and genomic variant profiles is a two-step process: first patient cohorts are generated by querying clinical terms using the i2b <dig> built-in functionalities and second, the cohort is uploaded to the bigq-plugin in order to retrieve the corresponding genomic variants of interest.

variants are annotated with useful biological data by annovar software and stored in the document-based nosql couchdb system. the data are then managed by a dedicated i2b <dig> cell and a visual-programming plugin for easily performing queries.

the system has been conceived also to deal with variants of unknown clinical meaning and generated by different ngs applications, possibly characterized by useful but heterogeneous biological data. for this reason, the data model is flexible, and adherent to the contents of annovar documents; the database can thus be easily updated with new versions of the variant annotations.

the query system has very promising performance, showing to scale well with the database volume, making it feasible to jointly query clinical and genetic data. we note that the choice of couchdb allows naturally relying on cloud-based implementations on elastic clusters, such as the bigcouch system. despite i2b <dig> instances are usually installed locally  and not provided as a software as a service module, one could in fact use amazon aws products to build its own i2b <dig> infrastructure on the cloud.

in the future we will compare our implementations to other state of the art extensions of i2b <dig> and transmart developed to deal with ngs data, and we will work on other plugins, in order to better enable the full exploitation of ngs data within the i2b <dig> infrastructure.

availability and requirements
project name: bigq.

project home page:http://www.biomeris.com/index.php/en/tasks/bigq-ngs-en.

operating system: linux.

programming language: java, perl.

license: gnu general public license.



additional file
additional file 1: 
this file contains supplementary tables, figures and bigcouch tuning parameters.



abbreviations
i2b2informatics for integrating biology and beside

ngsnext generation sequencing

nosqlnot only sql

etlextraction, transformation and loading

1kgp <dig> genomes project

vcfvariant calling format

matteo gabetta and ivan limongelli contributed equally to this work.

competing interests

rb and ds hold shares of the university spin-off biomeris s.r.l., which provides support in customizing i2b2-based solutions. mg is employed at biomeris s.r.l. since the 1th november  <dig> 

authors’ contributions

mg, il and rb conceived the study and the general design of bigq. il and mg drafted the manuscript. il and er designed and developed the data extraction and transformation for bigq-etl. bigq-etl data loading, bigq-cell and bigq-plugin were designed and developed by mg. ds contributed to the design of the bigq-cell. rb and ar revised the manuscript. rb supervised and coordinated the work. all authors read and approved the final manuscript.

