BACKGROUND
neuroblastoma is the most common solid pediatric tumor, deriving from ganglionic lineage precursors of the sympathetic nervous system  <cit> . it shows notable heterogeneity of clinical behavior, ranging from rapid progression associated with metastatic spread and poor clinical outcome to spontaneous, or therapy-induced, regression into benign ganglioneuroma. age at diagnosis, stage and amplification of the n-myc proto-oncogene  are clinical and molecular risk factors that the international neuroblastoma risk group  utilized to classify patients into high, intermediate and low risk subgroups on which current therapeutic strategy is based. about fifty percent of high risk patients die despite treatment making the exploration of new and more effective strategies for improving stratification mandatory  <cit> .

the availability of genomic profiles improved our prognostic ability in many types of cancers  <cit> . several groups used gene expression-based approaches to stratify neuroblastoma patients. prognostic gene signatures were described  <cit>  and classifiers were trained to predict the risk class and/or patients' outcome  <cit> . it was recently reported the design of a multi signature ensemble classifier that merges heterogeneous, neuroblastoma-related gene signatures to blend their discriminating power, rather than numeric values, into a single, highly accurate patients' outcome predictor  <cit> . however, it is difficult to interpret these results with respect to the underlying biology because the assembly of the signature is only computational. the translation of the computational results to the clinic requires the use of explicit statements, coupled with the capability of blending prior knowledge on the disease with newly acquired information from high throughput technology.

we developed a biology-driven approach which defines the gene expression profile of a biological process known to be relevant, by prior knowledge, to the progression of the tumor and we then evaluate the prognostic value of such signature. we have identified tumor hypoxia as a feature of highly aggressive neuroblastoma  <cit> . hypoxia is a condition of low oxygen tension occurring in poorly vascularized areas of the tumor which has profound effects on cell growth, genotype selection, susceptibility to apoptosis, resistance to radio- and chemotherapy, tumor angiogenesis, epithelial to mesenchymal transition and propagation of cancer stem cells  <cit> . hypoxia activates specific genes encoding angiogenic, metabolic and metastatic factors  <cit>  and contributes to the acquisition of the tumor aggressive phenotype  <cit> . we have used gene expression profile to assess the hypoxic status of neuroblastoma cells and we have derived a robust 62-probe sets neuroblastoma hypoxia signature   <cit>  which is an independent risk factor for neuroblastoma patients  <cit> . prognostic signatures that can be linked to a biological processes are of great interest because they may redirect the choice of drugs in the design of more effectives treatments. therefore, integration of the nb-hypo signature with the existing risk factors may result into an interesting, needed and improved tool for neuroblastoma patients stratification and treatment.

several statistical and machine learning techniques have been proposed in the literature to deal with output of explicit rules and good classification performance  <cit> . most available techniques, such as linear discriminant approaches, multilayer perceptrons or support vector machines, are able to achieve a good degree of provisional accuracy, but construct black-box model whose functioning cannot allow to derive information about the pathology of interest and its relationships with the considered diagnostic and prognostic factors. to overcome this drawback, different classification methods, capable of constructing models described by a set of intelligible rules, have been developed. methods based on boolean function synthesis adopt the aggregative policy according to which at any iteration some patterns belonging to the same output class are clustered to produce an intelligible rule. suitable heuristic algorithms  <cit>  are employed to generate rules exhibiting the highest covering and the lowest error; a tradeoff between these two different objectives has been obtained by applying the shadow clustering  technique  <cit>  which generally leads to final models, called logic learning machines , exhibiting a good accuracy.

in the present work, we describe the utilization of llm to generate rules classifying nb patients on the basis of nb-hypo and clinical and molecular risk factors. we found that this algorithm can generate rules stratifying high risk neuroblastoma patients who could benefit from new therapeutic approach related to hypoxia. finally, we introduce a workflow to identify the instances that generate rules instability and to generate rules with a high degree of stability.

RESULTS
our aim was to derive explicit and stable rules, based on risk factors, to stratify nb patient's outcome. initially, we studied a  <dig> nb patients dataset characterized by three common risk factors, age at diagnosis, inss stage, mycn oncogene amplification. "good" or "poor" outcome classes are defined, from here on, as the patient's status "alive" or "dead"  <dig> years after diagnosis, respectively . the composition of the dataset is comparable to what previously described  <cit> . we selected this dataset because the gene expression profile of the primary tumor, performed by microarray, was available for each patient. we applied the llm implemented by the rulex  <dig>  software to generate intelligible rules predicting patients' outcome. the results are shown in table  <dig>  the algorithm identified  <dig> explicit rules predicting outcome. each rule covered more than one patient. in fact, the total covering of patients predicted dead  adds up to 140% but it comprises 96% of non overlapping poor outcome patients. rules  <dig> ,  <dig>  and  <dig>  include  <dig> % non overlapping good outcome patients. classification error was calculated for each rule as detailed in the methods section. in general, the prediction error was quite low with the exception of rule  <dig>  which had 13% error. this high error was somewhat predictable because the rule applies to stage  <dig> patients known to be resilient to classification by common risk factors  <cit> . rule  <dig>  is in total agreement with the known good prognosis of patients diagnosed before one year of age  <cit> . we then assessed the concordance of the predictions with the stratification in low , intermediate  and high risk classes  proposed by the inrg on the same risk factors. hr patients were correctly included in the rules classifying poor outcome patients and lr patients mapped in the good outcome rules. the concordance between risk prediction by the rules and previous clinical knowledge provided the first indications of the suitability of llm to classify neuroblastoma patients.

a the total number of patients is  <dig>  the number of patients in each subdivision is shown.

b the total number of patients is  <dig>  the number of patients in each subdivision is shown.

c percentage relative to the total number of patients.

d the risk assessment is based on inrg classification. hr = high risk; ir = intermediate risk; lr = low risk.

a the rule id is composed by the table number followed by a dot and the rule number.

b covering is the fraction of examples in the training set that verify the rule and belong to the target class.

c error is the fraction of examples in the training set that satisfy the rule and do not belong to the target class.

d the risk assessment is based on inrg classification taking into consideration only inss stage, mycn status and age at diagnosis. hr = high risk; ir = intermediate risk; lr = low risk.

fifty one patients failed to respond to treatment and have poor prognosis  indicating the need to develop and test new risk factors to improve stratification and new therapeutic protocols. biology-driven gene expression signatures have the potential to fulfill the dual purpose of generating new attributes for patients stratification and to indicate the possible therapeutic strategy  <cit> . we previously described a 62-probe sets signature  that stratifies neuroblastoma patients on the bases of outcome  <cit> . we addressed the effects of the addition of nb-hypo as risk factor on outcome prediction. rulex utilizes only categorical attributes. we divided the patients in groups through the application of k-means clustering of the patients based on the 62-probe sets signature of nb-hypo. two was the optimal number of clusters to partition the dataset as shown by the within cluster distance when varying the number of clusters . the  <dig> patients were clustered in two groups of  <dig> and  <dig> elements with low and high nb-hypo expression respectively .

we tested the effect on classification of including nb-hypo to the previously analyzed inrg risk factors and the results are shown in table  <dig>  the algorithm produced a  <dig> rules classifier of which rules  <dig> ,  <dig>  and  <dig>  cover  <dig> % of non overlapping poor outcome patients and rules  <dig> ,  <dig> ,  <dig> , and  <dig>  cover  <dig> % of non overlapping good outcome patients. we called this classifier initial classifier. each rule utilizes some, but not all, considered risk factors and nb-hypo was included in three rules demonstrating its potential in nb stratification. the rules classifying poor outcome patients have a very low error ranging from  <dig> % to 3% indicating a good performance of the algorithm in predicting this class. furthermore, rules  <dig>  and  <dig>  include patients with stage  <dig> neuroblastoma, a group that is traditionally very difficult to be stratified, demonstrating that nb-hypo is an important prognostic factor for high risk patients. in contrast, the classification of good outcome patients produced rules with variable error, including rule  <dig>  with 23% error. the statistical significance of each rule was assessed by fisher exact test demonstrating a high significance of every rule with the exception of rule  <dig>  that had also minimal covering. finally, we introduced a new parameter, the stability value, which provides an indication of the robustness of the rules. the stability of the rules was somewhat variable ranging from  <dig>  of rule  <dig>  to  <dig>  and we addressed this issue in detail . in conclusion, we demonstrated that nb-hypo could be successfully utilized to generate rules stratifying nb patients. interestingly, the prognosis of stage  <dig> patients took advantage of nb-hypo providing the first evidence of the importance of this new risk factor in stratifying high risk patients.

a the rule id is composed by the table number followed by a dot and the rule number.

b the covering is the fraction of examples in the training set that verify the rule and belong to the target class.

c the error is the fraction of examples in the training set that satisfy the rule and do not belong to the target class.

d fisher p-value quantifies the statistical significance of the rule.

e stability measures the fraction of the occurrences of a given rule in a  <dig> rounds of  <dig> fold cross validations.

the impact of the llm rules on the clinic is directly related to their stability. our results indicated a variation in stability that impacted on the "credibility" of the rules and we decided to investigate further the source of variability and, possibly, to improve the reproducibility of the rules in cross validation. rulex  <dig>  is programmed to include every patients in at least one rule as shown in figure  <dig> thereby creating rules with very limited coverage, like rule  <dig> , and potentially highly unstable. we utilized the stability statistical measure and the core procedure to identify and eliminate the instances responsible for rule instability according to the stabilization flowchart outlined in figure  <dig>  the process consists in an iterative procedure that removes the instances that generate instability. the process is repeated till each rule has the maximal stability value of one. the fist iteration applied to the initial  <dig> patients generated seven core rules . figure  <dig> shows the representation of the individual patients in the core rules. the stability of the core rules was clearly improved and four of them reached the one value. the major change caused by the introduction of core rules was that  <dig> patients  were not covered by any rule. these patients shared the same characteristics  but contained equal proportion of good and poor prognosis patients. it follows that these patients are not classifiable and their inclusion in either class depends only on the casual imbalance in the distribution between training and test set. another source of instability is the rule a <dig>   with only two patients whose casual association with training or test set would decide the prediction. the last case of instability regards rule a <dig>  , that is almost equivalent to rule a <dig>  with the exception of one instance that is present only in the rule a <dig> . in particular, being a <dig>  equivalent to rule  <dig>  and a <dig>  equivalent to the rule  <dig> , rulex generates a rule  <dig>  only when that unique example patient that is not covered by rule  <dig>  is present in the training set.

the above mentioned  <dig> patients were removed from the dataset as source of instability and a second round of stabilization  was started on the remaining  <dig> instances. the second iteration generated the second classifier having three out of five rules with a stability equal to one  and five core rules in which only one had a stability smaller than one . we identified and removed the patient that was the cause of this instability. in the third interaction we obtained the final classifier consisting of four rules each with the stability of one and lacking isolated instances . in conclusion, we identified and removed  <dig> patients causing instability corresponding to 15% of the dataset. the characteristics of the final classifier are shown in table  <dig>  the statistical significance of each rule is less than  <dig>  and the error is less than 4% . rules  <dig>  and  <dig>  cover 100% of non overlapping poor outcome patients and rules  <dig>  and  <dig>  cover 96% of non overlapping good outcome patients. rule  <dig>  is important because it classifies stage  <dig> tumors, and predicts poor outcome when nb-hypo is high. moreover, low nb-hypo is involved in predicting good prognosis of nb patients . we list the inrg risk groups of the patients included in the final rules based on age, mycn and stage. interestingly, hr patients are covered by distinct rules supporting the conclusion that nb-hypo allows the identification of new risk groups among the nb patients. the relative relevance of the risk factors included in the rules for the classification was computed and the results are shown in figure  <dig>  relevance for good outcome patients was inss stage  <dig> , nb-hypo  <dig> , mycn status  <dig> , age at diagnosis  <dig> ; the relevance for poor outcome patients was inss  <dig> , nb-hypo  <dig> , age  <dig> , mycn  <dig> . the relevances of nb-hypo and inss stage were very close but far from the other risk factors providing evidence of the relevance of nb-hypo in patients'classification.

a the rule id is composed by the table number followed by a dot and the rule number.

b covering is the fraction of examples in the training set that verify the rule and belong to the target class.

c error is the fraction of examples in the training set that satisfy the rule and do not belong to the target class.

d fisher p-value quantifies the statistical significance of a rule.

e stability is the fraction of the occurrences of a given rule in a  <dig> rounds of  <dig> folds cross validations.

f the risk assessment is based on inrg classification taking into consideration only inss stage, mycn status and age at diagnosis. hr = high risk; ir = intermediate risk; lr = low risk.

the last task was to test the final classifier g for conflicts. according to the procedure indicated in the material and methods section. we identified potentially conflicting rules in the following pairs of rules:  <dig>  vs  <dig>  and  <dig>  vs  <dig> . in fact, the pattern x having nb-hypo = low, inss =  <dig>  mycn = normal, and age ≥  <dig> verifies rules  <dig>  and  <dig>  with opposite predictions. similarly, pattern with nb-hypo = high, inss ∈ { <dig> }, mycn = normal, and age ≥  <dig> satisfies the conditions rules  <dig>  and  <dig> , with opposite predictions. however, such instances do not occur as it can be determine by the data in figure  <dig> that demonstrate the lack of any overlapping among patients included in the rules predicting good  and poor  outcome. there are no patients in a conflicting situation is that because such cases were removed by the stabilization procedure as causes of instability. the potentially conflicting situations could also be computed by the rules:

if nb-hypo = low and inss =  <dig> and mycn = normal and age ≥  <dig> then nc

if nb-hypo = high and inss ∈ { <dig> } and mycn = normal and age ≥  <dig> then nc

generated as described in the material and methods section. however, the use of the stabilization procedure is superior because it identifies correctly not only  <dig> patients in a conflicting situation but also  <dig> additional instances causing instability.

the process of stabilization of the rules is associated with an improvement of the classification parameters. figure  <dig> shows a comparison of accuracy, recall, precision, specificity and negative predictive value among the initial classifier generated on the whole  <dig> patients dataset, the second classifier obtained after two cycles of stabilization and the final classifier. every indicator of the classifier performance improved concomitantly with the strengthening of the rules' stability with the exception of the proportion of misclassified that decreased to null values as expected by the nature of the stabilization workflow .

 <dig> fold cross validation is a powerful indicator of the performance of our classifier in the  <dig> patients' dataset. however, the stabilization workflow called for removal of patients from the dataset raising the question of the performance of the algorithm on an independent dataset of neuroblastoma patients. we tested the predictions of the final classifier  on an independent  <dig> patients' dataset . we deleted from the  <dig> patients' dataset  <dig> instances  with the same characteristics  as those eliminated from the  <dig> dataset because source of instability. the performance of the final classifier was then tested on the remaining  <dig> instances. the results are shown in table  <dig>  the final classifier demonstrated a good performance on the validation set resulting in 86% accuracy, 91% recall, 90% precision and 70% specificity and negative predictive value demonstrating the validity of our approach and its reproducibility on an independent dataset. these statistical measures of the performance are comparable, but somewhat lower to those obtained in cross validation .

a accuracy is the fraction of correctly classified patients and overall classified patients.

b recall is the fraction of correctly classified good outcome patients and the overall predicted good outcome patients.

c precision is the fraction of correctly classified good outcome patients and the predicted good outcome patients.

d specificity is the fraction of correctly classified poor outcome patients and the overall poor outcome patients.

e negative predictive value is the fraction of correctly classified poor outcome patients and the overall predicted poor outcome patients.

f final classifier indicates the classifier in table  <dig> obtained utilizing rulex.

g inrg risk factors classifier indicates the classifier in table  <dig> obtained utilizing rulex.

h inrg pretreatment schema classifier indicates the inrg pretreatment classification schema.

vlr = very low risk; hr = high risk; ir = intermediate risk; lr = low risk. we computed the performance of inrg pretreatment schema classifier on the  <dig> patients' cohort as follows. 

we assigned very low, low, intermediate, high risk utilizing the inrg consensus pretreatment classification schema considering only the age at diagnosis, inss stage and mycn status risk factors.

we then created two groups to make the risk assignment comparable with outcome.

the first included very low, low and intermediate risk patients and the second included high risk patients.

the first group was associated with a favorable outcome and the second was associated with an unfavourable outcome.

the performance of the classifier obtained considering only mycn, age and inss stage without the contribution of nb-hypo   on the  <dig> patients' dataset was then measured and the results are shown in table  <dig>  accuracy, precision, specificity were remarkably similar. the inclusion of nb-hypo in the classifier increased somewhat recall and negative predictive value indicating an improvement of the classification of good outcome patients and better precision in predicting poor outcome patients.

finally, we compared the final classifier with that of the inrg consensus pretreatment classification schema on the  <dig> patients' dataset. to attempt this comparison we had to classify the patients in risk groups using only  <dig>  out of more than  <dig> parameters considered by inrg and to merge low, very low and intermediate risk patients into one group to have a binary classification. the results are shown in table  <dig>  comparing the final classifier with the inrg classification schema, we observed similar accuracy but differences in the other classification parameters. the final classifier demonstrated an improvement in the classification of good outcome patients  and a better precision in predicting poor outcome patients , while inrg classification schema demonstrated a better classification of poor outcome patients  and a better precision in predicting good outcome patients . however, these considerations provide an indication of the relationship among these classifiers but they will have to be validated on a larger dataset.

in conclusion, iterative application of stabilization procedure generates fully stable rules that are a very powerful tool to classify nb-patients. figure  <dig> shows the workflow of the main steps performed in our rule generation and the references to the relevant tables and figures. the stability of the rule is achieved at the expenses of including every patient in the analysis, an accepted and common practice when deciding on the rules for recruiting patients in a clinical trials.

discussion
we developed explicit rules, predicting the outcome of neuroblastoma patients on the bases of age at diagnosis, mycn amplification, inss stage and nb-hypo signature. we demonstrate that llm algorithms generate clinically relevant rules and that stabilization of these rules through a novel procedure, improves the performance stratifying high risk patients traditionally difficult to classify. furthermore, the use of nb-hypo, a biology driven risk factor, identifies a cohort of poor prognosis patients as potential target of new therapeutic approaches perhaps aiming at counteracting tissue hypoxia.

classification is central to stratification of cancer patients into risk groups benefiting of defined therapeutic approaches. several statistical and machine learning techniques have been proposed to deal with this issue  <cit> . each of them trains a specific model that is used to predict the output for new unlabeled data. we privileged classification methods capable of constructing models described by a set of intelligible rules for their immediate translation in the clinical setting and for the possibility to incorporate previous medical knowledge in the algorithm. most available techniques, such as linear discriminant approaches, multilayer perceptrons or support vector machines, are able to achieve a good degree of provisional accuracy, but construct black-box model whose functioning cannot allow to derive information about the pathology of interest and its relationships with the considered diagnostic and prognostic factors. different classification methods capable of constructing models described by a set of intelligible rules  were developed to overcome this drawback.

rule generation techniques produce not only the subset of variables actually correlated with the pathology of interest, but also explicit intelligible conditions that determine a specific diagnosis/prognosis. as a consequence, relevant thresholds for each input variable are identified, which represent valuable information for understanding the phenomenon at hand. in general, each rule refers to a specific target output class and it is characterized by two statistical measures: the covering, which accounts for the fraction of examples in the training set that verify the rule and belong to the target class, and the error, which is given by the portion of patterns in the training set that satisfy the rule and do not belong to the target class. we introduced a third statistical measure, the stability, as a prerequisite for acceptance of a set of rules.

most used rule generation techniques belong to two broad paradigms: decision trees and methods based on boolean function synthesis. the approach adopted by the first kind of algorithms divides iteratively the training set into smaller subsets according to a divide and conquer strategy: this gives rise to a tree structure from which an intelligible set of rules can be easily retrieved. at each iteration a portion of the training set is split into two or more subsets according to the results of a test on a specific selected input variable. the target is to obtain at the leaves of the tree structure portions of the training set belonging to the same output class. these portions of training set are, in general, non-overlapping  <cit> . decision tree methods usually provide simple rules, which can be directly interpreted by an expert, and require a reduced amount of computational resources. however, the accuracy of the developed models is often poor when compared with that exhibited by black-box techniques. the divide and conquer strategy leads to conditions and rules that point out the differences among examples of the training set belonging to different output classes. it follows that decision tree approach implements a discriminant policy: differences between output classes are the driver for the construction of the model. several reports emphasized the unstable characteristic of the decision tree, quantifying the stability of a classifier or algorithm using syntactic or semantic stability notions  <cit> .

in contrast, methods based on boolean function synthesis adopt an aggregative policy: at any iteration some patterns belonging to the same output class are clustered to produce an intelligible rule. suitable heuristic algorithms  <cit>  are employed to generate rules exhibiting the highest covering and the lowest error; a tradeoff between these two different objectives has been obtained by applying the shadow clustering  technique  <cit>  which leads to final models, called logic learning machines , exhibiting a good accuracy. the aggregative policy allows to retrieve intelligible rules that better characterize each output class with respect to approaches following the divide-and-conquer strategy. clustering examples of the same kind permit to extract knowledge regarding similarities about the members of a given class rather than information about their differences. this is very useful in most applications and leads to models showing a higher generalization ability, as pointed out by intensive trials performed on different datasets through sc  <cit> .

llm is an efficient implementation of the switching neural network  model  <cit>  trained through an optimized version of the sc algorithm. llm, snn and sc have been successfully used in different applications: from reliability evaluation of complex systems  <cit>  to prediction of social phenomena  <cit> , form bulk electric assessment  <cit>  to analysis of biomedical data  <cit> . in particular, in this last field the ability of generating models described by intelligible rules have carried out many advantages, allowing to extract important knowledge from available data. identification of prognostic factors in tumor diseases  <cit>  as well as selection of relevant features microarray experiments  <cit>  are only two of the valuable targets achieved through the application of llm and snn. the learning approach implemented by llm privileges the covering but this is not the unique possible approach. in the present work we privilege stability over covering and we identify a procedure to reach this aim.

application of llm to nb patients dataset containing the common risk factors mycn, age at diagnosis, and inss stage, generated rules that fit with the inrg risk assessment demonstrating the concordance of the results of llm with previous clinical knowledge of neuroblastoma. we then included nb-hypo attribute to the classification algorithm and we demonstrated that llm considered this risk factor relevant for outcome prediction. this result was important because it showed that nb-hypo could generate clinically relevant rules identifying previously unrecognized homogeneous groups of patients.

clinical decisions require stability of the rules because there must be a reasonable confidence that the model is consistent and that the predictions will be insensitive to small changes in the dataset. for this reason we decided to adopt suitable methods to improve the stability of the rules even at the expenses of coverage. to this end, we developed a workflow to skim the dataset of unstable instances maximizing the stability of each rule. the stabilization procedure is iterative and it was terminated only when each rule has reached the maximal stability. the approach depends on the existence of an overlap among rules and it applies only to situations in which examples can be covered by more than one rule. this excludes the application of this kind of analysis to the decision tree representation that fragments the dataset into disjunctive subsets.

iterative rules stabilization generated the final classifier with four rules each endowed with the maximal stability of one. the final classifier predicts with 3% error that nb-hypo high, stage  <dig> , <dig>  older that  <dig> year patients have poor outcome prediction . stage  <dig>   <dig>   <dig> patients older than  <dig> year are currently classified as either low, intermediate and high inrg risk and follow different therapeutic protocols. our results indicate that evaluation of the nb-hypo profile can lump some of these patients into a single risk group characterized by highly hypoxic tumors therefore, candidate for new hypoxia targeted treatments substituting the current inefficient therapies.

nb-hypo and inss stage had similar relevance in the generation of rules. inss a critical parameter in neuroblastoma research and our data indicate that that nb-hypo is an emerging risk factor in determining the outcome. however, neither inss nor nb-hypo reach the relevance value of  <dig> pointing to the need of multiple risk factors for classification.

the possibility to blend previous clinical knowledge of neuroblastoma disease with newly discovered prognostic signatures was central to the choice of the algorithm. the relevance of merging gene expression profiling, histopathological classification was reported  <cit> . several studies deal with merging of genomic signatures, gene expression profiles, gene mutation, genomic instability, histopathologic and clinical classification systems in various combination for cancer classification, as exemplified by some publications  <cit> . the characterization of the tumor at diagnosis is indispensable for deciding the treatment and the nb-hypo may indicate the tumors that, as a result to the hypoxic status, express high genetic instability  <cit>  contain undifferentiated or cancer stem cells  <cit>  or a higher metastatic potential  <cit> . these characteristics of the primary tumor may be those that initiated the aggressiveness of the disease and could be targeted by individualized treatment. many therapeutic agents are being developed to target hypoxia  or cells in a hypoxic environment with gene therapy  <cit>  and are being tested in the clinic.

the acquired stability of the rules traded off a reduced coverage causing the exclusion of  <dig> out of  <dig> patients from the dataset. this process is similar to that used to design clinical protocols that is based upon selective recruitment of a narrow group of patients with similar expression of risk factors. there were obvious reasons to exclude some patients from the database. twenty four instances shared the same attributes but were equally distributed in the two outcome classes. under such conditions the classification was entirely dependent upon the casual imbalance of the distribution of such patients in the training set. similar consideration applied to the other patients that were excluded from the dataset to improve the stability of the rules. we conclude that our system generated a third class of non classifiable patients. from a clinical perspective, there is no loss in excluding these patients from the classification because no significant prediction could be made by this or any other algorithm on such an ambiguous cohort. in contrast, a clear gain is achieved by strengthening the stability of the rules that cover the remaining 85% of the patients. we identified instances that caused instability on our dataset but we do not exclude that others possible causes of instability could emerge in other situations and studies are in progress to address these issues.

removal of the instances that caused instability was instrumental to prevent theoretical conflicts in the classification. superficial inspection of the final classifier leads to the conclusion that there may one group of patient characterized by opposite prediction. however, such instances do not occur in the dataset after removal of the instability cases. therefore it is mandatory that the process of generating the rules of the final classifier be coupled with the stability analysis and removal of instability generating instances. it is noteworthy that instability is not limited to potentially conflicting instances but to other situations including a very low covering. in this case, the casual partition of such instances between training and test sets is the true gauge for outcome prediction.

rulex can be tuned to perform in such a way that it associates every instance with a prediction. under such constrains it deals with conflicting situation by favoring the rule that has the maximal covering among the various possibilities. we did not feel that the covering evaluation was a sufficient indicator to classify patients outcome. for this reason we preferred to choose the stabilization workflow to identify the truly stable rules.

the performance of the final classifier was validated in an independent dataset to exclude the danger of overfitting caused by the removal of instability instanced and testing in cross validation in the purged dataset. the performance statistical measures in the test set were quite satisfactory demonstrating the efficacy of the classifier in an unrelated dataset. the values were somewhat lower than those observed in cross validation. interestingly, the percentage of instances generating instability were the same in both  <dig> and  <dig> patients' datasets. these performance statistical measures are comparable, but somewhat lower to those obtained in cross validation. the relatively small number of poor outcome patients in the testing set may account for the reduced specificity observed. a larger testing data set would be needed to obtain a more accurate assessment of the performance.

CONCLUSIONS
the novelty of our work is to target stability, explicit rules and blending of risk factors as the characterizing elements to generate classification rules for nb patients to be conveyed to the clinic and to be used to design new therapies.

our first aim was to explore algorithms that could generate intelligible classification rules of neuroblastoma patients easily translatable into the clinical setting. we found that llm implemented by rulex, was a reliable algorithm, generating rules that paralleled the risk stratification of nb patients obtained by clinical previous knowledge.

the second major task was to develop a prognostic classifier of nb outcome capable to blend existing clinical and molecular risk factors derived from previous clinical knowledge of the disease with the newly discovered prognostic nb-hypo signature assessing the hypoxic status of the neuroblastoma tumor. we found that nb-hypo could be successfully associated to other known risk factors to generate relevant prognostic rules capable to stratify high risk patients.

we identified stability as a critical factor in the translation of the classifier into clinic and we developed a framework to maximize the stability of the rules at the expenses of the coverage. the final product was a very stable classifier covering 85% of the patients comprising the original dataset. this procedure can be easily extended to other classifiers provided that the instances are covered by more than one rule.

from the biology standpoint we found that nb-hypo is an important risk factor for neuroblastoma patients that can help to resolve high risk stage  <dig> patients as well as those with good prognosis. we propose that the final classifier derived in the present work be utilized as bases for designing new therapies needed for the poor prognosis patients correctly included in the nb-hypo containing rules of our final classifier.

