BACKGROUND
machine learning  techniques are widely used in the analysis of high-throughput data to answer a broad range of biological questions. applications in the field of medicine have transformed our understanding of complex genomic interactions and measurements  <cit> . ml has been successfully applied to biological disciplines including proteomics  <cit> , drug development  <cit> , dna sequence analysis , cancer classification , clinical decision making  <cit> , and biomarker discovery  <cit> . the versatility of ml algorithms to broad ranges of data and applications offers powerful, yet generalizable solutions to biological questions.

recently, the random forest  algorithm  <cit>  for ml has achieved broad popularity. rf is a form of ensemble learning and possesses several characteristics that impart versatility. it can be applied to two-class or multi-class prediction problems, model interactions among variables, can take on a mixture of categorical and continuous variables, provides variable importance measures , and has good predictive performance even for data with more variables  than samples ; potentially involving highly noisy and significantly correlated variables  <cit> . due to their non-parametric nature, rfs are fairly robust with relatively straightforward applications for inexperienced users  <cit> . consequently, this algorithm has expanded to a framework of models  <cit> .

to train a random forest model, a bootstrap  <cit>  sample is drawn, with the number of samples specified by the parameter sampsize  <cit> . by default, the bootstrap sample has the same number of samples as the original data: some samples are represented multiple times, whereas others are absent, leading to approximately 37 % of samples being absent in any given tree. these are referred to as the out-of-bag  samples  <cit> . independent of the sampsize setting, after each sample is drawn, a decision tree is created. in the most commonly-used implementation, fully-grown or unpruned decision trees are created  <cit> . the number of trees is denoted by the parameter ntree  <cit> . this collection of models is known as bootstrap aggregation or bagging  <cit>  and is commonly applied to high-variance and low-bias learners such as trees  <cit> . since individual trees are more prone to over-fitting than a collection of trees, an ensemble method has a significant advantage  <cit> ; however, this is limited by the correlation between the trees and can be mitigated by choosing a number of randomly selected input variables at each split of the tree. the number of random variables used at each split is denoted by the parameter mtry. of this subset of randomly selected variables, the one that forms the best split is selected  <cit> . the best split is selected on the basis of a specific objective function, most typically maximization of the gini coefficient or total gain in purity. this produces the most homogeneous groups and lowest oob error  <cit> . several empirical studies have shown the benefit of aggregating multiple trees to create a strong learner whereas, independently they would be considered unstable with lower classification accuracy .

machine learning algorithms frequently require estimation of model parameters and hyper-parameters, commonly through grid-searching  <cit> . surprisingly, though, this is not common practice in the literature for rfs, where default values are often used as it is widely believed that this method is parameter-insensitive, or at least robust to changes from default parameter settings . to test this assumption, we performed an exhaustive analysis of the parameter-sensitivity of rfs in two large, representative bioinformatics datasets. we show that our top performing tuned models were able to achieve greater prediction accuracies than the default models for both datasets and that the performance of the default parameterization is inconsistent. this emphasizes the value of per-dataset tuning of rf models.

RESULTS
experimental design
to evaluate the sensitivity of rf models to parameterization, we selected two datasets representative of those commonly used in computational biology. the first studies quality-control metrics in next-generation sequencing  <cit>  and comprises  <dig> features  with  <dig> training samples and  <dig> validation samples, and thus reflects low p/n ratio studies. each sample was classified as “good library” or “bad library” based on information external to the  <dig> features, and our models aimed to predict this binary response variable.

the second dataset reflected high p/n studies and comprises three categorical clinical variables and  <dig>  continuous mrna abundances for non-small cell lung cancer  patients  <cit> . we trained models to predict patient outcome, “no death” or “death”. there were  <dig> samples in the training cohort.

for both datasets, we performed two model-fitting steps . first, we selected a broad and comprehensive range of parameters , and trained a rf classification model for each combination, including the default parameters. models were trained on the training dataset and validated on a fully independent dataset. performance was scored using the area under the receiver operating characteristic curve   <cit> . second, we fit an rf regression model using the data from the previous step: parameters were set as the covariates and auc as the response. this allows us to characterize the association between prediction accuracy and parameterization. we randomly sampled 2/ <dig> of parameter sets for training and reserved the remainder for validation. we aimed to predict the withheld auc scores and assessed performance using spearman's rank correlation coefficient  and lin's concordance correlation coefficient .fig.  <dig> experimental design. classification-based model fitting began with a unique combination of n
tree, m
try, and sampsize parameters in conjunction with training data, illustrated by the gray boxes. each learned random forest model was used to predict the class of the validation data. subsequently, auc scores were calculated using the true class labels and these values were randomly subsetted into training and validation groups using 2/ <dig> and 1/ <dig> of the samples, respectively. in the second model fitting step, we evaluated whether auc could be predicted from parameter sets alone. a rf regression model was fit using the parameters n
tree, m
try, and sampsize as variables and auc as the response, illustrated by the blue boxes. default settings were selected to train the rf regression models and auc scores were predicted for the validation data. we evaluated the results using spearman's and lin’s correlation and determined the relative importance of each variable



prediction accuracy is a strong function of parameterization in low p/n studies
we first evaluated the parameter sensitivity of rf prediction accuracy in the low p/n dataset. we created  <dig>  different sets of parameters and evaluated the performance of each. most models succeeded at this task , with a median auc of  <dig>  and 96 % of models exceeding  <dig>  auc. however, the performance varied dramatically, with a range of  <dig> – <dig> , suggesting that some parameterizations greatly improve or hinder prediction accuracy. the default parameterization  performed well, with an auc of  <dig>  and ranked in the top 12 % of all models . this clearly demonstrates that the default settings are reasonable, but not optimal.fig.  <dig> prediction accuracy is a strong function of parameterization in low p/n studies. summary of low p/n predicted votes for each fitted random forest model . an auc plot is provided at the top indicating the relative performance of each model, represented by each column. each model was fitted from a unique combination of n
tree , m
try  and sampsize parameters  and their respectively outcomes  for each sample or row . votes are provided in values from 0– <dig> with  <dig> representing a “bad library” and  <dig> representing a “good library”. all columns are ordered in descending order of auc scores and rows are ordered in descending order of the fraction of correct votes for a given sample . all samples were subsetted according to the true class labels “good library” and “bad library”, though the votes may not be reflective of this. barplots for vote fractions are provided on the right of the main heatmaps and the values for each parameter are provided at the bottom of the figure. the n
tree parameter is illustrated in blue, m
try in magenta and sampsize in orange. lighter hues represent lower values with darker hues indicating higher values. a scatterplot in the bottom right corner illustrates a strong negative correlation between the m
try parameter with auc scores 



we asked if models were consistently struggling with the same samples. we looked for samples in the validation dataset where at least 50 % of models trained with different parameter sets made incorrect predictions. in total 73/ <dig>  of validation samples were difficult to classify. these were strongly asymmetrically distributed between the classes with 72/ <dig>  “good library” validation samples difficult to classify relative to only 1/ <dig>  “bad library” validation samples . interestingly though, the global error rate was not dramatically different between these two groups .

parameterization was strongly correlated to auc score  in this dataset, but tightly focused on specific parameters. the number of variables sampled per node  was strongly negatively correlated with auc  and mtry ≤  <dig> resulted in higher classification accuracy . in contrast, models were relatively robust to changes in the ntree and sampsize parameters .

to further explore the relationship between parameterization and performance, we univariately compared performance within each parameter , with benjamini-hochberg adjustment for multiple-testing  <cit> . while sampsize values did not differ significantly from each other, however, ntree of  <dig> had significantly lower aucs  than other setting . similarly, as noted above there was a near-linear relationship between increasing mtry and decreasing auc in the validation cohort . these findings illustrated the strong influence of parameter selection on classification accuracy, and that both linear and threshold effects can be observed.

while the results to this point demonstrate both that parameterization powerfully influences prediction accuracy and that the default parameter settings are sub-optimal. however they do not demonstrate if it is possible to improve upon the defaults via parameter-optimization studies. we therefore implemented 10-fold and stratified 10-fold cross-validation using the parameters in additional file  <dig>  the data was randomly divided into  <dig> even folds, using 9/ <dig> folds for training and the last fold for validation. this step was repeated so that each fold was used for validation once, so that the number of samples in validation was equal to the number of samples in the original training set . all validation folds were pooled to evaluate auc and cross-validated models were compared to non-cross-validated models using spearman's ρ and lin's ρc .

predicted classes for both 10-fold cross-validation and stratified 10-fold cross-validation were weakly, but statistically-significantly correlated to the predicted classes for non-cross-validated results , and strongly correlated to one another .

we found that cross-validation and stratified cross-validation resulted in 97 % of models having an auc of  <dig>  including the defaults. we used an additional metric, root mean squared error  to break ties. the optimal model in 10-fold cross-validation  had a rmse of  <dig> , whereas the default model  had a rmse of  <dig> . the optimal model in stratified 10-fold cross-validation  had a rmse of  <dig> , whereas the default model  had a rmse of  <dig> . overall, we found that 39 %  and 21 %  of models outperformed the untuned model , respectively. twenty one percent  of these models shared the same parameter values and were found to perform better than the default settings in both cross-validated and non-cross-validated results. we found the addition of a second metric, rmse useful in breaking ties and assessing model performance for low p/n data.

prediction accuracy can be a strong function of parameterization in high p/n studies
to contrast these data, we examined the effects of parameterization on prediction accuracy for high p/n data  <cit>  . we created  <dig>  different sets of parameters and evaluated the performance of each . again, we saw that model performance varied greatly with parameterization with a median auc of  <dig>  and 2 % of models exceeding an auc of  <dig>  . however, the performance varied dramatically, with a range of  <dig> – <dig> , suggesting that some parameterizations could greatly improve or hinder prediction accuracy. the default parameterization  performed well relative to other models, with an auc of  <dig>  and ranked 10th. this demonstrates the near optimal performance of the default settings.fig.  <dig> prediction accuracy is a strong function of parameterization in high p/n studies. summary of the predicted votes for the combined validation data for each fitted random forest model . a barplot for auc scores is provided at the top indicating the relative performance of each model, represented by each column. each model was fitted from a unique combination of n
tree , m
try  and sampsize parameters  and their respectively outcomes  for each sample or row . votes are provided in values from 0– <dig> with  <dig> representing a “no death” event and  <dig> representing a “death” event. all columns are ordered in descending order of auc scores and rows are ordered in descending order of the fraction of correct votes for a given sample . all samples were subsetted according to the true class labels “death” and “no death”, though the votes may not be reflective of this. on the right of the main heatmaps are respective barplots for vote fractions and a heatmap of parameter values is present at the bottom of the figure. the n
tree parameter is illustrated in blue, m
try in magenta and sampsize in orange. lighter hues represent lower values with darker hues indicating higher values. to the right of this is a scatterplot illustrating spearman's correlations of each parameter with the auc scores; positive correlations were observed for the parameters n
tree, m
try, and sampsize 



we asked if models were consistently struggling with the same samples. we looked for samples in the validation dataset where at least 50 % of models trained with different parameter sets made incorrect predictions. in total 89/ <dig>  of validation samples were difficult to classify. these were symmetrically distributed between the classes with 37/ <dig>  “death” events difficult to classify compared to 52/ <dig>  “no death” samples . the error rate was significantly different between these two groups .

parameterization was strongly correlated to auc in this dataset, with contribution from all parameters. we observed that mtry  was the most correlated, followed by ntree  and sampsize .

to further explore the relationship between parameterization and performance, we univariately compared performance within each parameter , with benjamini-hochberg adjustment for multiple-testing  <cit> . we observed that larger ntree values resulted in higher prediction accuracy and reduced performance variability compared to lower values , with no significant difference observed between values ntree ≥  <dig>  . similar results were observed for sampsize and mtry  where there was a near-linear relationship between increasing parameter values and auc in the validation cohort. additionally, no significant differences were observed in auc for sampsize ≥  <dig> and mtry ≥  <dig>  the mtry value here is notable since it was used as the default, providing some support to previous claims that the default performs well. these findings illustrated the strong influence of parameter selection on classification accuracy, and that both linear and threshold effects can be observed.

parameters can be used to predict performance
having shown that model performance is strongly influenced by ntree, mtry, and sampsize, we next asked how strongly these three parameters could predict auc directly. we assessed variable importance using the gini vim, where larger values indicate a variable is more important for accurate classification. we were able to predict aucs using this metric that closely reflects those of the true data for low p/n data . we observed that mtry demonstrated the highest gini vim for low p/n data .

similar results were observed for the high p/n data, where prediction accuracy was a strong function of parameter selection across all validation sets . interestingly, the parameters demonstrated relatively balanced importance measures with sampsize demonstrating the highest gini vim and ntree with the lowest .

importance ranks can be sensitive to parameter changes
finally, we asked if parameterization change could alter the identification of importance variables   <cit> . we focused on the low p/n data, and trained models using the settings in additional file  <dig> and ranked permutation vim for each quality metric from 1– <dig>  with  <dig> representing the most important variable. permutation vim is the mean decrease in classification accuracy after a random variable is removed from model fitting. larger values suggest a variable has more discriminative power  <cit> .

variables differed in their sensitivity to parameter changes when evaluating variable importance . the variable “average reads/starts” was robust against parameter changes and was considered the most important in 94 % of all samples, whereas “clusters” exemplified strong parameter sensitivity and was positively correlated to mtry. on the other hand, “% bases ≥ 50 %” was found to have higher vims with lower mtry values.fig.  <dig> importance ranks can be sensitive to parameter changes in low p/n studies. summary of the variable importance ranks for each sequencing metric . an auc plot is provided at the top indicating the relative performance of each model, represented by each column. each model was fitted from a unique combination of n
tree , m
try  and sampsize parameters  and their respectively outcomes  for each metric. each column of the main heatmap corresponds to a model's importance values, and were ranked from 1– <dig>  where  <dig> represented the most important feature and  <dig> the least. the importance values were ordered according to previously calculated auc scores using predicted vote and true class labels. each row represents a metric and are ordered according to the mean rank of its importance values. the importance values were simplified in the main heatmap and illustrate four groups only. blue indicates a rank of  <dig>  green a rank of  <dig>  gold a rank of  <dig>  and beige a rank of  <dig> and greater. a summary of overall rank groups for a particular metric are illustrated in a barplot on the right of the main heatmap and a covariate heatmap with all parameter combinations is illustrated at the bottom of the plot. the n
tree parameter is illustrated in blue, m
try in pink and orange for sampsize in orange. some parameters demonstrate robust behaviour to parameter changes such as “uncollapsed coverage” and “% bases ≥  <dig> quality”, which were ranked between 11– <dig> inclusive in 96 % and 95 % of all samples, respectively. these variables possessed vims that suggested they were less influential on classification accuracy. yet, “average reads/starts” was insensitive to parameter changes and was considered the most important variable. another variable “clusters” was parameter sensitive, illustrating that variables vary in their sensitivity to parameter changes which can ultimately influence classification accuracy



our order for variable importance deviated from that of the original study  <cit> , where “% bases ≥ 8×” was reported as the most discriminative variable. we examined how variable importance changed with differing ntree values  while holding mtry and sampsize constant  and observed that larger ntree values led to more stable vims.

discussion
there are two common assumptions regarding rf models. the first is that the default parameters lead to good performance  <cit>  and the second is that the algorithm is robust to parameter changes  <cit> . to help quantify the wide-spread nature of these assumptions we manually reviewed all papers published in bmc bioinformatics between january  <dig>   <dig> and november  <dig>   <dig> . we looked for papers that referenced the canonical rf paper  <cit>  during this ~11 month period. of the  <dig> papers that implemented rfs, exactly half performed a parameterization study to optimize parameters, and only 5/ <dig> papers reported the final parameter setting used. that is, about half of rf-studies could benefit from improved parameterization and another third from improved reporting. this highlights clearly the gap between machine learning theory and practice, and gaps in methods reporting that are not being caught by peer-review.

parameterization is difficult and its absence from the model fitting process may be due to limited experience, a lack of readily available heuristics or limited resources  <cit> . consequently, these factors lead to the inappropriate selection of parameters or lack thereof, directly influencing learning  <cit> . we sought to determine the effects of parameterization on classification accuracy and variable importance measures. our findings suggested data-dependent parameter sensitivities ultimately influence classification accuracy and vims for binary classification problems. our findings may not extend to regression analyses or multi-class problems, where the relationship between the variables and response is much more complex.

we observed that the default parameters have the potential to perform well, however results across all tests indicated that parameter tuning enabled higher model performance. the majority of high performing parameter combinations did not coincide with general patterns observed in the pattern selection process i.e., in most samples higher parameter values led to greater classification accuracy and the top performing parameters had lower values. such models may have performed well due to random chance or were over-fit. these results emphasize the importance of parameter tuning and how one cannot rely on any arbitrary parameter set to perform well. this also suggests that existing publications implementing untuned models may improve classification accuracy through model tuning. to reduce computation time and work for parameter selection, we applied a rf regression model, which predicted model performance more accurately than the more expensive 10-fold cross-validation and stratified 10-fold cross-validation. the rf regression model was also better at discriminating poor performing parameter sets from high performing parameter sets.

to our knowledge, this is the first computational genomic study that addresses parameter sensitivities using a comprehensive range of values for two unique biological data types. in particular, we observed that the low p/n data was sensitive to changes in mtry and the high p/n data demonstrated a synergism between all three parameters. additionally, not all variables exhibited robust behaviour towards parameter changes when determining vims . these findings challenge the assumption that rfs are relatively robust. parameters that did not a play key role independently had an observable and significant synergism when constructing rf regression models with interaction terms .

we also noted that our variable importance ranks did not coincide with  <cit> . this was largely explained by the bias in feature importance for the rf algorithm. variables that were highly correlated to truly influential variables or have more categories will be over-selected by the algorithm and do not reflect the true relative contribution of a variable in a classification or regression problem  <cit> . chong et al.  <cit>  implemented an alternate algorithm, “cforest”, from the r package “party” to generate unbiased vims. one area for future research is to investigate the sensitivity of parameter changes in the “cforest” algorithm.

moreover, characteristics of the data, such as, p > > n and minor class imbalances were observed. the numerous variables in the high p/n data constrained the selection range of mtry parameters, potentially confounding the results. in such samples, mtry ≠ p. this was not the sample for the low p/n data, where we were able to test all possible values of mtry. this limitation may also be viewed as beneficial since the number of randomly selected variables at each split is constrained and therefore, limits tree correlation within a forest.

an additional data characteristic limiting the classification accuracy in rf could be class imbalance  <cit> . the unequal number of classes in a dataset is technically considered class imbalance, however, in the scientific community, class imbalance corresponds to data with significant to extreme disproportional class numbers, such as, 100: <dig> or  <dig> : <dig>  <cit> . these types of “imbalanced data” were not considered here. furthermore, the minor classes “bad library” and “death” in the small p/n data and high p/n data respectively, had a higher classification accuracy suggesting, in some instances, the heterogeneity of a sample is more influential on classification accuracy. we also aimed to mitigate class imbalance effects through stratified sampling and by using the auc performance metric. alternate methods such as, cost sensitive learning  <cit>  and artificially balancing the data through down sampling the majority class  <cit> , over sampling the minority class  <cit> , or both  <cit>  have been shown to deal with class imbalance effectively. artificial balancing ensures that class priors are equal in tree classifiers and that the minority class is included in the bootstrap sample. on the other hand, cost sensitive learning incurs a greater cost for misclassified minority samples over majority samples. minor class imbalances were not observed to be an issue in this study, however, data should be analysed with caution in highly imbalanced studies.

CONCLUSIONS
we analysed the effects of parameterization using exhaustive selection methods and showed that tuning can be successfully applied to a non-parametric machine learning algorithm to improve prediction accuracy. although we only examined two different genomic datasets, we observed that parameter sensitivities are data-specific, necessitating per-dataset tuning. our findings illustrate this through discordant correlations between parameters and performance scores for low p/n and high p/n data. the model fitting process is a fundamental step in machine learning and careless parameter selection can lead to sub-optimal models and potentially missed findings.

