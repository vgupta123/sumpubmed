BACKGROUND
the need for data integration started when the number of applications and data repositories began to grow rapidly. the first approaches appeared in the 80s, and formed the basis for the research in this area. the evolution continued over mediator based systems, such as amos ii  <cit> , disco  <cit> , tsimmis  <cit>  and garlic  <cit> . then, agent technology was used in some systems like infosleuth  <cit>  and momis  <cit> . in recent times, the new technologies appearing have been used in data integration: extensible markup language, xml , and ontologies .

the rapid growth of the internet has provided users with access to an unprecedented number of heterogeneous information sources. this huge amount of information and the complexities of handling it have given rise to a lot of research concerning practical approaches to the semantic web.

semantic web searches have been based on existing systems, and the proposed approaches offer a limited amount of information for agents. search engines cannot interpret all the information available because many documents have not yet been semantically annotated. we propose the use of an ontology-based mediator framework  to access varied information from diverse biological databases  <cit> . komf has been successfully instantiated in the context of molecular biology for integrating data sources  <cit> .

this application can be used to extract integrated information from the set of databases included in the system, information which is retrieved as a set of ontology instances. however, the analysis of these instances is still limited in komf. in order to apply analysis tools it is necessary to store the instances appropriately to facilitate their access. however, the sheer number of instances that must be retrieved make the use of a traditional reasoner unfeasible  <cit> . thus, we propose the use of dbowl  <cit> , a persistent and scalable reasoner that is able to deal with this large number of instances. it stores the ontologies in a relational database, using a description logic reasoner to pre-compute the class and property hierarchies, and to obtain all the ontology information , which is also stored in the database. furthermore, a simple but expressive query language has been implemented, which allows us to query and reason on these ontologies. this reasoner implements both tbox  queries and abox  inferences. tbox queries can be evaluated directly using the query language. abox inferences however are evaluated when a query is sent to the system to obtain complete results. both tbox queries and abox inferences are implemented using only the information stored in the database.

in summary, the goal of this paper is to present a user query system based on combining a data integration solution with a reasoner, to boost the analysis potential for the knowledge obtained in response to user queries. the combination of a data integration system with a reasoner is a novel approach that opens up new ways of analyzing the information based on the knowledge. this is also the way to obtain a mediator which can reason on the integrated knowledge.

this process has been used to implement a demo tool  showing how the biopax level  <dig> ontology can be used as the integration schema to integrate uniprot  <cit> , kegg  <cit> , chebi  <cit> , brenda  <cit>  and sabiork  <cit>  databases.

previous works
this section describes the two previous works on which the proposal is based. first, we will show the main features of komf and how it can be configured to integrate biological data. then, we will describe dbowl, a persistent and scalable reasoner.

• komf
in this section, we briefly describe an ontology-based mediator framework  which uses a semantic directory , a generic infrastructure to register and manage ontologies, their relationships and also information relating to the resources. in the proposed framework  our goal is to provide access to the data using a common data model, and a common query language. our architecture provides a semantically coherent model representation of the combined data from the wrapped data sources and transparent access to the combined data through queries to the mediating view.

in this context, wrappers are an important part of the internal elements of data services  <cit> . a wrapper accepts queries from the mediator, translates the query into an appropriate query for the individual source, performs any additional processing and returns the results to the mediator. data sources in some domains such as molecular biology are usually public and downloadable. for these cases we have designed patterns to retrieve data sources stored as flat files for later storage in an xml database. data services, independently of the development process, are distributed software applications that receive queries in xquery and return xml documents.

as the proposal is to use ontologies as schemas to integrate data, we have chosen a global as view  approach  <cit> . in gav, each source is related to the global schema  by means of mappings. moreover, the use of ontologies will allow us to take advantage of reasoning mechanisms to improve the query rewriting. the komf architecture is composed of three main components: the controller, the query planner and the evaluator/integrator.

• dbowl
dbowl  <cit>  is a persistent and scalable owl  reasoner. dbowl stores the owl-dl ontologies in a relational database, and supports tbox queries , abox inferences  and extended conjunctive queries  queries  <cit> . currently we are finishing a sparql  <cit>   query engine for dbowl . in order to create the relational database for ontology storage, a description logic reasoner is used. thus, the consistency of the ontology as well as the inferences about the ontology structure are delegated to this reasoner and dbowl focuses on reasoning on instances . both, tbox queries and ecq queries are implemented by translation to sql. abox inferences are implemented by java functions and sql views.

dbowl consists of two services, an owl storage system and an owl querying system. the owl storage system  stores the owl ontology in the database. the relational schema is implemented using the oracle database management system and all the necessary information for implementing tbox queries and abox inferences is then stored in the database. finally, the dbowl reasoner evaluates the java functions implementing the abox inference and creates the sql views containing the inferred instances.

dbowl implements both tbox queries and abox inference. tbox queries can be evaluated directly using the query language. on the other hand, abox inferences are evaluated when a query is sent to the system to obtain complete results. currently, dbowl supports all the tbox queries implemented by racer  <cit> . in order to implement them, the information obtained from the dl reasoner is stored in the corresponding tables at load time. the abox inference rules currently supported by dbowl cover owl-dl completely.

in order to demonstrate the performance of dbowl, we use uob   <cit> , a well known benchmark to compare repositories in the semantic web. this benchmark is intended to evaluate the performance of owl repositories with respect to extensional queries over a large data set that commits to a single realistic ontology. furthermore, the benchmark evaluates the system completeness and soundness with respect to the queries defined. this first experiment is conducted on a pc with intel quad core of  <dig>  ghz and  <dig> gb memory, running windows vista with java jre  <dig> . <dig> . we use the benchmark  <dig> mg and  <dig> mg ontologies, which contain around  <dig>  and  <dig> . <dig> individuals respectively. dbowl response times are quite good and dbowl also returns all expected results.

methods
in this section we describe a process for creating a persistent and scalable knowledgebase from integrated data. as described in the previous section, users can use komf to query heterogeneous data sources, and use this information to perform domain specific analysis. however, komf has limited reasoning capabilities. therefore, the proposed methodology introduces dbowl as a persistent reasoner to perform more complex analysis.

thus, the designed methodology establishes a set of operations to be performed when a knowledgebase is to be constructed from diverse data sources . it follows four steps:

a. komf configuration . this task aims to produce the necessary elements to integrate information from heterogeneous data sources. it involves firstly registering the domain ontology to represent the domain. the next step is to create the necessary data services, register them in the system and then set up the relationships between each data service schema and our domain ontology. after this configuration, users can send queries in terms of the domain ontology, which will be solved using the registered data services. this part requires a lot of work that remains mainly in the data service development and mapping definition , as has been described in a previous section.

b. query building . as we aim to produce a knowledgebase centered on a specific need, it is necessary to design a query  to retrieve all the information that will be later analyzed. this step could be done using the visual semantic browser , which allows users to browse an ontology and query komf to retrieve relevant information. vsb is a tool designed to visualize different views of semantics. the main aim of the tool is to enable user interaction, also to locate and use semantics usually only available to computers. it provides the necessary elements to facilitate the inclusion of new algorithms with little effort. also, some algorithms have been adapted in this prototype for the visualization of ontology groups, mappings, ontologies and instances.

c. instance retrieval . the designed query is executed using komf, obtaining a set of instances as rdf  documents.

d. knowledgebase creation . the domain ontology and the retrieved rdf documents  are used to generate the query-based knowledgebase using dbowl.

the methodology requires the use of the komf framework and the dbowl reasoner described previously . user queries are sent to komf  to retrieve the required instances , which will be stored in dbowl . then, analysis tools can take advantage of the reasoning capabilities of dbowl. both user interfaces can publish their programming interface so that they can be used in traditional life science workflows as another data source or data transformation tool.

the proposed methodology has been used to produce a demo tool  for accessing biological data and to allow users to create knowledgebases from retrieved data, enabling its subsequent analysis using reasoning. in order to show how the use case is built we will describe each step as described in the methodology.

komf configuration
in order to configure komf we need to carry out the following tasks:

 <dig>  develop a set of data services. in this use case we have developed several data services for accessing metabolic data: uniprot  <cit> , kegg  <cit> , chebi  <cit> , brenda  <cit>  and sabiork  <cit>  .

 <dig>  choose a domain ontology as the integration schema of komf. in this use case we have chosen biopax level  <dig> , which covers metabolic pathways, molecular interactions, signaling pathways , gene regulation and genetic interactions. figure  <dig> shows the entities part of this ontology. it has been registered in sd-core .

 <dig>  the data services developed are also registered in sd-core, by defining the mappings between the data service schemas and the domain ontology .

 <dig>  finally, komf can be queried to obtain integrated results from the registered data services.

query building
in order to enable users to query komf  we have developed a tool that uses an extension of vsb  <cit> , which provides a user interface for visualizing the registered ontology and creating the user query . the interface allows users to select concepts of the ontology to build the queries easily. thus, this interface uses a heuristic to suggest links between predicates using the variables to facilitate the user query.

for example,  in the domain ontology we have the concept protein and organism :

- when the user clicks on the protein concept, the tool proposes to introduce the predicate protein ;

- when the user clicks on the organism concept, the tool asks users to input the predicate organism ;

- if the user clicks on the property biosource, the tool proposes the predicate biosource .

- if the user clicks on the protein concept, the tool asks users to input the predicate protein ;

- finally, if the user selects the property interact_with, the tool will propose to use interact_with;

using this user query interface, users can query komf to retrieve useful information. in order to show the use of this interface we will describe some simple examples that will be further detailed in later sections.

instance retrieval
the user query is evaluated using the configured komf, the query is planned and the result is obtained as a set of instances . these instances can also be visualized using the user interface. in this sense, the results obtained from the mediator can be visualized as rdf instances, flat files and a graphical representation. thus, expert users can directly analyse rdf documents, while other users can take advantage of an easy to interpret graph, showing the instances and their relationships.

for the query example shown in the previous section, the user will obtain a set of proteins interacting with the target ontology. thus, the user can easily visualize the interaction network of this protein in a graphical way.

however, at this point the advantage of using semantics is limited to the explicit representation of certain knowledge. we may need to take advantage of reasoning to discover new knowledge from the retrieved data, as will be shown in the following section.

knowledgebase creation and knowledge based analysis
using the set of retrieved instances, the user can decide to make other queries on the mediator, but he/she can also decide to make this knowledge permanent in the knowledgebase, and can take advantage of the dbowl reasoner.

in this demo tool each user can have five sets of instances permanently stored in dbowl, where they can be analyzed and reasoned on. once the knowledgebase has been created the users can use it to perform different analyses using analysis tools. for example, the visualization tool, vsb, can be used to analyze the structure of the knowledge stored. this visualization tool can be configured to use different icons for different instance types, so end users can better understand the resulting graph.

furthermore, new tools can be developed or existing tools can be adapted to analyze specific issues based on the expertise of domain experts. the advantage of using dbowl is that these tools  can take advantage of a persistent storage  and reasoning to infer new knowledge . thus, results  can contain asserted instances plus those obtained through reasoning.

reasoning examples
in this section some theoretical examples are shown, which use a knowledgebase with useful information for systems biology researchers taking advantage of the tool described.

the retrieval of information about different pathways will provide the user with a set of interactions. these interactions are represented as instances of the interaction concept or any of its descendants. thus, they can be classified using these descendants. for example we can have different control interactions, which can be classified as catalysis, modulation or templatereactionregulation. these interactions have a controltype property that can take values such as: inhibition, activation, inhibition-allosteric, inhibition-competitive, etc. however catalysis can only be of type activation, and so it has been defined in the ontology as a functional property  and has at least one value for controltype property with value activation:

- controltype is functional

- ∃ controltype has "activation"

however, once the knowledgebase is created the set of instances may contain errors. the use of the reasoner will solve this problem. if an interaction is retrieved from the mediator, it is classified as catalysis. however, if the control type is "inhibition", the reasoner infers that this is an inconsistency in the ontology. for example, the interaction named 'amp  negatively regulates phosphorylation of chrebp at thr by amp kinase' has a control type inhibition, and so its classification as a catalysis will be resolved as an annotation error.

another example is the physical entity, where we can find that an entity p is an instance of protein and complex classes . in this case the reasoner also infers that the knowledgebase has inconsistencies . for example the protein complex "cytochrome b6f complex" may be annotated in one database as a protein and as complex in a different database. thus, this inconsistency will be detected by the reasoner, and the application using this information can act to resolve this inconsistency.

RESULTS
in this paper we have presented a novel system that combines the use of a mediation system  with the reasoning capabilities of a large scale reasoner  to provide a way of finding new knowledge.

the study of data integration systems has allowed us to determine their main elements, and thus to extract the pattern for building this kind of system.

hermes  <cit> , disco  <cit>  and tsimmis  <cit>  are well known mediator systems. essentially, all of these tools have a three-tier software architecture: clients connect to a mediator  <cit> . the mediator parses a query, carries out query rewrite and query optimization, and executes some of the operations of a query. the mediator also maintains a catalog to store the global schema of the whole heterogeneous database system , the external schema of the component databases, and statistics for query optimization.

in the specific field of biological data the following examples exist: tambis  <cit> , biodataserver  <cit> , kind  <cit> , biozoom  <cit> , biokleisli  <cit> , discoverylink  <cit> , biobroker  <cit>  and biomoby  <cit> .

discoverylink  <cit>  is one such system, targeted to applications from the life sciences industry. it provides users with a virtual database to which they can pose arbitrarily complex queries, even though the actual data needed to answer the query may originate from several different sources, and no individual source, by itself, is capable of answering the query.

virtuoso  <cit> , comprehensive data integration software developed by openlink software, is also capable of processing distributed queries. because virtuoso is also a native quad store, its strength is its scalability and performance. in addition to the commercial edition, an open source version is also available. a relatively new application also provided by openlink is the openlink data spaces platform, which is said to be able to integrate numerous heterogeneous data from distributed endpoints.

observer  <cit>  presents an approach which aims to enhance the scalability of query processing in a global information system. besides, users can express queries using semantic concepts. this approach makes use of pre-existing ontologies to integrate the underlying data sources. thus, repositories can be viewed with respect to relevant semantic concepts. semantic relationships can be defined between different ontologies, and they can be used to solve user queries. information loss is also dealt with to provide a fast response to users when exact results are not required.

model-based mediation  <cit>  is a paradigm for data integration in which data sources can be integrated, using auxiliary expert knowledge. this knowledge includes information about the domain and is the glue that binds data source schemas together. the expert knowledge is captured in a data structure called knowledge map. in model-based mediation, the mediation architecture is extended, carrying data sources from the data level without semantics to the conceptual model level. this architecture introduces semantics into data sources and mediators, but it is not published nor is it accessible to agents or applications. mediators are monolithic systems and they are strongly coupled to wrappers, limiting dynamic integration and interoperability.

dbowl is an owl reasoner. as owl is based on dl, we must study dl reasoners. of these, racer  <cit>  is the most relevant and one of the most complete, and it implements both tbox and abox reasoning. furthermore, it provides its own query language, which allows simple conjunctive queries to be evaluated. it is not persistent however, and reasoning is implemented by reducing it to satisfiability. this means on the one hand, that each time we use the reasoner, we must load and process the ontology and, on the other hand, that large ontologies  cannot be loaded. finally, racer is currently a commercial tool, and therefore, other dl reasoners, like pellet  <cit>  are becoming more popular. pellet provides the same functionality as racer but also has the same problems. in the past few years there has been a growing interest in the development of systems for storing large amounts of knowledge in the semantic web. firstly, these systems were oriented to rdf storage  <cit> . nowadays, research is oriented to massive owl storage. several alternative approaches using relational technology have been presented. instance store  <cit>  uses a dl reasoner for inferring tbox information and storing it in a relational database. however, the ontology definition language does not allow the definition of binary relationships. from our point of view, this is an important expressiveness limitation. moreover, instance store only evaluates some abox reasoning, namely subsumption of concepts and equivalent classes. it implements them by reducing them to terminological reasonings and evaluates them using a dl reasoner. on the other hand, the quonto  <cit>  system reduces the ontology definition language to dl-lite  <cit> , a description logic which is a subset of owl-dl. therefore, the soundness and completeness of the reasonings is ensured. it evaluates subsumption of concepts abox reasoning and conjunctive queries. the queries are rewritten using the tbox information and are translated to sql. dldb-owl  <cit>  extends a relational database with owl inferences. this proposal uses a dl reasoner as instance store but the database schema is more complex. in its public distribution only the subsumption of concepts is implemented, but it is implemented using only the information stored in the database. finally, minerva  <cit>  also stores the ontology in a relational database, but uses a dl reasoner for evaluating tbox reasonings and a rule engine to evaluate abox reasonings which are defined using description logic programs  rules partially covering owl-dl. minerva combines relational technology with logic rules and also evaluates sparql queries. our proposal aims to combine all these results, providing a persistent and scalable tool for querying and reasoning on owl ontologies. to do this, we provide an optimized storage model which is efficient and scalable, we implement reasoning on top of a relational database and combine reasoning and querying.

the presented tool uses a graphical query interface to build user queries easily. this interface shows a graphical representation of the ontology and allows users to build queries by clicking on the ontology concepts. protégé  <cit>  is a well known editor for ontologies that has been applied to edit owl ontologies. however, protègè provides an interface oriented to semantic web experts, and so it does not provide an easy-to-use query interface for final users.

however, the proposed system can be improved in many ways. the main drawback of this proposal is the configuration of ka-sb, which because it requires performing some manual steps, it is difficult for non-expert users:

• firstly, the development  of a domain ontology is necessary. this is an important issue in all the proposals for using semantics to improve any kind of process. this could be solved by designing new tools for scientists who are not semantic web experts. in this sense, some systems have been proposed  <cit> .

• secondly, the configuration of komf requires the development of some steps that cannot be achieved by non-expert users. the first difficulty is that for accessing the information it is necessary to implement data services. this problem has several solutions: either to develop automatic tools for creating a data service  or to develop a repository with available data services. in line with this last solution we are working on making public all the developed data services in the biological domain. the second difficulty is that these data services have to be registered in the mediator. this issue can be solved by including automatic tools to find the mappings between the domain ontology and each data service schema .

CONCLUSIONS
the combination of data integration solutions with reasoners to provide analysis tools in biology is a novel approach that opens up new possibilities in domains such as systems biology. the process described by ka-sb shows a way to use retrieved instances from user queries in a reasoner. this process has been applied to develop a demo tool , which provides a lot of opportunities to take advantage of the integrated information by means of a user interface for testing different queries. the reasoner allows users to exploit the results to search for new knowledge or to perform analyses. however, in a context like systems biology it is important to provide tools able to deal with a large amount of information, as the omic era has produced an enormous amount of freely available data.

our approach can be useful for real systems biology applications, especially for those aiming to provide end-user interfaces with extended capabilities. in this sense, we will study how to apply our proposal to extend a real application in systems biology , the system biology metabolic modelling assistant  <cit> , which is a tool developed to search, visualize, manipulate and annotate identity data and assist in annotating the kinetic data.

list of abbreviation used
komf: khaos ontology-based mediator framework; xml: extensible markup language; sd-core: semantic directory core; gav: global as view; owl: ontology web language; ecq: extended conjunctive queries; sparql: sparql protocol and rdf query language; uob: university ontology benchmark; vsb: visual semantic browser; rdf: resource description framework; dlp: description logic programs

competing interests
the authors declare that they have no competing interests.

authors' contributions
mrg has designed the large scale reasoner dbowl, and has helped to draft the manuscript. ind designed the infrastructure of komf, carried out the implementation of the system and drafted the manuscript. ak, oc and jmc have implemented the demo tool. jam conceived the infrastructure, participated in its design and coordination and helped to draft the manuscript. all authors read and approved the final manuscript.

