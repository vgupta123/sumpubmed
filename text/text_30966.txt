BACKGROUND
metagenomics is the study of microorganisms by sequencing random pieces of their genomes directly from environmental and clinical samples  <cit> . in contrast to many traditional methods in microbiology, metagenomics require no prior cultivation of individual isolates and entire communities can therefore be studied directly in their natural state  <cit> . the recent development of cost-efficient high-throughput dna sequencing technologies has greatly increased the popularity and potential of metagenomics and it has become a key technique for the analysis of the human microbiome, its composition and connection to disease . environmental microbial communities are also extensively studied using metagenomics in order to assess their structure and diversity .

metagenomics are often analyzed in a gene-centric approach where the individual genes are quantified in a process called binning  <cit> . after quality assessment of the raw sequence data, each fragment is matched against a reference database which typically consists of annotated genomes, contigs or a catalogue of genes. the relative abundance of each gene  is then estimated by counting the number of matching fragments in relation to a reference value such as the total number of fragments in the sample. by comparing gene abundance between metagenomes, important differences in community structure, diversity and biological function can be identified. the identification of differentially abundant genes between metagenomes is however complex. most metagenomes contain a high diversity of microorganisms which carries a vast number of different genes. the resulting count data is therefore high-dimensional with many thousands of genes quantified in a single sample. metagenomic data is also plagued by high levels of biological and technical variability and the number of samples is often low  . most metagenomes are also vastly undersampled and genes can therefore be represented by only a few, or even zero, dna fragments. thus, methods for statistical inference of metagenomic count data need to be robust to noise and have a high power to identify the truly differentially abundant genes. in addition, the high dimensionality can result in a large number of false positives and controlling the type i error as well as unbiased estimation of the false discovery rate is therefore vital.

a wide range of methods have been developed for identification of differentially abundant genes in metagenomic count data. xipe-totec, one of the first methods developed for this purpose, uses a permutation-based approach to estimate the median difference for each gene. significance is calculated by comparing the estimated median to a null distribution generated by pooling the metagenomic samples  <cit> . another early metagenome analysis tool was img/m which performs a test of the relative gene abundances using a gaussian approximation under the null hypothesis  <cit> . shotgunfunctionalizer is a software package for r containing several methods but has a focus on regression type approaches using generalized linear models  <cit> . metastats is based on a t-test where the p-values are derived from an empiric null distribution calculated by permuting the samples  <cit> . stamp focuses on comparisons of pairs of metagenomes using fisher’s exact test, but have recently been updated to also include other statistical procedures such as welch’s t-test and the resampled t-statistic of metastats  <cit> . lefse applies the non-parametric kruskal-wallis and wilcoxon-mann–whitney tests to assess differences in gene abundance between groups and subgroups of metagenomes  <cit> . another package for analysis of gene abundances is fantom which implements several parametric and non-parametric standard tests together with an easy-to-use graphical interface  <cit> . the more recently developed metagenomeseq which uses a zero inflated gaussian model to correct for bias caused by undersampling in combination of inference using the empirical bayesian model implemented in limma  <cit> . metagenomic data shows similarities to sequence-based transcriptomics and methods originally developed for analysis of rna sequencing  data have therefore been applied to identify differentially abundant genes, in particular edger  <cit> , deseq <dig>  <cit>  and voom  <cit> . even though a wide range of methods have been suggested for the analysis of metagenomic data, there exists no comprehensive evaluation that investigates their performance and statistical properties under realistic settings.

in this paper we present a comparison of  <dig> methods for statistical analysis of metagenomic gene count data. each method was assessed based on its statistical power to identify differentially abundant genes, its model assumptions and ability to control the false discovery rate. the methods were evaluated on data created by resampling and downsampling of real metagenomes which, in contrast to simulations from parametric distributions, results in more realistic settings where the structures of true gene count data are preserved. our results revealed large differences in performance between the methods. the sample size, effect size and gene abundance greatly affected the ability to identify differentially abundant genes. most methods showed skewed p-value distributions under the null hypothesis indicating non-adequate model assumptions. most methods were able to control the false discovery rate but showed differences in the number of true positives detected. we conclude that no single method is optimal for all types of metagenomics datasets. the results presented in this study can therefore serve as a guide for selection of proper statistical methods for the analysis of metagenomic data.

RESULTS
sample size, effect size and gene abundance have a large impact on performance
fourteen methods for identification of differentially abundant genes were evaluated on groups of metagenomes created by resampling from two datasets, one based on illumina sequencing  and one from massively parallel pyrosequencing . effects were introduced to 10 % of the genes using downsampling of fragments and the gene ranking performance was compared using receiver operating characteristics  curves and their corresponding area under curve up to a false positive rate of  <dig>  . our results showed that the group size had a positive impact on the accuracy of gene ranking and the performance increased substantially when more samples were included  . for the qin dataset, no single method had the best performance for all investigated group sizes . at a group size of 3 +  <dig>  deseq <dig> had the highest performance with an auc <dig>  of  <dig>  followed by edger  and the overdispersed generalized linear model   . at larger group sizes, oglm had the best performance with an auc <dig>  of  <dig>  and  <dig>  for 6 +  <dig> and 10 +  <dig> respectively. the corresponding numbers for deseq <dig> were  <dig>  and  <dig>  and for edger  <dig>  and  <dig> . metagenomeseq had a low auc <dig>  at small group sizes , but at the largest group size the performance was second best after oglm . voom and metastats, which are both based on normal approximations, showed similar performances except at the smallest group size where metastats performed poorly.table  <dig> the gene ranking performance at different group sizes for all  <dig> methods

auc <dig> 

each listed value is the normalized area under curve up until a false positive rate of  <dig> . higher values represent higher gene ranking performance. the results are calculated based on  <dig> resampled metagenomes. the wilcoxon-mann–whitney test was not evaluated at the smallest sample size  due to lack of samples. the full area under curve  measurements are available in additional file 1: table s1

fig.  <dig> the performance of detecting differentially abundant genes increases for large group sizes. for each method, the receiver operating characteristics curve shows the true positive rate  and the false positive rate  at each position in the gene ranking list. panels a-c show results for the qin dataset and panels d-f show results for the yatsunenko dataset. group sizes of 3 +  <dig>  6 +  <dig> and 10 +  <dig> were included in the comparison and the effect size was fixed at a fold-change of  <dig>  each curve is based  <dig> resampled metagenomes. the methods included are edger, deseq <dig>  the overdispersed generalized linear model , metagenomeseq , metastats and voom 



the performance of the methods was more consistent across group sizes when evaluated on metagenomes resampled from the yatsunenko dataset  . here, edger had the highest performance in all group sizes with an auc <dig>  of  <dig> ,  <dig>  and  <dig>  for 3 +  <dig>  6 +  <dig> and 10 +  <dig> respectively. deseq <dig> was a close second followed by oglm and metagenomeseq . voom was the third best at 3 +  <dig> but had the lowest performance of all method at 10 +  <dig>  metastats again performed poorly at low sample sizes but was close to the top performing methods at 10 +  <dig> 

the ordinary student’s t-test using a square-root variance stabilizing transform had a surprisingly high performance with an auc <dig>  of  <dig>  for the yatsunenko dataset and  <dig>  for the qin dataset at a group size of 10 +  <dig> . interestingly, when the square-root transform was replaced with a log-transform, the performance decreased for most group sizes on both datasets. the non-parametric wilcoxon-mann–whitney test showed a lower performance than the t-test. finally, the poisson generalized linear model, the fisher’s exact test and the binomial test all had a consistently poor performance for all group sizes and datasets.

next, the impact of the effect size was investigated for a fold-change of  <dig>   <dig> and  <dig>  as expected, all methods performed better at larger effect sizes . for the qin datasets, the best method  had an auc <dig>  of  <dig> ,  <dig>  and  <dig>  for effect sizes  <dig>   <dig> and  <dig> respectively . for the yatsunenko datasets, edger had the highest performance with an auc <dig>  of  <dig> ,  <dig>  and  <dig>  for the three effect sizes. altering the effect size did not substantially change the relative performance of the methods.

the power to identify differentially abundant genes is dependent on the number of observed dna fragments. to investigate this effect, the genes were stratified into three roughly equally sized groups based on their average number of fragments . this showed that the gene abundance had a considerable impact on the gene ranking performance. for the qin dataset, all methods showed a poor ranking performance for genes with a low abundance , where deseq <dig> had the highest and metagenomeseq the lowest auc <dig> ,  <dig>  and  <dig>  respectively . the ranking performance increased substantially for genes with higher abundance  where oglm had the highest auc <dig>  at  <dig> . at the highest abundance group , all methods generated excellent ranking of the differentially abundant genes with the only exception of the generalized linear model, the fisher’s exact test and the binomial test . analogously to the qin dataset, all methods had a poor performance for the low abundant genes in the yatsunenko dataset  which increased substantially at higher abundance  . edger was the best method in all three categories with an auc <dig>  of  <dig> ,  <dig>  and  <dig>  . metagenomeseq  showed again a poor performance for low abundant genes  but was the third best method when the abundance increased . for the low abundant genes, all methods showed a lower performance for the qin dataset compared to the yatsunenko dataset, even though the cutoff was tenfold higher .fig.  <dig> gene abundance had a large impact on the performance to identify differentially abundant genes. for each method, the receiver operating characteristics curve shows the true positive rate  and the false positive rate  at each position in the gene ranking list. panels  show results for the qin dataset and panels  show results for the yatsunenko dataset. the genes were stratified into three parts based on the average number of dna fragments, i) ≤ <dig>  ii) 500– <dig> and iii) > <dig> for the qin dataset and i) ≤ <dig>  ii) 10– <dig> and iii) > <dig> for the yatsunenko dataset. the effect size was set to a fold-change of  <dig> and the group size fixed at 6 +  <dig> samples. each curve is based  <dig> resampled metagenomes. the methods included are edger, deseq <dig>  the overdispersed generalized linear model , metagenomeseq , metastats and voom 

auc <dig> 

each listed value is the normalized area under curve up until a false positive rate of  <dig> . higher values represent higher gene ranking performance. the results are calculated based on  <dig> resampled metagenomes. the wilcoxon-mann–whitney test was not evaluated at the smallest sample size  due to lack of samples. the full area under curve  measurements are available in additional file 8: table s4



most methods accurately estimated the effect size in the qin dataset . for the yatsunenko dataset however, several methods, including deseq <dig>  metagenomeseq, metastats and voom, showed underestimated effect sizes. this was in contrast to edger and oglm which both produced unbiased estimates for both datasets. the standard deviation of the estimated effect size decreased, as expected, for all method as the group size increased.

most methods have a biased p-value distribution under the null hypothesis
unbiased estimation of p-values under the null hypothesis is essential to control the type i error rate. we therefore used resampled metagenomes without added effects to investigate the p-value distributions for all  <dig> methods . the majority of the methods showed biased p-values with distributions skewed towards either low or high values . edger and deseq <dig> both had conservative p-values  while the p-values for oglm were too optimistic . these trends were consistent between the two datasets. metagenomeseq demonstrated too optimistic p-value distribution and this bias was more pronounced for the qin datasets where a large proportion of the genes had very small p-values . metastats had the most uniform p-value distribution  while voom exhibited slightly conservative p-values for the qin dataset  but had too optimistic p-values for the yatsunenko dataset . all tests based on t-statistics showed a unimodal p-value distribution where the variant using the square-root transformation was most uniform . finally, the poisson generalized linear model, the fisher’s exact test and the binomial test all had extremely optimistic p-values indicating that these methods will likely produce a high number of false positives.fig.  <dig> most methods have a biased p-value distribution under the null hypothesis. the p-value distributions on the qin dataset with no added effect and a group size of 6 +  <dig> averaged over  <dig> resampled data sets. for the quantile-quantile-plots, each grey line represent a resampled metagenome, the solid black line represents the average value and the dotted line the line with slope one corresponding to a uniform p-value distribution. the panels correspond to edger , deseq <dig> , overdispersed poisson glm , metagenomeseq , metastats  and voom . 

fig.  <dig> quantile-quantile plots and histograms for the second data set under the null hypothesis. the p-value distributions on the yatsunenko dataset with no added effect and a group size of 6 +  <dig> averaged over  <dig> resampled metagenomes. for the quantile-quantile-plots, each grey line represent a resampled metagenome, the solid black line represents the average value and the dotted line the line with slope one corresponding to a uniform p-value distribution. the panels correspond to edger , deseq <dig> , overdispersed poisson glm , metagenomeseq , metastats  and voom . 



several methods are able to control the fdr but their power differ
the ability to control the false discovery rate  was analyzed for each method by counting the number of true and false positives at an estimated fdr of  <dig> . . for the qin dataset, metagenomeseq detected the highest number of true positives  in median, followed by deseq <dig> , oglm  and edger   . however, metagenomeseq produced a high number of false positives  while the numbers were substantially lower for deseq <dig> , oglm  and edger . consequently, metagenomeseq failed to control the false discovery rate and at an estimated fdr of  <dig>  the true median fdr was  <dig> . deseq <dig>  oglm and edger were able to control the false discovery rate and at the  <dig>  cut-off, the estimated fdr were  <dig> ,  <dig>  and  <dig>  respectively. for the yatsunenko dataset, edger and deseq <dig> identified the highest number of true positives  followed metagenomeseq  and oglm  . all these methods maintained a low number of false positives   resulting in a true fdr at or below  <dig>  for all methods . the t-statistics showed a slightly conservative fdr estimated for both the qin and yatsunenko datasets . the poisson generalized linear model, the fisher’s exact test and binomial test completely failed to control the false discovery rate with a too high proportion of false positives.fig.  <dig> most methods can control the false discovery rate at predefined level. the figure shows boxplots of the number of true positives , the number of false positives  and achieved true fdr  at a cutoff of  <dig>  estimated fdr. panels  show results for the qin dataset and panels  show results for the yatsunenko dataset. the group sizes were set to 6 +  <dig> and the effect size to  <dig>  the results were based on  <dig> resampled metagenomes. the included methods are edger, deseq <dig>  the overdispersed generalized linear model , metagenomeseq , metastats and voom 



discussion
in this study, we evaluated the performance of  <dig> methods for the identification of differentially abundant genes between two groups of metagenomes. the statistical power, the uniformity of the p-values under the null hypothesis and the ability to control the false discovery rate were investigated using resampling of two large human gut metagenomic datasets, one based on illumina sequencing  and on massively parallel pyrosequencing . our results showed that the group size, effect size and gene abundance all had a large impact on the gene ranking performance of all methods. deseq <dig>  edger and the overdispersed poisson glm  had the best overall performance, but their results differed between the investigated data sets and conditions. deseq <dig> and oglm had the highest performance on the illumina dataset while edger was the best method on the dataset sequenced by massively parallel pyrosequencing. in addition, edger and oglm had the most accurate estimation of the effect size, while deseq <dig> produced biased estimates for the yatsunenko dataset. deseq <dig> and edger were originally developed for identification of differentially expressed genes in rna-seq data. both methods apply a negative binomial distribution where the gene-specific overdispersion is robustly calculated by a shrinkage estimator modelled by an empirical bayes approach  <cit> . for rna-seq data, this has been shown to be highly advantageous when few samples are available and our evaluation shows that this is also true for metagenomic counts  <cit> . however, at larger group sizes in the qin dataset, oglm had a higher performance than both deseq <dig> and edger. in contrast, oglm is a quasi-likelihood based method that assumes a poisson distribution where the gene-specific overdispersion is introduced by scaling the gene abundance  <cit> . even though oglm does not use any shrinkage approach to estimate the gene-specific overdispersion, it still had the highest performance for group sizes 6 +  <dig> and 10 +  <dig>  this suggest that the underlying empirical bayes models of deseq <dig> and edger may not be fully optimal for all forms of count data in metagenomics and thus not always the preferable choice for identification of differentially abundant genes.

another method that overall performed satisfactorily was metagenomeseq, which is specifically developed for handling the high number of zero observations encountered in metagenomic data. metagenomeseq uses a log-transformation ) followed by correction for zero-inflation based on a gaussian mixture model  <cit> . inference is done after transformation using a normal-inverse gamma empirical bayes model which moderates the gene-specific variance estimates  <cit> . interestingly, our results show that the t-test using an identical log-transform had a higher performance in many of the testing conditions, especially for low abundant genes and small group sizes. this suggests that the correction for zero-inflation applied in metagenomeseq may be disadvantageous under certain conditions and thus not recommended. however, it should be underlined that metagenomeseq was primarily designed for inference of taxonomic composition using counts from amplicon sequencing  where the average number of counts and the number of samples typically is higher  <cit> . this is also confirmed by our results which show that metagenomeseq has a substantially higher performance for genes with high abundance and at larger group sizes.

gene count distributions have a non-trivial dependence between their mean and variance. this can negatively affect methods that do not specifically describe this dependence, such as methods based on gaussian approximations. variance stabilizing transformations can be used to decouple this dependence  <cit>  and thereby increase the gene ranking performance significantly. the choice of variance stabilizing transformation is however dependent on the underlying distributional assumptions which are, for high-dimensional data, often hard to assess. we therefore evaluated the performance for two common transformations  and our results showed that the area under curve was higher for the square-root transformation than for the log-transformation for both investigated datasets. in fact, for large group sizes in the qin dataset, applying a log-transform actually resulted in worse performance in comparison to non-transformed data. thus, selecting an appropriate transformation has large impact on the statistical power of finding differentially abundant genes. furthermore, the t-test with the square-root transform had a higher performance than the non-parametric wilcoxon-mann–whitney test, even at a group size of 10 +  <dig>  in contrast to the t-test, the wilcoxon-mann–whitney test is less dependent on the underlying distributional assumptions but is vulnerable to ties  <cit> . since observations with zero counts are common in metagenomic data it may explain the surprisingly low power  <cit> . moreover, the poisson generalized linear model, the fisher’s exact test and the binomial test exhibited the worst performance under all tested conditions. these methods use, explicitly or implicitly, the group-wise pooled counts for inference without any estimate of the between sample variability. the low performance of these methods is thus due to their inability to correctly discriminate between overdispersion and effect. this further underlines the importance of proper modelling and estimation of the gene-specific variability for achieving a high statistical power in metagenomic analysis.

the p-values for non-differentially abundant genes should ideally be uniformly distributed between zero and one  <cit> . deviation from the uniform distribution is an indication of wrong model assumptions and can result in too many false positives and incorrectly estimated false discovery rates. when applied to resampled metagenomes where all genes satisfied the null hypothesis of not being differentially abundant, most methods showed skewed p-value distributions with either too optimistic or too conservative values. metastats, which estimates the significance using permutations, was the only method able to produce in average unbiased p-values. this demonstrates the advantage of using empirically derived null distributions for controlling the type i error rate. in contrast, metagenomeseq, generated highly biased p-values under the null hypothesis which resulted in a large number of false positives and a too optimistic false discovery rate. this is also in line with a recent evaluation of metagenomeseq on count data of operational taxonomic units which demonstrated a high false positive rate  <cit> . furthermore, the variability between individually resampled metagenomes was substantial and all methods produced both over- and under-estimated p-values for both datasets. this suggest a data heterogeneity where individual samples have substantial distributional differences that none of the methods can describe satisfactorily. modelling of the sample-specific variability and between-sample correlations in large-scale transcriptomics have previously been shown to substantially reduce the bias of p-values under the null hypotheses and analogous models should be pursued in metagenomics in order to ensure a reliable estimation of the type i error rate  <cit> .

calculation of the fdr is a common way to control the error rate in multiple testing of high-dimensional data  <cit> . correct and unbiased estimation of the fdr is dependent on the model assumptions under both the null and alternative hypotheses and is vital for reliable downstream biological interpretation  <cit> . the majority of methods in this study were able to control the true fdr at the specified estimated fdr cut-off. this was however not true for metagenomeseq which exhibited a too high proportion of false positives. the poisson generalized linear model, the fisher’s exact test and the binomial test were completely unable to control the fdr and returned a large number of false positives. applying these methods to metagenomic data is thus not recommended and may lead to erroneous biological conclusions.

two independent datasets were used as a basis for the evaluation; one generated using the illumina platform  and one using massively parallel pyrosequencing . all methods had a slightly higher gene ranking performance for the qin dataset, partially due to the substantially higher sequencing depth. however, all methods also showed a reduced performance for the low abundant genes in qin dataset compared to the yatsunenko dataset. this difference was substantial , especially considering that the cut-off for the lowest abundant genes was ten-fold higher . the discrepancy for the low abundant genes in the two datasets can be partially explained by the higher variability caused by the large proportion of genes with zero counts available in the qin dataset . this is likely a consequence of the binning process typically applied for short read data where the reference database is first assembled de novo. genes represented by a low number of fragments are often hard to assemble and may therefore be completely missing from the reference database, resulting in observations with zero fragments. zero-inflation is known to result in overestimation gene-specific variability and will thus cause in an overall reduction in the power for identification of differentially abundant genes. refined binning strategies using comprehensive reference databases, such as gene catalogue or fully sequenced genomes from a large collection of isolates, may result in a more accurate representation of the metagenome and thereby reduce the number of zero count observations  <cit> . however, these strategies are only be applicable to well-studied microbial communities such as the human microbiome. thus, development of new statistical methods that show a higher robustness against zero-inflation will be vital in order to maintain a high statistical performance in all forms of metagenomic studies.

simulated metagenomes are dependent on the underlying assumptions and the specific parametric distribution used to draw gene counts. comparisons using simulated data are therefore, in essence, subjective and may greatly favor methods with assumptions close to those used for the data generation. in this study, the performance of the statistical methods was evaluated on artificial datasets created by resampling of two real metagenomic datasets. in contrast to simulation from parametric distributions, resampling preserve many of the features of real metagenomic data, such as the underlying read distributions with its technical and biological variability and the gene-gene correlation which can have a large impact on estimation of the false discovery rate  <cit> . effects were introduced in the data by a downsampling strategy where individual dna fragments were randomly and independently removed from the dataset. it should be noted that our resampling approach works similar to an experimental randomization procedure in the sense that the effects added by downsampling are not systematically co-varying with other non-modeled factors for example, host age, life style or genetics. in a real experimental setup factors may covariate making inference even harder. however, we still argue that in contrast to simulating from a parametric distribution, our setup with resampled artificial metagenomes generates more realistic count data which leads to a more objective comparison of the statistical methods. furthermore, the methods included in this study were run using their recommended normalization techniques . however, normalization of metagenomic data has previously been shown to have a substantial impact on the analysis  <cit>  and a wide range of different techniques has so far been developed  <cit> . since the methods included in this study are based on different distributional assumption is it also likely that they perform optimally in combination with different types of normalization. further studies are therefore needed to identify which normalization strategies that should be combined with the different statistical methods in order to achieve maximum performance for identification of differentially abundant genes in metagenomic data.

CONCLUSIONS
statistical inference in metagenomics is challenging due to high levels of biological and technical variability in combination with high dimensionality of the count data and the few samples that are typically present. in this study  <dig> methods for identification of differentially abundant genes were evaluated. our results showed that group size, effect size and gene abundance greatly affected the performance and no single method was best under all investigated conditions. deseq <dig>  the overdispersed poisson generalized linear model and edger had all an overall satisfactory performance and are therefore suitable methods for inference of metagenomic gene count data. our results also showed that methods that do not correctly capture the between-sample variability have a very low performance and should be avoided. the results presented in this paper may thus serve as a guide for the design of future metagenomic experiments and as suggestions for appropriate statistical methods to use in the analysis of gene count data.

