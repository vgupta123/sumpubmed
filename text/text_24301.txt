BACKGROUND
a fundamental assumption of the "comparative genomics" approach is that clues to the functional roles of stretches of genomic dna might be inferred from patterns of sequence conservation between related organisms. until relatively recently it was assumed that longer stretches of conserved sequences would consistently overlap coding regions as, with the exception of relatively few non-protein-coding genes such as ribosomal rnas and trnas, transcribed non-coding rnas were thought to be relatively rare and cis-acting dna regulatory elements were believed in general to be rather short. recent findings have challenged these assumptions. long, highly conserved apparently non-transcribed cis-acting elements have been identified  <cit>  and estimates as to the proportion of transcripts that do not encode proteins continue to rise  <cit> . furthermore, other lines of evidence suggest that several genes encoding "conserved hypothetical" proteins, for which little or no evidence of expression at the protein level exists, may not in fact constitute protein coding regions  <cit> .

while it is probable that representatives of most gene families found in nature have been characterized , lineage specific gene families, genes, and exons - which may be incorporated into messages by alternative splicing and which may not be recovered by ab-initio predictors as components of optimal gene models, are not uncommon . in this context, comparisons between relatively closely related genomes can permit identification of novel exons or coding genes that exhibit low levels of similarity to annotated proteins  <cit> . the extent and degree of conservation of non-coding transcripts is only now being widely studied and reliable "independent" in-silico support for the coding nature of gene predictions and transcripts is thus highly desirable. accordingly, several comparative genomics approaches for the identification of coding regions through the differentiation of the evolutionary dynamics of coding and non-coding sequences have been proposed . such methods do not rely on the annotation of homologous sequences or the conservation of specific functional signals.

several of these methods are based on the expectation that the ratio of rate  of  synonymous substitutions to the rate or  of  non-synonymous substitutions will be higher for genuinely coding sequences  <cit> . mignone et al.  <cit>  proposed a measure of coding potential derived from the product of the synonymous/non-synonymous substitution rate ratio and a measure of perceived similarity of peptides potentially encoded by the sequences under examination. a hybrid method which implicitly uses measures of amino acid similarity and synonymous/non-synonymous substitution rates as well as non-comparative information  has been implemented in the software critica  <cit>  which, while designed for gene discovery in prokaryotes, can be adapted for the discrimination of coding and non-coding rnas in eukaryotes  <cit> .

here we present a machine learning approach for the discrimination of coding and non-coding conserved sequence tags  generated through local alignment of genomic sequences or transcripts. our method calculates various statistics related to the evolutionary dynamics of two aligned sequences - which need not correspond to entire genes or exons or models thereof. these features are considered by a support vector machine which designates the alignment coding or non-coding with an associated probability score. using a set of realistic genomic alignments that contains heterogeneous  examples, we show that our method is both sensitive and accurate and illustrate several situations in which it may be applied, including the identification of conserved coding regions in syntenous genome sequences and the discrimination of coding from non-coding cdna sequences.

RESULTS
data and annotation
to construct the training and validation sets, sequences from the encode  <cit>  regions of the human genome  were aligned with corresponding mouse syntenic regions as defined at ensembl  <cit> . both to avoid problems associated with erroneous insertions and deletions derived from sequencing errors, and to minimize heterogeneous alignment fragments  we analyse only the longest gap-free block in each hsp. we consider only gap-free blocks of length greater than  <dig> bases as shorter alignment fragments may be spurious . each gap-free alignment block  recovered was annotated as coding if it was shown, through use of the human refseq validated np accessions) messages or vega  <dig>  annotated cds, that at least 50% of the cst overlapped an annotated coding region. csts with between > <dig> and 50% overlap with coding regions and csts overlapping annotated pseudogenes were excluded from training and validation steps. other csts were labelled non-coding.

the statistics we use in the discrimination between coding and non-coding conserved sequences are based on the evolutionary dynamics expected of coding regions, we therefore exclude gap-free alignment blocks with less than 5% variation between the aligned sequences as identical or nearly identical sequences suffer from the same stochastic limitations as very short alignment fragments.

alignment of the encode  <cit>  regions with their syntenic counterparts in mouse yielded  <dig> gap-free blocks conforming to the conditions specified above. however, of  <dig> blocks that exhibited no overlap with annotated coding genes,  <dig> encoded putative peptides with significant similarity to uniprot entries. to minimize the use of potentially mis-annotated regions in the training and testing of the support vector machine, these blocks were excluded from subsequent experiments. the final dataset contained  <dig> blocks. preliminary studies using the feature values in isolation suggested that different features and different cutoff values for feature scores were discriminatory for different lengths of gap-free alignment blocks  and that a the major part of this variation was associated with the behaviour of alignment blocks of length between  <dig> and  <dig> bases . accordingly, all gap-free blocks considered were divided into short  and long  categories.

 <dig> processed csts were between  <dig> and  <dig> nucleotides in length , while  <dig> non-gapped regions of greater than  <dig> bases were recovered . the datasets were divided randomly into training  and validation  sets.

analysis of discriminating power of features
we developed  <dig> measures of the coding potential of aligned homologous sequences . some of these measures derive purely from the characteristics of the individual sequences, while others are measures of the characteristics and patterns of variation in the alignment.

support vector machine  is an effective classification method, but it does not directly estimate the feature importance.

the contribution of individual features to the discriminatory power of the trained svm model can be assessed using an exhaustive search, by performing learning using each of the features separately, as well as all of the possible feature combinations, and then evaluating the performances of the trained classifiers. while this wrapper approach allows the possibility of accounting for feature interactions inside the trained model, it is also known to be prone to overfitting to the dataset used in the feature selection process. we thus chose to assess the predictor importance using a model free method. a roc curve analysis was conducted for each predictor. a series of cutoffs was applied to the single features data to predict the coding/non-coding nature of the csts. sensitivity and specificity were calculated for each cutoff value and the roc curve was computed. area under the curve  was then calculated for each predictor and used as measure of the feature importance.

results of analyses performed independently for the long and short cst training sets are reported in table 1a and 1b and show similarities in the ranking of the discriminatory capacity of the features tested between both datasets particularly for the most relevant predictors. however, among the less discriminatory predictors, notable differences in the rankings are observed between long and short datasets.

for each feature, area under curve  calculated from roc curves calculated with training sets was used as a measure of discriminating power of the feature 

correlation between feature values
many of the  <dig> features employed in the current study, either share elements of their mathematical formula , or rely on characteristics of the genetic code such as partial redundancy concentrated at third codon positions. the degree of redundancy of information derived from pairs of features was evaluated using pearson pairwise-correlations for both long and short training datasets.

heatmaps shown in figure 1a and 1b depict correlations between feature values for long and short datasets respectively, where features are also clustered according to correlation scores. the absolute frequencies of the number of comparisons falling into  <dig> binned correlation intervals are also shown as histograms in the heatmap keys . we note that the majority of pairwise comparisons between feature values yield low to moderate correlation scores - suggesting that information recovered by different features is not significantly redundant. features describing stop codon frequencies  show negative correlations with other feature values. this is because, for these features, low values are associated with high coding potential . while feature scores are not strongly correlated, cochran-mantel-haenszel tests indicate strong interdependence of reading frames showing maximum scores, particularly for coding csts, indicating that features exploiting characteristics of the genetic code function in a consistent manner .

svm training and validation
all feature values were scaled using the program svm-scale from the libsvm package  <cit> . optimal values of the parameters c and g were estimated independently for long and short instances using the script grid.py provided with the libsvm distribution.

the support vector machine was trained under conditions allowing the export of probabilities associated with predictions. roc curves for training and validation of short and long csts are shown in figure 2a where the large percentage of the area under the curves indicates the high overall accuracy of models generated, and the similarity of training and validation curves indicates the consistency of the models. using the p-scores exported for each training instance cutoff values that allow recovery of 1% false positive results for both coding and non-coding categories were estimated on the training set  and allowed recovery of  <dig> % of gap-free alignment blocks annotated as overlapping with coding regions .  <dig> % of gap-free alignment blocks annotated as not overlapping coding regions were recovered as non-coding , while  <dig> % of all gap-free blocks yielded indeterminate values under these post-processing conditions.

the model and post-processing parameters developed were used to analyse the validation set to demonstrate that model over-fitting was not evident .  <dig> % of gap-free blocks annotated as coding were recovered as coding  with a combined false positive rate of  <dig> %, while  <dig> % of gap-free blocks annotated as non-coding were recovered as non-coding  with a combined false positive rate of  <dig> %. when false positive and false negative rates were both set at 1%,  <dig> % of alignments yielded indeterminate scores.

analyses using svm provides a significant improvement over discrimination with individual features
we calculated auc scores for individual features and the trained svm with the validation dataset . to determine whether the observed differences are significant we applied a non-parametric test based on the mann-whitney statistic  <cit>  using the software star  <cit> . at the 1% confidence level, the svm significantly outperformed all individual features for both the long and short validation datasets . the results of all pairwise comparisons of discriminatory power of individual statistics are reported in additional file 1: table s <dig> and broadly confirm the patterns suggested by analyses presented in tables 1a and b and additional file 1: table s <dig> 

pseudogenes and paralogs
our method uses only features expected to show biases in coding regions with respect to non-coding regions. as well as distribution of substitutions with respect to codon structure, we have shown that overall base composition and conceptual amino-acid similarity are strong indicators of the coding nature of aligned sequences. accordingly, it is expected that alignments derived from pseudogenes will show characteristics of coding alignments. while ancestral pseudogenes might be expected to show less coding potential than cases where only one aligned sequence is pseudogenic, underlying compositional factors and residual conceptual amino acid similarity are expected to yield a number of false positive coding predictions. the evaluation of the impact of pseudogenes on the predictive power of our method is further complicated by concerns regarding the accuracy of pseudogene annotation. non expressed pseudogenes my exhibit intact open reading frames, while the presence of premature stop codons is not guarantee that a sequence is not expressed at the protein level. nevertheless, we have evaluated the coding potential of  <dig> alignments excluded from the training and evaluation sets because either human or mouse annotations implied an overlap with an annotated pseudogene. for long csts overlapping annotated pseudogenes, 88%  are evaluated as coding  while 51%  of short csts that overlap annotated pseudogenes were classified as coding . these data are broadly consistent with our expectations: on the one hand they confirm, unsurprisingly, that pseudogenes retain certain characteristics of coding regions, on the other hand they indicate that our method can be used to assist in the identification and annotation of pseudogenes. the marked decrease in the recognition of short pseudogenic csts as coding with respect to the long category may also indicate that a proportion of the "pseudogenes" represented by long csts are in fact coding sequences - given the observed higher proportion of coding sequences among long alignments in general.

local alignment of whole genomes  will result in the recovery of some hsps derived from paralogous sequences. in general it is expected that the evolutionary dynamics of coding regions will be similar for paralagous and orthologous sequences. our preliminary studies indicate that alignments of orthologous coding sequences tend to yield slightly higher coding probabilities than paralagous coding alignments, but that the marginal difference observed is likely to be due, mainly, to the fact that orthologous alignments tend to be longer than those derived from paralagous sequences .

quality of training data
errors in the annotation of training data are expected to negatively affect the performance of the method proposed here through several mechanisms. first, during the training of the svm the hyperplane and margins generated by the svm will be sub-optimal as the svm attempts to generate a model which can classify the maximum possible number of points according to their a-priori annotation. the a-posteriori estimation of p-scores will also be influenced by mis-annotated instances. finally, p-score cutoff values allowing one percent false positive predictions will be artificially high  as mis-annotated instances will tend to be recovered as false positive predictions. to assess the importance of the quality of annotation of the training data used, we performed experiments where a set proportion of the annotations  of the data points used in training and testing were deliberately but randomly inverted - in order to simulate the situation likely to be encountered in the development of the method with poorly annotated genomes. jackknife experiments were performed with 70% of the training data where 1-10% of the annotations used were randomly inverted . data were scaled and svm parameters were optimized independently for each replicate. roc curves were plotted for the corrupted training data . as expected, sensitivity at any given false positive rate - as well as the maximum sensitivity of the method - fall as the proportion of mis-annotated training instances rises. however, the fall in maximum sensitivity of the svm is minimal  and broadly corresponds with the expected proportion of coding csts mis-annotated as non-coding. with the native validation data, the roc curves remain stable for both long and short csts . we interpret these observations as suggesting that the svm is relatively insensitive to the quality of training annotation and that the features used provide a strong separation between coding and non-coding csts.

quantity of training data
for each of a series of proportions of the original training dataset, subsamples were selected randomly  <dig> times and data scaling and parameter optimizations were performed for each subset of the original data. cutoff values allowing 1% false positive coding predictions with the training data were established and each model generated was tested against the entire validation set. figure  <dig> shows the average cutoff values, sensitivity with training and test data and false positive rates with validation data for each proportion of the training set tested. it is clear that with our data the training of the machine is relatively insensitive to the number of instances used - suggesting that, using the features developed here, a relatively small number of instances can adequately represent the diversity of evolutionary dynamics of coding and non-coding csts.

comparison of our approach with cstminer
cstminer  <cit>  is an application previously developed by our group to identify csts and classify them as coding or non-coding based on their evolutionary dynamics. it relies on a single scoring function, very similar to the cps score incorporated in our feature set , that considers, for each conceptual reading frame, the synonymous and non-synonymous substitution rates and the perceived similarity of aligned amino acids encoded in each conceptual reading frame  <cit>  threshold scores giving theoretical 1% false positive and false negative prediction rates were previously developed from distributions of coding potential scores derived from known coding and non-coding alignments. we have compared the performance of the new method with that of the algorithm used by cstminer on our test set. the cstminer algorithm showed an overall sensitivity of 71% with respect to coding csts  and 18% sensitivity with respect to non-coding csts , while 56% of all csts considered yielded indeterminate coding potential scores. roc curves for cstminer analyses of the test data are provided in additional file 1: figure s <dig>  when compared to the performance of the current method with the same dataset , the observed differences between the aucs and non-parametric mann-whitney tests provided incontrovertible support for the hypothesis that the incorporation of multiple additional indicators of coding potential and the use of machine learning methods provides a significant advance in the discrimination of coding and non-coding alignments . the marked decay of cstminer performance with respect to values obtained in initial evaluations  <cit>  is mostly due to the use of realistic alignments derived from genomic sequences. these alignments are often shorter than those used in previous cstminer evaluations and are typically heterogenous with respect to their coding nature .

discrimination of coding and non-coding rna
to assess the utility of our method in the detection of coding transcripts we generated alignments with  <dig> full-length human messages with corresponding sequences in swiss-prot used by frith et al.  <cit>  by megablast against the murine refseq mrna collection. we were able to identify at least one gap-free block satisfying the conditions for analysis with our svm  for  <dig> of the initial  <dig> transcripts . feature values were calculated for all these alignments in the same way as for genomic sequences and the feature values were scaled according to the genomic scaling ranges. after analysis with the svm  longest gap-free blocks were classified as coding, non-coding or indeterminate on the basis of the probability cutoffs established in the previous sections. of the  <dig> messages,  <dig>  harboured at least one gap free alignment block that was designated as "coding", while  <dig>  of the messages harboured only gap-free blocks classified as non-coding. we have compared our assessment of each human mrna in the set studied, with the assessments generated for the same messages by frith et al.  <cit>  using  <dig> other methods .

considering only messages for which usable alignments were generated , our method is the most sensitive in the detection of their coding nature . additionally our method exhibits agreement with the majority assessment for a higher number of transcripts than any other method and for each transcript studied our method is, on average, in accord with  <dig>  of the nine other methods . taken together, our data indicate that the approach presented here provides a good representation of the consensus of other methods designed to test the coding nature of transcripts, while escaping the explicit use of annotated homologs upon which blastx is reliant. frith et al.  <cit>  conclude that consensus between methods provides a reliable assessment of the coding status of transcripts, and that a high proportion of messages reported as non-coding by a majority of methods, even among those with associated entries in swissprot are unlikely encode real proteins. this last inference is supported by the generally poor level of experimental support for the expression of proteins encoded by transcripts evaluated as non-coding by the majority of methods  <cit>  . in the current experiment we have restricted our method to the analysis of messages for which alignments with murine refseq messages can be generated. inclusion of other organisms would be expected to raise the sensitivity of our method further.

 <dig> human transcripts with associated swissprot entries for which unambiguous homologs could be identified in the murine refseq collection were evaluated by the current method and  <dig> other approaches to discriminate coding from non-coding transcripts  <cit> . the table shows the number of transcripts evaluated as coding by each method, the mean number of other methods in agreement with the recovered evaluation and the percentage of transcripts for which each method agreed with the majority conclusion of the other methods

discussion
we present a new method for the discrimination of conserved coding sequences from conserved non-coding sequences exclusively on the basis of their evolutionary dynamics. this method is significantly more sensitive and selective than a previous approach developed in our group  <cit> . this is not surprising given that we use all of the information incorporated in the cstminer algorithm . indeed, we show that the incorporation of multiple indicators of coding potential and the use of machine learning techniques results in a significantly more powerful approach than the use of individual metrics, including that used by cstminer. we have used training and validation alignments that reflect the kind of data likely to be available to workers seeking to identify hitherto un-annotated coding regions to evaluate the proposed method. in particular, the data used in training and validation were generated from genome alignments and "coding" csts can be composed of coding and non-coding regions - avoiding over-estimation of the power of the method resulting from training and validation with entirely coding alignments. several methods have been proposed for the estimation of the coding potential of transcripts  <cit> . these methods do not assess evolutionary dynamics of homologous sequences, but depend on measures of characteristics of potential open reading frames in transcripts, and, on the degree of similarity of putative translation products to annotated protein sequences. they thus risk failing to detect coding messages that represent members of genuinely novel gene families. conversely, they are potentially prone to classifying transcripts as coding on the basis of similarity to transcripts incorrectly annotated as protein coding. our method should thus be seen as complementary to these approaches in the discrimination of transcripts, and is more suited to the discrimination of predicted exons and alignments generated from genomic comparisons.

the method presented here is, used in isolation, not suitable for the fine-level definition of boundaries of coding and non-coding regions. however, we believe that information derived from this approach should be useful in directing experimental approaches for the confirmation of the coding status  of candidate novel coding regions.

while numbers of indeterminate conserved regions were comparable with other studies , the method presented here consistently recovers around 85% of all coding csts  with 1% false positive designations. our approach is quick, produces a probability value indicating the confidence of the prediction and should be applicable to many different species pairs . this approach might be expected to be particularly useful in the detection of coding regions whose homologs are poorly represented in nucleotide or protein databases .

CONCLUSIONS
there is no reason to anticipate that the measures of evolutionary dynamics employed here should not be suitable for other species pairs and the pipeline proposed for the generation of training sets is easily transferable. we have also presented data suggesting that relatively few training instances are adequate for generation of a highly accurate discriminator and that the training of the svm is relatively insensitive to the presence of mis-annotated csts. in the light of this observation, it is also tempting to generate more specialized classifiers, for example considering divisions of gc content of csts to account for known genomic context variations in synonymous substitution rates  <cit> . other features apart from those proposed here are also under development and may easily be inserted into the method.

here we have used 1% confidence limits to illustrate the sensitivity and specificity of the proposed methodology. while poor annotation of training instances carries the risk of distorting confidence limits estimated for our system, in the majority of experimental situations, attempts at validation will proceed from the highest confidence predictions and confidence intervals need not be employed.

