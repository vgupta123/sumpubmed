BACKGROUND
today, mass spectrometry  is an indispensable technique for the analysis of proteins and peptides in the life sciences. various approaches have been developed to allow the comparison of protein abundances in cells between different environmental states. a growing number of studies in proteomics aim to quantitatively characterize proteomes for a better understanding of cellular mechanisms. these studies use either isotopic labeling or label-free methods for protein quantification  <cit> .

using isotopic labeling methods, protein mixtures are tagged with a stable isotope that can be used to tell samples apart by their mass shift and to directly compare peaks from different samples. with silac   <cit> , a metabolic labeling method, labels are introduced during cell growth and division. chemical labeling methods such as icat   <cit>  and itraq   <cit>  introduce the label into peptides after proteolytic digestion. these methods allow accurate quantification relative to the tagged sample at the expense of additional costly and time-consuming experimental processing steps. enzymatic labeling with 18o  <cit>  during or after proteolytic digestion is another technique that avoids the complications that chemical labeling may cause but can be applied if metabolic labeling is not possible. however, labeling efficiency differs between peptides, which causes difficulties when comparing abundances between different proteins.

in contrast, label-free methods directly use the signal intensities or spectral counts to estimate peptide abundances. but peak intensities also depend on peptide ionization efficiencies, which are influenced by a peptide's composition and the chemical environment. in other words, the sensitivity of a mass spectrometer varies between peptides. therefore, two peptides with identical abundance will generally lead to different peak intensities. in a recent review of label-free lc-ms, america and cordewener  <cit>  state that "normalisation of peptide abundance data is probably the most essential for improvement of the quantitative accuracy of the experiment." absolute quantification with very high accuracy using label-free methods is possible through the use of reference peptides  <cit> , an example being aqua   <cit> . but again, such methods require significant experimental effort.

consequently, label-free techniques are routinely used only for differential quantification, that is, the determination of concentration ratios between samples.

nonetheless, label-free methods have several intrinsic advantages over labeling techniques. obviously, they do not require labor- and cost-intensive labeling. also, there is no fundamental limit to the number of samples that can be compared. unlike labeling techniques, label-free methods do not increase the mass spectral complexity. they have the potential to analyze a higher range of protein concentrations and to achieve a higher proteome coverage.

there exist two fundamentally different experimental setups for label-free quantification using ms: in both cases, proteins are digested and peptides are separated using liquid chromatography . in the first case, the lc is directly coupled to an electrospray ionization  mass spectrometer, which allows a simple experimental setup and a rapid analysis of separated peptides. in the second case, lc fractions are spotted and analyzed using matrix-assisted laser desorption ionization  ms  <cit> . using maldi ms has certain advantages such as a more efficient data-dependent analysis: because the sample portions from the lc can be stored for several days and reanalyzed when necessary, it is possible to acquire fragmentation ion spectra for all ms parent ions that are of interest. spectra are easier to interpret and compare because mostly singly charged ions are produced and detected.

if an estimate of the peptide-specific sensitivity of the mass spectrometer were possible, this would allow the use of label-free techniques for absolute quantification. for this, different machine learning techniques have been proposed. lu et al.  <cit>  estimate the detectability of peptides for the apex method, i.e. the probability of a peptide being observed in a spectrum at all. the authors evaluate different classifiers for this purpose, and find that bagging with a forest of random decision trees produces the best results. the predicted values are then used to enhance the spectral counts-based, uncorrected abundance estimation by about 30%. tang et al.  <cit>  use two-layer neural networks to classify peptides into detectable and undetectable. they derive a minimum acceptable detectability for identified proteins , a cutoff value that maximizes the sum of true positives and true negatives. the mdip is shown to increase as protein abundance decreases, which, according to the authors, could be utilized to improve quantification. for both studies, data is acquired using lc coupled to esi. mallick et al.  <cit>  introduce the term proteotypic for peptides that are observed in more than 50% of the spectra they are expected in. the authors classify peptides from four different ms setups  into proteotypic and non-proteotypic peptides using a gaussian mixture model, and achieve a cumulative accuracy of up to 90%.

both lu et al.  <cit>  and tang et al.  <cit>  use predicted detection probabilities to correct peptide abundances. in both cases, the authors utilize detection probabilities as if these probabilities really were peak intensities. but for peak intensity correction, it seems much more appropriate to predict peak intensities directly. gay et al.  <cit>  classify peptides into observed and unobserved ones and, in addition, directly predict peak intensities via regression. unfortunately, this study is flawed by the fact that the same peptides were used in the training and test sets, to make up for the small size of the dataset.

for our initial evaluation, we do not use ms measurements of lc-separated peptides; instead, proteins are separated via 2d polyacrylamide gel electrophoresis . separated probes usually contain only a single protein that is subsequently digested and analyzed by maldi time of flight  mass spectrometry. in this experimental setup, given a correct preparation and only one protein in the digested gel spot, all peptides in a spectrum have exactly the same abundance. therefore, the peptide-specific sensitivity of the mass spectrometer can be accessed by comparing peak intensities in every such spectrum. a peptide-specific correction factor can then be calculated by dividing one over the corresponding peak intensity. it is understood that one cannot experimentally determine the peptide-specific sensitivity for all possible peptides.

in this study, we investigate if peak intensities of peptides in maldi-tof ms spectra can be predicted with machine learning  techniques. we do not predict the probability of a certain peptide to be detectable but instead predict its normalized peak intensity directly. this is an important step to facilitate the enhancement of label-free quantification accuracy. we reach a prediction accuracy of r =  <dig>  for across-dataset prediction, where r is the pearson correlation between predicted and observed intensities. our datasets stem from 2d-page separated proteins, but clearly, our results can directly be applied to enhance the quantification accuracy of maldi-based lc-ms experiments  <cit> . a study by mallick et al.  <cit>  shows that a slightly simpler problem, the prediction of proteotypic peptides, can be done successfully for lc-ms experiments. we are therefore hopeful that regression approaches like the one proposed here may be successful for lc-ms data, too.

let s be the sequence of a peptide we have identified, and let i be the intensity of the ms peak corresponding to this peptide. instead of directly using i as an estimate for the relative peptide concentration, we apply ml to compute a predicted intensity pi of the peptide solely from the peptide sequence. we can now calculate a corrected peptide intensity i' = 1/piÂ·i that replaces i in subsequent steps of the analysis. here, 1/pi is the peptide-specific correction factor. if the peak intensity estimate pi is accurate then i' is a better estimate than i of the relative peptide concentration.

methods
we use two sets of maldi-tof mass spectra to generate the datasets for this study. these were measured on a bruker ultraflex instrument  using proteins extracted from corynebacterium glutamicum  <cit> . the proteins were separated by 2d gel electrophoresis, then digested into peptide fragments with trypsin prior to ms analysis. the corresponding peptide sequences were derived from protein identification using mascot peptide mass fingerprinting  <cit>  and an in-house database containing c. glutamicum protein sequences. for the smaller dataset a, both the selection of spectra and determination of search parameters for the mascot identification were done manually, while for dataset b, both were done automatedly.

for dataset a, spectra were manually selected from a previously unpublished set of spectra. search parameters were chosen manually by an expert and included fixed  and variable modifications , and no missed cleavages were allowed. the list of spectra was filtered for the 20% of spectra with the best mascot score and the largest difference from the second best hit, resulting in  <dig> spectra being used for further analysis. of  <dig> identified proteins,  <dig> were present in multiple spectra. for dataset b, spectra were run through fully automated mascot peptide mass fingerprinting search with  <dig> different sets of search parameters. these included with and without oxidation of methionine, tolerance within { <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  750} ppm, and up to { <dig>   <dig>  2} missed cleavages allowed. the resulting list was filtered automatically to fulfill the following properties: a) protein mass in range  <cit>  da, b) pi between  <dig> and  <dig> because of the 2d gel used, c) highest mascot protein hit score above  <dig>  and d) sequence coverage above 15% using the search parameters that produced the highest score. spectra with more than one high-scoring hit were also removed. application of this protocol left  <dig> spectra for further analysis. of  <dig> identified proteins,  <dig> were present in more than one spectrum. to summarize the differences, a can be considered a small, carefully chosen dataset while b is larger and of lesser overall quality. an overview table can be found in additional file 1: datasets and a histogram showing the number of spectra per protein in additional file 2: spectranumbers. the additional files  <dig> and 4: dataset_a_protein_list and dataset_b_protein_list show lists of the identified proteins. we give a short outline of our spectra preprocessing pipeline; details are deferred to additional file 5: preprocessing). after de-noising with a savitzky-golay filter  <cit> , baseline correction, and removal of noise peaks, peak intensities are extracted from the spectra. we unfold isotopic distributions by adding up all peak intensities of isotope peaks that belong to the same peptide. the resulting list of peaks is matched against masses calculated from a theoretical tryptic digestion. the matching of sequences to peaks is straightforward in this case because the peptide sequences are known. we ignore missed cleavages and variable modifications in the matching process. we allow for a mass error of up to  <dig> da to make up for calibration errors. in case of multiple peaks inside the allowed mass error range, the one closest to the theoretical mass is chosen.

in dataset a, for  <dig> out of  <dig> expected peptides, we detected the corresponding peak in the mass spectrum. in dataset b,  <dig> out of  <dig> predicted peaks were detected. for most peptide sequences, only one peak intensity measurement exists: in dataset a,  <dig> % of peptide sequences are unique, whereas in dataset b, this is true for  <dig> % of the peptides.

abundances differ between spectra. to use intensities from different spectra together in one dataset, we need to normalize them. since the amount of protein in each spectrum is unknown, we introduce two normalizations for peak intensities. we compare the performance of both normalizations in our experimental evaluation.:

â¢ normalization by corrected mean ion current . the intensity of a peak p is scaled by the mean ion current, i.e. the mean of all baseline-corrected measurements c <dig> ...,cn in the spectrum:

 ipmic=ip1nâi=1nci, 

where ip denotes the raw intensity of peak p after peak extraction. here, index i runs over all raw values  of the spectrum the peptide was found in. by doing so, we take into account intensities of unmatched peptides as well as differences in the overall sensitivity of the detector.

â¢ normalization by sum of peptide peak intensities . the intensity of a peak p is scaled by the sum of all matched peptide's peak intensities i =  <dig> ...,p to yield

 ipsum=1000âipâi=1pii, 

where ii denotes the intensity of the ith peptide peak after peak extraction. a similar approach has been used by radulovic et al.  <cit> . the factor  <dig> is used for numerical reasons.

some peptides are present in more than one spectrum and, hence, these peptides show more than one peak intensity value. most learning architectures do not cope well with different target values per input.

therefore, we calculate an Î±-trimmed mean with Î± = 50% for peptides with more than three target values, and a mean for peptides with two or three target values. as we will see below, this also allows us to estimate the potential prediction accuracy. as a final preprocessing step, we logarithmize intensities with a natural logarithm such that the resulting error becomes additive, which stabilizes the variance  <cit> , and the values themselves become approximately normal distributed .

we can now state the peak intensity prediction problem as a supervised learning problem. a training set Î = {j, j =  <dig> ..., n} consists of input-output pairs  where x â ât is an input peptide feature vector, and y â â is the normalized intensity we want to predict . obviously, this is a  regression problem and a wide range of techniques can be applied.

when calculating target intensities for each peptide, we assume that all proteins were correctly identified, we assume perfect digestion, and we ignore variable modifications for all steps following identification. we are aware that this is not perfectly true in reality: there are a few missed cleavages, variable modifications were found during the database search, and we can not totally exclude the possibility of misidentification, even though mascot thresholds were chosen to practically exclude this case. in this sense, our datasets can be seen as "imperfect" and cleaner datasets could be used. but if intensity prediction is possible using this imperfect, partly erroneous and noisy data, results will only improve when cleaner datasets are available. to show that we in fact learn to predict intensities from peptide sequences, we shuffled the assignment of peptide sequences to target values and found that no prediction is possible for the shuffled dataset; see below.

the same arguments hold for the effect of ion suppression  <cit> : the signal of a compound is suppressed in the presence of other compounds that compete for ionization. intensity prediction could take this effect into account if each peptide occurred in multiple spectra with all possible combinations of other peptides, and if there was no contamination. however, such a dataset is impossible to acquire. knowing this, we neglect the fact that peptide peak intensities depend not only on the peptide's constitution, but also on the combination of other peptides present. we consider this effect an additional noise component our method has to cope with. in any application of our method, this effect would always be unknown, and we aim at a realistic estimation of the performance of our method.

machine learning methods
we selected two complementary non-linear regression architectures. Î½-support vector regression   <cit>  has excellent generalization abilities and copes well with high-dimensional input data  <cit> . however, it is difficult to interpret its models, and parameter-tuning can be time-consuming. the second learning architecture, the local linear map   <cit>  is less accurate if applied to higher-dimensional feature spaces but is very efficient . it is more transparent and can be used for peptide prototyping as shown in  <cit> . both architectures represent different principles of learning non-linear regression functions. there exist cases where a linear model outperforms more sophisticated regression models. therefore, we apply a simple linear model   <cit>  for comparison. other methods have been tested and perform similarly to or worse than those presented here .

Î½-support vector regression 
support vector machines are a class of learning algorithms that are designed to implicitly transfer input feature vectors into a high-dimensional feature space where classes are linearly separable and the optimal linear decision boundary can be calculated. in practice, however, it depends on the choice of a kernel function and the data whether linear separability is actually achieved. for support vector regression, the Îµ-insensitive loss function |y - f|Îµ = max { <dig>  |y - f| - Îµ} was introduced by vapnik et al.  <cit> . here, errors are only considered if they are higher than Îµ for some fixed Îµ >  <dig>  since the choice of an appropriate Îµ can be difficult, the Î½-svr introduced by schÃ¶lkopf et al.  <cit>  finds the best Îµ automatically by minimizing a cost function. instead, Î½, an upper bound of the number of allowed errors and a lower bound to the number of support vectors, has to be chosen a priori. the Î½-svr generalizes an estimator for the mean of a random variable discarding the largest and smallest examples , and estimates the mean by taking the average of the two extremal values of the remaining examples. this results in good robustness of the Î½-svr. other parameters that have to be chosen are the regularization parameter c and kernel width Î³. parameter c controls the trade-off between the weight of errors and the complexity of the regression function. parameter Î³ controls the width of the radial basis function k=eâÎ³||xâxâ²|| <dig>  which we use as kernel function. we use the libsvm implementation of the e <dig> package available for r  <cit> .

local linear map 
local linear maps  <cit> , a type of artificial neural net, combines an unsupervised vector quantization algorithm based on self-organizing maps   <cit>  with supervised linear learning principles for prediction of peak intensities. the llm can learn global non-linear regression functions by fitting a set of local linear functions to the training data. it has been successfully used for peptide prototyping  <cit>  and provides a basis for peptide feature profiling and visualization.

motivated by the som, an llm consists of a set of nl regular ordered nodes vi, i =  <dig> ...,nl, which are connected to each other via a two-dimensional grid structure, defining a neighborhood between the nodes and a topology in feature space. each node consists of a triple vi=. the vectors wiinââdin are used to build prototype vectors adapting to the statistical properties of the input data xÎ¾ââdin. the vectors wioutââdout approximate the distribution of the target values yÎ¾ââdout. the matrices aiââdinÃdout are locally trained linear maps from the input to the output space.

in the unsupervised training phase, the prototype vectors wiin are adapted following the som learning rule: the vectors wiin are pulled towards the input pattern xÎ¾ according to the distance between the input pattern and the corresponding closest prototype in input space wÎºin with Îº=argâ¡minâ¡i{||xÎ¾âwiin||}. the learning procedure changes the weights according to a gaussian neighborhood function hÏ with width Ï decreasing over grid distance rk:

 hÏ)=expâ¡2Ï2). 

the learning step widths Ïµ, Ïµa, Ïµout â  for updating neighbors and s are decreased during training. after adapting the prototypes, a classification can be applied by assigning every input vector x to its closest prototype.

after unsupervised adaptation and tessellation of the input space, an input feature vector is mapped to an output by the corresponding local expert:

 c=wÎºout+aÎº. 

the weights wiout and the linear map ai are changed iteratively by the gradient descent learning rules:

 Îwiout=ÏµoutâhÏâ),Îai=ÏµaâhÏâ)ât||xÎ¾âwiin|| <dig>  

the concept of approximating nonlinear functions by fitting simple models to localized subsets of the data is related to other regression approaches like locally-weighted regression   <cit>  and to radial basis functions   <cit> . hastie et al. demonstrated the usefulness of locally linear function fitting as well  <cit> . we use here our own implementation of llm as an r package.

linear model 
a linear regression model assumes the data to have the structure y = xtb, which corresponds to data lying on a straight line. here, x is the input data in matrix formulation, y the vector of target values, and b a vector of coefficients that have to be found when adapting the model to the data. the ordinary least squares  algorithm is applied to find the coefficients. ols minimizes the squared differences  between the model's output and the target values of the training examples.

feature extraction
we cannot directly use peptide sequences as input for the learning architectures, and derive numerical feature vectors xj to represent molecular features of peptide j. in bioinformatics, peptides are usually represented as strings over the alphabet of amino acid characters. however, a biochemist is more interested in the chemical properties of a peptide to characterize it. these paradigms motivate different feature sets:

â¢ a 20-dimensional feature set with only single amino acid counts .

â¢ a purely sequence-based 9220-dimensional sequence feature set . each peptide is mapped to the 9220-dimensional vector by counting how often a certain k-mer appears in the sequence. each feature vector contains  <dig> counts for all single amino acids ,  <dig> counts for all di-peptides,  <dig> counts for all tri-peptides, and two times  <dig> counts for terminal di-peptides at the beginning and end of the peptide sequence. additional file  <dig>  shows the frequency distribution of di- and tri-peptide strings in the used datasets. here, "di-peptides" or "tri-peptides" refer to substrings of the peptide sequences, not to single molecules consisting of two or three amino acids. we will show below that the pure sequence-based feature set is often not sufficient for a decent prediction of peak intensities, what motivates the use of the next feature set.

â¢ a 531-dimensional chemical feature set  computed from amino acid attributes. attributes are taken from the amino acid index database  <cit> . each amino acid index aa =  consists of twenty real values for the twenty amino acids. let m be the number of occurrences of the amino acid s in the amino acid index. for a peptide s = s <dig> ..sn, the value for the corresponding feature f is calculated as the sum of attribute values, f=âk=1naam. this value reflects the overall property of the peptide. there are  <dig> attributes in the amino acid index database, therefore we can calculate  <dig> such features. in addition, we use features for peptide length, mass, and numbers and fractions of acidic, basic, polar, aliphatic, and arginine residues. finally, three features for gas-phase basicity are added to the feature vector: a) the estimated gas-phase basicity is calculated as proposed by zhang et al.  <cit>  as well as b) a sum over the residual values of the amino acids that were used for this estimation, and c) that sum scaled with the length of the peptide.

in the course of our analyses, we also evaluate what features are particularly important for the task of predicting peak intensities. this leads us to the following feature set:

â¢ a reduced feature set resulting from forward stepwise selection on aa and seq. this selected subset feature set  is 18-dimensional; its features can be found in table  <dig>  the following section explains how they were chosen.

the "selected" column shows the number of times out of twenty runs of a forward stepwise selection that selected the corresponding feature. hand-picked features are printed in bold face. feature selection on the aa  and seq  feature set were done independently of each other. the seq feature set fully includes mono. no di- or tri-peptide string was selected consistently.

all features are centered and normalized by variance prior to training. datasets and feature vectors are available from . raw data and identifications are available from .

feature selection
often, accuracy, speed, and interpretability can be increased by reducing the number of features. to select a few features out of hundreds, we apply a simple greedy selection method as follows: a forward stepwise selection as described in  <cit>   was applied twenty times to the aa and seq feature set of dataset a . this method starts with the intercept and calculates a value fi=eâe+e+/ for each feature i, where e is the prediction error of a 10-fold cross-validation with the Î½-svr on the current model and e+ the prediction error of the model with the additional feature i. n denotes the number of training examples, and k the number of features in the smaller model. the feature that gives the highest value fi is added to the model before the next iteration. the procedure is repeated until no feature produces an fi that is higher than the 95th percentile of the f <dig> n-k- <dig> distribution. for the Î½-svr, we used the parameters chosen by the grid search on the full feature set, since it is infeasible to repeat a grid search for each selection step. the resulting feature set depends on random partitioning during 10-fold cross-validation. thus, each application of the selection algorithm can produce a different feature set. we selected those features that were selected most often , reviewed the chosen features, and added others that might also be important for peptide-specific sensitivity of a mass spectrometer.

evaluation techniques
to evaluate correlation coefficients recorded in the following, we want to estimate how good our prediction accuracy can possibly get. to do so, we analyze the variation of intensity values for each peptide. recall that many peptides are present in more than one mass spectrum, and one peptide sequence may correspond to multiple peak intensity values. if we compute the correlation of normalized intensity values for all peptides with multiple values, we find a correlation coefficient of r =  <dig>  for dataset a, and r =  <dig>  for dataset b . to generate training data for our learning approaches, we compute target values as the trimmed mean of intensities for peptides with more than three observations, which reduces the effect of outliers. comparing the target values of each peptide sequence to its trimmed mean for all peptides with multiple target values, we record a correlation coefficient of r =  <dig>  for dataset a and r =  <dig>  for b. the corresponding scatter plots are shown in additional file 8: tmbetweenpeptidecorrelation. since we use trimmed mean intensities as input, these correlation values can be interpreted as "upper bounds" for correlation coefficients any machine learning technique may achieve using this data. we are confident that for other datasets, even better prediction accuracies are possible.

we determine the best parameter set for each regression model using 10-fold cross-validation. we make sure that the ten sets contain disjunct peptide sequences. a grid search over the parameter space is performed to determine optimal parameters. the 10-fold cross-validation is applied for each parameter set. the best parameter set is the one with highest mean pearson correlation coefficient  between target and predicted value over all ten test sets. the mean squared error  has also been calculated. however, it is not comparable between datasets without additional normalization, whereas the pearson correlation coefficient is independent of the scale used. in addition, in this case, the parameter set with the lowest mse is always the same as the one with the highest r, or very close to it.

for Î½-svr, the grid search runs over c â {e- <dig>  e- <dig> ...,e13} and Î³ â {e- <dig>  e- <dig> ...,e5} in steps of e <dig>  and Î½ â { <dig> ,  <dig> ,..., <dig> } in steps of  <dig> . the parameters sampled by the llm are prototypes n â { <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  25}, width of the neighborhood function Ï â { <dig>   <dig>   <dig>   <dig> }, linear vs. exponential decrease of learning step size, and number of learning iterations t â { <dig>  50}.

the final model is built by retraining the whole dataset with the optimal parameter set determined in the model selection step. since the parameters may be adapted to the data used for cross-validation, we estimate the performance on new data by validating the prediction accuracy on the other dataset: the model trained on a is validated on b, and vice versa. both datasets have  <dig> peptides in common, therefore we also validate across datasets with these  <dig> peptides excluded. the mse of this across-dataset validation can be reduced by normalizing by variance and centering the data. pearson's correlation coefficient is not affected by such operations.

RESULTS
in order to assess the methods described above, we performed a series of experiments testing various aspects. we compare the prediction performance of three types of learning architectures on two different ms datasets for four different feature sets. performance is evaluated via 10-fold cross-validation, and we assess the generalization performance by predicting each dataset with a model trained on the other dataset. prediction results for mic normalization are shown in table  <dig>  we first present results on the best performing predictors and then analyze the influence of the individual components in more detail.

values " <dig> " indicate that the correlation coefficient was in the range . the best value in each section is printed in bold face.

best performance
the normalization by corrected mean ion current  generally has a slight advantage over the normalization by sum of peptide peak intensities , while other observations were identical for both normalization types. therefore, results and scatter plots for sum normalization are deferred to additional files. see table  <dig> for a summary of all additional files with scatter plots.

this table summarizes the scatter plots available as additional files, and in which file a certain plot can be found. the llm scatterplots for the seq feature set as well as all lm scatter plots are not been included because the results  are poor. "ds" abbreviates "dataset".

among all combinations, the best prediction result is achieved using the Î½-svr algorithm on dataset a with mic normalization and the mono feature set , shown in fig.  <dig>  here, 10-fold cross-validation yields an overall correlation of r =  <dig> . in the across-dataset validation, the correlation coefficient is only slightly reduced to r =  <dig> , or r =  <dig>  when peptides present in both datasets were excluded. Î½-svr, using chemical  or selected subset  feature sets results in prediction accuracy almost as good as for the mono feature set; see table  <dig>  as well as additional files of a and b scatter plots referred to in table  <dig>  these correlations are significant and show that we can predict peak intensities with statistical learning methods.

to show that predicted values are an actual signal related to peptide sequences, and not some random pattern the learning machines find in the data, we randomly shuffle the assignment of peptide sequences to peak intensities. no good correlation can be observed in cross-validation when evaluating shuffled datasets: from dataset a we generated  <dig> datasets with randomly permuted target values. for each of the  <dig> shuffled datasets, we trained a Î½-svr with sss features and parameters optimized using another shuffled dataset. the best correlation with this dataset was r =  <dig> . for the  <dig> datasets, we reach a mean correlation coefficient of r = - <dig> . none of the shuffled datasets generated for a good correlation coefficient . see fig.  <dig> for an exemplary scatter plot. this is a clear indication that we are picking up the true signal, that is, the predicted intensities are correlated to the peptide sequence.

the scatter plots of target vs. predicted values in fig.  <dig> are typical for dataset a. the cross-validation plot shows a point cloud that is almost diagonal and shows considerable spread especially for low values. the across-dataset prediction plot  shows that values of a are systematically predicted too high when the model trained and parameter-tuned on dataset b is used. an analysis of the statistical properties of the target values of both datasets reveals that b has a higher mean and its distribution is more skewed towards higher values. this difference can be explained by the fact that both datasets were wet-lab processed by different persons, dataset b by multiple persons even, and they were analyzed with different settings of the mass spectrometer. in applications of our method, an additional normalization step should be applied that accounts for these differences.

dependence on peak intensity
the most reliable prediction for both datasets is achieved with slightly above intermediate intensities. low target values have the highest prediction error . note that there is a large number of samples with small target values in our training sets, so this effect cannot be attributed to undersampling, meaning that low intensities are more difficult to predict. note that we predict the logarithm of intensities, whereas noise in the mass spectra is additive and, hence, will have a stronger effect at low intensities. also, noise in regions of lower intensities behaves differently from that of higher intensities  <cit> . the problem might be overcome when more measurements for each peptide become available. otherwise, the use of two or more different models specialized for different intensity ranges might overcome this problem.

dependence on the learning architecture
of all the learning architectures Î½-svr gives the best results in all cases, and the llm's performance is comparable in the cross-validation and slightly worse in the generalization case . where data becomes available gradually during experiments, we suggest applying the llm in early stages: it is faster to train than the Î½-svr and can be adapted with additional data without time-consuming complete retraining. when enough data has been collected and results begin to stabilize, the Î½-svr should be used to obtain a final prediction model. the lm shows bad overfitting: it beats the llm for the aa feature set in the cross-validation, but shows absolutely no correlation when generalizing to new peptides, and only very little correlation for the lower-dimensional feature sets. this suggests a non-linear relationship between feature vectors and the target values for any of the peptide representations.

influence of features sets
a comparison of different feature sets shows that the mono and sss feature sets generally lead to similar prediction results, the sss feature set having a slight advantage in more cases than the mono set. thus, our feature selection increases accuracy slightly for most cases. for the 531-dimensional aa feature set, only the Î½-svr successfully extracts the relevant information to achieve comparable prediction accuracy. for the other learning algorithms, the high dimensionality with some features being highly correlated to each other, leads to bad generalization performance, i.e. inability to predict intensities for new peptides. the sparse 9220-dimensional seq feature set performs worse than any other feature set. while, in principle, this feature set captures more information about amino acid order, it is inappropriate for our small training datasets. this is a general problem: there are indications that not only the amino acid frequencies but also their order determine peak intensities. however, the amount of information necessary to capture this relationship explodes. even if the amino acid order is encoded only partially, as in this case, more training data is needed.

analysis of the selected features
the features selected most often in the feature selection on dataset a were the estimated gas-phase basicity at  <dig> k , the absolute number of arginine residues , the relative population of conformational state e , the hydropathy scale based on self-information values in the two-state model at 36% accessibility , the hydrophobicity coefficient in rp-hplc, c <dig> with  <dig> %tfa/mecn/h2o  of the aa feature set. from the seq feature set, the numbers of arginine , phenylalanine , and methionine  residues were selected most often. table  <dig> shows the exact numbers. details of the results of the forward stepwise selection  can be found in additional file 9: stepwise selection.

forward stepwise selection is a greedy method and does not find an optimal solution. none of the selected sets from each single run of the method leads to better performance than that of the original aa set. thus, we add in other features that extend the description of the peptide.

we access the importance of the single features that constitute the final sss feature set using random forests for regression  <cit> . fig.  <dig> visualizes the percentage increase of the prediction error if values of the corresponding feature are randomly permuted. according to this, vasm <dig> is the most important feature, followed by gb <dig> and the peptide's theoretical mass.

in general, the automatically selected features are of higher importance than hand-picked ones . of the hand-picked features, the mass and kerr-constant increments  show the highest increase in error, i.e. are most important.

the high importance of vasm <dig> indicates that the amino acids' conformation might play a role for peptide-specific instrument sensitivity. however, not much is known about the conformation of peptides after tryptic digestion, when crystallized within the matrix, or as ions in gas-phase. looking at the chemistry, it makes sense that the gas-phase basicity influences ionization efficiency. the number of positive charges have been reported by mallick et al. to be relevant for the probability of observing a peptide ion  <cit> . the latter has often been chosen in the feature selection but is the least important one in the sss feature set according to our feature importance accession. the number of histidine residues  has been found to be correlated with detection probabilities in maldi ms by mallick et al.. it is the only basic residue except for k and r, presumably making a difference, though weakly, in basicity for tryptic, i.e. already quite basic peptides. however, it is one of the three least important features according to the random forest method in our dataset. we can exclude the correlation  between h and fauj <dig> as the cause of their low importance: the importance ranking is the same if one of both is left out completely. the hydrophobicity features  are of intermediate importance. since all except a few peptides in the studied datasets are tryptic, i.e. have lysine  or arginine  as the c-terminal amino acid, the r feature value almost always indicates whether the peptide string ends in r or not. in the latter case, it most certainly ends in k. the feature r is highly correlated with the gb <dig> value: the estimated gas-phase basicity takes on values distributed around three levels, which correspond to peptides ending in r , those ending in k, and those that have neither. nonetheless, both features seem to be complementary according to our results.

a two-sample t-test of the list of normalized intensities of peptides containing a given mono- or di-, or tri-peptide substring against the list of those not containing it can show whether differences in the mean intensities between these groups are significant. we apply this test to complement the information obtained by the forward stepwise selection on seq. in this test, each substring is tested independently of all others, whereas the forward selection would not choose a substring if its effect is already covered by another one. details including p-values are given in table  <dig>  the amino acid features number of arginine , lysine , methionine , and the terminal di-peptide gk show significant differences between their mean values for both datasets. apart from gk, significant di-peptides differ between both datasets and almost always contain one of the aforementioned , so their effect can be attributed to these single amino acids. in dataset a, there are significant differences for histidine , tyrosine , phenylalanine , and glutamine , whereas none of these shows up in dataset b. here, tryptophane  containing peptides show a significant difference in the mean intensity instead. if unmatched peptides are included with a target value of zero, the amino acids threonine  and tryptophan  show a significant difference for both datasets .

results of two-sample t-tests of set s+  against the set s- of those not containing it in the corresponding dataset . only substrings that occur in more than  <dig> / <dig>  peptides and with a p-value â¤  <dig>  are shown. an "a" as a prefix denotes that the substring is located at the beginning of the string, "e" as suffix means it is located at the end of the string. otherwise, the substring can occur anywhere in the peptide . rows in bold face mark substrings that are present in the lists of both datasets.

dataset dependence
generally, dataset a gives much better results than b, even though the latter is larger . an obvious reason is the higher within-peptide variance of normalized intensities and the higher fraction of peptides without replicate measurements in dataset b. nonetheless, the algorithms are able to draw the main trends from b, since a can be predicted with a model trained on b even better than b itself . this shows that despite the high variance in a dataset, the learning algorithms can capture the main trends, thus enabling prediction of a less noisy dataset.

CONCLUSIONS
the machine learning approach presented here is able to predict peptide peak intensities in maldi mass spectra, thus showing that the prediction of sensitivity factors for mass spectrometry based on chemical and sequence features is feasible. even for small datasets, significant correlations can be achieved.

although we have not yet evaluated our method against uncorrected spectral counts and correction based on detection probabilities, we believe that our method is better suited for peptide quantification than the latter, since we directly aim at predicting the reciprocal of the peptide-specific instrument sensitivity. as the next step in our research, we want to assess how predicted intensities affect quantification accuracy. we will apply our method to protein mixtures of known concentrations to estimate how well we can predict protein concentrations at the semi-quantitative level. also, the application of the method to other ionization techniques, in particular data from electrospray ionization , is an obvious next step. we are confident that we can improve prediction performance using larger datasets with more replicates. regarding additional features to be included for ml, features comprising conformational aspects of peptides appear very promising. due to the high computational costs, it will have to be assessed whether the estimation of such features is feasible for this application.

authors' contributions
wt generated the features, produced and evaluated the Î½-svr results, and did the forward feature selection. as carried out the llm experiments. ok contributed to the feature extraction. as, ok, sb, and wt wrote the manuscript. tn revised and edited the manuscript. all authors agreed on the final manuscript.

supplementary material
additional file 1
overview of dataset properties.no. duplicated proteins: number of proteins for which more than one spectrum is contained . no. matches: number of distinct peptides for which peaks are found in the spectra, considering only peptides without missed cleavages. no. non-matches: number of theoretical peptides for which no match was found. duplicated peptides: percentage of peptides found in more than one spectrum. modifications: peptide modifications considered in the peak matching procedure.

click here for file

 additional file 2
histogram of the number of spectra per protein. more than 50% of the proteins in dataset b are presented by only one measurement.

click here for file

 additional file 3
lists of the identified proteins. identified proteins, their coding region, description, mascot score, and gendb  <cit>  id.

click here for file

 additional file 4
lists of the identified proteins. identified proteins, their coding region, description, mascot score, and gendb  <cit>  id. additional information about the mascot search parameters that led to the highest score are included: coverage, number of peptides , missed cleavages, mass tolerance, modifications, and the number of search parameter sets that led to the identification of the corresponding protein.

click here for file

 additional file 5
raw spectra preprocessing and peak extraction details.

click here for file

 additional file 6
q-q plots for target values of both datasets. intensities have been normalized by mic and logarithmized. the target values of dataset b fit the normal distribution almost perfectly. those of dataset a deviate from a normal distribution at both ends.

click here for file

 additional file 7
number of times di-/tri-peptides occur in the seq feature set of dataset a. while a good portion of the dimers occurs more often than ten times in the whole dataset, most of the trimers do not show up at all or just once. in principle, the sequence feature set captures some of the amino acid order in the peptides. however, considerably more data is necessary to fill this feature space.

click here for file

 additional file 8
scatter plots of duplicate normalized intensity values against trimmed-mean target values. the recorded correlations can be considered upper bounds of the achievable prediction performance if only multiple measurements per peptide were used .

click here for file

 additional file 10
cross-validation scatter plots and pearson correlations for dataset a . cross-validation scatter plots of dataset a with the Î½-svr 

click here for file

 additional file 11
cross-validation scatter plots and pearson correlations for dataset a . cross-validation scatter plots of dataset a with the llm 

click here for file

 additional file 12
cross-validation scatter plots and pearson correlations for dataset a .

click here for file

 additional file 13
cross-validation scatter plots and pearson correlations for dataset b . cross-validation scatter plots of dataset b with the svr 

click here for file

 additional file 14
cross-validation scatter plots and pearson correlations for dataset b . cross-validation scatter plots of dataset b with the llm. 

click here for file

 additional file 15
cross-validation scatter plots and pearson correlations for dataset b .

click here for file

 additional file 16
across-dataset prediction scatter plots and pearson correlations for dataset a . across-dataset prediction of dataset a with a model from b with the Î½-svr 

click here for file

 additional file 17
across-dataset prediction scatter plots and pearson correlations for dataset a . across-dataset prediction of dataset a with a model from b with the llm 

click here for file

 additional file 18
across-dataset prediction scatter plots and pearson correlations for dataset a .

click here for file

 additional file 19
across-dataset prediction scatter plots and pearson correlations for dataset b . across-dataset prediction of dataset b with a model from a with the Î½-svr 

click here for file

 additional file 20
across-dataset prediction scatter plots and pearson correlations for dataset b . across-dataset prediction of dataset b with a model from a with the llm 

click here for file

 additional file 21
across-dataset prediction scatter plots and pearson correlations for dataset b .

click here for file

 additional file 9
details of the forward stepwise selection process. features selected as well as performance values for each run of the forward stepwise selection process are shown in separate tables for selection in aa and seq.

click here for file

 acknowledgements
as funded by deutsche forschungsgemeinschaft , wt funded by international nrw graduate school for bioinformatics and genome research. the authors thank andreas wilke, nicole hansmeier, christian rÃ¼ckert, and jÃ¶rn kalinowski for providing the spectra and mascot identification.
