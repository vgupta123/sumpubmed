BACKGROUND
over the last few years the internet has become a large information repository that is accessed manually in the vast majority of cases. however, the flexibility and openness of the internet in computer systems is lacking when software applications are connected. the main aim of the semantic web is to automatically perform tasks done in the current contents web. this will be done by making explicit the semantics of the contents, thereby providing unambiguous knowledge to web documents and applications. for any field of knowledge, and particularly in life sciences, research on semantic web infrastructures and applications can be especially helpful to improve efficiency in the finding, collection and organization of data stored in the growing number of resources which make their semantics explicit.

in the context of life sciences, the frame of systems biology is being merged  <cit> . it is supported by all high-throughput methods which generate large amounts of data that cannot be covered simply by the human mind. this field includes a wide variety of concepts and methods but, in general, it can be considered the analysis of living systems, through the study of the relationships among the elements in response to genetic or environmental perturbations, with the ultimate goal of understanding the system as a whole. a “system” can be considered at different levels, from a metabolic pathway or gene regulatory network to a cell, tissue, organism or ecosystem. the number of information repositories and services for biological elements  is growing exponentially. consequently, systems biology is the prototype of a knowledge-intensive application domain for which the semantic web should be particularly interesting.

initially, our system will comprise the biochemistry, molecular biology and physiopathology related to amines. we are using this system to develop, validate and apply our infrastructure and are focusing at this stage mainly on amines derived from cationic amino acids. they are histamine and polyamines, which have been the main area of research in our laboratory for the last  <dig> years  <cit> . their biosynthetic pathways start with the alfa-decarboxylation of their respective amino acid precursors by enzymes that cannot be easily purified from their native sources.

these metabolic pathways also involve different proteins with transferase, oxidase and dehydrogenase activities that are not fully-characterized yet  <cit> . however, these pathways are considered well-defined modules of secondary nitrogen metabolism, with minimum input and output from/to other biochemical modules, where most of their components have at least been defined. concerning their physiological roles, these compounds play pleiotropic roles in human and animal physiology, being involved in many different physiopathological conditions. histamine has been related to allergies and inflammation, gastric acid secretion, neurotransmission and tumour progression  <cit> . polyamines are essential for cell growth and their levels are closely linked to the survival of every living cell. thus, their metabolism is a promising target for anti-proliferative strategies of chemoprevention and the treatment of cancer and parasitic infections. in addition, they also act as differentiation and neurotransmission regulators  <cit> . from this, we can deduce that the choice of these biomodules as pilots for our integration project has the following advantages:

• the physiopathological problems  associated with these compounds and their metabolic pathways are global and affect most of humanity at some stage of their lives. these circumstances explain the growing interest in the information obtained from this project.

• the information on amine-related processes is dispersed among specific bibliographies of many different scientific areas: oncology, immunology and haematology, neurobiology, pharmacology and basic biophysics, biochemical and molecular biology research, which makes manual integration of all the valuable data more difficult. thus, this project can contribute to the efficiency of the scientific advances in the field.

• many molecular questions still remain to be solved with respect to the structure/function relationships of their components, the extracellular and intracellular communication pathways involved in regulating these metabolic pathways and the physiological effects of these amines. therefore, the integration of information  can provide the best perspective for analyzing these complex molecular relationships and networks established in living systems  <cit> .

• many of the ontologies and tools developed throughout this pilot project could be easily extended to solve other biological problems, as deduced from the most recent bibliography  <cit> .

• finally, this interdisciplinary group combines experimental and bioinformatics approaches for studies in this field. thus, any result or prediction can be easily checked and even experimentally validated  <cit> .

we propose a generic infrastructure for publishing and managing knowledge and information on the semantic web. this infrastructure is based on a resource directory, called semantic directory, containing information about web resource semantics. this paper focuses on the resolution of data integration problems by concentrating on our proposal of a generic infrastructure architecture. we have developed an ontology-based mediator, which has been applied to solve a data integration problem in this biological domain . the main advantage of this mediator is that data sources and services can be easily plugged into the system . furthermore, the semantic description is a simple process, thanks to the use of the proposed infrastructure . this mediator is currently available through a use case published as a web site  <cit> .

related works
this section describes some related works which are grouped depending on their goals. thus, we first describe data integration systems . the second group is composed of systems that have been developed to solve biological problems, and which provide solutions from different points of view: web services, workflows, and web portals.

our system is a data integration proposal with a test web interface, which uses ontologies to describe resource semantics and these resources are published as web services. the workflow described in the use case has been hand coded , and in the future, it will be interesting to include workflow management capabilities.

our main goal is to provide an infrastructure for interoperating applications in the semantic web. this infrastructure can be used to build different kinds of applications, so we have developed as a use case the ontology-based mediation system, a system for locating semantic web services  <cit>  and an ontology clustering algorithm  <cit> . in this paper, we focus on data integration solutions. thus, we have proposed a mediation architecture based on the wrapper-mediator approach, which has some interesting characteristics:

• it follows an ontology-based approach, in which the ammo ontology is used as integration schema, but we have also used this ontology to perform basic reasoning processes .

• wrappers are published as web services to enable their distribution and use in different applications.

• the mediator is divided into components to enable its extension by including new components .

• information about relationships between resources and the ammo ontology is distributed using a generic infrastructure for semantic web applications.

these characteristics have been proposed in previous works, and have been successfully combined to build a useful application in systems biology for solving real scientific problems. the use of this application by asp members has provided them with an easy-to-use way of solving daily tasks by integrating different data sources. furthermore, the system provides an intuitive interface, in which the user can click on enzymes in a specific metabolic pathway. this approach can be extended to other metabolic pathways, and can include specialized data sources as kegg, reactome, etc. .

data integration approaches
data integration systems are formally defined as a triple <g,s,m> where g is the global  schema, s is the heterogeneous set of source schemas, and m is the mapping that maps queries between the source and the global schemas. both g and s are expressed in languages over alphabets comprised of symbols for each of their respective relationships. the mapping m consists of assertions between queries over g and queries over s. when users send queries to the data integration system, they describe those queries over g and the mapping then asserts connections between the elements in the global schema and the source schemas.

the most important proposal to solve the data integration problem is the wrapper/mediator architecture. in this architecture, a mediator  is established between data sources  and applications. a wrapper is a data source interface that translates data into a common data model used by the mediator. the user accesses the data sources through one or several mediator systems which present high-level abstractions  of combinations of source data. the user does not know where the data comes from but is able to retrieve it using a common mediator query language.

mediator-based integration has query translation as its main task. a mediator in our context is an application that has to solve queries formulated by the user at runtime in terms of either a single or an integrated schema. these queries are re-written in terms of the data source schemas in order to delegate the query resolution to the data sources. thus, expressing the relationships between the integrated schema and the data source schemas is a crucial step in the mediation system development. the two main approaches for determining the relationships  between the integration schema  and the data source schemas  are: global-as-view  and local-as-view  <cit> .

in gav, each element in the integration schema should be described in terms of a view  over the data sources. in other words, the mapping makes it explicit how to retrieve data from several elements in the integration schema. this approach is effective when the set of data sources is stable .

it is noteworthy that elements in the integration schema are defined in terms of the data sources, so the addition of a new data source implies the redefinition of some elements in the integration schema. this approach benefits from easier rewriting methods.

in lav, each element in the data source schemas should be described in terms of the integration schema. this kind of approach is effective when the integration schema is stable and well established in the domain/application. in this case, the extension of the system is easy because it only implies adding the description of the new data source in terms of the integration schema. this approach implies a more difficult query reformulation and evaluation, which contrasts with the benefits of greater scalability.

data integration systems
data integration systems  <cit>  deal with problems that could be solved with the infrastructure presented in this paper. thus, we propose to relate the semantics , with the resources' data schemas using mappings, as is done in mediation applications. our proposal uses this information to solve a wide range of problems, in which mediation is a sub-range. consequently, it is feasible to develop new information integration applications, by adding new components able to solve specific tasks.

the wrapper-mediator approach provides an interface to a group of  structured data sources, combining their local schemas into a global one and integrating the information of local sources. therefore, the views of the data that mediators offer are coherent. these mediators perform semantic reconciliation of the common data model representations provided by the wrappers. some good examples of wrapper-mediator systems are tsimmis  <cit>  and manifold  <cit> . several improvements have been made on traditional mediators. one of the most important is the use of standard representation languages, like xml. thus, the mix  <cit>   and mocha  <cit>  projects are xml-based. however, these kinds of systems are usually built as monolithic systems in which reuse is not possible. besides, the metadata used to integrate the data sources is not made explicit, so the relationships between resources and integration schemas are not public. this implies that the provenance of the integrated data is unavailable to the users. our proposal allows this knowledge to be available. the publication of the different components as web services therefore makes it possible to reuse them.

the next level of abstraction on web integration corresponds to ontology-based systems. their main advantage over mediators is their capacity to manage schemas that are unknown a priori. this is achieved by means of a mechanism that allows contents and query capabilities of the data source to be described declaratively. observer  <cit>  uses different ontologies to represent data source information. users explicitly select the ontology to be used for query evaluation. the existence of mappings between ontologies allows the user to change the ontology initially selected. the main disadvantage is that the wrappers are developed for a specific mediator, so they cannot be reused in other mediators. model-based mediation  <cit>  is a paradigm for data integration in which data sources can be integrated, using auxiliary expert knowledge. this knowledge includes information about the domain and is the glue that joins data source schemas together. the expert knowledge is captured in a data structure called knowledge map. in model-based mediation, the mediation architecture is extended, taking data sources from the data level without semantics to the conceptual model level. this architecture introduces semantics into data sources and mediators, but it is not published nor is it accessible to agents or applications. mediators are monolithic systems and they are strongly coupled to wrappers, limiting dynamic integration and interoperability.

in the specific field of biological data there are the following examples: tambis  <cit> , biodataserver  <cit> , kind  <cit> , biozoom  <cit> , biokleisli  <cit> , discoverylink  <cit> , biobroker  <cit>  and biomoby  <cit> .

web services based systems
biomoby  <cit>  is a project with the goal of producing an open-source, simple, extensible platform to enable the discovery, representation, integration, and retrieval of biological data from widely disparate data hosts and analysis services. in this platform, data and data analysis tools  are distributed in web services. resources are registered in a central server called moby central. biomoby objects are lightweight xml coded data used as query input and output values.

in summary, the primary components of this infrastructure are moby services , moby objects  and moby central . this system also offers object and service hierarchies in order to classify information and services, helping users to understand the meaning of data required by services. this proposal also includes how to easily develop web services in order to access biological data. although it introduces the use of web services, it is not exactly an integration architecture, because it is not possible to solve problems directly, but requires access to different databases or services. furthermore, this proposal does not provide a workflow definition and execution system, so several proposals are being developed in order to define and execute workflows based on biomoby services .

semantic moby  <cit>  is an extension created to solve the problems discovered in moby. this proposal defines four roles for software agents: service providers, ontology providers, discovery servers and service consumers. the architecture is based on an ontology that describes the relationships between these elements, and allows users to annotate services with different ontologies. the main idea in this proposal is to build rdf graphs with the service descriptions and to locate services using graph patterns. thus, the main functionality it proposes is to match rdf graphs with user queries represented as graph patterns.

the national institute for bioinformatics  in spain has addressed the development of a web client for locating and executing biomoby services  <cit> . the description of biological input/output objects is coordinated and standardized by means of a data type taxonomy in such a way that services can communicate with each other, wiring natural bioinformatics workflows. automatic interfaces and help system builders have been incorporated into the architecture to make it more cohesive and to facilitate user communication. beyond traditional bioinformatics platforms, data persistence systems, user management and scheduling abilities have produced a new generation of bioinformatics platforms.

workflow management systems
the taverna project  <cit>  has developed a tool for the composition and execution of bioinformatics workflows. this tool includes a graphical interface for the creation and execution of workflows, which are described using a language called the simple conceptual unified flow language , where each step within a workflow represents one atomic task. this platform has been adapted in order to access biomoby services.

remora  <cit>  was designed to create and launch biomoby workflows. the interface was simplified using the standard scheme of the traffic lights colour code . it can only use its own workflow created by its own web interface. biowep  <cit>  allows users to execute predefined workflows. it supports workflow annotation by using a simple ontology for bioinformatics processors  and implements the search and selection of workflows on the basis of their annotation. it also supports retrieval of workflows on the basis of users' profiling. biowep provides a semantic workflow repository. the user can search this repository using a graphical interface defining complex queries to find the desired workflow. nowadays, this is the only portal/wms that uses semantic annotation in the workflow systems.

gpipe  <cit>  offers a set of syntactic and algebraic operators which are able to represent analytical workflows in bioinformatics. iteration, recursion, the use of conditional statements, and management of suspend/resume tasks have traditionally been implemented on an ad hoc and hard-coded basis. gpipe is a prototype graphic pipeline generator for pise that allows the definition of a pipeline, parameterization of its component methods, and storage of metadata in xml formats. gpipe has been implemented and tested on the emboss package. in order to employ other algorithms, it is necessary to describe the command-line user interface by means of xml. thus, this proposal is only applicable to command-line applications, which should be in the local machine in which the workflow is going to be built and executed. it is not possible to use external applications published as web pages or web services.

biowbi and wee  <cit>  have been designed to assist researchers in defining their data sources, drawing graphically and executing analysis workflows. these tools constitute the basic components of a much more general bioinformatics e-workplace, available via a web-browser that provides a collaborative space which is able to support both their analysis activities concerning the data management and the design and execution of their analysis processes. this proposal includes an xml description of algorithms, so their parameters are represented as data types. thus, the only knowledge that a user needs to know in order to connect two applications is the parameter types, and it is not possible to check the consistency of the built workflow. this information would not be enough to build useful workflows, because parameters of the same type  may have different semantics.

RESULTS
generic infrastructure
this section presents the generic infrastructure used as the core for the resolution of data integration problems in systems biology. this infrastructure is based on a resource directory, called semantic directory . we define the semantic directory as “a server to register semantics about available web resources, one or more registered ontologies, mappings between resources and these ontologies, and it provides services to browse all the registered semantics.”

the internal elements of the semantic directory are described by means of metadata. in order to deal with this metadata, the semantic directory is composed of two inter-related ontologies , which describe the internal semantics of the semantic directory . this metadata can be managed by tools that can range from a simple owl parser to a complex ontology reasoner. nowadays, most of the ontologies are published without additional information such as who the owner is. this problem makes it difficult to identify, or to use and reuse published ontologies. for this reason we use omv to register additional information about ontologies to help users locate and use them.

sdmo is the ontology in charge of registering information about resources and relationships between these resources and ontologies registered in the semantic directory. sdmo and omv are related by a class included in sdmo, which provides a way of relating resources  with registered ontologies . the current version of sdmo is composed of five classes:

• omv: this class is used to link resources with registered ontologies .

• resource: this class is used to store information  about resources.

• mapping: this class is used to set the relationships between resources and ontologies. each mapping is related with a similarity instance that establishes the similarity between ontology concepts and resource elements.

• similarity: the similarity class contains three properties  to establish the similarity between an ontology concept and a resource element. the similarity value is a real value between  <dig> and  <dig>  indicating the probability of two concepts being the same. when adding manual mapping, the similarity value is  <dig>  but if we use an automatic matching tool, this value may be less than  <dig>  this similarity is used in the mediator to filter those mappings that are not taken into account in the query rewriting.

• user: this class is added in order to deal with users in the applications.

the semantic directory provides three interfaces representing the main tasks it can perform:  resource metadata repository;  ontology metadata repository; and  semantic register. these components have been described by means of an api. this api has been represented as three java interfaces . this first implementation uses jena to register and search for information. registry methods are available to resource owners, and there are three possibilities:

º to make explicit the relationships with one of the ontologies registered in the semantic directory. these mappings will contain two parts: an expression in terms of the resource structure and an expression in terms of the ontology. the syntax of the mappings depends on the kind of resources registered and the applications developed over the semantic directory. thus, a different kind of application needs a different extension of the semantic directory with a different mapping syntax. the next section presents an ontology-based mediator which uses a semantic directory, so we will describe the syntax of the mappings for this case there.

º to take advantage of a tool for finding mappings between resource structures and one of the ontologies registered in the semantic directory. the first implementation contains a tool  which can find mappings between xml documents and owl ontologies, and between pairs of owl ontologies. however, the semantic directory interface can be implemented using different tools.

º to take advantage of a tool for finding mappings between resource structures and all the ontologies registered in the semantic directory. the response time in this case depends on the number of ontologies registered and the tool used to compare structures.

our goal is to provide applications which will make the semantics of the resources explicit through its commitment with an ontology registered in the semantic directory. the applications that can be developed using the semantic directory components depend on the extension of the infrastructure using new components . thus, semantic aware applications use the semantic directory to find the semantics of the resources registered in order to access the relevant information. these resources have to be registered in the semantic directory, but this will not involve making changes in them.

ontology-based mediator: an application example
as we propose the use of an ontology which is supposed to formalize a shared and consensus knowledge, the ontology used to integrate the data will be stable. for this reason, we have chosen a gav approach. in gav, each element in the data source schemas should be related with the terms of the integration schema.

in order to benefit from the semantics, we have decided to develop an ontology-based mediator, which will take advantage of the generic infrastructure  for dealing with semantics. thus, we focus on a mediation architecture that uses resources registered in a semantic directory. registering of resources in the semantic directory is a key step towards the development of the integration solution, and this task is helped by ontologies. the architecture of the proposed ontology-based mediator  is composed of four main components:

• query solver: this component analyzes the query plan , and performs the corresponding call to the data services involved in the sub-queries  of the query plan . this component will obtain a set of xml documents from different data services.

• integrator: results from data services  are composed by this component, in this way, obtaining the results of the user query. the current implementation of this component uses the mappings to translate the xml document to ontology instances, and then a conjunctive query evaluator is applied to the set of instances found. future versions can include a reasoner, but this will imply taking care of the formal consequences of retrieving certain instances.

in our proposal , the sources are made available by publishing them as web services . our primary goal here is to integrate databases accessible via internet pages. in this context, wrappers are an important part of the internal elements of data services. a wrapper is an interface to a data source that translates data into the common data model used by the mediator. in our case, we have chosen xml as the common data model. the development of data services that require the development of a wrapper has been studied in previous work  <cit> . however, biological data sources are usually public and downloadable. in these cases we have designed some patterns to retrieve a data source stored as a flat file to store it in an xml database. in summary, data services, independently of the development process, are distributed software applications that receive queries in xquery and return xml documents.

in the context of mediator development, the process of registering resources in a semantic directory implies finding a set of mappings between one or several ontologies and the data service schema . these mappings will be the key elements to integrate all the data sources, and these mapping will be the way in which the resource semantics are made explicit. the mappings used are defined as a pair . p is a set of path expressions on the resource schema, and q a query expression in terms of the ontology. in a first approach we have chosen xpath as the language to express p, and conjunctive queries to q. for example to set the mappings between the swiss-prot data service and the ontology we have established the following mappings :

√ /result/polypeptides/polypeptides_name

→ polypeptides

√ /result/polypeptides/organism_name

→ organism

√ /result/polypeptides/amino_acid_sequence

→ amino_acids_sequence

√ /result/polypeptides/polypeptides_name and /result/polypeptides/id

→ polypeptides and swissprot_id

√ /result/polypeptides/polypeptides_name and /result/polypeptides/molecular_weight

→ polypeptides and molecular_weight

√ /result/polypeptides/polypeptides_name and /result/polypeptides/pdb

→ polypeptides and pdb_id

√ /result/polypeptides/polypeptides_name and /result/polypeptides/synonym_polypeptides_name

→ polypeptides and name_of_entities

√ /result/polypeptides/organism_name and /result/polypeptides/organism_name

→ organism and organism_name

√ /result/polypeptides/organism_name and /result/polypeptides/taxon

→ organism and taxon_id

√ /result/polypeptides/amino_acid_sequence and /result/polypeptides/ amino_acid_sequence

→ amino_acids_sequence and sequence

√ /result/polypeptides/amino_acid_sequence and /result/polypeptides/ length_sequence

→ amino_acids_sequence and length

√ /result/polypeptides/polypeptides_name and /result/polypeptides/ organism_name

→ belong_to

√ /result/polypeptides/polypeptides_name and /result/polypeptides/ amino_acid_sequence

→ amino_acids_sequence

the amine system project: integration of data on biological amine-related information
in this pilot project  we have developed and tested the ontology-based mediator described in the previous section. the long term goal of this project is to register the most relevant public databases at different levels of study: metabolite properties and concentrations, macromolecular structures, assigned functions, docking among biomolecules and information on biochemical pathways. this proposal is flexible as has been shown in the previous section, but in order to validate its viability for solving specific and real issues, we have tackled the resolution of a well known bioinformatics problem by integrating a limited but increasingly growing number of databases. the initial problem to be solved is summarized as following, and the use case developed to solve it is named ammo-prot:

problem: a common and useful strategy to determine the 3d structure of a protein, which cannot be obtained by its crystallization, is to apply comparative modelling techniques. these techniques start working with the primary sequence of the target protein to finally predict its 3d structure by comparing the target polypeptide to those of solved homologous proteins  <cit> .

in order to solve this problem, protein structure data and some tools to compare them are required from databases. these databases have been used to validate our integration tool. first of all, we need to define the domain ontology in order to relate it to the resource semantics. there are several ontologies related to this domain, the most representative being go () and the molecular biology ontology tambis (). however, these ontologies are very large and describe very light weight semantics . thus, we have developed our own ontology  which is based on the gene ontology concepts but with enriched relationships which improve its semantics . the definition of the relationships between concepts will allow us to retrieve interrelated information . in addition, future versions of the mediator will benefit from the improved semantics to infer new knowledge.

being based on the go ontology guarantees interoperability with other applications also based on this ontology. in figure  <dig>  we have shown only the relevant concepts for our domain helping users to understand the domain semantics. the ammo is used as the pivot that will integrate the whole domain, which includes concepts/relationships necessary for the use case described above and other ongoing projects. this ontology has been described with owl.

at present, the ammo ontology has been used to register the semantics of different consolidated resources/tools in bioinformatics: swiss-prot  <cit> , pdb  <cit> , modeller  <cit>  and jmol (). they allow us to retrieve information about protein structures which is used at an initial analysis stage. this information will subsequently be the basis of future developments to extend the possibilities of our system, and to allow users to retrieve information on many other queries on metabolic, signalling and molecular interactions and relationships i.e., to further progress in automatic data integration resources on a given biochemical problem. in order to achieve this goal new databases  are being wrapped.

in its present stage, the developed web tool has as its goal to show the viability of the proposal and its application in the real case mentioned above , so we have divided the problem into a set of simple steps. initially, the species must be selected by filling its  names in the “organism” field . on the other hand, the web interface shows a pathway that is used as the entry point  to retrieve structural information on the target by clicking on the protein picture. the process for information searching and integration is illustrated by the two examples of human polypeptides explained below.

in the first example, we click on the enzyme “ssat ”. thus, the first query to be solved is:

p ← polypeptide and name and organism and name and belongs_to.

the query built inside the web application is decomposed into two sub-queries to be sent to the swiss-prot and pdb databases, which are the databases related to the polypeptide concept. first the sub-query will return data as an instance of the polypeptide concept. thus, the query results comprise information available in the swiss-prot database on two isoforms for the human ssat protein. then, the graphical web interface will allow the users to manually select from information  for one or another isoform. in this specific case, both structures have been previously determined by experimental methods, so that the application will make use of the second sub-query to retrieve information from the pdb database, which could be downloaded or displayed using the jmol tool (). when launching a new query about an unsolved structure , as is the case for “tgase ,” an initial search for information about this enzyme would start in the swissprot database as in the previous example. results will show all the entries related with our query, which correspond with different tissue-specific tgase types. in this task, we are going to focus on tgase z, which is widely expressed in humans. as there is no entry in pdb for this protein, the next step involves executing the modeller tool  <cit>  to predict a 3d structure of the polypeptide. modeller is an automated homology modelling tool, which performs the necessary steps to carry out: searching for homologous proteins, target-template sequence alignments, model building on the bases of the template coordinates and basic geometry optimization. in our example  the user query for this process is:

q: polypeptide  and amino_acid_sequence and sequence  and amino_acid_sequence  and polypeptide  and homologue_sequence  and homologue_sequence_aas and homologue_sequence_polypeptide.

then, using the primary sequence retrieved from swissprot, a search on pdb is performed to obtain a set of homologous proteins of our target, corresponding to structures that have been previously determined experimentally by using mainly x-ray diffraction or nmr techniques. we obtain a set of template candidates that are filtered by the sequence identity shared with the target sequence. in a homology modelling process the quality of the final model is highly dependent on the identity between target and template sequences, so those templates sharing an identity below 30% are going to be discarded by the application  <cit> . in this first version, the template showing the highest identity with the target protein will be selected for the next steps. once the most suitable template is chosen, further steps of alignment and modelling are performed by the automated tool. template and target sequences are aligned before the modelling process, where spatial coordinates of the template are extrapolated to build the target structure.

finally, the results of this process are five different models of the same target protein, which can be selected in the web interface. subsequently, the model can be either downloaded or viewed using jmol, in order to analyze whether the solution is realistic or not . this visualization tool is included as part of the application, providing added-value features.

discussion and 
CONCLUSIONS
this paper presents an ontology-based mediator that uses an infrastructure for building applications in the semantic web. it is being validated in a specific biological context, that of amine-related biochemistry, and consequently it has been named the amine system project (). of course, this semantic mediator can be adapted very easily to obtain structural information for many other biochemical problems as can be deduced form the ammo ontology organization . the proposed ontology is an extension of the gene ontology, so the developed system can interoperate with other systems using this ontology . if the ammo ontology is modified to add new concepts, existing mappings will still be valid, ensuring system scalability. however, if a concept is changed, related mappings should be redefined. in addition, the developed proposal is a generic infrastructure and mediator, so the ontology used can be changed to be used in other domains . in the development of the mediator and in previous works  <cit>  <cit>  <cit> , we have discovered some limitations. the main one is the maintenance of data services, because the services developed use public databases that are not under our control. thus, the long term success of this proposal and similar ones relies on the collaboration of data and tool owners. for this reason, the data services integrated into this proposal have been developed from stable data sources, providing their data as files, data services or databases . in addition, the developed data services include the use of internal cache systems to prevent source unavailability.

the proposed solution is based on the registration of the resources' semantics by relating them with ontologies. however, the location of relevant ontologies in a specific domain is an open problem which is being dealt with by relating ontologies  and organizing them. in this way, we are studying how to extend our proposal to include mechanisms for organizing ontologies in order to facilitate their location.

protein structures contain fundamental information regarding their function, location and interactions, which is most of the information in their biological missions. our use case  returns the correct information for the protein structures included initially in the pilot project. the pilot could be easily adapted to any other metabolic pathway. in addition, queries can be defined in a user friendly way for biochemists, that is, by clicking on the protein symbols organized as a metabolic pathway scheme after definition of the required species.

combining information integration with prediction techniques results in efficient information retrieval and expands the spectrum of applicability of structural bioinformatics techniques to non-experienced users. therefore, the problem presented in this paper is important in this context, as automatic performance of the process will reduce the effort required to solve questions on protein structures. all of these characteristics will expand the spectrum of applicability of structural bioinformatics techniques to non-experienced users. genomic projects have exponentially increased the number of known polypeptide sequences. thus, any effort to improve efficiency for the extraction of structural information at its highest level should help advance many on-going systems biology projects. our project fulfils all of these aims and objectives.

list of abbreviations used
asp: amine system project

ammo: the amine metabolism ontology

kegg: kyoto encyclopedia of genes and genomes

lav: local as view

gav: global as view

tsimmis: the stanfordibm manager of multiple information sources

xml: extensible markup language

mocha: middleware based on a code shipping architecture

tambis: transparent access to multiple bioinformatics information sources

rdf: resource description framework

inb: national institute for bioinformatics

biowep: workflow enactment portal for bioinformatics

wms: workflow management system

pise: pasteur institute software environment

emboss: european molecular biology open software suite

biowbi: bioinformatic workflow builder interface

wee: workflow execution engine

omv: ontology metadata vocabulary

sdmo: semantic directory metadata ontology

owl: web ontology language

uri: uniform resource identifier

api: application programming interface

maf: matching framework

xquery: xml query language

xpath: xml path language

go: gene ontology

pdb: protein data bank

competing interests
the authors declare that they have no competing interests.

authors' contributions
ind designed the infrastructure of semantic directories, carried out the implementation of the system and drafted the manuscript. rm designed the biological problem to be solved. rm and apa carried out the study of the databases and analyzed result provided by ammo-prot. amg developed the study of mechanisms to predict structures. jlu and fsj participated in the design of the study, coordinated the test phases and performed analysis of the results. fsj and helped to draft the manuscript. jfam conceived the infrastructure, participated in its design and coordination and helped to draft the manuscript. all authors read and approved the final manuscript.

