BACKGROUND
choosing rationally between alternative models is one of the most complex and critical problems in systems biology  <cit> . given two or more models, and one or more data sets, model selection should identify the model topology and set of kinetic parameters that explains the data best - while simultaneously penalising overly-complex models. a combination of experimental and theoretical arguments can be developed to inform the choice  <cit> . calculating the bayesian evidence  is a quantitative approach to answering this question  <cit> .

here, we report on the application of nested sampling - a statistical approach to computing the bayesian evidence- to the inference of model parameters, and the estimation of log z, in a model of circadian rhythms  <cit> . an extensive analysis of nested sampling for integration and inference for multivariate gaussian distributions identifies suitable configurations of nested sampling and associated algorithms in an analytically tractable case. the algorithms employed are generic, simple to configure, or are self-tuning; hence the computational methods can be easily applied in other contexts.

systems biology models are primarily of interest because they explain data and are capable of making testable predictions  <cit> . parameter estimation is a necessary task when a model has been proposed and the parameters that provide the best fit to experimental data must be identified. modelling and parameter estimation are often interleaved: it may emerge that a model does not have the capability to explain certain features of the data, and the model may be refined as a result. new data may be generated in the lab, and the process of modelling and parameterisation iterates once more. pokhilko et al. describe their revision of the circadian clock in exactly this way  <cit> . parameter estimation is central to this process.

ideally, the representation of the individual reactions that make up a systems model would be based on the known chemistry of the enzymes and substrates involved. however, in practice, there may be uncertainty about the chemical process and its participants. some species may be assumed to be essentially invariant and excluded from the model, or quantitative values for binding constants may not be known. often the system is modelled at a level of granularity where multiple chemical steps are represented as a single reaction in order to deal with the complexity of the cell. for example, transcription and degradation are complex processes but are typically each modelled as a single step. as a result, the reactions in a systems model are open to scrutiny, and a justification for modelling decisions is necessary  <cit> .

while justifications in terms of the literature are perfectly valid, and point estimates of the goodness-of-fit to experimental data for some specific combination of parameter values can provide insights and permit model comparison  <cit> , we propose a quantitative measure of evidence derived from the fit of the model to the data. this measure, log z, is the result of an integration , over the potential parameter values rather than the fit of some exemplar combination of parameters. evidence calculations can be expected to throw new light on the structure of models.

nested sampling has been successfully applied in astronomy for model selection and parameter inference  <cit> , where cosmological models of up to  <dig> parameters have been analysed, and techniques for partitioning multi-modal likelihood functions have been developed  <cit> . the properties of nested sampling have been determined for simple models , and the algorithm has been compared to alternative techniques including  importance sampling  <cit> . recently, the convergence of nested sampling, its statistical uncertainty and the impact of dimensionality have been addressed theoretically  <cit> . nested sampling is a valid monte carlo technique with convergence rate o and computational cost o   <cit> .

biological systems models are often simulated or analysed by complex computational procedures. consequently, the cost of evaluating the likelihood function is often high, and nested sampling must be configured to progress through the prior volume as rapidly as possible without introducing unacceptable errors. we specifically address the problems of sampling the prior , defining and exploring the prior volume, and experimentally quantify the uncertainty in the inferred parameter means and standard deviations for a circadian model that is representative of the single feedback loop topology.

in systems biology, an approximate bayesian computing approach to selecting between alternative models of the jak-stat pathway demonstrated strong evidence in favour of one model  <cit> . model selection using the bayes factor /p) has been shown to be capable of placing an ordering on alternative signal transduction models that is decisive  <cit> . bayes factors have been computed for systems models by annealing-melting integration  <cit>  and by population mcmc methods  <cit> , techniques which have been made available in the biobayes package  <cit> . nested sampling has been used for dna motif selection  <cit> , and an application to model selection in systems biology has recently been reported  <cit>  where the multinest package developed for cosmology  <cit>  was used to compare signal transduction models of  <dig>   <dig> and  <dig> parameters. we are able to configure nested sampling to run with  <dig> active points  thus considerably reducing the number of posterior samples required: for example, on the circadian models we consider here, log z can be estimated to an accuracy of ± <dig>  with  <dig> active points , while  <dig> active points reduce the variability in log z to ± <dig>  but require  <dig> times the computational effort . we also present novel algorithms for the crucial exploratory sub-step of nested sampling, and for the calculation of the transitional likelihood of the systems model.

approach
the evidence z  p is a quantitative measure based on the overall correspondence between the data  and the model , obtained by integrating the product of the likelihood function and the prior over the space of parameter values  <cit> .  specifies the probability model of the data given the parameters, p.) in the inference of the posterior distributions of parameters , the evidence plays the role of a normalising constant and need not be evaluated  <cit> . however, the evidence plays a central role in model comparison and must be computed. the prior assigned to parameters enters into the evidence calculation and can influence the outcome of model comparison through the evidence   <cit> . where possible, priors should be selected on the basis of physical considerations  <cit> : uniform and jeffreys’ priors have been adopted in cosmology  <cit> ; uniform and gamma priors have been employed in biological systems modelling  <cit> . 

  posterior=prior∗likelihoodevidence 

  p=ppp 

in high dimensions, log z can be computed effectively by a nested sampling strategy that exploits statistical properties of the shrinkage of the prior volume. a set of posterior samples is produced as a by-product of nested sampling, and the first and second moments of parameters can be calculated from these samples. such an analysis may tell us that certain parameters are very narrowly constrained, while others have a broader distribution, thereby identifying the kinetic parameters for which accurate experimental measurements can validate  the model. this computation can also further inform experimental design by quantifying how the parameter estimates depend on the data and experimental protocols used, as well as the extent to which parameters can be constrained by observing particular sets of species. parameter inference is a challenging problem, and the subject of much on-going research  <cit> .

after determining the number of samples to use in nested sampling, and defining the priors for parameters, we use nested sampling to investigate the posterior distributions of a nine parameter model of circadian rhythms  <cit> . this analysis is repeated for  <dig> to  <dig> cycles of simulated experimental data to explore the impact of time series length on these parameter statistics.

we then explore the problem of model selection for variations of the circadian model with alternative hill coefficients , and once more explore the impact of additional data on model selection.

nested sampling - an overview
the posterior distribution p of the parameters θ, and the evidence p, that is, the support for the data d under hypothesis hi, are the two central results of bayesian inference  <cit> . two models h <dig> and h <dig> can be compared through the ratio of their posterior probabilities , a calculation that can be decomposed into the bayesian evidence  and the prior probability of the respective hypotheses which may favour one model over another.

occam’s razor is implemented as the evidence is proportional to the volume occupied by the posterior relative to the volume occupied by the prior, and hence additional parameters expand the number of dimensions that the evidence must be computed over  <cit> . the evidence  is a scalar quantity that can be viewed as an integral over the elements of mass  associated with the prior density π. 

  pp=pppp=z1pz0p 

  z=∫lπdθ=∫ldxdx=πdθ 

the prior mass can be accumulated from its elements  in any order. following  <cit> , the cumulant mass of likelihood >λ can be defined , and this allows the evidence to be written as a one-dimensional integral of the  likelihood l over the unit range  . 

  x=∫l>λπdθ 

  z=∫01ldxl)≡λ 

given a sequence of decreasing values 0<xm<…x2<x1< <dig> where the likelihood li=l can be evaluated, the evidence can be approximated numerically as a weighted sum . 

  z=∑i=1mliwiwi=Δxi 

to obtain the sequence x1…xm, the nested sampling algorithm maintains a set s of n active points, each containing a vector of parameter values. on each iteration, the worst point, x, is identified :i∈s; l* = l) and replaced by a new point, z, drawn uniformly from the prior and subject to the constraint l >l*.  the new point can be found by randomly selecting one of the existing active points  as a starting point. this procedure shrinks the prior mass geometrically  according to the beta distribution: p=beta=ntin− <dig> for n active points  <cit> . the uncertainty in the shrinkage ratio gives rise to an error in z which scales approximately as n−1/ <dig> <cit>  . the shrinkage of the prior for a sequence of points from a two-dimensional parameter space is illustrated in figure  <dig> 

inferences about the posterior can be obtained from the sequence of m discarded points, p. each point is assigned the weight pi=lwi/z, and the first and second moments of the jth parameter in the vector θ can be estimated by  and  respectively. 

  〈θj〉=∑i=1mpiθij 

  varθj=∑i=1mpi2−〈θj〉 <dig> 

obtaining new samples θz from the truncated prior  >l*) is a major challenge  <cit> . random walk mcmc  <cit>  and rejection sampling within an n-dimensional ellipsoid  <cit>  can be used, and these techniques can be coupled to sample multi-modal likelihoods  <cit> . we explore the use of the stepping out procedure of slice sampling  <cit>  as a simple method of obtaining new points  <cit> . in the methods section, we evaluate its use as an exploration method within nested sampling, showing that a single slice sampling step applied in each dimension is sufficient to obtain correct results in up to  <dig> dimensions using n= <dig> active points. as the prior volume shrinks more rapidly for lower values of n, and the number of likelihood evaluations reduces accordingly, it is important to establish this feature of nested sampling’s configuration for practical applications. adaptively tuning the step size used in stepping out increases computational efficiency significantly and can be done automatically.

a transitional likelihood function for sparse data
in order for the evidence integral to be computed, a likelihood function l must be defined. for simple stochastic models, such functions may be available in analytic form through solution of the chemical master equation. more generally, systems models will require simulation to obtain a trace of behaviour which can then be compared with the data, or we can approximate the likelihood of the model connecting one observed data point with the next, as we now discuss.

it is shown by  <cit>  that the change Δy in a birth-death process y is normally-distributed for short time periods Δt over which the rates do not change . denoting the birth and death rates by β and δ, respectively, Δy is given by . 

  Δy=y−y∼nμ=−δ)Δtσ=+δ)Δt 

this result motivates the use of stochastic differential equations to model the system dynamics: β and δ are derived from the propensities of y in a straightforward manner.

turning to parameter inference, given a discretely-sampled time series, the likelihood of observing y <dig> …,yn is the product n for i= <dig> …,n− <dig> where μ and σ are derived from the birth and death rates as in . the likelihood of the model is the product of the likelihood of observing each species. this transitional likelihood function requires the data to be sampled at short time intervals. when the data is sparsely sampled, as is often the case, additional data points can be imputed to bridge the gap between observations. adopting a markov chain monte carlo approach, heron et al. alternate between sampling from the parameter space and sampling from the imputed data space  <cit> . this strategy is not readily applicable here as the bridge points would need to be included alongside the parameters, thus considerably increasing the dimensionality of the problem, with no obvious way to specialise the treatment of the imputed data.

noting that the expected change Δy is −δ)Δt and that repeated applications of this estimate yield a good predictor of y over time spans many times greater than Δt, we generate the most likely time evolution of the model from the known data vector at ti, impose the condition that the bridge must end at the known data vector at ti+ <dig>  and approximate the likelihood of y−y as the product of the probabilities of the Δys between the bridge points. this procedure is presented in detail in methods. the likelihood of each Δy is computed from the cumulative density of the normal distribution Φ . 

  l=Φ−Φ 

RESULTS
after a brief introduction to circadian models, we present the results of parameter inference and model selection obtained using the transitional likelihood function with nested sampling.

a simple model for circadian rhythms
circadian clocks are gene networks found widely amongst organisms, controlling biological processes ranging from cyanobacterial cell division to human sleep-wake cycles  <cit> . these networks function by generating endogenous ∼ <dig> hour oscillations in gene expression that can synchronise to the external light-dark cycle. this process, known as entrainment, enables organisms to optimally time biochemical processes relative to dawn and dusk, providing an adaptive advantage  <cit> . the clocks of different organisms appear to have a similar structure based on interlocked sets of negative gene-protein feedback loops augmented by additional positive loops  <cit> . computational models of these feedback structures based on ordinary differential equations  have become useful tools for quantifying the biochemical mechanisms underlying circadian dynamics  <cit> . figure  <dig> shows a minimal ode model of the clock in the fungus n. crassa. this is based on a single negative feedback loop in which the gene frequency  is repressed by its protein product. frq transcription is upregulated by light, providing a mechanism for light entrainment  <cit> . the model comprises  <dig> differential equations describing the dynamics of frq mrna and the cytoplasmic and nuclear forms of frq protein: 

  m˙=vs+θkinkin+pnn−vmmkm+m 

  p˙c=ksm−vdpckd+pc−k1pc+k2pn 

  p˙n=k1pc−k2pn 

as is common for models of this type, hill and michaelis-menten kinetics are assumed for transcription and degradation respectively, while translation and nuclear transport are modelled as first order reactions. collectively, the reactions are parameterised by  <dig> kinetic constants: vs, the maximum frq transcription rate; ki, the michaelis constant for frq repression; vm, the maximum frq degradation rate; km, the michaelis constant for frq degradation; ks, the frq translation rate; vd, the maximum frq degradation rate; kd, the michaelis constant for frq degradation; k <dig>  the rate at which cytoplasmic frq enters the nucleus; k <dig>  the rate at which nuclear frq enters the cytoplasm; and n, the hill coefficient. n quantifies the binding cooperativity of frq repression; i.e. the number of sites on the frq promoter that can be bound by frq molecules to prevent transcription. consequently, it is assumed to be a positive integer  <cit> .

the models and parameter values used in this study are listed in the additional file  <dig> . in equation , the forcing term θ models the effect of light. setting θ= <dig> simulates constant darkness , yielding free-running oscillations with a period of around  <dig> hrs  <cit> . entrainment to light-dark  cycles is modelled by switching θ between  <dig> and a maximum value θm at lights-on , and then switching θ back to  <dig> at lights-off : 

  θt=θmiftdawn≤modt,24≤tdusk,0otherwise. 

birth and death rates for all model species are obtained from the reaction propensities, and are used in the likelihood calculation as described above . for example, cytoplasmic protein pc is produced or consumed in the following four reactions : 

   the birth and death rates  for pc follow directly. 

  β=ksm+k2pn 

  δ=vd//Ω)))+k1pc 

the system size coefficient  is introduced to account for the averaging of stochastic fluctuations that occurs in population-derived data. the likelihood function must be modified as a result. 

  l=Φ−ΦΦ−Φ 

when −δ) approximates the observed Δy, the likelihood in  can be maximised by minimising +δ), as reducing the standard deviation increases the density. ordinarily this is necessary  <cit> ; however, correlations between samples generated from realisations of the circadian model result in the minimisation of +δ) dominating the fit to the observed Δy to such an extent that the inferred parameter means do not approximate the true values. the normalisation term on the denominator of  alters the trade-off between fitting −δ) to the observed Δy and minimising +δ), reducing the effect of this bias. ε was set to  <dig> , and computational explorations showed that the precise value used was not critical to the results obtained.

application of nested sampling to the circadian model
synthetic data ranging from  <dig> to  <dig>  <dig> hr circadian cycles was generated using the variant of gillespie’s stochastic simulation algorithm introduced in  <cit> . five time series were generated for each time span in both dd and ld conditions. ld cycles with different daylengths were simulated using equation  by setting θm equal to  <dig>  and varying the size of . all time series were sparsely sampled at 2hr intervals to correspond to typical experimental protocols. nine of the eleven model parameters were integrated by nested sampling. the hill coefficient  and system size  were kept constant.

for all rate parameters and constants, a uniform probability density function between positive  limits was used as the prior since rates cannot take negative values , and zero values would eliminate the reaction from the model. the circadian model does not contain scale parameters  for which the jeffreys’ prior might be appropriate  <cit> . the same priors were used for the inference of parameters of the dd model from all realisations of the model, and, similarly, a fixed prior was used for the ld model and in the analysis of alternative values for n. as each of the time series generated for each condition is an independent stochastic realisation of the model, the inferred parameter distributions can be expected to include the generating parameters, but this cannot be guaranteed.

the mean, standard deviation  and coefficient of variation  for parameters ki and km are shown in figure  <dig> for the free-running  system .

estimates of the mean are broad when inferred from a single cycle of data. these estimates are more precise for  <dig> or more cycles, with the inferred standard deviation decreasing towards a constant value for  <dig> or more cycles of data. for ki, the value used to generate the data  is close to the inferred value for  <dig> or more cycles of data, whereas for km the inferred values are generally higher than the value used in the generating model . to validate the results, parameter inference was performed on one of the data sets using a standard implementation of mcmc  <cit>  and using nested sampling . the mcmc simulations confirm that all parameters have a unimodal distribution within the specified priors , from which it follows that the mean and standard deviations computed by  and  are meaningful summary statistics. 

scaling the standard deviation by the mean shows a 10-fold difference between ki and kd, indicating that the transcription threshold ki is significantly more tightly constrained  than the protein degradation threshold kd. the cvs for the nine parameters integrated span a wide range as shown in figure 4a for  <dig> and  <dig> circadian cycles, where the parameters are ordered from most to least constrained. the cvs for all parameters and all cycle lengths are plotted as a heat map in figure 4b. the highly-constrained parameters include k <dig> and k <dig> – the rates for protein transport to and from the nucleus.

the reduction and convergence of the parameter standard deviations with increasing circadian cycles is not due to the increase in the number of data points . the total entropy in the data, which is defined as the sum of plog for all bridge points ), increases with the increasing number of data points that result from sampling at uniform intervals over a successively larger number of circadian cycles. keeping the data set size constant approximately equalises the entropy of the data given the generating model as illustrated in additional file 1: figure s <dig>  it can be seen that repeating the parameter inference using a fixed number of  <dig> samples over 1– <dig> cycles by varying the sampling interval gives essentially the same results as obtained with varying numbers of data points . we conclude that the entropy in the data does not determine the constraints on parameter values for this circadian model.

these results are remarkably consistent given the two sources of variability. firstly, each of the  <dig> data series analysed is an independent realisation of the model. secondly, nested sampling is itself a stochastic procedure. despite this variation, the analysis is able to quantify the extent to which parameters are constrained, and, further, demonstrates that these constraints vary from parameter to parameter.

finally, the calculation of log z is proposed as a model selection criterion. setting the hill coefficient  to integer values from 2– <dig> can be considered to specify different models. as previously, dd data was generated for 1– <dig> circadian cycles using n= <dig>  and nested sampling was applied to these four variations of the circadian model. figure 4d shows that the hill coefficient used to generate the data is correctly recovered for  <dig>   <dig> and  <dig> cycles of data. when one or two cycles of data are analysed, the value of log z for the best-fitting model is within one standard deviation of the second best model . for  <dig> or more cycles, the best model is at least  <dig>  log units greater than the second best .

taken together, our results imply that measuring gene expression levels over multiple circadian periods is a more efficient strategy for facilitating robust parameter inference than recording data at high temporal resolutions. for the model considered here, good estimates of parameter means require  <dig> or more cycles of data, whereas accurate estimates of standard deviations require four or five cycles of data. reliably determining the hill coefficient requires at least  <dig> cycles of data.

the choice of which kinetic constants to measure independently depends on our aims and the specific experimental system of interest. our results on ld cycles show that this choice may also be partially determined by the experimental protocols used. here, a true in vivo value for the highly-constrained parameter ks that was substantially different from the inferred value would invalidate the model. however, should we wish to improve the inferred parameter estimates, we should measure the degradation thresholds kd and km as these are more loosely constrained.

CONCLUSIONS
nested sampling generates a sequence of posterior samples from the parameter space as a by-product of computing the evidence integral. weighting the parameter values in these samples  by the probability of the sample gives estimates of the mean and standard deviation for all model parameters.

the nested sampling algorithm has only one variable, the number of active points n. analysis of gaussian likelihood functions indicates that  <dig> active points is sufficient to compute the evidence integral in up to  <dig> dimensions. the inferred means and standard deviations of parameters are relatively insensitive to the width of the uniform prior we adopt. in contrast, the value of the evidence has some sensitivity to the prior as a narrow prior may omit significant regions of the likelihood integral, whereas the exploration procedure for finding new points may fail to locate all such regions if the prior is too broad. the use of slice sampling permits an automated self-tuning of the exploration procedure that reduces computation significantly. in contrast with gibbs sampling where  <dig> or more steps are required  <cit> , we show that a single slice step is sufficient as an exploration procedure.

we have demonstrated in an exemplar circadian model that the estimates of posterior densities  are influenced predominately by the length of the time series, becoming more narrowly constrained as the number of circadian cycles considered increases. we have also shown the utility of the coefficient of variation for discriminating between highly-constrained and less-well constrained parameters.

in contrast with mcmc approaches, nested sampling has no burn-in period, and does not require complex annealing schedules or output analysis  <cit> . nested sampling is therefore well-suited for integration into model analysis software as a robust technique for calculating parameter moments.

