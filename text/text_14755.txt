BACKGROUND
the advent of cost-effective high-throughput functional genomics methods has spurred on the generation of transcriptional datasets in many diverse areas of biology. a common goal in the analysis of such data is to discover the regulatory pathways behind biomedical phenomena and as our understanding of these regulatory mechanisms increases, commercially and publicly available databases of regulatory interactions grow steadily. ideally, a regulatory interaction implies a direction of causality, i.e. the perturbation of an upstream regulator causally leads to a downstream consequence. we consider two types of interactions in this work:  signed interactions that specify whether an increase in the upstream regulator causally leads to an increase or a decrease in the downstream entity, and  unsigned interactions that merely state that an upstream entity causally regulates a downstream entity, but do not specify the direction of effect. throughout this paper, the word upstream is used to refer to regulators one step previous to a gene in a biological pathway. commercial products, such as qiagen’s ipa application , are based on manually curated networks with a large number of signed causal relationships extracted from nearly  <dig> million findings  <cit> . at the time of writing, qiagen’s webpage  lists more than  <dig>  citations of biomedical articles making use of their commercial product on top of such a network. unfortunately, such highly curated networks are not freely available to the academic community for further algorithmic development and generation of biomedical insights.

several statistical approaches have been suggested to infer active upstream regulators from gene expression data based on a large set of signed causal interactions. the company selventa inc. pioneered the general approach  <cit> . chindelevitch et al.  <cit>  derived the exact null distribution for a plausible scoring scheme to rank putative upstream regulators. kramer et al.  <cit>  provide an approximation to this approach based on a normal distribution, which forms the basis for the popular ipa pathway analysis tool. zarringhalam et al.  <cit>  consider bayesian approaches that also incorporate biological context into the inference procedure. based on these algorithms, networks of biological interactions derived from commercial vendors such as ingenuity  and selventa  have been used to study processes as diverse as in vitro differentiation  <cit> , modeling of cellular proliferation  <cit> , and drug-induced liver injury  <cit> .

in this work, we propose a new extended method to detect upstream regulators geared towards mixed networks paired with an efficient statistical inference approach. importantly, we will demonstrate that the publicly available string database  <cit>  <dig> has matured to a point to reproduce key findings from several previously published studies. the string <dig> human database contains ∼ <dig>  molecular interactions of which ∼20% are either undirected  or directed and not signed. in our method, we make use of both types of interactions.

the rest of the paper is structured as follows: we first present the intuition behind our new approach to analyze mixed networks and contrast it with previously proposed methods. in the following sections, we outline the ideas for efficient statistical inference and give a mathematical derivation to compute p-values based on the proposed statistics. importantly, we demonstrate superior execution times even for the previously proposed approaches and show that the new statistic is preferable based on simulations. finally, we demonstrate the biological plausibility of results based on publicly available networks in the context of controlled in-vitro overexpression experiments, stem-cell differentiation data and animal models of neuropathic pain. we close by summarizing our work and providing avenues for future extensions.

methods
approach
our method will infer likely upstream regulators given  a set of up- and down-regulated transcripts from a specific biological experiment and  a mixed network of regulatory interactions potentially relevant in the current biological context. we define the network as a directed graph where the nodes are biological entities and the edges represent interactions between the entities. a causal signed edge in the network consists of a source node  regulating the target node . signs + or − indicate up- or down-regulation respectively. an unsigned causal relation is an edge where the direction of regulation is either unknown or ambiguous. ambiguity can arise when one source of information  describes an increasing regulatory relationship between two entities and another source postulates a decreasing one. this might be due to different biological contexts or simply erroneous findings in one of the articles. figure  <dig> shows a schematic representation of a potential upstream regulator in a causal network with corresponding experimental data. note that we assume a positive direction of regulation for the putative upstream regulator. all predictions flip if we have a negative direction of regulation.
fig.  <dig> schematic graph of an upstream causal regulator connected to a set of downstream transcripts. for each transcript, the predicted and observed direction of regulation are displayed. the bottom panel annotates positive and negative evidence for the activity of regulator x as defined for different scoring schemes discussed in the main text: a enrichment score, b quaternary score and c correctness score



in this work, we extend the ideas of  <cit>  to include unsigned edges and derive the null distributions of the relevant test statistic for exact statistical inference. in addition, we provide a novel computational algorithm that runs significantly faster and has benefits even when using purely signed networks.

given an upstream regulator x and the corresponding observed direction of regulation of the down stream nodes, we can construct a contingency table by tabulating all potential combinations of prediction and observation .
n
++
n
+−
n
+0
q
+
n
−+
n
−−
n
−0
q
−
predicted r
n
r+
n
r−
n
r0
q
r
n
0+
n
0−
n
00
q
0
n
+
n
−
n
0
n
4× <dig> contingency table



in table  <dig>  q+, q−, qr and q <dig> denote the total number of +, −, unsigned  and  <dig> predictions respectively. similarly, n+, n−, and n <dig> denote the total number of observed +, − and  <dig> perturbations according to the gene expression data. the entries of the table represent the agreement between the predictions made by the regulator and the actual observed values and correspond to several exemplary cases in fig.  <dig>  for instance, x <dig>  x <dig> and x <dig> are all correct predictions in which the predicted and observed directions of regulation match exactly. these correspond to cells n−− and n++ respectively. in contrast, x <dig> and x <dig> represent incorrect predictions and correspond to cells n+− and n−+. x <dig> and x <dig> are cases in which the direction of regulation is unknown or ambiguous and the experimental data indeed show differential expression . x <dig> and x <dig> are cases in which we would expect differentially expressed genes but don’t find any in our experiment. x <dig> corresponds to cell n− <dig> and x <dig> to cell nr <dig>  differentially expressed transcripts that are not predicted to be regulated by the upstream regulator under consideration are captured in cells n0+, n0−, and n <dig> .

note that the total number of predicted and observed altered genes are determined a priori according to the gene expression data and the causal graph. this implies that the margins of the table for each upstream regulator are fixed and the table is completely determined by the upper left 3× <dig> corner. the probability of the table t under the null model that the predictions made by the regulator  are distributed at random given the constraints on the margins of the table, can be computed by a generalization of the hypergeometric probability mass function. essentially the probability is obtained by the ratio of the total number of permutations  of the gene expression values that do not change the table, and the total number of possible permutations while keeping the margins fixed, i.e., p is given by 
  <dig> q+n++,n+−,n+0q−n−+,n−−,n−0qrnr+,nr−,nr0q0n0+,n0−,n00nn+,n−,n <dig> 

here n denotes the sum of the row  margins. the terms in the numerator of the above fraction are the multinomial coefficients and represent the total number of identical tables under random permutations of gene values. we refer to this number as d-value and denote it by d. the denominator is the total number of possible permutations  and is denoted by dtot.

next, we show how to assign various scoring schemes to the table and compute their statistical significance. under the null hypothesis and for any given test statistic s, the significance of an observed value s <dig> of the test statistic is computed by summing the probability of the tables with the same or a more extreme values of the test statistic, i.e., ∑s≥s0p.

the scoring schemes are defined based on the available information on the direction of regulation in the causal graph. table  <dig> shows the most general scenario of mixed networks and subsumes important special cases. for example, if sign information is ignored or not available, the first two rows in table  <dig> will be equal to zero. any differentially expressed transcript that is predicted to be regulated by x is positive evidence for an active regulator x. consequently, the score for the goodness-of-fit of the predictions to the observed experimental data is given by the enrichment score . 
  <dig> es=nr++nr−. 

under the null hypothesis, this test statistic has the following probability mass function. 
  <dig> p=∑es=s0qrnr+,nr−,nr0q0n0+,n0−,n00nn+,n−,n <dig> 

using vandermonde’s identity, it is not difficult to show that the above probability mass function is equivalent to 
  <dig> p=qrnr++nr−q0n0++n0−nn++n−. 

this amounts to fisher’s exact test, a statistic that is routinely used for gene set enrichment tests  <cit> . the test was also proposed by  <cit>  to analyze causal signed networks. however, this score does not use any information on direction of regulation and is unable to predict the likely direction of regulation of an upstream regulator. nevertheless, in networks of unsigned edges it may be the only available option.

in contrast, chindelevitch et al.  <cit>  propose the correctness statistic that focuses on signed edges only. it scores an upstream regulator and its putative direction of regulation by considering the difference between correct and incorrect predictions. as the approach considers three different states for predictions. i.e. correct, incorrect, and not regulated, it was also called the ternary score. transcripts are considered positive evidence for the upregulation of regulator x if their predicted direction of regulation matches their actual direction of differential expression. similarly, they count as negative evidence if the directions do not match . as this method ignores all unsigned edges, the third row of the table is assumed to be zero. the goodness-of-fit score of the table in this case is 
  <dig> cs=n+++n−−−. 

the significance of the above statistic  can be computed in a similar fashion as for the enrichment score, i.e. ∑cs=s0p.

finally, we can introduce our new score which is applicable in mixed networks, but retains the ability to assess directionality for regulators if sign information is available. as this score considers not only correct, incorrect and not regulated transcripts as the ternary or correctness score does, but also ambiguously regulated transcript, we name it the quaternary score. in this case, we combine the evidence metric of the enrichment score for the unsigned interactions with the metric of the correctness score for the signed interactions. starting from an enrichment perspective, the score adds information on likely directionality by penalizing transcripts with incorrectly predicted direction of differential expression. from a correctness score perspective, we include information on activity of the regulator by counting evidence from unsigned interactions . hence, this score can be viewed as an intermediate, matching the enrichment score when no information on the direction of regulation is available and transitioning to the correctness score when all the interactions are signed. in most publicly available networks, it is expected that there will be a mixture of the both types of interactions . this score is defined to be 
  <dig> qs=n+++n−−+nr++nr−−. 

the significance of the quaternary score can be computed in a similar fashion to the other scores. the degrees of freedoms in the randomization of the tables are  <dig>   <dig> and  <dig> in the enrichment, correctness and quaternary cases, respectively. this results in time complexity of o, o and o for computing the entire score distributions. in particular the o and o complexities are impractical for most applications. it is important to note that in all cases above, the scores contain all entries of the table corresponding to the degrees of freedom. including additional terms in the score will not change the distribution but merely shifts it.

in  <cit> , the authors presented an algorithm for approximating the significance of the correctness score. essentially, their algorithm approximates the sum by identifying classes of tables with low probabilities and discarding them from the computations. due to the nature of their algorithm, the entire distribution of the scores must be approximated before the significance of the observed score can be computed. since we are primarily interested in computing the p-values, approximation of the entire distribution is unnecessary and we only need to approximate the probability of the scores that are as or more extreme than the observed score. in the next section we show how we can exploit the structure of the probability distribution of the scores to achieve a more efficient algorithms for enrichment, correctness, and quaternary scores alike.

model
in this section, we outline the theoretical foundation of our algorithm. the probability of scores of tables with given fixed margins follows a specific pattern that can be exploited to approximate the p-value of an observed score s <dig> in an efficient manner. for the ease of presentation, we describe the method in the 3× <dig> setting . the method is naturally generalized to the 4× <dig> case . if the margins of the table are fixed, the table will be completely determined by  <dig> cells in the table, i.e., there are a total of  <dig> degrees of freedom. for example, the table can be parametrized by the upper left 2× <dig> corner of the table, i.e, n++, n−−, n−+, n+−. we may replace one of these parameters  with the score of the table. for a set of fixed margins, we can enumerate all tables in a specific order. for instance we can impose the following dictionary ordering on the entries of the table: 
  <dig> n++,n−−,n−+,n+−,n+ <dig> n− <dig> n0+,n0−,n <dig> 

we emphasize again that the tables are determined once  <dig> parameters are known. figure  <dig> shows probabilities of tables ordered by s, n++, n−− and n−+ on the x-axis. as can be seen, there is a repeated pattern of probabilities for classes of tables defined by these parameters. for example, in the class of tables with a fixed score s, n++ and n−−, we see that the distribution is unimodal . indeed, we will show later that any set of tables with fixed s, n++ and n−− are at most bimodal with the two modes being directly next to each other when ordered in the dictionary ordering of .
fig.  <dig> 
a probabilities of matrices in the dictionary ordering s,n
++, n−−, n
−+, n
+−, n
+ <dig>  n
− <dig>  n
0+, n
0−, n
 <dig> along the x-axis where all margins equal are equal to  <dig>  figure b, c and d show the graph at increasingly higher resolutions



we now give an informal description of the algorithm that will help in motivating the theoretical arguments. let m, m and m denote the categories of tables with fixed parameter values as indicated by the argument. more precisely, m is the set of tables for a given fixed s, m is the set of tables for a given fixed s and n++, and m is the set of tables for a given fixed s, n++ and n−−. there are several tables in each of these categories of various probabilities, ranging from a unimodal graph in m class to a graph consisting of several peaks in m and m classes. note that for a fixed s, the class m contains the tables in m and m classes . our algorithm essentially identifies the peaks of the m class for each possible value of s. by adding the probabilities of tables in a local neighborhood of each peak in the m class, the probability of s is approximated, from which the p-value can be approximated within any user-defined tolerance. in order to achieve this, all the peaks in m  as well as the tables in a local neighborhood around the peaks need to be identified. we achieve this task through a series of adjustments to the entries of tables that can efficiently transition between the tables in each category. the starting point of the algorithm is the table with the minimum possible score smin. there exists only one such table and hence must also be the table of maximum probability within the m class. the algorithm proceeds to find the table with the maximum probability in the next class  by adjusting the entries of the current maximum probability table in a way that a) moves the table to the next desired class and b) the adjustments perturb the table as minimally as possible. the intuition behind this is that the tables with maximum probability in each class correspond roughly to the tables where the entries are most evenly distributed given the margin and class constraints. the reason for this is that the probability of a table is maximized when the numerator in eq.  is maximized which happens approximately when the entries of the table are as close to each other as possible. hence starting from m, we need to adjust the table minimally to move to the table with maximum probability in the next class m. once the table with maximum probability in m is identified, we identify other peaks in subclasses m and m through similar adjustments .

the process of adjusting the tables can be viewed as a permutation process where the symbols +, − and  <dig> are re-distributed into three buckets of sizes q+, q− and q <dig> . the total number of symbols are given by n+, n− and n <dig> respectively . each such distribution is essentially moving the symbols from one bucket to another which results in a table of the same margins but with possibly different entries. we refer to such permutations as moves. the simplest moves are those which interchange  <dig> different elements from two different buckets. for example, we can remove a + from the q+ bucket and place it into the q− bucket; remove a − from the q− bucket and place it into q+ bucket. note that we may need to combine several such moves to obtain a table within the desired class. also, note that the table of the maximum probability in the m class automatically defines m and m classes, i.e., classes in which the table resides. once the algorithm is at this table, all the tables within m class with probability higher than a pre-specified threshold are enumerated via valid moves and their probabilities are added to the probability of the score. as we will see later, there is only one move that generates all tables in m class. for thresholding we use the maximum d-value of all tables  times some ε , i.e., any table in the m class with probability below this threshold value is discarded. this is the same thresholding scheme which was proposed in  <cit> . next, the algorithm moves to the table of maximum probability in m as well as m classes and the same process is repeated until all the n−− values are exhausted, at which point the algorithm moves to the next m and m classes and repeats the process once more. once the m is exhausted the algorithm moves to the next score class toward the tail of the distribution to which the observed score is closer. at each stage of the algorithm, if the table of maximum probability in the m or m classes has probability below the threshold, the entire classes are discarded, which results in significant speed up of the algorithm. if no thresholding is applied the algorithm will be of complexity o where k is the number of considered scores as opposed to the o complexity of the brute force algorithm. however, in practice the approximation scheme will result in complexity much lower than o. we now formalize the definition of moves and prove a few results that are essential for the description of the algorithm.

definition 1
a transposition is a move in which an element x is moved from a bucket qi to a bucket qj, i≠j, and an element y is moved from the bucket qj to the bucket qi. we denote this transposition by .

transpositions are essentially the minimal permutations of the symbols {+,−,0} in the buckets q+, q− and q <dig> that result in tables with the same margins. note that transpositions change a given table only if x≠y. an equivalent way of describing the transpositions is as follows. each transposition corresponds to a 3× <dig> matrix as follows. starting from a zero 3× <dig> matrix whose columns correspond to symbols +, − and  <dig> and whose rows correspond to buckets q+, q− and q <dig>  we place a − <dig> entry where the element is being removed from the corresponding bucket and a + <dig> where the element is being added to the other bucket. other elements of the matrix remain  <dig>  for example τ= corresponds to the matrix m=−10110− <dig>  note that applying the transposition τ to a given 3× <dig> table t is equivalent to adding the matrix representation of τ to t, i.e., τ=m+t. moreover, for appropriately chosen positive integers k, the operation km+t ) will result in a table with equal margins as in t. here, appropriate means that the entries of the resulting table must remain non-negative. the operation km+t is a permutation  that may not necessarily correspond to a transposition. there are a total of  <dig> possible transpositions, each corresponding to a transposition matrix m <dig> m <dig> ...,m <dig>  it can be shown that for any tables t and t′ with the equal margins, there exist a linear combination of the transposition matrices such that 
  <dig> t′=∑i=118kimi+t, 

where ki≥ <dig>  <cit> . in particular, this implies that any arbitrary permutation  σ can be written as a linear combination of transposition matrices i.e σ=∑i=118kimi. in other words, σ can be decomposed as a product of transpositions which keeps the matrix margins fixed and all the entries of the table non-negative. moreover, since matrix addition is commutative, the order in which the transpositions are applied is irrelevant. this is not to say that two moves are commutative as elements of the permutation group, but for a given move, the overall order in which one applies the transpositions to a table is of no importance and the resulting table will always be the same. next, we need to define the notion of evenness in the distribution of the entries of a given table with fixed margins. evenness is used as a proxy for the table of maximum probability in each category. define the auxiliary function d=2+2+ <dig>  minimizing this function will aid in obtaining the most evenly divided table. for example, if we were to distribute the +, − and  <dig> symbols in the q+ bucket as evenly as possible, we would need to minimize d i.e the number of +, − and  <dig> has to be as close to each other as possible. similar reasoning holds for distributing the symbols to other buckets. in general, the measure of evenness can be computed as follows. let t be a 3× <dig> table and let 
 d=d+d+d+d+d+d. 

then the most evenly divided table of given fixed margins is the one with minimum d value.

let m′ be the most evenly divided table of given fixed margins and let τ be a transposition and σ be an arbitrary move that includes τ as a factor in its decomposition. if σ is not a transposition different from τ then we have d≤d)≤d). the first part of the inequality follows from the fact that m′ is the most evenly divided table, hence any move applied to the table will deviate it from evenness. the second part of the inequality holds since the decomposition of σ is not a transposition different from τ and  σ contains τ therefore adjustments applied by σ are at least as large as adjustments applied by τ.

as stated before, the algorithm proceeds from the table with minimum score and identifies tables of maximum probability in subsequent categories. in order to make such transitions, we need to define the notion of principal moves that transition from the table at the current stage of the algorithm to the most probable table in the next desired category. first, we need the definition of a minimal move.

definition 2
minimal moves are the moves generated by considering all possible combinations of transpositions without replacement.

for example, consider the move τ1= applied according to the dictionary ordering , i.e., τ <dig> adjust the entries by the indicated amounts in the same dictionary ordering. in matrix form τ1=−1100001− <dig>  moreover, the move τ <dig> can be decomposed into a product of transpositions as τ1=. since each transposition is repeated once, τ <dig> is a minimal move. on the other hand, consider τ2=. in matrix form, τ2=−110−2023−1− <dig> and can be decomposed as τ2=22=22τ <dig>  note that τ <dig> is not a minimal move since some of the transpositions are repeated more than once.

both of these moves keep the margins of the table fixed as can be readily seen from their matrix representations. since τ <dig> is a product of more transpositions  than τ <dig>  then τ <dig> will adjust the table less. it should be noted that any arbitrary move σ can always be decomposed as σ=σmσn where σm is a minimal move and σn is a product of transpositions that is not necessarily minimal. this is because σ=∏l∈iτl, is a product of transpositions and since the order of applying the transpositions does not matter, we can rearrange the transpositions to attain σ=σmσn. from this, it follows that the minimal moves are precisely those where σ=σmσn with σn= <dig>  here  <dig> represents the identity transposition, corresponding to the zero 3× <dig> matrix.

the algorithm relies on the fact that we can move to the table of highest probability in each class ). for example, if we want to move from the table of highest probability in m, to the table of highest probability in the next class in the dictionary ordering, we can generate a set of moves to make this jump directly. we refer to these moves as principal moves. if there are multiple moves that can achieve this task, we select one at random. the principal moves for decreasing n++ are generated using the algorithm in fig.  <dig>  note that the algorithm in fig.  <dig> uses the function constraints  which returns the set of indices at which the move σ is negative, where indices range from  <dig> to  <dig> in the dictionary ordering .
fig.  <dig> algorithm for computing principal moves which decrease n
++ and fix s




let mmax be the table of maximum probability in m and let mmax  be a table of maximum probability of class m where n++−l is the next possible value of n++ in the dictionary ordering of . the algorithm in fig.  <dig> generates the list of moves σ″∈Σ″ s.t for some σ″ we have σ″=mmax. that is to say if there is indeed a valid matrix mmax then there must be some σ″ that can take us to mmax. we note again that minimal moves contain the set of all possible combinations of constraints that can arise in any arbitrary move. for instance, in the previous example, if τ <dig> is applicable then τ <dig> is also applicable since constraints ⊂ constraints . we should also note that the principal moves change n−− by at most  <dig>  therefore l= <dig>  however, we did use the notation l to stress the fact that for some degrees of freedom, the next value in the dictionary ordering may be greater than  <dig> . we can also generate the principal moves which increase n++ with a slight modification to the algorithm in fig.  <dig>  in a similar fashion, the principal moves which find the next possible value of the next degree of freedom n−− can be calculated. the algorithm in fig.  <dig> will generate the moves which decrease n−− and keep s and n++ fixed.
fig.  <dig> algorithm for computing principal moves which decrease n
−− and fix s,n
++




the only principal move in m is σ1=. this move decreases n−+ and keeps s, n++ and n−− fixed. similarly σ1−1= reverses the effect of σ <dig>  moreover, it can be shown that the only principal s increasing moves that exist, increase the score by  <dig>   <dig> and  <dig>  hence the scores in the domain of the correctness statistic are differenced by  <dig>   <dig> and  <dig>  the algorithm in fig.  <dig> is a slight modification of algorithms  <dig> and  <dig> that generates the principal moves which increase the score by  <dig> 
fig.  <dig> algorithm for computing principal moves which increase s by 1



principal moves which increase the score by  <dig> and  <dig> are generated in a similar way. next we present a few facts about tables with maximum and minimum possible scores that we need in our algorithm. these will be the tables on the tails of the distribution. in order to get the table with maximum score, we have to put the maximum number of + symbols in the q+ bucket, maximum number of − symbols in the q− bucket and the remaining + and − symbols in the q <dig> bucket. the rest of the entries are determined by the margins of the table. therefore, we have to set the entries as n++= min{q+,n+}, n−−= min{q−,n−}, n0+= min{q <dig> n+−n++} and n0−= min{q0−n0+,n−−n−−}. in particular this shows that there is only one table with maximum score. similarly there is only one table with minimum score and the entries of the table are given by n+−= min{q+,n−}, n−+= min{q−,n+}, n0+= min{q <dig> n+−n−+} and n0−= min{q0−n0+,n−−n+−} and there is only one table with minimum score.

theorem 1
in the m class, there exists at most two matrices with maximum probability.

proof
we know that there exists at least one table t∈m with highest probability. let t′∈m be such that t′=σ1− <dig>  consider the ratio of the probabilities of t′ and t: 
  <dig> dd=n+−n−0n0+ 

  <dig> =× 

we see from eq.  that as we increase n−+ the probability becomes smaller than or equal to prob. moreover, since n−++n+−=+, n+0+n−0=+ and n0++n0−=+, we see that eq.  can equal  <dig> only if n−+=n+−− <dig>  n+−=n−++ <dig>  n+0=n−0− <dig>  n−0=n+0+ <dig>  n0+=n0−+ <dig>  n0−=n0++ <dig>  hence we see that there can be at most two tables with maximum probability and the proof is complete. □

similarly, it is not difficult to see that for m there exist at most two tables which are most evenly devided. we can now state the algorithm which computes the probability of a score formally.

the algorithm in fig.  <dig> shows that for computing the probability of a score s, we have to iterate through all the possible values of n++ and n−−. we apply the principle moves to find out if it is possible to increase or decrease the values of n++ and n−−. the starting values of n++ and n−− are those of the matrix with highest probability mmax for a given score s. to get mmax, we have to start at the matrix of minimum score, then apply principal moves to get the matrix with highest probability of the next score. the procedure is repeated until we reach our target score. when increasing or decreasing the values of n−−, we choose the principal move σ which maximizes the probability and leaves n++ fixed. this method can naturally be generalized to the 4× <dig> case. the only difference is that the 4× <dig> case can have more than one table of minimum score. when this happens, the tables of minimum score have  <dig> degree of freedom, so the table of highest probability can be found by computing the mode of the hypergeometric distribution. in practice, the algorithm in fig.  <dig> as implemented in the r package is modified to allow discarding matrices below a certain threshold, and thus approximate the probability or p-value of a score similar to  <cit> . when increasing/decreasing the values of n++ and n−−, we can discard the classes m and m which have a maximum probability less than a certain threshold. similarly, since the probabilities in m are at most bimodal with the two modes being directly next to each other, we can stop increasing/decreasing n−+ when the probability falls below a certain threshold. thresholding significantly speeds up the algorithm since there are many tables of negligible probabilities.
fig.  <dig> algorithm for computing the probability of a score s



data processing
all gene expression data were normalized and differentially expressed genes were computed using the r limma package. unless otherwise stated, in all analyses we used a minimum  <dig>  fold change and < <dig>  fdr corrected p-value to detect differential expression. differentially expressed genes were assigned to + <dig>  or − <dig>  according to the sign of the fold change.

RESULTS
all results are based on the r package quaternaryprod which implements the above outlined strategy to compute enrichment, correctness, and quaternary p-values given differentially expressed genes and a mixed input network. the package is written in rcpp  <cit>   and is available to download from bioconductor. we will first demonstrate the quality and speed of the approximation approach. we will then show that the quarternary statistic compares favorably with previous statistics in a simulation setting. finally, we demonstrate the ability of our algorithm to recover plausible biological hypothesis using the publicly available string <dig>  <cit>  network in the context of controlled in-vitro overexpression experiments, stem-cell differentiation data and animal models of neuropathic pain.

benchmarking and quality of approximation
we benchmarked our algorithm against previous implementation of the correctness score  <cit> . to assess the speed of the algorithms, we generated  <dig> tables with values ranging from  <dig> to  <dig> for the q+, q−, n+, n− margins and values ranging from  <dig> to  <dig> for q <dig>  the qr is set to  <dig> in correctness score calculations. the range of the values were selected to reflect typical gene expression and network connectivity values. for each table the p-values of the score and the elapsed time were calculated for both algorithms. the threshold value was set to 1e- <dig> in both algorithms. on average our algorithm runs 45× faster than that of  <cit>  with a maximum speed up to a 1000× depending on where the observed score falls in the distribution. additionally we tested the speed of the algorithm in computing the significance of the enrichment score as compared with the fisher’s exact test implemented in the r fisher.test function. the time taken in both algorithms are very comparable and typically ≤ <dig>  sec.

next we assessed the speed and accuracy of the algorithm in computing the significance of the quaternary score. both speed and the accuracy depends on the selected value of the threshold. for example, setting the threshold value to  <dig> will result in a brute force computation of the p-value , but slow runtime  complexity). on the other extreme, setting the threshold to  <dig> will run the fastest, but with very low accuracy.

results on simulated data
in order to illustrate the performance of the three scoring statistics  in networks with various degrees of ambiguity, we consider a hypothetical network consisting of  <dig>  transcripts and  <dig>  potential upstream regulators. we assume an active upregulated regulator r <dig> with  <dig> downstream transcripts. we also consider an inactive regulator r <dig> which shares the same set of  <dig> downstream transcripts. r <dig> and r <dig> differ in the direction of regulation for  <dig> of their  <dig> downstream transcripts, i.e. they share a certain degree of their downstream response, but also differ substantially. all other regulators will not be considered here and will, in general, overlap only to a small degree with r1’s and r2’s downstream transcripts. we reflect their presence by choosing a multiple testing corrected significance threshold of  <dig> /5000=10− <dig>  we then simulate  <dig> expression data sets based on r1’s active state by randomly assigning expression changes to  <dig> % of r1’s down stream transcripts correctly and  <dig> % incorrectly. in addition, we randomly add  <dig> downregulated transcripts and  <dig> upregulated transcripts that reflect other ongoing changes in the system, potentially related to other regulators in the network.

each expression data set is generated based on the true underlying network structure. to reflect our incomplete knowledge of direction of regulation we subsequently set a larger and larger fraction of edges to an unsigned state and compute p-values for all three scoring statistics. note that we do not consider deletion or insertion of random edges here. such analysis has been conducted in  <cit> . our focus is on the presence of ambiguous edges in the network. simulation results are depicted in fig.  <dig>  thin lines represent individual simulations and thick lines represent the averages across the  <dig> simulations.
fig.  <dig> this figure shows simulations results in recovering upstream regulators using the quaternary , correctness  and es  scoring statistics. thin lines represent individual simulations and thick lines represent the averages across the  <dig> simulations. the x-axis represents the percentage of ambiguous edges in the network and the y-axis represents the p-value on a log scale. figure on the left depicts an active regulator r <dig> simulated to be up-regulated. figure on the right depicts an inactive regulator r <dig> that shares downstream transcripts with r <dig> but has a different direction of regulation for  <dig> % of its edges. the dotted black line marks a significance level of 1e- <dig>  further details in the main text



firstly, note that the quaternary p-value reduces to the correctness p-value in the case of no ambiguity and to the enrichment p-value for complete ambiguity. furthermore, the enrichment p-value is always constant as only the direction of regulation changes in our simulation. in our example, the enrichment p-value would always correctly flag r <dig> and incorrectly flag r <dig> as active. the correctness p-value correctly identifies r <dig> as active and r <dig> as inactive when full information on direction of edges is available. however, the performance deteriorates quickly when more and more ambiguous edges are present and no regulators are detected as active. our new quaternary statistic is able to optimally make use of the available information. it is able to predict the correct activity status for the regulators even with significant ambiguity. specifically, it retains the ability to detect r2’s inactive state even with little information of directionality of regulation and will, therefore, lead to more precise hypotheses for follow-up, if the direction of regulation information in the network is trustworthy. in contrast, if our knowledge of direction of regulation is faulty, enrichment scores might give superior results in some cases. similarly, if unsigned edges are not trustworthy, the correctness score would be preferable to the quaternary score. in general, we assume a network topology as well as specified direction of regulation to reflect a  version of the underlying true network. in that setting, the quaternary score should be the statistic of choice. detailed characteristics of the simulation depend on the chosen patterns values, but the outlined patterns remain valid for a wide range of parameter choices.

recovering known stimuli in an in-vitro setting
to demonstrate the performance of our method in conjunction with the publicly available string <dig> network, we use the same validation set as suggested by  <cit> . this dataset was derived from  <cit>  in which they used recombinant adenoviruses to infect non-cancerous human mammary epithelial cells with a construct to overexpress specific oncogenes. this provides an excellent test dataset as there are clear single perturbations to recover. as in  <cit> , we focus on the c-myc, h-ras, and e2f <dig> expression signatures. differential gene expression analysis of these data sets resulted in  <dig>   <dig>  and  <dig> differentially expressed genes respectively. table  <dig> shows the top  <dig> regulators predicted by the algorithm along with the fdr corrected p-values of the scoring schemes. note that the p-values differ from the original publication due to the applied multiple testing correction and the use of a different network. in the c-myc experiment, the algorithm recovers the up-regulation of max as the top hypothesis. it has been demonstrated that oncogenic activity of c-myc requires dimerization with max  <cit> . myc is the second top hypothesis. in this case the cs p-value is more significant than the qs p-value. there are a total number of  <dig> genes downstream of c-myc in the network. of these  <dig> are ambiguous, only one of which is connected to a differentially expressed gene. for the e2f <dig> experiment, e2f <dig> is returned as the top hypothesis. e2f <dig> and e2f <dig> are close family members and have a very similar role as transcription factors that function to control the cell cycle and are similarly implicated in cancer  <cit> . note that in contrast to qs, the cs algorithm is unable to recover this hypothesis at a significant fdr corrected p-value. the fraction of unsigned edges implicating e2f <dig> is relatively high at  <dig> % and this result demonstrates the advantages of the qs algorithm in such cases. in the h-ras experiment, egr <dig> is the top hypothesis returned by the algorithm with a very significant quaternary p-value. egr <dig> is a key regulator of oncogenic processes and is downstream of, and positively regulated by, hras  <cit> , fitting the direction of regulation observed in our results. in summary, we are able to recover either the known perturbation, a paralogous gene, or a downstream mediator of the perturbed gene’s activity. in all cases the biology behind the expression signature is sufficiently explained, and we would expect the accuracy of our predictions to improve as coverage of the interaction network expands.
fdr corrected p-values of the  <dig> scoring schemes are listed: quaternary score , correctness score  and enrichment score 



factors for stem cell directed differentiation
directed differentiation of stem cells to specific cell types is an important challenge in regenerative medicine. using a time course of stem cell differentiation to a pancreatic endocrine fate we previously showed that the cs statistic was able to identify interleukin  <dig>  as a novel secreted factor involved in this process  <cit> . however this result was only obtained with the cs statistic in conjunction with a proprietary network. repeating the analysis with the string <dig> network we are only able to obtain significant results  with the qs statistic. table  <dig> shows the top  <dig> regulators.
pancreatic endocrine maturation



aurora kinase b , gastrin , il <dig>  fgf <dig> and neurog <dig> are all predicted to be up-regulated during endocrine specification. of these il <dig>  neurog <dig> and gastrin have known roles in pancreatic endocrine formation. we consider this good evidence that the qs statistic provides significant additional power to identify upstream regulators of stem cell differentiation compared to cs and that this allows the method to be successfully used in conjunction with freely available causal networks. next we turned to a model of early forebrain and eye field development . neural progenitor cells were replated from a fibronectin matrix to cellstart and treated with the secreted factor activin a for  <dig> days in order to generate retinal precursors. microarrays were used to profile the transcriptome of the cells before and after treatment . there were a total of  <dig> differentially expressed genes which were used as input to the qs statistic in conjunction with the string <dig> network. the top  <dig> most significant hypotheses are shown in table  <dig> 
early forebrain and eye field development



of these tgfb <dig>  is the primary ligand of the canonical transforming growth factor beta signaling pathway that is also activated by activin a  <cit> . we consider therefore that while the method is unable to recover the precise treatment applied to the cells it has successfully identified the correct activated pathway. the activation of ptk <dig>  is also expected and consistent with the replating of the cells onto a new extracellular matrix as ptk <dig> is directly downstream of signals initiated by cell-ecm interactions  <cit> . the activation of vegfa  and bmp <dig>  signaling is unexpected as neither of these factors are present in the exogenously provided media after replating. returning to the original transcription data revealed that both genes encoding these factors were expressed by the cells at least  <dig> fold higher post-treatment  suggesting that these pathways are activated endogenously within the culture in response to the replating and activin a treatment. bmp <dig> in particular is known to play a key role in eye development consistent with the overall hypothesis that the cells are being driven to an ocular fate  <cit> . atf <dig> activation is also novel in this system. there is no concomitant change in expression of the atf <dig> gene as we observe for vegfa and bmp <dig>  but there is evidence in other models that activation of atf <dig> via phosphorylation by p <dig> kinase can occur in response to activin a treatment  <cit> , suggesting that this transcription factor may play an important role in mediating the downstream effects of activin a.

an animal model of neuropathic pain
characterisation of animal disease models is an important class of biomedical experiment and we wished to test whether our method could provide insight into regulatory pathways using data from such a model. neuropathic pain is a significant chronic pain state caused by injury or other damage, e.g. inflammatory, to the nervous system.  <dig> % of the european population is thought to suffer from chronic pain, with  <dig> % exhibiting chronic neuropathic pain  <cit> . we previously reported a gene expression signature from a model of neuropathic pain  <cit>   and here we apply causal reasoning to identify the underlying molecular basis for the establishment of a chronic neuropathic pain state.

of the top hypotheses, the majority are immunological . the most significant causal hypothesis is il1b, a key cytokine involved in the development of neuropathic pain and which has been shown to directly enhance excitatory currents within neurons of the drg  <cit> . the third causal hypothesis, il <dig>  has also been shown to directly modulate neuronal activity, reducing inhibitory currents  <cit> . both hypotheses fit with the known underlying pathology of neuropathic pain whereby a large pro-inflammatory response occurs in response to injury, leading to long term maladaptive plasticity that maintains a chronic neuropathic pain state  <cit> .
animal models of neuropathic pain



CONCLUSIONS
in this work, we have closed an important gap in utilizing causal networks to analyze differentially expressed genes. our proposed quaternary test statistic incorporates all available evidence on the potential relevance of an upstream regulator as exemplified in fig.  <dig> and can be seen as a generalization of the well-known enrichment score used in gene set enrichment approaches  <cit>  and the correctness statistic suggested in  <cit> . this new approach broadens the use of these types of statistics for highly curated signed networks in which ambiguities arise but also enables the use of networks with unsigned edges, i.e. mixed networks, which are prevalent in the academic sector. a direct estimation of the null distribution of the proposed statistic would lead to a prohibitively slow o algorithm. in this work, we design and implement a novel computational method that can efficiently estimate p-values for commonly occurring tables in current biological settings. most importantly, we demonstrate the ready applicability of the implemented method to analyze differentially expressed genes using the publicly available string <dig> network. while the precision of inference is not as high as with commercially available networks at this point, the derived putative upstream regulators describe relevant biology and can readily be used for follow-up hypothesis testing. we see future work for the inference of upstream regulators given mixed networks primarily in the area of plausible and efficient incorporation of biological context and the construction of higher level models. while zarringhalam et al.  <cit>  provided an initial proposal for bayesian inference incorporating context on signed networks and kramer et al.  <cit>  extend upstream regulator discovery beyond the first layer, many questions around efficient inference, publicly available data and best practices remain to be solved.

with this work we hope to broaden the appeal of prior causal network methods in the academic community by demonstrating that biologically plausible inference is possible with currently available networks and the r package quaternaryprod we provide with this paper. we believe this will generate biologically testable hypotheses in specific use cases, but also spur method development to tackle outstanding questions in this field.

endnote
 <dig> in this paper, we consider string <dig> as available under a creative commons attribution  <dig>  license.

abbreviations
qsquaternary score

cscorrectness score

esenrichment score

fdrfalse discovery rates

none.

funding
no funding was provided for this study.

availability of data and materials
∙ software: r package quaternaryprod. ∙ project home page: github.com/carltonyfakhry/quaternaryprod or www.bioconductor.org/packages/ <dig> /bioc/html/quaternaryprod.html ∙ license: gpl- <dig>  ∙ operating systems: platform independent. ∙ programming languages: c++, r. ∙ data and code for experiments: https://github.com/carltonyfakhry/quaternaryprod-script ∙ any restrictions to use by non-academics: none.

authors’ contributions
ctf, dz and kz developed and tested the methods and drafted the manuscript. ctf implemented the algorithms in c++. dz and kz conceived the project. ctf, kz and dz reviewed the literature. pchoudhary, ag and bs performed the biological experiments and interpreted the biological results. pchen participated in the design and coordination of the project. all authors read and approved the final manuscript.

competing interests
the authors declare that they have no competing interests.

consent for publication
not applicable.

ethics approval and consent to participate
not applicable.
