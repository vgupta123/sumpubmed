BACKGROUND
electrostatic interactions in a molecule are of utmost importance for analyzing its structure  <cit>  as well as functional activities like ligand binding  <cit> , complex formation  <cit>  and proton transport  <cit> . the calculation of electrostatic interactions continues to be a computational bottleneck primarily because they are long-range by nature of the 1rpotential  <cit> . as a consequence, efficient approximation algorithms have been developed to reduce this computational complexity  method  <cit> , the fast multipole method  <cit>  and the hierarchical charge partitioning   <cit> ). the approximation algorithms can be parallelized on increasingly ubiquitous multi- and many-core architectures to deliver even greater performance benefits.

widespread adoption of general-purpose graphics processing units  has made them popular as accelerators for parallel programs  <cit> . the increased popularity has been assisted by  phenomenal computing power,  superior performance/dollar ratio, and  compelling performance/watt ratio. for example, an 8-gpu cluster, costing a few thousand dollars, can simulate  <dig> ns/day of the jac benchmark as compared to  <dig> ns/day on the kraken supercomputer, housed at oak ridge national lab and which costs millions of dollars  <cit> . the emergence of gpus as an attractive high-performance computing platform is also evident from the fact that three out of the top five fastest supercomputers on the top <dig> list employ gpus  <cit> .

although the use of approximation algorithms can improve performance, they often lead to an increase in the memory boundedness of the application. achieving optimum performance with a memory-bound application is challenging due to the 'memory wall'  <cit> . the effect of the memory wall is more severe on gpus because of the extremely high latency for global memory accesses . furthermore, for maximum performance on the gpu, execution paths on each gpu computational unit need to be synchronized. however, an important class of approximation algorithms, i.e., multi-scale approximations result in highly asynchronous execution paths due to the introduction of a large number divergent branches, which depend upon the relative distances between interacting atoms.

to test these expectations, we present a hybrid approach wherein we implement the robust multi-scale hcp approximation algorithm in a molecular modeling application called gem  <cit>  and map it onto a gpu. we counteract the high memory boundedness of hcp by explicitly managing the data movement, in a way that helps us achieve significantly improved performance. in addition, we employ the standard gpu optimization techniques, such as coalesced memory accesses and the use of shared memory, quantifying the effectiveness of each optimization in our application. hcp results in supreme performance on the gpu despite the introduction of divergent branches. this is attributed to the reduction in memory transactions that compensates for divergent branching.

recently, several molecular modeling applications have used the gpu to speed-up electrostatic computations. rodrigues et al.  <cit>  and stone et al.  <cit>  demonstrate that the estimation of electrostatic interactions can be accelerated by the use of spherical cut-off method and the gpu. in  <cit> , hardy et al. used a multi-scale summation method on the gpu. each of the aforementioned implementations artificially maps the n atoms of a molecule onto a m-point lattice grid and then applies their respective approximation algorithm. by doing so, they reduce the time complexity of the computation from o to o. in contrast, we use hcp, which performs approximations based on the natural partitioning of biomolecules. the advantage of using the natural partitioning is that even with the movement of atoms during molecular dynamics simulations, the hierarchical nature is preserved, whereas with the lattice, atoms may move in and out of the lattice during the simulation. our implementation realizes a maximum of  <dig> -fold speedup over a hand-tuned sse optimized implementation on a modern 16-core cpu, without any loss in the accuracy of the results.

methods
electrostatics and the hierarchical charge partitioning approximation
we use the analytic linearized poisson-boltzmann  model to perform electrostatic computations  <cit> . equation  computes the electrostatic potential at a surface-point  of the molecule due to a single point charge, q. the potential at each vertex can be computed as the summation of potentials due to all charges in the system. if there are p vertices, the total surface potential can then be found as the summation of potential at each vertex.

  ϕioutside=qiεin11+αεinεout1+αdi-α1-εinεoutr 

computing the potential at p vertices results in a time complexity of o where n is the number of atoms in the molecule. to reduce the time complexity, we apply an approximation algorithm called hierarchical charge partitioning , which reduces the upper bound of computation to o.

hcp  <cit>  exploits the natural partitioning of biomolecules into constituent structural components in order to speed-up the computation of electrostatic interactions with limited and controllable impact on accuracy. biomolecules can be systematically partitioned into multiple molecular complexes, which consist of multiple polymer chains or subunits and which in turn are made up of multiple amino acid or nucleotide groups, as illustrated in figure  <dig>  atoms represent the lowest level in the hierarchy while the highest level depends on the problem. briefly, hcp works as follows. the charge distribution of components, other than at the atomic level, is approximated by a small set of point charges. the electrostatic effect of distant components is calculated using the smaller set of point charges, while the full set of atomic charges is used for computing electrostatic interactions within nearby components. the distribution of charges for each component, used in the computation, varies depending on distance from the point in question: the farther away the component, the fewer charges are used to represent the component. the actual speedup from using hcp depends on the specific hierarchical organization of the biomolecular structure as that would govern the number of memory accesses, computations and divergent branches on the gpu. under conditions consistent with the hierarchical organization of realistic biomolecular structures, the top-down hcp algorithm  scales as o, where n is the number of atoms in the structure. for large structures, the hcp can be several orders of magnitude faster than the exact o all-atom computation. a detailed description of the hcp algorithm can be found in anandakrishnan et. al.  <cit> .

gpu architecture and programming interface
for this study, we have used state-of-art nvidia gpus based on the compute unified device architecture or cuda framework. cuda is a framework developed by nvidia, which facilitates the implementation of general-purpose applications on gpus. below is a brief description of the nvidia gpu hardware architecture and the cuda programming interface.

nvidia gpus consist of 240- <dig> execution units, which are grouped into  <dig> and  <dig> streaming multiprocessors  on fermi and gt <dig> architectures, respectively. an overview of these architectures is shown in figure  <dig>  multiple threads on a gpu execute the same instruction, resulting in a single instruction, multiple thread  architecture. this is what makes gpu very suitable for applications that exhibit data parallelism, i.e., the operation on one data element is independent of the operations on other data elements. therefore, it is well suited for molecular modeling where the potential at one vertex can be computed independently of all others.

on nvidia gpus, threads are organized into groups of  <dig>  referred to as a warp. when threads within a warp follow different execution paths, such as when encountering a conditional, a divergent branch takes place. execution of these group of threads is serialized, thereby, affecting performance. on a gpu, computations are much faster compared to a typical cpu, but memory accesses and divergent branching instructions are slower. the effect of slower memory access and divergent branching can be mitigated by initiating thousands of threads on a gpu, such that when one of the threads is waiting on a memory access, other threads can perform meaningful computations.

every gpu operates in a memory space known as global memory. data which needs to be operated on by the gpu, needs to be first transferred to the gpu. this process of transferring data to gpu memory is performed over the pci-e bus, making it an extremely slow process. therefore, memory transfers should be kept to a minimum to obtain optimum performance. also, accessing data from the gpu global memory entails the cost of 400- <dig> cycles and hence, on-chip memory should be used to reduce global memory traffic. on the gt <dig> architecture, each sm contains a high-speed,  <dig> kb, scratch-pad memory, known as the shared memory. shared memory enables extensive re-use of data, thereby, reducing off-chip traffic. whereas on the latest fermi architecture, each sm contains  <dig> kb of on-chip memory, which can be either be configured as  <dig> kb of shared memory and  <dig> kb of l <dig> cache or vice versa. each sm also consists of a l <dig> cache of size  <dig> kb. the hierarchy of caches on the fermi architecture allows for more efficient global memory access patterns.

cuda provides a c/c++ language extension with application programming interfaces . a cuda program is executed by a kernel, which is effectively a function call to the gpu, launched from the cpu. cuda logically arranges the threads into blocks which are in turn grouped into a grid. each thread has its own id which provides for one-one mapping. each block of threads is executed on a sm and share data using the shared memory present.

mapping hcp onto gpu
the problem of computing molecular surface potential is inherently data parallel in nature, i.e., the potential at one point on the surface can be computed independently from the computation of potential at some other point. this works to our advantage as such applications map very well onto the gpu. we begin with offloading all the necessary data  to the gpu global memory. to ensure efficient global memory accesses patterns, we flattened the data structures. by flattening of data structures we mean that all the arrays of structures were transformed into arrays of primitives so that the threads in a half warp  access data from contiguous memory locations  <cit> . the gpu kernel is then executed, wherein each thread is assigned the task of computing the electrostatic potential at one vertex. at this point the required amount of shared memory, i.e, number of threads in a block times the size of the coordinates of each vertex, is allocated on each streaming multiprocessor  of the gpu. the kernel is launched multiple times as required, until all the vertices are exhausted, with implicit gpu synchronization in between successive kernel launches. on the gpu side, each kernel thread copies the coordinates of its assigned vertex onto the shared memory. this results in a reduction of the number of global memory loads as explained in the results section. the limited amount of per sm shared memory does not allow us to offload the coordinates of constituent components of the biomolecule and hence, coordinates of complexes, strands, residues, and atoms have to remain in global memory. the hcp algorithm is then applied to compute the electrostatic potential, and the result is stored in the global memory. all the threads perform this computation in parallel, and after the threads finish, the computed potential at each vertex is transferred back to the cpu memory, where a reduce  operation is performed to calculate the total molecular surface potential. according to the algorithm, evaluation of distance between the vertex and molecular components requires each thread to accesses coordinates from the global memory. this implies that potential calculation at each vertex necessitates multiple global memory accesses, which makes hcp memory-bound on the gpu.

hcp also introduces a significant number of divergent branches on the gpu. this phenomenon occurs because for some threads in a warp, it may be possible to apply hcp approximation while for other, it may not be possible to do so. therefore, these two groups of threads would diverge and follow respective paths, resulting in a divergent branch. in the results section, we show how the associated costs of divergent branching in hcp on the gpu can be amortized to deliver a performance boost.

test setup
to illustrate the scalability of our application, we have used four different structures with varied sizes. the characteristics of the structures used are presented in table  <dig>  the gpu implementation was tested on the present generation of nvidia gpus.

the host machine consists of an e <dig> intel quad core running at  <dig>  ghz with  <dig> gb ddr <dig> sdram. the operating system on the host is a 64-bit version of ubuntu  <dig>  distribution running the  <dig> .28- <dig> generic linux kernel. programming and access to the gpu was provided by cuda  <dig>  toolkit and sdk with the nvidia driver version  <dig> . for the sake of accuracy of results, all the processes that required a graphical user interface were disabled to limit resource sharing of the gpu.

we ran our tests on a nvidia tesla c <dig> graphics card with gt <dig> gpu and the nvidia fermi tesla c <dig> graphics card. an overview of both of these gpus is presented in table  <dig> 

RESULTS
in this section, we present an analysis of  the impact of using shared memory,  the impact of divergent branching,  the speedups realized by our implementation, and  the accuracy of our results. on cpu, the timing information was gathered by placing required time-checking calls around the computational kernel, excluding the i/o required for writing the results. on gpu, the execution time was measured by using the cudaeventrecord function call. for a fair comparison, time for offloading the data onto the gpu global memory and storing the results back onto the cpu was taken into account along with the time for execution of the kernel. single precision was used on both platforms. all the numbers presented are an average of  <dig> runs performed on each platform. for hcp, the 1st-level threshold was set to 10Å and the 2nd-level threshold was fixed at 70Å.

impact of using shared memory
at every approximation level, hcp reuses the vertex coordinates to compute the distance between the vertex and molecule components. therefore in the worst case when no approximation can be applied, same data is accessed four times from the global memory . we used the shared memory to reduce these global memory accesses. percentage reduction in the number of global memory loads due to the use of shared memory on gt <dig> architecture, with and without hcp approximation, is shown in table  <dig>  the base line for each of these columns is the respective implementation, i.e., without_hcp and hcp, without the use of shared memory. these numbers were taken from the cuda visual profiler provided by nvidia  <cit> .

from the table, we note that the global memory loads are reduced by 50% for all structures, when the hcp approximation is not used. whereas with hcp, the amount by which loads are reduced varies from structure to structure. this can be reasoned as follows. when no approximation is applied, the coordinates of vertices and that of all atoms are accessed from global memory, which requires cycling through the residue groups. therefore when shared memory is not used, the vertex coordinate is loaded twice, once for residue and once for the atom. while when shared memory is used, it is loaded only once, i.e., for copying into the shared memory, thereby, resulting in a 50% reduction in global memory loads.

but in the case of hcp, the number of times a vertex coordinate is loaded from global memory depends upon the structure. this is because for each structure the effective number of computations to be performed are different. for example, for a structure with 1st level of approximation and no shared memory usage, vertex coordinates would be loaded three times from the global memory -  to compute the distance to the complex,  to compute the distance to the strand and  finally to compute the distance to the residue. while with shared memory it would be accessed just once. similarly, for a structure with no approximation, the vertex would be accessed four times, without using shared memory. therefore, the table suggests that least number of components could be approximated for the virus capsid, and hence, maximum percentage reduction.

use of the shared memory resulted in a drastic reduction in the number of global loads and hence, provided about  <dig> -fold speedup to our application.

impact of divergent branching
divergent branching on a gpu occurs when the threads of a warp follow different execution paths. in our gpu implementation, each thread takes charge of one vertex and as shown in figure  <dig>  it is possible for threads within a warp to follow different execution paths, thereby, introducing a divergent branch. here we quantify the cost of divergent branches that are introduced. for the ease of exposition, we limited our analysis to one level of hcp only, but this can be extended. as the execution paths are now serialized, the time taken for the execution of a divergent branch, denoted by tdivbranch can be characterized as follows:

  tdivbranch=t1+t <dig> 

where ti denotes time taken for execution path i, as shown in figure  <dig>  from the figure, it can be noted that both execution paths perform the similar task of calculating the potential. path# <dig> calls the function, calcpotential () just once while for path# <dig>  calcpotential () is called from within a loop that iterates over all atoms of that residue. hence, it can be inferred that time for divergent branches is directly proportional to the number of atoms in the molecule.

  t2∴tdivbranch∴tdivbranch≫≈≈∝t1t2#atoms×tcalcpotential#atoms 

thus, total time for all the divergent branches in the system:

  tdivbranch=#divbranches×tdivbranch∴tdivbranch∝#atoms 

from , we can say that the cost of divergent branches would be maximum for the molecule which has the greatest #atoms but this is not true as can be seen from figure  <dig>  in the figure, we present the speedups achieved due to hcp on cpu as well as the gpus with #atoms increasing from left to right. as all the speedups are positive, we can safely infer that gpu is indeed beneficial for hcp despite the introduction of divergent branches. we also note that speedup achieved due to hcp on gpus, increases with the increase in #atoms in the structure, thus, dissatisfying . hence, there exists an aspect which compensates for the cost of introduction of divergent branches. as hcp is a memory-bound application, number of memory transactions dominate the execution time. looking back at the algorithm , we observe that hcp does reduce the number of memory transactions. it does so by applying the approximation, which results in reduced fetching of coordinates from the global memory. now, coordinates of only the higher level component are required and hence, compensating for the cost of divergent branches. execution time for the entire application with hcp can be derived as follows:

  twith_hcpwheretmem=twithout_hcptmem+tdivbranch=global memory access time 

hcp would guarantee improved performance on the gpu if the gain in time due to reduced memory transactions is greater than the cost of divergent branches. however, computing memory access times on the gpu is an extremely difficult task as one has no knowledge of how warps are scheduled, which is essential as the warps send access requests to memory controllers. hence, no direct method to measure global memory access times exists. we used an indirect approach and found out the reduction in memory transactions as well as the increase in divergent branches for our application. these numbers have been taken using from the cuda visual profiler provided by nvidia and are presented in table  <dig> <cit> . the memory transactions in the table portray the sum of 32-, 64- and 128-byte load and store transactions per sm. also, the number of divergent branches represent the divergent branches introduced on one sm. from the table, it is seen that the reduction in memory transactions is orders of magnitude greater than the increase in divergent branches. from the table, we note that the number of memory transactions reduced per one divergent branch is maximum for the capsid, which results in the fact that hcp+gpu is most effective for capsid. figures  <dig> and  <dig> corroborate this fact and hence, we can attest that it is the reduction in memory transactions which help make gpus favorable for hcp. this proves that even an algorithm with divergent branching can be benefited by the gpu, provided there is some aspect which amortizes the cost of the divergent branches introduced.

speedup
figures  <dig> and  <dig> present speedups achieved by our implementation on nvidia tesla c <dig> and nvidia fermi tesla c <dig> gpus respectively. both the figures present speedup over the cpu implementation optimized by hand-tuned sse intrinsics and parallelized across  <dig> cores, without the use of any approximation algorithm. speedups achieved due to the use of gpu alone as well as that due to the combination of gpu and hcp are presented for all four structures.

from both these figures, we note that speedup due to the gpu alone is almost constant for all three structures barring mb.hhelix. this is because mb.hhelix is an extremely small structure and it does not requires enough gpu threads for the computation of its molecular surface potential, thereby, leaving the gpu under utilized. this phenomenon is prominent in case of the fermi tesla c <dig> which actually results in a slowdown due to under-utilization of the gpu. for other structures the threshold of the number of threads is met and almost similar speedup is achieved across both the figures. the observed speedup is around 11-fold on tesla c <dig>  whereas on tesla c <dig> the speedup is around 25-fold. the increased speedup on c <dig> may be attributed to several architectural differences between fermi and gt <dig> gpus, like the ability for concurrent kernel execution, ecc support and fewer sms. however, the architectural feature that we feel has the most impact for this algorithm, is the presence of a hierarchy of caches on fermi, as they allow for greater exploitation of the locality of data. for no approximation, all atoms need to be accessed sequentially, thereby, making the caches play an important role and hence, fermi tesla c <dig> is deemed to be more effective.

as explained in a previous section, application speedup due to the combination of gpu and hcp increases with the increase in number of memory transactions reduced per divergent branch increased. therefore, from table  <dig>  number of memory transactions reduced is maximum for virus capsid and hence, it attains the maximum speedup. next highest reduction in the number of memory transactions is for 2eu <dig> and hence, next highest speedup and so on. our application manages to achieve up to  <dig> -fold speedup with hcp on tesla c <dig> for capsid while the corresponding speedup on fermi tesla c <dig> is approximately  <dig> -fold. the actual execution time of our implementation on both gpus is < <dig> sec.

speedup achieved with hcp on tesla c <dig> is less than that achieved on tesla c <dig> due to the fact that the algorithm fails to take the advantage of the caches present on fermi, as before. with hcp, not all memory requests are sequential as coordinates of both atoms and high level components are required, making the caches less potent than before. speedups achieved across all figures for without_hcp version are almost consistent for both the gpus, which is because it does not introduce divergent branches. whereas, the version with hcp, results in divergent branches and also varying amounts of speedups across structures, depending upon how much cost of the divergent branches can be amortized by the corresponding reduction in memory transactions.

accuracy
to get the best performance on gpus, we used single precision as double precision on gt <dig> architecture adversely impacts the performance by as much as 8-times. although double precision on fermi is almost half as fast as single precision, we decided to stick with single precision for greater performance than accuracy. to get an estimate of the accuracy of our results, we compute the relative root mean squared error  of the single-precision-gpu implementation against the double-precision-cpu implementation. the results are shown in table  <dig>  we also present the error due to hcp both on cpu and the gpu. hcp, being an approximation algorithm, does introduce some error on the cpu. from the table, we note that the error introduced by the gpu itself is fairly negligible when compared to the error introduced by hcp alone on the cpu. thus, the total error due to hcp and gpu is almost equivalent to the error on the cpu. therefore, we can safely conclude that single precision on gpu does not jeopardize the accuracy of our computed results.

due to the paltry error introduced by single precision on the gpu, it may be deemed acceptable for the computation of molecular surface potential on the gpu but may be unsatisfactory for molecular dynamics. in case of molecular dynamics simulations, even a minute error in one time step can have a substantial effect on the results as the error would accumulate during the course of the simulation. it is here that superior double precision support of fermi would come in handy.

CONCLUSIONS
with the emergence of gpu computing, there have been many attempts at accelerating the electrostatic surface potential  computations for biomolecules. in our work, we demonstrate the combined effect of using a multi-scale approximation algorithm called hierarchical charge partitioning  and mapping it onto a graphics processing unit . while mainstream molecular modeling algorithms impose an artificial partitioning of biomolecules into a grid/lattice to map it onto a gpu, hcp is significantly different in that it takes advantage of the natural partitioning in biomolecules, which facilitates a data-parallel mapping onto the gpu.

we then presented our methodology for mapping and optimizing the performance of hcp on the gpu when applied to the calculation of esp. despite being a memory-bound application, we leveraged many known optimization techniques to accelerate performance. in addition, we demonstrated the effectiveness of the introduction of divergent branching on gpus when it reduces the number of instructional and memory transactions.

for a fairer comparison between the cpu and gpu, we optimized the cpu implementation by using hand-tuned sse intrinsics to handle the simd nature of the application on the cpu. we then demonstrated a  <dig> -fold reduction in the execution time of the application when compared to that of the hand-tuned sse implementation on the  <dig> cores of the cpu. furthermore, we ensured that the use of single-precision arithmetic on the gpu, combined with the hcp multi-scale approximation, did not significantly affect the accuracy of our results.

for future work, we will apply our hcp approximation algorithm to molecular dynamics  simulations on the gpu, given how well it performs in the case of molecular modeling. for md simulations, the use of double precision is mandatory as the error incurred in each time-step would accumulate over time, thereby immensely affecting the accuracy of the md results. in addition, we plan to exploit the use of the cache hierarchy on the nvidia fermi to accelerate the memory-bounded aspect of our application.

competing interests
the authors declare that they have no competing interests.

authors' contributions
md implemented and optimized the hcp approximation on the gpu. md also studied the impact of divergence and memory transactions on the gpu, collected all the required results and drafted the manuscript. wf conceived the study, co-designed the gpu mapping of hcp and helped draft the manuscript. both authors read and approved the final manuscript.

