BACKGROUND
real-time quantitative pcr  is increasingly being seen as a bench-marking analytical tool for many trace dna detection strategies, across a diverse range of areas encompassed within bioanalytical science  <cit>  high quality performance characteristics associated with this technique include throughput, reproducibility, specificity, and sensitivity. these characteristics in association with its wide applicability  <cit> , have meant that rt-qpcr is now being seen as a 'gold standard' for comparative purposes across a number of disciplines inclusive of regulation and legislation  <cit> 

because of the wide applicability of rt-qpcr, there is now a wealth of information pertaining to the analytical results derived from this molecular technique  <cit>  for this reason, it is imperative that the performance characteristics and uncertainty contributions from a particular application of rt-qpcr are known with high confidence. without these defined criteria, the method cannot be qualified as performing 'fit for purpose', and speculation may arise regarding the interpretation and assurance with which results are derived  <cit> .

for a given assay, measurement uncertainty estimation helps identify components of variability and make reasonable estimates of these components' effects upon the end result  <cit>  current quality regulations dictate that any result from an analytical laboratory should be given with an associated uncertainty estimate, and this is now included under the remit of iso  <dig>  <cit> . in relation to rt-qpcr, much of the current focus of measurement uncertainty estimation is based upon identifying factors associated with the experimental conditions of the analytical technique  <cit> . for example, uncertainty in rt-qpcr can arise from the use of laboratory equipment and reagent preparation associated with the initial dna extraction procedure.

additionally, recent studies have examined the underlying mathematical model associated with rt-qpcr, inclusive of the precision of replicate standard curves  <cit> . progressing from this, the use of a sigmoidal function to model fluorescence data as a more reliable alternative to using standard curves has been proposed  <cit> . aspects associated with the in-house validation of rt-qpcr measurements have also been examined  <cit> , and some of the pertinent factors that account for a lot of the variation in rt-qpcr measurements have been reviewed  <cit> .

a critical aspect that has not been examined in great detail is the area of data handling and interpretation. the vast volume of data being generated by rt-qpcr means that a large number of statistical tools and approaches can be applied to analyse the results  <cit> . however, if there are no guidelines or standardised approaches for this data handling and interpretation, then significant variation in the end result can also be attributed to this area. as one of the functions of measurement uncertainty estimation is to identify all potential factors that contribute towards the variation in the end result, then the area of data handling and interpretation is a fundamental aspect that should also be examined in detail.

previous studies have identified production of calibration curves, interpretation of data from duplex and singleplex reactions, and transformation of data, as areas that can contribute towards variation in the interpretation of rt-qpcr data  <cit> . the current study contributes further towards standardised methodologies that can be implemented when results are interpreted from trace detection situations. in the current paper, two additional aspects of data analysis are presented, which can potentially give rise to different interpretations of results from real-time pcr experiments if standardised guidelines are not adhered to in their implementation.

the first aspect concerns the identification and subsequent handling of outlying values. inclusion of outlying values in a data set is liable to give rise to erroneous interpretations. for many data sets arising from analytical procedures, it is often advantageous to display the distribution of the data set visually, to aid in the identification of potential outliers. there are also a number of statistical tools available that facilitate an objective test as to whether a data point should be classified as outlying  <cit> .

however, there are inherent difficulties associated with conducting analysis of data arising from rt-qpcr  <cit> . some of these problems originate from the artificially imposed end cycle number, which represents the total number of amplification cycles performed on the rt-qpcr platforms. the rather arbitrary assignment of this value coupled with the non-normal distribution of blank controls and data points that lie close to this value can make the identification of outlying values problematic. the establishment of guidelines on how to identify and handle outlying values given the regulatory and legislative dimension that rt-qpcr now occupies, is thus of fundamental importance in minimising potential measurement uncertainty.

this paper describes a simple visual approach as an initial step to identifying potential outlying values using a 'box and whisker' plot, and then suggests the use of a statistical test to objectively assess values to determine whether the data points are outliers. additionally, the implementation of iso guidelines is discussed in relation to acceptance or rejection of statistical outliers.

the second aspect relating to analysis of data arising from rt-qpcr concerns the comparison of calibration curves. calibration curves are produced based on measuring an instrument response according to a range of standards of known analyte concentration. for rt-qpcr studies, the dna content of an unknown sample is then estimated by translating the value of the measurand associated with a sample into its corresponding dna concentration based upon the equation relating to the calibration curve.

calibration curves arising from pcr are often compared to one another in order to judge if they are performing the same. one way of doing this is to examine the magnitude of the regression coefficient associated with the calibration curve produced through simple linear regression. this regression coefficient is equal to the gradient or slope of the line and is related to the efficiency of pcr amplification. the current state of the art in rt-qpcr for comparison of calibration curves is to do so visually in order to assess any difference between these regression coefficients  <cit> . this visual comparison is potentially very subjective, and there is a need for an objective statistical test that will indicate the likelihood of two regression coefficients being equal. such a test is described in detail in this paper so that it can be applied using basic functions found in standard spreadsheet software, and the implementation of its principles should further help minimise variation due to probable subjective judgements.

this paper further highlights original applications of statistical tools to the analysis of data arising from rt-qpcr techniques. approaches for identifying and handling outlying values, and techniques for comparing calibration curves, are examined. these aspects are discussed with a view to helping minimise potential measurement uncertainty arising from the area of rt-qpcr data analysis. the analysis of these aspects illustrates preliminary methods in which to standardise the reporting of results. to fully explore and model the optimal approach to data handling from rt-qpcr requires a collaborative effort between scientists and statisticians alike. it is hoped that the approaches outlined here will contribute towards the development of a set of best practice guidelines with which to help standardise handling and interpretation of data arising from rt-qpcr.

RESULTS
outlier testing
a preliminary method to identify potential outliers is presented here, based on initially displaying data sets as graphical box and whisker plots  <cit> . these graphs were produced using statistica  <dig>  software  but can be reproduced using other statistical software packages.

the rationale behind the box and whisker plot was as follows. the midpoint of a data set was calculated and represented by the median. a box drawn around this midpoint represented the inter-quartile range, which encompassed 50% of the range based on the 1st quartile  to the 3rd quartile . the whiskers outside of the box represented an additional selected range, encompassing 5% to 95% of the range of results. data points that lay substantially beyond the range of this box and whisker plot were identified as potential outliers. typically, potential outliers were identified when the value associated with a data point was larger than an upper limit of  <dig>  times the height of the box, or below the lower limit of  <dig>  time the height of the box.

a more formal definition is outlined below:

upper limit =  + )

lower limit =  - )

the default setting for the outlier coefficient is  <dig> .

if the value associated with a data point was above the upper limit, or below the lower limit, it is thus characterised as a potential outlier.

data set  <dig> was produced based on measuring cycle threshold  values associated with three samples x, y, and z, which consisted of  <dig> replicate observations per sample. the ct value indicated the cycle number where the target analyte signal crossed a pre-set threshold value during rt-qpcr. the graph in figure  <dig> shows the results from data set  <dig> that has been displayed according to a box and whisker plot, using the description outlined above where the whiskers encompass 5% to 95% of the range of results.

an outlier can be defined as a data point that does not follow the typical distribution of the rest of the data set and can be regarded as an irregular observation. example causes of outlying data points include the sample being atypical, the underlying distribution of the data set being non-normal in nature, operator error, a measurement mistake or transcription error, or purely due to chance variation. potential outliers can arise due to this latter chance variation, where the data point is correct in nature and is simply more divergent than the majority of the data set, or they can arise due to errors where the value of the data point is erroneous. an objective test is needed to calculate the probability that a single data point is different from the rest of the data set purely due to chance alone.

the use of the "box and whisker" plot is a useful diagnostic aid in achieving this, and a number of further statistical tools exist which can be used in conjunction to conduct this objective test  <cit> . the grubbs' test  <cit>  for identification of outlying values will be outlined here due to its ease of application, computational simplicity, and its recommendation as described in the international standard organisation document iso 5725- <dig>  <cit> .

alternative names for the grubbs' test are the maximum normalised residual test and the extreme studentized deviate.

the null hypothesis for the grubbs' test is that there are no outliers in the data set, whilst the alternative hypothesis is that there is at least one outlier present. the test statistic for the grubbs' test is computed from:

g = max sd
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaacqqghbwrcqqggaaicqqg9aqpcqqggaaicqqgtbqbcqqghbqycqqg4baecqqggaaidawcaaqaaiabbufabjabdmfaznaabaaaleaacqwgpbqaaeqaaogaeyoei0iafmywaklbaebacqggdbqxaeaacqqgzbwccqqgkbazaaaaaa@3f91@

where:

g = test statistic associated with grubb's test

yi = ith observation from data set / suspected outlier

y¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaacuwgzbqwgaqeaaaa@2e01@ = sample mean

sd = standard deviation of data set

the test statistic can be interpreted as the largest absolute deviation from the sample mean in terms of the sample standard deviation.

it is possible to calculate the probability associated with the test statistic using formulae, but it is more common to determine the critical values associated with the test statistic using tables available in statistical publications  <cit> . we have used tables to determine the probability associated with the test statistic, so that the approach for identifying outliers can be applied using standard spreadsheet software rather than specialist statistical software.

based on iso  <dig> guidelines, outlying data points can be characterised according to the probability that their associated values can arise due to chance alone. those values, which lie between 95% and 99% of the expected range of the characterised distribution, are termed stragglers , and those values, which lie beyond 99% of the range of the characterised distribution, are termed outliers .

from the box and whiskers plot illustrated in figure  <dig>  potential outliers were observed in all three samples of x, y and z. the most extreme potential outlier was observed in sample z and the least extreme potential outlier in sample x. the grubbs' test was applied to each of the samples as an objective assessment as to whether these data points should be classified as outliers or stragglers according to iso guidlines. the results of the grubbs' test are shown in table  <dig> 

for sample x, the test statistic associated with the potential outlier was  <dig>  based on  <dig> data points. according to statistical tables  <cit>  in order for the extreme value to be a straggler  the test statistics must be  <dig>  or higher for  <dig> observations. this implied that the extreme value was not inherently different from the majority of values for sample x, and was within the 95% confidence interval of the mean of that data set. thus the extreme value was included for subsequent analysis, as it was not considered inherently different from the rest of the data set.

for sample y, the potential outlying value has a response value  of  <dig> , which appeared distinct from the rest of the range of the distribution for sample y. the grubbs' test statistic for this value was  <dig> , which indicated that it lay between 95% and 99% of the normal range of the sample y based on statistical tables. according to iso guidelines this should be considered as a straggler.

the extreme value in sample z had an associated ct value of  <dig> . the grubbs' test statistic associated with this data point was  <dig>  that indicated that the value lay beyond 99% of the range of sample z, given a normal distribution. this value was thus considered as an outlier according to iso guidelines.

iso  <dig> provides recommendations regarding the handling of potential outliers  <cit> . it suggests retention of stragglers in a data set unless there is a technical reason not to do so, based on the rationale that at the 95% level of confidence, there is a reasonable probability  that the straggler could arise from the data set purely due to chance alone. however, for data points classified as outliers, iso guidelines recommend rejection of the value before the subsequent data analysis, unless sufficient justification is given to retain it. this is based on the premise that there is an unacceptably high chance that the value does not belong to the rest of the data set.

retention of statistical outliers in a data set can cause the mean value to be changed slightly whilst the confidence interval can be greatly increased. for example, when the single outlier has been removed from sample z , the mean and standard deviation associated with the data set are  <dig>  and  <dig>  respectively. however, inclusion of the single outlier gives a mean of  <dig>  and a standard deviation of  <dig> . this effect is particularly important not only if the sample is being used to construct a calibration curve, but also if that sample is being analysed in order to estimate an unknown analyte level associated with it.

there are potential difficulties associated with conducting the analysis of data arising from rt-qpcr. many of these difficulties arise from the artificially imposed end cycle number. this end cycle number represents the maximum number of cycles that the rt-qpcr platform is instructed to perform for a given assay. results arising from rt-qpcr platforms are typically expressed as cycle threshold values . because of the nature of rt-qpcr, blank control samples consisting of no template controls do not give a zero ct value, but instead give a response value that is equal to the end cycle number. samples that fail to amplify, or ones that are true blanks, will have a mean equal to the end cycle number with a standard deviation of zero. potentially this can cause problems when visually inspecting a data set for outlying values, as data points near the end cycle number may be wrongly rejected when they represent true values. this is of particular importance when estimating limits of detection and quantification associated with an assay where the confidence associated with a reported result may be in question.

aside from the statistical test outlined here, there are a number of approaches that can be implemented in order to accommodate or reject any data points that could be considered outliers. for example, one approach is to use the unadulterated data in order to assess the performance of different pcr platforms. this is based on the premise that if the same quality regulations are adhered to in the operation of each platform, the number of erroneous data points in each data set will be indicative of the overall performance of that platform. a further approach is to artificially impose a maximum ct value, above which any values will be rejected based on the risk of being outlying values, resulting from non-specific amplification, for example. dependent upon the capabilities of the real-time pcr platform, this approach has the potential to incorrectly omit values that validly add information to the analysis. an additional approach includes the use of experience to identify potential outliers, which can be extremely subjective and variable in nature, but is arguably a method of practical use. other approaches exist to improve the quality of data arising from rt-qpcr, many of these being subjective in nature or exhibiting a potentially high chance of incorrectly rejecting data points that would add value to the analysis. the statistical procedure and approach outlined above provides a basis for more objective determination of when to accept or reject potential outlying data points.

limitations of approach and comparison to other methods
a limitation of the grubbs' test is that the test assumes that the data set follows an approximately normal distribution. the user should first test their data set for significant departure from normality before proceeding. tests such as the shapiro-wilk w test  <cit>  and lilliefors test  <cit>  can be used to test for normality. transformation of the raw data values into a mathematical derivative, for example through using logarithms, may help normalise the data set and make it amenable for further parametric statistics  <cit> . outlier tests for non-normally distributed data can be applied, but the power of these tests is relatively poor, and they are more difficult to apply.

the particular form of the grubbs' test illustrated in this paper detects one statistical outlier at a time. alternative tests for single outliers exist, for example dixon's q test  <cit> , although the grubbs' test is usually considered to be more robust, and the grubbs' test is recommended as an applicable outlier test according to the international standard organisation guidelines iso 5725- <dig>  <cit> .

the grubbs test is only valid for the detection of two or less outliers in a data set. derivations of the grubb's test also exist for detecting pairs of outlying values, but a discussion of these is beyond the remit of the current article. it is possible to use the grubbs' test iteratively on the remainder of the data set in order to identify all potential outliers. however, such multiplicity of testing decreases the power of the grubbs' test and this use of repeated significance tests in a single study can thus increase the probability of obtaining false positive results. statistical techniques for handling the iterative elimination of outliers can be used, and for identification of multiple outliers within a data set that consists of  <dig> or more values that follow a normal distribution, it is recommended that further outlier tests such as the rosner's test be considered  <cit> .

comparing regression coefficients
for rt-qpcr, the majority of calibration curves are fitted using a simple linear regression model, although alternative models are available  <cit> . this simple linear regression model uses the method of least squares, which establishes the best fitting straight line based on minimising the residual variance between the predicted model and the observed data points. the resulting linear regression equation is often displayed in the form of y = bx + c, where y is the dependent variable, x is the independent variable, b is the gradient of the line, and c is the intercept of the line on the y-axis. comparisons between different calibration curves are usually conducted based upon using the regression coefficient. this regression coefficient  represents the gradient, or slope of the line, associated with the calibration curve, and is also related to the amplification efficiency of the real-time pcr reaction. for many rt-qpcr applications, an assumption is made that the pcr efficiency of the standards used to construct the calibration curve is the same as the pcr efficiency associated with the samples under evaluation. thus, comparison of regression coefficients is an important quality step. this regression coefficient corresponds to the gradient of the rate of change of the dependent variable  per unit change in the independent variable . the most common way of comparing calibration curves together for real-time pcr is to inspect the gradient of the two regression lines visually on the same or separate graphs  <cit> . however, this only provides a subjective assessment of the differences between the two lines.

outlined below is an objective statistical test that calculates the probability that any differences observed between two regression coefficients are due to chance alone.

this test is a simplified derivative of an analysis of covariance. the analysis of covariance is recommended as the best choice for comparing regression lines  <cit> , and a review and explanation of the full procedure are given in  <cit> . the test is explained in detail using data sets  <dig> and  <dig> so that the computations involved in the worked example can be readily implemented using only basic spreadsheet software commonly available to the bio-analytical community.

the validity of this test is based upon the following assumptions:

• simple linear regression is used to produce the calibration curves

• each of the two data sets have the same values for the independent variable x

• variances between the data sets are statistically equal

data sets  <dig> and  <dig> are described in table  <dig>  these were analysed by simple linear regression as shown in table  <dig> in order to produce two calibration curves . this resulted in two estimates of the regression coefficients  associated with each of the calibration curves for data set  <dig> and  <dig> respectively. from this analysis:

b <dig> = - <dig> 

b <dig> = - <dig> 

these two regression coefficient estimates are similar, but the magnitude of the regression coefficient b <dig> is slightly larger than b <dig>  figure  <dig> also illustrates that the two regression lines appear to converge at low copy number values, whilst they diverge at higher copy number values.

the objective test for differences between the two regression coefficients uses the calculation of a term called the heterogeneity of regression coefficients. this heterogeneity of regression coefficients tests the null hypothesis that b <dig> and b <dig> are estimates of the same gradient. this can be calculated using the following formulae:

heterogeneity of regression coefficients ss = ∑i=1n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaaawcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildaaaa@33a6@  –  where 'ss' is equal to the sum of squares.

the ∑i=1n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaaawcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildaaaa@33a6@  is equal to the sum of squares associated with the regression item, where the sigma symbol is used to indicate that this is summed across both data sets  <dig> and  <dig>  these values relate to the regression items from the analysis of variance tables which are produced in the original regression analysis conducted on data sets  <dig> and  <dig>  as illustrated in table  <dig>  the regression ss characterises the component of the variation in the dependent variable  that is accountable by the independent variable .

from table  <dig>  the regression ss for data set  <dig> was  <dig> , and the regression ss for data set  <dig> was  <dig> . therefore the ∑i=1n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaaawcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildaaaa@33a6@  was equal to  <dig> .

the joint regression ss uses the sum of the products  and is calculated as:

joint regression ss = 2∑i=1nss
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadawcaaqaamaabmaabawaaabcaeaacqwgtbwucqwgqbaudawgaawcbagaei4waslaemieagnaeiilawiaemyeaknaeiyxa0fabeaaaeaacqwgpbqacqgh9aqpcqaixaqmaeaacqwgubgba0gaeyyeiuoaaogaayjkaiaawmcaamaacaaaleqabagaegomaidaaagcbawaaabcaeaacqwgtbwucqwgtbwudawgaawcbagaei4waslaemieagnaeiyxa0fabeaaaeaacqwgpbqacqgh9aqpcqaixaqmaeaacqwgubgba0gaeyyeiuoaaaaaaa@4caf@

where ∑i=1n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaaawcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildaaaa@33a6@ sp is the sum of the products for x and y summed across both data sets, and ∑i=1n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaaawcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildaaaa@33a6@ ss is the sum of squares of x summed across both data sets. within a data set, these terms are further defined as follows:

sp=∑i=1n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaacqwgtbwucqwgqbaudawgaawcbawaamwaaeaacqwg4baecqggsaalcqwg5bqeaiaawufacagldbaaaeqaaogaeyypa0zaaabcaeaadaqadaqaaiabdiha4naabaaaleaacqwgpbqaaeqaaogaeyoei0iafmieagnbaebaaiaawicacaglpaaaasqaaiabdmgapjabg2da9iabigdaxaqaaiabd6gaubqdcqghris5aowaaewaaeaacqwg5bqedawgaawcbagaemyaakgabeaakiabgkhitiqbdmha5zaaraaacagloagaayzkaaaaaa@4b30@

and

ss=∑i=1n2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaacqwgtbwucqwgtbwudawgaawcbawaamwaaeaacqwg4baeaiaawufacagldbaaaeqaaogaeyypa0zaaabcaeaadaqadaqaaiabdiha4naabaaaleaacqwgpbqaaeqaaogaeyoei0iafmieagnbaebaaiaawicacaglpaaaasqaaiabdmgapjabg2da9iabigdaxaqaaiabd6gaubqdcqghris5aowaawbaasqabeaacwagacaakdaiyagmaaaaaa@44d1@

using this formulae the sp for data set  <dig> is calculated as - <dig> , and the sp for data set  <dig> is calculated as - <dig> . hence:

∑i=1n2= <dig> 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaamaabmaabagaem4uamlaemiuaa1aasbaasqaamaadmaabagaemieagnaeiilawiaemyeakhacaglbbgaayzxaaaabeaaaogaayjkaiaawmcaaawcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildgcdaahaawcbeqaaiabikdayaaakiabg2da9iabiwda1iabieda3iabicdawiabigdaxiabc6cauiabiodaziabiida4iabimda5aaa@4764@

the denominator for the joint regression sum of squares is the summation of the sum of squares for data sets  <dig> and  <dig>  as both data sets use the same values associated with the independent variable x, then the sum of squares of x for both data sets is the same. hence:

∑i=1nss=2∑2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaaiabdofatjabdofatnaabaaaleaadawadaqaaiabdiha4bgaay5waiaaw2faaaqabaaabagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildgccqgh9aqpcqaiyagmdaaeabqaamaabmaabagaemieag3aasbaasqaaiabdmgapbqabagccqghsislcuwg4baegaqeaagaayjkaiaawmcaaawcbeqab0gaeyyeiuoakmaacaaaleqabagaegomaidaaaaa@45e0@

the sum of squares of x for data set  <dig> is calculated as  <dig> , thus:

∑i=1nss= <dig> 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaaiabdofatjabdofatnaabaaaleaadawadaqaaiabdiha4bgaay5waiaaw2faaaqabaaabagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildgccqgh9aqpcqaixaqmcqaizawmcqgguaglcqai5aqocqai3awncqaiwaamcqaixaqmaaa@4142@

for the joint regression ss, both the numerator and denominator have been evaluated, which gives a value of  <dig> .

values for both items in the heterogeneity of regression coefficients have now been calculated. thus:

heterogeneity of regression coefficients =  <dig>  –  <dig>  =  <dig> .

to test if b <dig> and b <dig> are significantly different from one another, the heterogeneity of regression coefficients ss is divided by ∑i=1n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaaawcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildaaaa@33a6@ residual ss

where:

∑i=1n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaaawcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildaaaa@33a6@ residual ss = residual ss + residual ss

the ∑i=1n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaaawcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildaaaa@33a6@ residual ss is equal to the sum of the residual sum of squares from data sets  <dig> and  <dig>  these are listed in the analysis of variance tables associated with the original two regression analyses .

∑i=1n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaaawcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildaaaa@33a6@ residual ss =  <dig>  + <dig>  =  <dig> 

the degrees of freedom for each individual residual ss are  which equals  <dig>  where n is the number of points plotted on the calibration curve. thus, where n is the same between the two regressions, as in this worked example, the degrees of freedom relating to the ∑i=1n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaadaaewbqaaawcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabgghildaaaa@33a6@ residual ss is calculated as * <dig>  which equals  <dig>  this residual ss characterises that proportion of the variation in y that is not dependent upon x for both regression models. thus, the residual ss is a measure of the amount of variation left over in the experiment, which is not accounted for by both models.

an f-variance ratio test can then be used to test if the variance associated with the heterogeneity of regression coefficients ss is significantly greater than the variance associated with the residual ss, as shown in table  <dig> 

for this test, the null hypothesis  assumes that there is no significant difference between the two regression coefficients. the p value then represents the probability that the difference between the two regression estimates is purely due to chance. adopting common statistical probability threshold values, if the p value is below 5%  the null hypothesis is rejected. if p <  <dig>  the alternative hypothesis is accepted, and the difference between the two regression estimates is real and the estimates are significantly different. in the example illustrated above, the p value was equal to  <dig> , which indicated that the regression coefficients associated with the calibration curves produced by the two rt-qpcr platforms were not significantly different from one another.

this objective comparison between two calibration curves is useful as the statistical test takes into account the majority of those variables inherent in normal regression analysis. for example, the test uses the regression sum of squares associated with both calibration curves, which represents that part of the variation in the dependent variable  that is accountable for by the independent variable . thus, the regression sum of squares is a measure of how well the linear regression model fits the experimental data. in addition, the test takes into account the residual sum of squares associated with the calibration curves, which is an estimate of the amount of variation remaining in the experimental data that is not explained by the linear regression model. because the number of degrees of freedom associated with such items as the heterogeneity of regression coefficients sum of squares and the joint remainder sum of squares can be calculated, variance estimates can be made. these estimates then facilitate an f variance ratio test that can be used to predict the probability that differences arise between the two regression coefficients due to chance alone.

based on conventional rt-qpcr theory  <cit> , the regression coefficient and the pcr amplification efficiency are related according to the following equation:

b=−
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqacigacagaaeqabaqabegadaaakeaacqqgibgycqgh9aqpcqghsisldaqadaqaamaalaaabagaeeymaedabagaeeibawmaee4ba8maee4zacmaeeyraueaaagaayjkaiaawmcaaaaa@3798@

where e = efficiency of pcr reaction. thus any differences observed between the regression coefficients associated with the two calibration curves will also be related to the two pcr amplification efficiencies associated with the data sets. small changes in both e, and hence b can influence results substantially, hence it is important that b and e are compared in some way. whilst this aspect has been recognised for some time, no advice has been given regarding a statistical or objective test on how to achieve this for rt-qpcr studies. the test described in this paper shows one potential way of meeting this requirement.

the above technique may be usefully applied in further situations to objectively compare the gradients associated with two calibration curves. for example, it would be applicable to use the statistical approach in relation to historical data sets. a calibration curve based on a current data set may be compared to a calibration curve conducted six months previously. this may be conducted in order to qualify if a reference material is behaving comparably between the two time points. any significant differences between the two calibration curves may be due to stability or other issues, and are worth investigating further before using the reference material for any additional analytical work. furthermore, the technique for comparing regression coefficients can be implemented when examining two calibration curves that have been produced using methods that are similar except for one factor. for example, the statistical technique can be applied when standard curves are produced from the same analytical standards but use different real time pcr platforms. in this example, the scientist may wish to compare the calibration curves between the platforms in order to determine if they are essentially giving the same estimates. additionally, the technique can be applied when the sources of the standards used to produce the calibration curve are different. for example, calibration curves on the same real-time pcr platform may be produced using plasmid or genomic dna which have been quantified and diluted to the same concentration. the analyst may then want to objectively compare the gradient associated with each of the calibration curves in order to determine statistically if they are operating to the same efficiency. more mundane, but no less important applications of the technique would involve examining the relationship between two calibration curves based on replicate runs of the same experiment in order to determine if significant differences occur.

limitations of approach and comparability with other methods
the test described above has been explained in detail, to facilitate its implementation using only the basic functions found in standard spreadsheet software. similar results can also be achieved if more specialised statistical software is available to the analyst. however, knowledge of statistical expressions, and familiarity regarding terminology that may be specific to the software package is often assumed.

statistical tests for the difference between regression coefficients can be implemented using an analysis of covariance approach through a general linear model.  <cit> . the specific details concerning the theory behind this model can be found in  <cit> , but a brief description of the application of the analysis of covariance is given here. the general linear model can be used to test three hypotheses: that the two gradients are significantly different from zero; that the gradients of the two lines are the same ; and the hypothesis of equality of intercepts . using statistical software this can be achieved by specifying the ct value as the response variable and the log  as the covariate. in addition, the model used to make the comparison is specified as the interaction between the two variables. using data sets  <dig> and  <dig>  an analysis of covariance results table is shown in table  <dig> 

the null hypothesis associated with the 'intercept' item is that there are no significant differences between the intercepts associated with the two regression lines. as the p value is not significant, it is accepted that any differences observed between the intercepts associated with both lines was purely due to chance alone.

with reference to the 'slope' item, the null hypotheses states that the gradients are not significantly different from zero. as the associated p value indicates the test is statistically significant, it is accepted that the gradients associated with the two regression lines was different from zero.

finally, the null hypothesis associated with the 'heterogeneity of regression coefficients' item is the same as the original test described above. this specifies that there are no significant differences between the two estimates of the regression coefficients. the p value associated with this item is non-significant, therefore the null hypothesis is accepted that there are no differences between the two gradients. the degrees of freedom, sum of squares, mean square estimates, f and p values associated with the basic test described above, and the analysis of covariance using statistical software are exactly the same.

the test for comparing regression coefficients, as described in this paper, is applicable for two calibration curves. the technique can be further extended to three or more calibration curves by computing additional statistics inclusive of the sum of squares and sum of products for the additional data sets, and taking into account the joint remainder sum of squares associated with all data sets. additionally, the technique is potentially applicable to data sets which have been measured at different intervals and points for the independent x variable. however, care must be taken in this respect, as the interpretation of the results may be further complicated as comparisons are not independent of each other. such non-orthogonal comparisons can render statistical conclusions invalid. due to the complexity involved in these further calculations, the use of specific statistical software in order to conduct the analysis of covariance using the general linear model is suggested. additionally, because of the multiplicity of testing between pairs of regression lines, the chance of error increases so a more stringent significance level should be adopted. these further extensions to the technique are discussed in  <cit> .

CONCLUSIONS
rt-qpcr has become established as a benchmark in many areas for trace detection of dna, and is increasingly being used in legislative and regulatory enforcement. because of the increased focus on the technique, it is of paramount importance that all factors that can contribute to significant measurement uncertainty in the end result of the assay are identified. currently, most of this evaluation of measurement uncertainty has concentrated on the experimental work associated with the rt-qpcr process, and little work has been done to investigate the uncertainty at the data handling and processing stage. previous work  <cit>  identified several aspects associated with the data handling stage that can cause variation in the end result unless standard guidelines are adopted for their implementation. the current paper highlights two additional aspects of data handling that can potentially contribute towards significant measurement uncertainty in the final outcome.

the first aspect involved the identification of potential outlying values associated with a data set, followed by their correct characterisation via an objective statistical test and recommended acceptance or rejection criteria according to iso guidelines. the second aspect involved an objective test to calculate the probability that two calibration curves were statistically different, enabling the use of a more quantitative approach to comparing regression coefficients than have been previously reported.

the approaches described in this paper illustrate preliminary methods with which to standardise the reporting of results. as is inherent with the application of any statistical tool, these approaches have limitations but they serve to emphasise additional areas where more work needs to be conducted in order to standardise the data handling aspects associated with rt-qpcr assays. additionally, the approaches are not unique, but their application to the area of real-time pcr is novel.

the focus of the current study is the implementation of statistical techniques in order to examine some factors that can account for variability in results associated with rt-qpcr measurements. this statistical analysis of data helps contribute towards a greater understanding of some of the areas of uncertainty involved in rt-qpcr.

additional studies have detailed the mathematics associated with rt-qpcr, involving discussions on the use of cycle threshold values and the precision associated with replicate standard curves  <cit> . whilst linear standard curves are often used as calibration curves, a recent study has suggested an approach utilising a sigmoidal function to model fluorescence data as an alternative, in order to increase the reliability of rt-qpcr measurements  <cit> . a further study provides an in-depth review regarding some of the factors that can account for significant variability in rt-qpcr measurements including template, operator, data analysis and subsequent reporting of results  <cit> . thus, in any approach to aid in standardisation of rt-qpcr measurements, it is the summation of many studies inclusive of statistics, mathematical modelling, technical and practical approaches that cumulatively provide a better understanding of all components that can add significant uncertainty to a result.

to fully realise and explore the harmonisation of data handling regarding rt-qpcr assays requires collaborative efforts between scientists that routinely conduct the assays and can define the problem, and statisticians that can suggest the optimal approach. it is hoped that by describing these two aspects in detail in this paper, it can be seen that further variation at the data handling stage can contribute to uncertainty in the end result. the procedures outlined in this paper thus have the potential to contribute towards providing a set of standardised guidelines for the handling and processing of data arising from rt-qpcr methods, to enable a more systematic and controlled approach to interpretation of the end result.

