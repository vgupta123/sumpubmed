BACKGROUND
advances in automation and miniaturization of experiments and the development of reliable biological assays have made high-throughput screening  a vital step in the drug discovery process  <cit> . hts involves the testing of a large number of candidate molecules on a biological target to identify potential drug molecules. detection technologies such as the scintillation proximity assay , fluorescence polarization  and fluorescence resonance energy transfer  are used to identify the target-binding activity of the compounds  <cit> . screening more than  <dig>  compounds a day on a biological target has become routine, but largely through increases in assay automation via liquid handling robots and assay parallelization  <cit> . in these assays, the most common practice is to individually test each molecule against a standardized target. usually, only a small fraction of a compound library shows activity, while the majority of the compounds show no activity. unfortunately, because each compound is tested only once, the presence of experimental errors  requires substaintal efforts to validate most hts results  <cit> . due to the large size of these chemical libraries  replicate screening is prohibitive. therefore an arbitrary number of active compounds  are usually chosen for secondary screening to identify inactive compounds that erroneously passed through the primary screen  <cit> . disappointingly, active compounds that were missed due to false negative results in the primary assay cannot be identified from this approach and are therefore lost.

one method to reduce the sensitivity of an assay to false postitive and false negative results is pooling. in a pooling design, each compound is tested multiple times in combination with other compounds. since very few compounds in a library are active in the assay, pooling effectively provides internal replicate measurements to confirm compound activity. however, constructing efficient pooling designs is difficult, as one would ideally like to guarentee correct identification of a known number of active compounds while correcting for random experimental errors. the general problem of pooling designs has been well studied and is described elsewhere  <cit> .

in  <dig>  a novel pooling design method called shifted transversal design  was introduced for biological assay design  <cit> . std is based on the dual objectives of  minimizing the number of times any two compounds appear together in a test and  maintaining the pool sizes roughly equal. when compared to other pooling designs, std provides similar or better performance in nearly every area. in this paper, we introduce a pooling strategy called poolhits, based on the std algorithm, specific for the purposes of drug screening. first, we prove a limit to the error-correcting capacity of std-based pooling strategies. this limit is important for drug screening as it dictates the error limit in an assay that can be used with a pooling design of this kind. second, we modifiy the pooling design algorithm to limit the number of drugs tested in each assay, thereby enforcing a realistic experimental constraint of hts. third, we introduce a block design method that both simplifies and improves the assay design.

RESULTS
preliminaries – std-based pooling strategy
a std-based pool construction starts with the specification of the compound library size , maximum number of active compounds expected  and maximum number of errors expected . the std algorithm guarantees that the pooled design will be able to correctly identify upto d active compounds in the presence of upto e false positive and negative errors in the screen. std is able to provide such guarantees because it uses a combinatorial procedure to ensure that no two compounds are pooled together more than a minimum number of times, to prevent confounding decoding results. also, the number of compounds pooled in each test is roughly the same, ensuring correct intensity-concentration mapping for the test results. this implies that for any underlying structure of compound activities and testing errors, as long as their numbers lie within the specified experimental parameters, the std design guarantees successful identification of the active compounds. it has been observed  <cit>  and recently shown  <cit>  that pooling designs are capable of correcting errors much larger than those that they guarantee for. these input parameters  are used to choose the design parameters of the std construction algorithm q and k. std is a layered construction with k layers, each of size q × n. each compounds appears only once in each layer. the std construction algorithm produces a t × n, 0– <dig> matrix m = std, with t  rows that are the assays to be performed and n columns that represent the compounds in the library. the columns with entry  <dig> in a row are the compounds to be pooled in that assay. an example of such a pooling design is shown in figure  <dig>  created using m = std for n =  <dig>  d =  <dig> and e =  <dig>  the process of mapping the experimental parameters to the design parameters is shown below in algorithm  <dig> 

algorithm 1:

inputs – n, d and e

 <dig>  choose a prime number q, with q <n. start with the smallest prime,  <dig> 

 <dig>  find the compression power, Γ = min{γ|qγ+ <dig> ≥ n}, therefore Γ=⌈log⁡nlog⁡q⌉− <dig>  set k = dΓ + 2e +  <dig> 

 <dig>  check if this choice of q and k satisfy the guarantee requirements of identifying d active compounds and correcting e errors, using the inequality, k ≤ q +  <dig> 

 <dig>  if the inequality is satisfied continue to step  <dig>  else choose the next prime in step  <dig> and repeat steps  <dig> and  <dig> 

 <dig>  once the smallest prime number qmin and its corresponding compression power Γmax are found, all q > qmin will satisfy the inequality in step  <dig>  therefore, cycle through the values of Γ in { <dig>  ..., Γmax} to find the corresponding q. for each Γ find the smallest q that satisfies, q ≥ n1/Γ + <dig> 

 <dig>  calculate the number of tests  needed by each q and k pair from, t = q × k.

 <dig>  choose the q and k pair producing the least number of tests.

 <dig>  design the pooling matrix, m = std.

a detailed description of the procedure used to construct the t × n, 0– <dig> matrix m can be found in the original paper  <cit>  and is reproduced in the methods section  of this paper.

having designed the pooling scheme, m, the decoding algorithm is given below. the pooled assays, t in number, are carried out and the results classified into two states – positive or negative, using a chosen threshold.

 <dig>  a compound present in at least e +  <dig> negative tests is tagged inactive.

 <dig>  a compound present in at least e +  <dig> positive tests, in which all other compounds have been tagged inactive, is tagged active.

note that each compound is present in dΓ + 2e +  <dig> tests and no two compounds are mixed together more than Γ times. a more elaborate decoding algorithm designed to handle the presence of larger-than-designed-for values of d and e is provided in the original paper and its matlab implementation is provided .

error rate
the std-based pooling strategy creates a design for a specified number of errors, e, resulting in the addition of 2e extra tests to the total of t pooled tests. assuming random experimental errors, the number of errors present in the result will increase as the number of tests increase. in hts, there can be instrument, biological, chemical, or human errors that increase  or decrease  the measurement from its true value  <cit> . these multiple sources of error make it difficult to estimate the total number of errors, e, before knowing the number of pooled tests  that will be used. however, often in hts we know the overall random error-rate for an assay, which we call e. for example, the pooling design for  <dig> compounds  expecting  <dig> active compounds  and  <dig> testing errors  needs  <dig> tests  using std . if we define the error-rate  as the percentage errors expected per test then e=et× <dig>  for the example above, this error-rate e is ~ <dig> %. however, when the number of errors  is changed, keeping everything else the same, the corresponding number of pooled tests  and hence the error-rate  change. from figure  <dig> it can be seen, that the error-rate  reaches a maximum and then drops off with respect to e. the std construction is efficient for low values of e but requires many more tests  for higher e values, as seen in figure  <dig>  this nonlinear relationship between t and e indicates that a construction based on the input parameter e alone is not satisfactory. we propose the use of an expected error-rate e as a input parameter instead. to do so we have modified the std strategy as follows.

algorithm 2: error-rate based std strategy

inputs – n, d and e

steps  <dig> and  <dig> same as 

 <dig>  if dΓ ≤ q, calculate emax⁡=⌊q−dΓ2⌋, the maximum number of errors that can be corrected by this choice of q, for the given n and d.

 <dig>  check if the input error-rate e is achievable using the inequality, e≤emax⁡q× <dig>  if e is achievable then continue to step  <dig>  else go back to step  <dig> and try the next q.

 <dig>  cycle through all values for e from  <dig> to emax to find the minimum e  that satisfies the inequality in step  <dig>  use emin to calculate, k = dΓ + 2emin +  <dig> and the number of tests, t = qk.

 <dig>  similar to algorithm  <dig>  cycle through the values of Γ smaller than the Γmax to find the corresponding q and hence t.

 <dig>  use the design parameters  that need the minimum number of tests.

 <dig>  construct the pooling design, m = std, as usual.

¿from step  <dig> we find that there is an upper limit to the error-rate  for a given library size  and chosen maximum number of active compounds expected . this limit is a function only of the design parameter q as shown in equation  <dig> .

  e≤⌊q−dΓ2⌋q+2⌊q−dΓ2⌋+1)× <dig> Γ=⌈log⁡nlog⁡q⌉− <dig> 

for the simplest case of d =  <dig>  this expression simplifies to e≤q−12q2× <dig>  for all the odd prime numbers and e ≤  <dig> % for q =  <dig>  thus any assay with an error rate greater than  <dig> % cannot guarantee accurate results when screened using an std-based scheme. in more realistic cases, the error-rates corrected by std-based schemes are much smaller than this limiting case of d =  <dig> because d and Γ would have substantial values. this finding implies that no matter how large the specified number of errors  in the original std-based pooling strategy, the corresponding number of tests needed  would adjust itself to keep the error-rate  corrected by the design low. an example of the effect of equation  <dig> can be seen in figure  <dig> for a library of  <dig> compounds  and various values of expected active compounds .

mixing constraint
to our knowledge, no pooling designs address the physical limitation that only a finite number of compounds can be mixed in each well. this limitation arises in drug screening for the following three reasons.

 <dig>  each compound must be present at a sufficiently high concentration so as to be detectable by the assay within a physiologically reasonable range.

 <dig>  the total ionic strength of the test solution must be low enough to prevent precipitation of compounds or possible changes to the biological target.

 <dig>  the assay must be reasonably simple to physically construct.

from these constraints we can conclude that the total number of drugs that can be practically included in any experimental test well is relatively small. for example, for a relevant screening concentration of ~ <dig> μm, we can assume that each well can have ~ <dig> compounds mixed in it. if more drugs are mixed, the cost of creating the assay increases and the ionic strength of the well mixture may become too high, resulting in inaccurate screening results. this limit can be specified as an input parameter to the pooling design, based on the high-throughput assay being implemented.

here we extend the std-based pooling strategy to introduce an explicit mixing constraint. let m be the mixing constraint, defined as the maximum number of compounds mixed in a well of the pooling scheme. normally the std design mixes at most ⌈nq⌉ compounds in each well  <cit> . however, this feature can break down when the input values of n, d and e  are such that, for a certain choice of q and k , k = q +  <dig> and ⌈n−1qΓ⌉ <q -  <dig>  resulting in an unusually large number of compounds mixed in some tests and some tests with no compounds in them. the original std construction can be easily modified by removing tests with no compounds in them. the details of the correction are provided in proof  <dig> of the methods section. having made this correction, we can implement the mixing constraint as follows .

algorithm 3: mixing constraint implementation

inputs – n, d, e and m

steps  <dig> through  <dig> are the same as .

 <dig>  using the q and k values obtained so far in the algorithm to choose one of the following options.

a. if k <q +  <dig> and ⌈nq⌉ ≤ m, use construction  <dig>  requiring t = qk tests.

b. if k = q +  <dig>  n = qΓ+ <dig> -  <dig> and qΓ ≤ m, use construction  <dig> requiring t = q tests.

c. if k = q +  <dig>  ⌈n−1qΓ⌉ <q -  <dig> and qΓ ≤ m, use construction  <dig>  requiring t = q <dig> + ⌈n−1qΓ⌉ +  <dig> tests.

 <dig>  similar to the previous algorithms, cycle through the values of Γ smaller than Γmax to find the corresponding q and hence t.

 <dig> use the design parameters  that require the minimum number of tests.

repeated blocks
limiting the number of compounds that can be mixed in a pooling strategy reduces the savings in tests that could otherwise have been obtained. for example, without the mixing constraint, screening a library of  <dig>  compounds with  <dig> expected actives and no error requires only  <dig> tests, using std, each mixing  <dig>  compounds in them. however, a mixing constraint of  <dig> compounds per test increases the required tests to  <dig>  using std. now, consider the std-based strategy for  <dig> compounds while expecting only  <dig> active compound in a similar error-free setting and a mixing constraint of  <dig> compounds per well. the number of tests needed in this case is  <dig>  using the std design. if we now divide the original  <dig>   <dig> compound library into  <dig> blocks of  <dig> compounds each and used the std design repeated on those  <dig> blocks, the total number of tests is only  <dig> ×  <dig> =  <dig>  almost half the original requirement! the trade-off here is that the block assay design guarantees the detection of only  <dig> active compound out of every  <dig> compounds tested. however, given that we expected only  <dig> active compounds in a library of  <dig>   <dig> compounds, this implies that there is ~ <dig> % chance of finding, at best,  <dig> active compound among the  <dig> randomly selected compounds for a block . this block decomposition algorithm is inspired by a form of pooling used in nmr screening  <cit> . this example demonstrates that we can make an informed choice about a block size that can, not only, help reduce the number of tests needed while enforcing the mixing constraint but the smaller block size also implies that a better error-rate can be handled by the design .

for any block of nb compounds from the library of n compounds, the number of blocks needed to cover the whole library is b=⌈nnb⌉. the creation of a std for this block of nb compounds, using the error-rate e and mixing constraint m  requires the specification of the maximum number of active compounds expected for the given nb, called db. the choice of db is from a hypergeometric distribution such that, with at least a probability of pb  the block of nb compounds contains at most db active compounds. using the hypergeometric distribution, the following inequality  can be solved for db.

  ∑i=0db≥pb 

based on this choice of db, a std-based pooling design of size tb × nb can be generated using algorithm  <dig>  the number of tests needed for the whole library would be b × tb.

poolhits
we now define poolhits as an std-based pooling strategy which takes in as input the compound library size , maximum number of active compounds expected , maximum error-rate expected , mixing constraint , and design confidence metric . poolhits produces a t × n mixing matrix m, which guarantees the success of the pooling scheme for the given input parameters. the algorithm for poolhits, which includes error-rate specification, mixing constraints, and optimal block size selection, is as follows. the matlab implementation of the poolhits algorithm is provided .

algorithm 4: poolhits algorithm

inputs – n, d, e, m and pb

 <dig>  choose a value of db in { <dig>  ...,d - 1}.

 <dig>  find the set of nb that satisfy the inequality in equation  <dig> 

 <dig>  for each nb in this set use algorithm  <dig> to evaluate std, if it exists, for the given e and m. calculate the total number of tests needed from b × tb, b=⌈nnb⌉.

 <dig>  choose the next value of db and repeat steps  <dig> and  <dig> 

 <dig>  after testing all values of db in { <dig>  ...,d - 1} and the corresponding nb, select the db, nb pair that require the least number of tests. the whole library design  should also be included while making this choice.

 <dig>  design the pooling matrix, m = std, for the choice of q and k.

a typical example of a result of applying algorithm  <dig> to the pooling design problem is shown in table  <dig>  consider a case of a  <dig>   <dig> compound library , where we expect upto  <dig> active compounds  with 1% assay error-rate , a limit on mixing not more than  <dig> compounds in a test , and at least a 99% chance of finding the active compounds . from this problem specification we have two observations. first, a single whole library design is not possible because the mixing constraint permits only  <dig> compounds per assay, thus we must use a repeated block design. second, we see that there is an optimal number of blocks  that requires the least number of tests and has a block size  between the two extremes . the best design chosen by poolhits is to implement a block pooling scheme for nb =  <dig>  db =  <dig>  e =  <dig> and m =  <dig>  using std, and repeat this design  <dig> times over to cover the whole library. the reason for choosing nb =  <dig> rather than nb =  <dig> , while both provide equal compression, is that the former provides a better actual error-correcting rate of  <dig> %. it is useful to note that this design is capable of screening a library of  <dig> ×  <dig> =  <dig>  compounds vs. the specified  <dig>   <dig> compounds. however, because of the extra error-correction, we can take advantage of this design by ignoring the extra compounds or using some form of control compound in their place. the matlab implementation of this example is provided .

sample case of n =  <dig>   <dig>  d =  <dig>  e = 1% and m =  <dig> showing the efficiency of selected block design parameters. the choice of error-rate  does not permit a full library design , thus it is left empty.

CONCLUSIONS
in this paper, we present a pooling strategy called poolhits which implements tailored modifications and enhancements that make the shifted transversal design  algorithm appropriate for drug discovery. first, we demonstrate how to switch from specifying the number of errors  for a std-based strategy to an error rate , which is the percentage of errors expected in tests. we show that there is an upper limit to the error-rate that can be handled by this std-based algorithm and this error limit  strongly constrains pooling designs to less noisy screening assays. we implement error rate as an input experimental parameter via algorithm  <dig> 

second, we introduce and implement explicit mixing constraints to make pooling significantly more relevant to hts assays via algorithm  <dig>  we also provide a necessary correction to the std construction algorithm pertinent to the mixing constraint.

third, we introduce the concept of repeated block designs that retains the efficiency of pooling strategies in the face of the mixing constraint and error-rate limitations. we show that by using this block design, we are able to simplify the assay construction, increase the error tolerance and decrease the assay size. the combination of these features produce the poolhits strategy described in algorithm  <dig>  the matlab implementations of the poolhits algorithm and an example of its use are provided in the additional files section. poolhits provides a promising route to both reducing the cost and increasing the accuracy of high throughput drug screening. although poolhits primarily focuses on drug screening, these same design methods can apply equally well to other screening environments where the number of perturbations to a system is finite or small  and the error rate of the assay is approximately known.

