BACKGROUND
experimental techniques for determining the composition of highly complex proteomes have been improving rapidly over the past decade. the application of tandem mass spectrometry-based identification routines has resulted in the generation of enormous amounts of data, requiring efficient computational methods for their evaluation. there are numerous database search algorithms for protein identification such as mascot  <cit> , sequest  <cit> , omssa  <cit>  and x!tandem  <cit> , as well as de-novo methods like lutefisk  <cit>  and pepnovo  <cit> . furthermore, there are a few methods like inspect  <cit>  which use sequence tags for pruning the possible search space using more computationally expensive and more accurate scoring functions afterwards. database search algorithms generally construct theoretical spectra for a set of possible peptides and try to match these theoretical spectra to the measured ones to find the candidate which match best. in order to distinguish between true and random hits, it is necessary to define a scoring threshold, which eliminates all peptide identifications with scores below the scoring threshold. this threshold value is chosen quite conservatively to get very few false positives. consequently, there is a significant number of correct identifications below the threshold that are not taken into account, although these spectra often correspond to interesting  proteins. one of the goals of this work was to increase the number of reliable identifications by filtering out false positives in this 'twilight zone', below the typical threshold. there are various studies addressing this issue  <cit>  by calculating the probability that an identification is a false positive.

standard identification algorithms are based on ms/ms data and do not use the information inherent to the separation processes typically used prior to mass spectrometric investigation. since this additional experimental information can be compared to predicted properties of the peptide hits suggested by ms/ms identification, false positive identifications can be identified. in sax-spe, it is important to know whether a peptide binds to the column or flows through. this information can also be incorporated into the identification process to filter out false positive identifications. oh et al.  <cit>  elaborated several chemical features such as molecular mass, charge, length and a so-called sequence index of the peptides. these features were subsequently used in an artificial neural network approach to predict whether a peptide binds to the sax column or not. the sequence index is a feature reflecting the correlation of pi values of consecutive residues. strittmater et al.  <cit>  included the experimental retention time from an ion-pair reversed-phase liquid chromatographic separation process into a peptide scoring function. they used a retention time predictor based on an artificial neural network  <cit>  but a number of other retention time predictors exist  <cit> . if the deviation between observed and predicted retention time is large, then the score of the scoring function becomes small. since they only considered the top scoring identifications , they missed correct identifications of spectra where a false positive identification had a larger score than the correct one. we also address these cases in our work, demonstrating that filtering out identifications with a large deviation between observed and predicted retention time significantly improves the classification rate of identifications with small maximal scores. only recently, klammer et al.  <cit>  used support vector machines   <cit>  to predict peptide retention times. nevertheless, they used standard kernel functions and stated that they needed at least  <dig> identified spectra with high scores to train the learning machine.

when applying of machine learning techniques to the prediction of chromatographic retention, a concise and meaningful encoding of the peptide properties is crucial. the features used for this encoding must capture the essential properties of the interaction of the peptide with the stationary and the mobile phases. these properties are mostly determined by the overall amino acid composition, by the sequence of the n-and c-terminal ends, and by the sequence in general. one of the most widely applied machine learning techniques are svms. svms use a kernel function which is used to encode distances between individual data points . there are numerous kernel functions described in the literature which can be applied to sequence data. some of them are totally position-independent, like the spectrum kernel  <cit>  which basically just compares the frequencies of patterns of a certain length. other kernels like the locality-improved kernel  <cit>  or the weighted-degree kernel  <cit>  account for patterns at a certain position. since patterns could occur shifted by a particular amount of characters, the oligo kernel  <cit>  and the weighted-degree kernel with shifts  <cit>  also account for these signals in a range controlled by an additional parameter. all of these kernels  were introduced for sequences of the same length. however, the length of peptides typically encountered in computational proteomics experiments varies significantly, ranging roughly from 4– <dig> amino acids. because it can be assumed that the local-alignment kernel  <cit> , which can also handle sequences of different lengths, does not suit this kind of problem perfectly, we elaborated a new kernel function, which can be applied to sequences of different lengths. consequently, this new kernel function is applicable to a wide range of computational proteomics experiments.

in  <dig> petritis et al.  <cit>  evaluated different features like peptide length, sequence, hydrophobicity, hydrophobic moment and predicted structural arrangements like helix, sheet or coil for the prediction of peptide retention times in reversed-phase liquid chromatography-ms. they used an artificial neural network and showed that the sequence information, together with sequence length and hydrophobic moment yield the best prediction results. in their study, they used only the border residues of the peptide sequences; their evaluation showed that a border length of  <dig> worked best for their dataset. since they used one input node for every position of the borders of the peptide, they needed a very large training set, which means that they trained their learning machine on  <dig>  peptide sequences.

since one cannot routinely measure such an amount of training sequences before starting the actual measurements, it is reasonable to apply a sort of gaussian smoothing effect to the sequence positions. this means that in our representation, not every amino acid at every position is considered but rather regions  where the amino acid occurs. the distance of the amino acids of two sequences is scored with a gaussian function. the size of this region modeled by our kernel function can be controlled by the kernel parameter σ of the kernel function and is learned by cross validation. by this and because we use support vector machines in combination with our kernel function, the number of necessary training sequences can be decreased dramatically. by just using the amino acid sequence, we do not rely on features which are important for certain separation processes. this means that we learn the features , sequence length, hydrophobic regions ...) which are important for the prediction process within the data because they are reflected in the amino acid sequence. this is why our kernel function can be used for retention time prediction in ip-rp-hplc as well as for fractionation prediction in sax-spe.

when applied to the same dataset as oh et al.  <cit>  used, our kernel function in conjunction with support vector classification predicts 87% of the peptides correctly. this is better than for all reported methods. furthermore, our retention time prediction model is based on a new kernel function in conjunction with support vector regression  <cit> , which allows us to predict peptide retention times very accurately, requiring only a very small amount of training data. this method has a better performance on a comparative test set than the artificial neural network method used by strittmater et al.  <cit> , even with a much smaller training set. additionally, our method outperforms the methods introduced by klammer et al.  <cit> . in the first part of the paper, we demonstrate that our new kernel function, in combination with support vector classification, achieves better results in sax-spe fractionation prediction than any published method. next, we show that our kernel function also performs very well in peptide retention time prediction in ip-rp-hplc with very few training data required. this allows us to train our predictor on a dataset acquired in one run to predict retention times for two further runs, and to filter the data by deviation in observed and predicted retention time. this leads to a huge improvement in the classification rate of the identifications of spectra for which only identifications with small scores can be found, and also improves the classification rate of high scoring identifications. the "methods" section briefly gives an introduction to support vector classification and support vector regression. then our new kernel function is introduced and we explain our p-value based filtering approach. finally, there is an explanation of the datasets used in this study.

RESULTS
in this section, we present the results for two different application areas of our new kernel function. the first one is peptide sample fractionation prediction in sax-spe, and the second one is peptide retention time prediction in ip-rp-hplc experiments. for peptide sample fractionation prediction, we demonstrate that our method performs better than the established method. in retention time prediction, we show that we perform very well with just a fractional amount of training data required. this allows us to train our predictor with a dataset measured in one run to predict retention times of the next runs very accurately. the peptide identifications are improved afterwards by filtering out all peptides which have a large deviation between observed and predicted retention time.

performance of peptide sample fractionation prediction
to be able to compare our results with existing methods, we used the same dataset and the same setup as oh et al.  <cit> . this means that we randomly partitioned our data into a training set and a test set, having  <dig> peptides for training and  <dig> peptides for testing. the performance was measured by classification success rate , which is the number of successful predictions divided by the number of predictions. the whole procedure was repeated  <dig> times to minimize random effects. the training was conducted by a five-fold cross-validation  and the model was trained using the best parameters from the cv and the whole training set.

to compare our new kernel function with established kernels, we used the best four feature combinations of oh et al.  <cit>  and trained an svm with the polynomial and the rbf kernel for each feature combination. feature number one is molecular weight, the second is sequence index, the third is length and the fourth feature is the charge of the peptide. we used the same evaluation setting as described above and in the five-fold cv the svm parameter c ∈ {2-4·2i|i ∈ { <dig>   <dig> ..., 14}}. for the σ parameter of the rbf kernel, σ ∈ {2-15·2i|i ∈ { <dig>   <dig> ..., 24}} and for the degree d of the polynomial kernel, d ∈ { <dig>   <dig>  3}. the results are shown in table  <dig>  it seems as if the fourth feature  is the most important factor but molecular weight also seems to improve the prediction performance.

an independent approach which just uses the sequence information of the peptides was performed using the local-alignment kernel by vert et al.  <cit> . using the same setup as described above, we used the blosum <dig> matrix  <cit>  and the kernel function parameters were the following: β ∈ { <dig> ,  <dig> ,  <dig> ,  <dig> , 1}, d ∈ { <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  13} and e ∈ { <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  13}. nevertheless, the performance of these kernel approaches led to inferior results than the published method by oh et al.  <cit> . therefore more appropriate kernel functions are needed, like our new paired oligo-border kernel , which is explained in the "methods" section. the kernel function has a kernel parameter b which is the border length of the peptide. a small b means that only few border residues of the peptides contribute to the kernel function, and a border length equal to the sequence length would mean that all residues contribute to the kernel function value. to determine the best border length of the pobk, we performed the evaluation for all b ∈ { <dig> ..., 30}. the evaluation of border length b depicted in fig.  <dig> shows that for a b greater than  <dig>  the sr does not change significantly, with a slight improvement for b =  <dig>  this is why in the following, only the pobk for b =  <dig> is considered.

a comparison of the sr for different methods can be found in fig.  <dig>  the first two bars represent the sr performance of the best svms using standard kernels of table  <dig>  the third bar demonstrates the performance of an svm with the local-alignment kernel. the fourth bar shows the performance of the best predictor in oh et al., which is  <dig> . the last bar represents the sr of the pobk, which is introduced in this paper, for peptide sample fractionation and retention time prediction. the sr of this method is  <dig> , which is significantly better than all other approaches.

correctly predicted peptides in peptide sample fractionation prediction
in oh et al.  <cit>  the prediction process with  <dig> random partitionings was done for the best four predictors, and for every peptide, the whole predictions were stored. these authors then classified a peptide by the majority label which had been assigned to the peptide. by this method, they were able to assign  <dig> of the  <dig> peptides correctly, which corresponds to an sr of  <dig> .

to be able to compare this procedure with our method, we made the assumption, that for a particular peptide, the svm would make a correct assignment more often. furthermore, we assumed that if we also stored the predictions for each peptide and each run, we could also get a majority predictor which yields good performance. the evaluation of this procedure shows that we are able to predict  <dig> peptides correctly in this setting, which is an sr of  <dig> . fig.  <dig> shows a histogram of the srs for the different peptides for the method by oh et al.  <cit>  and the svm with the pobk.

evaluation of model performance for peptide retention time prediction
for peptide retention time prediction, we had several goals. the first one was to elaborate a retention time predictor showing equivalent performance as established methods but requiring just a fraction of the training set size.

to demonstrate that our retention time predictor fullfills the desired constraints, we performed a two-deep cv on the petritis dataset  <cit>  described in the "methods" section. this means that we partitioned the data randomly into ten partitions and performed a cv with the data from nine of the ten partitions to find the best parameters. later, we trained our model with the best hyperparameters and the data of the nine partitions to evaluate the performance of the predictor on the omitted tenth partition. this was done for every possible combination of the ten partitions and the whole procedure was repeated ten times to minimize random effects.

a plot of the observed normalized retention time against the predicted normalized retention time can be seen in fig.  <dig> for one of the ten two-deep cv runs. since the standard deviation over the ten runs was  <dig> , this plot is quite representative for the model performance. petritis et al.  <cit>  showed that their method performs better than those of meek  <cit> , mant et al.  <cit> , krokhin et al.  <cit>  and kaliszan et al.  <cit> , using this dataset for validation. thus, in table  <dig>  we only compare the performance of our method with the work of petritis et al.  <cit> . this comparison is somewhat biased since we only had a fraction of the original validation set for training, which means that our training set size was  <dig> times smaller than that of the other methods. nevertheless, our method performs better than the model  <cit>  which is used by strittmater et al.  <cit>  in their filtering approach. the only model with a better performance is the artificial neural network with  <dig> input nodes and  <dig> hidden nodes  <cit> . it is obvious that a model like this needs a very large amount training data. petritis et al.  <cit>  trained their model with more than  <dig>  training peptides. therefore, this type of model is not suitable for retention time prediction for measurements under different conditions or with different machines because it is very time consuming to acquire identification and retention time data for more than  <dig>  training peptides before starting the actual measurements. to demonstrate that our method is robust enough for training on verified data of one single run, we constructed a non-redundant dataset out of datasets vds <dig>  and vds <dig> . a detailed description of these datasets can be found in the "methods" section. for different training sizes s ∈ { <dig>   <dig> ..., 170}, we randomly selected s peptides for training and  <dig> peptides for testing. fig.  <dig> indicates that for the pobk,  <dig> verified peptides are enough to train a predictor which has a squared correlation coefficient between observed and predicted normalized retention time greater than  <dig>  on the test set. this number is much smaller than the number of verified peptides we get for one run since vds <dig> has  <dig> peptides, vds <dig> has  <dig> peptides and vds <dig>  has  <dig>  this evaluation shows that with our predictor, it is possible to measure one calibration run with a well defined and easily accessible peptide mixture prepared from real biological samples to train a predictor, which can then be used to predict retention times for the peptides very accurately. furthermore, fig.  <dig> shows a comparison of the pobk to the methods introduced by klammer et al.  <cit>  and petritis et al.  <cit>  as described in the "methods" section. our method needs significantly less training data for a good prediction and has also superior performance if all training sequences of our dataset are used. one possible explanation for the low performance of the models from petritis et al. is that their models need a larger amount of training data. this is supported by the fact that they used about  <dig>  <cit>  and about  <dig>   <cit>  training peptides in their studies. to compare our method with the work by krokhin  <cit> , we used our verified datasets. this means that we e.g. trained our model on vds <dig> and predicted the retention times for peptides of the union of vds <dig> and vds <dig>  which were not present in vds <dig>  this means that if a peptide occured in vds <dig> and in vds <dig>  we only kept the peptide identification with the biggest score. for the pobk, we performed a five-fold cv with svm parameters c ∈ {2i|i ∈ {- <dig>  - <dig> ..., 0}}, v ∈ { <dig> · <dig> i|i ∈ { <dig>   <dig>  2}} and σ ∈ { <dig> · <dig> i|i ∈ { <dig>   <dig> ..., 21}} to determine the best parameters.

afterwards we trained our model with the whole training set and the best parameters and measured the squared correlation between observed and predicted retention time on the test set. this procedure was repeated ten times to minimize random effects. since there exists a web-server for the method by krokhin  <cit> , we could also compare the observed retention times with the predicted ones on our test sets with this method. to calculate the hydrophobicity parameters a and b of this method, we used our two standard peptides introduced in the "methods" section. furthermore, we used the  <dig> Å column since the other coloumns lead to inferior results. as can be seen in table  <dig>  the model by krokhin performs quite well even though it had been elaborated on another type of sorbent. nevertheless the pobk achieves a significantly higher squared correlation coefficient. it should be noted that the web-server by krokhin is restricted to three different coloumns. the advantage of our method is that there is not any restriction to a certain type of experimental setup. one only needs a small amount of training peptides and can train a model which can immediately be used for retention time prediction. it should be mentioned that the pobk has a higher squared correlation between observed and predicted retention time on our datasets than on the testset by petritis et al. this could be due to the fact that petritis et al. performed shotgun proteomics peptide identification  <cit> . it is commonly accepted that shotgun proteomics peptide identification has a significant false positive rate.

improving peptide identifications by using retention time prediction
the second goal for retention time prediction was to elaborate a retention time filter which could be used for improving peptide identifications. in this setting, we trained our learning machine on one of the vds  and predicted the retention times for the remaining ds . the peptides of the training and test sets were made disjoint by removing all identifications of the test set which belonged to spectra having an identification which was also present in the training set. on every training set, we performed a five-fold cv with svm parameters c ∈ {2i|i ∈ {- <dig>  - <dig> ..., 0}}, v ∈ { <dig> · <dig> i|i ∈ { <dig>   <dig>  2}} and σ ∈ { <dig> · <dig> i|i ∈ { <dig>   <dig> ..., 21}}. since the results of the pobk for all three datasets in table  <dig> show nearly the same very good squared correlation coefficient of about  <dig>  between observed and predicted normalized retention times, we restricted ourselves in the following to training our learning machine on vds <dig> and evaluated the filtering capability of our filtering approach on ds <dig> and ds <dig> 

the performance evaluation of our filter model was done by a two-step approach. in the first step, we measured the number of true positives and the number of false positives for the identifications returned by the mascot  <cit>  search engine. this was conducted for different significance values. mascot provides a significance threshold score for the peptide identification at a given significance level. this significance level was  <dig>  in all our studies. to be able to compare the identification performance for different levels of certainty we chose different fractions of the significance threshold score. this means for example, that for a fraction of  <dig> , all identifications have to have a score which is equal to or greater than half of the significance threshold score. the evaluation was accomplished for varying threshold fractions t ∈ { <dig> ,  <dig> ,..., 1}. in this setting, we could evaluate the classification rate . this is the number of true identifications divided by the number of spectra having at least one identification with a score higher than t times the significance threshold score. if there was more than one identification with the maximal score for one spectrum, the spectrum was excluded from the evaluation. in the second step, we filtered the data by our retention time model which was learnt on the training set and conducted the same evaluation as in the first step. after this we compared the classification performance of these two evaluations.

fig. 6a demonstrates the good cr for identifications with high mascot scores since a threshold fraction equal to one means that all identifications have a score equal or larger than the significance threshold score given by the mascot search engine. nevertheless, even for these identifications, filtering with the retention time filter improves the cr from 89–90%. an even greater improvement can be achieved for identifications with smaller scores. if all identifications are constrained to have a score equal or larger than 60% of the significance threshold score, the cr improves from 55–77% by using our filter. a cr of  <dig>  is still quite good and, as can be seen in table  <dig>  the number of true positives increases from  <dig> to  <dig>  this means that many more spectra can be identified with an acceptable number of false positives by applying our retention time filtering approach. fig. 6b shows that our model is valuable for removing false identifications since many false positives are outside the trapezoid and are removed by our filter for a threshold fraction of  <dig> . figure 6c shows this even more drastically for a threshold fraction of  <dig> . the whole evaluation shows that our retention time prediction can be used to improve the level of certainty for high-scoring identifications and also to allow smaller thresholds to find new identifications with an acceptable number of false positives.

CONCLUSIONS
in this paper, we introduced a new kernel function which was successfully applied to two problems in computational proteomics, namely peptide sample fractionation by sax-spe and high resolution peptide separation by ip-rp-hplc. furthermore, we demonstrated that the predicted retention times can be used to build a p-value based model which is capable of filtering out false identifications very accurately.

our method performs better than all previously reported peptide sample fractionation prediction methods and for retention time prediction, our method is  the only learning method which can be trained with a small training size of  <dig> peptides but still achieving a high correlation between observed and predicted retention times. this small required training set allows us to imagine the following application which would be very helpful for proteomic experiments. one could identify a well defined protein mixture before starting the experiments and use the verified peptides for training the predictor. next the predictor can be used to predict retention times for all identifications of the following runs. this predicted retention time can then be applied to improve the certainty of the predictions. it can also be used to identify a much larger number of spectra with an acceptable number of false positives. this is achieved by lowering the significance threshold and filtering the identifications by our p-value-based retention time filter. since all our methods are integrated into the openms  <cit>  library, which is open source, every researcher is able to use the presented methods free of charge. also, we offer the prediction models as tools which are part of the openms proteomics pipeline   <cit> . these tools can be easily combined with other tools from topp, allowing wide-range research applications in computational proteomics.

