BACKGROUND
one of the most important and challenging problems in post-genomic stage of bioinformatics is the automated tfbs discovery  <cit> ; computational identification of potential binding sites in upstream region of genes, which is a necessary step to understand the regulatory network within the living cell. these binding sites can be identified as over-represented and over-preserved short segments in the upstream sequences by means of a local alignment. in this problem, local alignments are usually assumed to be gapless and can be represented by a number of starting points in the input sequences. apparently, this is a multivariate optimization problem.

optimization problems with large numbers of parameters are generally prone to the problem of local optima, and discovery of tfbs  is no exception. in particular, one of the most promising types of stochastic pattern discovery methods in terms of its flexibility and wide range of application, generically called gibbs sampling  <cit> , is known to be rather strongly affected by the local optima problem  <cit> . in theory, the stochastic nature of gibbs sampling is presumed to prevent it from becoming trapped completely in a local optimum. in practice, because of the strong disturbance from local optima, gibbs sampling requires initial values that are set sufficiently close to the global optimum for reliable convergence. practical but inefficient solutions to this problem are performing numerous independent gibbs sampling runs with different initial conditions, or merely resorting to extremely long runs, hoping that the global optimum will be attained. in short, gibbs sampling has ample room for improvement as a search method in the solution space.

in pattern discovery and bioinformatics in general, improvement of search methods in the solution space has been neither systematic nor satisfactory. the method most frequently tried is the simulated annealing  <cit> . frith et al.  <cit>  tested a few different annealing procedures, but these resulted in a performance gain of only a few percentage points. improvement of the selection of initial parameters is of course possible, namely, by a heuristic approach  <cit> . however, it is unclear how helpful such heuristic guidance would be when patterns have much larger variations.

in general, there has been a real disparity between the lack of interest in improving the search methods and the strong interest in creating new models for tfbs discovery. moreover, the active introduction of new ideas into this field is making the disparity even stronger, because many of the new ideas are related to increasing the number of parameters. for example, automated phylogenetic footprinting  <cit>  is a promising way to improve detection performance, but it involves more parameters than the conventional methods because it takes the phylogenetic mutation history and the parameters to model that history into account . there have been many other recent proposals involving an increased number of parameters in the model, including the improvement of the background model by a higher-order markov model  <cit> , the simultaneous optimization of multiple models  <cit> , the introduction of site-site dependence  into the mutational model of tfbs  <cit> . there is no guarantee that improvement of sensitivity and specificity by improved model and score function always make their score-landscape more smooth. many benefits of sophisticated models can be easily vanished due to the "dimensional curse" of the increased number of parameters, unless proper consideration is made for the search method as well.

in this paper, we demonstrate that simulated tempering   <cit> , which is one of many proposals from the field of thermodynamics for the systematic avoidance of local optima in multivariate optimization problems, is quite useful for reducing the vulnerability of gibbs sampling to local optima. the application of st to a genetics problem has already been reported  <cit> . sa and potential deformation  <cit> , which has already succeeded in other problems of bioinformatics, are also rooted in the field of thermodynamics. st and sa employ a new parameter called "temperature" t, the introduction of which into a local-alignment problem has already been reported  <cit> . the novelty of st is that it attempts to adjust the value of t adaptively to the current score of alignments. by changing t, st adopts continuously changing search methods ranging from a fast deterministic-like search to a random-like search, reducing the possibility of being trapped in local optima. this principal is schematically shown in fig.  <dig>  in the present work, we implemented and tested an st-enhanced gibbs sampling algorithm for tfbs discovery, which we call gibbsst. the validation of our algorithm is also presented on synthetic test data and promoter sequences of saccharomyces cerevisiae.

RESULTS
gibbs sampling with temperature
in this section, we introduce a temperature, t, into the "classic" gibbs sampling algorithm proposed by lawrence et al.  <cit>  the details of the algorithm  will be introduced later along with the implementation of our algorithm. for simplicity, it is assumed that all n of input sequences have exactly one occurrence  of the pattern, which is always wm bp long, and negative strands are not considered.

the algorithm holds a current local alignment, a, and a current pwm , qi,j, which are iteratively updated as a markov chain until the convergence to a pattern. the alignment a is represented by the starting points of aligned segments, xk, which form a gapless sequence block. the first half of an iterative step is the re-calculation of elements of the current pwm according to the current alignment, excluding the k-th row. then in the second half of a step, the k-th row of the current alignment is updated by sampling a new value of xk according to weights derived from qi,j. let l, l, ... denote the entire sequence of the row to be updated. we set the probability of the new starting point being x proportional to

β,β=1/t,     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqggoaakdawcaaqaaiabdgfarnaabaaaleaacqwg4baeaeqaaagcbagaemiuaa1aasbaasqaaiabdiha4bqabaaaaogaeiykakyaawbaasqabeaaiigacqwfyogyaagccqggsaalcqwfyogycqgh9aqpcqaixaqmcqggvawlcqwgubavcqggsaalcawljagaaczcamaabmaabagaegymaedacagloagaayzkaaaaaa@4125@

where qx=∏i=0wm−1ql,i
mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgrbqudawgaawcbagaemieaghabeaakiabg2da9maaradabagaemycae3aasbaasqaaiabdygasjabcicaoiabdiha4jabgucariabdmgapjabcmcapiabcycasiabdmgapbqabaaabagaemyaakmaeyypa0jaegimaadabagaem4vac1aasbaawqaaiabd2gatbqabawccqghsislcqaixaqma0gaey4diunaaaa@450b@ is the likelihood that the x-th substring  of the k-th input sequence comes from the probabilistic model represented by the current pwm, and px=∏i=0wm−1pl
mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgqbaudawgaawcbagaemieaghabeaakiabg2da9maaradabagaemicaa3aasbaasqaaiabdygasjabcicaoiabdiha4jabgucariabdmgapjabcmcapaqabaaabagaemyaakmaeyypa0jaegimaadabagaem4vac1aasbaawqaaiabd2gatbqabawccqghsislcqaixaqma0gaey4diunaaaa@42cc@ is the likelihood that the same subsequence comes from a totally random sequence of the base composition observed for the entire input, p <dig> , <dig>  . the t is a positive value which is the "temperature" of the system. note that the computational complexity of the single step of the optimization is not changed by introducing the temperature.

it is easy to see that the above introduced iteration step maximizes ∏i=0wm− <dig> i/pl)β
mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqewaqaaiabcicaoiabdghaxnaabaaaleaacqwgsbabcqggoaakcqwg4baecqghrawkcqwgpbqacqggpaqkcqggsaalcqwgpbqaaeqaaogaei4la8iaemicaa3aasbaasqaaiabdygasjabcicaoiabdiha4jabgucariabdmgapjabcmcapaqabagccqggpaqkdaahaawcbeqaaggaciab=j7aibaaaeaacqwgpbqacqgh9aqpcqaiwaamaeaacqwgxbwvdawgaaadbagaemyba0gabeaaliabgkhitiabigdaxaqdcqghpis1aaaa@4e0a@, unless t is extremely large. since k circulates all n of input sequences, this is a maximization of β ∑ ∑ qi,j log after all. hence, the gibbs sampling introduced here has the relative entropy of the pattern pwm against the background model as its goal-function  to be maximized, and so does our algorithm.

however, following the convention of statistical physics, we refer to tfbs discovery as a minimization of the potential u, which is currently . because we are not proposing a new definition of u, we do not evaluate the sensitivity and specificity of our new algorithm. in principle, the sensitivity and specificity must be independent from the search method in the limit of large step number.

when t = β =  <dig>  it is reduced to the classic gibbs sampling without the idea of temperature. in this case, there always is a finite probability of selection of non-optimal x, which gives rise to the escape from the local minima. however, the magnitude of the escape probability may not be sufficient for deep local minima, because the probability is ultimately limited by the pseudocount.

the temperature strongly affects the behavior of the optimization algorithm. it is easy to see that when t is large enough, the x selection is almost random , and the algorithm is very inefficient despite the high immunity to the local minima problem. when t →  <dig>  on the other hand, a very quick convergence to local minima only results, because the movement in the solution space is a "steepest-descent" movement. in simulated annealing, the temperature is initially set to an ideally large value, th, where essentially no barrier exists in the potential landscape, and then slowly lowered. there is a theoretical guarantee that sa converges to the global minimum when the temperature decreases slowly enough  <cit> . however, it is frequently unrealistic to follow the theory because of the large number of iterations required for annealing.

temperature scheduling
simulated tempering is an accelerated version of simulated annealing and has two main features. first, the temperature of the system is continuously adjusted during the optimization process and may be increased as well as decreased. second, the adjustment of temperature is performed without detailed analysis of the potential landscape. temperature control is performed by introducing the second markov chain  that is coupled with u.

in st, the temperature of the system takes one of the nt temperature levels, t <dig> <t <dig> <t <dig> ... <tnt−1
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgubavdawgaawcbagaemota40aasbaawqaaiabdsfaubqabawccqghsislcqaixaqmaeqaaaaa@3274@ . during the optimization, the temperature is updated accordingly to the transition rates, r, given by a metropolis-hastings-like formula:

r ∝ 1/     

r ∝ s-/,     

where s± is given by

zi±1ziexp⁡exp⁡.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadawcaaqaaiabdqfaanaabaaaleaacqwgpbqacqghxcqscqaixaqmaeqaaagcbagaemowao1aasbaasqaaiabdmgapbqabaaaaowaasaaaeaacygglbqzcqgg4baecqggwbaccqggoaakcqghsislcqwgvbqvcqggvawlcqwgubavdawgaawcbagaemyaakgabeaakiabcmcapaqaaigbcwgaljabciha4jabcchawjabcicaoiabgkhitiabdwfavjabc+caviabdsfaunaabaaaleaacqwgpbqacqghxcqscqaixaqmaeqaaogaeiykakcaaiabc6cauiaaxmaacawljawaaewaaeaacqai0aanaiaawicacaglpaaaaaa@5427@

the zi are a normalizing factor usually called the partition function of the system, defined as

zi=∑exp⁡.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgabgwdawgaawcbagaemyaakgabeaakiabg2da9maaqaeabagagiyzaumaeiieagnaeiicaanaeiikagiaeyoei0yaasaaaeaacqwgvbqvaeaacqwgubavdawgaawcbagaemyaakgabeaaaagccqggpaqkasqabeqaniabgghildgccqgguaglcawljagaaczcamaabmaabagaegynaudacagloagaayzkaaaaaa@421f@

how should the temperature levels be decided in st? unlike the case of simulated annealing, no conclusive theory or rule is known for the decision of algorithmic parameters of simulated tempering, except for the requirement of small temperature intervals. according to the equations above, the equilibrium distributions of u defined for neighboring values of ti must be overlapped to ensure finite transition rates between these temperature levels. this mainly requires small temperature intervals.

the temperature levels must be decided empirically, which leaves us a vast combination of ti to explore. however, considering the success of classic gibbs sampling , we can safely assume that th ~  <dig> for the current problem. moreover, a good starting point has already been pointed out by frith et al.  <cit> . in their paper, they introduced temperature in a manner similar to ours, and reported that a slight improvement of performance was observed only when they fixed the temperature to slightly lower than  <dig>  so, in this paper, we planned to test only five different settings of temperature levels, called tlc <dig> to  <dig> , as shown in table  <dig>  for example, tlc <dig> must be pretty close to the already reported condition of fixed t. then, we extend the temperature range toward low temperature regime, retaining access to the high-temperature regime by increasing the temperature interval.

tlc <dig> was used only in "comparison with fixed-t methods".

"classic" mode: t is always 1

the point of this experimental design is to investigate the trade-off between small t <dig> and small temperature interval. small t <dig> lowers |t| and accelerates convergence until the temperature interval becomes too large for a smooth transition between temperature levels. the third possibility, increasing the number of temperature levels, nt, will be briefly examined in the discussion.

test code
we implemented our new algorithm, called "gibbsst", into a c++ code. by default, the code randomly selects  <dig> local alignments as initial values and starts independent gibbsst optimization runs from them. the results from these multiple runs are merged  upon output. it is unrealistic to expect the current version of gibbsst to reach global optima from the fewer number of initial values. also, the merging of multiple runs reduces the scatter of the resultant convergence profile, which is useful for evaluating our algorithm.

test on synthetic data
in this section, our algorithm is tested on various synthetic test datasets. the performance of our algorithm is evaluated as a function of the temperature settings, and the optimal performance will be compared to that of classic gibbs sampling. such an empirical approach is crucially important for st because there is no conclusive theory regarding the determination of temperature levels of st. basically, our model for synthetic tfbs is the one proposed in the "motif-challenge" problem  <cit> , although the level of variation, controlled by the number of mutations added to the synthetic consensus sequence, d, is quite limited by our validation scheme .

since our current goal is to make our algorithm less prone to the local optima problem, it is highly desirable that the synthetic datasets are well-characterized in terms of their global optimum alignment. if the true global optimum in a dataset  is known, a performance coefficient of the current answer can be defined. in this paper, we use a performance coefficient based on the segment overlap between two alignments  <cit> , defined as

∑i=1nmax⁡/∑i=1nmin⁡,     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaaewbqaaigbc2gatjabcggahjabciha4jabcicaoiabicdawiabcycasiabdefaxnaabaaaleaacqwgtbqbaeqaaogaeyoei0yaaqwaaeaacqwg4baedawgaawcbagaemyaakgabeaakiabgkhitiabdmha5naabaaaleaacqwgpbqaaeqaaagccaglhwuaayjcsdgaeiykakcaleaacqwgpbqacqgh9aqpcqaixaqmaeaacqwgobgta0gaeyyeiuoakiabc+cavmaaqahabagagiyba0maeiyaakmaeioba4galeaacqwgpbqacqgh9aqpcqaixaqmaeaacqwgobgta0gaeyyeiuoakiabcicaoiabdefaxnaabaaaleaacqwgtbqbaeqaaogaey4kasyaaqwaaeaacqwg4baedawgaawcbagaemyaakgabeaakiabgkhitiabdmha5naabaaaleaacqwgpbqaaeqaaagccaglhwuaayjcsdgaeiilawiaegomaijaem4vac1aasbaasqaaiabd2gatbqabagccqggpaqkcqggsaalcawljagaaczcamaabmaabagaegonaydacagloagaayzkaaaaaa@6d92@

where yi is the starting positions of the segments forming the true global optimum. this is a very effective way to isolate the features of the goal-function, the sensitivity and specificity , from the efficiency of the search method itself . a local optima resistant algorithm must show a rapid increase of the averaged performance coefficient, even from randomly given initial conditions.

with  <dig> different modes our discovery code was applied to the synthetic datasets generated under the conditions shown in table  <dig> : tlc <dig> to  <dig>  classic gibbs , and proposal of frith et al.  were compared. to evaluate the average performance over various inputs,  <dig> independent datasets were generated and analyzed for each condition. the other algorithmic parameters were the same for all combinations.

the parameters used for dataset generation in the six synthetic conditions: wm, n, wb, and d denote the width of the pattern, the number of input sequences, the length of the background sequences, and the number of mutations in a pattern occurrence, respectively.

 <dig> this was used only in "comparison with fixed-t methods".

fig.  <dig> shows a typical time course of the value of score and t in a gibbsst iteration. this data obtained by tlc <dig> shows that the transition of temperature levels was smooth, suggesting that all tlcs tested were appropriate regarding their temperature intervals. also, the plot illustrates how gibbsst solves the local optima problem; the optimization process encountered a series of local optima , but gibbsst escaped from those local optima by increasing the temperature for a brief period, then resumed optimization exploiting the efficiency at lower temperature.

fig.  <dig> shows time course of the average performance coefficient  for various algorithm settings. also, the standard deviation of the performance coefficient is shown as an error-bar for selected cases. in all pattern length and pattern variation level tested, the superiority of the gibbsst algorithm over the classic gibbs sampling is vividly shown. the performance coefficient profile of gibbsst is always above that of classic gibbs sampling. in many cases it smoothly converges to  <dig>  which means the global optimum is reached. on the contrary, in some cases, classic gibbs sampling shows extremely poor convergence to the global optimum because the randomly selected initial values were inappropriate for classic gibbs sampling. there are statistically significant performance gaps between gibbsst  and classic gibbs sampling for all of the cases unless step number is too large .

when t was fixed to  <dig> , the performance was significantly improved in all cases tested. however, the extent of performance improvement was always smaller than that of gibbsst. it is interesting to note that t =  <dig>  performed slightly poorer than tlc <dig> .

we can conclude that gibbsst achieves a substantial improvement in performance over existing gibbs sampling methods when the pattern length is small and the pattern-variation level is high. it is difficult to decide the optimal temperature setting because there is very little difference in performance among tlc <dig>   <dig> and  <dig>  although tlc <dig> shows the best performance. for a further performance improvement, the use of lower t <dig> than that of tlc <dig> seems to deserve serious consideration.

comparison with fixed-t methods
can the fixed-t methods, that is, conventional gibbs sampling with the temperature fixed to a lower value than  <dig>  be a substitute for gibbsst? certainly, temperature reduction of only 10% showed a considerable performance improvement in fig.  <dig>  however, lowering the temperature is not a universal solution because when the temperature is fixed to an exceedingly low value, sampling based on the temperature is rather similar to that of the inefficient steepest descent method. to demonstrate this vulnerability of fixed-t methods and the superiority of gibbsst, several fixed-t methods  are shown in comparison to the gibbsst algorithm in fig.  <dig>  a special dataset  was prepared and used in this experiment because a dataset with a rough score landscape  is ideal for the current objective. in addition, a special temperature set  is used to explore the possibility of lower temperatures. fig.  <dig> shows the time-course of the average relative score  of  <dig> datasets for various methods. the score is normalized with respect to the maximum score obtained for each dataset. the two insets show enlarged plots of the first  <dig> steps and the last  <dig> steps.

lowering the temperature seems to be an ideal method to improve the convergence, as long as the score increase in the first  <dig> steps is concerned : the t =  <dig>  setting shows a dramatically fast score increase in this region. however, the score increase of t =  <dig>  eventually slows down: it is overtaken by t =  <dig>  at ~ <dig> steps, and by t =  <dig>  at ~ <dig> steps. in general, the greater the performance of a temperature setting is in the initial phase, the earlier the score ceases to improve. as a consequence, the scores of t =  <dig> , t =  <dig>  and t =  <dig>  are stagnant in the final phase of optimization  and are perfectly in reverse order of their performance in the initial stage. the most probable reason for the fast score increase's subsequent performance deterioration is, of course, the local optima in the search space. our proposal, gibbsst, is immune to such a general trend: its performance in the initial phase is not much poorer than that of best fixed-t methods, but its score in the final phase is better than any other setting tested. considering that a small score difference may correspond to vastly different alignments in a rough score landscape of biological sequences, this level of difference in the final score is more than sufficient to clarify the superiority of gibbsst over fixed-t methods.

although fixed-t methods do have simplicity and a limited usability as a substitute of gibbsst, a crucial problem exists in employing lowered and fixed temperature in gibbs sampling. the temperature dependence of the behavior of the optimization process, like that shown in fig.  <dig>  is quite "nonlinear": there is no way to know the optimal temperature in advance. for that reason, even if a fixed-t setting better than gibbsst exists, the fixed-t setting is not likely to be available. the optimal temperature's possible dependence on characteristics of input sequences  further complicates the situation, and increases the possibility of exceedingly lowered temperature. consequently, the fixed-t method is very inconvenient as an acceleration method in pattern discovery problems. in addition to the fact gibbsst outperforms all sampling scheme tested in fig.  <dig>  it should be emphasized that gibbsst is the only method so far that has been proposed to utilize temperature lower than  <dig>  without damaging the search robustness.

test on biological data
in this section, we demonstrate the usefulness of our algorithm for making more realistic tfbs predictions. although our algorithm was quite effective for synthetic datasets, the statistical characteristics of natural promoter sequences may be very different from those assumed for synthetic datasets. such a difference may demand further adjustment of the algorithmic parameters of simulated tempering  according to the realistic potential landscape of natural promoters.

we selected six transcription factors of saccharomyces cerevisiae for use in this test. there are two main reasons for this choice. first, very comprehensive information is available for this eukaryote from the saccaromyces cerevisiae promoter databases  <cit> . the promoter sequences, the regulatory relationships, and their evidence can be easily obtained from this curated database.

the second reason is related to the characterization of test data in terms of the global optimum. using eight real tfbs of saccharomyces cerevisiae and their flanking regions as examples, friberg et al.  <cit>  compared several different score-functions with respect to their sensitivity. in their test, the value of the score-functions were evaluated for all possible alignments in the flanking region and the rank of the biologically correct alignment  was evaluated as an index of sensitivity of the score-functions. a scoring function called map  yielded rank =  <dig> for five out of eight examples. their definition of map was the one used in mdscan  <cit> , which would be quite close to our current definition of score-function if it did not use the 3rd-order markov model to describe the background sequences. thus, now we have a list of transcription factors whose binding-sites have fairly large possibilities to be the global-optimum in terms of our current potential function.

the transcription factors we selected, reb <dig>  <cit> , rap <dig>  <cit> , pdr <dig>  <cit> , mig <dig>  <cit> , mcm <dig>  <cit> , and abf <dig>  <cit> , are introduced in fig.  <dig>  the other two examples were omitted because there were too few specific sites  and too few known binding sites  found in scpd. for each transcription factor,  <dig> different datasets with different window placement were prepared. tfbs in minus strands were not excluded. according to friberg et al.  <cit> , the flanking regions of mcm <dig> and abf <dig> sites contain other sites associated with higher values of the current score function than the biologically correct binding sites. when the randomized placement of the window includes these non-target sites, the result may be an increased level of difficulty in the reconstruction of mcm <dig> and abf <dig> binding sites .

the results are shown in fig.  <dig> using the same format used for synthetic datasets. the lower average value of the performance coefficient can be attributed to binding sites of other transcription factors flanking the target tfbs, correlations in the background, and incompatibility between the score function and the target tfbs. in the cases of mcm <dig> and abf <dig>  the average performance coefficient is especially low. the alignment snapshots of mcm <dig> were closely examined, and we found that the snapshots contain almost as many ttcc----ggaaa- and -tttcc----ggaa as the biologically correct motif . these "phase-shifted-motifs" are considered to be a major form of local optima related to performance degradation  <cit> . when gibbsst was applied, both shifted-motif and correct-motif were sampled more frequently , but their composition was not improved. it seems that gibbsst is not particularly suitable for solving the shifted-motif problem. the snapshots of pdr <dig> were also examined, but for this case, a totally different pattern of failure was identified . gibbsst was unable to find any hit in the mig <dig> datasets, although this cannot be attributed to any defect of our algorithm, because, for these datasets, meme also completely failed even when the correct wm was specified .

still, the performance superiority of gibbsst over classic gibbs sampling is clear in a majority of the tested cases. the general trend of a larger improvement for smaller wm and a larger variation among sites is not changed. also, the best-performing temperature setting  was generally unchanged from the case of the synthetic dataset. although other settings performed best in some cases , further consideration is required since some cases also showed a marked degradation of the overall performance. when t is fixed to  <dig> , the results are classifiable into two categories. in the first category, which includes reb <dig>  rap <dig>  and abf <dig>  the performance of t =  <dig>  is identical to that in the synthetic datasets: the performance is better than t =  <dig>  and worse than that of tlc <dig>  in the second category, the result deviates surprisingly from that observed for synthetic datasets: the performance actually deteriorated when the temperature was lowered. for mcm <dig> and pdr <dig>  encouraging the search algorithm to perform locally efficient sampling  reduces the algorithm's efficiency in a global sense. a natural interpretation of this phenomenon is that the datasets of these two tfbs bear an especially complicated score landscape, which is confirmed later in fig.  <dig>  the optimal temperature setting seems to depend strongly on the characteristics of input sequences, and the adaptive nature of gibbsst might be an effective solution to alleviate the dependence.

pdr1
it is worthwhile to take a close look at the result for pdr <dig>  because it is quite different from the results for other transcription factors. the time courses of the relative score and performance coefficient in the first  <dig> steps are shown in the left and right halves of fig.  <dig>  respectively. the relative score is defined as the ratio of the current score to the score of the biologically correct answer. the plots show a quick increase of the performance coefficient and relative score followed by a quick convergence of the relative score  and a sudden decrease of the performance coefficient for gibbsst  only. slower and steadier convergences of the relative score  and the performance coefficient were observed for classic gibbs and gibbsst . apparently, gibbsst with appropriate temperature settings found a global optimum that was inaccessible to classic gibbs sampling, although the global optimum was not biologically correct.

abf1
the result for abf <dig> is also interesting because of the low performance coefficient. in fig.  <dig>  alignments obtained for this case by classic gibbs sampling and gibbsst are compared. these are snapshots taken from runs that yielded the highest scores. these examples show that gibbsst improves the quality of alignments far more efficiently than classic gibbs sampling does. here, gibbsst requires only  <dig> steps for the same level of progress, which requires  <dig> steps of classic gibbs sampling. in only  <dig> steps gibbsst achieved an alignment with clear features of the binding site of abf <dig> .

the reason for the low performance coefficient is revealed by close examination of this alignment. the three segments marked in the alignment closely resemble the known abf <dig> sites, but they have no biological evidence in scpd. these biologically non-confirmed sites engender the large disparity between the high score and low performance coefficient observed for abf <dig>  nevertheless, the high efficiency of gibbsst in convergence to a high-quality alignment is remarkable. we conclude that these data illustrate the strength of gibbsst in terms of the fast alignment improvement. they also show the limit of our current validation scheme in terms of the dependence on the "correct" answer.

temperature setting
the local optima dependence of optimization algorithms can also be analyzed as initial value dependence. an index of initial value dependence is the ratio of "successful" initial values to all initial values tested for a condition. this index is connected directly to the number of  initial values required  for finding one pattern in the solution space. we define the initial value as "successful" when a run started from an initial value reaches 99% of the score of the known global optimum  at somewhere before  <dig> steps.

the resultant index for synthetic tfbs is shown in the upper half of fig.  <dig>  unlike the plots of the performance coefficient profile, these data show that the optimal temperature setting is not necessarily tlc <dig>  it depends on the input sequence characteristics. for two conditions, tlc <dig> was optimal. for wm =  <dig>  d =  <dig>  tlc <dig> was optimal in terms of the local minima resistance. the "classic" algorithm was optimal for wm =  <dig>  d =  <dig>  but the difference between the "classic" algorithm and tlc <dig> was small. for these wm =  <dig> cases, the overall performance improvement of tlc <dig> , and  <dig> shown in fig.  <dig> derives mainly from the lower average temperature , which is only a side effect of gibbsst. these cases illustrate the necessity for more sophisticated temperature settings, but gibbsst exhibits better overall performance than the classic method even for these cases, as shown by data of the performance coefficient.

as shown in the lower half of fig.  <dig>  tlc <dig> showed the greatest effect of alleviating the initial value dependence for biological test data. for reb <dig> and rap <dig>  the situation tested was too easy to differentiate tlcs. for abf <dig>  the data is not really reliable for the reason introduced in the previous section. it is noteworthy that the magnitude of enhancement of "successful" initial values was remarkably large for some conditions. for example, in the case of pdr <dig>  gibbsst requires only one-fourth of the initial values of those required for classic gibbs sampling . our conclusion for temperature settings is as follows. the temperature setting tlc <dig> is the optimal selection when wm <  <dig> and large levels of pattern variation are expected. in such a case, a possibly lower minimum temperature than that of tlc <dig> should be considered for further performance improvement . temperature settings tlc <dig>  tlc <dig> and tlc <dig> will work well for longer and rigid patterns . when wm ≥  <dig>  tlc <dig> remains the best selection, but a better temperature setting should be devised for these cases regarding the initial value dependence. alternatively, gibbsst should be tested for wm =  <dig> test data with larger pattern variations.

discussion
the performance of computational tbfs discovery can be enhanced by means of improvement of the search method in its own right. we assumed that a good search method must have resistance to local optima, to yield solution of better quality in fewer iterative steps. we also assumed that a good search method must not be strongly sensitive to the initial values. these goals were realized and demonstrated by our new algorithm, gibbsst. in the long run, this approach frees up computational resources for more biologically appropriate modeling of tfbs.

many functions should be added to gibbsst. for example, non-oops occurrence models, better background models and automatic adjustment or scanning of wm are important. there is no fundamental difficulty in incorporating these functions into gibbsst. the standard method of estimation of the p-value  <cit>  can also be implemented with ease, because the standard model and score definition is used in gibbsst.

although we employed the relative entropy in the present work, there is a wide range of possible score functions to be combined with gibbsst. because it is independent of the biological model, gibbsst only requires evaluation of

exp⁡−−u)t     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacygglbqzcqgg4baecqggwbacdawcaaqaaiabgkhitiabcicaoiabdwfavjabcicaoiabb6gaujabbwgaljabbeha3jabbccagiabbggahjabbygasjabbmgapjabbeganjabb6gaujabb2gatjabbwgaljabb6gaujabbsha0jabcmcapiabgkhitiabdwfavjabcicaoiabb+gavjabbygasjabbsgakjabbccagiabbggahjabbygasjabbmgapjabbeganjabb6gaujabb2gatjabbwgaljabb6gaujabbsha0jabcmcapiabcmcapaqaaiabdsfaubaacawljagaaczcamaabmaabagaeg4nacdacagloagaayzkaaaaaa@6184@

for its gibbs sampling section, and the partition function, z, for its temperature selection section. any u is compatible with st because evaluation of u is a totally encapsulated part of the algorithm. however, it should be noted that the concern about the computational complexity of the score function is reduced because of the substantial improvement of efficiency by st. we can now employ score functions with more complex representation of biological specificity of binding sites. we are especially interested in rareness-based score functions  <cit> , because of their improved biological sensitivity and relatively heavy computational burden.

lower minimum temperatures and more sophisticated temperature scheduling should be tested, especially when gibbsst is applied to long rigid patterns. one trivial possibility that should be addressed is increasing nt, that is, the use of numerous small temperature steps. the problem with this simple idea is that temperature adjustments by means of small temperature steps would be unable to keep up with the rapid change of the alignment score. in fact, we frequently observed this phenomenon and the resulting severe degradation of performance during our preliminary testing of gibbsst. in other words, sudden and large changes in the value of the goal function are the most noteworthy features of tfbs discovery based on gibbs sampling, when its combination with simulated tempering is considered.

this is only one example of the many possibilities of algorithmic design that should be explored before gibbsst is extended to other interesting problems of bioinformatics. we confined our study to the simplest of the tempering schemes and to elementary optimization of the temperature levels. several improvements of the tempering scheme itself  <cit>  are yet to be tested. however, we have secured a good starting point, tlc <dig>  for exploration that is validated for both synthetic and biological promoter sequences. as evident in figs.  <dig>   <dig>  and  <dig>  gibbsst is most effective for hidden patterns that have a high level of variation  compared to their length  this fact is attributable to the shorter distance in the solution space between highly variable patterns and background noise compared to long and rigid patterns. this condition coincides with objectives of biological interest: sequence motifs with large variation. however, we were unable to validate gibbsst in a so-called "twilight-zone" of sequence pattern detection mainly because our test scheme depends on the success of meme, although it is strongly anticipated that the performance gain in the twilight zone is even larger than that observed in the presented data. a better method of validation is necessary to advance our method in this direction. this direction should be advanced in combination with the better score function, evaluation of sensitivity, and specificity in an integrated manner.

introduction of different methods into gibbsst is possible and promising. according to our preliminary test, the overall efficiency of gibbsst with the best temperature setting measured by the performance coefficient profile is roughly comparable to that of gibbsmotifsampler  <cit> , a conventional gibbs sampling method combined with a sophisticated selection of initial parameters . introduction of any successful initial alignment setting, not excluding the combinatorial approaches  <cit> , into gibbsst as a preprocessing stage should be considered in the future as candidates for a very efficient pattern discovery program.

seed-based initialization in search methods, that is, a preprocessing to find promising partial patterns, is quite useful to highlight the advantage of gibbsst. even when not explicitly defined as such, all seed-based approaches assume that all partially correct solutions in the search space can be recognized and kept track of. in other words, a seed-based approach always assumes the availability of a complete catalog of all the deep basins illustrated in fig.  <dig>  although nobody has ever reported any number statistics of basins in the concrete score landscape of the local-alignment problem, in some situations, such a catalog is going to be difficult to create. such a breakdown of seed-based search methods is expected under two extreme conditions: when the score of the target pattern is too close to the noise-level, or the search space to be explored is simply immense. the first condition corresponds to the twilight zone. the second condition is mainly relevant to complicated models like patterns with special types of flexibility . gibbsst can be extended and will be useful to any patterns in important subjects in bioinformatics  that meet either or both of these two conditions.

CONCLUSIONS
our new algorithm for tfbs discovery, gibbsst, is based on an adaptive adjustment of the search stringency and shows a much increased resistance to local optima. by combining gibbs sampling and simulated tempering, gibbsst creates a robust platform for difficult pattern detection in biological sequences.

