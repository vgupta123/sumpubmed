BACKGROUND
high-throughput methods in molecular biology have challenged existing data analysis methods and stimulated the development of new methods. a key example is the gene expression microarray and its use as a screening tool for detecting genes that are differentially expressed  between different biological states. the need to identify a possibly very small number of regulated genes among the  <dig> s of sequences found on modern microarray chips, based on tens to hundreds of biological samples, has led to a plethora of different methods. the emerging consensus in the field  <cit>  suggests that a) despite ongoing research on p-value adjustments  <cit> , false discovery rates  are more practical for dealing with the multiplicity problem, and b) classical test statistics requires modification to limit the influence of unrealistically small variance estimates. nonetheless, many competing methods for detecting de exist, and even attempts at validation on data sets with known mrna composition  <cit>  cannot offer definitive guidelines.

in this context, the introduction of the so-called optimal discovery procedure  constitutes a major conceptual achievement. building on the neyman-pearson lemma for testing an individual hypothesis, the author shows that an extension of the likelihood ratio test statistic for multiple parallel hypotheses  is the optimal procedure for deciding whether any specific gene is in fact de: for any fixed number of false positive results, odp will identify the maximum number of true positives. the odp establishes therefore a theoretical optimum for detecting de against which any other method can be measured.

unfortunately, the optimality of odp is a strictly theoretical result that requires, for all genes, a full parametric specification of the densities under null and alternative hypothesis. in practice, even assuming normality, the gene-wise means and variances are unknown, and they become nuisance parameters in the hypothesis testing. consequently, the authors of  <cit>  have suggested an estimated version eodp, which can be implemented in practice. it is, however, not clear how eodp performs compared to the theoretical optimum, or other existing methods, except under the most benign circumstances .

the main questions of this paper are therefore a) whether the optimality of odp is retained by eodp, and b) whether we can improve on eodp's performance in practice. previously, we have introduced a multidimensional extension of the fdr procedure  that combines standard error information with the classical t-statistic. we demonstrated that the fdr2d performs as well or better than the usual modified t-statistics, without requiring extra modeling or model assumptions  <cit> . in this paper, we show that fdr2d also outperforms eodp on simulated and real data sets. we also demonstrate how a synthesis of the eodp and fdr2d procedures can further improve the power to detect de.

the two-sample problem
we demonstrate the application of eodp and fdr2d in the common situation where we want to detect genes that are de between two biological states. we assume n <dig> and n <dig> arrays for each group, each containing probes for m genes. for gene i, we observe a vector of expression values xi of length n <dig> + n <dig>  which consists of the observations xi <dig> in the first group, and xi <dig> in the second group. we define the groupwise means and standard deviations as usual, and refer to the pooled standard deviation as

s˜i2=si12+si22n1+n2− <dig> 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgzbwcgaacamaadaaaleaacqwgpbqaaeaacqaiyagmaagccqgh9aqpdawcaaqaaiabcicaoiabd6gaunaabaaaleaacqaixaqmaeqaaogaeyoei0iaegymaejaeiykakiaem4cam3aa0baasqaaiabdmgapjabigdaxaqaaiabikdayaaakiabgucariabcicaoiabd6gaunaabaaaleaacqaiyagmaeqaaogaeyoei0iaegymaejaeiykakiaem4cam3aa0baasqaaiabdmgapjabikdayaqaaiabikdayaaaaoqaaiabd6gaunaabaaaleaacqaixaqmaeqaaogaey4kasiaemoba42aasbaasqaaiabikdayaqabagccqghsislcqaiyagmaagaeiola4caaa@5161@

furthermore, we assume that we are dealing with a random mixture of de and nonde genes, with a proportion π <dig> of genes being nonde.

odp statistics
the theoretical odp statistic assumes that for all i =  <dig>  ... m genes, the density functions of the expression values under the null hypothesis of no de, fi, and under the alternative hypothesis of de, gi, are fully known in advance. for the random mixture of de and nonde genes outlined above, the odp statistic for the observed expression values xi of the i-the gene can then be written as

sodp=∑j=1mgj∑j=1mfj.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgtbwudawgaawcbagaee4ta8kaeeiraqkaeeiuaafabeaakiabcicaogqabiab=hha4naabaaaleaacqwfpbqaaeqaaogaeiykakiaeyypa0zaasaaaeaadaaewaqaaiabdegannaabaaaleaacqwgqbgaaeqaaogaeiikagiae8heag3aasbaasqaaiab=lgapbqabagccqggpaqkasqaaiabdqgaqjabg2da9iabigdaxaqaaiabd2gatbqdcqghris5aagcbawaaabmaeaacqwgmbgzdawgaawcbagaemoaaogabeaakiabcicaoiab=hha4naabaaaleaacqwfpbqaaeqaaogaeiykakcaleaacqwgqbgacqgh9aqpcqaixaqmaeaacqwgtbqba0gaeyyeiuoaaagccqgguaglaaa@54e2@

the procedure then rejects the null hypothesis for all genes i with si ≡ s ≥ λ, i.e. all genes with large si are declared to be de. using the neyman-pearson lemma, it can be shown that this procedure is optimal in the sense that for any pre-specified false positive rate , the odp will have the maximum true positive rate. this optimality property can also be expressed in terms of fdr  <cit> .

requiring full specification of all null and alternative distributions, however, is impractical. in any realistic application, only an estimated odp statistic

s^odp=∑j=1mg^j∑j=1mf^j
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcamaabaaaleaacqqgpbwtcqqgebarcqqgqbauaeqaaogaeiikagccbegae8heag3aasbaasqaaiab=lgapbqabagccqggpaqkcqgh9aqpdawcaaqaamaaqadabagafm4zacmbakaadawgaawcbagaemoaaogabeaakiabcicaoiab=hha4naabaaaleaacqwfpbqaaeqaaogaeiykakcaleaacqwgqbgacqgh9aqpcqaixaqmaeaacqwgtbqba0gaeyyeiuoaaoqaamaaqadabagafmozaymbakaadawgaawcbagaemoaaogabeaakiabcicaoiab=hha4naabaaaleaacqwfpbqaaeqaaogaeiykakcaleaacqwgqbgacqgh9aqpcqaixaqmaeaacqwgtbqba0gaeyyeiuoaaaaaaa@5424@

is feasible, where the densities f^i
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgmbgzgaqcamaabaaaleaacqwgpbqaaeqaaaaa@2f98@ and g^i
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgnbwzgaqcamaabaaaleaacqwgpbqaaeqaaaaa@2f9a@ are estimated from the data. in  <cit> , the authors propose to assume that all genes follow a normal distribution ; under this assumption, only means and variances have to be estimated from the data. in our two-sample situation, this amounts to

s^odp∑j=1mφφ∑j=1mφ     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcamaabaaaleaacqqgpbwtcqqgebarcqqgqbauaeqaaogaeiikagccbegae8heag3aasbaasqaaiab=lgapbqabagccqggpaqkdawcaaqaamaaqadabaaccigae4nxdy2aaewaaeaacqwf4baedawgaawcbagae8xaakmae8xmaedabeaakiabcyha8jqb+x7atzaajawaasbaasqaaiabdqgaqjabigdaxaqabagccqggsaalcugfdpwcgaqcamaadaaaleaacqwgqbgacqaixaqmaeaacqaiyagmaaaakiaawicacaglpaaacqgfgpgzdaqadaqaaiab=hha4naabaaaleaacqwfpbqacqwfyagmaeqaaogaeiifawnaf4hvd0mbakaadawgaawcbagaemoaaomaegomaidabeaakiabcycasiqb+n8azzaajawaa0baasqaaiabdqgaqjabigdaxaqaaiabikdayaaaaogaayjkaiaawmcaaawcbagaemoaaomaeyypa0jaegymaedabagaemyba0ganiabgghildaakeaadaaewaqaaiab+z8amnaabmaabagae8heag3aasbaasqaaiab=lgapbqabagccqgg8bafcugf8oqbgaqcamaabaaaleaacqwgqbgaaeqaaogaeiilawiaf43wdmnbakaadaqhaawcbagaemoaaomaegimaadabagaegomaidaaagccagloagaayzkaaaaleaacqwgqbgacqgh9aqpcqaixaqmaeaacqwgtbqba0gaeyyeiuoaaagccawljagaaczcamaabmaabagaegymaedacagloagaayzkaaaaaa@7ee3@

where φ is the joint-density for the normal distribution with mean μ and variance σ <dig> 

conceptually, under the null hypothesis, we have the usual estimates μ^j=x¯j
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwf8oqbgaqcamaabaaaleaacqwgqbgaaeqaaogaeyypa0jafmieagnbaebadawgaawcbagaemoaaogabeaaaaa@342c@ and σ^j02=sj2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwfdpwcgaqcamaadaaaleaacqwgqbgacqaiwaamaeaacqaiyagmaagccqgh9aqpcqwgzbwcdaqhaawcbagaemoaaogabagaegomaidaaaaa@36eb@ from the combined data, and under the alternative hypothesis, the corresponding group-wise means μ^j1=x¯j1
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwf8oqbgaqcamaabaaaleaacqwgqbgacqaixaqmaeqaaogaeyypa0jafmieagnbaebadawgaawcbagaemoaaomaegymaedabeaaaaa@360c@ and μ^j2=x¯j2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwf8oqbgaqcamaabaaaleaacqwgqbgacqaiyagmaeqaaogaeyypa0jafmieagnbaebadawgaawcbagaemoaaomaegomaidabeaaaaa@3610@ with the pooled sample variance σ^j12=s˜j2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwfdpwcgaqcamaadaaaleaacqwgqbgacqaixaqmaeaacqaiyagmaagccqgh9aqpcuwgzbwcgaacamaadaaaleaacqwgqbgaaeaacqaiyagmaaaaaa@36fc@. for the practical implementation, we follow  <cit>  and pre-normalize all genes to have zero mean.

the second step in applying the odp to data is the calibration of the procedure. there is no distribution theory for the statistic, so it is not clear how to choose the threshold λ to achieve a desired fdr level.  <cit>  suggest a conventional algorithm that computes the estimated odp statistic s^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcaaaa@2deb@ under random permutations of the group labels; they use the resulting null distribution of s^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcaaaa@2deb@ to compute the q-value for each gene, which represents its global fdr . we follow this approach for our implementation, but use the local false discovery rate , with essentially identical results as theirs.

multidimensional local false discovery rate
fdr approaches focus on the distribution of the specific statistic z used to test the gene-wise null hypotheses, in contrast to odp, which is based on the distribution of the data. given a mixture of de and nonde genes as described above, the density f of z can be written as

f = π0f <dig> + f <dig>      

where f <dig> and f <dig> are the densities of the test statistic z for nonde and de genes, respectively, and π <dig> the proportion of truly nonde genes. the local fdr for any observed value z of the test statistic is then

fdr=π0f0f,     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqqgmbgzcqqgkbazcqqgybgccqggoaakcqwg6bgecqggpaqkcqgh9aqpiigacqwfapacdawgaawcbagaegimaadabeaakmaalaaabagaemozay2aasbaasqaaiabicdawaqabagccqggoaakcqwg6bgecqggpaqkaeaacqwgmbgzcqggoaakcqwg6bgecqggpaqkaagaeiilawiaaczcaiaaxmaadaqadaqaaiabiodazagaayjkaiaawmcaaaaa@46b3@

and can be interpreted as the expected rate of false positives among genes with test statistic z, see  <cit> . practically, the densities f can be estimated from the histograms of the test statistics computed from the real data, and f <dig> is estimated similarly from the test statistics computed from permuted data.

formulated as a decision procedure like odp, we specify a test statistic z and a desired threshold α for the local fdr; we then compute for each gene the value of the test statistic zi = z and the decision criterion fdri = fdr and declare genes with fdri <α to be de.

as the more usual global fdr of a set of test statistics is just the average of their local fdr  <cit> , little seems to be gained by using the local fdr. note, however, that equations  and  still hold if we replace the univariate test statistic z by a vector z of test statistics. we have recently shown that for the two-sample problem, using a bivariate test statistic and the associated two-dimensional fdr is more powerful than conventional fdr for univariate test statistics  <cit> . specifically, the test statistic z =  with

z <dig> = t and z <dig> = log se,     

where t is the usual t statistic, and se the standard error of the mean,

se=s˜1/n1+1/n <dig> 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgzbwccqwglbqzcqgh9aqpcuwgzbwcgaacamaakaaabagaegymaejaei4la8iaemoba42aasbaasqaaiabigdaxaqabagccqghrawkcqaixaqmcqggvawlcqwgubgbdawgaawcbagaegomaidabeaaaeqaaogaeiilawcaaa@3c88@

yields smaller fdr not only compared to the conventional t-statistic on its own, but also compared to a number of popular modified t-statistics  <cit> .

in the following, we will use the abbreviations fdr1d and fdr2d for local fdr computed based on univariate and bivariate test statistics, respectively. note that in practice, the fdr2d is estimated in a similar manner as the fdr1d, using two-dimensional histograms instead of one-dimensional histograms, together with a somewhat more sophisticated binomial smoothing procedure, see  <cit>  for details.

procedures to be evaluated
the central aim of this paper is to compare the operating characteristics of four different procedures for detecting de on a number of real and simulated data sets:

 <dig>  t1d uses the standard t-statistic with conventional fdr1d and serves as a reference.

 <dig>  s1d uses the logarithm of s^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcaaaa@2deb@ in  with fdr1d; this procedure is equivalent to the estimated version of odp described in  <cit>  and its implementation in the edge software.

 <dig>  t2d uses the test statistic in  for calculating fdr2d; this is the same procedure as described in  <cit> .

 <dig>  s2d is a novel procedure that combines the logarithm of s^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcaaaa@2deb@ and the standard error for computing fdr2d, see below.

RESULTS
feasibility of s2d
we first evaluate the s2d procedure, based on the bivariate test statistic

z <dig> = logs^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcaaaa@2deb@     and     z <dig> = log se,

with s^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcaaaa@2deb@ defined as in  and se as in . the only practical concern is that the smoothing procedure described in  <cit>  may have problems with s^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcaaaa@2deb@. indeed, the reason for taking the logarithms of the test statistics is to facilitate smoothing, by avoiding crowding at the boundary values.

figures  <dig> and  <dig> show the scatter plot of the bivariate test statistics for two real data sets described in methods, with the estimated fdr2d overlayed as isolines. we exploit the useful fact that we can always average the fdr2d over one of the component statistics to get the fdr1d for the other component statistic:
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcaaaa@2deb@ on the horizontal axis and log se on the vertical axis. each symbol corresponds to a gene. the isolines shown are the local fdr based on the s2d method.  the same as  for the lymphoma data.  the local fdr for the brca data, shown as a function of log s^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcaaaa@2deb@ on the horizontal axis. the black line shows the local fdr computed via s1d, the red line shows the fdr based on s2d, averaged across the log standard errors as shown in  above.  the same as  for the lymphoma data.

fdr1d=∫fdr2ddlog⁡se,
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqqgmbgzcqqgkbazcqqgybgccqaixaqmcqqgkbazcqggoaakcyggsbabcqggvbwbcqggnbwzcuwgtbwugaqcaiabcmcapiabg2da9maapeaabagaeeozaymaeeizaqmaeeocainaegomaijaeeizaqmaeiikagiagiibawmaei4ba8maei4zacmafm4uamlbakaacqggsaalcyggsbabcqggvbwbcqggnbwzcqwgzbwccqwglbqzcqggpaqkcqwgkbazcyggsbabcqggvbwbcqggnbwzcqwgzbwccqwglbqzcqggsaalasqabeqaniabguiiydaaaa@5b36@

see  <cit> . figures  <dig> and  <dig> show s1d  overlayed with the averaged s2d  for both data sets, with excellent agreement. this indicates that the smoothing required for computing s2d has been successful. this is consistent with the relationship between t-statistics and logs^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcaaaa@2deb@ for the data at hand , which is essentially linear for genes with t-statistic |t| >  <dig>  suggesting that the same general smoothing procedure is applicable.

performance on simulated data sets
we perform simulations with  <dig>  genes per array, a proportion of truly nonde genes π <dig> =  <dig> , and two independent groups with n =  <dig> arrays per group. we combine three different levels of variance heterogeneity between genes with two different settings for the balance between up- and down-regulation, for a total of six different simulation scenarios:

 <dig>  variances can be 'similar'  across genes, 'balanced', which allows for moderate differences in variance between genes, and 'variable', which allows large differences.

 <dig>  in the 'symmetric' case, roughly 50% of the de genes are up- and down- regulated; in the 'asymmetric' case, only about 20% of all genes are down-regulated, the rest is up-regulated.

we have included the asymmetric scenario, because this is where odp is expected to perform better than standard methods in a theoretical setting  <cit> . all expression values are assumed to follow a normal distribution; see methods for further details of the simulation procedure.

for each scenario, we generate  <dig> data sets, for a total of  <dig> genes. for each procedure, the fdr values are computed by keeping track of the de status of each gene, grouping the genes in intervals  or grid cells  based on their test statistic, and computing the percentage of false positives in each interval or cell.

in order to compare different fdr procedures, we summarize their results via operating characteristics  curves: for each procedure, we sort the groups of genes as described above by their local fdr, and compute the corresponding global fdr as cumulative mean of the local fdrs from the smallest to the largest. this global fdr is then plotted against the cumulative percentage of genes in these intervals or grid cells. the resulting curve shows the true global fdr for a set of top-ranked genes as a function of the size of that set . the results for the different simulation scenarios and all four procedures are shown in figure  <dig> 

there is little or no difference in relative performance between the procedures under the symmetric and asymmetric scenarios in figure  <dig>  it is also clear that the differences in performance are most pronounced when the variances are similar, less so when the variances are balanced, and minor when the variances are highly variable. the ranking of the different procedures is consistent through all six scenarios: as expected, t1d has the worst performance; equally as expected, s1d does clearly better than t1d. novel findings of this paper are that a) t2d does still better than s1d, and b) s2d improves over t2d, although only marginally.

performance on real datasets
we evaluate the performance of the different procedures on two real data sets:

• the brca data  <cit>  contains  <dig>  genes and was collected from  <dig> patients with hereditary breast cancer, who had mutations either of the brca <dig> or the brca <dig> gene .

• the lymphoma data  <cit>  contains  <dig>  genes and was collected from  <dig> patients with diffuse large b-cell lymphoma, comprising n <dig> =  <dig> survivors and n <dig> =  <dig> non-survivors.

here, the local fdr estimates are computed based on the mixture model . the estimate of f is computed by smoothing the histograms of the observed statistics, and similarly f <dig> from permuted test statistics. the permuted statistics are obtained from permutations of the group labels to generate the null distribution. technically, we also need an estimate π^0
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwfapacgaqcamaabaaaleaacqaiwaamaeqaaaaa@2f9a@ of the proportion of nonde genes, although for the purpose of comparing the different procedures, it does not matter which estimate, as long as we use the same value for all procedures, see methods. in fact, in comparing different fdr procedures, it is important that this parameter is set to the same value.

for each procedure, we rank the genes by their estimated fdr, and compute their estimated global fdr among the top-ranked genes as the cumulative mean of their local fdrs. the global fdr is then plotted as a function of the percentage of genes declared de. for comparison purposes, we also include the fdr as computed by the edge software.

the resulting oc curves are shown in figure  <dig>  we get the same ranking as for the simulated data: t1d performs worst and is easily bettered by s1d; t2d performs better than s1d for the 2% most highly regulated genes, and is equivalent otherwise; s2d has a slight advantage over t2d on the brca data. additionally, as a check that our implementation of odp is correct, we are happy to see that edge and s1d yield virtually identical fdr curves.

we  <cit>  have previously compared t2d with other procedures such as sam  <cit> , efron's modified t  <cit> , and an empirical bayes modification of the t-statistic  <cit> . to add more comparisons, we have run two procedures by pounds and cheng  for the two real data sets. we use their own software, with a little modification so that we can specify the π^0
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwfapacgaqcamaabaaaleaacqaiwaamaeqaaaaa@2f9a@ parameter to be the same as in the other procedures. the results in figure  <dig> show both splosh and robust fdr to perform worse than the other procedures. for these datasets, the robust fdr estimate coincides with the standard fdr estimate.

discussion
the main motivation for using the fdr has been that it offers a way of dealing with multiplicity that is less restrictive and more powerful than traditional p-value adjustments. the challenge is how to explicitly exploit the multiplicity by pooling information across genes in order to make the fdr even more powerful.

in the case of t1d, the test statistic is computed gene-by-gene and does not use information shared with other genes. moderated t-statistics  <cit> , which borrow strength across genes for estimating standard errors, are more powerful than simple t-statistics. the odp appears to be the ultimate in combining information, where to some extent all genes contribute to the statistic for each other gene. the fdr2d approach on the other hand augments the grouping of genes based on individual test statistics by sub-grouping them based on their variability. in all cases we find that when there are few instances of genes with similar variability, the performance of the different methods tends to converge towards the simple t1d  and 2).

from a practical point of view, it seems that the smoothing procedure underlying our implementation of fdr2d seems to work as well for the statistic logs^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgtbwugaqcaaaa@2deb@ in s2d as for the t-statistics in t2d, and arguably even better: when comparing figures  <dig> and  <dig> in this paper with figures  <dig> and  <dig> in  <cit> , we find in the former less of a tendency to underestimate the fdr for genes with small effect sizes, as discussed in the previous paper.

at first glance, the empirical odp statistic seems to rely on the assumption that the expression values for all genes are normally distributed. from a practical point of view, however, the empirical odp procedure works even if the normal assumption does not hold, because it relies on the permutation algorithm. in this sense, the normal densities in  only represent a scoring function that exponentially downweights contributions from genes with different mean structure and/or large variability. however, the performance of the empirical odp will depend on how precisely the normal assumption holds for the data at hand. some loss of the optimality property in the real data applications is probably due to non-normality. but even in the simulations, the empirical odp is not better than t2d. this can only mean that the presence of large number of nuisance parameters degrades the performance of odp.

CONCLUSIONS
the estimation of the nuisance parameters required to apply the odp in practice makes the procedure described in  <cit>  no longer optimal. we have shown in this paper that the combination of a conventional t-statistic with the standard error of the mean as described in  <cit>  can outperform the empirical odp. further improvements can be made by combining the odp test statistic with standard error information, but the gains are comparatively small.

the odp procedure exploits similarities in the distribution for a collection of genes, for example similarity in variance. when variances between genes are dissimilar, there is little gain by the odp compared to the standard t-statistic. one advantage of the odp over the modified t-statistics is that the adaption is done automatically, without calculating a model-based or heuristic fudge factor for the denominator.

the computational demand of calculating the odp statistic is a serious practical disadvantage: each density term f or g requires computation across the whole dataset, so a single odp statistic already involves substantial computations. doing this for the whole collection of genes and for repeated permutations of the group labels is an order of magnitude more laborious than the computation required for the standard statistics.

