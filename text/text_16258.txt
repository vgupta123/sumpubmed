BACKGROUND
in recent years, increasingly sophisticated computational methods have been developed to identify optimal genetic modifications to achieve a desired metabolic engineering objective. the problem of identifying optimal genetic modifications can be expressed in terms of operating state variables such as reaction flux, and control  variables such as the presence or absence of gene expression. the optimal design “tunes” these variables such that the solution meets the engineering objective while satisfying several constraints reflecting physicochemical considerations, experimental observations and assumptions about the physiology of the cell or organism. due to biological variability  <cit> , stochastic effects associated with gene expression, and imprecision in engineering implementation, it is questionable that enzyme levels can be precisely tuned to exactly match the target values calculated using computational design tools. more likely, the target enzyme levels, and thus the corresponding reaction flux capacities, can only be achieved with a finite degree of uncertainty. addressing uncertainty at the design stage is a challenging issue that has become increasingly important not only for engineering biological systems, but also man-made systems such as electronic devices. indeed, the past decade has witnessed a paradigm shift in design of electronics and computational design tools, where all modern electronic circuits are now designed to maximize tolerance to manufacturing and operational variations or to include tuning circuitry for post-manufacturing re-calibration. as metabolic engineering efforts progress from proof-of-principle to scaled-up manufacturing, computational methods to effectively address biological and engineering uncertainties at the design stage will become increasingly important in ensuring the identification of the most robustly optimal gene modifications.

the uncertainty in achieving targeted enzyme values suggests that the enzyme levels, and hence the corresponding flux carrying capacities , could be considered statistical distributions rather than fixed value parameters. in this statistical interpretation, a flux constraint in a conventional deterministic optimization problem represents the most conservative point in the flux capacity distribution, since a deterministic problem enforces all constraints with zero uncertainty. although the deterministic approach affords relatively straightforward problem formulation and is most commonly practiced  <cit> , this approach might lead to choosing an intervention set that may be far from optimal in a statistical sense. alternatively, a sampling-based optimization approach , with the obvious caveat of being computationally intensive, probabilistically explores a possible space of enzyme activities, i.e. flux capacity distributions, and solves for an optimal intervention set for each sampled instance of flux capacities. repeated sampling produces multiple intervention sets and a corresponding distribution of objective function values. another alternative for incorporating uncertainties in an optimization problem is chance-constrained programming , which selects an optimal solution with a user-defined degree of probabilistic confidence in meeting constraints. chance-constrained programming was first introduced in  <cit>  to solve the problem of temporal planning when uncertainty is present. since then, ccp has been utilized in numerous applications, including circuit sizing  <cit> , soil conservation  <cit> , ground water management  <cit> , energy management  <cit> , and molecular property optimization  <cit> .

current strain optimization methods generally seek to identify combinations of gene-level modifications that will result in an improvement of the desired cellular objective. these modifications are commonly gene deletions, but may be also up- or down-regulations of gene expression. a notable example of a computational method to identify gene knockouts is optknock  <cit> . this method uses bi-level programming to identify gene deletions that satisfy the coupled objectives of metabolite overproduction and biomass formation. another gene deletion strategy is genetic design through local search   <cit> , which employs a heuristic and flux balance analysis  to iteratively find sets of zero flux reactions  that would result in the maximization of the target reaction flux. other, related methods for large-scale problems involve metaheuristic approaches to iteratively improve a candidate set of gene deletions by generating and selecting variants of the candidate set via assessment of the objective function. an example of this approach is optgene, which uses an evolutionary algorithm to improve the set of gene deletions with respect to an objective function  <cit> .

optimization methods have also been described to identify targets for gene expression modification. optreg  <cit>  is a constraint-based method that uses bi-level programming to determine which sets of genes should be amplified or down-regulated to satisfy a coupled pair of engineering and cellular objectives. another class of computational strain design methods utilizes elementary mode  analysis. one recent example is computational approach for strain optimization aiming at high productivity , which ranks reactions based on their contributions to the yield of desired product  <cit> . another example is flux design, which selects reactions for up-regulation or deletion based on their correlation with the objective flux computed from ems that contribute to the target product  <cit> . despite increasing sophistication, these and other current computational strain design methods implicitly assume that reaction flux changes can be implemented precisely, and thus do not consider uncertainties as part of the problem formulation.

in this paper, we investigate three computational methods to address uncertainty in strain optimization. specifically, we compare two probabilistic methods, ccp based optimization  and sampling based optimization , against deterministic optimization . the performance of each method is tested on two metabolic models for which enzyme level changes and corresponding flux capacity distributions are estimated either from kinetic parameters or steady-state flux data. the performance of the solutions, i.e. predicted target fluxes and corresponding intervention sets, is evaluated using monte carlo simulations  designed to simulate the variable outcomes resulting from experimental implementation of the modifications specified by the optimization solutions.

methods
chance-constrained optimization 
figure  <dig> illustrates the difference between a deterministic and probabilistic interpretation of an uncertain upper-bound constraint on the flux of reaction j. in the deterministic interpretation, the value of flux vj of any feasible solution is enforced to be strictly less than all of the values in the upper-bound  distribution capju. this yields the constraint:

  probvj<capju= <dig> 

in the probabilistic interpretation, the constraint is not always satisfied, i.e. there is a nonzero probability that flux vj will be equal to or larger than some of the values in the distribution capju. in the case of ccp, the constraint is relaxed by introducing a parameter ε, which reflects the confidence level for the probability that the solution satisfies the constraint:

  probvj<capju≥1-ε 

to generalize the previous inequality to also consider the effects of up- or down-regulating the activity of an enzyme , we introduce two sets of binary decision variables yju and yjd. in this paper, we use the phrasing “up- or down-regulation” to describe engineering modifications that result in expression level changes of enzymes or groups of enzymes regardless of the method. a value of  <dig> indicates that the corresponding enzyme is up- or down-regulated, whereas a value of  <dig> indicates the corresponding enzyme expression is unchanged.

  probvj≤1-yju1-yjdssuj+yju1-yjdcapju+yjd1-yjucapjd≥1-ε 

where  <dig>  ssuj denotes the reference  state upper bound for reaction j. the fact that there are two random variables  does not pose a challenge in solving such an inequality, as at most one of them will have a nonzero coefficient at a time. mathematically, the sum of the two decision variables must be less than or equal to one , which simplifies the above inequality into the following:

  probvj≤ssuj+yjucapju-ssuj+yjdcapjd-ssuj≥1-ε 

a graphical illustration of the probabilistic constraints is shown in figure  <dig>  down-regulating a reaction decreases the upper bound, or the flux capacity. it could also decrease the lower bound to zero. the capacity change could leave the flux unchanged or decrease it below the level of the reference  state lower bound. up-regulating a reaction increases the flux capacity, but does not affect the lower bound. the flux value could remain the same or rise above the reference state upper bound. in this study, we model the capacity change resulting from a gene expression modification as a probabilistic  event, which leads to a flux capacity distribution .

various approaches have been developed to solve ccp problems based on properties such as the distribution of random variables, linearity, and type  of the chance constraints  <cit> . one method to solve a ccp problem is to convert the probabilistic constraints ) into their deterministic equivalents at their specified confidence level ε. this approach requires that the random variables of the problem are independent, and appear only in an exclusive linear form, such that the coefficients of all but one are always zero  <cit> . our formulation meets all of these conditions; therefore, the chance constraints can be converted into their deterministic equivalents. using the inverse of the cumulative distribution functions  for capju and capjd, inequality  can be reformulated as:

  vj≤ssuj+yjufj,u-1ε-ssuj+yjdfj,d-1ε-ssuj 

where fj,u- <dig> and fj,d- <dig> denote the inverse cdfs of capju and capjd respectively, which can be numerically calculated if needed.

recasting the chance constraints into the equivalent deterministic constraints, the uncertain optimization problem of maximizing the flux of a desired product through gene up/down-regulation operations can be formulated for a system of arbitrary size consisting of n metabolites and m reactions. without loss of generality, reversible reactions are split into forward and backward components such that the reaction set comprises only irreversible reactions. the chance-constrained cell optimization problem has the following constraints:

  maximizevtarget-α∑j=1myju+yjd 

s.t.

  ∑j=1msijvj= <dig> ∀i∈n 

  vbiomass≥ <dig> vbiomassmax 

  vj≤ssuj+yjufj,u-1ε-ssuj+yjdfj,d-1ε-ssuj,∀j∈m 

  vj≥sslj1-yjd,∀j∈m 

  ∑j=1myju+yjd≤l 

  yju+yjd≤ <dig> ∀j∈m 

  yjd+ykd≤ <dig> yju+yku≤ <dig> ∀j∈m,k=j'sbackwardcounterpart 

  yju∈ <dig> ,yjd∈ <dig> ,∀j∈m 

the main objective of the problem is to maximize the target reaction flux vtarget. it is expected that the optimal value of vtarget will increase monotonically with l, the number of allowed interventions . on the other hand, the engineering cost is also expected to increase with the number of interventions. therefore, the objective function in  also includes the term -α∑j=1myju+yjd, which imposes a small penalty α for each added intervention, and balances the optimal flux of the target reaction against the number of required interventions. constraint  represents the steady state assumption that the rate of production of each intracellular metabolite is equal to its rate of consumption. constraint  guarantees a minimal growth rate equaling at least 1% of the theoretical maximum of the wild-type  organism. a minimal growth rate constraint is required to guarantee that the cell remains viable. this parameter can be adjusted by the user based on the metabolic model, available data and expectations for cell viability, which does not alter the optimization algorithm. to maximize the growth rate while simultaneously maximizing a certain target metabolite, a bi-level optimization with two objectives  can be applied in place of the constraints  and . however, linear bi-level programs are np-hard  <cit>  and there are no efficient algorithms to solve large-scale problems  <cit> . constraint  sets the upper bound flux capacity for each reaction j. constraint  sets the lower bound flux for each reaction j to sslj  or zero, based on the value of the binary variable yjd. constraint  sets an upper bound on the number of allowed interventions. inequality  ensures that enzyme manipulations are exclusive, i.e. a reaction can be either up- or down-regulated in a solution, but not both. similarly, constraint  guarantees that the forward and backward directions of a reversible reaction are not both up- and down-regulated at the same time. constraint  specifies that the decision variables yju and yjd can only be  <dig> or  <dig> 

deterministic optimization 
the deterministic formulation  can be derived from the ccp formulation by setting ε =  <dig> in , i.e. vj is strictly less than all possible values the random variables capju or capjd can take.

monte carlo-based optimization 
chance-constrained optimization can be emulated by repeatedly solving the fixed constraint  optimization problem in which the constraint parameters  are set to randomly drawn values using a mc sampling procedure for each instance of the problem. the mc sampling requires a priori knowledge of the distributions for the flux capacities . the procedure for computing the distributions is described below. using the randomly drawn set of flux capacities, the capacity constraints become fixed constraints. effectively, we replace the inequality in  with the constraint below:

  vj≤ssuj+yjuxju-ssuj+yjdxjd-ssuj,∀j∈m 

where xju and xjd are the randomly drawn set of flux capacities. each mc sample, i.e. set of randomly drawn flux capacities, defines an instance of an optimization problem. the solution to this optimization problem is a set of interventions and a corresponding optimal flux value for the target reaction. repeating the process  many times, we obtain a distribution of optimal target flux values.

computing capacity distributions
traditionally, a gene up/down-regulation operation has been modeled as a deterministic event leading to a fold-change in the level of the corresponding enzyme, and hence a fold-change in the flux capacity of the reaction catalyzed by the enzyme. here, we model enzyme level modification as an uncertain event using a probability distribution. we assume a normal distribution  <cit>  with an average fold-change of μ =  <dig> following gene up-regulation and a spread of δ = 6σ =  <dig>  where σ denotes the standard deviation. the average fold-change value reflects experimental data reported in gene over-expression studies involving mammalian cells, specifically adipocytes  <cit> . we note that the average fold-change value is a user-specified parameter that can be adjusted to reflect different cell types and experimental data, and thus does not lead to loss of generality. the spread δ is chosen so that μ - δ/2 >  <dig>  which ensures that the flux capacity after up-regulating the enzyme level is higher than the unmodified state. a decrease in enzyme level, and hence reaction flux capacity, is modeled by a normal distribution nd with an average fold-change of μ =  <dig>  and a spread of δ =  <dig> 

based on the probabilistic interpretation of fold-changes in enzyme levels resulting from gene modifications, we also estimate the resulting reaction flux capacities as probability distributions. we use two different estimation methods depending on whether the model is kinetic or stoichiometric. in the case of a kinetic model, a fold-change in enzyme level is assumed to directly correlate with a fold-change in the maximal reaction velocity . here, the maximal reaction velocity has the same units as reaction flux. therefore, flux capacity distributions were calculated by simply multiplying the enzyme fold-change distributions with vj,max. in the case of a stoichiometric model, the distributions of flux capacities are approximated using enzyme control flux  analysis  <cit> . briefly, ecf analysis calculates the effect of enzyme level changes on flux distribution based on elementary mode analysis  <cit>  and a power law model for the relationship between reaction flux and enzyme activity. typically, the ecf problem is underdetermined, and the solution is obtained as a range of minimal and maximal flux for each reaction. we use the maximal flux value as the corresponding reaction flux capacity. the maximal flux values, calculated using sample points from the distributions of enzyme level modifications  and nd), form a capacity distribution.

monte carlo-based evaluation  framework
we evaluate ccopt, detopt, and mcopt using monte carlo  simulations designed to mimic the expected variations in outcomes when the intervention sets identified by the three different optimization methods are experimentally implemented. for ccopt and detopt, each solution is a single optimal flux of the target reaction and a corresponding set of interventions. the mcopt solution comprises a distribution of maximal fluxes and their corresponding sets of interventions. to compare these solutions, we perform separate mceval simulations using the interventions obtained from ccopt, detopt, and mcopt, and apply flux balance analysis   <cit>  with the objective function of maximizing the target flux.

  maximizevtarget 

s.t.

  ∑j=1msijvj= <dig> ∀i∈n 

  vbiomass≥ <dig> vbiomassmax 

  ∀j∈m,vj≤ssuj,ifreactionjisunmodifiedxju,ifreactionjisup-regulatedxjd,ifreactionjisdown-regulated 

  ∀j∈m,vj≥sslj,ifreactionjisup-regulatedorunmodified <dig> ifreactionjisdown-regulated 

in the fba problem, the flux capacity constraints are drawn from the capacity distributions ) if the corresponding reaction  belongs to the optimized set of interventions. otherwise, the capacity constraints are set to maximal steady state value  calculated for the unmodified reference state. similar to mcopt, mceval repeatedly solves a series of optimization problems to generate a distribution of optimal target flux values. unlike mcopt, mceval does not seek to identify an intervention set reflecting decisions on enzyme activity modification. rather, each instance of mceval simply solves for the optimal flux and the corresponding flux distribution based on capacity constraints specified by the ccopt, detopt, or mcopt solution that is to be evaluated.

RESULTS
to assess the benefits and limitations of the optimization methods, we compare their performance using test cases involving both a kinetic and a stoichiometric model. the kinetic model describes the metabolism of chinese hamster ovary  cells in fed-batch culture  <cit> . the stoichiometric model describes the metabolism of adipocytes undergoing differentiation and growth  <cit> .

cho cell model
the cho cell model comprises  <dig> metabolites and  <dig> irreversible reactions. the kinetic parameters of the model were previously estimated by fitting the model equations to experimentally obtained metabolite time course data  <cit> . these parameters are used to estimate the effects of enzyme activity increases and decreases on the corresponding reaction flux capacity distributions. the flux capacity distributions for the adipocyte model are estimated from steady state metabolic flux data obtained in previous studies  <cit> . additional details of the model including reaction definitions are provided as additional file  <dig>  the test objective is the synthesis of a recombinant protein product, a therapeutic antibody.

we first estimate the steady state flux values of a nominal reference state and the corresponding capacity distributions. the reference state fluxes  are estimated through a linear programming formulation that maximizes/minimizes each reaction flux subject to

  sv=0;0≤vj≤vj,max;vj=vjmeas,j∈measureddata 

where vj,max is the maximal velocity of reaction j and measureddata is a set of measured exchange flux values for glucose, glutamine, glycine, glutamate and ammonia. the maximal velocities  are reported in  <cit>  for only  <dig> of the  <dig> reactions in the model that explicitly defined with rate expressions. to calculate the vj,max values for the remaining reactions, we solve a series of flux maximization problems subject to the  <dig> pre-defined maximum velocities. the capacities reflecting up/down-regulations of enzyme activities capju/capjd are obtained by multiplying the maximum velocities with the assumed enzyme activity distributions:

  capju/d=vj,maxnu/dμ,σ <dig> 

we compare the intervention sets obtained from ccopt with ε =  <dig>  and ε =  <dig>   and those from detopt, and evaluate the intervention sets using monte carlo simulations . in figure  <dig>  the intervention sets  identified by each optimization method are shown above their corresponding optimal target flux values. empty sets represent no identified interventions. for l =  <dig>  detopt and ccopt at ε =  <dig>  and ε =  <dig>  all select reaction  <dig>  which is the lumped antibody synthesis reaction. for l =  <dig>  ccopt adds reaction  <dig> to form an intervention set of { <dig>  17}. up-regulating reaction  <dig> increases the synthesis of cysteine, which could be a limiting reactant. as reported in  <cit> , one of the rate-limiting steps of antibody production in cho cells is the folding and assembly of polypeptides in the endoplasmic reticulum, which requires cysteine residues. for l =  <dig>  ccopt further adds reaction  <dig>  which lumps together several steps in glycolysis. up-regulating the flux through glycolysis increase the supply of pyruvate for oxidation in the tricarboxylic acid  cycle, which in turn could provide additional energy for antibody synthesis  <cit> . for l =  <dig>  ccopt adds reaction  <dig>  which acts to balance the cytosolic redox by oxidizing nadh and possibly relieves feedback inhibition of glycolysis.

compared to ccopt, detopt predicts smaller maximal antibody synthesis rates  due to the conservative choice of reaction flux capacities. the maximal synthesis rate predicted using ccopt is more than twice the flux predicted by detopt . the intervention set identified by detopt consists of only a single reaction even when the maximal number allowed interventions is raised, indicating that the deterministic method does not fully utilize the degree of freedom available in the problem.

figure  <dig> shows the distribution of maximum antibody production rates obtained using mceval for the intervention sets reported in figure  <dig>  in all cases, the maximum flux predicted by detopt falls outside the probable  range calculated by mceval, whereas the maximal flux predicted by ccopt falls within this range. when only one intervention is allowed , the selected reaction is the same for ccopt and detopt. however, the flux predicted by ccopt is higher, and is also more reliable in a probabilistic sense. when the degree of freedom is higher , and different intervention sets are selected, mceval calculates higher probable ranges for the intervention sets identified by ccopt compared to detopt. for example, for l =  <dig>  the probable range for ccopt lies between  <dig> and 2870 nmol/106cells/day whereas both the 5th and 95th percentile values for detopt are at  <dig> 

figure  <dig> shows the distribution of solutions resulting from  <dig> iterations of the monte carlo optimization method . mcopt generates the same solution as ccopt and detopt for l =  <dig> and ccopt for l =  <dig>  for l =  <dig>  mcopt identifies four sets of interventions: { <dig>   <dig>  17}, { <dig>   <dig>  17}, { <dig>  17}, and {17}. the first set is dominant at a frequency of  <dig> %, and matches the ccopt solution. for l =  <dig>  the trend is the same as l =  <dig>  with one dominant solution  that matches the corresponding ccopt solution. this set also corresponds to the highest predicted target flux among all intervention sets comprising four reactions.

in the case of l =  <dig>  the aggregate effect of uncertainties in flux capacities is to result in a normally distributed target flux. however, this is not the case for l <  <dig>  where the dominant target flux values generated by mcopt distribute narrowly with nearly zero spread. moreover, the mean target flux values rise only incrementally from l =  <dig> to  <dig>  suggesting that the probabilistic outcomes accumulate at the lower bound of the probable range due to one or more bottlenecks in the network that are not relieved until all  <dig> reaction flux capacity modifications are introduced.

similar to the ccopt and detopt solutions, the mcopt solutions are evaluated using mceval . the mceval results for l =  <dig> and  <dig> are identical to the mcopt results for l =  <dig> and  <dig> shown in figure  <dig>  respectively. for l =  <dig>  mcopt generates two sets of interventions, where one dominant set is identified with  <dig> % frequency. results of mceval confirm that this solution  indeed has a higher probable target flux value. a similar trend is observed for l =  <dig>  the set with the highest probable target flux values is identical to the ccopt solution and the dominant  mcopt solution. the probable ranges  calculated by mceval for the mcopt intervention sets { <dig>   <dig>   <dig>  17}, { <dig>   <dig>   <dig>  17} and { <dig>  17} are ,  and  nmol/106cells/day, respectively. the mceval simulations produce a normal distribution of target fluxes only for the solution { <dig>   <dig>   <dig>  17}, presumably because only this set of interventions sufficiently relieves the flux capacity bottlenecks in the network. the results of these evaluations indicate that ccopt and mcopt essentially identify the same best intervention sets, where ccopt arrives at the results without requiring the sampling run-time cost of mcopt.

adipocyte model
in the second case study using the adipocyte model  <cit> , we maximize the production of tripalmitoylglycerol as a representative triacylglycerol  in adipocyte lipid droplets  <cit> . this model includes  <dig> irreversible reactions and  <dig> metabolites. the details of the model are provided as additional file  <dig>  unlike the cho cell case study, we did not use vj,max values to estimate the flux capacities and reference state fluxes. instead, the reference state flux values are calculated by maximizing each reaction subject to a set of measured untreated control data reported in  <cit> . to estimate the flux capacity distributions, enzyme control flux  analysis  <cit>  is used, where the analysis calculates the impact of a change in an enzyme’s activity on the steady state flux distribution of the metabolic network. the first step in calculating the distributions is to generate all elementary modes . for the base adipocyte model,  <dig>  ems were identified using efmtool  <cit> . in the second step, em coefficients  are calculated through an iterative process. the third step is to estimate the emcs for a change in enzyme activity. an increase or decrease in enzyme activity is modeled by a normal distribution nu or nd as described in methods . the fourth step is to calculate the flux distributions using the adjusted emc vectors. since the enzyme activity change is described by a distribution, multiple flux distributions are calculated. for each reaction in the network, the reaction flux capacity is the set to the maximal flux value of the reaction from the flux distributions. repeating the third and fourth steps for all reactions generates a statistical distribution of flux capacities for the network. the maximum tg production rate and intervention sets obtained from ccopt and detopt are shown in figure  <dig>  for both ccopt and detopt, the maximal predicted target flux increases with the number of allowed interventions. as was the case for the cho cell model, ccopt predicts a larger maximal flux and generates a more diverse set of solutions compared to detopt. in general, detopt underutilizes the degrees of freedom available at larger l values. for example, the detopt solution comprises only  <dig> interventions when up to  <dig> interventions are allowed, whereas the ccopt solution utilizes all  <dig> allowed interventions. a second general trend is that the smaller sets of interventions are subsets of the larger sets. an interesting observation is that a single intervention  yields no change in the predicted maximal flux. this is expected, as reactions  <dig> and  <dig> are in series, and both are required for tg synthesis. a change in one without a change in the other merely shifts the limiting capacity to the unchanged reaction.

reaction  <dig> is a part of the tca cycle. reactions  <dig> and  <dig> are palmitate biosynthesis and tripalmitoylglycerol biosynthesis, respectively. all three reactions directly impact synthesis of tg, which is formed from esterification of palmitate with glycerol phosphate, with the latter derived from glycerone phosphate. previous reports  <cit> , including our own work  <cit> , have shown that the addition of long-chain fatty acids stimulates cellular tg accumulation. at first glance, the intervention targets selected by ccopt appear trivially intuitive. however, other, equally intuitive alternatives also exist, which were not selected. for example, another intuitive intervention to increase net tg accumulation is to down-regulate lipolysis . this intervention was not selected, because the reference  state lower bound for reaction  <dig> is already zero, and a further reduction would have no impact on tg production. in this regard, the optimization results depend not only on the model, but also on the observed reference state.

as was the case for the cho cell model, the results of ccopt more closely match the results of mceval simulations compared to detopt . since neither detopt nor ccopt identified any solutions for l =  <dig>  mceval simulations are not shown. for l =  <dig> and  <dig>  the maximal fluxes predicted by detopt  lie at the lower end of the distributions generated by mceval. in contrast, the maximal fluxes predicted by ccopt consistently fall in the probable  range  of the mceval distributions. for l =  <dig>  the 95th percentile value obtained from mceval simulations of the ccopt intervention set is significantly larger than the 95th percentile value obtained from mceval simulations of the detopt intervention set. additionally, the flux values predicted by ccopt with and are both in the probable range as calculated by mceval.

applying mcopt to the adipocyte model generates one solution for l =  <dig> and  <dig> and two solutions for l =  <dig> . the solutions with the highest frequency are identical to the ccopt solutions. these solutions are {}, { <dig>  24} and { <dig>   <dig>  26} for l =  <dig>   <dig>  and  <dig>  respectively, and occur with 100%, 100% and  <dig> % frequency. of the two mcopt solutions for l =  <dig>  the dominant solution has the higher probable target flux values, which is consistent with the results of mceval simulations .

computational complexity and scalability of methods
our optimization problems  are formulated as mixed integer linear programming . a milp problem requires a subset of variables to take on integer values, while the other variables can take on non-integer values. this problem is np-hard  <cit> , and thus it is unlikely that there exists an efficient  algorithm to obtain a globally optimal solution. in the present study, we implemented our optimization methods  using the gnu linear programming kit   <cit>  in matlab. the runtime of our computational experiments solving the milp problems was on the order of a few seconds on a core i <dig>  <dig>  ghz cpu.

in addition to the scalability issue inherent to milp problems, another computational challenge lies in estimating the flux capacity distributions. for the stoichiometric model of this study, we used enzyme control flux analysis   <cit>  to obtain these distributions. the ecf method in turn relies on elementary mode  analysis, which can be applied to metabolic models comprising < ~ <dig> reactions, but remains intractable for genome-scale models. an alternative strategy is to model the fold-change in flux capacity, i.e. enzyme activity, resulting from a gene expression modification using a probability distribution, e.g. a normal distribution. this strategy requires knowledge of maximal enzyme velocities . if these parameters are not known, they may be estimated from fba, which has been demonstrated on genome-scale models.

these types of limitations, while not trivial, are comparable to other computational strain design methods. for example, bi-level optimization, used in optknock  <cit> , is also np-hard  <cit> , and thus can be intractable for large-scale problems. as an np-hard problem, the runtime grows exponentially with the number of allowed reaction modifications  <cit> . methods that rely on em analysis  <cit>  face a similar limitation as our capacity estimation problem, as the analysis is generally only practical for small to mid-scale models. methods based on local search  <cit>  or metaheuristics  <cit>  are computationally less prohibitive than milp, and likely offer the best alternative for large-scale problems. on the other hand, these methods cannot guarantee global solution optimality, and may arrive at solutions that are far from exact.

CONCLUSIONS
this study investigates three distinct ways of capturing uncertainty about parameter values when formulating an optimization problem with the objective of identifying targets for enzyme activity adjustments that maximize the production of a desired molecule. the three approaches are chance-constrained programming , monte carlo sampling-based solution of the uncertain problem , and deterministic optimization based on worst-case assumptions . evaluation of the approaches for two test cases  using monte carlo simulations  shows that a more sophisticated probabilistic approach such as ccopt has several advantages compared to a conservative conventional approach like detopt. chance-constrained programming explores a larger portion of the solution space and is able to find a more diverse set of options. additionally, ccopt consistently outperforms detopt in terms of predicting the more likely maximum of the objective function value. comparisons of the intervention sets from ccopt and detopt using mceval shows that the maximal fluxes predicted by ccopt was always in the probable  range calculated by mceval, whereas the maximal fluxes predicted by detopt typically lies outside of this range. when compared to the sampling-based optimization approach , ccopt consistently finds the solution most frequently selected by mcopt, but at a fraction of the computational cost .

the ccopt formulation can be readily extended to capture other types of uncertainties, such as biological variability in measured data and cell transfection efficiency, making ccopt an effective technique for probabilistic strain optimization.

abbreviations
ccopt: chance-constrained optimization; detopt: deterministic optimization; mcopt: monte carlo-based optimization; mceval: monte carlo evaluations; ccp: chance-constrained programming; cdf: cumulative distribution functions; ecf: enzyme control flux; em: elementary modes; fba: flux balance analysis; cho: chinese hamster ovary; tca: tricarboxylic acid; tg: tripalmitoylglycerol; glpk: gnu linear programming kit; milp: mixed integer linear programming.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
sh and kl conceived the idea of applying probabilistic methods to gene modifications. mo provided probabilistic expertise. all authors participated in formulating the problem. my implemented the methods, generated all the figures and drafted the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
detailed models of cho cell and adipocyte.

click here for file

 acknowledgement
this work was supported by the national science foundation under grant no.  <dig> and the wittich family foundation.
