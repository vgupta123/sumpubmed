BACKGROUND
membrane proteins are involved in a variety of important biological functions  <cit>  where they play the role of receptors or transporters. the number of transmembrane segments of a protein and some characteristics such as loop lengths can identify features of the proteins, as well as their role  <cit> . therefore, it is very important to predict the location of transmembrane domains along the sequence, since these are the basic structural building blocks defining the protein topology. several works have dealt with this prediction task from different approaches, mainly using hidden markov models   <cit> , neural networks  <cit>  or statistical analysis  <cit> . a rich literature is available on proteins prediction. for reviews on different methods for predicting transmembrane domains in proteins, we refer the reader to  <cit> .

this work addresses the problem of protein transmembrane domains prediction by making use of a grammatical inference  based approach. gi is a particular case of inductive inference, an iterative process that takes into account a set of facts and tries to obtain a model consistent with the available data. in gi the model resulting from the induction process is a formal grammar  inferred from a set of sample strings, composed by a set m+ of strings belonging to a target formal language and, in some cases, another set m- of strings that do not belong to the language. the results of the inference process gives as the result a language  that, in essence, models all the common features of the strings. this grammatical approach is suitable for the task due to the sequential nature of the information. some works apply formal languages methods to molecular biology  <cit> . figure  <dig> depicts a general gi scheme. several classifications of the gi algorithms can be made, for instance: when both sets are non-empty we remalgoalgorithm refer to complete presentation algorithms; positive presentation algorithms are those that use an empty m- set; taking into account these algebraic properties of the obtained languages, it is possible to distinguish between characterisable and non-characterisable algorithms. it is difficult to identify what information is suitable to be considered into m-, therefore we will take into account only positive presentation in our approach. for more information, we refer the reader to  <cit> .

usually, the model used in gi is a finite state abstract machine commonly named finite automata. hmms are closely related to finite automata, and therefore our approach is also related to several works that succesfully tackle this task  <cit> . nevertheless, it is to note that the topology of a hmm, number of states and their connection, is a priori fixed by an expert that takes profit from known information. once the topology is fixed, the available data is used to set the probability of each transition of the hmm. as stated above, the input of a gi algorithm is a set of sequences, therefore no aid from an expert is needed, because both the topology of the automaton and the probability between states is automatically stablished by the algorithm.

generally speaking, hmms provide a good solution when the topology of the hmm can be fairly set. in that case, the sequences provided are used just to set the transition probabilities among states. a gi approach tries to extract more information from the sequences and provides good prediction tools using only sequential information. the most important drawback of gi is the lack of enough data to infer proper models.

gi has been used previously in various bioinformatics related tasks, such as gene-finding or prediction of coiled coil domains  <cit> . the good performance of those works leds us to apply gi algorithms to the prediction of other domains in proteins, such as transmembrane segments.

our work takes into account a set of protein sequences with known evidence of transmenbrane domains. firstly, these sequences are processed in order to distinguish among inner, outer and transmembrane residues. this labelling allows to obtain an even linear structure . it is possible to model this structure by using an even linear language  that can be learned using gi techniques. the obtained language is then used to build a probabilistic transducer . the resulting transducer allows to process any unknown protein sequence to obtain a transduction. the transduction shows those detected transmembrane domains. the experimental results have been compared with tmhmm  <dig>   <cit> , pred-tmr  <cit> , prodiv-tmhmm  <cit> , hmmtop  <dig>   <cit> , phobius  <cit> , tmpred  <cit>  and memsat <dig>  <cit> .

RESULTS
introduction
we consider the prediction of transmembrane domains as a transduction problem. that is, given an amino acid sequence, the output of our system is a sequence with the same length which distinguishes between those amino acids that are within a transmembrane domain and those that are not.

the available data are transformed in a training set with even linear structure. an item of the data set is a string whose first half is made up by the symbol sequence of the protein and the second by the symbols of the expected output string in reverse order. in order to learn the ell with this set, we considered as the main feature the segments of a given length k set as a input parameter. the class of ktss languages is a well-known subclass of the regular languages and it is characterized by the set of segments of length k that appear in the words of the language, therefore, we can take profit of previous learning results in order to address this task  <cit> .

the transducer is obtained using the structure of the inferred elg . the general method is described in algorithm  <dig>  please refer to section notation and definition to details.

algorithm  <dig> transmembrane grammatical inference approach
input:

• a set p of amino acid sequences with known transmembrane domains.

• a set l of domain labeled sequences. each string x in p has its corresponding string lx in l.

output:

• a transducer to locate transmembrane domains.

method:

• combine the sets p and l to obtain the training set m with strings xlxr

• apply to the strings in m the transformation function σ

• apply a gi algorithm for  regular languages

• undo the transformation σ to obtain the elg from the regular language

• return the transducer obtained from the elg

end

the returned transducer can be used to analyse problem sequences to obtain the corresponding transduction.

datasets
due to the fact that each approach to transmembrane prediction uses its own dataset, in order to test our approach six different datasets has been considered. the first one was a set of  <dig> membrane proteins used in  <cit> , which we refer to as the tmhmm set. experimental topology data is available for these proteins, most of them have been analysed with biochemical and genetic methods , and only a small number of membrane protein domains of this dataset have been determined at an atomic resolution. the dataset contains  <dig> multi-spanning and  <dig> single-spanning proteins. the original dataset was larger, but those proteins whith conflicting topologies for different experiments were not included.

the second set used was tmpdb  <cit> , whose latest version  contains  <dig> transmembrane protein sequences . the topologies of these sequences are based on definite experimental evidences such as x-ray crystallography, nmr, gene fusion technique, substituted cysteine accessibility method, asp-linked glycosylation experiment and other biochemical methods. the third and fourth datasets are subsets of tmpdb, where homologous proteins have been removed: the third set, tmpdb-α-nr, contains  <dig> alpha-helix non redundant proteins; and the fourth set tmpdb-αβ-nr, has been obtained by adding  <dig> β-barrel proteins to the third set.

the fifth dataset used is the 101-pred-tmr database, a set of  <dig> non-homologous proteins, extracted form swissprot database, used in  <cit> . these proteins were selected from a set of  <dig> proteins, discarding those with more than 25% of similarity.

the last dataset used was the mptopo dataset  <cit> . in its last version  the set contains  <dig> proteins:  <dig> of them β-barrels and the rest α-helix transmembrane. all the segments have been experimentally validated. the 3d structure of  <dig> of these proteins has been determined using x-ray diffraction or nmr methods, therefore, these transmembrane segments are known precisely. the rest of transmembrane segments correspond to  <dig> helices that have been identified by experimental techniques such as gene fusion, proteolytic degradation, and amino acid deletion. the proteins whose topologies are based solely on hydropathy plots have not been included in the dataset.

codification
protein sequences can be considered as strings from a  <dig> symbols alphabet, where each symbol represents one of the amino acids. in order to reduce the alphabet size without loss of information, we considered an encoding based on some properties of the amino acids . the table  <dig> shows the correspondence of each amino acid for dayhoff encoding. this encoding has been previously used in some gi papers  <cit> .

the dayhoff encoding takes into account the properties of each residue: sulfur polymerization, small, acid and amide, basic, hydrophobic and aromaticity. for example, for the input protein segment svmedtllsvlfetynpkvrpaqtvgdkvtvrv the output encoded sequence would be: beeeccbeebeefcbfcbdedbbcbebcdebede

performance measures
several measures are suitable to evaluate the results. some of them, addressing gene-finding problems, are reviewed in  <cit> . this measures can also be applied to functional domain location tasks. among all the proposed measures, sensitivity and specificity are probably the most used. intuitively, sensitivity  measures the probability of predicting a particular residue inside a domain. specificity  measures the probability of predicted residues to be actually into a domain. therefore, sn and sp can be computed as follows:

 sn=tptp+fnsp=tptp+fp 

where:

true positives : correctly localized amino acids into a tm domain.

true negatives : correctly annotated amino acids out of a tm domain.

false positives : amino acids out of a tm domain annotated as belonging to a domain.

false negative : amino acids into a tm domain not correctly localized .

note that neither sn nor sp, took individually, constitute an exhaustive measure. a single value that summarizes both measures into a better one is the correlation coefficient , also referred to as mathews correlation coefficient  <cit> . it can be computed as follows:

 cc=−⋅⋅⋅ 

unfortunately, although cc has some interesting statistical properties  <cit> , it has also an undesirable drawback. it is not defined if any factor of the root is equal to zero. in the literature there exist some measures that overcome this inconvenient, in this work we will use the approximate correlation  which is defined as follows:

 acp=14ac=⋅ <dig> 

we have to note that we were not able to calculate cc for every sample of the testing set . in those cases, the samples were not taken into account. the approximate correlation ac has a 100% coverage, including those samples for which it was not possible to calculate cc or sp. this can explain the relevant difference between ac and cc observed in some experiments. in addition to this, we have used the common segment-based measure segment overlap,  defined by  <cit> :

 sovδobs=1n∑smin−max+1+δmax−min+1len 

where n is the total number of residues observed within all the domains of the protein, s <dig> and s <dig> are two overlaped segments, e is {end; end}, b is {beg; beg} and δ is a parameter for the accepted  deviation. we used a value of δ =  <dig> 

we have also calculated the number of segments correctly predicted at three accuracy thresholds: 100%, 90% and 75%, that is, number of segments with the 100%, 90% or more, and 75% or more of their amino acids are correctly predicted. this measure is similar to sensibility, but it is based on segments. therefore it is necessary to calculate also the sp measure in order to complement it. this measure allows to obtain a reliable evaluation for those segments that contain false negatives not only at the extremities of the segment. for example, this occurs when a viewed segment is recognized as more than one segment, and there are some false negatives between two of this predicted segments. figure  <dig> shows how this measure is calculated.

experimentation
note that our approach needs some information to learn a model. in order to obtain probabilistic relevance in the test of our method, we followed a leaving one out scheme: each sample protein of the dataset is annotated using as training set all the other samples. the process is repeated until all sample proteins have been used as test sequences. we carried out various experiments, taking into account different annotations for the test sequences. each experiment was carried out over the six databases tmhmm, tmpdb, tmpdb-α-nr, tmpdb-αβ-nr, 101pred-tmr and mptopo. note that all these sets but tmpdb have non homologous sequences.

we hereby provide a description of each experiment, all the experiments but the last consider a previous reduction using the dayhoff code: the first one  considered a two-classes encoding, that is, residues inside and outside a transmembrane domain; the second experiment  added another class in order to consider the topology of the protein ; the third experiment  also included a class to distinguish among transmembrane domains with previous inner and outer regions; the fourth  experiment took into account the previous encoding with a special labelling of the last five residues of each region preceding a transmembrane one; the fifth experiment  added special symbols to track the transition to a transmembrane and out to one; the last experiment  did not consider the dayhoff encoding and used the annotation of the second experiment.

each of the experiments builds a different model for the language of the tm proteins, that highlights differents propierties of them, by searching different patterns among the amino acids, depending on whether they belong, for instance, to a tm zone or not , to an inner or outer zone , to a tm domain with previous inner or outer regions , to the sequence of the last  <dig> residues that precede a tm segment , or to the set of amino acids that represent a transition from a tm zone to an inner or outer zone, or vice versa . figure  <dig> shows the annotation and encoding of an example sequence for each different experimental configuration.

once encoded the sequences, and for each of the described encodings, a set of experiments were run to test the best learning parameter of the inference algorithm. the best accuracy was obtained in the experiment with the configuration of exp <dig> and exp <dig>  the hmm-based methods we compared our system with, obtain a slightly better precision. the difference in results can be explained with the fact that gi algorithms need a greater quantity of data than the amount needed by hidden markov models in order to achieve the same accuracy.

the main advantage of our approach is that it learns the topology of the model from samples, without the need of the external knowledge, as in hmm-based methods, where states and edges are determined by an expert. in a gi method, the automata are built by the algorithm, which stablishes the topology, number of states, the transitions or edges between states and probabilities of transition. tables  <dig>   <dig>   <dig>   <dig>   <dig> show the experimental results of the fifth and sixth experiments  with the six datasets.

experimental results and comparison with results of other methods, with the tmhmm α-helix database. as discussed in the performance measures section, 100%, ≤ 90% and ≥ 75% stand for segments correctly detected in that percentage.

experimental results and comparison with results of other methods, with the tmpdb α-helix database. as discussed in the performance measures section, 100%, ≥ 90% and ≥ 75% stand for segments correctly detected in that percentage.

experimental results and comparison with results of other methods, with the tmpdb-α-nr α-helix database. as discussed in the performance measures section, 100%, ≤ 90% and ≥ 75% stand for segments correctly detected in that percentage.

experimental results and comparison with results of other methods, with the mptopo α-helix database. as discussed in the performance measures section, 100%, ≥ 90% and ≥ 75% stand for segments correctly detected in that percentage.

experimental results and comparison with results of other methods, with the 101pred-tmr α-helix database. as discussed in the performance measures section, 100%, ≥ 90% and ≥ 75% stand for segments correctly detected in that percentage.

although it may seem erroneous or non-sense to build a model to predict both α and β transmembrane domains, we would like to illustrate with this experiment the way a gi approach distinguishes from other approaches: if the dataset contains enough data  from differents classes , the model obtained should be able recognize all the different patterns. table  <dig> compares the results of the experiment carried out over tmpdb-α-nr and tmpdb-αβ-nr datasets. the results with tmpdb-αβ-nr are slightly worse, but it can be explained because the set of β-barrel proteins contains only  <dig> sequences, and it is difficult to learn an accurate model from this set. in fact, when we train and test with only this set of β-barrel proteins  the result are roughly worse:   <dig>  for sp,  <dig>  for ac and  <dig>  for sov3obs; and in exp6: <dig>  for sp,  <dig>  for ac and  <dig>  for sov3obs.

CONCLUSIONS
this work addresses the problem of the localization of transmembrane segments within proteins by making use of grammatical inference  algorithms. gi has been effectively used in some bioinformatic related tasks, such as gene-finding or prediction of coiled coil domains. igtm exploits the features of proteins by using even linear languages as the inferred class of languages. we tested different labellings for the input sequences, with the best accuracy achieved using a labelling that takes into account several changes in the sequence topology: from inside and outside the membrane to it and vice versa. we compared our method with other methods to predict transmembrane domains in proteins, obtaining slightly less accuracy with respect to them. this should be due to the fact that in gi the training phase need more data than the most common approach, based on hidden markov models. in addition to this, many of the available prediction tools are closed, that is, there is no way to know exactly the training set used by the tools which we have compared igtm with, therefore it is possible that some of our six datasets included proteins used by these tools in the training phase . the same problem happens with online prediction tools, where the data considered to build the tools is not available. then, since the other methods can have been trained on sequences that share homology with the test set , the comparison could be not very reliable. however, the obtained results show that gi can be used effectively in bioinformatics related tasks. furthermore, the main advantage of gi when applied to bioinformatics tasks is that an expert is not needed in order to give additional information . an online version of igtm is publicly available at 

it remains as a future work to use this method together with another one . this could lead to improve the performance. at present we are testing other inference algorithms to learn the automata, the use of new codings to the sequences  <cit> , and the consideration of new datasets .

