BACKGROUND
as with many types of experimental data, expression data obtained from microarray experiments are frequently peppered with missing values  that may occur for a variety of reasons. randomly scattered mvs may be due to spotting problems, poor hybridization, inadequate resolution, fabrication errors, or contaminants on the chip including scratches, dust, and fingerprints. because many down-stream microarray analyses such as classification methods, clustering methods, and dimension reduction procedures require complete data, researchers must either remove genes with one or more mvs, or, preferably, estimate the mvs before such procedures can be employed. consequently, many algorithms have been developed to accurately impute mvs in microarray experiments  <cit> .

the first evaluation of mv estimation methodology in microarray data was reported by troyanskaya et al.  <cit> , who compared a variety of algorithms and concluded that two methods, k-nearest-neighbors  and singular value decomposition , performed well in their test data sets. others have developed more sophisticated algorithms and shown that in some situations, these variants outperform knn  <cit> . although one study  <cit>  evaluated the performance of their method along with a few others over seven microarray data sets, typically these reports have employed a limited number of data sets to evaluate their methods. another study has assessed the performance of imputation methods on a pair of data sets with strong and weak correlation structure, respectively, and concluded that the preferred choice of method and parameters are different for each set of data and dependent on the structure of expression matrix  <cit> .

in this study, we present a comprehensive evaluation of the performance of current imputation methods across a wide variety of types and sizes of microarray data sets, to assess their performance under different conditions and establish guidelines for their appropriate use. in addition, we develop and test two selection procedures for determining the most appropriate imputation method for a given data set. to this end, we have implemented and tested existing methods for mv imputation, to assess the performance of each of these methods under various conditions and determine the circumstances for which different imputation procedures are preferred.

specifically, we tested eight different algorithms from the literature that have been shown to perform well at imputing mvs in microarray data sets: knn.e , knn.c , svd, ordinary least squares   <cit> , partial least squares   <cit> , bayesian principal component analysis   <cit> , local least squares   <cit> , and least squares adaptive   <cit> . we compared the performance of these methods on nine data sets of various sizes, for different percentages of missing data, and under varying algorithm parameters. based on this evaluation we proposed two selection procedures, entropy-based selection  and self-training selection , for determining the most appropriate method for new data. ebs determines the optimal method via an entropy measure of data "complexity", and a linear model is fitted using the nine selected data sets for prediction. the complexity of a data set is a measure of the difficulty in mapping the data set to a lower-dimensional subspace. computation of this procedure is fast once the model is fitted, but also more dependent on the selection of data sets in the model fitting. sts, on the other hand, performs self-training simulation. its computation is more intensive but the performance is better. the sts scheme outperforms any single imputation method, and combining the two complementary schemes presents an appealing solution for mv imputation of microarray data.

RESULTS
optimization of parameters of each method
optimal parameter values for the eight methods under investigation, for each data set, are reported in table  <dig>  these optimal values were determined using a set of initial simulations . for the nieghbor-based imputation methods , the optimal number of neighbors was generally in the range of  <dig> to  <dig> and is consistent with previous investigations  <cit> . the range of components selected for pls was between  <dig> and  <dig>  while svd typically used a percentage of eigenvalues between  <dig>  and  <dig> . lls has its parameter optimization built into the algorithm, but we report the optimal values determined by the method, which we held fixed for the remaining simulations. although the optimal k values varied greatly , the number of tested k values in this range was typically around  <dig> or  <dig>  neither the lsa nor bpca methods required parameter optimization, as the parameter settings are predetermined in both cases.

performance of the imputation methods
the average lrmse values for each method in all nine data sets are given in figure  <dig>  there is significant variation in imputation success  across data sets and imputation methods, although the  <dig> top performing methods  were all very competitive with each other. the overall ranking for each method on each data set is given in table  <dig>  overall, lsa performed the best, followed by lls and then bpca. the differences between these methods, though statistically significant , were slim , especially compared to the differences between these methods and the remaining methods. further, it should be noted that no single method uniformly outperformed the others, so that a unanimous "best method" cannot be declared.

the entropy of each data set, calculated using , is given in table  <dig>  contrary to what was expected, there was no observable correlation between the entropy level and experiment type . figure  <dig> shows the relationship between the entropy of each data set and the performance  for the imputation methods from simulation ii. the performance of many of the imputation methods was highly dependent on the entropy of the data, as indicated by the significance of the regression slope Î²^mi associated with each method . for example, knn.e performed well in high entropy data and poorly in low entropy data , whereas pls and svd performed well for low but not high entropy data . the result of svd is expected, since svd essentially relies on successful dimension reduction, which corresponds exactly with the definition of the entropy measure. bpca also relies on dimension reduction, but the probabilistic model shrinks the principal axes that are not relevant for imputation. therefore, bpca is quite robust to changes in the data complexity, and its performance is relatively stable over the range of entropy values. however, in the two highest entropy data sets , lsa and most of the other local-imputation methods outperform bpca, the best global-method, by a relatively wide margin. hence when the entropy measure of the data is high, local methods appear better suited to imputation than the global methods.

performance of ebs
as indicated in figure  <dig> and table  <dig>  the performance of the imputation methods varied with the complexity of the data. the ebs scheme makes use of this relationship to select the best imputation method for a given data set. the accuracy of the ebs scheme in selecting the top performing method as determined by the leave-one-out cross validation using simulation ii is given in table  <dig>  since the lsa algorithm was the top performer in the majority of the simulations, and the second or third best in the remainder, it is not surprising that the ebs scheme ended up selecting this method for every data set. in cases where the lsa algorithm was not optimal, the lls algorithm was either the top or second best performer, and the ebs scheme selected this algorithm as second best in those cases. thus, one of the top two selected methods by ebs was always among the first or second best performing algorithms. overall the linear model and ebs scheme provide deep insight into the mv imputation problem. this information alone, however, is not quite enough to support an effective selection of an mv imputation method.

performance of sts
the accuracy of the sts scheme in selecting the top performing method is given in table  <dig>  though the overall accuracy of the sts scheme was only slightly higher than the ebs scheme, in cases where the two schemes differed , the sts scheme selected methods that were closer to the optimal imputation method. this is reflected in table  <dig>  which compares the average lrmse scores based on the sts scheme with the gold standard optimal method and the ebs method. the sts scheme was significantly closer to the optimal lrmse score than the ebs scheme , although the sts scheme was still significantly different from the optimal lrmse value , p <  <dig> , paired t-test). however, the sts scheme ci for the mean difference  is an order of magnitude closer to zero compared to the ebs scheme ci. using ten "second-tier" simulations was sufficient to determine the best imputation method, as in every case the friedman test of equality between the rank-sum statistics of the methods was decidedly rejected, with p-values on the order of 10- <dig> to 10- <dig> 

discussions and 
CONCLUSIONS
we performed an extensive evaluation of existing methods for imputing missing values in microarray data. in contrast to the recent comprehensive comparative study in gene clustering of microarray data  <cit> , where the order of performance of the methods was consistent across all investigated simulated and real data sets, our investigation demonstrates that the optimal imputation algorithms are all highly competitive with each other, and that no method is uniformly superior. the imputation method most commonly employed by researchers, knn, was clearly bested by the more sophisticated algorithms we tested . therefore, knn should not be the default choice for imputing mvs. moreover, troyanskaya et al. <cit>  found that neighbor selection based on the euclidean distance was favorable, whereas we found that correlation based neighbor selection outperformed euclidean neighbor selection in all but two cases . thus, correlation based neighbor selection appears to be more robust to varying levels of complexity in the data.

overall, the lsa, lls, and bpca imputation algorithms performed the best in our simulation study. both the lsa and lls algorithms are based on selection of gene neighbors for imputation, but they also each have features which resemble global based imputation. lsa uses array-based imputation in addition to gene-based imputation, and lls allows for the selection of a very large number of genes  to use for imputation. since lls solves the least squares system using the moore-penrose pseudoinverse of the neighbor gene expression matrix, singular matrices can be used. the pseudoinverse is determined by the svd of the expression matrix, and this method is similar to the supervised principal components procedure  <cit> , which has been shown to be successful in other genomic applications. lsa is more consistent than the other algorithms over the data sets we investigated , and thus has the overall advantage. in particular, the difference between lsa and lls occurs mainly on the high entropy data sets ros and ali, where the performance of lls drops considerably , while lsa has the lowest lrmse. however, we again emphasize that the overall differences between these top three  methods is slim, and that there is no conclusively best method. several extensions and improvements from these methods have been proposed in the literature  <cit>  and may outperform the methods we evaluated in this study.

for each imputation method, we used the optimal parameter settings for each data set. generally, the parameter settings were similar across all data sets. for knn and ols, the number of neighbors selected was between  <dig> and  <dig>  although in one case  <dig> neighbors was the optimal choice. for pls, the number of components was between  <dig> and  <dig>  and for svd the percentage of eigenvalues was typically in the range of  <dig>  to  <dig> . the variation in the optimal parameter setting was greatest for lls, though this range actually corresponded to only a small number of actual choices, and further the lrmse curve was relatively flat over that range of values. it should be noted that neither bpca nor lsa required parameter optimization, and lls has parameter selection built into the algorithm internally, making these algorithms attractive choices for automated imputation of mvs.

while we have evaluated a broad spectrum of imputation algorithms in the literature, our coverage is by no means exhaustive. some methods we excluded include  <cit> , which uses non-negative least squares, projection onto complex sets   <cit> , which can include biological knowledge in a set theoretic framework, and  <cit> , which uses gene ontology information in the mv imputation. in particular, gan et al. <cit>  compare pocs with lsa on seven different data sets, and find that pocs performs as well or better than lsa in all cases. however, pocs has many parameters that the user must set or optimize over, making replication of the algorithm's success difficult. the algorithms we selected have repeatedly been shown to be successful in multiple studies, and are also easy to use and available from the web. further, the ebs and sts selection schemes can in principle be used with any imputation algorithms.

our assessment of the imputation algorithms and selection schemes is based on an overall average of the lrmse. other authors have examined the distribution of squared imputation errors  as a function of the true gene expression values to determine if certain algorithms are more accurate for imputing high  expression levels  <cit> . for example, the results of nguyen et al. <cit>  indicate that knn does not perform well in the tails of the mv distribution. others have assessed the impact of mv imputation on downstream analysis such as detection of differentially expressed genes and cluster analysis  <cit> . though it is possible to incorporate this type of information into the selection schemes , future study is needed to determine if this information would prove useful in selection of mv imputation algorithms.

previous studies have shown that the performance of an imputation algorithm depends on the underlying correlation structure of the gene expression matrix  <cit> . we have further developed this idea by proposing an entropy measure which succinctly captures the complexity of the expression matrix. this measure  summarizes the correlation structure of the data via the dispersion of the eigenvalues of the covariance matrix. low entropy indicates that the gene expression values are strongly correlated and that the data can be reduced to a lower dimensional space. in contrast, high entropy indicates complex data with local substructure, which cannot effectively be reduced to a lower dimensional space. the global-based imputation methods  performed better on microarray data with lower complexity, as evidenced by the positive regression coefficient associated with each method in the regression model . in contrast, neighbour-based methods  performed relatively better in high entropy data, and have negative regression coefficients in the model . these findings correspond with those found by  <cit> . the top three performing algorithms  are all highly competitive with each other, and are less sensitive to changes in data complexity than the remaining algorithms. however, the gap between lsa and the global imputation algorithms increases for the highest entropy data sets . one notable exception to the success of lsa are the sp.afa and sp.elu data sets, where both lls and bpca outperform lsa, even though the entropy values of these data sets are relatively high. possibly this is related to the smaller number of samples in these data sets; since there is less information in each individual gene for imputation, lsa, ols, and knn are not as effective as bpca and lls, which pull information from multiple genes simultaneously.

the entropy measure can be formally used to select an appropriate imputation algorithm for a particular data set, via the ebs scheme . it has an appealing advantage of fast computation since, for each new data set, the preferred method is selected only according to its entropy measure. the ebs scheme does have a practical limitation, in that it requires imputing mvs from multiple other data sets in order to fit the regression model . in addition its performance is dependent on which data sets were used to fit the model, although the cross validation results using the leave-one-out data sets demonstrate that the method performs well despite these potential limitations. one use of the ebs scheme is to reduce the number of imputation methods under consideration. for example, with low complexity data  <  <dig> ), we may restrict our attention to imputation methods like lsa, lls, and bpca, while with high complexity data  >  <dig> ) we may instead shift our attention to local imputation methods like lsa, knn.c, and ols..

the subset of methods selected by the ebs scheme can then be further evaluated by using the sts scheme to select the optimal imputation algorithm from among this reduced number. the sts scheme learns the structure of the expression data and selects the optimal imputation algorithm by self-training. this is accomplished by generating a small percentage of mvs among the genes with complete expression profiles to simulate the missing pattern in the original data, under the assumption that expression values are missing at random. results from simulation iii indicate that this scheme picks the optimal or near-optimal imputation algorithm in every case. the selection scheme is sensitive to variation in the order of imputation ranking associated with variation in percentage of mvs, as evidenced by the selection of bpca over lls in sp.elu and sp.afa. however, in these cases the difference between the sts and optimal method in terms of imputation success was slight. in our experiments, we used  <dig> simulations to determine the best imputation algorithm in the sts scheme. this proved to be sufficient for distinguishing between the methods, as p-values from the rank-sum statistics were all on the order of magnitude of 10- <dig> to 10- <dig> in practice, even fewer simulations may be used to determine the best method, for example by using a sequential selection scheme where the set of imputation methods is evaluated after each iteration and "pruned" down until the best method is determined. this is a potential avenue of research that we will explore in the future. as noted above, the computational cost of sts can be further reduced by using the entropy of the data as a screening tool for the imputation algorithms, and when coupled in this fashion the two selection methods provide effective, complementary tools for determining the best imputation algorithm for a particular data set.

