BACKGROUND
machine learning methods have found many applications in gene expression data analysis, and are commonly used to classify patient samples into classes, corresponding to for example cancer sub-type, based on gene expression profiles. supervised learning is a powerful tool in these studies since it can be used both to establish whether the classes of interest can be predicted from expression profiles and to provide an explanation as to what genes underlie the differences between classes. the expression data in such studies typically undergo an analysis pipeline in which the most important steps are data normalization, gene selection and machine learning. although there are several comparative studies of methods for normalization, gene selection and machine learning, none have studied how all of these analysis steps influence each other and the final model performance.

a wealth of methods exists for microarray normalization, gene selection and machine learning. normalization of microarray data involves several possible steps  <cit> , including background correction  <cit>  and dye-normalization  <cit> . the relative performance of different normalization approaches, although not in the context of machine learning, has previously been evaluated using spike-in data sets  <cit> . previous studies have also shown that normalization has an impact on clustering  <cit> . one of the challenges in using machine learning and gene expression data to study medical diagnosis is the large number of genes  compared to the relatively limited number of patients . many gene selection methods have therefore been developed to cope with this problem  <cit> . approaches to gene selections are either filter methods or wrapper methods. filter methods score, rank and select the best individual genes before the machine learning methods is applied, while wrapper methods score subsets of genes according to the performance of machine learning models induced from the subset. machine learning methods are commonly used in bioinformatics applications both for clustering  and for inducing predictive models from examples   <cit> .

since gene selection is a necessary step in machine learning-based analysis of microarray patient data, all existing comparative studies have investigated the effect of gene selection and machine learning methods on classification performance. most of these studies considered tumor classification. however, to the best of our knowledge, no study has also taken data normalization methods into account. pirooznia et al.  <cit>  studied the performance of three gene selection methods and six machine learning methods on eight microarray data sets, and mainly highlighted the importance of gene selection and the number of selected genes. romualdi et al.  <cit>  investigated four gene selection methods and six machine learning method using both simulated data and two microarray data sets, and demonstrated that non-parametric methods such as support vector machines  and artificial neural networks  were more robust than parametric methods. lee et al.  <cit>  performed an extensive comparison of  <dig> machine learning methods and three gene selection approaches on seven microarray data sets. their main conclusions were that more sophisticated classifiers such as svms perform better than classical methods and that the choice of gene selection method has a large effect on the performance. li et al.  <cit>  investigated eight gene selection methods and seven machine learning methods using nine data sets, and concluded that svm methods perform best and that the choice of machine learning methods is more important than the choice of gene selection methods. statnikov et al.  <cit>  applied four gene selection methods and four machine learning methods, as well as several different svm methods and some ensemble methods, to classify patients in  <dig> data sets. they also concluded that svm methods performed better than non-svm methods, that no significant improvement was obtained using ensemble methods and that gene selection improved all machine learning methods. all comparative studies used cross validation to evaluate the performance of different methods. several previous studies have stressed the challenges related to the applications of machine learning methods and the importance of objective evaluation  <cit> . in particular, zervakis et al.  <cit>  used several gene selection and machine learning methods to show how the performance of gene selection methods vary with different validation strategies and concluded that independent test sets are important for validation.

the previous comparative studies have shown that classification performance varies a great deal from data set to data set. this is a challenge in comparative studies since it limits our ability to find general trends in terms of methods and combinations of methods that perform best across data sets. here we approach this problem by comparing methods and pairs of methods on individual data sets, and then by visualizing trends across data set using heat maps. thus we are able to draw general conclusions both about the individual effect of normalization, gene selection and machine learning on classification performance, and also to say something about synergistic effects that occur when these methods are used in combination. our approach to studying synergy between methods, and the fact that we study the effect of normalization as well as gene selection and machine learning, makes our study unique. our main conclusions are that support vector machines  with a radial basis kernel, linear kernel or polynomial kernel of degree  <dig> perform best across data sets. we show that these methods exhibit a synergistic relationship with gene selection based on the t-test and the selection of a relatively high number of genes. all these methods perform better on normalized than on non-normalized data, however, while the radial basis kernel and the linear kernel perform best when the data is not background corrected, the polynomial kernel benefit from background corrected data.

RESULTS
we evaluated classification models induced from seven different two-channel microarray expression data sets with two known classes . each classification model is the result of a combination of different computational methods for microarray normalization, gene selection, number of selected genes and machine learning. we included five different approaches to normalization, three gene selection methods,  <dig> different numbers of genes and eight different machine learning methods . in total  <dig> models were induced corresponding to all possible combinations of the different methods and data sets. we will refer to data sets and methods according to the acronyms given in table  <dig> .

acronyms defined here are used throughout the paper. "fixed parameters" in the methods were given fixed values, while "optimized parameters" were optimized in the inner cross validation using a grid search. *the number of samples belonging to each class is given in parenthesis. **dimensions after background corrected normalization  are given in parenthesis.

combinations of methods were validated by employing a double cross validation  approach where the inner loop was used to find good parameter settings ) and the outer loop was used to estimate the predictive power of the final model in terms of average error rate . normalization was done initially, before the cross validation, while gene selection was done inside the outer cross validation.

method choices explain variation in classification performance within data sets
the average error rate obtained by evaluating all the different combinations of methods on the seven data sets was  <dig>  with a standard deviation of  <dig>  . the observed variation has two causes; the varying performance of different combination of methods used to induce models and the varying difficulty of discriminating the two classes in different data sets. the later could to some degree be explained by the varying distributions of observations over the two classes in the data sets. to reduce this effect, error rates were adjusted by dividing by the theoretical error rate obtained by random class assignment from the known distribution of classes. thus adjusted error rates below one correspond to models performing better than random class assignment. henceforth, when we discuss error rates, we will refer to the adjusted version of error rates if not specifically stated otherwise. the average adjusted error rate from all the  <dig> models was  <dig>  with a standard deviation of  <dig> . this indicates that, although the adjustment reduces the influence due to unbalanced class sizes, most of the variation has other causes. figure  <dig> shows that the error rates vary a lot between different data sets, but also that there is variation within data sets. it is this latter variation, due to the choice of combinations of methods, that is of interest to us in this paper.

to analyze the effect that different methods have on the classification performance, we used multiple linear regression  <cit>  with methods as predicting variables and error rate as the response variable. predicting variables included both first order terms and second order interaction terms constructed from data set , normalization , gene selection method , number of selected genes  and machine learning method  . the regression model had an adjusted r-squared  <cit>  value of  <dig> . the analysis showed that data set  is by far the most explanatory predicting variable completely overshadowing the other variables. this is in accordance with what we already saw in figure  <dig>  and provides little information as to what is the best combination of methods. thus, we performed a new regression analysis without data set  as predicting variable, resulting in an adjusted r-squared value of only  <dig> . as a consequence of the lack of general, interpretable results from the regression analysis across data sets, we changed strategy to first analyze each data set individually and then search for general trends across data sets. this seems to be a viable strategy; adjusted r-squared values for these new regression models from the seven individual data sets range from  <dig>  to  <dig> , indicating that variation in error rate within data sets indeed can be explained by method choice. also, all the first order terms  and most interaction terms  have significant explanatory power in the regression models .

significant differences in performance of individual methods
to study the performance of individual methods, e.g. the gene selection method relief, we plotted error rates resulting from applying all relevant combinations of methods to individual data sets and to all data sets . we also compared all methods head-to-head and visualized statistically significant differences between pairs of methods across data sets .

normalization
across data sets, the normalization methods perform rather similar ; the somewhat worse results for no  <dig> and  <dig>  can be explained by the fact that these normalization methods are not relevant to the agilent data sets  of which the finak data set results in extremely well-performing models for almost all methods . within data sets, however, the picture is rather complex . in the sørlie data set, models based on non-normalized data  somewhat surprisingly outperform models from normalized data  by a large margin, but non-normalized data is also significantly better than all normalized data in herschowitz. on the other hand, all normalized data is statistically better than non-normalized data in ye, alizadeh and jones. thus the major differences are observed between non-normalized and normalized data. significant differences between normalization methods  are sporadic, although no  <dig> is significantly better than no  <dig> and no  <dig> in three data sets.

gene selection
also gene selection methods exhibit a rather complex pattern with some data sets showing a gene selection preference . the t-test outperforms the two other methods in ye and galland, and is significantly better than paired distance and relief in five and four data sets, respectively. relief is significantly better than paired distance in alizadeh, galland, herschkowitz and ye. however, paired distance is significantly better than the two others in jones and sørlie. hence, while the general trend is that the t-test performs best followed by relief and paired distance, the sørlie data set again shows the opposite trend.

number of selected genes
number of significant genes in each data set and the corresponding significance thresholds. the t-test was used to compute p-values for each gene and the bonferroni correction was used to judge significance .

machine learning
in general, support vector machines  have the best classification capabilities followed by artificial neural networks , while decision trees  have the worst performance . however, dts clearly outperform other machine learning methods in sørlie, which again shows the opposite trend of the other data sets. although there are some clear conclusions to be drawn from the performance of machine learning methods, figure  <dig> also portray a rather complex picture where most methods perform significantly better than most other methods in at least one data set. notably, svms with a radial basis  kernel are only outperformed in single data sets, but svm linear and svm poly  <dig> are also performing well.

synergistic relationships exists between methods
one of the main questions we ask in this study is to what degree we can observe synergistic relationships between methods. in order to answer this question we need to study combinations of methods. to have enough data to claim statistical significance, we limited our study of synergy to comparing all method-pairs of the same type, i.e., machine learning-normalization , machine learning-number of genes , machine learning-gene selection , gene selection-normalization , gene selection-number of genes  and normalization-number of genes  .

obviously, the performance of individual methods affects the performance of pairs of methods. however, in practice we always select one method of each type , thus methods that do not significantly deteriorate the performance of other individually well-performing methods are of interest as well as methods that decrease performance of other methods and therefore represent combinations that should be avoided. method-pairs that contain the number of selected genes  are generally exhibiting the pattern already seen for single methods; too few genes severely reduce the performance of most methods while more than  <dig> genes do not significantly improve performance. thus we will mainly focus on pairs consisting of methods for machine learning, normalization and gene selection . furthermore, we are particularly interested in pairs where the best machine learning methods  are significantly affected by normalization or gene selection. the overall performance of a method-pair can be summarized by counting the number of data set where this pair is significantly better than other pairs .

machine learning and normalization 
 compared to all other pairs of machine learning and normalization, pairs containing svm rb perform the best. svm rb performs better on normalized than on non-normalized data. the best normalization method to use with svm rb is no  <dig> followed by no  <dig>  although the improvement over no  <dig> and no  <dig> is rather small. the same pattern can be seen for the second best machine learning method, svm linear, indicating that these methods combine well with normalization methods not utilizing background correction . however, the picture is rather complex. for example, the third best method, svm poly  <dig>  works best with background corrected normalizations , while normalizations without background correction is no better than non-normalized data.

machine learning and gene selection 
 a clear trend is that the t-test is the best gene selection partner to all machine learning methods, although the actual improvement sometimes is rather small. svm rb combined with the t-test is the best performing method-pair, and t-test is also the best gene selection partner for svm linear and svm poly  <dig> 

gene selection and normalization 
 almost all pairs of gene selection and normalization methods are significantly better than almost all other pairs of this type in at least two data sets. the lack of clear trends is maybe not so surprising considering the complex behavior of these methods when studied individually. surprisingly, there is a synergy between the t-test and non-normalized data, which is the best pair of gene selection and normalization. if we look at the performance of the normalization methods for each gene selection method separately, we again see the trend from our analysis of individual methods; three to four data sets result in significantly better models when normalized while at least two other data sets actually result in better models when not normalized.

the comparison of method-pairs confirms the strong performance of the individually best performing machine learning methods . these methods require at least  <dig> genes to achieve their best classification performance. a trend is that svm rb is somewhat more robust with respect to normalization, gene selection and number of selected genes than the other well-performing machine learning methods. for example, both svm linear and poly  <dig> perform worse when not using gene selection based on the t-test, while svm rb performs almost as well with one of the two other methods. this example illustrates a very important trend in our study; although some methods perform well by themselves, their performance can be severely hampered by unfortunate choices for the other methods. in this context, data normalization is shown to be very important; the wrong normalization method can severely reduce the performance of many of the best machine learning methods. also, these best methods perform better on normalized than non-normalized data. there is also a very clear positive effect of using normalized data with dt methods, while nn methods actually perform best on non-normalized data.

of particular interest are synergistic effects where two methods perform significantly better together than any of the two methods do individually. we specifically searched for such patterns , and confirmed synergistic relationships between the three best machine learning methods  and both the t-test  and the selection of at least  <dig> genes . there is also synergy between no  <dig>  and both svm rb and linear , while svm poly <dig> has a synergistic relationship with background corrected data . one surprise is that this analysis confirms synergy between non-normalized data and the t-test . however, since the best machine learning methods perform well with normalized data and the t-test, this result is a curiosity of little practical importance.

statistical significance
to establish statistical significance of the reported error rates, we performed permutation tests by inducing models and estimating error rates after randomly shuffling the class labels of all data sets . since such tests are computationally expensive, we limited the study to one method combination that was chosen based on earlier discussions related to performance and synergy; no  <dig>  t-test,  <dig> genes and svm rb. the permutation tests show that shuffled data sets result in adjusted error rates centered close to  <dig>  and that our reported error rates for the original data sets are highly significant with the exception of sørlie. as discussed earlier, the sørlie dataset stands out from the other data sets in many ways. for example, sørlie results in better models when non-normalized and when inducing models using decision trees.

discussion
in this paper, we have studied the effect of data normalization, gene selection and machine learning on the predictive performance of models induced from cancer-related expression data. performance was rigorously assessed by repetitively employing a double cross validation approach to each method combination and each data set. we analyzed seven cancer related two-channel microarray data sets published in high-impact journals. we were particularly interested in studying the effect of normalization in two-channel experiments where a generally agreed-upon standard for normalization still does not exist. indeed we see some trends related to normalization. normalized data resulted in better models than do non-normalized data when employing the best machine learning methods. in particular, the two best machine learning methods  showed a synergistic relationship with normalizations not using background correction .

the data sets in this study result in machine learning models that perform rather differently both with respect to error rate and also with respect to relative performance between methods. although all data sets are from two-channel microarray experiments related to cancer, the classes we have used to train and evaluate our models are defined somewhat differently. however, all classes are based on clinical observations . we chose to approach this challenge of heterogeneous data sets and classes by initially evaluating the data sets separately and then by looking for general trends across data sets.

overfitting occurs when the learning framework selects models that performs better on the training set but worse on the external test sets  <cit> . this is a particularly severe problem for the types of data sets studied here, since we have many more genes than patients, and thus a high risk of selecting genes that discriminate classes in the training set but that do not generalize to the test set  <cit> . to detect overfitting and obtain robust results, we performed a double cross validation where the inner loop was used to optimize parameters  and the outer loop was used to estimate classification performance. a k-fold cv was chosen since previous research has shown that this reduce the bias compared to leave-one-out cv  <cit> . in addition, we re-ran the cv several times to minimize the effect that particular data splits have on the results.

to simultaneously study the effect of normalization, gene selection and machine learning implies testing a large number of method combinations. in order to reduce running time, we therefore had to make certain adjustments to the analysis pipeline used to induce and validate models. firstly, while we did gene selection inside the outer cv loop, we chose not to perform a separate selection inside the inner loop . since the inner loop is used to optimize parameters only, we found this to be a reasonable compromise to reduce running time while still keeping training and test sets completely separate when estimating the reported error rates in the outer cv loop. secondly, we decided to fix several parameters in the machine learning methods. although we carried out tests to make sure we optimized the most important parameters, this approach could give some advantage to methods that have parameters that were in fact optimized. for example, optimizing the gaussian kernel parameter σ could somewhat benefit svm rb since this is the only svm method left with a parameter that was tuned. finally, we only performed permutation tests for one method combinations . however, this test was sufficient to show that permutated data results in adjusted error rates centered close to  <dig>  and thus that the best methods in this study clearly produce statistically significant results.

due to the above mentioned risk of overfitting, and also due to practical issues such as running time, gene selection is required on data sets with many genes and few observations . we were rather surprised to see that all data sets upheld predictive performance even when  <dig> genes were selected from the gene selection methods, and that the error rate actually improved for three data sets. interestingly, the three data sets for which performance continuously improved when more genes were selected, were the same three data sets that have the fewest number of discriminatory genes ; ye, sørlie and galland. these data sets are also among the four worst performing data sets in terms of average error rate . thus, one might conclude that the inclusion of many genes give better performance for data sets with many weakly discriminatory genes as compared to data sets with strongly discriminatory genes. it was also encouraging to see that our training pipeline seems robust to overfitting in that the inclusion of weakly discriminatory genes  did not affect classification performance negatively for any data set.

the gene selection methods considered in this study were all filter-based methods. these methods select genes prior to machine learning by typically ranking genes based on their individual ability to separate classes  <cit> . unfortunately, this reduces the possibility for advanced non-linear machine learning methods to find complex discriminatory patterns or decision boundaries based on genes that individually are weakly discriminatory or even completely non-discriminatory. wrapper-based gene selection methods can in principle find such genes by iteratively testing subsets of genes that result in high-performing models  <cit> . however, for the data sets that we are studying here, with tens of thousands of genes and only around  <dig> patients, wrapper methods are unrealistic not only in terms of time complexity, but also due to the risk of overfitting. in this context, we designed a new gene selection method  that first selects genes with high variance or high absolute mean, and then ranks pairs of genes with high discriminatory power . our hope was that this method would identify discriminatory gene-pairs containing genes that individually were not selected by other gene selection methods and thus would improve models from data sets with only weakly discriminatory genes. intriguingly, this method turned out to be the best gene selection method on the worst performing data set in this study, which also is the only data set without any significantly discriminatory genes .

in this study, we selected methods that are commonly used to analyze gene expression data. thus, although we see rather small difference between, for example, gene selection methods, this does not mean that gene selection is not important. initially, we also included a gene selection method that rank genes based on variance, however, the consistently poor performance of this unsupervised method spurred us to omit it from further analyzes.

for comparing the performance of methods, we used the wilcoxon signed rank test. this test considers the ranking of comparable pairs of method combinations . such paired tests are sensitive, and reveals interesting significant differences even when average performances are rather similar . for individual methods, the bonferroni correction was used to decide on statistical significance. however, when pairs of methods were considered, much fewer error rates were available to the statistical test, thus making methods for multiple hypothesis correction too insensitive. in these cases , we opt to use a fixed p-value threshold of  <dig>  to identify interesting synergistic relationship, knowing that among these one must expect an increased number of false positives .

the data sets and class definitions investigated in this article have previously been used to compare unsupervised clustering methods  <cit> . there are some interesting parallels to be drawn between these two studies. first and foremost, both studies experienced huge differences in performance between data sets, and in particular the results from sørlie did not agree with that of the other data sets. normalization was shown to have a positive effect on both clustering and machine learning, but it was harder to draw any decisive conclusions about the relative performance of different normalization methods . although the clustering study mainly focused on unsupervised gene selection methods, both studies found that relatively high numbers of genes were needed to obtain good performance.

CONCLUSIONS
in this study, we have performed a comprehensive study of the effect that normalization, gene selection, the number of selected genes and machine learning method have on the predictive performance of resulting models. a unique aspect of this study was the inclusion of different normalization methods in the comparisons. indeed, we showed that there is a significant positive effect of normalization on the best methods; however, the relative performance of different normalization methods is complex. the best machine learning methods in this study were support vector machines  with a radial basis kernel followed by svms with a linear kernel and svms with a polynomial kernel of degree  <dig>  we showed that there is a positive, synergistic relationship between these methods and gene selection based on the t-test and the selection of at least  <dig> genes.

