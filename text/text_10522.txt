BACKGROUND
the high dimensionality of the feature space is a characteristic of learning problems involving spectral data. in many applications with a biological or biomedical background addressed by, for example, nuclear magnetic resonance or infrared spectroscopy, also the number of available samples n is lower than the number of features in the spectral vector p. the intrinsic dimensionality pintr of spectral data, however, is often much lower than the nominal dimensionality p – sometimes even below n.

dimension reduction and feature selection in the classification of spectral data
most methods popular in chemometrics exploit this relation pintr < p and aim at regularizing the learning problem by implicitly restricting its free dimensionality to pintr.

 popular methods in chemometrics, such as principal component regression  or partial least squares regression  directly seek for solutions in a space spanned by ~pintr principal components  – assumed to approximate the intrinsic subspace of the learning problem – or by biasing projections of least squares solutions towards this subspace  <cit> , down-weighting irrelevant features in a constrained regression . it is observed, however, that although both pcr and pls are capable learning methods on spectral data – used for example for in  <cit>  – they still have a need to eliminate useless predictors  <cit> . thus, often an additional explicit feature selection is pursued in a preceding step to eliminate spectral regions which do not provide any relevant signal at all, showing resonances or absorption bands that can clearly be linked to artefacts, or features which are unrelated to the learning task. discarding irrelevant feature dimensions, though, raises the question of how to choose such an appropriate subset of features  <cit> .

different univariate and multivariate importance measures can be used to rank features and to select them accordingly  <cit> . univariate tests marginalize over all but one feature and rank them in accordance to their discriminative power  <cit> . in contrast, multivariate approaches consider several or all features simultaneously, evaluating the joint distribution of some or all features and estimating their relevance to the overall learning task. multivariate tests are often used in wrapper schemes in combination with a subsequent classifier , or by statistical tests on the outcome of a learning algorithm . while univariate approaches are sometimes deemed too simplistic, the other group of multivariate feature selection methods often comes at unacceptably high computational costs.

gini feature importance
a feature selection based on the random forest classifier  <cit>  has been found to provide multivariate feature importance scores which are relatively cheap to obtain, and which have been successfully applied to high dimensional data, arising from microarrays  <cit> , time series  <cit> , even on spectra  <cit> . random forest is an ensemble learner based on randomized decision trees , and provides different feature important measures. one measure is motivated from statistical permutation tests, the other is derived from the training of the random forest classifier. both measures have been found to correlate reasonably well  <cit> . while the majority of the prior studies focused on the first, we will focus on the second in the following.

as a classifier, random forest performs an implicit feature selection, using a small subset of "strong variables" for the classification only  <cit> , leading to its superior performance on high dimensional data. the outcome of this implicit feature selection of the random forest can be visualized by the "gini importance"  <cit> , and can be used as a general indicator of feature relevance. this feature importance score provides a relative ranking of the spectral features, and is – technically – a by-product in the training of the random forest classifier: at each node τ within the binary trees t of the random forest, the optimal split is sought using the gini impurity i – a computationally efficient approximation to the entropy – measuring how well a potential split is separating the samples of the two classes in this particular node.

with  being the fraction of the nk samples from class k = { <dig> } out of the total of n samples at node τ, the gini impurity i is calculated as

  

its decrease Δi that results from splitting and sending the samples to two sub-nodes τl and τr  by a threshold tθ on variable θ is defined as

  

in an exhaustive search over all variables θ available at the node , and over all possible thresholds tθ, the pair {θ, tθ} leading to a maximal Δi is determined. the decrease in gini impurity resulting from this optimal split Δiθ  is recorded and accumulated for all nodes τ in all trees t in the forest, individually for all variables θ:

  

this quantity – the gini importance ig – finally indicates how often a particular feature θ was selected for a split, and how large its overall discriminative value was for the classification problem under study.

when used as an indicator of feature importance for an explicit feature selection in a recursive elimination scheme  <cit>  and combined with the random forest itself as classifier in the final step, the feature importance measures of the random forest have been found to reduce the amount of features. most studies using the gini importance  <cit>  and the related permutation-based feature importance of random forests  <cit>  together with random forests in a recursive feature elimination scheme, also showed an increases in prediction performance.  while these experiments indicate the efficiency of the gini importance in an explicit feature selection  <cit>  one might raise the question whether a random forest – the "native" classifier of gini importance – with its orthogonal splits of feature space is optimal also for the classification of spectra with correlated features and data-specific noise , or if other classification models may be a better match with properties of spectral data.

objective of this study
thus, in the present work, we were interested in evaluating the combination of a feature selection by gini importance together with standard chemometric classification approaches, such as discriminant pcr and pls classification  which are known to be well adapted to spectra, and in studying their performance in dependence of specific characteristics of spectral data. in a first experiment we evaluated the joint application of explicit and implicit dimension reduction, using uni- and multivariate feature selection strategies in combination with random forest, d-pls and d-pcr classification in an explicit recursive feature elimination . in a second experiment, we studied the influence of different noise processes on random forest and d-pls classification to identify optimal conditions for explicit and implicit dimension reduction. in both experiments we were interested in identifying general properties and differences of the methods employed in the classification of spectral data.

workflow of the recursive feature selection, and combinations of feature importance measures  and classifiers  tested in this study. compare with results in table  <dig> and fig.  <dig>  hyper-parameters of pls/pcr/random forest are optimized both in the feature selection  and the classification  step utilizing the training data only. while gini importance  and regression coefficients  have to be calculated within each loop , the univariate measures  have only to be calculated once.

RESULTS
visualizing feature importance
measuring feature relevance using the gini importance is subject to selection bias on factorial data  <cit> . splits are more often sought on variables with a higher number of different factors, and a correction of the gini importance is necessary in such cases  <cit> . spectral data, except for count data, represent continuous signals, with a distribution of n different values for each spectral channel or feature. each feature will allow the same number of distinct splits in a random forest classification, and, hence, a measurement of the relevance of spectral regions for a specific classification problem will be unaffected by this potential source of bias.

both univariate tests for significant class differences returned smooth importance vectors when employed on the spectral data . the smoothness of the gini importance was dependent on the size of the random forest  – small forests resulted in "noisy" importance vectors, only converging towards smooth vectors when increasing the overall number of trees in the forest or the overall number of splits. as such changes influence the absolute value of this measure, the gini importance could not be interpreted in absolute terms – like the p-values of the univariate tests – but only allowed for a relative comparison. for such a comparison between different variables and between different measures, the features were ranked according to their importance score . here, univariate importance measures and gini importance agreed well in many, although not all, spectral regions . an example of the most prominent differences between univariate feature importance and multivariate gini importance are highlighted in fig. 3b. spectral regions deemed unimportant by the univariate measures – with complete overlap of the marginal distributions as shown in fig. 3b – may be attributed high importance by the multivariate importance measure , indicating spectral regions with features of higher order interaction.

inspecting the gini feature importance we observed – similar to  <cit>  – that some spectral regions were selected as a whole, suggesting that correlated variables were assigned similar importance. thus, the importance measure may be interpreted like a spectrum, where neighbouring channels of similar importance may be considered as representatives of the same peak, absorbance or resonance line. this can be used in an exploratory visualization of feature relevance . as the random forest prefers splits on correlated variable over splits on uncorrelated ones  <cit>  it should be noted, however, that this "importance spectrum" may be somewhat biased towards overestimating the importance of major peaks spanning over many spectral channels.

feature selection and classification
the classification accuracies provided by the first experiment based on the real data allowed for a quantitative comparison of the methods applied and for testing for statistically significant differences between results on the full set of features in comparison to the subselected data sets . on one half of the data, the feature selection hardly changed the classification performance at all , while on the other half a feature selection improved the final result significantly , almost independently of the subsequent classifier. in the latter group optimally subselected data typically comprised about 1–10% of the initial features . such a data dependence in the benefit of a preceding feature selection is well known . different from  <cit> , however, we did not see a relation to the apparent degree of ill-posedness of the classification problem .

the best classification results on each data set are underlined. approaches which do not differ significantly from the optimal result  are set in bold type . significant differences in the performance of a method as compared to the same classifier without feature selection are marked with asterisks . the mir data of this table benefit significantly from a feature selection, whereas the nmr data do so only to a minor extent. overall, a feature selection by means of gini importance in conjunction with a pls classifier was successful in all cases and superior to the "native" classifier of gini importance, the random forest, in all but one cases.

significance of accuracy improvement with feature selection as compared to using the full set of features; and percentage of original features used in a classification that has maximum accuracy . the significance is specified by -log <dig>  where p is the p-value of a paired wilcoxon test on the  <dig> hold-outs of the cross-validation . for comparison, -log =  <dig>  and -log = 3; the value of  <dig>  reported for mir bse binned in the second row of the first column corresponds to a highly significant improvement in classification accuracy, corresponding to a p-value of 10- <dig> 

random forest, the only nonlinear classifier applied, performed slightly better than the linear classifiers on the unselected data sets , but improved only moderately in the course of the feature selection . given that random forest performs well on the unselected data sets, and that little or no benefit is incurred by an additional explicit feature selection , it is apparent that an implicit feature selection is at work and performs well when training the random forest classifier. ultimately, however, the random forest classifier was surpassed in performance by any of the regularized linear methods on all data sets . this rather weak classification performance of the random forest may be seen in line with  <cit> , but contrasts results of e.g.  <cit>  using random forest in the classification of microarrays, similar to spectra in their high dimensionality of their feature vectors. few differences could be observed between d-pls and d-pcr classification. among the different feature selection strategies, the wilcoxon-test and the gini importance performed better on average than the iterated selection based on the regression coefficients , with slightly better classification results for the gini importance . overall, while the gini importance was preferable in feature selection, the chemometric methods performed better than random forest in classification, in spite of their limitation to model linear dependencies only.

the two linear classifiers of this study generally seek for subspaces ck maximizing the variance var of the explanatory variables x in the subspace c

  

in case of pcr or the product of variance and the  correlation corr

  

with the response y in case of pls  <cit> . thus, for a better understanding of d-pcr and d-pls, both corr and var were plotted for individual channels and for individual learning tasks in fig.  <dig> . on data sets which did not benefit greatly from the feature selection, we observed variance and correlation to be maximal in those variables which were finally assigned the largest coefficients in the regression . conversely, in data sets where a feature selection was required, features with high variance but only moderate relevance to the classification problem  were frequently present in the unselected data . this might be seen as a likely reason for the bad performance of d-pcr and d-pls when used without preceding feature selection on the bse and wine data: here the selection process allowed to identify those features where variance coincided with class-label correlation , leading to a similar situation in the subsequent regression as for those data sets where a feature selection was not required .

in summary, observing that the degree of ill-posedness is not in itself an indicator for a required feature selection preceding a constrained classification, it might be argued that non-discriminative variance – hindering the identification of the optimal subspace in pcr, and disturbing the optimal trade-off between correlation and variation in pls – may be a reason for the constrained classifiers' failing on the unselected data and, consequently, a requirement for a feature selection in the first place.

feature selection and noise processes
the first experiment advocated the use of the gini importance for a feature selection preceding a constrained regression for some data sets. thus, and in the light of the unexpectedly weak performance of the random forest classifier, we studied the performance of the d-pls and the random forest classifier as a function of noise processes which can be observed in spectral data  to identify optimal situations for the joint use of explicit and implicit feature selection.

in this second experiment, random forest proved to be highly robust against the introduction of "local" noise, i.e. against noise processes affecting few spectral channels only, corresponding to spurious peaks or variant spectral regions which are irrelevant to the classification task . the random forest classifier was, however, unable to cope with additive global noise: already random offsets that were fractions of the amplitude s of the spectra  resulted in a useless classification by the random forest. as global additive noise stretches the data along the high dimensional equivalent of the bisecting line , the topology of its base learners may be a disadvantage for the random forest in classification problems as shown in fig.  <dig>  single decision trees, which split feature space in a box-like manner orthogonal to the feature direction are known to be inferior to single decision trees splitting the feature space by oblique splits  <cit>  . random offsets often occur in spectral data, for example resulting from broad underlying peaks or baselines, or from the normalization to spectral regions that turn out to be irrelevant to the classification problem. thus, one might argue that the "natural" presence of a small amount of such noise may lead to the rather weak overall performance of the random forest observed in the first experiment .

partial least squares performed slightly better than random forests on all three data sets at the outset . in contrast to the random forest, pls was highly robust against global additive noise: on the synthetic classification problem – being symmetric around the bisecting line – the random offsets did not influence the classification performance at all . on the real data – with more complex classification tasks – the d-pls classification still showed to be more robust against random offsets than the random forest classifier . conversely, local noise degraded the performance of the d-pls classification . the d-pls classifier seemed to be perfectly adapted to additive noise – splitting classes at arbitrary oblique directions – but its performance was degraded by a large contribution of non-discriminatory variance to the classification problem .

in the presence of increasing additive noise, both univariate and multivariate  feature importance measures lost their power to discriminate between relevant and random variables at the end , with the gini importance retaining discriminative power somewhat longer finally converging to a similar value for all three variables correlating well with a random classification and an  random assignment of feature importance . when introducing a source of local random noise and normalizing the data accordingly, the univariate tests degraded to random output , while the gini importance measure  virtually ignored the presence and upscaling of the non-discriminatory variable .

feature selection using the gini importance
overall, we observed that the random forest classifier – with the non-oblique splits of its base learner – may not be the optimal choice in the classification of spectral data. for feature selection, however, its gini importance allowed to rank non-discriminatory features low and to remove them early on in a recursive feature elimination. this desirable property is due to the gini importance being based on a rank order measure which is invariant to the scaling of individual variables and unaffected by non-discriminatory variance that does disturb d-pcr and d-pls. thus, for a constrained classifier requiring a feature selection due to the specificities of the classification problem , the gini feature importance might be a preferable ranking criterion: as a multivariate feature importance, it is considering conditional higher-order interactions between the variables when measuring the importance of certain spectral regions, providing a better ranking criterion than a univariate measure used here and in similar tasks elsewhere  <cit> .

a comparison of the computing times of the different feature selection and classification approaches  shows that the computational costs for using the gini importance is comparable to the cost of using the other multivariate feature selection criterion tested in this study. on average the computing time was no more than twice as long as for the more basic univariate importance measures.

the table reports the runtime for the different feature selection and classification approaches, and the different data sets . values are given in minutes, for a ten-fold cross-validation and with parameterisations as used for the results shown in tables  <dig> and  <dig>  for all methods, a univariate feature selection takes about five times as long as a classification of the same data set without feature selection. both multivariate feature selection approaches require approximately the same amount of time for a given data set and classifier. their computing time is no more than twice as long as in a recursive feature elimination based on a univariate feature importance measure.

CONCLUSIONS
in the joint application of the feature selection and classification methods on spectral data neither the random forests classifier using the gini importance in a recursive feature selection, nor a constrained regression without feature selection were the optimal choice for classification. random forest showed to be robust against single noisy features with a large amount of non-discriminatory variance. unfortunately it also showed to be highly sensible to random offsets in the feature vector, a common artefact in spectral data. d-pls was capable of dealing with such offsets, although it failed in the presence of non-discriminatory variance in single, highly variable features. the removal of such irrelevant – or even misleading – predictors was crucial in the application of the constrained classifiers tested in this study. overall, the combined application of gini importance in a recursive feature elimination together with a d-pls classification was either the best approach or – in terms of statistical significance – comparable to the best in all classification tasks, and may be recommended for the separation of binary, linearly separable data.

the results also suggest that when using a constrained learning method – such as the d-pls or d-pcr classifier as in this study – the main purpose of a feature selection is the removal of few "noisy" features with a large amount of variance, but little importance to the classification problem. then, the feature elimination is a first step in the regularization of a classification task, removing features with non-discriminatory variance, and allowing for a better regularization and implicit dimension reduction by the subsequent classifier. considering the similarity of pls, ridge regression and continuum regression  <cit>  – all of them trading correlation with class labels, and variance of the data for a regularization – one might expect this to be a general feature for these constrained regression methods.

only binary classifications tasks were studied here, but one may expect that results generalize to multi-class problems as well when using, for example, penalized mixture models in place of a d-pls classification. it might be worthwhile to test whether using a constrained classifier in the final classification step of a recursive feature selection is able to increase the classification performance on other data as well, for example on microarrays where a recent study  <cit>  reported of a general advantage of support vector machines with rbf-kernel over the random forest classifier.

of course, rather than advocating a hybrid method using random forest for feature selection and a constrained linear classifier to predict class membership, it might be advantageous to adapt the random forest classifier itself to fit the properties of spectral data in an optimal fashion. for individual tree-like classifiers, a large body of literature about trees using such non-orthogonal, linear splits in their nodes is available  <cit>  and may be used for such an adaption of the random forest classifier.

