BACKGROUND
reverse engineering of gene regulatory network is a vibrant research area  <cit> , whose scope is reconstructing the biological mechanisms underlying gene activity. several types of statistical models and algorithms have been proposed for deriving and representing gene interaction networks  <cit> . relevance networks  <cit>  are one of the most basic models, where gene pairs showing highly significant correlation in their expression values are assumed to be functionally associated.

unfortunately, this assumption is not valid when data from different studies are integratively analyzed. systematic biases across studies can originate spurious correlations that do not actually reflect any interactions among genes. on the other side, they can hide associations that are actually present among the measured quantities  <cit> . these systematic variations are usually known as “batch-effects”, and they can arise even when all studies share the same experimental design and measure the same quantities. the name originates from systematic biases that are present across “sample batches” within single studies, due to small differences in the processing of each batch  <cit> .

meta-analysis  and data-merging  are two approaches widely employed in the literature for addressing systematic variations in studies that share the same experimental design. in ma statistical methods are separately applied on each dataset for obtaining statistics of interest, e.g., differential expression p-values. the results from each study are then combined for creating summary statistics. the latter approach merges samples from different studies in a unique dataset, on which subsequent analyses are performed. while ma methods implicitly take in account batch-effects, dm require suitable batch-effect removal  algorithms  <cit> .

in this work we compare meta-analysis and data-merging methods in the context of retrieving gene-gene interactions in compendia of microarray studies. to this scope we compiled two different collections of microarray experiments, containing  <dig> and  <dig> studies on escherichia coli and yeast, respectively. for each collection we identified candidate interactions for multiple transcription factors by combining relevance networks with meta-analysis and data-merging methods, in turn. the candidate interactions are then compared against lists of known, experimentally verified interactions, in order to contrast the effectiveness of ma and dm methods in retrieving actual relationships.

the comparison between the two approaches is furthermore deepened on synthetic data, where a large variety of scenarios is simulated across different networks, levels of systematic bias, number of considered studies and number of samples in each study. all experimentations underlined that batch-effects are detrimental for the analyses, and that ma and dm prove similarly effective in addressing issues arising from systematic variations.

finally, we present an application on human peripheral blood mononuclear cells , for the reconstruction of the ikaros transcription factor regulatory network. for this specific application we used a bayesian-network, constraint-based learning approach in place of relevance networks, providing evidences that the results of this study transfer on more complex network-learning approaches.

related work
to the best of our knowledge, there is no other study that systematically contrasts ma and dm methods in the context of retrieving gene-gene interactions. several studies exist that evaluate the relative performances of ma methods for gene network reconstruction . in short, it is not possible to rigorously come to a unique conclusion regarding the best meta-analysis algorithm for network reconstruction. the observed discrepancy among these studies is a result of numerous factors, including data complexity and heterogeneity, difficulties in determining a golden truth, and the inclusion of a limited number of meta-analysis approaches in the experimentations.

the most common ma techniques applied in the spectrum of gene network reconstruction are based on fisher’s method  <cit> , vote counting approaches , fixed and random effect sizes  <cit> . segal et al.  <cit>  was the first one that marched towards unlocking hidden biological knowledge by using meta-analysis for network reconstruction. numerous approaches then followed, as described in  <cit> . in all cases, meta-analysis approaches seemed to perform better than individual reverse-engineering methods.

similarly, the applicability of data-merging methods in the context of network reverse engineering has been investigated in several works . in earlier studies, the vast majority merely used normalization methods to merge the compendium of expression data  <cit> . robust multi-array average method   seemed to outperform other normalization methods such as linear scaling procedures based on the median signal intensity  <cit> , quantile normalization through mas algorithm  <cit> , gcrma  <cit> , dchip pm  <cit> . however, rma normalization proved to be ineffective in removing batch effects which affect particular genes and may affect different genes in different ways  <cit> .

recent approaches have been developed for identifying and removing batch effects  <cit>  but have not been widely used. such approaches include combat  <cit> , surrogate variable analysis   <cit> , distance-weighted discrimination   <cit> , mean-centering   <cit> , and geometric ratio-based method  <cit> . in relevant studies, combat seems to outperform these methods as it robustly manages high dimensional data with small sample sizes. a previous attempt to evaluate the effectiveness of batch adjustment methods was made by the maqc-ii project  <cit> . it is necessary to bear in mind that even the most effective batch effect removal method cannot sufficiently reduce the batch effects in cases of poor experimental design  <cit> .

the literature regarding ma and dm application in the context of differential expression is particularly rich , and a complete review is out of the scope of the present work. we point out that we found only a single study  <cit>  that directly compares the performances of the two approaches on finding differentially expressed genes. interestingly, this study concludes that both approaches achieve comparable results.

methods
experimentation protocol
we devised a large experimentation in order to compare ma and dm methods in several scenarios, meaning over different biological systems, levels of systematic bias, number and composition of available studies. for each scenario we followed the same experimentation protocol, detailed below and presented in fig.  <dig> as well.fig.  <dig> experimentation protocol schematic representation. a collection of microarray dataset is assumed to be generated from multiple, independent studies all following the same experimental protocol and measuring the same quantities. the studies investigate the same biological system regulated by an unknown gene interaction network. the data collection is analyzed with two different approaches, namely meta-analysis and data-merging. in the first approach correlations among transcription factors and genes are first calculated on each dataset and then summarized, while in the latter the data are merge together, corrected for eventual batch-effects and the correlations are estimated on the pooled data. the correlations retrieved by the two approaches are then compared with a set of known interactions that partly and possibly noisy reconstruct the original gene interaction network. meta-analysis and data-merging approaches are then evaluated on the basis of their ability of assign highly-significant correlations to known interactions



let m be a collection  of m microarray datasets. all studies in m follow the same experimental protocol, analyze the same type of biological specimens, and measure the same n expression values . however each dataset dj includes a separate set of sj samples. this means that each study in m investigates the same gene-regulatory network, and that the data of all studies have been generated according to this network. thus, any systematic bias across datasets should be due to  technical differences occurred during the measurement process or to the presence of confounding factors.

for each collection m there is a set t = {tf <dig>  tf <dig>  …, tft, …, tf|t|} of |t| transcription factors of interest. we assume to know the list it of genes that interact with each transcription factor tft, i.e., it contains all genes that are targets of tft along with the genes that regulate tft.

we apply a relevance network approach for retrieving these known interactions. in detail, for each collection m and each transcription factor tft the correlations among the expression values of tft and the remaining n −  <dig> probesets are calculated over all datasets in m, using in turn an ma or dm approach. ma algorithms separately compute the correlations on each dataset and then summarize them, while dm methods pool together the data from all datasets and directly compute the final correlation values.

let ct,i,x be the correlation between transcription factor t and probeset i produced with the ma or dm method x, and pt,i,x the p-value assessing the null hypothesis h0 : ct,i,x =  <dig>  the set of n −  <dig> correlations  for transcription factor t is indicated as ct,x. both ct,x and pt,x are sorted according to the absolute values of the correlations, so that the most relevant associations appear at the top of both vectors.

relevance networks postulate that genes included in it should be strongly correlated with tft, therefore ma and dm methods are evaluated with respect to their ability of assigning highly significant correlations to known interactions. different metrics are used to compare each ct,x against its corresponding it, and dm / ma approaches are ranked according to their respective performances.

the following sections describe in detail the experimental and synthetic data collections used in the experimentations, along with the algorithms, correlation measures and performance metrics included in the analysis.

all simulations and analyses were performed in the r software  <cit> .

data
escherichia coli data compendium
the regulatory network of the escherichia coli  k- <dig> bacterium has been extensively studied  <cit> , and consequently it is an ideal test bed for our experimentation. studies in the geo repository on e. coli comprising more than twenty expression profiles and using the affymetrix e. coli antisense genome array were taken in consideration for inclusion in the analysis. imposing a single microarray platform ensures that all datasets measure the same probesets. studies applying experimental interventions known to artificially disrupt gene-gene interactions, as for example gene knock-out, were excluded from the compendium. eleven studies were included in the collection, whose characteristics are reported in the , for a total of six-hundred eighteen samples measured under a variety of conditions. probesets without annotations were excluded from the analysis, leaving a total of  <dig> probesets, each corresponding to a specific gene .

the regulondb database was used in order to retrieve known tf-gene interactions in the e. coli regulation program  <cit> . this database publicly and freely provides more than  <dig> transcriptional regulatory interactions, manually retrieved and curated from the literature. interestingly, each interaction is assigned to an evidence class, ranging within the levels ‘weak’, ‘strong’ and ‘confirmed’. the level of evidence is determined by the experimental method used in the original study reporting the interaction. experimental procedures where false positives are prevalent, like computational predictions or gene expression analysis, are catalogued as ‘weak’. other procedures providing evidence of physical interaction or anyhow excluding explanations alternative to a gene-gene interaction  are considered ‘strong’. when a regulatory relationship is supported by multiple, independent strong evidences, then it is classified as ‘confirmed’.

preliminary experimentation including all regulondb regulatory relationships led to poor results, close to random guessing . we hypothesized that large number of false positives in the weak interactions could negatively affect the results, thus we decided to exclude them from the analysis, leaving a total of  <dig> strong and confirmed regulatory relationships.

finally, we decided to consider only transcription factors having at least three known interactions, for a total of  <dig> genes included in tecoli.

yeast data compendium
the same criteria used for compiling the e. coli compendium were used for building a collection of seven yeast datasets, all measured with the affymetrix yeast genome s <dig> array platform and containing a total of four-hundred twenty seven  samples . a total of  <dig> probesets were not associated with a given gene name, and  <dig> genes were associated to more than one probeset. we removed non-annotated genes and we randomly selected a single probeset for genes with multiple measurements, leaving a total of  <dig> probesets.

known interactions were retrieved from yeastract  <cit> , which is the largest database of this type for the yeast organism to date, with more than  <dig>  reported gene-gene interactions. similarly to regulondb, yeastract lists manually curated regulatory relationships retrieved from the literature, and it also provides information about the experimental procedure used for assessing each reported interactions. we again required ‘strong’ evidence, leaving  <dig> gene-gene known and reliable interactions in the analysis. also for this compendium genes with at least three known interactions were included in tyeast, for a total of  <dig> transcription factors.

synthetic data
several collections of synthetic datasets were produced for better characterizing ma and dm performances under different scenarios.

data were sampled from artificial networks specifically devised in order to resemble real-life gene regulatory programs, following the scale-free theory introduced by barabási  <cit> . according to this theory, biological networks are not randomly organized, and the number of connections incident to a node is regulated by the power law p ~ k− γ, where k is the number of interactions, γ is a parameter whose value depends by the specific domain, and p is the fraction of genes having k connections. in other words, real-world gene regulatory programs have few transcription factors  that regulate large numbers of genes, while the remaining nodes have relatively few connections. each synthetic network is represented by a direct acyclic graph  composed by a set of nodes  v = { <dig>  …, n} and a set of directed edges e = {}. if the edge  is present in the network, then gene i is a parent of  gene j. the set of parents of node j is indicated as in.

these artificial networks were equipped with a parameterization suitable for the simulation of gene expression data and batch effects. each gene i was associated with a baseline expression value αi uniformly sampled in the interval , while each edge  is equipped with a randomly generated coefficient βij ∈  ∪  representing the strength of the interactions between i and j.

batch-effects across studies are assumed to be composed of an additive and a multiplicative component, following an approach already used in  <cit> . the first component shifts the gene average value, while the multiplicative error intensifies the sample-specific variance.

the expression value ysjk for sample s, gene j and study k is generated as follows: ysjk=αj+∑i∈injβijysik+ϵsj+γjk+δjkϵsj' 

according to this formula, each expression value ysjk is a linear combination of its baseline value αj and the expression values of its regulating genes ysik, i ∈ in. the quantity ϵj is random noise distributed as n  that represents unmodeled regulatory mechanisms that concur in determining the expression of the gene. the two factors γjk and δjkϵsj' respectively represent the addictive and multiplicative component of the systematic bias in study k, and are both randomly sampled from the distribution n. the random variable ϵj' is again distributed as n.

during our experimentations, five independent synthetic networks with four-thousand nodes each were created using the barabasi.game function from the r package igraph  <cit> . for each network we simulated different compendia by varying the number of studies in  <cit> , the number of samples for each study in  <cit> , and the hyper-parameter τ controlling the level of systematic bias in , thus obtaining  <dig> different scenarios for each network and  <dig> in total.

finally, for each network the list t of transcription factors includes all genes directly connected to at least twenty other genes. this leads to an average of  <dig> transcription factors for each network, each one connected on average with  <dig> genes. we consider only direct interactions in order to ensure that the corresponding associations are strong enough to be effectively retrieved from the data.

relevance networks reconstruction
when a single dataset is available, the relevance network for the transcription factor tft can be easily reconstructed by computing the vector ct containing n −  <dig> associations between tft and all other probesets i. these association measures are eventually coupled with measures of statistical significance pt, and the genes qt belonging to the reconstructed network can be selected by imposing an appropriate decision threshold θ to either the association or the significance values. when multiple datasets are available, the same procedure can be followed with the vectors ct,x and pt,x computed through the meta-analysis or data-merging method x .

in our experimentations we use in turn the pearson and spearman correlation measures  <cit>  for estimating the association values ct,i. pearson correlation quantifies the association between two random variables x and y as ρx,y=∑xi−x¯yi−y¯sxsy where x¯,y¯ are sample means and sx, sy sample standard deviations. the spearman correlation uses the same formula on x and y rankings. the null hypothesis h0 : ρx,y =  <dig> can be properly assessed for both correlation measures  <cit> .

performances metrics
the correlation values ct,x and corresponding p-values pt,x are compared with the list of known interactions it in order to assess x effectiveness in correctly retrieving gene regulatory relationships. in the ideal case high correlations would be assigned exclusively to actual interactions, while any other gene-pair would be reported as weakly associated. however, in real cases it is probably incomplete and noisy, undermining a fair evaluation. moreover, only a handful of regulatory relationships are usually known for each gene, while the number of possible gene-pairs is two or three order of magnitudes larger, dramatically increasing the possibility of retrieving false positives due to mere multiple-testing issues.

in order to better characterize the performances of each method, we adopted several metrics commonly used in the machine-learning area of information retrieval, a field whose operational settings strictly resemble the one depicted above  <cit> .

the receiver operator characteristic  area under the curve  is a metric that integrate sensitivity and specificity information for all possible values of the decision threshold θ. the auc ranges in the interval , where one corresponds to perfect rank ,  <dig>  corresponds to random ordering and zero to perfectly inverted predictions. interestingly, auc values can be interpreted as the probability of correctly ranking two randomly selected interactions according to their status .

the area under the precision recall curve  is similar to the auc and summarizes precision and recall information for varying θ. with respect to auc, auprc has demonstrated to have higher discriminative power when very few positive cases  are available  <cit> .

both auc and auprc evaluate the whole list of correlation values, providing a measure of global performance. however, researchers using network reconstruction algorithms often restrict their attention to a few predicted gene-gene interactions, the ones deemed more reliable. these interactions are ideal candidate for subsequent in vitro or in vivo experimental validation, which are usually too expensive or demanding to be performed on all predictions.

thus, we are interested in evaluating the partial performances of the methods on the interactions corresponding to the highest correlations in ct,x. to this end we use a version of auc known as partial auc , which considers a restricted region of the whole sensitivity / specificity curve . the mcclish formula  <cit>  standardizes pauc values in , allowing the pauc to have the same probabilistic interpretation of the auc.

we also devised a new metric that is specific for assessing partial performances, namely the area under the false discovery rate . let qt,x,r be the list of r interactions with highest correlation according to ct,x. the aufdr integrates the proportion of correctly predicted interactions in the range , i.e., aufdr=∑i=1:rqt,x,i∩iti, and it is subsequently normalized in order to assume values in  <cit> , with one indicating that all top r predictions are known interactions.

in all our analysis we use in turn vectors ct,x and pt,x for evaluating methods’ performances. highly significant associations often corresponds to p-values that are indistinguishable from zero at machine precision, leading to ties in pt,x that severely affect performance computations. in contrast, the vector ct,x does not suffer from this drawback, varying in ranges that seldom include particularly low values. the impact of these issues on performance assessment is discussed in detail in the result section.

integrative approaches
the meta-analysis, data-merging and baseline approaches included in the experimentation are now explained in detail. table  <dig> provides a summary of the methods.table  <dig> ma, dm and baseline methods included in the experimentations. for each method a synthetic description is provided describing its main characteristics



meta-analysis
meta-analysis has been described as “the process of synthesizing data from a series of separate studies”  <cit> . a typical ma application investigates a set of statistics  derived in different studies and produces a summary statistic, for example a weighted average . other, sophisticated ma approaches exist for more complex applications, for example meta-regression  <cit> , where differences in the design of the studies or the sampling strategy are treated with a regression approach.

the ma methods used in this study can be thought as a function accepting correlations ct,i <dig>  …, ct,im between gene i and transcription factor t computed over studies 1 … m, as well as their corresponding p-values pt,i <dig>  …, pt,im, and producing a single statistic and p-value: ct,ixpt,i,x=fct,i <dig> …,ct,im,pt,i <dig> …,pt,im 

we selected from the literature five ma methods whose operation matches the above definition and that are based on different assumptions and theoretical backgrounds.fisher method  <cit>  is one of the first known ma approaches. under the assumption that all pt,i,x <dig>  …, pt,i,xm assess the same null-hypothesis in multiple, independent studies following an identical design, then the quantity χt,i2=−2·∑jlogpt,ij follows a χ <dig> distribution with 2 · m degrees of freedom, and can be used for calculating the summarized p-value pt,i,fisher. we set ct,i,fisher = χt,i <dig> 

stouffer method  <cit>  is conceptually similar to fisher’s, although it combines z-scores defined as zt,ij = Φ−  <dig> instead of p-values. Φ−  <dig> is the inverse of the standard normal cumulative distribution function, and the statistic zt,i=∑jzt,ijm follows a standard normal distribution that can be used for deriving pt,i,stouffer. also in this case ct,i,stouffer = zt,i

fixed-effects approach  <cit>  assumes that all studies investigate the same correlation Ĉt,i, whose estimation is biased by a study-specific error factor, i.e., ct,ij = Ĉt,i + ϵj, j = 1 … m. on the basis of these assumptions, ct,i,fixed can be computed through a weighted mean

 ct,i,fixed=∑jwj·ct,ij∑jwj 

where the weights wj are inversely proportional to the correlations variances. pt,i,fixed is computed by comparing ct,i,fixed fisher z-transformation against its theoretical normal distribution  <cit> .random-effects models do not assume that each study estimates the same correlation Ĉt,i; the datasets are assumed to be enough ‘similar’ to be jointly analyzed, but at the same time the ground truth correlation c^jt,i may differ across studies. particularly, c^jt,i,j=1…m are assumed to be sampled from a distribution with mean Ĉt,i and unknown variance τ^, while in turn each ct,ij is an estimation of its corresponding c^jt,i subject to a study-specific error ϵj, i.e., ct,i,xj=c^jt,i,x+ϵj.

the summary correlation ct,i,random is estimated with the fixed-effects weighted average, with the weights wj computed as inversely proportional to the sum of the study-specific and between-study variance, i.e., wj=1vj+τ^. interestingly, if all studies share the same ground truth effect , then the random-effects model reduces to the fixed-effects one.

the rank-product method differs from the previous approaches since it combines correlation ranks instead of correlations or p-values  <cit> . the vector ctj containing the correlations between the transcription factor t and all other probesets in study j can be easily converted in a vector of ranks rtj, where higher correlations rank first. the rank-product method combines ranks rt,i <dig>  …, rt,im from different studies by multiplying them: rt,i,rank−product=∏jrt,ij. true gene-gene interactions are then expected to be placed on the top of the vector rt of combined ranks.

the rank-product is actually a special case of a larger family of rank-based methods  <cit> , differing among each other mainly for the formula used for combining the single ranks . some authors have reported that rank-based methods can provide more reliable results than classical ma methods when heterogeneous datasets are analyzed together  <cit> .

a common drawback of these methods is that statistical significance must be assessed through permutation-based procedures, which usually are quite computationally demanding. however, in this study we adopt a recently introduced formula  <cit>  for computing approximate, yet accurate p-values for the rank-product results.



these five approaches were implemented in r and included in the analyses. moreover, we included one further method, namely the fr-effects model, based on a combination of fixed and random-effects models. in short, the fr-effect model first estimates τ^, and if the between-study variance is significantly different from zero  the random-effects model is used, otherwise the fixed-effects is used.

data-merging
in contrast with meta-analysis, the data-merging approach pools all data together and then estimates statistics on the resulting dataset. expression profiles measured in different studies, or even in the same study but in different batches, present systematic variations in their distribution  <cit> , and these variations are detrimental for the analysis. batch-effect removal methods attempt to alleviate this problem, by identifying and removing systematic biases. we selected five different dm approaches, among the ones most often used on microarray data:combat is a method specifically devised for removing batch effects in gene-expression data  <cit> . this method assumes the batches to be known, and that systematic variations follow an additive-multiplicative model

 ysjk=αj+xβj+γjk+δjkϵsjk' 

where ysjk is the expression of gene j in sample s in batch k, aj is the overall gene expression of j, x and βj are respectively the design matrix and the gene-specific coefficients vector, while the remaining terms are the additive and multiplicative batch effects, respectively. these effects are estimated through an approach that uses hyper-priors and pool information across all available probesets. we used the combat implementation of the r package sva in all analyses.rma  is an algorithm for background correcting, normalizing and summarizing microarray data. the normalization phase is carried out with the quantile normalization method, that substitutes the expression value of each probe t with the average expression calculated over all probes that rank equally across all available profiles. in our experimentation we used the rma function of the r package affy.

rma-combat. we also include the hybrid solution rma-combat, consisting in a pipeline that first applies the rma method and then combat.

surrogate variable analysis . the sva approach introduced by leek and storey  <cit>  attempts to identify and remove all confounding factors negatively affecting the analysis, including eventual batch-effects. similarly to combat, this method explicitly takes in account the study design. in the common case–control scenario, the sva model is the following: ysj=αj+βjxs+∑kγjkgks+ϵsj, where ysj is the expression of gene j in sample s, aj is the overall gene expression of j, xs is a binary variable indicating whether sample s is a case or a control, βj represents the average difference in expression between the two conditions in gene j, and ϵsj is a random error. the term ∑kγjkgks represents the cumulative effect on ysj of k unknown confounding factors gks, multiplied by their gene-specific coefficients γjk. sva attempts to estimate confounding factors’ global effect by deriving a set of surrogate variables h <dig>  h <dig>  …, hk whose span covers the same linear space spanned by the vectors gk. these surrogate variables can then be used as covariates in all subsequent analysis in order to rule out the effect of the unknown confounding factors.

to the best of our knowledge, no previous study applied sva on gene-network reconstruction, and a detailed discussion about how to adapt sva for this task is reported in the additional file  <dig>  briefly, assuming that each tfi has a significant effect only on a restricted subset of genes, all major systematic variations involving a large portion of transcripts should be due to experimental factors, batch-effects or confounding factors. given this assumption, for the data collections used in this study the sva model becomes: ysj=αj+∑kγjkgks+ϵsj. from a computational perspective this formulation implies that the surrogate variables are estimated by applying a singular value decomposition to the expression matrix, after having centered each gene on its mean. the estimated surrogate variables are then used for computing the vectors ct,sva and pt,sva. this means that ct,i,sva is a partial correlations  <cit> , quantifying the linear association between the transcription factor tft and gene i given the information embedded within the surrogate variables.

scaling the expression values of each dataset so that all genes have the same mean and standard deviation is a further suitable approach. in particular, we scale the expression of each probeset in each dataset to zero mean and unitary standard deviation.

no-correction. the naïve solution of pooling all data together without removing systematic variations is included in the analysis as well, in order to contrast the effectiveness of the other methods.



baseline approaches
a relevant question is whether employing complicate statistical techniques in order to co-analyze several datasets actually provides any advantage with respect to analyze a single dataset in isolation. dm and ma methods heavily process the data, following assumptions that are not always satisfied. consequently, these methods may induce biases rather than remove batch-effects. to answer this question we adopted a single-dataset approach, consisting in separately analyzing each dataset and then averaging the performance within each data collection. more in detail, let πΠ1 … πΠm be the performances obtained on datasets d <dig>  …, dm in collection m by using the metric Π. the single-dataset approach calculates a weighted performance πΠ=∑sj·πΠj∑sj, that can be interpreted as the result to be expected if a single dataset randomly chosen from the collection is analyzed.

finally, we also include a random-guessing approach consisting in randomly sampling ct,i from a uniform distribution. theoretically, we expect this method to achieve the lowest performances among all other algorithms.

reconstruction of the ikaros interaction network on pbmc data
generalizing the results of this work to any network learning algorithm is out of the scope of this paper. however, we perform a proof-of-concept application in order to provide initial evidence that the results obtained in the context of relevance networks, arguably the simplest type of reverse engineering networks, are also valid when more complicated algorithms are used.

to this purpose, we analyze a set of peripheral blood mononuclear cells  gene expression datasets extracted from geo. we attempt to reconstruct the regulatory network of the ikaros transcription factor by applying the ses  algorithm  <cit> . the predictions were validated against a list of experimentally determined ikaros targets as retrieved from the literature  <cit> . the ikzf <dig> gene encodes the transcription factor that belongs to the family of zinc-finger dna proteins  <cit> . ikaros displays crucial functions in the fetal and adult hemo-lymphopoietic system. it functions as a regulator of lymphocyte differentiation and its loss has been connected with the development of lymphoid leukemia.

the following sections describe in detail the used data and the analysis pipeline.

pbmc compendium and ikaros known regulatory relationships
we assembled a compendium of seven public microarray gene expression datasets of human pbmc. pbmc are the populations of blood cells having a round nucleus that constitute a pivotal part of the peripheral immune system. these include lymphocytes , monocytes, macrophages, dendritic cells. their abundance and the simplicity of their extraction  render them interesting candidate for scientific studies. note that the selection of human microarray datasets serves for further testing the validity of our results in the spectrum of human subject studies.

for assembling this compendium, only studies comprising randomly-selected healthy-control subjects were taken in consideration. in particular, for each study only the control group was retained for our analysis. the idea is that control groups formed by randomly chosen healthy individuals can be considered as independent sampling from the same population, and are thus suitable for being analyzed through ma and dm methods. in total, the collection counts  <dig> expression profiles all measured with the affymetrix human genome u <dig> plus  <dig>  array . the expression of ikaros is measured by nine of these probesets. we used in turn each of these probesets and we merged together their respective networks.

finally, a list iikaros of ikaros regulatory relationships was built from literature information and computational analyses. particularly, we built a list iikaros containing  <dig> unique interactions by merging together  <dig> ikaros targets identified through chip-seq and microarray analysis  <cit>  along with  <dig>   <dig>   <dig> and  <dig> ikaros-gene interactions found in cd43-  cd19+ , t-naïve and t-reg cells, respectively. these latter lists were derived from the analysis of dnase-seq data from the encode project  <cit> , following the approach presented in  <cit> . briefly, dnase hyper-sensitive regions  were identified using hotspot v <dig>  <cit> , and dhs peaks were subsequently scanned for footprints of dna-binding proteins by the wellington algorithm using pydnase  <cit> . transcription start sites  were obtained from the university of california, santa cruz  genes track, and the region flanking 5kb upstream to 5kb downstream of the tss was defined as the promoter region. the footprints within the promoters were subsequently scanned for identifying binding motifs specific for  <dig> transcription factors, using the transfac database  <cit>  and the match algorithm  <cit> . genes whose promoter contained a motif instance were considered as potential regulatory targets. this allowed identifying  candidate regulators and  candidate targets for each tf, including ikaros.

deconvolution of pbmc and outlier identification
the presence of different cell-types in the pbmc samples implies that expression values are averaged over a mixture of different distributions. subjects included in each study may have significantly different cell proportions, and this in turn may generate correlations among probesets that do not reflect any underlying gene-gene interaction  <cit> . in order to avoid this scenario, we estimate the cell-proportions for each sample through a deconvolution approach and then we eliminate subjects that appear to be outliers and that may prejudice the analysis. we use the deconvolution method introduced by abbas and co-authors  <cit>  and implemented in the cellmix r package  <cit> . this approach uses a fixed set of expression signatures characterizing the expression profiles of seventeen different cell types in order to estimate the proportion of these cell types in the pbmc data. the multivariate outlier detection was conducted by using the pcout  <cit>  algorithm from the “mvoutlier” r package  <cit> . this algorithm utilizes simple properties of principal components and is particularly effective in high-dimensional data.

ses algorithm
the ses algorithm  <cit>  as implemented in the ‘mxm’ r package was used in order to reconstruct ikaros regulatory network. the ses algorithm attempts to identify highly predictive signatures for a given target. in this context, a gene expression signature consists of the minimal set of gene expression measurements that is necessary in order to predict the value of ikaros. as demonstrated in  <cit> , the signature of a target corresponds, under broadly accepted assumptions, to the variables that are adjacent to the target in the bayesian network representing the data distribution at hand. consequently, these gene expression signatures also correspond to the set of potential regulators/targets of ikaros in the context of the available measurements. lack of statistical power may make two or more signatures statistically indistinguishable. the ses algorithm is specifically devised in order to cope with this problem and to attempt to retrieve statistically equivalent signatures.

ses belongs to the class of constraint-based, bayesian network reconstruction algorithms  <cit> . while relevance networks assess the presence of gene-gene interactions through simple pairwise correlations, constraint-based algorithms use tests of conditional independence in order to find variables that are associated to the target given any subset of other measurements. this implies that ses should return only genes whose association with ikaros is not mediated by any other measured gene. in contrast, relevance network cannot distinguish among direct and indirect associations.

ses requires the user to set a priori two hyper-parameters, a threshold for assessing p-values significance and the size of the maximum conditioning set. in our analyses these hyper-parameters were set to  <dig>  and  <dig>  respectively. the signatures found on single probesets were merged together, as well as the results retrieved on the nine different probesets measuring ikaros.

network reconstruction and validation
based on our previous findings, we picked the combat and fixed-effects methods as representatives for the dm and ma approaches, respectively. we also used the no-correction and single-dataset approaches in order to characterize the scenarios where batch-effects are ignored or a randomly chosen dataset of the pbmc collection is analyzed in isolation. for the combat and no-correction approaches the deconvolution and outlier deletion steps were performed on their respective merged datasets, while for the fixed-effects and single-datasets methods the two pre-processing steps were performed independently for each study of the pbmc collection.

network reconstruction performances were measured in terms of precision, recall and odds ratio. let qikaros,x be the list of ikaros interactions retrieved using ses couple with the ma or dm method x, and ¬ iikaros the list of genes that are not part of ikaros regulatory network . precision is defined as precx=qikaros∩iikarosqikaros, and indicates the proportion of actual interactions that are present in the retrieved signature. recall  is computed as recallx=qikaros∩iikarosiikaros, that is the proportion of genes that are in the ikaros regulatory program and are classified as such.

the odds ratio quantifies the likelihood that a given proportion of regulatory relationships is retrieved by chance, and is computed as oddsratiox=precxprectrivial, where prectrivial=iikarosn represents the sensitivity achievable by classifying all n genes as belonging to the ikaros regulatory program. an odds ratio of one indicates performances that are indistinguishable from random guessing, and we used a hypergeometric test  <cit>  in order to assess the null hypothesis h0 : oddsratiox =  <dig> 

RESULTS
e. coli and yeast compendia
figure  <dig> and additional file 1: tables s <dig> – s <dig> report the results on the e. coli and yeast compendia computed using the pearson correlation. results based on spearman correlation follow similar patterns and are reported in the . panels in the top row present the results obtained on the e. coli compendium, while findings on the yeast collection are summarized in the other two subplots. each panel reports two different performance metrics. the panels on the left side summarize global performance metrics, having the auc on the x-axis and the auprc on the y-axis. subplots on the right side report partial performances, with the pauc on the x-axis and the aufdr on the y-axis. ma, dm and baseline methods correspond to circular, triangular and square markers, respectively. in each panel, the size of each marker is directly proportional to average between the coefficients of variation  computed on the x and y-axis metric. the cv is a convenient way for representing variability with respect to the order of magnitude of the measurements, and is computed as the ratio between standard deviation and average value. non-filled markers indicate methods that are statistically significantly different from both methods that perform best in the two metrics .fig.  <dig> results of the experimentations on e. coli and yeast compendia using the spearman correlation. panels on the left side report global performance metrics , while panels on the right report partial performance information . results in the top row are computed on the e. coli dataset compendium, while results on the yeast dataset collection are reported in the other two panels. ma, dm and baseline methods are indicated with circular, triangular and square markers, respectively. non-filled markers indicate methods that are statistically significantly different with respect to the best performing ones in both metrics . the size of each marker is directly proportional to the coefficient of variation  between its respective metrics



all four panels present a similar picture, with several dm and ma methods clustering together and achieving comparable performances, while the random-guess, single-dataset and no-correction approaches usually providing significantly worst results. best performing methods usually present a variability that is smaller than the one of the outperformed methods.

all in all, the results show that systematic biases across studies must be taken into account for retrieving gene-gene interactions, and that both ma and dm approaches are effective in dealing with such systematic variations.

retrieving gene-gene interactions in the yeast dataset collection have proven to be harder than in e. coli. performances were generally poorer, with auc and pauc values up to  <dig> point inferior than the corresponding performances in the e. coli compendium, and both auprc and aufdr ranging far below  <dig> .

results are further summarized in fig.  <dig> through a rank-product analysis. the combination of both e. coli and yeast compendia with the two correlation measures and the four different metrics provides a total of  <dig> different ways to rank ma and dm methods according to their performances. these sixteen ranks are synthesized with the rank-product method and the final results are reported in the top panel of fig.  <dig>  all methods are listed on the x-axis, ordered from left to right according to log-transformed rank-product score . higher scores characterize methods that consistently achieve the top positions across all ranks. rank statistical significance is assessed with the methods reported in  <cit> , and p-values <  <dig>  are indicated with filled markers. the coefficient of variability for each method determines the color of the corresponding marker, with lighter color corresponding to higher cv.fig.  <dig> rank-product analysis of ma and ber methods. methods are ranked according to their performances, separately for each combination of data compendium , correlation measure  and performance metric , for a total of  <dig> different ranks. these ranks are then combined using the rank-product method, and the statistical significance of the ranks are evaluated with the method reported in  <cit> . the negative logarithm of the rank-product score is reported on the y-axis, while methods are listed on the x-axis. triangular markers indicate ber methods, round markers ma methods, square markers baseline approaches. the color of each marker is directly proportional to the coefficient of variation  of the respective log-transformed rank-product score . methods that tend to be consistently ranked in the top positions are placed on the top-right of the plots, while poorly performing methods remain the in the bottom-left corner. the plot on the top report the global, final rank of both ma and ber methods, while the two plots on the bottom focus on ber and ma methods, respectively



the sva, combat, rma-combat, fixed-effect and scaling methods are confirmed as the best performing methods, occupying the first position in the rank-product analysis. sva shows a relatively high variance, indicating that sometimes it fails in reaching the top positions in terms of performances. the random-guess approach is stable in last position, followed by the single-dataset, stouffer and no-correction methods. the two bottom panels in fig.  <dig> restrict the rank-product analysis to the dm and ma methods, respectively. the sva, rma-combat and combat method should be the methods of choice within the dm approaches, while fixed-effects, rank-product and fisher excel among the ma methods.

similar figures restricting the rank-product analysis to global and local performances only, as well as pearson and spearman correlations and e. coli versus yeast are available in the additional file  <dig>  the conclusions that can be drawn from these figures are in close agreement to the ones discussed until now.

figure  <dig> reports the performances computed using the vector of p-values pt,x instead of the correlation values ct,x. in e. coli there is a dramatic worsening in performances for most of the methods. a decrease in performances can also be observed for the yeast compendium, although to a lesser extent. a possible explanation for these patterns is the presence of several high-significant correlations, whose corresponding p-values are exactly zero or too low to be distinguished at machine precision. these zero p-values create ties that severely affect the ranking of the candidate interactions and consequently the evaluation of the performances.fig.  <dig> results of the experimentations on e. coli and yeast compendia using the spearman correlation p-values. details as in fig.  <dig>  methods generally achieve lower performances when p-values are used instead of correlations for ranking candidate gene-gene interactions. this is mainly due to the prevalence of close-to-zero p-values that create ties negatively affecting the performance metrics



a close inspection of the results seems to confirm this hypothesis. table  <dig> reveals that methods showing a large performance decrease in e. coli between the ct,x and pt,x -based results have a large percentage of p-values that are exactly zero. sva, rank-product and fixed-effects methods do not produce zero p-values, and they do not suffer any performance loss. however, random-effects and fr-effects do not produce zero p-values as well, and they still achieve worse performances when pt,x is used instead of ct,x. the answer to this issue lays in the fact that there is not a bijective correspondence between ct,x and pt,x for the random-effects methods, and consequently neither for the fr-effects one. in other words, if ct,i > ct,j holds, then pt,i < pt,j holds as well if the correlations are computed with the fixed-effects model, but not if they are computed with the random-effects method. the statistical significance of correlation in the random-effects approach depends on the estimation of the between-study variance τ^, and this variance is separately estimated for each correlation. consequently, candidate interactions are ranked differently by the random-effect model depending whether correlations or p-values are used, and the results seem to indicate that the ranking provided by the correlation values better reflects the actual underlying gene-gene interactions.table  <dig> proportion of p-values being exactly zero for e. coli and yeast, pearson correlation results 

the majority of dm methods assigns a zero p-value to some percentage of the predictions, while only the fisher and stouffer ma methods do so. these percentages are higher in e. coli than in yeast, suggesting that in the first compendium the statistical associations are stronger or more detectable due to higher statistical power



synthetic data
the results on simulated data for the auc metric are reported in figs.  <dig> and  <dig>  results on other metrics follow similar patterns, and the respective figures are reported in the additional file  <dig>  the numerical results for all simulated scenarios are in additional files  <dig> and  <dig>  as expected, results improve for increasing number of studies or samples, while larger level of systematic bias corresponds to worse performances. the single-dataset approach is systematically outperformed by ma or dm methods in all scenarios. the no-correction approach also achieves poor performances for high level of batch-effects, even though it is quite competitive for mild systematic biases. auc ≈  <dig>  for the random-guess approach in all cases. the remaining ma and dm methods achieve comparable performances, both in terms of average performance and respective variance. sva seems to be an exception, thought, achieving quite lower performances. quite surprisingly, sva performances drop significantly with the maximum total sample size, i.e., when  <dig> studies with  <dig> samples each are analyzed . concomitantly, the number of surrogate variables estimated in these setting is ~ <dig>  versus ~5- <dig> when the total sample size is lower. we argue that such an elevated number of surrogate variables negatively affects the computation of conditional correlations, leading to a worsening in performances.fig.  <dig> auc results on simulated data for different number of studies using pearson correlations. each row reports the results obtained on the data collections including  <dig>   <dig> and  <dig> studies, respectively. for each row the performances of each method are reported for level of systematic bias τ equal to  <dig> ,  <dig>  and  <dig>  all results are averaged over five different synthetic networks and different sample sizes . standard deviations are indicated by the whiskers at the top of each plot. sd stands for single-dataset, while fem, rem and frem stand for fixed, random and fr-effects method, respectively

fig.  <dig> auc results on simulated data across different sample sizes using pearson correlations. each row reports the results obtained for a given sample size . for each row the performances of each method are reported for level of systematic bias τ equal to  <dig> ,  <dig>  and  <dig>  all results are averaged over five different synthetic networks and different numbers of studies included in each collection . standard deviations are indicated by the whiskers at the top of each plot. sd stands for single-dataset, while fem, rem and frem stand for fixed, random and fr-effects method, respectively



also for the synthetic data results computed using the p-value vectors pt,x show a decrease in performance . particularly, across all simulation scenarios, correlation functions and performance metrics results based on correlations outperform the corresponding results based on p-values 52 % of the times. the average difference in performance varies depending on the metric:  <dig>  for auc,  <dig>  for auprc,  <dig>  for pauc and  <dig>  for aufdr. interestingly, this effect becomes more marked with increasing sample size and decreasing systematic bias , confirming that the performance loss is due to an excess of statistical power that generate zero or close to zero p-values.

reconstruction of the ikaros interaction network on pbmc data
table  <dig> summarizes the results of the reconstruction of the ikaros regulatory program on pbmc data. combat achieved the best performances, followed by the fixed-effect method, the single-dataset approach, and no-correction. all methods achieved odds ratio statistically significantly different from one at the  <dig>  level. for the single-dataset approach, the results actually varied depending on the specific study, ranging from highly significant  to random guessing . we correlated the odd ratios and p-values achieved on each dataset with the sample size, and interestingly no association was detected .table  <dig> reconstruction of ikaros regulatory program in pbmc data collection. for each method the number of predicted and correctly retrieved interactions is reported, along with the odds ratio, precision and recall performances 

odds ratio statistical significance is assessed through the hypergeometric test. for the single-dataset approach distinct performances and significance p-values were computed for each dataset, summarized here as a weighted average of the performance and with the interval spanned by the p-values, respectively



figure  <dig> reports the ikaros regulatory program reconstructed on the pbmc data using ses coupled with combat. yellow nodes indicated genes included in iikaros.fig.  <dig> ikaros regulatory program as reconstructed by applying the ses and combat algorithms on pbmc data. correctly retrieved interactions are marked in yellow



discussion
in the present work we have compared two different approaches, data-merging and meta-analysis, on the reconstruction of relevance networks in collection of microarray, gene-expression data. the comparison has been performed on two compendia of studies retrieved from the literature, on escherichia coli and yeast, respectively. further analyses on simulated data have been used for strengthening and deepening the conclusion of the comparison. finally, a contrived case-study on human pbmc data have been presented for showing how the results of this study might transfer on more sophisticated network reconstruction approaches.

the results on both simulated and real data provide coherent conclusions, which can be summarized in the following points:batch-effects must be carefully taken into consideration for retrieving gene-gene interactions from microarray data. the naïve solution of ignoring systematic biases  was outperformed by the other methods in all experimentations. this result supports our claim that batch-effects can hide actual dependencies between the measured quantities or create spurious associations between elements that are not functionally related.

dm and ma methods are equally effective in contrasting batch-effects. according to the results it is not possible to state that one approach is universally better than the other one. however, within their respective approaches, and acknowledging that the results vary across the performed experimentations, the sva/combat/rma-combat and the fixed-effects methods have usually achieved the best performances. in contrast, the single-dataset method usually provides poorer results, supporting the hypothesis that integratively analyzing multiple datasets leads to improved and more robust findings.

correlation statistics should be preferred to p-values in ranking associations. performances have proven to drastically change depending on whether they are computed on correlations or p-values. we have observed that this effect is mainly due to ties generated by zero or close to zero p-values.



this study presents a number of limitations that should be carefully considered when implementing the recommendations above. first, within-study batch-effects were only partially addressed, by pre-processing each single dataset with rma. while the quantile normalization step included in the rma algorithm should have removed at least part of the within-study biases, it is known that this approach is not optimal  <cit> . this is also demonstrated by our results, where the rma method never achieved the best performances. secondly, the design of the comparison slightly advantages dm method, particularly because all datasets belong to the same data collection and thus measure the same probesets. when this is not the case , dm method are not easily applicable, while ma methods can be straightforwardly used. finally, we also notice that in our experimentations we did not explore joint uses of correlations and p-values for ranking gene-gene interactions. a possible practice is to filter the candidate interactions by using the p-values and then raking the most significant gene-pairs according to their correlation values.

the sva method merits a separate note. to the best of our knowledge, this is the first study employing this methodology in the context of retrieving gene-gene interactions. adapting sva for this task has required a dedicated sub-study, reported and commented in the additional file  <dig>  despite the excellent performances obtained on the real data, we notice that this method performed quite poorly on the synthetic data. this drop in performances is particular evident for large samples sizes. a possible explanation might be the inclusion of several irrelevant surrogate variables when large datasets are analyzed: out of  <dig> surrogate variables produced when  <dig> samples are available in the merged dataset, only  <dig> explain more than 1 % of variance. these noisy variables might in turn make the estimation of partial correlations and respective p-values quite inaccurate. further studies are needed in order to better investigate this phenomenon.

future work will also focus on the generalization of the present results towards more sophisticated network reconstruction algorithms, particularly bayesian and causal networks  <cit> . we already presented a first, contrived case-study where we have reconstructed  the regulatory network of the ikaros transcription factor from human pbmc data. this case-study presented several characteristics that made it harder to solve than the reconstruction of the e. coli and yeast regulatory networks: different cell-type proportions across subjects, a many-to-many correspondence between genes and probesets, the list of known interactions was partially derived from animal models instead than human data. moreover, we used a constraint-based network reconstruction algorithm instead of relevance networks. despite all these difference both combat and fixed-effects method demonstrated to be able to retrieve subsets of genes significantly enriched for known ikaros interactions and to outperform both the no-correction and single-dataset approach, as expected from the results of the comparison presented in this study.

CONCLUSIONS
batch-effects should be carefully taken into account when retrieving gene-gene interactions, and researchers can adopt either a dm or ma approach depending on the specific application at hand. correlation statistics should be preferred over p-values for assessing and comparing the strength of associations, especially for large sample sizes.

availability of supporting data
the data sets use in this article are available from their respective repositories. see tables s <dig> to s <dig> in the additional file  <dig> for the appropriate references.

code for replicating the analysis is available at http://www.mensxmachina.org/.

additional files
additional file 1: the supplementary material provides additional data and results supporting the conclusions of the study, including detailed descriptions of the e. coli and yeast data compendia as well as all results produced on these compendia. 

 additional file 2: using surrogate variable analysis for network reconstruction. this additional file presents a sub-study investigating modifications of the sva model that allow to use the sva method on network reconstruction tasks. 

 additional file 3: simulations results based on correlations. the simulation results table presents the results obtained on the synthetic data by using correlations as measure of association. 

 additional file 4: simulations results based on p-values. the simulation results table presents the results obtained on the synthetic data by using the correlation p-values as measure of associations. 

 competing interests

the authors declare that they have no competing interests.

authors’ contributions

vl, dgc and it devised the study, vl, gs and ak performed the analysis, vl and ak wrote the manuscript. all authors read and approved the final manuscript.

