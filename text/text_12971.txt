BACKGROUND
scientific documents provide useful information in many domains: an example is microorganism ecology, which involves a variety of microorganisms  and habitats  that have been described in detail in the literature. however, reading the huge amount of articles published nowadays is too time-consuming for a human. natural language processing  techniques have therefore been designed to process these documents quickly and make the extracted information available for further studies.

the identification of mentions of bacteria and biotopes in scientific texts has been addressed during the last two bionlp bacteria biotopes shared tasks  <cit> . detecting mentions of microorganisms and habitats is indeed a first step to access the semantic content of these texts. identifying relationships between the detected entities is the next step to represent the knowledge conveyed by a text more completely. this is the topic of the present work.

the model emphasized here, as proposed by the bacteria biotopes task organizers, is that of knowledge acquisition from texts. this differs from information extraction in the following respect: instead of focusing on each mention of an entity in a text and each occurrence of a relation between two entity mentions, it takes a step back to look at entities and their relations. this has two important consequences:

• as emphasized by the organizers  <cit> , this requires to detect which mentions refer to the same entity, i.e. entertain a co-reference relation. this can be modeled as co-reference chains, i.e. equivalence classes of mentions which all co-refer to the same entity.

• relations must be found at the level of entities, i.e. at the global level of full co-reference chains instead of individual entity mentions.

co-reference has been addressed in information extraction as far back as in the muc- <dig> evaluation campaign  <cit> . an anaphora is a dependence relation between two referential expressions, while co-reference holds among referential phrases that refer to the same referent in a text; a co-reference chain collects the full set of co-referential phrases and can include anaphora and/or co-references .

co-reference resolution consists in finding in a text all expressions referring to a given entity, regardless of the surface forms of these expressions  <cit> . it has been identified as an important need for information extraction both in clinical texts  <cit>  and in the scientific literature, including molecular biology  <cit> . protein co-reference resolution was proposed as a supporting task  in the bionlp shared task  <dig>  <cit> . in this purpose, co-reference relations were annotated in a training corpus and provided to the participants. the best participant system  <cit>  reused and extensively adapted the existing co-reference resolution framework reconcile  <cit> .

several other systems  <cit>  used syntactic parsers and rules to detect co-reference relations. they follow a similar overall schema: determine the set of anaphors  and potential antecedents  based on the results of syntactic analysis; then given an anaphor, the rules enforce the compatibility of antecedent candidates with the anaphor and manage a notion of salience or discourse preference. the selection of the best antecedent candidate is done in a sieve-like manner  or through a score  <cit> .

however, after the  <dig> shared task,  <cit>  designed a rule-based method based on domain-specific information, whose minimal configuration outperformed the participant systems. they emphasize the difficulty to transfer such knowledge across domains. the organisers of the bacteria biotope task stressed in both the  <dig> and  <dig> bionlp shared tasks that co-reference resolution was important to detect bacteria and biotope relations  <cit> . we too hypothesize that co-reference resolution should be important to acquire knowledge about relations between bacteria and their locations in the bionlp-st  <dig> bacteria biotope  task. despite this general opinion, according to the bionlp-st  <dig> workshop proceedings, no participant in the  <dig> bb task investigated how co-reference impacts this task. this is the focus of this paper.

our general strategy has been to start from state-of-the-art methods, combine them and optimize them to address the bacteria biotope requirements, to obtain an evaluation of their worth in the framework of the present task. therefore, whereas the choice of features that we made is specific to the task, the strategy used to determine these features, create the system, optimize it and evaluate it should be general and applicable to a wide range of tasks.

this paper presents the following contributions:

• an end-to-end system for mention and relation detection in the bacteria biotope corpus;

• the design of a co-reference resolution component meant to help relation detection in this task, and its evaluation against the gold co-reference annotations provided with the corpus;

• a study of the impact of co-reference resolution on relation detection.

the system has state-of-the-art performance compared to bionlp-st  <dig> participant systems, which it outperforms when co-reference resolution is applied.

in the remainder of this paper, we introduce the corpus provided for the bionlp-st  <dig> bacteria biotope task. we then present the methods we designed to perform mention identification, co-reference resolution, and relationship identification. we report and discuss the results obtained on each of these three sub-tasks after the official challenge, and conclude with our views on requirements for further progress on this task.

corpus presentation
the corpus comprises web pages about bacterial species written for non-experts. each text consists of a description of individual bacterium and groups of bacteria, in terms of first observation, characteristics, evolution and biotopes. the corpus is split into three sub-corpora: training corpus , development corpus , and test corpus .

gold annotation statistics on training and development corpora.

two types of relationships between entities occur in this corpus: a localization relation  and a partof relation. the localization relation occurs between a bacterium  and a host  while the partof relation occurs between two entities of the same type.

• six annotated entity mentions belonging to the three entity types:

- bacteria: "borrelia afzelli pko", "borrelia afzelli" and "borrelia";

- habitat : "skin lesion from a lyme disease patient in europe" and "lyme disease patient in europe";

- geographical : "europe".

• three localization relationships between:

- bacteria and habitat mentions: "borrelia afzelli"  located in "skin lesion from a lyme disease patient in europe" and "lyme disease patient in europe" ;

- bacteria and geographical mentions: "borrelia afzelli"  and "europe" .

• a partof relation between two habitat mentions: "lyme disease patient in europe"  and "skin lesion from a lyme disease patient in europe" .

moreover, as explained in the methods section , the gold standard co-reference relations are used by the official relation scoring system to conflate co-referring mentions when comparing a gold relation annotation and a system relation annotation. knowing which mentions are thus considered as equivalent is therefore all the more important.

methods
in general, given a specific task, two broad strategies can be distinguished:  use a very generic framework or system and apply it to the specific task at hand; this is the choice implemented for instance by  <cit> , who applied the same system, with minimal specific extensions, to all the bionlp  <dig> tasks;  extend a combination of state-of-the-art methods by finely tuning them to this task; we opted for this second strategy, assuming that it has better potential to fit the task and obtain better results.

we process each document in three steps implemented in three modules . in the end-to-end system , each module takes as input the results of the preceding modules. when gold mentions are provided , the pipeline starts with step  <dig> 

 <dig>  mention identification takes as input a text and produces entity mentions, each with its span and type;

 <dig>  co-reference resolution produces co-reference chains between these mentions, possibly adding anaphoric expressions that point to some of these mentions; these anaphoric expressions constitute new  entity mentions;

 <dig>  relation detection produces binary relations between entities; each of the two entities linked by a relation is represented by one of its mentions found at step  <dig> 

to identify entity mentions and to detect relationships between these mentions, we relied on machine-learning methods.

entity detection is well modelled as a sequence classification task. conditional random fields  and structured support vector machines  are state-of-the-art methods for sequence classification and generally obtain similar results . since one of the authors  is the author of a very competitive log-linear toolkit which includes crfs  <cit> , we chose this classifier over others.

relation detection is modelled as a standard classification task. among state-of-the-art classifiers, maximum entropy  and support vector machines  are popular classifiers which obtained good results on previous tasks . our log-linear toolkit also covers maxent classification, hence our choice of this classifier for relation detection.

to summarize, we used two distinct formalisms implemented in the wapiti system  <cit>  to build our models:

• conditional random fields   <cit>  to identify bacteria and biotope mentions;

• maximum entropy   <cit>  to detect the relationships between entities.

when introducing the corpus, we explained that co-referring expressions and co-reference relations are not fully annotated in the training and development corpora. this does not provide good conditions to train supervised machine learning methods, and prevented us from trying to adapt an existing trainable system such as reconcile  <cit> . instead, we adopted linguistically-inspired rule-based methods which take advantage of the particular discourse structure and of the particular types of referring expressions of the input texts.

mention identification
to identify bacteria and biotope mentions in the text, we used a crf-based framework we specifically designed for this task  <cit> . we did not perform any cross-validation but automatic feature selection was carried out through the l <dig> regularization. we built our model using both "classical" internal features  and a few lexical features:

• presence of the token in the ontobiotope ontology terms http://bibliome.jouy.inra.fr/mem-ontobiotope/ontobiotope_bionlp-st <dig> obo, this resource was provided by the organizers for the purpose of the normalization stage in the first sub-task). this ontology comprises  <dig>  concepts from the biotopes domain. in this ontology, each concept has been given a unique id and is associated with preferred terms and associated synonyms. we noticed that  <dig> % of the tokens from both training and development corpora found in the ontology correspond to a habitat name in the reference.

• presence of the token in the ncbi taxonomy. the ncbi taxonomy   <cit>  describes a small part  of the living species on earth, based on public sequence databases. we extracted from this taxonomy all names belonging to the bacteria category , resulting in a list of  <dig>  bacteria taxa, including a few variants of bacteria names. we noticed that  <dig> % of the tokens from both training and development corpora found in this subset of the ncbi taxonomy correspond to a bacteria name in the reference annotations.

• we used the cocoa  annotation categories. indeed, we observed that a few annotation categories are often tied with one of the three kinds of entities we have to process: cell, chemical, mutant organism, organism, protein, unknown with bacteria mentions, body part, cell, cellular component, chemical, disease, food, geometrical part, habitat, location, multi-tissue structure, organism, organism subdivision, pathological formation, tissue with habitat names and company, habitat, technique, unknown with geographical names.

• finally, we also created unsupervised word clusters using brown's algorithm  <cit>  with liang's code   <cit>  on a total amount of  <dig>  scientific documents:  <dig> documents from the training, development and test sub-corpora, and  <dig>  new documents from the same sources. these new documents were not used during the challenge. they have been provided by the organizers as part of a distinct research project, for additional experiments based upon the methods we implemented in the bionlp  <dig> shared task. we used the following parameters to build those clusters: creation of a total amount of  <dig> classes, using all tokens that occur at least two times in the overall corpus, with a maximum depth of three levels during the trees building.

the annotation scheme allows for nested entity mentions, i.e. the span of a mention can be included in the span of another, larger mention. to deal with nested entity mentions, we built a list of the nested mentions found in the training and development corpora. for instance, respiratory tracts of animals and animals are two habitat mentions where the latter is nested in the former. we recorded the latter as a mention that can be found within a larger span. we used the recorded list in a post-processing stage: when an occurrence of these bacteria or biotope mentions was found within a mention predicted by the crf model, we added it as an additional mention.

from the excerpt shown in figure  <dig>  our pipeline identified the phrases "borrelia afzelii pko" and "borrelia afzelii" in the ncbi taxonomy; the phrases "skin lesion", "patient" and "europe" in the ontobiotope ontology; and the following phrases were annotated by cocoa: "borrelia afzelii", "species", "patient", "organism" and "borrelia species" , "skin lesion" , "lyme disease", "acrodermatitis chronica atrophicans" and "aca" , "europe" , "monoclonal antibody"  and "hybridization" ; cluster id were given to each token using the brown algorithm. the crf used those features in combination with surface feature to identify bacteria and biotope entity.

co-reference resolution
co-reference resolution over entity mentions consists in identifying on the one hand a referring expression , i.e. an expression which co-refers to an already mentioned entity, and its antecedent, i.e. a former entity mention with which it co-refers. the term anaphor generally specifically applies when the referring expression points at a preceding expression in the text , as opposed to a free-standing referring expression such as a proper name which does not need this condition to obtain .

we can summarize as follows the principles driving our approach to co-reference resolution in the present context. co-reference resolution is addressed here as a means to cluster entity mentions into equivalence classes representing entities, so that relation detection can be managed at the entity level, as detailed in the next section. we consider this requires to emphasize co-reference resolution precision over recall: a false positive co-reference is liable to lead to the incorrect propagation of relations to larger clusters of mentions; whereas a false negative co-reference is likely to split a cluster into two smaller clusters of mentions, possibly propagating a relation to fewer mentions, and only provoking a missed relation if the relation is only expressed once on an anaphoric expression. therefore our goal here is not to setup the most complete and general co-reference resolution system, but one that will be useful for relation detection in the present corpus, oriented towards a high precision while having a reasonable coverage. note also that co-reference resolution is only considered here among mentions of the three types of entities addressed in the bacteria biotope task, i.e. bacteria, habitats, and geographical locations. this, and other properties described below, makes co-reference resolution in the present corpus and task distinct from previously reported work, e.g., in the bionlp  <dig> coref task  <cit> .

some general co-reference resolution systems are available for download. we tried two of the best: reconcile  <cit>  , based on machine learning, has state-of-the-art performance on common co-reference resolution test sets, such as muc- <dig>  muc- <dig>  and ace; and the stanford deterministic coreference resolution system  <cit> , based on deterministic rules, which was the top ranked system at the conll- <dig> shared task. however, because these generic systems do not have specific knowledge about which entity types are targeted in the present corpus, off-the-box application of these systems reveals issues in the determination of entity mentions and attempts to find co-reference relations with other noun phrases in the texts. this makes their results poor for bacteria and biotope co-reference resolution in the present corpus. to illustrate this point, we provide more details on the performance of reconcile in the results section. we obtained these results by running reconcile on the training and development corpora and converting its output to the format expected by the conll- <dig> co-reference resolution scorer . as mentioned in the introduction of the methods section, re-training a system such as reconcile, as performed in  <cit> , was not an option because of the absence of full co-reference annotations in the training corpus. as a consequence, we designed and implemented a heuristic, rule-based co-reference resolution method which relies on a linguistic observation of the training corpus.

three types of anaphors can be distinguished in general:

• pronouns ;

• definite noun phrases , including those with a possessive adjective ;

• proper names , to which can be added named entities .

pronouns are usually the most ambiguous type, and are more likely to incur false positive co-references. for instance, in sentence  are facultatively anaerobic  that are metabolically similar to the . they are ubiquitous to , , and ., the pronoun they could refer either to the first bacterium name vibrios, to the generic name bacteria, or to the closest name enterobacteriaceae, making it difficult to choose the correct antecedent. besides, possessive pronouns  nearly never play a role in the training corpus to help relation extraction. we thus decided not to handle pronouns at all. we return on this choice in the results section. an important difference between the present task and the coref task of the bionlp  <dig> shared task  <cit>  is that the latter did not address entity mentions as anaphors. in contrast, in the present task, only entity mentions are linked by gold co-reference relations.

co-reference necessarily obtains between mentions of the same entity type. because of that, it heavily relies on the former detection and typing of entity mentions in the source text. however, the entity mentions targeted in the entity identification step only cover named entities, including proper names, and some noun phrases . this means that anaphors of the other types must be detected in the present step. we return to this point later.

we adopted two strategies to handle definite noun phrases and proper names:

similarity of form co-referring proper names should be identical or differ in a minimal way. an edit distance  <cit>  is a convenient way to compare two strings and ensure that they differ by less than a number e of edit operations. strube et al.  <cit>  obtained improvements in the processing of definite noun phrases and proper names by adding a minimum edit distance comparison to their feature set. there are however conventional ways of abbreviating bacterium names, such as writing b. subtilis for bacillus subtilis. we therefore implemented the detection of a series of such conventional abbreviations. this strategy is also applicable to some extent to definite noun phrases , although in principle the co-reference of such anaphors would better be obtained indirectly through their having the same antecedent bacterium strain . furthermore, we noticed that bacteria mentions containing only the term bacteria, bacterium or bacterial are very generic; because of that, they seldom carry a relation annotation.

besides, their co-reference can be fairly ambiguous because such a term can refer to different bacteria in the same text. we therefore decided to block similarity-based co-reference detection for these expressions. finally, we tested constraints on the maximum hierarchical distance between a mention and its antecedent : we designed patterns to detect the hierarchical level of each bacterial mention along the cline {strain <subspecies <species <genus <family . . . }, and set a maximum hierarchical distance  allowed between two co-referring mentions.

actual anaphora resolution an anaphora occurs when an anaphor points to a preceding expression in the text. typical examples include the use of pronouns  and definite noun phrases , possibly involving a demonstrative adjective . we provide more detail on our handling of anaphora resolution in the remainder of this section.

we observed in the training corpus that among the three types of entities, anaphora mostly involves bacteria whereas similarity-based co-reference is a prevalent phenomenon among habitat mentions and among geographical locations, and also applies to bacteria. actually, as presented in the corpus section above, each document is centered on a bacterium or group of bacteria, in the style of an entry in an encyclopedia. our anaphora resolution methods therefore exploit the structure this induces on the documents, with one or more bacterium entries and their descriptions. this generates the use of frequent anaphora patterns such as sentence-initial this bacterium expressions referring to the current bacterium entry.

based on a study of the training corpus, we modelled this as follows:

• centering. we track the focus bacterium all along a text. in an approximation of the centering mechanisms  <cit>  at work in text cohesion and anaphora resolution, we consider that the focus bacterium is initially the first bacterium in the document ; in each new paragraph, the first mention of a bacterium becomes the focus bacterium for the paragraph.

• recentness. we also track the last bacterium mentioned in the text. the combination of centering and recentness plays the same role as, e.g., the salience measure of  <cit>  or the discourse-type rules of  <cit> .

• sentence-initial definite descriptions of bacteria including a demonstrative adjective. as in  <cit> , we collected in the training corpus the most frequent head nouns referring to bacteria. this resulted in the following patterns: this  and this group of organisms, whose occurrences are considered to refer to the last bacterium.

• generic sentence-initial definite descriptions of bacteria including a definite determiner ) are considered to refer to the focus bacterium. the reasoning here is that because they do not use a demonstrative adjective, they instruct the reader not to consider the last bacterium; and because they are not more specific about a particular species or strain, they enforce no other particular constraint on their antecedent, so default to the current focus.

• more specific sentence-initial definite descriptions of bacteria including a definite determiner ) enforce a particular constraint on their antecedent: not only being any kind of bacterium, but more specifically being a genus, or a species, or a strain. they should thus select a focus bacterium which has the specified property. however, this selection of a specific antecedent was not implemented at the time of submission, and was replaced with the simple choice of the last bacterium.

• because they are more ambiguous, non-sentence initial referring expressions  ) were not considered.

restricting the type of antecedent to bacteria entity mentions for the selected demonstrative and definite noun phrases is similar to the filtering of antecedents of demonstrative noun phrases implemented in  <cit>  or to the semantic type classification of  <cit> . the latter mention that not using such constraints on semantic types causes a loss of  <dig> points in precision and  <dig> points in f-measure.

the co-reference resolution module takes as input the entity mentions found by the mention identification module; it can also be tested on gold mentions.

similarity-based co-reference resolution is run first and only uses these pre-computed mentions. it processes them in the order of the text; for each mention, it finds each anterior mention of the same type which either is identical, obeys the strain matching rule or the name matching rule, or differs by at most e edit operations among character insertion, deletion, substitution or swapping .

anaphora-based co-reference resolution is run next: it scans the text from start to end, looking for instances of referring expressions as defined above. for each referring expression, it determines an antecedent  as explained above.

after co-referring mention pairs are thus recorded by the two strategies, transitive closure is applied to compute equivalence classes, thus forming co-reference chains including some of the input mentions and possibly the added referring expressions.

relationships identification
let us recall first that the problem to solve at this stage must be managed at two levels:

the knowledge level with domain entities and relations between these entities; and

the information level with instances of relations between mentions of these entities found in the texts. for convenience we shall generally abbreviate this term as relation instance.

thus we need to detect relations between entities  based on the observation of relation instances . moreover, the gold standard in the training and test corpora is specified through relation instances between entity mentions  which are meant to represent relations between entities . evaluation is performed at the knowledge level by taking into account gold co-reference chains, as explained by the task organizers  <cit> ; these co-reference chains are not provided with the test corpus.

ideally we want to address this situation by embedding a classical information extraction framework, which learns to detect relation instances between two entity mentions based on a maximum entropy framework, within the knowledge acquisition framework:

from knowledge to information: in the training phase, we use the input mentions, the co-reference chains, and the gold relation instances to generate positive and negative examples of relation instances.

information extraction: we learn to detect relation instances, making independent predictions for each of them.

from information to knowledge: after inference, the detected relation instances, the input mentions

and the co-reference chains should be used to make predictions at the knowledge level about entity relations.

description of knowledge as information: a subset of relation instances should be selected to express each predicted relation.

however, in the last two steps, we found out that because the co-reference chains produced by our current implementation and the gold co-reference chains used to score the relations are not reliable and/or complete enough, it is difficult in the current settings to setup a clean strategy for these steps. therefore, after multiple experiments, we decided to keep all relation instances except those involving a mention created by the anaphora resolution strategy of the co-reference resolution component: these mentions were replaced by a pre-existing co-referring mention of the same co-reference chain. finding better strategies is left for future work.

we first define the set of possible candidate relation instances p  as the set of relation instances between any pair of input mentions whose entity types respect the signatures of these relations. in the present domain, the relation signatures are the following:

• partof: habitat→habitat

• localization: bacteria→geographical

• localization: bacteria→habitat

the set g⊂p is the set of gold relation instances augmented through transitive closure by the equivalence classes found by the co-reference module.  g is a very small subset of  p, even using the equivalence classes. this leads to a strong unbalance between positive and negative examples and hence to a strong bias toward negative labels, which produces models with a poor recall. to overcome this, we first restrict  p to the subset ps⊂p of relation instances between entity mentions whose distance is smaller than a threshold s, where s is measured in sentences . the rationale is that positive examples are denser when entity mentions are closer to each other. co-reference links amplify this and reduce the number of missed relation instances.

the training corpus is then constituted of all the positive examples from g  and of a set of negative examples of the same size, randomly selected from ps\g. since the set of negative examples is defined as the complementary of positive examples, if coreferences are not taken into account, some positive examples can potentially be chosen as negative ones in the training corpus. this would be the case for instance in figure  <dig>  where c. coli is not explicitly linked to habitats pigs, birds, and surface water: without the co-reference links, the pairs {c. coli, pigs}, {c. coli, birds}, and {c. coli, surface water} would be counted as negative examples. with the co-reference link between campylobacter coli and c. coli, the gold standard localization relations between campylobacter coli and its habitats are propagated to c. coli, and the three above-mentioned pairs become positive examples. in our experiments, we observed that this concerns  <dig> % of examples, initially considered as negative by our baseline system.

the information extraction maximum entropy model is trained with the following features:

• the base features include the words within each entity mention, as a sequence, as well as as a bag of words; the type of each entity; and the distance in sentences between them;

• the pos features include the same features with pos-tags instead of words;

• the cocoa categories, also used for entities identification;

• the n-context features include words and pos-tags from n positions on each side of each entity mention.

the development of the system requires selecting the best set of features and, if needed, context size n, as well as finding best distances s to select examples to build the train and test corpora and to tune the model regularization. our experiments are reported in the next section.

RESULTS
identification of bacteria and biotopes
impact of selected feature sets on mention identification. the best results are highlighted in bold

• our baseline only includes surface features ;

• our baseline plus one set of features among the following four sets:

- the clusters produced with brown's  <cit>  algorithm;

- the ncbi taxonomy;

- the ontobiotope ontology;

- the cocoa annotations.

• optimal configuration: the baseline and all four features sets;

• optimal configuration minus one set of features: we removed either the brown clusters, the ncbi taxonomy, the ontobiotope ontology, or the cocoa annotations.

the evaluation of the mention identification component was performed using the conlleval.pl script   <cit>  created to evaluate results in the conll- <dig> shared task. we achieved a global f-measure ranging from  <dig>  with our baseline features to  <dig>  using the optimal configuration. the feature sets that increase the results are the following ones, in ascending order of the obtained gain: the ncbi taxonomy , the brown clusters , the cocoa annotations  and the ontobiotope ontology .

we noticed that the optimal configuration generally maximizes the scores on all metrics for each category, except for precision which is better on geographical  using the baseline and the brown clusters only and on habitat  using the baseline; nevertheless, in these two cases, recall is very low  compared to that of the optimal configuration.

when analyzing the feature sets that make the system lose points with respect to the optimal configuration, we observed that not using the cocoa annotations allowed us to slightly increase the recall of the habitat category . however, this decreases the corresponding overall results. removing any other feature set produced lower results than the optimal configuration.

co-reference resolution
scoring similarity and anaphora strategies
in the  <dig> documents of the training corpus, based on gold entity mentions, our similarity strategy computed  <dig> co-reference chains involving  <dig> mentions, whereas our anaphora strategy computed  <dig> co-reference chains involving  <dig> mentions . in the  <dig> documents of the development corpus, similarity found  <dig> co-reference chains with  <dig> mentions, and anaphora found  <dig> co-reference chains with  <dig> mentions. we can thus observe that, not presuming their quality, our similarity co-reference resolution strategy builds  <dig> to  <dig> times more co-reference chains than the anaphora strategy.

number of co-reference chains and of entity mentions linked by these chains in the training and development corpora. sim = similarity-based co-reference resolution, ana = anaphora-based co-reference resolution

the question we want to address here is how good is our co-reference resolution component. since this component has been designed to help the global task of relation extraction, this should be examined from an extrinsic point of view though its impact on relation extraction. this evaluation will be performed in the next subsection.

however, the performance of relation extraction depends on a number of factors which themselves interact with the behavior of co-reference resolution. therefore, it would be useful to obtain an evaluation of co-reference resolution from an intrinsic point of view too, so that most of its parameters can be tuned before they interact in complex ways with the relation detection component.

the most obvious way to evaluate the co-reference chains predicted by this component is to match them against those provided with the training and development corpora . examination of these gold co-reference relations revealed though what we take as missing co-reference relations . it seems therefore difficult to obtain a fair intrinsic evaluation of the co-reference resolution module. nevertheless, for want of a better solution, we did perform this intrinsic evaluation by comparing the predicted co-reference chains against the gold co-reference annotations.

comparing co-reference chains is a complex task. among the reasons for this state of affairs are the fact that small changes in co-reference relations can break or merge chains and hence have a strong impact on naïve evaluation measures, the fact the task does not require to normalize the chains to a canonical reference entity which could be matched unambiguously with a gold entity, and the consideration given to "singleton" entities . therefore there is no ideal method to compare co-reference chains, and a variety of scores have been designed and are usually showed together.

we have used version  <dig> of the scorer initially designed for the conll- <dig> co-reference resolution task . it implements what is presented as consensual versions of muc, b-cubed and ceaf . blanc is currently being revised and will be included in a next release.

this scorer compares predicted co-reference chains to gold co-reference chains. it implements four methods , each of which produces recall, precision and f-measure scores. for details on these co-reference evaluation measures, please see the previous links.

we evaluate the two co-reference resolution methods we have implemented: similarity of form  and anaphora , and their union . for reference, we also evaluate the application of the off-the-shelf version of the reconcile system   <cit>  presented in the methods section.

results are computed on the training and development corpora, using the gold mentions as input . table  <dig> shows that the similarity strategy, with a recall in the range   or  , finds nearly all the co-reference relations present in the gold annotations. however, its precision is lower on both corpora: this may correspond partly to erroneous co-reference relations and partly to missing co-reference relations in the gold annotations .

impact of co-reference strategy on co-reference resolution . sim = similarity-based co-reference resolution, ana = anaphora-based co-reference resolution, all = sim ∪ ana. bcube = b <dig>  ceafe = ceaf-entities, ceafm = ceaf-mentions, muc = muc. boldface highlights f-measure of best strategy. for reference, we also show the results obtained with the off-the-shelf version of reconcile 

the anaphora strategy obtains very low precision and recall against the gold annotations. by definition, this means that very few of the co-reference relations it finds are present in the gold annotations. the low precision is largely explained by the fact that a large part of the mentions involved in these co-reference relations  do not obey the constraints imposed on an entity mention in the annotation guidelines, but does not necessarily mean that they are not useful to help relation detection. it is indeed also caused by some inaccurate co-reference detection, independently of the guidelines. finally, the low recall also concurs with the quantitative observations made in the beginning of this section: the anaphora strategy finds a much smaller number of co-references than the similarity strategy.

consequently, the union of similarity-based and anaphora-based co-references  does not improve over the similarity-based strategy alone, and its recall is even slightly lower in many instances.

reconcile system
the bottom pane of table  <dig>  shows the results of reconcile co-reference resolution on the same corpora. recall, precision and f-measure values are much lower than those obtained by our similarity strategy, albeit higher than those of our anaphora strategy. a possible reason why reconcile performs better than our anaphora strategy could be that it does detect some of the easier similar mentions  which are part of the gold co-reference annotations. these are also detected by our similarity strategy, but not by our anaphora strategy which by definition mostly collects co-referring pairs that are not in the gold annotations. reconcile does not however include features geared towards abbreviation processing: although it does detect mentions such as campylobacter jejuni and c. jejuni , it rarely finds them as co-referring.

besides these observations, the lower results of reconcile are largely due to the fact that it performs its own mention recognition. as a consequence, it identifies only part of the bacteria, habitat and geographical mentions: its recall  is  <dig>  on the training corpus and  <dig>  on the development corpus. this is about half the recall of our mention detection component, but co-reference recall cannot be expected to grow linearly with mention recall.

conversely, reconcile also identifies some pronouns and other noun phrases not in the gold standard , which is less of a problem to detect the co-reference relations. it is therefore somewhat more relevant to compare reconcile's co-reference resolution results to those obtained by our co-reference component based on the mentions predicted by our entity detection component instead of the gold mentions: this is the setting in task  <dig> 

the right pane of table  <dig>  shows these results. they are only slightly lower than those we obtain on the gold mentions. this is due to the high recall and precision of our entity detection component on bacteria mentions , which make up the largest part of the co-reference relations. as a result, this does not change the positioning of reconcile co-reference resolution  with respect to our results.

we can summarize and complete these observations about the performance of a generic system such as reconcile as follows:  its mention detection should be adapted to the present task;  its features should be extended to cope with the abbreviation specificities of bacteria mention co-reference;  its features should be extended with domain-specific knowledge about the hierarchical levels of bacteria mentions presented above; for instance, among  <dig> occurrences of this strain detected by reconcile which it included in co-reference relations, most are linked to an identical this strain mention, while only two are linked to a bacteria mention  and can therefore possibly lead to a useful co-reference relation for the relation detection task;  its other anaphora-type resolution capabilities may be relevant to the present task; however, as for our own system, we cannot evaluate them because of the lack of gold annotations for this type of co-reference;  after these adaptations, it should be retrained on an annotated corpus to take into account these new features. this is unfortunately not possible for the same reason.

having run reconcile on the training and development corpora however shows the advantages that would be brought by using an existing high-performing framework. for instance, out of  <dig> occurrences of pronouns it and they in the training corpus, which we decided not to handle, reconcile detects  <dig> anaphoric relations. for example, it finds the correct antecedent for the example mentioned when discussing pronouns in the co-reference resolution section . this points at directions for further work, which would again benefit from anaphora annotation in the corpus.

in comparison though, our co-reference component processed  <dig> occurrences of definite noun phrase anaphoras introduced by this concerning a bacterium in the training corpus, while reconcile handled very few of these.

documents with co-reference annotation
actually, we noticed that some of the texts, mostly those with lower identification numbers, have no gold co-reference annotation at all: assuming that this might be a side-effect of different periods in the annotation process, we also provide an evaluation where only documents with gold annotations are taken into account.

impact of co-reference strategy on co-reference resolution . because no gold annotation has been removed, recall is unchanged with respect to table  <dig>  sim = similarity-based co-reference resolution, ana = anaphora-based co-reference resolution, all = sim ∪ ana. bcube = b <dig>  ceafe = ceaf-entities, ceafm = ceaf-mentions, muc = muc. boldface highlights f-measure of best strategy. for reference, we also show the results obtained with the off-the-shelf version of reconcile 

the incompleteness of gold co-reference annotations from the point of view of an intrinsic co-reference evaluation makes it difficult for us to assess our objective of a precision-oriented co-reference resolution component. this intrinsic evaluation characterizes it on the contrary as a high-recall component with moderate precision, but the co-reference annotations provided with the training and development corpora were not meant initially as a gold standard to evaluate co-reference resolution. the present exercise seems to reach its limits in this respect.

finally, we also tested the impact of two parameters of the similarity strategy, which we summarize shortly. not blocking the single-bacterium mentions incurs a loss of  <dig> to  <dig> f-measure points on both training and development corpora: considering these mentions as systematically co-referent would be like considering that two occurrences of pronoun it are always co-referent. we also set the maximum hierarchical distance to various numbers between  <dig>  and  <dig> . more stringent constraints decreased f-measure by up to one point without gaining precision.

the best results of these parameters on the development corpus were used to determine the configuration to retain to produce the co-reference chains passed to relation detection: this led to discard the hierarchical distance constraint and to block the single-bacterium mentions.

relation detection
we evaluated relation detection using the official scoring program on the development corpus  and on the test corpus . recall, precision and f-measure were computed for each run.

relation detection is the overall task addressed in the present work, and as such it depends on the input it is provided  as well as on its own parameters . the questions we examine here are the determination of its optimal set of parameters, and an extrinsic evaluation of co-reference resolution.

in all the reported experiments, only one parameter was optimized at a time and all other parameters were set to the optimal value for clarity. the optimal set of all parameters was found using an almost full grid-search on the development corpus.

the influence of the maximum distance s allowed between the two entity mentions forming a relation both while building the training corpus and at decoding time is illustrated on figures  <dig> and  <dig>  an increased distance allows the system to build a larger corpus with more variety in the examples, which leads to an increased precision of the system without too much impact on its recall.

on the other hand, at decoding time, plots are reversed. relation instances between more distant entity mentions are sparser, leading to a decreased precision. the best f-measure is achieved at a maximum distance of s =  <dig> where  <dig> % of the relations cannot be predicted even with the co-references. increasing the distance reduces the number of missed relation instances, thereby increasing recall but at the cost of a decreased precision.

the impact of the size n of the context of mentions in which features are collected is shown on figure  <dig>  our experiments did not evidence an improvement of context sizes larger than one token.

impact of co-reference resolution on relation detection. these experiments use the optimal  set of features tested in table  <dig>  hence the best results here are optimal with respect to feature sets too. none = no co-reference resolution; sim = string similarity; ana = anaphora; all = sim+ana. boldface highlights the optimal value on the development corpus, italics the associated value on the test corpus

impact of accumulated feature sets on relation detection. these experiments use the optimal co- reference resolution strategy evidenced in table  <dig>  hence the best results here are optimal with respect to co-reference resolution too. boldface highlights the optimal value on the development corpus, italics the associated value on the test corpus. by construction, highlighted values are the same as in table 6

these observations made on the development corpus are also valid on the test corpus, a good property of our protocol since it denotes that controlled experiments on the development corpus were predictive of system behavior on the test corpus. hence the setting selected because it was optimal on the development corpus  proved to obtain the best f-measure on the test corpus .

the anaphora strategy of our co-reference component performed poorly in this extrinsic evaluation. this confirms its low recall and precision in the intrinsic evaluation presented in tables  <dig> and  <dig> and the much smaller number of co-reference relations that it found compared to the similarity strategy. while precision of relation extraction is improved by co-reference resolution, recall is not. we hypothesize that this comes from the changes that co-reference resolution induces in the generation of training examples.

the system generates positive examples from the relations found in the training corpus. as explained in section relationships identification, co-reference increases the number of positive examples: our best system increases it by a factor of  <dig>  from  <dig>  to  <dig> . these new positive examples were previously considered as negative, and could be selected when building the set of negative examples. this happened to 10%  of the negative examples of the no co-reference setting.

the fact that recall does not increase implies that the new positive examples do not provide knowledge about previously unknown relation patterns. in contrast, the fact that precision increases can be related both to the increase in training set size and to the reduction in the noise caused by incorrect negative samples.

discussion
the tables show that our best relation detection results are higher that the top performing system of bionlp-st  <dig>  <cit>  : without co-reference resolution, it is on par with it on gold mentions, and twice as high on predicted mentions; and it outperforms it by a large margin with co-reference resolution . however, we believe there is still room for much improvement, because of the current following limitations.

we were not able to take much advantage of the context of entity mentions, i.e., information outside the tokens that form the entity mention. this is counter-intuitive and seems to be linked to the difficulty of the system to learn enough useful clues to compensate for the noise added by a more extended context.

we observed that the gold co-reference chains provided by the task organizers are not complete with respect to the needs of relation instance detection as defined above. in contrast to some clinical data sets  <cit> , they never include pronouns  or demonstrative adjectives , which are nonetheless instrumental to linking locations to bacteria in a number of instances: see for instance this species in figure  <dig>  which links the bacterium of the previous sentence to its localizations in the current sentence.

generic noun phrase anaphoras such as isolated strains in the genus  represents a group of . . . isolated strains have been obtained from a variety of  . . . are not annotated either, although they provide the link to a localization relation spirochaeta−→freshwaters which is annotated in the gold standard.

co-reference sometimes also percolates through sortal relations in constructions such as the genus  consists of some  <dig> , most of which are  saprophytes, where bacterial species is not strictly speaking co-referring with burkholderia , but still supports the inference that burkholderia is found in habitat soil. this points at the need for more elaborate inference paths to identify localization relations.

a recurring example is that of diseases  <cit>  which we also identified in the training corpus: bacteria often cause diseases in hosts , hence the path bacteria−→disease + disease−→host, which relies on the detection of two relation legs, offers strong support to the establishment of a localization relation bacteria−→host.

future work includes short-term easy steps and longer-term steps. we used word clusters as additional features for mention detection, it will be straightforward to test them in relation detection: they might bring useful generalizations for context features. manual collection of features  <cit>  is a complementary option here.

longer-term investigations are needed to discuss and design a more complete gold annotation of co-reference chains. we consider this is a precondition to design effective strategies for going up from information extraction to the knowledge level and to represent relations by suitable relation instances. this will also make it possible to train machine-learning co-reference methods, including building upon existing frameworks such as reconcile  <cit> .

CONCLUSIONS
the methods tested here and the reported experiments confirm that accurate co-reference resolution is key to accurate relation detection at the knowledge level in the present type of corpus. co-reference chains first play a role in the generation of positive and negative examples of relation instances, hence on the training phase of supervised relation detection. additional experiments, not detailed here for reasons of space, show that if co-reference chains are not used when training, the gain of using them only at the inference stage is only  <dig>  f-measure point, which is  <dig> times less than the  <dig>  to  <dig> f-measure points gained when using co-reference chains at both stages. second, they play a role in the management of the detected relation instances. third, they are used by the evaluation program to assess the accuracy of the proposed relation instances as valid representatives of relations between entities.

we designed and included in our relation detection system a co-reference resolution component which is close in spirit to that of  <cit> . this component contributes from  <dig>   to  <dig>  f-measure points  when run on mentions predicted by our mention detection component, and  <dig> f-measure points  with gold mentions. its predicted co-reference chains are used in the first and second steps, but the gold co-reference chains used in the evaluation step are those prepared by the task organizers. this makes it important for this step that predicted co-reference chains be close to gold co-reference chains, but also that gold co-reference chains be accurate and complete. however, we have discussed some limitations in the present gold co-reference chains which we believe need to be overcome for more progress to be made on relation extraction on this type of documents.

the methods presented here and tested on the official bionlp-st  <dig> test corpora achieve a  <dig>  f-measure taking as input the gold entity mentions, and a  <dig>  f-measure when taking as input entity mentions predicted by our crf system. this outperforms the best results obtained on this corpus in the challenge  and thus provides new baselines for the task. we believe a better treatment of co-reference, obtained through a more complete gold annotation of anaphora and other co-referring expressions, should make it possible to identify more relevant features for the type of co-reference-mediated relation detection encountered in the present type of corpus.

competing interests
the authors declare that they have no competing interests.

authors' contributions
all authors designed the experiments, wrote, read and approved the paper. cg prepared the mention detection module and a first version of the relation detection module, pz prepared the co-reference module, and tl prepared the final relation detection module.

