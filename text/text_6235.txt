BACKGROUND
with the completion of the hapmap project  <cit> , large-scale, high-density single-nucleotide polymorphism  markers and information on haplotype structure and frequencies become available. a variety of statistical approaches have been proposed for association studies using haplotypes  <cit>  and more are expected for whole genome association studies. the utilities of such approaches are frequently very difficult to obtain through analytical analysis. evaluations on those methods commonly rely on experiments based on simulations or empirical data. for example, one can generate a large number of samples based on population models such as the coalescent theory, and the program ms  <cit>  is routinely used in the community. thus statistical properties of new approaches can be investigated in a controlled manner as functions of population parameters such as recombination rates, mutation rates, population structure and migration rates. but simulations based on population genetics models may not be able to capture the true property of ld in human populations, due to their simplified assumptions. conclusions based on such simulated data may be misleading or inaccurate in reality. some researchers  <cit>  have used their in-house tools to generate empirical data based on real data, in hopes that their empirical data could inherit major properties of human populations such as ld patterns. but each group may use its own models and tools, and comparisons on results from different groups are usually impossible in general. a public available program that can generate a large number of independent samples based on real human data can be of great use for evaluating new proposed approaches. we have implemented a program in c++, called gs, that can efficiently generate such samples based on real data. two heuristic approaches have been implemented. one is based on phased haplotype pairs and the other is based on haplotype block structures. the program can also directly take data from the hapmap project as its inputs. our experiments show that genotype data generated by both approaches observe similar local haplotype structures and ld patterns as those in the input data, and also keep a proper level of variety. therefore, genotype data can be used in testing algorithms for tag snp selection and haplotype inference.

simulation data is always indispensable for evaluation purposes. the major goal of this tool is to allow users to generate large scale simulation data for association studies, including both fine mapping and genome wide association. both quantitative and qualitative traits have been incorporated. to generate disease phenotypes, users can either specify a one-locus disease model or a two-locus disease model with or without epistasis. many studies and growing evidences have revealed the importance of epistasis in the etiology of complex traits. more and more efforts have been made to detect epistatic interactions  <cit> . for example, marchini et al. studied the feasibility and power of a full two-locus model in the context of genome wide association studies and compared its performance with a single locus based approach and a two-stage design  <cit> . evans et al. compared the performance of four different search strategies  to detect interacted loci using two-locus models  <cit> . luliana et al. compared two natural two-stage approaches: the conditional approach and the simultaneous approach  <cit> . but for all the above studies, simulations were based on genotypes only at the trait loci. although it is hardly possible to simulate thousands of individuals for hundreds of thousands snp markers for thousands times for genome wide association studies due to storage and time constraints, our program can embed the disease genotypes into genome regions that mimic genomic content in human populations. associations with markers can be tested in a more realistic scenario.

implementation
the program has implemented two methods to generate haplotypes/genotypes, i.e., the extension method and the block method. both methods can be used to generate qualitative and quantitative phenotype data. we first introduce the two methods in the context of generating case-control data, and then briefly discuss how quantitative traits can be generated. the extension to two-locus models will then be discussed.

extension method
the first model is an extension to the one used in  <cit>  that takes phased haplotype pairs as its inputs. for example, one can use the haplotype results from the hapmap project as inputs, which can be downloaded from the hapmap website. users first create a disease model by specifying the disease allele frequency  and the penetrance of each genotype. alternatively, users can define a disease model using the population prevalence and genotype relative risks. a simple relationship exists between penetrance parameters and genotype relative risk parameters  <cit> . therefore, in the following we discuss our procedure only using one set of parameters, the penetrance. the program first picks a snp t from the input data, where one of its two alleles has the frequency approximately equal to the daf specified in the parameter file. this allele is regarded as the high-risk variant.  to generate the genotype at the disease locus for a case, it first calculates the conditional probability of each genotype  given the individual being affected based on equation  <dig> 

  pr=prpr∑jprpr 

the actual genotype g will then be selected based on the conditional distribution. the genotype frequencies in the above formula can be obtained from allele frequencies under the assumption of hardy-weinberg equilibrium. the probability of being affected given a particular genotype  is given by users as a parameter. to generate the haplotype pairs h <dig> and h <dig> for this affected individual, the program randomly selects two haplotypes h <dig> and h <dig> from the inputs with the genotype at the disease locus t as required  = g, where hi  denotes the allele at the tth locus on haplotype hi). in their original paper  <cit> , haplotype h <dig> will be given the same alleles as h <dig> from locus t - l to t + l, where l is a parameter that can be specified by users. to extend h <dig> to the right for one more locus, it randomly selects another haplotype h <dig> that has the same alleles as h <dig> from locus t - l +  <dig> to locus t + l, and let h <dig>  = h <dig> . by iterating the above process, one can extend h <dig> to the right and then to the left. we found that ld patterns from samples generated this way greatly depend on the parameter l . even for one particular data set, the extend of ld may vary substantially in different segments. a single l can not accommodate all the cases. we have extended the above method by introducing two parameters, lmin and lmax. the overlapped length for both the initial assignment and the extension of h <dig> will be stochastically determined by two values ll and lr , one for each direction. the values of ll and lr depend upon the strength of local ld. more specifically, lr is initialized as lmin. the value of lr is increased by  <dig> if the ld measure d' between locus t + lr and locus t + lr +  <dig> is greater than a uniformly distributed random number between  <dig> and  <dig>  the process will terminate when the value of d' is smaller than such a random number or when lr = lmax. the value of ll can be determined similarly to the left. the haplotype h <dig> will be given the same alleles as h <dig> from locus t - ll to t + lr initially. at each extension step, the procedure is the same as in the original paper  <cit> , but with differences in determining the length of the overlapped region. for example, to extend to the right for one more locus, suppose the current locus  is t <dig>  the leftmost locus t <dig> of the overlapped region is stochastically determined based on pairwise ld with the constraint that lmin ≤ t <dig> - t <dig> ≤ lmax. a haplotype that shares the same segment with h <dig> from t <dig> to t <dig> will be randomly selected and its allele at t <dig> +  <dig> will be copied to h <dig>  a detailed description of the above procedure can be found in the manual of the program. the haplotype h <dig> can be obtained similarly. the required number of cases can be generated by repeating the process. one can generate normal individuals using the same approach based on the genotype distribution conditional on the fact that the individuals are normal. by using two parameters, the method takes both long-range ld  and short-range ld into considerations.

block method
the second generating model is actually a markov model based on haplotype block structures inferred from real data such as hapmap data. ld patterns such as a block-like structure have been commonly observed from experimental data for dense snps. instead of directly using haplotype pairs, one can also take the haplotype block structures as inputs. as a markov chain, each block is a state that consists of several common haplotypes controlled by an emission distribution. the connections of haplotypes between adjacent blocks are specified by a transition probability matrix. more specifically, for each block, the input data consist of the number of markers, the common haplotypes with their population frequencies, and the probabilities of each common haplotype connecting the common haplotypes in the next block. an example is given in fig.  <dig>  such a structure can be inferred based on real data using some software such as haploview  <cit> . to generate samples, users first specify a disease model  and the program selects a locus with its allele frequency approximately equal to the daf. users can also specify a disease locus directly. the genotype of a case  at the disease locus is generated in the same way as the extension method does. for each allele at the disease locus, a common haplotype with the allele embedded will be selected according to their frequency distribution. the two haplotypes will then be extended independently to both directions based on the transition probabilities. a large number of samples can be generated that will share similar ld patterns with real data but with different haplotypes and genotypes. to maintain a proper level of variety, we have considered snps that are not in any blocks, as well as possible rare haplotypes that do not exist from the input block file. for many block definitions, not all snps have to be within some blocks. to incorporate those missed snps, the original genotype data file that is used in generating the block structure has to be provided to the program. when generating a haplotype of a sample, the program imputes the missed snps sequentially based on their physical positions. for each position, an allele is chosen based on the allele in the previous position, and their frequencies and the pairwise ld between them estimated from the genotype data. furthermore, rare haplotypes are usually been dropped in the markov model. only major haplotypes and their frequencies are available for each block. to incorporate rare haplotypes, we stochastically generates  one "rare" haplotype each time when a block is selected. the alleles of the haplotype are sampled solely based on the allele frequencies and pairwise ld. the frequency of the rare haplotype is defined so that the summation of haplotype frequencies within each block will be one . the transition probabilities from the rare haplotype to haplotypes in the next block are proportional to the haplotype frequencies in the next block. when a block is selected again in generating another sample, a new rare haplotype will be generated and its frequency will be determined in a similar fashion. in such a way, every possible haplotype within a block will have a chance being selected as a rare haplotype for some samples. these new haplotypes will be rare overall in the samples because each time a different one might be selected. the procedure is designed so that it is slightly biased to haplotypes with common alleles.

quantitative traits
to generate phenotypes for a quantitative trait, a quantitative trait nucleotide is chosen according to a specific allele frequency or a specific marker position provided by users. the phenotypic value of each individual is generated according to the classical single-locus quantitative trait model  <cit> . more specifically, users can specify the additive  and dominance  genetic variances attributable to the quantitative trait nucleotide as proportions of the total phenotypic variance. denote the proportions as πa and πd. let vo denote the variance due to all other  factors and assume its value is  <dig>  then va and vd can be calculated based on

 va+vdva+vd+1=πa+πd,andvavd=πaπd. 

if one assumes that the phenotypic value of an individual can be partitioned as

  y=z+ui={zhomozygous wildz+aheterozygousz+2ahomozygous mutant, 

where z follows the standard normal distribution, a is half of the difference between two homozygous genotypic values , and k is a parameter representing the dominance effect, it is known  <cit>  that va and vd can be written as

  va = 2pa2) <dig>  and vd = ak) <dig>  

where p is the frequency of the mutant allele. thus a and k can be obtained based on the above equations. the phenotypic value of an individual can be calculated by substituting a and k into equation  <dig> 

two-locus models
for a one-locus diallelic disease model, users can specify at most three penetrances, one for each genotype. the number of parameters reduces to two if one considers some commonly used models such as dominant models, recessive models, or additive models. for a two-locus diallelic disease model, two interacting sites are involved, and in theory, one can specify nine penetrances, one for each genotype combination of the two sites. there are also many restricted models with less than  <dig> free parameters that are of great interest to the community. by focusing on fully penetrant models , li et al.  <cit>  enumerated all the  <dig> possible combinations and summarized  <dig> unique ones. we believe that it is necessary to allow incomplete penetrances  in simulating complex diseases. but with incomplete penetrances, the possible number of models becomes infinite. to incorporate both incomplete penetrances and some commonly used restrict models, the gs program provides two distinct methods to allow users to specify a two-locus model. for the first method, all the nine parameters are free and users have the freedom to assign each penetrance with any probability value. thus the program can generate datasets with any desired disease models. however, in many cases, it may not be intuitive to assign values for the nine penetrances directly. instead, users might have some information about the population prevalence of a particular disease, and information on marginal effects  of individual locus. and they might want to test the power of their method under some particular two-locus interaction models. to meet such needs, the gs program has implemented nine commonly used models in the literature. for each of these nine models, users only need to specify the population prevalence p, and genotype odds ratio  <dig> + θ for each locus. the program can automatically calculate the penetrance table. for example, table  <dig> represents a jointly dominant-dominant model, where at least one disease allele is required at both loci to increase disease odds and both loci have the same effect size. each cell of the table represents the odds of the disease for an individual with the corresponding genotype combination. let pr denote the probability of an individual being affected given its genotype combination of gi , and let pr denote the probability of an individual not being affected given its genotype gi. based on the definition of the odds of a disease

  oddgi=prpr=pr1−pr, 

the penetrance of gi can be calculated using the following formula,

  pr=oddgi1+oddgi. 

a corresponding penetrance table is give in table  <dig>  once the population prevalence p and the genotype odds ratio  are fixed in this model, the baseline value α, which indicates the odds of disease when the two loci do not carry any disease alleles, can be calculated by plugging the terms in table  <dig> into the following formula,

  p=pr=∑iprpr. 

the frequencies on genotype combinations ) can be obtained from allele frequencies under the hardy-weinberg equilibrium assumption. the details of all the nine built-in models can be found in the manual of the program. once the three by three penetrance table is ready, the gs program can calculate the conditional probability of each genotype combination given affected status using a similar formula as equation  <dig>  actual genotypes at the two loci will be selected based on the conditional distribution. the haplotypes will then be selected independently for these two loci, using either the extension method or the block method. the above two-locus model does not explicitly consider linkage disequilibrium between them . we will consider models with two disease loci that are in ld, as well as haplotype-based disease models in a subsequent version.

formats
the inputs to the program can be obtained from hapmap project or from users' own research projects. phased haplotype pairs from hapmap website can be directly incorporated into the software using the extension method. to use the block method, one can generate block structures from phased haplotype pairs using the program haploview  <cit> . both approaches allow us to generate large data sets that mimic the true local ld from human populations. a variety of parameters can be specified by users, including some that control the output formats. there are basically three different output formats including a widely used format in the community, linkage format. users can choose to output phased haplotype data or unphased genotype data. the disease causing snp can be kept or removed in the final outputs. more details about file formats can be found in the manual of the program. the program does not directly model population structures. but one can create a data set that is a mixture of different populations with different allele frequencies and effects by combining samples generated based on inputs from different populations. to generate random numbers, the mersenne twister algorithm  <cit>  has been adopted.

RESULTS
datasets
we have tested both generating methods using two different data sets. the first dataset consists of all  <dig> encode regions from the hapmap project, which is a portion of the  <dig> regions from the encode project. these encode regions were selected either manually, or randomly based on gene density and level of non-exonic conservation. details about selection criteria can be found at encode website  <cit> . the  <dig> hapmap-encode regions were resequenced in  <dig> unrelated individuals and genotype data were obtained from about  <dig> thousand snps of all  <dig> hapmap samples. the haplotype pairs of each individual can be downloaded directly from the hapmap phase i data release. for each region, we take the parental haplotypes from  <dig> trios in a population with northern and western european ancestry. we mainly present our results using one region  in the paper and results on other regions can be found on our website as supplementary materials. the second dataset is from a genome wide association study of a neurological disease  <cit> . this dataset is one of the first sets of publicly available genome wide snp data, which can be downloaded from coriell institute for medical research  <cit> . in its first stage, the study genotyped 550k snps across genome using the illumina infinium ii snp chip, for  <dig> patients with sporadic amyotrophic lateral sclerosis  disease and  <dig> normal individuals. we have to preprocess the data before running our program, by either inferring haplotypes or predicting haplotype block structures. we chose the most recent version of a widely used program fastphase  <cit>  for haplotype inference. haploview was selected for haplotype block structure prediction.

because the phenotypic models are standard, we mainly assess the performance of our program in terms of the similarity of haplotype structures between simulated data and input data. the haplotype structure is measured mainly by two quantities, i.e., the number of haplotype blocks and the average number of block length within each region, as well as visual examination of local ld patterns. for each region, its haplotype structure is first inferred using gabriel's method  <cit>  implemented in haploview based on its default values, and the number of haplotype blocks and the average number of block length is obtained. for each method and for each fixed set of parameters,  <dig> replicates will be generated and these simulated data will be reloaded to haploview to obtain their haplotype structures. the average number of blocks and average length of blocks of the  <dig> replicates will be compared to values obtained from the input data.

parameters
for the extension method, there are two important parameters lmin and lmax, which are used to adjust the segment length in the extension method based on local ld values. to evaluate the effect of lmin and lmax on haplotype structures, we take one region from the als dataset and one encode region , and perform extensive tests using different combinations of lmin and lmax. the als region  was randomly selected from chromosome one with  <dig> snps. the enr <dig> region  has  <dig> snps. the major difference between these two regions is snp density. because enr <dig> consist of a much dense snp set, it has a much larger average block length  comparing with the als data . we tested a wide variety of combinations of these two parameters on the als data  and the enr <dig> region . results  show that the performance of the program is quite robust and consistent. the program shows suboptimal results only when lmin takes extremely small values . this result demonstrates that the program gains the flexibility to capture local ld by using these two parameters. the optimal choice of the two parameters mainly depend on marker densities. but for each density, a variety of values will work almost equally well. for our experiments, we use  =  for the als data  and  =  for encode regions .

allele frequencies, ld and block structure
we first compare allele frequencies of simulated data and original input data for both approaches on  <dig> regions . results show that allele frequencies from simulated data are very close to those from original data for both methods . figure  <dig> shows the results of the two methods on the als region and the enr <dig> region. snps in the input data are arranged along the x axis in a decreasing order according to their minor allele frequencies. the original frequency is represented by a blue line and the average frequency from  <dig> replicates in simulated data is represented by a red line. only small variations can be observed for both datasets and for both approaches. overall, the block method seems to have higher variation than the extension method. in terms of haplotype structures, data generated from both methods have shown a "blocky" structure . local ld values within blocks are very similar to those in the original human data. simulated data has also shown some varieties as expected. the results from all  <dig> regions from  <dig> replicates for both methods are summarized in table  <dig>  the similarity of simulated data and input data is evaluated in terms of block parameters such as the number of blocks, the average number of markers in each block, the percentage of overlapped markers within blocks. in general, the extension method shows a consistent performance across the regions tested in all measures. the block method performs well on the als data, and the percentages of overlapped snps in original data and simulated data are high. however, the block method generates many more blocks with smaller sizes in simulated data. we suspect that the performance of the block method greatly depends on snp density, and investigate the dependence using additional experiments. we take the same physical region enr <dig> from chromosome 2p <dig> , but construct three different densities: hapmap phase i data, phase i with redundant snps removed , als data. the number of blocks and the average length of blocks in the original data reduced from  <dig> to  <dig>  and from  <dig> to  <dig>  respectively, when the number of snps reduced from  <dig> to  <dig>  those numbers in simulated data also reduced from  <dig> to  <dig>  and from  <dig> to  <dig>  respectively . the block method tends to generate more blocks with small sizes when the input data consist of extremely high dense snps because small variations will break long blocks  into smaller ones. in addition, for blocks with large number of snps, the summation of all common haplotype frequencies is usually far less than one, which leads to a higher chance of introducing artificial rare haplotypes and breaks long blocks into short ones. we further investigate how the incorporation of pairwise ld when imputing rare haplotypes and snps that are not within any blocks affects block structures. the last row in table  <dig> presents the result of the block method without using pairwise ld. comparing with row  <dig>  not considering pairwise ld when imputing missing will result in smaller number of longer blocks. further examinations reveal that small blocks  can be formed in the imputed regions when using pairwise ld.

efficiency
the gs program is efficient and runs fast. it can generate  <dig> replicates of  <dig> individuals with around  <dig> markers within minutes on a desktop with a  <dig>  ghz processor and  <dig> gb memory. we have also tested the program in the context of genome wide association studies using the als data, which is based on illumina 550k snp chips  <cit> . for the extension method, the gs program needs haplotypes as its inputs. the program fastphase  <cit>  was used to obtain haplotype information from genotypes. we randomly selected one chromosome  with  <dig>  snps, and it took fastphase about  <dig> hours to obtain the haplotype pairs of  <dig>  samples from the als dataset. this limits us to perform the tests using only one chromosome for the extension method in this study. on average , it took about  <dig> minutes in generating  <dig> cases and  <dig> controls from the haplotype pairs of the  <dig> samples for the 36k snps. because our algorithm is linear in the number of snps, the estimated time to generate 550k snps in a sequential manner is roughly  <dig> hours. if a cluster with multiple processors were used, the computation for different chromosomes can be run independently, and the total time is bounded by the running time of the chromosome with the largest number of snps. for the block method, gs needs haplotype block structures as inputs. however, haploview cannot handle datasets with such sizes at a chromosome level for structure prediction. we could not test the block method at the genome level. based on our experiences on small datasets, the block method is slight faster than the extension method. therefore, for simulations at a genome level, the block method may require less or similar time as the extension method does.

discussion
we develop a program to generate genotypes/haplotypes by perturbing real data based on two approaches. qualitative or quantitative phenotypes can be generated based on genetic models. the goal of generating simulated data by perturbation is to create a large number of replicates that share similar properties with real data. for the extension method, the randomness is mainly from the sampling procedure at each step when an extension occurs. experiments show it is robust across a wide range of parameter values and snp densities. for the block method, noise can be introduced when imputing rare haplotypes or imputing snps not within original blocks, in addition to deviations due to random sampling. test results show it is more suitable for data with small blocks . one should also notice one practical limitation while simulating data using perturbations. when simulated data only inherit properties from the set of input samples, it may never be able to represent the whole population if the inputs are biased. for example, the hapmap project only consists of a small number of samples in each of its ethnic groups. rare snps may not be typed and rare haplotypes may not be observed. data generated based on hapmap samples cannot reveal the true distribution of rare snps. this problem will be alleviated with the availability of more real data in the future.

the standard error of the average block length of each encode region is usually great , which reflects the fact that block lengthes vary dramatically. therefore, the number of blocks and the average length of blocks can not give a whole picture of the block structure. visual examination reveals that local ld structures from simulated data generated by both methods show high concordance with those from original data. however, none of the two methods can have a good control on long range ld. another limitation of the program is that it requires inputs to be either haplotypes or haplotype structures. both of them have to be inferred from genotype data, and the inference usually takes much longer time than the simulation itself. we will investigate new perturbation approaches directly based on genotype data. the method in generating genotypes at the disease loci assumes that the alleles are in hardy-weinberg equilibrium  in the population where samples are drawn. it is known that a population can deviate from the hwe due to many reasons  <cit> . for example, if there are differences in the survival rates of individuals with different genotypes, deviations from hwe might occur. some other reasons that cause deviation from the hwe include non-random mating, preferential selection of samples, etc. the samples generated by our program are not suitable to study diseases that might deviate from hwe for those reasons.

CONCLUSIONS
we have developed a software tool  that can efficiently generate a large number of samples with genomic and phenotypic variations based on hapmap data or any real data. experiments show that the two approaches can produce data that share similar local ld patterns as those in the input data. both single-locus and two-locus disease models have been incorporated in the implementation. the data generated by the program can be used for a variety of purposes, including the evaluation of algorithms for haplotype inference, tag snp selection and association studies. it can be used to evaluate algorithms for gene fine mapping as well as algorithms for genome wide association studies.

availability and requirements
project name: generating samples based on hapmap data

project home page: 

operating system: windows and linux/unix.

language: c++.

any restrictions to use by non-academics: none.

list of abbreviations
snp – single-nucleotide polymorphism;

ld – linkage disequilibrium;

daf – disease allele frequency;

authors' contributions
jl initiated and designed the research, and wrote the manuscript. yc implemented the program and performed the tests. both authors have read and approved the final version of the manuscript.

