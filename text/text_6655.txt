BACKGROUND
with the advent of genomic sequence data, phylogenetics is progressively switching to large-scale analyses, using many genes in parallel  <cit> . among the diverse methods that have been proposed for dealing with multigene data sets is the so-called supermatrix method  <cit> . this method consists in concatenating the sequences of all available genes into one single "supergene", which is then subjected to standard phylogenetic reconstruction methods. an obvious advantage of relying on large sequences is the expected increase of the statistical support; as long as all or most of the genes included in the analysis have an evolutionary history congruent with that of their host species , the small amounts of phylogenetic signal contained in each gene should in principle add up and overwhelm stochastic errors, thus leading to a well-supported phylogenetic tree.

however, high statistical support does not necessarily imply that the trees obtained are correct. in some cases, in particular under poor taxon sampling  <cit> , highly resolved trees have been proposed, which have nevertheless been followed by subsequent critical re-analyses, claiming that, however strongly supported, the trees obtained were probably wrong  <cit> . more generally, there are several cases where standard phylogenetic reconstruction methods yield wrong but statistically well-supported trees. these so-called systematic  errors have been known about for a long time in the field  <cit> , and are expected to be also present, in fact even enhanced, in a phylogenomic context  <cit> .

a first explanation of systematic errors in phylogenetics is that they are caused by the mutational saturation of the sequences: if some positions have undergone multiple substitutions, this will blur the phylogenetic signal, and thereby increase the probability for several species to display convergent sequence patterns  at those positions. many reconstruction methods are not able to correctly identify these convergences, and will instead interpret them as shared derived characters. as a consequence, they will be misled towards reconstructing a wrong tree. a typical instance of this phenomenon, called the long branch attraction  artefact  <cit> , occurs when two phylogenetically distant species, evolving significanlty more rapidly than the rest of the taxa , deceivingly appear as closely related in the estimated tree. similarly, when a distant outgroup is used, a divergent species may be attracted by the long branch separating the in- and the outgroup, and thus be artefactually put at a basal position  <cit> .

according to this explanation, removing the most saturated sites should improve the accuracy of the reconstruction. in this direction, several methods have been proposed, for selecting less diverged sequences  <cit> , or filtering out saturated sites  <cit> . in most cases, these methods seem to bring a significant improvement. in addition, they are particularly advantageous in a phylogenomic context, where the amount of data is not limiting: fairly stringent filtering criteria can be applied, removing a large amount of data, but still leave behind a more than sufficient amount of phylogenetic signal to obtain well resolved trees  <cit> .

an alternative way to deal with lba artefacts is to avoid long branches altogether  <cit> . for instance, one can simply eliminate the fast-evolving taxa, and replace them by slow-evolving close relatives. this method was applied to the animal phylogeny, using 18s ribosomal rna  <cit> , and led to a reappraisal of the position of nematodes. specifically, whereas fast-evolving nematodes, such as caenorhabditis, would appear at the base of the group of bona fide coelomate bilateria, a more slowly evolving one, trichinella, appeared within arthropods. a related method consists in 'breaking' a long branch, by adding a series of intermediate taxa thought to emerge along this branch  <cit> .

altogether, a combination of a better taxon sampling and a more careful selection of sites or sequences makes it possible to converge to reliable phylogenies. and indeed, at most evolutionary scales , a consensus seems to be emerging regarding most evolutionary relationships in all these groups. on the other hand, such careful methods require significant expertise in phylogenetics. more fundamentally, they are exclusively focussed on the quality of the data, leaving open the problem of understanding why current reconstruction methods are so prone to systematic artefacts.

after all, lba artefacts were initially described as an explanation of the statistical inconsistency of maximum parsimony   <cit> , in contrast to probabilistic methods, which are consistent in a broad range of conditions  <cit> . as such, lbas were expected to disappear once more reliable methods such as the maximum likelihood or the bayesian frameworks became routinely used. yet, artefacts are also observed under these methods, especially under poor taxon sampling. in a statistical perspective, the explanation of this apparent paradox is straightforward: the consistency property assumes that the underlying model is correct. hence, such systematic errors simply betray that current models are mis-specified  <cit> .

note that explaining systematic artefacts as a model violation problem, as we do now, rather than one of mutational saturation, or of taxon sampling, as we did above, are not mutually exclusive arguments. when the data are not or are weakly saturated, current models , all lead to the correct topology. it is only when the data are more strongly saturated, and the taxon sampling is not sufficiently rich to reveal the true extent of saturation, that a good model becomes necessary: in such situations, only the model can correctly estimate the frequency of multiple substitutions across the alignment, thereby avoiding systematic errors. thus, what phylogenetic artefacts betray is fundamentally a lack of robustness of current models. more specifically, it points to the inherent propensity of these models to under-estimate the true level of saturation.

many directions have already been explored to improve phylogenetic models, by accounting for compositional biases  <cit> , across site heterogeneities of the rates  <cit> , or of the substitution processes  <cit> , or by acknowledging the variation of site-specific rates with time  <cit> , non-independence between sites  <cit> , etc. some of these models have indeed resulted in improved phylogenetic inference  <cit> . in the present work, we will focus on site-heterogeneities of the amino-acid replacement processes, which may have a particularly strong impact on the way the model evaluates sequence saturation. a striking feature that one readily observes when working with protein alignments is the biochemical specificity observed at each site: in spite of the fact that there are  <dig> amino acids, only  <dig> to  <dig> distinct residues are typically observed at a given variable column, suggesting that most positions undergo repeated substitutions among a very restricted subset of the amino-acid alphabet  <cit> . obviously, this pattern has a direct bearing on the expected level of homoplasy, as convergent evolution towards the same amino-acid will be all the more frequent as few amino-acids are allowed at a given site. it is therefore crucial to correctly account for this fact in models of protein evolution that are to be used for phylogenetic reconstruction.

in this direction, we have previously developed a mixture model that accounts for across site heterogeneities in the evolutionary processes  <cit> . thanks to a dirichlet process device, implemented in a bayesian monte carlo framework, this model, cat, effectively clusters the columns of the alignment into biochemically specific categories, each of which is described by its own amino-acid profile of equilibrium frequencies. by bayes factor evaluation, we have shown previously that cat generally has a better fit than homogeneous models based on one single empirical substitution matrix, such as wag  <cit> , jtt  <cit> , or even the most general site-homogeneous and time-reversible model   <cit> . we now apply the cat model to the bilaterian phylogenomic dataset of philippe et al  <cit> . this dataset displays an interesting example of systematic artefact when analyzed with current models of evolution, depending on the outgroup, two highly supported, yet contradictory, phylogenetic positions are obtained for two fast-evolving phyla, nematodes and platyhelminths. this artefact offers an experimental protocol for testing alternative models of evolution. specifically, a good model should not lead to contradictory results depending on the chosen outgroup. in the present work, we use this case study to compare the performance of cat to that of a site-homogeneous model based on the wag matrix  <cit> .

RESULTS
robustness of cat against lba
we analyzed the phylogenetic position of nematodes or of platyhelminths as a function of both the outgroup and the evolutionary model. the dataset of philippe et al.  <cit>  was randomly cut into two halves, meta <dig> and meta <dig>  which were analyzed in parallel. apart from their specific amino-acid replacement processes, the two models under investigation, cat and wag, share the same features, including gamma-distributed rates across sites  <cit>  .

as observed previously  <cit> , under wag, the position of nematodes is strongly dependent on the outgroup : when a distant one  is used, nematodes are found at the base of the coelomates , whereas they are sister-group of arthropods , when two choanoflagellates and a cnidarian are added to the outgroup. these mutually contradictory positions for the nematodes are both supported with posterior probability  <dig>  which makes the overall pattern a clear case of systematic artefact. as previously suggested  <cit> , the coelomata positioning, although supported by several recent large-scale phylogenomic analyses  <cit> , might be an artefact due to an attraction of the fast-evolving group of nematodes by the long branch separating the ingroup and the outgroup, leading to an apparent basal emergence of nematodes. according to this interpretation, adding intermediate taxa to the outgroup has the effect of breaking this long branch, resulting in the presumably correct positioning of nematodes, within protostomes. a similar behavior is observed for platyhelminths : they are basal if the tree is rooted with fungi, whereas they are sister-group of arthropods, forming the clade of protostomes, in the presence of choanoflagellates and the hydra.

in contrast, under the cat model , both nematodes and platyhelminths keep their sister-group relationship with arthropods in all cases, even with the most distant outgroup. therefore, in these two cases, cat does not lead to mutually contradictory conclusions such as those proposed by wag. furthermore, it yields the topology that we expect if the interpretation in terms of lba is correct. all these results are independently recovered for both halves of the alignment, except for the monophyly of deuterostomes, which was recovered with a low posterior probability  in one case . taken together, they suggest that cat is more robust to lba than wag.

model comparison by cross-validation
as a measure of model fitness, we evaluated the predictive power of cat and wag by cross-validation between the two datasets meta <dig> and meta <dig>  for each model, the parameters  were learnt on one of the two datasets, and used to compute the likelihood of the other dataset. as is usual in a bayesian monte carlo framework, the likelihood of the testing set is averaged over a sample obtained by mcmc from the posterior distribution under the learning set.

in the case of nematodes , cat gave a higher cross-validated log-likelihood than wag, indicating that it offers a better description of real data. similar results were obtained when platyhelminths were used instead of nematodes . note that, to compute the cross-validated likelihood under cat, we have made an approximation , but one which leads to an underevaluation of the cross-validation score of cat. since in all cases, cat turns out to have the best score, this approximation does not invalidate our conclusions.

these results confirm our previous observations based on bayes factor evaluations  <cit> , showing that, in most instances, cat offers a better statistical fit than wag. note that we previously found a few cases where cat was not the best model  <cit> , but this may be due to the small size of the single-gene datasets used in that former study, whereas in the present case, the very large number of columns makes it probably easier for a parameter-rich model such as cat to achieve good performances.

a posterior predictive saturation analysis
part of the better fit of cat may come from its higher ability to correctly detect sequence saturation, which would explain its greater robustness to lba artefacts. this interpretation raises the issue of whether cat offers a sufficient account of saturation. in this respect, comparative evaluations such as the cross-validatory comparison performed above, do not offer in themselves any measure of the absolute goodness-of-fit of the models under scrutiny, a problem usually referred to as model assessment  <cit> .

the rationale for assessing a model is usually based on the following argument: when a model is adequate, that is, when it correctly describes the true evolutionary process, then the true data should be indistinguishable from data simulated under this model. we can thus check the adequacy of a model by actually performing simulations, and comparing the value of a pre-defined summary statistic evaluated on the true data , with the distribution obtained for this statistic under the simulated replicates . a significant deviation between the observed value and the null distribution will indicate a model-misspecification problem.

the outcome of the goodness-of-fit test depends on the chosen statistic. usually, one chooses a statistic that is meant to be particularly sensitive to those patterns in the data that we are interested in, or that are thought to play a fundamental role in the estimation procedure. in the present case, we chose two statistics that directly measure what we take to be the main cause of systematic artefacts, namely, sequence saturation. specifically, we measure the number of substitutions , and the level of homoplasy , defined as the mean number of homoplasies  per site . we only considered the case of the nematodes, using the most distant outgroup , and performed the test under fixed topology, which we successively set to the coelomata tree favored by wag, and to the ecdysozoa topology preferred by cat.

under wag, the observed  mean number of substitutions per site, n, is high, reaching  <dig>  per site under the coelomata tree . this is nearly twice as much as the mp estimate , indicating that wag recognizes a high level of sequence saturation in the absolute. note however that the posterior mean value of n is even higher under cat, up to  <dig>  per site . in the two cases, the predictive distribution of n is close to the observed distribution , which is expected, since the length of the branches are free variables, which can adjust so as to match the observed and the predictive number of substitutions.

the posterior mean number of homoplasies per site h is also high under wag , and again, significantly higher than the mp estimate . however, the predictive mean number of homoplasies is much lower than the observed value . the difference, Δh =  <dig> , is large compared to the width of both the observed and the predictive distributions , indicating a clear lack of adequacy of wag. in contrast, in the case of cat, the posterior mean number of homoplasies, which is higher than under wag, does not seem to significantly deviate from the observed distribution . this indicates that, in contrast to wag, cat correctly accomodates the saturation patterns of sequences. note that the deviation observed in the case of wag is not simply accounted for by a difference in the observed and predicted number of substitutions , which suggest that the lack of adequacy of wag lies in the way the substitutions are distributed either over the sites, over the branches, or over the  <dig> states. since cat and wag differ only by the way they handle the amino-acid replacement processes, and otherwise, assume the same patterns of rate variations across sites, and ignore heterotachy, it seems likely that the observed deviation mainly lies in the way substitutions are distributed over the  <dig> states.

similar results were obtained on the ecdysozoa tree . altogether, these observations confirm that the data are indeed saturated, to an extent that cat not only better evaluates than wag , but also, correctly anticipates .

effective size of the amino-acid alphabet
what exactly makes cat more able to detect homoplasy? as mentioned in the introduction, when looking at protein alignments, only a restricted subset of amino-acids is usually found at a given site. in accordance with this observation, under the cat model, most sites are indeed inferred to evolve under highly peaked amino-acid profiles , that give a significant probability to only a few amino-acids. importantly, these restrictions are encoded in the stationary probabilities  of the site-specific amino-acid replacement processes, and thus, they will have an influence even in the long run, after many substitutions. in contrast, in standard one-matrix models, such as wag, the site-specific biochemical preferences are mediated mostly by the relative exchangeability parameters, and not by the stationary probabilities, which are in general close to, if not set equal to, the global empirical frequencies. since the relative exchangeabilities only encode the transient behavior of the substitution process, in the long run, the same broad amino-acid profile is then inevitably expected at all positions under such matrices.

to measure the impact of this difference between wag and cat on real cases, we performed a posterior predictive analyis of the mean number of different amino-acid per columns. as shown in figure  <dig>  the observed mean number of distinct residues per column  is  <dig>  on meta <dig>  under wag, the predictive biochemical diversity is significantly greater, of about  <dig>  , which means that wag is strongly rejected for its inability to account for the site-specific biochemical specificities observed in real data, at least in the present case. in contrast, the biochemical diversity predicted by cat  is much closer to the observed value , although cat is also rejected  by this posterior predictive test.

these spread-versus-peaked tendencies of wag and cat probably have a direct influence on the way these two models deal with sequence saturation. essentially, if most sites undergo repeated substitutions among two or three amino-acids, then the true probability for a site to display the same state independently in two non-related species is only about one-third to one-half. if, however, one assumes that all  <dig> amino-acids can be observed at any position, as do standard empirical matrices, this probability is estimated to be much lower, as low as 1/ <dig> if all amino-acids are considered as equally frequent. under non-uniform equilibrium frequencies , this probability is higher, but still quite low. in contrast, models acknowledging site-specific restrictions of the amino-acid alphabet, as does cat, can in principle correctly estimate this probability. this in turn implies that, in a real phylogenetic analysis, whenever two taxa will display the same state at the same site, cat will be much more ready to interpret this observed identity as a homoplasy, rather than as a shared derived character. thanks to this phenomenon, cat is inherently more robust against homoplasies than wag, which may be the reason why it does not produce the coelomata artefact.

to get a more quantitative view of this phenomenon, we computed an index based on information-theoretic arguments, and meant as a measure of the effective number of amino-acids implied by a model of evolution . this index measures the effective size of the amino-acid alphabet. in the case of wag, it is defined globally, over the alignment, and is here equal to  <dig> . under cat, it is site-specific, and its average value over the alignment is equal to  <dig> . by taking the inverse of these two numbers, one can estimate the probability of homoplasies, under complete saturation, at  <dig>  for cat, versus  <dig>  for wag.

these estimates are only valid at stationarity. to know what happens at lower saturation, we measured the average frequency at which the substitution process returns to its initial state after  <dig>   <dig>  n substitution events, under each model . since the process is reversible, reversions and convergences are equivalent, and we are thus computing the n-step homoplasy probability, i.e. the posterior predictive average probability that, at a given site, two species separated by n >  <dig> substitutions along the tree will be found in the same state.

as can be seen from this figure, the n-step homoplasy probability of wag reaches the stationary state very rapidly, and then stays at a very low level . thus, on the average, the effect of the exchangeability parameters of the matrix are damped after only  <dig> to  <dig> substitutions, after which the probability of the final state is essentially dictated by the stationary probabilities of the process. in contrast, under cat, the n-step homoplasy probability is always higher than under wag, and remains high  even for large values of n. note that, in both cases, the stationary value of the homoplasy level is close to that predicted above from the effective size of the alphabet, which confirms the fundamental link between the spread of the stationary profile under each model, and how these models acknowledge sequence saturation.

impact of the effective size on the lba artefacts
to more directly test the connection between the effective size of the amino-acid alphabet and the lba problem, we performed a phylogenetic estimation under two overly simple models. at one extreme, we used the equivalent of the jukes and cantor model  <cit>  for amino-acids, assuming all  <dig> amino-acid to be equally likely at each site. under this model, the size of the amino-acid alphabet is maximal, and we therefore expect a strong under-evaluation of the true probability of homoplasies, and thus a higher sensitivity to lba  in this case. we call this model the "large-orbit" model . at the other extreme, we devise another flat model, but now, such that the set of allowed amino-acids at a given site is equal to the set of observed amino-acids at that site. in other word, we constrain cat so as to give each site its own profile, which is fixed and gives a probability 1/k to each of the k amino-acids observed at the corresponding column, and a probability  <dig> to all other, non-observed, amino-acids. this "small-orbit" model should be the most skeptical among the flat models in interpreting shared character states as synapomorphies, and therefore, should not produce the artefact. both models assume gamma-distributed rates across sites. note that the "small-orbit" configuration is not a bona-fide statistical model, as it amounts to assuming that non-observed states at a given position would never be observed if new taxa were to be added to the alignment. nevertheless, this toy-model is useful for measuring the impact of the change of the size of the alphabet, everything else remaining the same.

we analyzed the two data sets meta <dig> and meta <dig> using these two settings, and under the four taxon configurations. the results are concordant with our expectations: the large-orbit model yields the same mutually contradictory trees as wag, whereas the small-orbit configuration leads to the same conclusions as cat, i.e. both platyhelminths and nematodes were found sister group of arthropods, irrespective of the choice of the outgroup. this strongly suggests that the size of the amino-acid alphabet effectively accessible at each site is the dominant factor accounting for cat's robustness against long branch attraction artefacts.

CONCLUSIONS
we demonstrated that site-specific amino-acid replacement patterns are a crucial aspect of protein evolution, which is not correctly accounted for by the wag empirical matrix. this particular model-misspecification problem is probably the major reason for the sensitivity of wag to lba artefacts.

in contrast, explicitly accounting for site-specific amino-acid stationary probabilities seems to be efficient at detecting saturation, and in some cases , results in a complete disappearance of artefacts previously observed when using standard models.

empirical matrices other than wag were not tested in this work, nor was the behavior of the most general site-homogeneous reversible model  investigated. however, it should be stressed that all these alternatives, like wag, encode amino-acid specificities in the relative exchangeability parameters. as we mentioned above, and as has been noticed previously  <cit> , these relative exchangeabilities have an influence only on the short-term behavior of the amino-acid replacement process, whereas the amino-acid patterns observed in protein alignments are probably the result of site-specific selective forces, and are thus expected to be observed also in the long-term. hence, we think that there is a logical flaw in the very idea of encoding biochemical realism into a single matrix. in practice, for all such matrices, there will always be a saturation level for which the probability of the states observed at a site is dominated by the global stationary probabilities, and thus, for which the model will underestimate the true saturation level.

concerning the site-heterogeneous models, our main focus was on cat, a mixture model that we proposed previously, but other more simple models accounting for site-specific amino-acid preferences should also display a similar robustness against saturation and lba artefacts. for instance, even the small-orbit pseudo-model investigated above was able to overcome the two artefacts investigated in this paper. we also observed that constraining the total number of categories of cat to be as low as  <dig> was sufficient to obtain the correct tree in all cases . our feeling is that simple mixture models, based on an array of a fixed, and low, number of pre-specified empirical amino-acid profiles will be the best compromise between robustness and computational efficiency.

the substitution processes are also likely to be site-heterogeneous at the nucleotide level, for coding as well as for non-coding sequences. mixture models have also been implemented at this level  <cit> , and they may display a similar robustness agains systematic artefacts. note, however, that, given the relatively small size of the nucleotide alphabet, the actual level of saturation may not be as strongly under-evaluated by standard one-matrix models in the case of nucleotides sequences, as it is for proteins.

finally, it should be stressed that there might be other kinds of model violations also causing lba artefacts, such as across time rate variation   <cit> , or global compositional biases. a model handling all these features should ultimately be considered, and may offer a more satisfactory answer to the problem of tree reconstruction artefacts.

