BACKGROUND
high throughput next-generation sequencing  technologies are capable of generating massive amounts of data in the form of paired-end or single-end reads with either fixed or variable lengths. the size of data files is often in the magnitude of mega- or giga-bytes  and is likely to increase further in the years to come. while sequencing costs have dropped precipitously and sequencing speed and efficiency have risen exponentially, development of computational tools for preliminary analysis of these gigantic datasets have lagged behind data generation. hence, there is an increasing demand for efficient and user-friendly programs for preliminary sequencing data analysis.

currently, there are four commercially predominant ngs platforms, including illumina/solexa, roche/ <dig>  abi/solid and abi/ion torrent  <cit> . these massively parallel dna sequencing technologies have been applied to transcriptome sequencing , de novo genome sequencing, and genome re-sequencing. rna-seq is a widely used approach to transcriptomic profiling  <cit> . two representative efforts in de novo genome sequencing are the genome 10k project to obtain whole genome sequences for  <dig>  vertebrate species  <cit>  and the 5k insect genome initiative  to sequence the genomes of  <dig>  arthropod species  <cit> . genome re-sequencing is an experimental procedure that involves sequencing individual organisms whose genome is already known  <cit> . as a new genomics approach, genome re-sequencing has been applied to a wide range of fundamental and applied biological research including genetics, evolution, biomedicine, human diseases and environmental health, with good examples being the  <dig> genomes project  <cit>  and the cancer genomes project  <cit> .

prior to in-depth analysis of ngs deep sequencing data, e.g., differential gene expression and alternative splicing analysis for rna-seq studies, structural variants identification for genome re-sequencing studies, and genome assembly for de novo genome sequencing studies, investigators are often concerned about the following issues:  basic statistics of a sequencing run such as total numbers of raw, cleaned, and unique reads as well as the degree of reads redundancy;  sequencing library quality, i.e., does the library truly represent the genome of the re-sequenced organism, and  the number of sequencing runs required, i.e., how many runs are necessary to get a full representation of the sequencing library or to suffice a de novo genome assembly. although many tools such as partek genomics suite, clc genomic workbench, or noncommercial platforms like galaxy and genepattern are currently available and capable of indirectly addressing these issues, one would have to possess some basic bioinformatics training and script writing skills in order to manipulate and turn the generated results into useful and straightforward information that can be easily understood by an experimentalists. motivated by filling this gap, the limitations of existing tools, and also driven by the demand for accelerating data-to-results turnaround, we have developed a novel toolkit named seqassist . seqassist specifically addresses the aforementioned three issues and provides investigators who conduct rna-seq, de novo genome sequencing or genome re-sequencing experiments with a quick overview and preliminary analysis of their ngs data.

implementation
seqassist was programmed using perl with an additional java-enabled graphic user interface to enhance efficiency and user-friendliness. it currently consists of three separate workflows: sa_runstats, sa_run2ref and sa_run2run. sa_runstats generates the basic statistics such as total number of raw and cleaned reads, length and copy number of unique sequences, and reads redundancy in a single sequencing run or a pooled dataset of several runs . the input of this workflow is a fastq-formatted sequencing data file. the data file is preprocessed to trim off adaptors and low quality read ends with a default cutoff of base-calling quality score  at  <dig>  followed by removal of n-containing reads. then, the cleaned reads are aligned against each other using bwa-mem , one of the three burrows-wheeler transform-based algorithms in the bwa software package  <cit> . bwa-mem is a robust, fast and accurate aligner that supports paired-end reads, performs chimeric alignment, and tolerates sequencing errors . based on the alignment information in the bwa-mem-generated sam  file  <cit> , the number of unique reads is counted and both identical and inclusive  reads are removed. two reads are considered identical if they match 100% with each other and they are of equal length, while inclusive reads are defined as the sub-sequences of a longer read and only the longest read is kept as the unique read. the redundancy rate is calculated as the percentage of redundant reads in the total number of unique cleaned reads . the output of this workflow includes the total numbers of raw, cleaned, redundant and unique reads, and the redundancy rate. also included in the output is a tab-delimited plain text file that lists all unique sequences along with their length and read count . this file can be used to further infer gene expression levels if the run data is produced for an rna-seq experiment.

  redundancy rate=number of redundant readstotal number of unique cleaned reads×100% 

sa_run2ref analyzes the breadth, depth and evenness of genome-wide coverage of an individual or pooled sequencing dataset at a nucleotide resolution. coverage breadth is defined as the percentage of a reference sequence  that is covered by sequencing reads ; coverage depth is defined as the average times a reference sequence is covered ; and coverage evenness is defined as the coefficient of variance of scaffold coverage breadth . therefore, outputs from sa_run2ref can inform what genomic loci are covered and how a genomic locus , scaffold or the entire genome is covered. in the sa_run2ref workflow , cleaned reads are aligned against the reference genome sequence, generating a sam file. information stored in columns  <dig>   <dig> and  <dig> for each alignment in the sam file represents mandatory fields rname , pos , and cigar , respectively  <cit> . this information is extracted along with the length of each scaffold of reference genome to compute scaffold coverage breadth and depth and genome coverage evenness. these statistics are provided in the output files. the output also includes a plain-text file that records the coverage depth of each individual base on the entire genome. this file can be used as an input for genome browser tools to visualize coverage depth of any genomic regions. in case that the user conducts an rna-seq experiment and provides gene model sequences  as the input, the workflow will calculate coverage breadth and depth for each gene model. this information can be readily transformed into gene expression measurements.

  coverage breadth=number of reference bases mapped by sequencing readslength of the reference sequence in bases×100% 

  coverage depth=total number of bases mapped to the referencelength of the reference sequence in bases 

  coverage evenness=standard deviation of scaffold coverage breadthaverage scaffold coverage breadth 

sa_run2run compares two separate sequencing datasets generated for the same or different dna libraries, computes the basic statistics for each individual dataset, and estimates the redundancy rate between the two datasets. sa_run2run informs the user of the redundancy level both within each individual run and between two sequencing runs. like the sa_runstats workflow , each input run dataset in the sa_run2run workflow  independently goes through the same preprocessing, self-alignment and removing redundant reads steps to generate two new datasets containing unique cleaned sequences. then, the two new datasets are aligned against each other using bwa-mem, generating a new sam file. after combining all aligned reads, reads common to both datasets  are identified, the counts of redundant reads  are calculated for both overlapping and non-overlapping reads. the output statistics from sa_run2run include total numbers of raw reads, cleaned reads, and unique reads , and numbers of total and unique overlapping reads. the redundancy rates within each dataset and between the two datasets can be further derived from these statistics. similar to the sa_runstats output, a list of unique sequences along with their length and count number is provided for each run. however, different from the sa_runstats output, the list generated by sa_run2run is broken into two files, one containing overlapping reads and the other non-overlapping reads. to compare two paired-end sequencing runs, one has to run this workflow twice: run1_r <dig> fastq vs. run2_r <dig> fastq and run1_r <dig> fastq vs. run2_r <dig> fastq. the sa_run2run workflow intends to guide the user in deciding whether to perform more runs on a sequencing library by looking at the percentage of reads in a new run covered by the reads in a previous run or the pooled reads of multiple previous runs.

RESULTS
to test all seqassist workflows, a synthetic dataset was generated by  clipping  <dig> distinct fragments with a length of  <dig> bp at different loci of the escherichia coli str. k- <dig> substr. mg1655l genome  to construct  <dig> artificial chromosomes,  clipping  <dig> sequences of 75- <dig> bp in length from each artificial chromosome, and  repeating each sequence  <dig> times. these steps resulted in a dataset of  <dig>  reads and a reference genome consisting of  <dig> short artificial chromosomes, both of which were used to test the sa_runstats and sa_run2ref workflows. the synthetic dataset was further split in two halves to create run  <dig> and run  <dig> that were used to test the sa_run2run workflow. seqassist output results for the synthetic dataset are in agreement with expected results , validating the scripts coded for all three workflows.

here we demonstrate the applications of seqassist to preliminary analysis of multiple experimental ngs datasets. as the sa_runstats workflow is an integral part of the sa_run2run workflow, we focus on the sa_run2ref and sa_run2run workflows in the following experiments. all the experiments were performed on a dell m <dig> blade server equipped with  <dig> gb of ddr <dig> memory at  <dig>  mhz speed, an intel® xeon® e <dig> cpu quad-core that runs at  <dig>  ghz, and two separate hard drives of  <dig>  tb and  <dig>  tb. the operating system was red hat enterprise linux server release  <dig>   using the centos 64-bit distribution.

experimental ngs datasets
we selected multiple ngs datasets and two organisms of contrasting genomic complexity to demonstrate the features of seqassist. these datasets represented both fixed and variable length reads generated on illumina and  <dig> sequencing platforms, respectively. the chosen organisms were the bacteria escherichia coli, a prokaryote with a simple and small circular genome of  <dig>  mb in length  <cit> , and the water flea daphnia pulex, an eukaryote with a recently published draft genome consisting of  <dig>  scaffolds with a total length of ca.  <dig> mb  <cit> .

two datasets obtained from an e. coli genome sequencing project were downloaded from http://data.clovr.org/. one dataset  contains  <dig>  shotgun  <dig> titanium sequences , and the other  contains  <dig> , <dig> paired-end shotgun illumina sequences . the bio.seqio module in biopython  was employed to convert the sff format to the fastq format for the  <dig> dataset by the command $ python -c "from bio import seqio; seqio.convert;". the d. pulex datasets were collected in-house by repeatedly sequencing two libraries, each of which was prepared from genomic dna isolated from an individual animal. one animal came from a population named ect  and the other from another population named tco .

to demonstrate the scalability of seqassist, we have also chosen a human genome re-sequencing dataset of the ceu hapmap individual na <dig> at a 15-fold coverage with an insert size of  <dig> bp and  <dig> % duplicate reads. the dataset  consists of  <dig> paired-end illumina genome analyzer iix runs and is downloadable from ncbi's sra database at http://www.ncbi.nlm.nih.gov/sra/?term=erx <dig>  the reference diploid human genome  consists of  <dig> pairs of chromosomes or  <dig>  mb in total.

the sa_run2ref workflow
the ect d. pulex gdna library was sequenced twice without multiplexing, generating two paired-end sequencing datasets, ect and ect_rerun. these two datasets as well as their combined dataset were run through the sa_run2ref workflow, producing statistics presented in table  <dig>  approximately 88% of the cleaned reads from the ect or the ect_rerun dataset was mapped to the reference genome, covering 76% of the  <dig>  scaffolds or 64% of the entire genome at a 9-fold depth. the combined dataset covered less than 1% more scaffolds than individual datasets, and it also had similar genome coverage breadth and evenness as the two separate datasets, even though it doubled the genome coverage depth. the distribution of scaffold coverage breadth showed a very similar pattern with ca.  <dig> scaffolds uncovered for all three datasets . in comparison with the two separate datasets, the combined dataset covered  <dig> and  <dig> more scaffolds at > 4-fold depth or  <dig> and  <dig> more at > 10-fold depth than the ect and the ect_rerun datasets, respectively . the number of scaffolds with a coverage breadth of 50% or less in the two separate datasets was  <dig>  or  <dig>  more than that in the combined dataset. these results indicate that the additional sequencing run  did not improve much coverage breadth or evenness, and that the two runs covered almost the same scaffolds.

the two datasets of paired-end reads were generated using illumina miseq by sequencing the same genomic dna  library prepared for a water flea  from an ect population. library preparation involved shearing of extracted gdna using a covaris m <dig> focused-ultrasonicator . the average of library insert size distribution was  <dig> bp.

all the ngs run datasets were generated by sequencing the tco gdna library which was split into two fractions: a large fraction  with an average insert size of  <dig> bp and a small fraction  with an average insert size of  <dig> bp. an illumina miseq was used for sequencing and both fractions were each sequenced five times in a  <dig> × or  <dig> × multiplexing fashion, resulting in datasets lf <dig> to lf <dig> and sf <dig> to sf <dig>  the reads collections were mapped to a d. pulex reference genome by running the sa_run2ref workflow.

overlapping reads are defined as those found in both runs. unique overlapping reads from both runs are those left after removing redundant reads  from the combined overlapping reads from both runs.

with regard to redundant reads, a distinction is drawn between identical and inclusive reads. if reads are of the same length, there is no difference between these two types of redundant reads. consequently, the number of unique overlapping reads in run <dig> is the same as those in run <dig> and both runs . preprocessing of illumina datasets may cause length differentiation between cleaned reads. for datasets of reads with variable length, the number of unique reads after removing identical reads is higher than that after removing both types of redundant reads . as a result, the number of unique overlapping reads in run <dig> may differ from that in run <dig> and in both runs .

analysis of the human genome re-sequencing dataset
two paired-end sequencing runs, err <dig> and err <dig>  within the erx <dig> dataset  were analyzed using the three workflows to show the scalability of seqassist. both runtime and memory usage were recorded and shown in table  <dig> and figure  <dig>  respectively. the human genome is  <dig> times bigger than the d. pulex genome, and the size of the human genome re-sequencing runs in base pairs  is nearly triple that of d. pulex . however, the runtime of the human data through the sa_run2ref workflow  was ca.  <dig> ~ <dig>  times that of the ect  or the ect_rerun  data . for the sa_run2run workflow, their runtimes differed only by  <dig> -fold, i.e., 96~ <dig> min for d. pulex vs. 348~ <dig> min for human. the memory space consumed by seqassist when running the two human datasets did not exceed 10% of the 284-gb ram, except for a surge that occurred when multiple threads were used for calculating the statistics after the completion of bwa alignment at ca.  <dig> seconds in the sa_run2ref workflow . these results suggest that seqassist is fully capable of handling any sequencing run data generated by current ngs platforms for organisms with a reference genome of any size and complexity, and results can be produced rapidly within a working day . this feature satisfies the demand for a quick turnaround from mega data to preliminary results.

the read length was  <dig> bases and the number of reads was  <dig>  m and  <dig>  m for err <dig> and err <dig>  respectively. the runtime is rounded up to the closest minute. na: not applicable.

CONCLUSIONS
we have demonstrated the main features of seqassist using multiple genome re-sequencing datasets. output statistics from seqassist can guide the user in evaluating the quality of a dna library prepared for genome re-sequencing and in deciding whether there is a need to perform additional sequencing runs on the library. based on the low coverage breadth  and the high reads mapping rate  , it appears that the ect gdna library may not be a good representation of the entire genome. the same holds true for the tco library if considering its maximal breadth  and mapping rate . in terms of the number of multiplexed sequencing runs required for the tco library, five runs of the large fraction seemed to be sufficient because they reached the maximal coverage breadth, with a total raw reads number of  <dig>  million.

in summary, the sa_runstats workflow generates basic statistics about an ngs dataset, including numbers of raw, cleaned, redundant and unique reads, redundancy rate, and a list of unique sequences with length and read count. the sa_run2ref workflow estimates the breadth, depth and evenness of genome-wide coverage of the ngs dataset at a nucleotide resolution. the sa_run2run workflow compares two ngs datasets to determine the redundancy  between the two ngs runs. statistics produced by seqassist or derived from seqassist output files are designed to inform the user: whether, what percentage, how many times and how evenly a genomic locus  is covered by sequencing reads, how redundant the sequencing reads are in a single run or between two runs.

seqassist is a useful and informative tool that can serve as a valuable "assistant" to a broad range of investigators who conduct genome re-sequencing, rna-seq, or de novo genome sequencing and assembly experiments. for rna-seq experiments, seqassist output files that contain unique sequences along with their mapped genomic loci and copy numbers may be readily transformed into gene expression data. an investigator who de novo assembles a genome from sequencing data may use seqassist to map the original reads to the assembled genome and obtain a ratio of mapped to cleaned reads. as an additional parameter to existing metrics  <cit> , this ratio can be used to objectively compare the quality of different assemblies made from the same sequencing data. for further in-depth analyses of ngs data, one is advised to use other appropriate tools available from the bioinformatics community. for instance, one may choose to apply spliced aligners such as rum  <cit>  and spliceseq  <cit>  to identify splice junctions for alternative splicing detection of rna-seq data, or employ a structural variant  discovery software such as breakdancer  <cit> , pindel  <cit>  and prism  <cit>  to call sv events and discern breakpoints from genome re-sequencing data.

we plan to improve visualization features of seqassist in the future versions. specifically, the nucleotide-resolution mapping and coverage depth  information generated from sa_run2ref shall be transformed into interactive visual graphics to allow the user to visualize gene coverage or expression levels.

availability and requirements
• project name: seqassist

• project home page: http://orca.st.usm.edu/cbbl/seqassist/

• operating systems: linux 

• programming language: perl v. <dig> . <dig> with the following packages: parallel::forkmanager and getopt::long

• other requirements: bwa , cutadapt , gnuplot, and java runtime environment 

• license: free for commercial and academic uses.

• any restrictions to use by non-academics: none.

competing interests
the authors declare that they have no competing interests.

authors' contributions
pg and cz conceived the project. pg designed the software architecture. yp implemented the code. nw and pg supervised the coding. asm developed the gui. asm and yp conducted the computational experiments. cz, nw and pg supervised the experiments. ndb, jgl, ajk and pg generated the daphnia pulex sequencing data. pg, asm and yp drafted the manuscript. all authors revised, read and approved the manuscript.

