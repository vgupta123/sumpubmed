BACKGROUND
in silico experiments are performed using a set of computer analysis and processing tools that are executed in a specific order. to automate the execution of these tools, they are usually organised in the form of a pipeline, so that the output of one tool is automatically passed on as the input of the next tool. in such a process, it is helpful to have tools that are designed in a way that guarantees the interoperability of all execution steps. the interoperability ensures that the output of a tool is processed by the subsequent tool even if the output format of the former does not match the input format of the latter. aside from enabling task automation and data flow control, pipelines may be particularly advantageous if they allow an increasing number of possible operations offered to the user by combining different tools. for example, if we have four analysis tools: blast
 <cit> , that finds sequence similarities for a dna sequence; clustalw
 <cit> , which aligns a set of sequences from different species; phylip
 <cit> , which finds phylogenetic relationships from sequences of different species; and paml
 <cit> , that infers sites under positive selection from a set of closely related sequences. in addition to their individual functionality, we can combine blast, clustalw and phylip in a pipeline to find possible phylogenetic relationships for a dna sequence. alternatively, we can also compose a pipeline using blast, clustalw and paml to infer sites under positive selection. because the output of blast is not compatible with the input of clustalw, additional reformatting by ad hoc scripts is required to ensure the interoperability of the tools in the pipelines.

the traditional and simplest implementation of pipelines involves hardcoding the execution steps into programs or scripts. this approach leads to problems when pipelines need to be expanded, because the addition of new tools to such a pipeline is error prone and time consuming. an experienced programmer is needed to change the hard-coded steps of such pipelines to include new tools in the pipeline while maintaining bug-free functioning. these problems are a major concern not only for bioinformatics laboratories that want to continuously update their pipelines with new software developments, but also for those who want to consolidate open and cooperative systems
 <cit> .

an additional level of flexibility may be achieved by workflow management systems such as taverna
 <cit> , galaxy
 <cit>  and pegasus
 <cit>  that are well suited for analysis tasks that are systematically repeated without changes in the course of execution, such as genome annotation
 <cit>  and the tasks registered at the myexperiment website
 <cit> . some workflow management systems also support dynamic execution of workflows, such as kepler
 <cit>  and others
 <cit> , where dynamism occurs during the mapping and execution phases of the workflow’s life cycle
 <cit>  mainly for the instantiation of workflow components based on a high-level workflow description and data type compatibility verification. in these systems, the composition of the high-level workflow description is usually left to the user, which can either assemble his own group of tools or reuse an existing workflow description. however, in applications in which tools can be combined in different ways into a pipeline, it is difficult for the user to keep track of all possible combinations. this requires an automatic approach one level above execution, during the composition of the pipeline. this type of situation arises, for example, in format mapping, i.e., the conversion between software file formats that relies on a combination of conversion tools to map one format into another. consider, for example, the following conversion system: tooltαβ maps format α into format β, tool tβγ maps format β into format γ, and tβδ maps format β into δ. a workflow approach to implement such a conversion system requires the creation of five different workflows, one for each possible mapping . in this case, to convert αinto β, we would have wαβ; to convert α into γ, we would have wαγ; and to convert α into δ, we would have wαδ. if a new conversion tool is added into this system, such astδ∈, additional workflows are needed to implement the new functionality . without an automated process for composing workflows, these new workflows have to be created by users or by the system’s developers. in this case, the ideal solution would employ pipelines that are arranged “on the fly” in an automatic way, depending on the functionality required, instead of being statically programmed into a limited set of workflows.

in this paper, we propose a graph-based approach to design extensible pipelines. this approach is a solution for pipeline applications with multiple functionalities that require different combinations of steps in each execution. by automatically combining tools on demand into a pipeline according to the required functionality, it becomes unnecessary to specify every potential sequence of tools beforehand. for developers, this allows the implementation of low-maintenance bioinformatics pipelines. also, users do not have to compose a pipeline for every different task, since all possible compositions are automatically available to the user. extensibility is achieved once new tools are easily added to the pipeline system without any necessary change on the system’s code. in this way, the system can expand and the number of tools that it comprises can increase without the need for a specialised user with programming skills. to that end, we have developed special data structures and a pipeline system algorithm. we demonstrate the applicability of our approach by implementing a format conversion pipeline for the fields of population genetics and genetic epidemiology.

RESULTS
we represent the connectivity of pipeline components  with a directed graph. if there is an edge e connecting two nodes v <dig> and v <dig> in a graph g, with e acting as the incoming edge of v <dig> and the outgoing edge of v <dig>  then e is a component that receives input v1and that generates output v <dig>  pipeline components are programs , and they receive one or more inputs, perform some processing on these inputs and generate one or more outputs. inputs and outputs are data file types. in terms of bioinformatics pipelines, graph edges are tools such as blast and clustal, as well as tools that guarantee interoperability. nodes represent the input and output formats required or generated by these tools .

a path in the graph is any sequence of nodes connected by edges leading from one node to the next. this sequence of nodes can also be seen as a sequence of the edges that connect them. therefore, a path through the graph connecting an input x to an output y  represents a pipeline, a sequence of tools, that must be executed to generate y  from x.

to implement the graph-based approach, we developed  a data structure called a tool registry, which contains information about the tools, such as the inputs that they receive, the outputs that they generate and the names of their executable file, among other information, and  a pipeline system algorithm, which creates a graph representation of the tool registry, finds a path through the graph and generates an executable function-specific pipeline.

the pipeline system algorithm is illustrated in figure
 <dig> and works generally as follows:  it receives as input the start and end points of the pipeline, which are, respectively, the original file to be processed and the desired resulting file, as well as the tool registry;  it builds a directed graph based on the registry file, using inputs and outputs as nodes and tools as edges connecting their respective inputs and outputs;  it applies a graph-traversing procedure to find a path through the graph connecting the start and end points, which represents the execution steps of a pipeline for a specific processing task; and  it returns this pipeline in an executable format. in figure
 <dig>  letters represent data file types that are processed by bioinformatics tools. although it is a simplification of real world cases, the illustration is intended to show how the connectivity among tools is represented in the graph, based on the descriptions on the tool registry.

in case there are alternative paths  available for the required processing task, the graph-traversing procedure selects the best one according to some criterion. we defined two alternative criteria for the best pipeline: performance, as measured by the speed of the pipeline, and input dependencies, according to which the selected pipeline is the one requiring the smallest number of input files. these criteria are called weight criteria . the performance criterion in calculated on the basis of the tool’s response time for processing one or more input files of a specific size . the choice for one criterion or another can be presented to the system’s user, or the decision can be made by the system’s designer beforehand. we discuss the use of different selection criteria in section discus-sion. the components of our graph-based approach and the steps through the algorithm are explained in detail next.

we use the following notation to represent specific graph elements: esource,target, where e is the edge that connects a source node to a target node, and pstart,end,…,), where pstart,end is a path through the graph that begins at the start node and finishes at the end node passing by zero or more nodes.

the tool registry
all information about the tools that are part of the pipeline system is stored in a tool registry. each entry on the registry describes a particular tool with the following attributes  for a partial representation): the input that it accepts , which is a file type; the output that it generates , which is also a file type; its executable file name ; its programming language ; an identification number ; a list of extra input file types required to run it ; a list of secondary output file types generated by the tool ; a performance measure indicating its average execution time ; free text observations that the tool provider thinks the user should know in order to run it ; and the provider’s name  and contact information . this information must be given by the tool provider before it is added as a new component of the pipeline system. a complete sample file is provided in the additional file
1: table s <dig> 

new tool versions can be added to the registry with a new tool name. if the input and output file types from both versions are the same, the algorithm would find both tools as alternative paths and choose the one with best performance. if the input or output is different from the previous version, new format type names must be provided at the new version’s entry on the registry.

pipeline components
pipeline components are programs or scripts that receive one or more inputs, perform some processing on these inputs and generate one or more outputs. to generate executable pipelines automatically, we define a specific format for the command line calls used to invoke the pipeline components: <tool> <input>  <output> 

here, input and output are the tool’s parameters stored in the tool’s entry in the tool registry. parameters in square brackets are optional and correspond to the tool’s extra inputs and secondary outputs. we discuss an extension to this command line format in section discussion.

pipeline system algorithm
to generate an executable pipeline for a specific functionality, such as converting data file a to data file f, our pipeline system algorithm builds a directed graph on the basis of the tool registry, and it finds a path through this graph using the original input to be processed  as start point and the desired output  as the end point. this path represents a pipeline where the sequence of edges in the path is the sequence of tools to be run. this process is illustrated in figure
 <dig> .

the algorithm receives as input the start and end points, the tool registry file  and the weight criterion to be applied to the graph edges . it starts by building a directed graph g based on the information in the tool registry file. this process is accomplished by taking each entry in the tool registry, represented by e <dig> …,ej, and parsing it into a tuple ej, where each element corresponds to a field  in the tool registry. it then adds the input and output information, ej and ej, as nodes in graph g and the tool’s name, ej, as an edge connecting its respective input and output. if the input and output file types of a specific tool are the same, an edge is created in the same way as before. in this case, edge’s source and target nodes will be the same. provided that a tool to trim or filter files of the same type, generating an output file with different content but of the same file type as the input, is included in the tool registry, our solution allows adding tools that perform these tasks. to each edge, we assign a weight wj that is calculated according to the chosen criterion  for selecting among multiple paths. if wt is performance, then wj is the performance measure ej; if wt is dependencies, then wj is the length of the input dependencies list for that tool, length.

after that, the same process is repeated for adding to the graph both the list of input dependencies  and the list of secondary outputs  for all tools. the only difference is that the graph edges connecting extra inputs and outputs to other nodes receive a symbolic zero weight, since they do not account for any processing task. also, if a node is equal to an extra input or to an extra output already found in g, an alias is created  so that these extra input or output nodes can be added to the graph .

with the tool registry represented as a directed graph, the algorithm then searches for a path  to connect the start and end points . this process is accomplished using a graph-traversing shortest path procedure that implements dijkstra’s shortest path algorithm . if a path exists, it represents the sequence of tools that need to be run to generate the desired output. this process is illustrated in figure
 <dig>  where the path connecting the start and end points a and f is pa,f,,,), and its corresponding tool path is pa,f,,,). if no path is found, then there is no available pipeline for the required processing task. on the other hand, if there is more than one possible path connecting the start and end points, the shorted path procedure chooses the path with the smallest sum of its composing edges’ weights. as mentioned before, this process entails selecting the path that will result in a pipeline composed of the best performing scripts  or requiring less user intervention .

after finding the pipeline for the required processing task, the algorithm generates an executable version of this pipeline. this process is illustrated in figure
 <dig>  the executable version indicates the inputs required to run the pipeline , the command line call for each tool , and the outputs that are generated .

required inputs  include, in addition to the original file to be processed, the input dependencies that might exist for each tool that will run in the pipeline. for example, of all the tools in pa,f,,,), tce  is the only one with an extra input file e4 = {y}. this information is extracted from the tool registry. thus, li = {a,y}. similarly, the output files of the pipeline  include, in addition to the desired output file, any secondary outputs that might be generated by each tool in the pipeline. for example, in pa,f, none of the tools has an extra output file; in this case, lo = {f}. these lists of inputs and outputs are used to generate the files .inputs and .outputs.

in the file .exec, tools are invoked by a command line call with the following format : 

  enenenen]enen] 

 where en is the programming language call, en is the executable name, en is the input, and en is the output for all en∈p. the parameters en] and en] are optional and represent extra inputs and secondary outputs for each tool.

running the executable function-specific pipeline
the executable function-specific pipeline in the .exec file can be run as a shell file or incorporated into another application as a set of system calls. the user just needs to provide the required input files . tools in the .exec file execute locally on the same machine. since our pipeline design approach focuses on pipeline composition instead of execution, we have adopted a simpler execution mechanism. for error control, we provide a .err file, which stores error messages generated during the execution of the function-specific pipeline. quality control procedures for input data must be implemented within each independent tool by its provider, since each processing task or data format will have its own requirements. this type of setup helps to maintain the system’s modularity and extensibility.

for a broader application that requires a more user-friendly interface, the three files generated by the pipeline system algorithm can be easily incorporated into a graphical interface to create an interactive pipeline. an example of an interactive pipeline system written in php is provided at the project’s website and is described in section a format conversion pipeline application. this web-based system reads the .inputs file and presents to the user an upload page requiring all inputs specified in this file. when all required inputs are uploaded into the system, it executes all system calls in the file .exec, in order. after the last system call is finished, the interface system reads the file .outputs and presents the user with a link to each of the output files specified in the list. a similar procedure can be used to incorporate the pipeline system into a standalone application.

adding new tools
to add a new tool to the pipeline system, a new entry must be added in the tool registry containing the information about the new tool, therefore, no programming is required. this update can be performed directly by the tool’s developer or by the system’s administrator upon request from the tool’s developer that, in this case, must send all the required information about the tool. the ordinary user sees only the final result and the next time that he uses the pipeline system, the new tool’s functionality will be considered as part of the pipeline composition. this is possible since our pipeline system algorithm automatically and on demand generates the tool graph including this information. the only requirement for adding a tool to a pipeline system implemented with our algorithm is that it must follow the command line format described earlier in section pipeline components. also, if the new tool requires a file type that is not already specified in the pipeline system, it is recommended that the developer provides a sample of such an input file so that a benchmark can be run to determine the tool’s performance.

a format conversion pipeline application
we applied our graph-based approach to implement an automatic pipeline system for data format mapping in the fields of population genetics and genetic epidemiology. these fields, and others such as gene expression and proteomics analyses, require a specific set of data analysis procedures that use several different software packages
 <cit> . since most of these programs are not compatible in terms of accepted input and output formats, solutions to allow interoperability are required. we proposed elsewhere
 <cit>  a conversion pipeline to solve this interoperability problem in the context of dna re-sequencing data. this conversion pipeline is composed of a set of scripts that convert one specific format to another. by combining such specialised scripts in a pipeline, we increase the number of possible conversions that are available to the user. in the
 <cit>  pipeline, however, possible combinations of scripts are hard-coded into the system, and thus, extension with new tools is costly because of the need for an experienced programmer to alter all of the pipeline code. to avoid this problem, we applied our graph-based approach to implement a dynamic version of this conversion pipeline. by combining the conversion scripts on demand into a pipeline based on the specific conversion required, it becomes unnecessary to specify beforehand the sequence of scripts for performing every possible conversion. we also added new tools to the original pipeline to increase the scope of its functionality.

currently, our format conversion pipeline handles data formats that are compatible with the following software: polyphred , phase , dnasp , structure , sweep , haploview  and r-based tools for population genetics and genetic epidemiology such as hierfstat  . the pipeline also handles general purpose file formats such as sdat, nexus and prettybase. it comprises  <dig> conversion tools implemented in perl, which allow for  <dig> possible format conversions.

to make the format conversion pipeline interactive and available online, we implemented our pipeline system algorithm as part of the web interface shown in figure
 <dig>  its website is hosted at
http://pggenetica.icb.ufmg.br/divergenome/pagina/dynamicpipeline/tools.php. the algorithm is invoked after the user selects the input format and desired output format . examples of the file formats are available at our re-sequencing pipeline website . the tool registry for this application is shown partially on table
 <dig> and in more detail in the additional file
1: table s <dig>  the registry is used by the pipeline system algorithm to generate the graph in figure
 <dig>  here, graph nodes represent data formats, and edges represent the conversion tools’ codes with their corresponding weights. in our application, we used the performance criterion for selecting the best path among alternatives. thus, edge weights are the performance measures that are specified for each tool in the tool registry. note that extra inputs and outputs are represented by double circled nodes, as before, and they are renamed by adding a numerical index to their format name, in case they already appear in the graph . the weights of the latter incoming or outgoing edges are set to  <dig> since they do not account for any processing task. to demonstrate the functionalities provided by our automatic pipeline approach, we present three different potential usage scenarios.

columns input and output are file formats that are accepted and generated by a conversion tool; tool and language are the conversion tool’s name and its programming language; code is the identifier of the tool; xi is the list of extra input files required for the tool’s execution; xo is the list of extra output files that is generated by the tool; and performance is a measure related to the tool’s execution time. other information not represented on this table can be found in the additional file
1: table s <dig> 

sdat to r-hierfstat
first, let us suppose that, in a population genetics study, a researcher downloaded a dataset in sdat format, containing a matrix of genotypes per sample and locus, and now the researcher wants to perform an analysis with the r package hierfstat to compute and test fixation indices for any hierarchical level of population structure. since the sdat format is not a valid input for hierfstat because the latter requires additional population information, the user needs to convert the sdat format. to perform this conversion, the user chooses the two file formats of interest on the web interface shown in figure
 <dig> , sdat and rhierfstat. as visualised in the graph in figure
 <dig> , there are two possible paths for this conversion: p1) or p <dig>  ). from these, the first path is chosen since its sum of edge weights  is smaller than that of the second path , meaning that the pipeline corresponding to the former path is the fastest. the tool path for this selected path is p1) .

the tool path p1is used by the pipeline system algorithm to generate the executable pipeline for the specific conversion, as described in section pipeline system algo-rithm. the three output files of the executable pipeline are shown in table
 <dig> . they are handled internally by the system and the users see only the final web interface. upload boxes for each required input are built into the interface based on the .inputs file . each sdat file corresponds to different populations that should be included in the study. although for simplicity we show only one extra sdat file for the tools converting from sdat to rhierfstat in table
 <dig> and figure
 <dig>  in practice these tools currently accept up to five populations. after these input files are uploaded, the interface reads the .exec file, runs it as a shell file, and presents the output files in .outputs as links for the user to download from.

executable pipelines for file-format conversions:  sdat format to r hierfstat input format;  polyphred output format to structure input format; and  software phase output format to fasta format. in practice, input and output files handled by the pipeline system are renamed to include a timestamp identifier of each specific pipeline . this guarantees that inputs and outputs stored in the system are unique for each dynamically generated pipeline.

polyphred to structure
in the second scenario, the software package phred-phrap-consed-polyphred is used for variation screening and one follow up analysis is to infer population structure using the program structure. this may be useful, for example, if a set of linked chromosome regions have been re-sequenced in a set of individuals, and the linkage model of structure
 <cit>  is intended to be used to explore the population structure of this genomic region. the output and input files generated and accepted by these two software programs are not compatible, and thus, the user needs to convert the output of the end-line software polyphred, containing individual genotypes and, into the input for structure. to do so, the user chooses the two file formats of interest on the web interface shown in figure
 <dig> , polyout and structure_format.

the path found by our algorithm, shown in purple arrows in figure
 <dig>  is p <dig>  , ), which corresponds to the tool path p <dig>  , ) . the executable pipeline that is generated by our algorithm for the specific conversion and implementation of this tool path is shown in table
 <dig> . note that this pipeline requires only one input file but generates three output files, which are displayed in figure
 <dig> . this is because the end-line tool sdat2structure.pl in p <dig> has two extra output files , which are necessary to run the program structure.

phase to dnasp
for the third scenario, we take the fact that, in population genetics studies, it is common to run the software phase to infer haplotype phase and then perform general population genetics analysis with the program dnasp. since the input and output of these software tools are not compatible, the user needs to convert the output of phase, containing phased polymorphic sites, to the input format for dnasp, a fasta file. this conversion can be accomplished by selecting the two file formats of interest on the web interface shown in figure
 <dig> . the path found by our algorithm that connects phase output format  and fasta format is depicted in figure
 <dig> with blue arrows and is formalised as p4). its corresponding tool path is p <dig> . the executable pipeline generated by the algorithm for the specific conversion is shown in table
 <dig> . it requires three input files, , and generates one output file, . this is because the tool phase2fasta.pl has two extra input files, which are necessary to build the new fasta sequence .

discussion
building extensible systems is essential to ensure that new tools and data formats can be used with existing systems. this principle applies to the design of pipelines, a common task in most bioinformatics laboratories. here, we propose a graph-based approach to this view of extensible pipelines, in contrast to traditional ad hoc pipeline designs.

our approach is suitable for sequential pipelines in which each execution requires different combinations of steps through the pipeline. we have shown one such pipeline application for format mapping for population genetics and genetic epidemiology analyses. this pipeline provides  <dig> possible format conversions that originate from the combination of  <dig> independent conversion tools. by combining these scripts on demand into a pipeline according to each required conversion, it is not necessary to specify every possible combination of scripts beforehand. moreover, with the graph-based implementation, new format conversion tools can be easily incorporated, and the system can stay updated. for instance, our group is developing conversion tools compatible with the sam formats created by the 1000genomes project team
 <cit> . our approach also allows prompt integration of third party conversion tools developed by collaborators or available in public software repositories. the process of third-party adding new tools to the system was tested with the tools sdat2rgenetics.pl, sdat2rhierfstat.pl and sdat2nexus.pl which were later incorporated by different group members of our laboratory.

notably, when planning the addition of a new tool to the pipeline system, it is possible to take advantage of graph properties such as node connectivity to maximise the number of new functionalities. for example, taking our application graph in figure
 <dig>  it is clear that if you develop a conversion tool that maps formatx into the nexus format, you gain only one additional conversion when adding this tool to the system . on the other hand, if you develop a conversion tool mapping formatx into sdat format, you gain  <dig> additional conversions . we provide a java program in the project’s website  to help with this analysis.

in contrast, to implement the same format conversion pipeline with a workflow management system
 <cit> , it would be necessary to create a separate workflow for each possible inter-format conversion. also, these workflow management systems are more frequently used in genomic sciences and focus on workflow execution, while their users  have to select and combine their specific components. another example is the pegasus framework
 <cit> , which is very robust on managing workflow execution but does not address the problem of automatic composition. differently, our approach has been developed keeping in mind users who may not be necessarily bioinformatics experts and who require assistance on the combination of tools to be used in a specific analysis. for this purpose, our approach incorporates pipeline automatic composition as a conceptual and operational instrument to facilitate its use.

similar work on automatic service composition, such as magallanes
 <cit>  and bio-jetii
 <cit>  also focus on linear workflows and components with basic interfaces . however, the main difference is that they present a different implementation for the automatic composition problem, not graph-based, and their approaches consider web services to compose the workflows, without performance information. the automatic pipeline approach, on the contrary, integrates ad hoc bioinformatics tools or scripts, in our case format conversion tools for population genetics or genetic epidemiology, with an associated performance measure that is used to select among possible alternative pipeline executions. another difference regards the generation of an executable pipeline. in the case of magallanes, it does not generate an executable workflow but only a model to be instantiated with web services by workflow management tools. similarly, in bio-jeti automatic service composition starts only after the user has assembled a high-level workflow specification manually through a graphical interface.

at present, our system can only perform automatic composition based on computer-measurable metrics, such as processing time, memory usage, and accuracy, among others. this is to guarantee the composition of a pipeline without user intervention. however, our approach has the potential to accommodate a user-centered choice, either based on his preferences or the context of his analysis. to implement that, instead of automatically selecting a pipeline among alternatives, our algorithm can be modified to present these alternative pipelines to the user, which can then select the best one.

a current limitation of our approach is that it cannot yet be used for automatically designing pipelines that require the execution of parallel steps because it focuses on the problem of finding alternative sequential steps to achieve a particular aim. however, adjusting our algorithm to support the second type of pipeline is straightforward. this can be done by taking alternative paths through a tool graph with overlapping edges as single pipelines where non-overlapping steps are executed concomitantly. therefore, if there are three edges connecting nodes a and b, that is, three different tools processing file type a into b, the parallel algorithm would select all three tools to be executed at the same time.

for future development, we are studying an extension to the current algorithm to allow the inclusion of software tools that require specific command line parameters, such as strings and thresholds. currently, pipelines are created with a set of tools that each use a standard command line interface that allows for the specification of one or more input files and one or more output files. we are working on a xml implementation of the tool registry to incorporate definitions of different classes of input parameters for the tools, such as files , strings and numerical values. this extension will allow the incorporation of bioinformatics tools that require different types of parameters, and general bioinformatics programs available in public repositories such as bioperl and biojava. we will consider current work on semantic service description, such as owl-s
 <cit>  and the web services description language 
 <cit>  to develop the xml-based tool registry. finally, although here we have focused on applications that are composed of software tools, our graph-based approach could also be used to create pipelines that are composed of workflows or web services. this would only require a modification of the function that generates the executable pipeline so that it generates executable code that is compatible with each specific technology.

CONCLUSIONS
our graph-based approach enables the automatic creation of pipelines by compiling a specialised set of tools on demand, depending on the functionality required. it allows the implementation of extensible and low-maintenance pipelines and contributes towards consolidating openness and collaboration in bioinformatics systems. it is targeted at pipeline developers and is suited for implementing applications with sequential execution steps and combined functionalities. the algorithm serves as an alternative to workflow systems since it generates pipelines automatically without living the composition to the end-user. we have shown that this is the case for format conversion applications, in which the automatic combination of conversion tools increases the number of possible conversions available to the user and increases the extensibility of the system to allow for future updates with new file formats. future developments will include an adaptation of our pipeline algorithm to enable the generation of pipelines with parallel steps and to allow the inclusion of tools that require external parameters. extensions are also possible to generate executable code that is compatible with specific technologies, such as web services and workflows.

