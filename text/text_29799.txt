BACKGROUND
dna microarrays are commonly used to measure, in parallel, the steady-state concentration of tens of thousands of mrnas, providing an estimate of the level of expression of the corresponding genes. they come in two flavors: 1) spotted arrays allows for the simultaneous measurement of two samples on the same array, we'll refer to these arrays as multi-channel arrays; 2) affymetrix genechips arrays with a significantly higher density but only allowing for the hybridization of one sample, we'll refer to those as high-density arrays, hdas. a typical piece of information that investigators seek to extract from microarrays is the list of differentially expressed genes  between a treatment and a control condition. this can either take the form of a defined subset or an ordering of the whole transcriptome on which some meaningful  threshold is applied. until recently most of the efforts to derive a statistical method to solve this problem have been focusing on a general approach applicable to both type of arrays. the strategy when analyzing hdas is to estimate the absolute level expression of a given gene in each condition and then compute the log-ratio of the expression levels between two conditions. methods applied to log-ratios on multi-channel arrays can readily be applied to these computed ratios.

a fundamental issue when analyzing microarrays data is the imbalance between the dimensionality of the data  vs. the number of samples or replicates . on the theoretical side, this imbalance gives rise to the necessity of adjusting statistical thresholds to the context of multiple testing  <cit> , seriously complicating the accurate estimation of sensitivity and specificity. because widely used statistical methods are based on an estimate of the gene-specific variation, they technically require at least  <dig> replicates per conditions. in the context of multi-channel arrays, experts in the field have been recommending the use of  <dig> replicates  <cit> . but, for practical reasons, most laboratories have been settling for  <dig> replicates per condition. proof-of-principle experiments with no replication  are still performed routinely by several labs and typically analyzed by the simple method of identifying genes as putative degs if they show a ratio above  <dig> or below  <dig> .

in the work presented here, a significant increase in the accuracy of degs detection from a low number of replicate arrays will be obtained by taking advantage of the design itself of hdas. all of the affymetrix genechips contain k pairs of 25-mer dna oligonucleotides, called probe pairs, per probe-set. the probe-sets are designed to measure the expression of a single gene. each probe pair is composed of a match  and mismatch  probe, the former being the exact  <dig> nucleotides and the later containing one single mismatch at position  <dig>  mm probes were introduced to estimate the level of non-specific hybridization of the corresponding pm probe, but the recent trend has been to ignore the mm measures and only use pm measures seeking alternative methods to estimate non-specific hybridization. the measure of interest being the level of expression of the genes, it is necessary to transform a set of k probe pair's intensities into a single probe-set measurement. this step is referred to as the expression summary. it is still debated which expression summary method gives the best results but both mas <dig>   <cit>  and rma  <cit>  have been widely used. a recent summarization approach, farms  <cit> , have been found to compare favorably to more classical summarization approaches and will be included in the comparisons.

the publication of the choe et al. dataset in  <dig>  <cit>  has provided an objective benchmark to evaluate the accuracy of differential gene expression of various combinations of methods. in this work the authors have amplified  <dig>  crnas and prepared two samples, 'control'  and 'spike' , where  <dig>  crnas were spiked at various ratios of concentrations from  <dig>  to  <dig>  the two samples were then hybridized to three affymetrix genechips and scanned using standard affymetrix protocol. the work was done using the affymetrix drosophila array  where each gene is represented by a probe-set of  <dig> probe pairs of 25-mer dna oligonuceotides. there are a total of  <dig>  probe-sets present on the array. two key characteristics of this dataset will be important to remember when carrying out the statistical analysis: 1) a large proportion  of the probe-sets corresponds to crnas that were not present in the samples, leading to a bimodal intensity distribution; 2) only concentration ratios above  <dig> were introduced in the spiked sample, inducing a strong asymmetry in the distribution of log-ratios. using this validation dataset, it was shown  <cit>  that a preferred set of steps can be identified to optimize the detection of degs in hdas. the optimal pipeline identified corresponds to the use of mas <dig>  for the background correction, probe-level normalization, pm adjustment and the use of the medianpolish algorithm for expression summaries , followed by a loess normalization on the probe-set level data and finally use cyber-t  <cit>  to identify degs.

the most successful and widely used methods to identify degs are currently based on variations of a regularized t-statistic where the standard deviation term is weighted  <cit> , a constant added to it  <cit>  or both  <cit> . regularization terms are always, but in various ways, estimated from the dataset being analyzed to account for experiment-specific behavior of the data. recent work from barrera et al.  <cit>  attempts to identify degs directly from the probe-level data using a two-way anova approach. their results suggest that this class of approaches can outperform probe-set level methods or give comparable results with less replicates, but their validation relies on the use of the affymetrix latin square dataset which contains degs with concentration ratios of  <dig> and above on a small number of genes . all statistical methods tested  perform well on this dataset and it is often difficult to quantify slight improvements in accuracy.

in this paper, i propose a method that scores better accuracy than the preferred pipeline identified by choe et al.  <cit>  and will demonstrate that this method doesn't require the three technical replicates provided by the dataset to correctly identify 75% of the degs within 10% of false positives. the proposed method, called pl-lm , directly models the treatment effect versus a baseline control using the probe-level data, using a linear model. the treatment effect and average intensity fitted by the linear model are then modeled by a gaussian mixture model , which is used to separate the degs from non-degs. the outcome of the gmm is the probability of each probe-set to belong to the cluster of degs, thus the pl-lm method as a whole should be characterized as a feature selection algorithm. this second stage of the analysis borrows idea introduced by jia and xu  <cit> , where they used a gmm to cluster genes showing similar expression responses vs. a quantitative phenotype. the average intensity and treatment effect in pl-lm are equivalent to the βk <dig> and βk <dig> in  <cit> . there are a few fundamental differences between the two approaches: 1) in pl-lm, the optimization of the gmm is decoupled from the fitting of the linear model, 2) the linear model of pl-lm is based on probe-level data, and 3) the clustering step aims essentially at separating degs from non-degs.

RESULTS
data from the choe et al.  <cit>  dataset was log2-transformed, quantile-normalized  <cit>  and a linear model  was fit for each probe-set, estimating parameter t, the treatment effect in equation  <dig> using a standard least-square method  <cit> . figure  <dig> presents normalized probe-level data for three typical probe-sets: a) a probe-set that was spiked at four times the concentration of the control; b) a probe-set that wasn't spiked, and c) a probe-set corresponding to a crna that wasn't amplified. for each probe-set the value estimated for the treatment effect, t, is reported. a first observation that can be made is the fact that values obtained for t underestimate the ratio of concentration . this trend has been also reported by choe et al.  <cit>  for other methods on their dataset. it seems to be either a limitation introduced by the impossibility to accurately estimate non-specific signal for each probe or a real bias introduced by the experimental methods to prepare the dataset. another important observation is that crnas that were not spiked tend to have a negative value of t, suggesting that they are seen as decreasing in the spiked vs. the control sample. examination of the raw data presented in figure 1b confirms this trend since for each probe, all three spiked replicates fall below the intensities of the three control replicates. on the other hand, for probe-set corresponding to crnas that were not amplified the intensities seem to be similar between the spiked and control samples .

for each probe-set, the average log <dig> intensity is also determined by considering all probes and will be referred as i. to identify degs, a mixture model with three gaussian components  was fitted to the two-dimensional data points defined by t and i. the number of gaussian components to use in the mixture is dictate by the distribution of data points with respect to t and i as observed from a scatter plot. figure  <dig> shows the result of this model. upon multiple trials, starting from random assignations of data points to mixture components, the same classification was obtained . the three mixture components are represented in figure  <dig> as ellipses, visually translating the mixtures parameters. not shown in the figure, the mixing proportions of the three components are 17%, 74% and 9%, and can be respectively attributed to non-spiked probe-sets, not amplified crnas and degs. probe-sets having a conditional probability of belonging to the third component above  <dig>  are identified by larger dots and grayed out. this conditional probability will be used as the ordering statistic to compute roc curves and allows for an objective comparison with other methods to identify degs. observations made on the three typical probe-sets of figure  <dig> can be seen as widespread among their classes. there also seem to be a slight intensity-dependant bias over t that can be observed in all three components.

to assess the accuracy of the ordering induced by the pl-lm method, roc curves were computed for results obtained by using all six arrays from the dataset and for all combinations of two arrays . as a comparison, three other approaches were applied to the dataset: 1) the preferred pipeline identified by choe et al.  <cit>  using the mas <dig>  algorithm  <cit>  but using the medianpolish as the summarization step, loess normalization  <cit>  and cyber-t regularized t-statistic  <cit> ; 2) a simple approach combining the rma expression summary  <cit> , quantile normalization  <cit>  and absolute fold-change ordering; 3) the farms summarization method, with the ordering obtained from either the absolute fold-change or by applying the cyber-t approach. resulting roc curves are shown in figure  <dig>  in an attempt to quantify the overall accuracy of each method, the area under the curve  of the roc curves have been computed and reported in the legend. first, both the pl-lm  and cyber-t perform equally well up to the identification of around  <dig> true positives, after which the pl-lm significantly outperforms the cyber-t method whether using all arrays or a single replicate per condition. the fold-change method applied to quantile-normalized rma summaries, even when using three replicates per condition, performs on a much lower level than cyber-t, highlighting a clear necessity for a proper statistical analysis when reporting hda data, even in the absence of replication.

to assess if a higher level of analysis on the cyber-t regularized t-statistic could help distinguish between degs and non-degs, the distribution of this statistic vs. the average intensity i is presented in figure  <dig>  spiked probe-sets are shown as large and dark dots, not amplified crnas as large and light dots, and probe-sets that where not spiked as small black dots. an interesting feature of cyber-t is that it was able to completely remove the intensity-dependant bias on its statistic. but, in doing so, seems to blur the boundaries between the clusters identified from the pl-lm method. it is also important to notice that the clear clustering apparent in figure  <dig> is revealed only by the use of the treatment coefficient t and is mostly lost using probe-set-based statistics.

CONCLUSIONS
by adapting and combining previously proposed approaches , the pl-lm method was able to outperform by a significant margin the preferred method identified by choe et al.  <cit>  on their validation dataset: mas <dig>  background correction and pm adjustment, median polish expression summaries followed by loess normalization and cyber-t  <cit> . moreover, by using probe-level data to assess the variability of a differential expression measure, the pl-lm method maintains its level of performance on the validation dataset even when only one array per condition  is used, no matter which arrays are chosen. these results indicate that, within the experimental setup used for the validation study of choe et al., no replication was necessary to correctly identify 75% of the degs within 10% of false positives .

it is important to qualify the previous statement since the replicates that were performed in this study were technical replicates obtained by hybridizing the same pool of labeled crnas to three arrays: the pl-lm method doesn't require technical replicates to achieve high accuracy. in several contexts, it will still be recommended to perform biological replicates to account for possible variations due to limitations of the experimental protocol . but in general, if each biological sample provides a statistically sound sampling of the population to study, this variability should be negligible. despite being a controversial technique  <cit> , it is possible in practice to reduce this variability by increasing the amount of rna per sample, and can readily be achieved by pooling multiple samples together. the downside of this approach is that quantification of the biological variations not due to the factor of interest is lost, but this loss might be more than compensated by the reduction in array costs. in fact, to properly quantify the biological variation and to avoid relying on a priori assumptions to regularize it, typically requires a number of arrays that falls well above the budget of a large number of studies. in the study on pooling carried out by jolly et al.  <cit>  they compare an anova analysis performed on probe-set expression levels obtained from  <dig> vs.  <dig> samples with the analysis of pooled rna done based on the observed fold-change. they concluded that the pooled strategy should only be used "where the expected response or phenotype is robust and its variation in that response is minimal". it is debatable whether their conclusions stems from the fundamentally diverging approaches used to analyze the pooled vs. individuals datasets or if it truly reflects limitations set forth by the pooling process. the pl-lm method, by being amenable to both multiple and single replicate context, could be used to remove the method bias when comparing the pooled vs. non-pooled approach.

the pl-lm method still carries a few drawbacks. the most limiting aspect is the manual nature of the mixture modeling step: deciding on the number of components and which component is  modeling the degs is currently done by manual inspection of the optimized mixture parameters. in a different context, jia et al.  <cit>  have been using the bayesian information criterion  to determine the optimal number of components. the use of a graphical representation as shown in figure  <dig> greatly simplifies this step but, depending on the dataset analyzed the group separation might be less obvious, and mostly, the procedure relies on the knowledge of it operator. it is also a possibility that on a dataset with an extremely low proportion of degs , the mixture step will require a large number of components before dedicating one to the degs, complicating the visual assessment of the mixture model. in such circumstance, the mixture could be used to model non-degs and an outlier detection approach used to identify degs .

in essence, the pl-lm method implements in a single step the rma expression summary  <cit>  and the linear model proposed by kerr et al.  <cit> . by quantifying directly the difference induced by the treatment without the need to estimate the expression level of a probe-set for each array, the method achieves, on a wholly defined validation dataset, better accuracy without any technical replicate. the reduction in the number of estimated parameters and possibility to consider across treatments observations increases the robustness of the approach to quantify the treatment difference.

