BACKGROUND
the scale of information available for life sciences research is growing rapidly, bringing increasing challenges in hardware and software  <cit> . the value of these raw data can only be proved if adequately exploited by end-users. this reinforces the role of integration and interoperability at several service layers, allowing them to focus on the most relevant data for their research questions  <cit> .

biomedical data are complex: heterogeneously structured, originating from several different sources, represented through various standards, provided via distinct formats and with meaning changing over time  <cit> . from next generation sequencing hardware  <cit>  to the growing availability of biomedical sensors, tapping this on-going data stream is an unwieldy mission  <cit> . accessing, integrating and publishing data are essential activities for success, common to commercial and scientific research projects  <cit> . many life science researchers perform these tasks on a regular basis, whether through the manual collection and curation of data, or the use of specific software  <cit> .

in recent years, data integration and interoperability is focused on three interdependent domains: cloud-based strategies  <cit> , service-oriented architectures  <cit>  and semantic web technologies  <cit> .

cloud-based approaches are adequate for institutions that want to delegate the solution for computational requirements. processing high throughput sequencing data or executing intensive analysis algorithms involves a technical layer that is greatly enhanced by using grid- and cloud-based architectures  <cit> . this removes any technological hardware concern from the researchers' work. in addition, improved availability and ease-of-access, to and from cloud-based resources, further promotes the use of cloud-based strategies. however, for the majority of researchers, their problems are much smaller and focused, and, where the implementation of a full cloud-based stack is relevant, access to these resources is difficult and expensive.

workflow management tools represent a leap forward for service interoperability in bioinformatics. the ability to create comprehensive workflows eased researchers’ work  <cit> . nowadays, connecting multiple services and data sources is a recurring task. yet, tools such as yabi  <cit> , galaxy  <cit>  or taverna  <cit>  lack automation strategies, essential for real-time features.

the semantic web paradigm has been promoted as a perfect fit for the innate complexity of the life sciences. the complex biological data relationships are better expressed through semantic predicates than what is possible in relational models or tabular files  <cit> . although applications such as the semantic diseasecard  <cit>  or architectures like semantic automated discovery and integration   <cit>  already commoditize semantic web technologies, this paradigm is not a “one size fits all” solution.

in this work we introduce an open-source framework to streamline data integration and web service interoperability tasks. our goals are two-fold: enabling the automated real-time, reactive or event-driven analysis of data, and empowering the creation of state-of-the-art applications.

traditional integration approaches, in use by data warehouses, rely on batch, off-line extract-transform-load  processes. these are manually triggered on regular intervals of downtime, which can range from weeks to months or even years. however, in the life sciences domain, the demand for fresh data cannot be ignored. hence, we need to deploy new strategies that are dynamic  <cit> , reactive  <cit>  and event-driven  <cit> . thus, today’s platforms must act intelligently, i.e., in real-time and autonomously, to changes detected in integrated environments.

there is untapped potential in the real-time and event-driven integration of data. researchers want to access the most up-to-date datasets; thus, ensuring that data are synchronized across resources continues to be an on-going challenge. combining this need with the resources’ heterogeneity and distribution, and we have a major bottleneck for faster scientific progress.

our approach is based on the creation and deployment of intelligent agents. agents track data changes on remote data sources, identifying new events based on user-specified conditions. next, events trigger the processing of actions based on user-specified templates. agents monitor resources in csv, json, xml or sql formats, which cover the majority of data sources and services available. templates handle integration actions, interacting with files, databases, emails or urls. as a result, there are endless combinations for customisable integration tasks, connecting events detected by the agents with actions configured in templates. among others, the framework empowers live data integration and heterogeneous many-to-many interoperability. in addition to the mentioned integration scenario, this platform is suitable to several other research problems within and beyond the life sciences.

implementation
the framework’s current version includes two components: server and distributed clients, both available to use and download at https://bioinformatics.ua.pt/i2x/.

the server is the main platform component, powering the framework’s core features. the distributed client scripts enable deploying local agents, using an internal packaged library.

the source code is available at github , under the mit free software license.

architecture
figure  <dig> details the framework’s architecture, including its basic components, described next.fig.  <dig> framework architecture highlighting the different system layers. a external original resources are accessed for data extraction; b local or distributed agents poll original resources; c the internal data store uses a relational database  to store data and an object cache  for improved performance; d the application engine, is implemented in ruby, with the rails framework, and controls the entire application and its api; e the postman applies the data extracted by the agents to the templates and executes the final delivery; f the external destination resources receive the data from the system.



original resources refer to the data sources being monitored by the configured agents. in our initial version these endpoints can provide data in sql, csv/tsv, xml or json format.

agents are intelligent distributed engines tracking resources. deployed agents form a multiple agent system focused on extracting content for verification in format-specific algorithms. internally, each agent analyses resources detecting changes in comparison to previous states. agents are modelled to include the configuration properties required to setup automated real-time content change detection from heterogeneous resources. this includes miscellaneous agent features such as scheduling, endpoints, connection strings, data selectors or caching definitions, among others.

agents are executed in the main server or in remote locations using the framework’s distributed client scripts. this results in improved security as data exchanges between the original data sources and the server component are reduced, and all transactions are performed via https.

the agent’s monitoring scheduling is also flexible, as available schedules are not hard-coded. the schedule list is defined in the application settings and can change on distinct server instances. more importantly, whereas the basic modus operandi is reactive integration relying on scheduled polling, content changes can also be pushed directly to agents, enabling passive integration.

within the agents we have detectors. these are the algorithms that perform the actual resource tracking. detection algorithms look for changes in the output content from the origin resources. the detector connects to the data store to analyse, retrieve and compare event metadata obtained by the agents.

in the current implementation, detectors can identify changes in four distinct formats: csv/tsv files, sql databases , xml or json data. these formats cover the output of the majority of data sources and web services available. for instance, we can write complex queries that fully explore sql’s potential to extract data for detection. moreover, this open architecture makes it easy to build on these methods to expand the detection to extra formats.

agents can be configured with one or more seeds. seeds enable the dynamic population of values in the agents’ configuration. although agents are flexible enough to cover the collection of data from disparate sources, there are scenarios where we need to launch agents for a very large number of targets with a similar configuration . this is where seeds are used. seeds’ configuration is identical to agents’ as the framework uses similar selectors to obtain seed values that fill in the gaps in the associated agents’ configuration, enabling the dynamic creation of agents. a sample scenario where seeds can be helpful revolves around the extraction of data from xml content feeds. the outcomes of agents’ monitoring process are events. each content/state “change” detected by the agent creates a new event. this triggers the execution of actions through delivery templates, enabling a controlled flow of data within distinct resources. events are atomic and contain the metadata required for finalizing the integration task.

the internal data store combines a relational database with a high-performance cache to persist application data. the platform uses the relational database to store all internal data, from agents’ configurations to user details. in addition to these, it stores all the required content verification elements. in addition, the system also features a redis cache for faster change detection and event generation.

the fluxcapacitor is the main application controller, registering and proxying everything. with all components deployed independently, the fluxcapacitor connects all components, acting as as a flow manager and ensuring that all operations are performed smoothly, from event detection to the final delivery. moreover, fluxcapacitor also enables the framework’s api, empowering the various platform web services.

the postman, as implied by its naming, performs the delivery of each template. that is, it finalises the integration tasks, receiving event data and applying them to the delivery template for execution.

destination resources are the templates’ objects to which the postman will connect for the final delivery. delivery templates define the actions executed for new events. the current version contains templates for executing sql queries, sending emails, calling web services  or managing files . like agents, the templates set can be extended with additional plugins to connect custom services or delivery types.

to connect agents and templates we have integrations. that is, integrations define the data origin and destination. integrations match multiple agents with multiple templates, enabling a many-to-many data distribution.

at last, the log engine stores summary information for all actions and flows. each log entry contains the minimal set of information required to re-enact specific transactions or errors. this includes timestamps, origin/destination and the messages sent. for improved tracking and analysis, this backend uses the sentry platform.

agents, templates and integrations’ metadata are stored in the internal database. we devised a flexible and dynamic data model that allows the easy configuration and update of these properties via the platform’s web interface. further details regarding the internal framework architecture and model are available on the documentation at https://bioinformatics.ua.pt/i2x/docs.

system workflows
figure 2’s simplified sequence diagram features the framework’s extract-transform-load pipeline, from data source polling to resource delivery. the sequence steps are listed next.fig.  <dig> framework monitoring and integration sequence diagram. in addition to the listed steps, all actions are logged internally for auditing, error tracking and performance analysis. two alternative pipelines can be executed: a distributed agents generate a different sequence from step  <dig>  where fluxcapacitor mediates all interactions; b events data can be pushed directly into the platform, generating a new sequence starting at step  <dig> 

poll content : the agent connects to external data sources and loads content.

return content : the data source returns the requested content.

check content : the agent sends acquired data to detector for content change detection.

check changes : the detector imports the data into the internal data store and checks if there have been any changes since the last update .

return data : the data store returns a dataset with the unique new content.

return events : the detector generates a set of events and returns them to the agent.

push event : the agent pushes event data iteratively  to the fluxcapacitor

forward data : the fluxcapacitor sends event data to the postman for delivery.

load template postman-data store: the postman loads the configured templates from the data store.

return template data store-postman: the data store returns matching templates.

deliver postman-destination resource: the postman performs the final delivery, transforming the template with the event data, thus concluding the etl workflow.



there are two more pipelines enabled that imply minimal changes to the framework’s base pipeline: a) when dealing with remote agents, and b) when event data are pushed from external resources. the latter complements the original reactive polling-based approach, which relies on recurring data fetching from the original sources, with a passive push-based strategy, where the original sources must send the new data to an open endpoint in our platform.

for the first alternative , there are a couple additional steps on the content detection sequence. the fluxcapacitor mediates the content change detection, acting as a middleware layer between the remote agent and the detection engine. this means that remote agents only interact with the main application controller and api, the fluxcapacitor.

on the second scenario , when external providers push events directly, the content detection sub-sequence is ignored. this is where our platform achieves optimal performance. as it is not necessary to fetch data from the original sources, we can skip the change detection algorithm and start the event processing immediately. the internal pipeline starts on step  <dig> as data are pushed directly from the external resources to the api. in this case, the original data sources are responsible for sending new data to the platform’s api. the drawback of this approach is that it implies implementation changes in the original data sources.

agent distribution
this distributed architecture relies on the ability to deploy intelligent remote agents. agents can be executed in the server-side, on the same machine as the server component, or at the client-side, where there is direct access to resources.

this feature is relevant when we consider the amount of data available in legacy systems, such as relational databases or csv files, which are not available through public urls or services, and also secure private environments. in these scenarios, client agents are configured with the server details, and deployed on the resource location.

a script controls local agent execution, loading all required code through a standalone library. the etl workflow for distributed agents is similar to what happens in the main server. the major change regards the content change detection engine. whereas with server-side agents the cache verification occurs within the server codebase, with client-side agents a web service call to the server api performs the verification. likewise, with new events, the client agent initiates the delivery through another web service call.

dynamic content change detection
to ensure a reliable stream of changed data, this framework relies on a set of content change detection services. as we cannot pre-emptively identify what is new in the original data sources, these services perform basic detection and filtering actions  <cit> , specialised for each integrated data format. this enables the rapid identification of new events. the architecture adopts an atomic data storage approach for content change detection. that is, agents are configured to extract defined data elements from the original sources, and each of these is independently stored, without dependencies to other resources, datasets or agents. this verification process occurs in four steps.a new detector loads agent metadata according to the resource data format. for instance, the csv detector implementation is different from the sql database detector one, although both share the same interface.

the detector polls the resource for data, which returns the requested dataset. again, this process is detector-based, requiring format-specific algorithms.

the detector validates the retrieved data in an internal cache. the cache acts as the atomic data storage component, storing each data element uniquely.

if the retrieved data are not in the cache, i.e., data are fresh, a new event is created, triggering the data push to the main application controller for delivery and storing the new data in the cache. if the data are not new, the integration process stops.



by default, the platform resorts to a solution based on the internal relational database to store events metadata for verification. to improve detection performance, the framework can use a redis cache instead, making the content change detection process faster and more efficient.

the cache verification process can use two distinct data elements. on the one hand, users can configure the cache property for each agent specifying what variable the change detection algorithm will monitor. this should be set to track unique data properties, namely identifiers. on the other hand, if there are no data elements that can identify integrated data unequivocally, the framework creates and stores an md <dig> hash of the data elements’ content. as changing content results in a new hash, we can detect new events without compromising the system performance.

job processing
scalability issues in automated real-time integration systems are a cause for concern. for example, in modern data processing it is mandatory that information is processed as efficiently as possible. hence, several challenges surface at the integration and interoperability level. scalability, performance, processing time or computational requirements are some of the bottlenecks we face. to tackle them, we employ a queue-based approach to control the on-demand execution of monitoring jobs. since these are the more computationally intensive algorithms, each monitoring task is placed on a homogeneous queue, without priority ordering. during the predetermined scheduling intervals, the framework launches the respective agents.

in parallel with the application server, a queue tracking service is continuously running, dequeuing jobs according to the system’s resources load. for instance, if the system can only execute two tasks simultaneously, and there are four monitoring tasks on the queue, they will be processed two at the time, in order of arrival. tasks start with the agent monitoring and proceed until the final delivery. moreover, the job execution daemon is flexible enough to allow distributing the load through multiple processing cores. as such, for computationally intensive tasks we can fully explore multithreading capabilities to optimize the overall system performance.

in spite of being a rather simplistic scaling method, this strategy prevents system overloads without compromising the near real-time solution, i.e., introduced delays are not significant to the application workflow.

extracting, transforming and loading data
at a basic level, this proposal introduces an intelligent etl proxy. the framework simplifies the process of extracting, transforming and loading data from distributed sources to heterogeneous destinations. the data extraction for each resource is configured in the agent, through selectors. selectors are key-value pairs, mapping a unique variable name, the key, with an expression to extract data elements, the value. where keys are strings, values are specific to each data format. for example, with csv data, values are column numbers, but with xml data, values are xpath query strings.

delivery templates perform the transform and load process. their configuration can have several variables, named within the %{<variable name>} expression. the custom template engine identifies variables at runtime that match the selector keys configured in the agent . a variable in the template must have a corresponding selector in the agent. yet, variables can be used multiple times throughout the template.

additional features, such as data mapping in templates, can be achieved using a ruby code script  within the ${code} function. we do not impose any restrictions to possible mappings, as long as the ruby script is valid and outputs content suitable to the delivery format.

for instance, an integration task that extracts data from a csv file to a sql database has an agent with a set of selectors matching csv column numbers, the selector value, with template variable names, the selector key. during the transform and load process, the postman replaces sql template variables  with values obtained for each variable at each csv row – fig.  <dig> fig.  <dig> applying data transformations. data from original resources  can be easily translated and transformed  using the framework’s templates: a csv data are automatically inserted into a sql database; b data are extracted from a sql query into a csv file; c xml elements are extracted  and sent to a web service via post request.



besides transforming static data for variables, templates can call internal functions or execute custom transformation code. functions, named within ${<function name>}, provide quick access to programmatic operations. for example, ${datetime} outputs the time of delivery in the template.

as we cannot pre-emptively foresee all possible data mapping and transformation operations, we enable the execution of complex operations in ruby code. the ${code} function enables writing scripts that are evaluated during the delivery. this endows templates with a generic and flexible strategy for performing complex data deliveries. taking this approach to the limit, we can perform a delivery that is entirely based on the execution of custom ruby code. code blocks can be of any size, as long as they are valid and finish returning a value. this covers both simple transformations such as numerical calculations, and complex operations such as matrix-based translations or mappings. hence, we enable templates with conditional transformations, equations solving, strings manipulation, or calls to system functions, among many other operations. further details are available in the framework documentation, online at https://bioinformatics.ua.pt/i2x/docs.

RESULTS
this framework brings a new perspective to the scientific data integration landscape, summarised in three main features, discussed in detail next.automated real-time data integration is achieved through the deployment of intelligent agents, which can operate remotely, to monitor data sources.

improved data delivery to heterogeneous destinations using a template-based approach, allowing transmitting and transforming data.

advanced integration and interoperability, as we can use the framework to empower multiple service-oriented architectures, from publish/subscribe to cloud-based integration-as-a-service.



real-time content monitoring
although the real-time paradigm is seldom applied to the life sciences domain, there are relevant research opportunities, besides genomics, open to exploration. for example, in health care, real-time analysis  <cit>  can be used to improve patient data monitoring or, at an institutional level, to enhance the collection of statistical data  <cit> .

this framework ensures real-time reliable data transmission and up-to-datedness. real-time refers to the entire integration process. after the initial test, we heuristically decided that “every 5 minutes” represented the best trade-off between the overall application performance and researchers’ real-time demands to be the smallest update interval. nevertheless, the platform can be setup to monitor data sources in any given interval, from every second to every year.

local  or remote  agents perform resource-monitoring tasks. while server-side monitoring is enclosed within the server, remote monitoring brings three key benefits: distribution, improved load control and better security.

the ability to download, configure and execute monitoring tasks locally adds a unique distribution layer. at the architecture level, we can deploy and configure any number of remote agents, pushing data to the framework’s main server.

local agent execution moves the monitoring schedule responsibility to the agent owners. as a result, agents’ scheduling is more flexible. agents run as a standalone script with an associated configuration file. scheduled tasks or manual ad-hoc execution  automate the script execution.

at last, using client-side agents results in a more secure ecosystem. all communications with the platform are already secured through https. yet, with client-side agents, sensitive content, such as authentication credentials or private api tokens, are not stored in the server. all configurations are saved locally, in a private json file. moreover, user-based access tokens, unique  <dig> character strings, control the api data exchanges required for remote monitoring. users can add one or more tokens and revoke existing ones in the server’s web interface. this token-based strategy also ensures that client-side agents only access the user's integrations and templates.

template-based delivery
the use of template-based engines in software is common and the integration domain is not an exception. from meta-programming  <cit>  to service composition  <cit> , templates are used to simplify recurring tasks and streamline processes.

template-based integration strategies traditionally refer to the data extraction activity of the etl warehousing workflow  <cit> . however, as this proposal uses intelligent agents for this task, templates' use fulfils the transform and load requirements. as detailed in the methods section, the framework includes a comprehensive template mechanism, based on variables and functions, to generate integration data for delivery to many destinations. this comprehensive approach is simple, yet flexible and powerful.

in summary, the framework allows four types of deliveries for now. these are succinctly described next.sql queries to mysql or postures databases. as sql is a powerful data manipulation language, complex query delivery can be combined with the agents’ detection output to perform advanced transformation.

send emails. emails can be sent to multiple recipients . all email properties  are available for customisation with the framework’s templating engine.

file manipulation. we can define deliveries as writing files in the user platform workspace or in the user’s associated dropbox account. we can create new files, or append or delete existing files. by managing files in the users’ dropbox account, we ensure that they are always synchronised with external changes.

requests to urls. the framework allows contacting miscellaneous services via http get or post, enabling data exchanges in text, json or xml. this is the most powerful publishing method as it allows the interaction with most modern rest web services, which traditionally accept new data via post and make data available via get.



currently, building tools to call rest services or manage files brings distinct requirements and implies custom ad hoc implementations. however, our template flexibility provides an abstraction on top of these methods. whether we want to append lines to a csv file, send an email or post data to a rest service, users control the entire process in the server’s simplified interface.

advanced integration & interoperability
as previously mentioned, this proposal introduces an open source framework that can act as the foundation for distinct applications with distinct architectures. whether we are dealing with event-driven applications or publish/subscribe environments, this framework can be easily adapted to support these systems.

event-driven architecture
traditional service-oriented architectures follow a request-response interaction model  <cit> . although this basic operation principle sustains many systems, lack of support for responses to events is a major drawback  <cit> .

event-driven architectures adopt a message-based approach to decouple service providers from consumers  <cit> . in these service-oriented architectures, event detection is essential to get a reliable stream of changed data  <cit> . event-driven architectures are used for direct responses to various events and for coordination with business process integration in ubiquitous scenarios.

our framework enables this kind of reactive integration. intelligent agents are configured to detect events, with the server acting as a message broker and router. for a truly event-driven environment, the framework also supports receiving events via push mechanisms. this means that we can deploy applications that fully operate on an event-driven paradigm.

publish/subscribe
this framework is also an enabler of publish-subscribe software  <cit> . the basic principle behind this strategy revolves around a dynamic endpoint, the publisher, which transmits data to a dynamic set of subscribers  <cit> . publish/subscribe architectures decouple communicating clients and complement event-driven architectures with the introduction of notifications. these can be a specialised form of events and ensure that we can actively push the desired data to subscribers in the shortest possible time.

translating this basic principle to the framework’s model, we can see the agents as a controlled set of publishers , which can be configured, using integrations, to interact with specific templates, the subscribers. this framework encloses the necessary tools and api that will allow applications to harvest the full potential of this paradigm in a seamless way.

adopting the proposed work, data owners now have the tools to deliver custom notifications when new data are generated. this further advances the state of the art, namely on the life sciences  <cit> , becoming vital when we consider the amount of legacy systems used to store data and the availability  of interoperability tools to connect these systems.

likewise, we can perform asynchronous push-based communication, broadcasting event data to any number of assorted destinations.

from an interoperability perspective, agents can also be on the subscriber end of the architecture. events notification data can be pushed into to the system, triggering the integration in real-time.

integration-as-a-service
cloud-based integration-as-a-service is currently a major goal in service-oriented architectures  <cit> . our framework empowers that paradigm, moving one step closer towards interoperable science data. this architectural approach abstracts algorithms, features, data or even full products as services  <cit> , which should be available online via http/s.

the framework can operate on both ends of the integration and interoperability spectrum. besides being a service consumer, for data extraction and loading tasks, it also provides services for miscellaneous real-world problems.

as we embrace the integration-as-a-service notion, the relevance of frameworks to ease the process of exchanging and translating data amongst multiple service providers becomes clear. by combining the qualities of service-oriented, event-driven, publish/subscribe and cloud-based architectures, this framework endows users and developers with the required toolkit to build future-proof biomedical informatics software.

discussion
the proposed framework’s flexibility makes it suitable for miscellaneous integration use cases, connecting private or public data sources with existing services. more importantly, it allows researchers to create their own scenarios, with agents and templates suitable to their work.

the following discussion introduces a human variome data integration scenario and details some current limitations and future perspectives. this scenario can be tested at https://bioinformatics.ua.pt/i2x. this is a fully functional version of the platform, where everyone can register an account to create agents, templates and integrations.

automating variome data integration
our first application scenario is provided in the configuration samples list available on the platform’s web interface. in it, we tackle a prime challenge for life sciences researcher regards the integration of human variome data: collecting unique mutations associated with a specific locus. this feature is already available in several systems. wave  <cit>  or cafe variome  <cit>  centralise data from distributed locus-specific databases  and make them available through web interfaces. the leiden open-source variation database  provides a turnkey solution to launch new lsdbs, with web and database management interfaces  <cit> . these features make lovd the de facto standard for lsdbs, with more than  <dig> million unique variants stored throughout  <dig> distinct installations. lovd has an api enabled by default, which allows obtaining the full list of variants associated with a gene. these are returned in atom format, a feed specification built in xml. in addition to lovd, there are several other lsdbs using legacy formats, such as excel or csv files, or relational databases.

despite the quality of available systems, data integration is limited by various constraints. for instance, the adopted pipeline - extract and curate data, enrich datasets, deliver results - creates a time-based snapshot of available human variome data. however, researchers require access to constantly updated datasets. to accomplish this we can use two distinct strategies: 1) we repeatedly launch our integration pipeline, processing everything from scratch or 2) we create an ad hoc integration solution tailored to this particular scenario. the maintenance and development effort underlying any of these strategies delays real progress.

for simplicity purposes, we considered extracting data from the lovd platform only. when curators submit new variants to lovd, their data becomes available in the feed api. an agent is configured to monitor the feed for a single gene or, through the definition of a seed, for many genes based on a predefined list. after the initial data population process, events are detected when new variants are published. this starts a new integration task, which can deliver data directly to a database  or send them to a url-based service . figure  <dig> displays the platform prototype web interface showcasing the integration, agent and template configurations for this scenario.fig.  <dig> web interface for proposed platform prototype. this interface highlights the integration configuration for automating human variome integration. this integration features one agent  and one template . the former configures how to extract mutation data from lovd api and the latter specifies the configuration for storing extracted data in a relational database.



this scenario is already in place in the data aggregation for wave’s backend , which highlights three key benefits of the framework: autonomy, flexibility and data quality. first, the data warehousing process is triggered autonomously, without user intervention. next, agents can track any number of genes and deliver data for new variants to any number of heterogeneous destinations. at last, variome datasets in the centralised application are always up-to-date with the latest discovered variants. agent scheduling can monitor lsdbs frequently, resulting in improved database completeness.

furthermore, in an ideal scenario, lsdbs owners can deploy local agents to monitor their databases internally or, using push, send events  directly to the main server platform for integration.

limitations and future perspectives
although this open-source framework already supports advanced features for real-time content monitoring and data delivery, it has some limitations. integration scenarios where the data acquisition involves combining data from multiple data sources or the execution of multiple data extraction steps require a more comprehensive integration solution. likewise, there are several data compression and encryption methods that were not accounted for.

hence, we are continually improving our solution, namely with the inclusion of new detection formats and delivery methods.

data verification and rule processing are some of the main targets for future improvements. our goal is to make a system that is completely independent from any origin or destination resource. this means that the platform should not rely on any external resources to ensure that data are new, which implies storing processed events in an internal database. as the platform needs to know what is new to trigger the integration process, we need to maintain a cache of everything the platform has already processed. these metadata could be complemented with delivery verification information, where we would store if and when the data were actually received by the destination resources. although this would be another configuration burden for users, the overall integration algorithm will be improved with this feature.

we also plan to expand the framework’s data warehousing features by focusing on rule processing. the goal is to detach the etl transform task from the delivery task, enabling more complex data transformation that obeys custom heuristics. in a simple use case, we want to perform the delivery only for data above a given threshold. despite being able to perform these validations using ruby code variables, simplifying these processes with dedicated methods will improve the framework’s usability.

in the long haul, we can further enhance rule processing with the inclusion of semantics. latest developments on semantic web technologies and frameworks are responsible for an increased adoption within the life sciences field. as such, we plan to include support for linkeddata and sparql agents and delivery templates, and new inference and reasoning engines, allowing researchers to perform more complex operations with their data.

the mandatory configuration of scheduling properties for agents will also be the subject of future research work. we assume a constant flow of information in and out of the framework. in its current state, the framework is already tailored to scenarios where the data sources’ content change quite often. traditionally, they require intensive manual effort to ensure the up-to-datedness of integrated data. with our proposal’s automation features and regular data monitoring, live data integration is ensured independently from the dataset update interval. for instance, regularly updated datasets can be monitored every 5 minutes or, in opposition, datasets that are seldom updated can be monitored daily or weekly.

to further improve this, we plan to include algorithms for the automatic identification of the best schedules for each resource. for instance, when a monitored resource generates events on a daily basis, the system should automatically understand that it is not efficient to schedule the resource for monitoring every 5 minutes. this will enhance the handling or large volumes of data and further improve the framework’s performance.

there are numerous high-quality data integration platforms. nevertheless, it is common for existing solutions, such as pentagon  or talend open studio , among others, to have three major drawbacks: 1) they lack distribution, enforcing local access; 2) they operate in closed desktop environments; and 3) they lack automation features.

first, our approach enables the deployment of a distributed multi-agent architecture, where any number of agents can be running autonomously, processing data in any number of machines.

moreover, monitoring agents can be deployed locally, where some desktop application to be integrated is running, or online, where most public systems are available. with this, we overcome traditional local-based integration problems. in our approach, a central web-based server operates online to control agents’ distributed execution and data delivery.

at last, current solutions require manual integration triggers, from script execution to loading cumbersome platforms, whereas our solution is fully automated, ensuring real-time integration.

CONCLUSIONS
nowadays, access to vast amounts of life sciences data is a commodity. hence, modern integration strategies have arisen to better explore available information. in spite of their quality, existing models lack built-in mechanisms for handling change and time. that is, connecting data and services is a manual task, whose only results are time-limited snapshots.

with this research work we introduce a framework for enhanced integration and interoperability. this system’s innovative features - automation, real-time processing, flexibility and extensibility - convey true added value to biomedical data integration and interoperability.

automation brings a new approach to the field, stimulating reactive and event-driven integration tasks. furthermore, live data integration brought by automated and real-time intelligent agents ensure up-to-date information. the proposed framework is non-obtrusive, requires no changes in most original data sources, and can process data to and from heterogeneous data sources, making it an extremely flexible and adaptable framework.

this system is relevant for both researchers and developers: researchers can sign up to use the online platform and developers can download and modify the source code for local deployment. on the one hand, the platform’s streamlined configuration process puts a powerful integration and interoperability framework at the hands of less technical experienced users. we believe that the combination of the platform’s integration features with its easy-to-use web interface make the creation of integration tasks much more straightforward. although the concept of integrations, agents and templates may be difficult to understand, once the user grasps these notions, deploying complex integration procedures becomes trivial. this enables anyone to create business-level data and service integration tasks with reduced effort. on the other hand, developers can download and use the framework open-source code. as the framework supports dynamic real-time message translation, formatting and delivery to multiple resources, it can play a central role in service-oriented architectures, from publish/subscribe to event-driven up to cloud-based integration-as-a-service environments.

this research work delivers a system that bridges the gap between data and services through a new intelligent integration and interoperability layer, further enabling the creation of next generation bioinformatics applications.

availability and requirements
project name: i2x

project home page:https://bioinformatics.ua.pt/i2x/

operating system: platform independent

programming language: ruby, javascript

other requirements: ruby web server, relational database management system

license: mit

any restrictions to use by non-academics: not applicable

abbreviations
apiapplication programming interface

bccblind carbon copy

cccarbon copy

csvcomma-separated values

etlextract-transform-load

httphypertext transfer protocol

httpshttp secure

jsonjavascript object notation

lovdleiden open-source variation database

lsdblocus-specific databases

md5message digest algorithm 5

mitmassachusetts institute of technology

restrepresentational state transfer

sparqlsparql protocol and rdf query language

sqlstructured query language

tsvtab-separated values

uriuniform resource identifier

urluniform resource locator

xmlextensible markup language

competing interests

the authors declare that they have no competing interests.

authors’ contributions

pl is responsible for creating and developing the i2x framework. jlo revised the manuscript and supervised the work. all authors read and approved the final manuscript.

