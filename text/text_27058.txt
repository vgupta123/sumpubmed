BACKGROUND
alignment of vast number of nucleotide sequences is one of the central problems in current genomic studies. sequence alignment is used in many kinds of biological studies, including phylogenetic analysis, identification of conserved domains, prediction of protein structure and database searches.

alignment of multiple sequences remains a considerable challenge in terms of computation time required to complete an alignment of a reasonably large sequence dataset. even pairwise sequence alignment becomes hard when the sequences are very long. most alignment algorithms use some sorts of heuristics to reduce the computation time. every alignment method was designed with some expectations on the size of data to be analyzed and what sort of computer hardware is likely to be used. the progress in alignment techniques during the last  <dig> years would not be possible without the parallel constant improvement in computer technology. the alignment methods being developed these days work better than those of  <dig> ago partly just because they are designed to run on faster processors and use more memory.

the most simple and widely used such heuristic is the method of progressive alignment  <cit> , which was employed in a number of alignment programs such as multalign  <cit> , multal  <cit> , multalin  <cit> , clustal w  <cit> , and muscle  <cit> . under the idea of progressive alignment, first the sequences are compared pairwise, one to one. each pair of sequences is aligned with the dynamic programming algorithm, and the evolutionary distance between the sequences is estimated. based on those distances the phylogenetic tree is constructed and then used for building multiple alignment.

an alternative approach to solve the problem of multiple sequence alignment is the iterative procedure of refining the global alignment, for example in prrp program  <cit> . also a genetic algorithm for multiple sequence alignment was proposed  <cit> . mafft package  <cit>  contains both progressive and iterative alignment methods, depending on a novel approach of finding homologous regions with the help of fast fourier transform.

another direction focuses on increasing the performance of traditional dynamic programming based methods. this includes divide-and-conquer techniques, implemented in dca  <cit>  and oma  <cit> . there are also studies attempting to take advantage of the parallel computation, especially popular for progressive alignment techniques like clustal w  <cit> .

recently it became popular to locate regions of local conservations before attempting to construct the global alignment. this idea was used for pairwise alignment in mummer  <cit> , shuffle-lagan  <cit> , avid  <cit>  and gs-aligner  <cit> . for multiple sequences a similar technique was used in dialign  <cit> , chaos  <cit> , and mafft  <cit> .

while there is considerable progress in recent multiple alignment research, it still remains a challenging problem to align multiple genomic sequences. therefore we investigated an non-conventional heuristic approach to multiple sequence analysis, which we present in this work. a prototype of mishima was briefly introduced by kryukov and saitou  <cit> .

implementation
extraction of important information
it is believed that only a certain part of a genome is functional, and most of the genome content is never expressed or used. as a result some parts of the genome remain mostly unchanged during the evolution, while other parts quickly accumulate mutations through neutral evolution  <cit> . conserved parts of the genome can be very helpful in identifying homology. if we could quickly locate conserved areas in the genome, that information obtained could be used to assist the process of alignment.

this idea has already been implemented in other methods, in which homology was first detected by pairwise sequence comparison and the obtained information was then used to build the alignment . however for large genomic sequences the pairwise comparison step is very time consuming. also pairwise comparison does not reveal features shared by multiple sequences. on the other hand, it is often the case that several sequences share some regions of high similarity. we propose a new heuristic method for locating conserved patterns shared by multiple sequences that can be directly used in a sequence alignment procedure.

divide-and-conquer approach
rearrangements of dna fragments are known to happen very rarely in the evolutionary process, once sequences started to evolve independently in different species. majority of the mutations result from substitutions or insertion/deletion events. therefore in most cases it is a valid approach to use divide-and-conquer procedure for sequence alignment. its principle is that sequences can be split into parts that will be aligned independently and a complete alignment will finally be constructed by assembling the partial alignments together  <cit> .

the main difficulty in this method is to find the splitting points. the dca program uses a pairwise sequence comparison with a dynamic programming-based method to find the splitting positions  <cit> . however the cost of pairwise comparison increases rapidly with the increase of sequence length and/or number of sequences. below we propose a more rapid method to find the splitting positions.

once the sequences are divided into sufficiently short segments, these segments can be aligned by calling a conventional alignment program. dca uses msa alignment program  <cit>  to align the parts of sequences. in this study we use clustal w  <cit>  and mafft  <cit>  to align the sequence segments separately.

clustal w is not suitable for aligning long sequences or large number of sequences because of high computational requirements. mafft can be very fast with small datasets, but its computation time increases rapidly when aligning larger datasets, like complete bacterial genomes. application of these programs can be extended to much larger datasets by combining with the divide-and-conquer approach.

it should be noted that such divide-and-conquer approach is not able to detect duplications, inversions and genomic rearrangements. this approach is valid under the assumption that all sequences can be aligned in a linear fashion to each other to form a multiple alignment.

using k-tuple frequencies
the core idea of mishima is to analyze k-tuples found in the original sequences and to evaluate them based on their frequencies. when a particular nucleotide k-tuple is found exactly once in each sequence, we consider it as a likely homology signal. a k-tuple that has its close variants found once in each sequence is considered to be less likely, but still possible homology signal. analyzing all k-tuples up to certain length allows us to select those k-tuples that represent the most probable homology shared by multiple sequences. these k-tuples can then be used to anchor the sequences before employing divide and conquer method to complete the alignment.

the basic principle of mishima can be outlined as follows:

 <dig>  find potentially useful k-tuples based on the number of their occurrences in the sequence data.

 <dig>  analyze the potentially useful k-tuples, and select those that represent most probable local homology.

 <dig>  use the selected k-tuples as anchors, split the sequences into segments.

 <dig>  align the segments independently from each other.

 <dig>  join partial alignments to complete the final multiple sequence alignment.

dictionary of k-tuples
as a first step we count the number of occurrences of each short k-tuple in the original sequence dataset. we keep this information in a dictionary structure, indexed by a k-tuple sequence. the number of possible nucleotide k-tuples is 4k, so the maximum k we can use is limited by the amount of memory we can use for the dictionary. we store k-tuple frequencies as 32-bit numbers, so 4*4k bytes is required to store the frequencies of all nucleotide k-tuples. this allowed us to use k of  <dig> and  <dig> on 32-bit machines.

we found that knowing the number of occurrences of each k-tuple in the original sequence dataset is not enough to efficiently decide which k-tuples are more likely to represent the local homology. therefore we decided to also store the number of sequences exhibiting each k-tuple. additionally we store the index of the last sequence where the k-tuple was found, which allows us to collect all the frequencies using single read through the sequence dataset.

choosing the right lengths of a k-tuple
length k is a very important factor affecting the chance for a k-tuple to be useful for alignment purposes. a random sequence of length l is expected to contain l/4k occurrences of each k-tuple. let us take l as a*n, where n is the number of sequences and a is the average sequence length. the expected number of occurrences for one k-tuple is a*n/4k. since we are mostly interested in finding k-tuples with exactly n occurrences , an ideal case for this method would be where a*n/4k = n, which means a = 4k. thus, to have the best results with this method, the length of sequences to be analyzed should be comparable to, or shorter than 4k.

this is an intuitive estimation, to give some feeling of relation between the dataset size and k-tuple's k. of course the real biological sequences, especially those that we are trying to align, are often not completely random, which suggests that longer sequences can be aligned if they contain significant homology. therefore the capability of mishima to align the real sequences should be higher than this estimate. but non-conserved intergenic regions in a genomic sequences may well be considered as random sequence, and the total size of such regions in a sequence data is a limiting factor for mishima. this shows that the length k is the basic parameter affecting the performance of this method.

choosing the optimal length of a k-tuple is critical for the successful alignment. if k is too small, too many occurrences of each k-tuple will be found. although to some extent this depends on the length of the sequences, it is difficult to predict whether the particular k is too small or not. therefore we use all k-tuples with k from  <dig> to m in this method. for selecting m - the upper limit of k - we suggest to depend on the memory limitation. most of the steps of the mishima algorithm are performed in linear time, which makes it practical to use the largest m allowed by the amount of available memory.

we ended up using  <dig> bytes for each k-tuple in a dictionary. the total amount of memory required for dictionary is therefore 12* bytes. this allows us to use m of  <dig>  with the dictionary occupying less than  <dig> mb of ram.

finding seeds
our initial method of using identical k-tuples shared by all sequences has a low sensitivity, and we decided to use inexact matching instead. we do this by updating the dictionary to include the number of inexact copies of a k-tuple in the sequence dataset. our method allows up to one substitution difference between two k-tuples.

after a k-tuple dictionary is constructed and updated with inexact match counts, it becomes possible to select k-tuples which represent potential homology. potentially useful k-tuples are defined as those that have one inexact match in every sequence. such k-tuples, or seeds, are extracted and saved for further analysis.

the next step is to find the locations of seeds in sequence data. locations are extracted in a second read through the sequence data. it should be noted that this step also completes in linear time.

in cases where no seeds could be found, the external aligner is used to align the whole dataset.

seed compatibility
now we will introduce measure for compatibility of two seeds, based on their relative coordinates in sequence data. for now we will consider only seeds that are found not more than once in every sequence. if both of two seeds, a and b, can be found in a sequence, there are two possible orders: either the coordinate of a is smaller than that of b, or vice versa. we can mark these two cases as a->b and b->a.

if all sequences in a dataset exhibit the same order of seeds a and b, these two seeds are compatible. however, if sequences have these seeds some in a->b order while some other in b->a order, we can then define an incompatibility distance as number of sequences, where a and b order is different from the majority case. a distance defined in this way can be effectively used to evaluate the possibility that two seeds together represent a possible homology signal. this distance is used to construct a maximum non-conflicting set of seeds in the following procedure.

a constant number of t best seeds is selected . then the matrix of size txt is constructed. each cell of the matrix contains the incompatibility distance between two seeds. in the first step all of the selected t seeds are included and the matrix contains txt distance values. then the matrix is analyzed to find the sum of all numbers for every row. this sum corresponds to the total amount of incompatibility introduced by one seed. now the seed which introduces the largest amount of incompatibility is removed from the set. the matrix is recalculated and the procedure iterates until no incompatibility is left unresolved. finally only compatible seeds remain, which are used as alignment anchors. if all seeds are mutually incompatible with each other, there will be still one anchor left after removing the incompatibility. this anchor is used to divide the sequences into  <dig> parts, which are aligned separately.

the number of anchors found in all our example datasets is included in additional file  <dig>  in *.anchors files.

iterative procedure of dictionary analysis
after the dictionary step has finished and a set of non-conflicting seeds is selected, the algorithm will have a set of anchors, connecting the sequences. the sequences are then divided and regions between the anchors are aligned independently from each other. after all partial alignments are complete, they are concatenated to construct a final complete alignment.

alignment of the regions between the seeds is performed using an external aligner provided the sequences are short enough. otherwise mishima is used again  to divide the sequences into shorter parts. the depth of the recursive operation in mishima can be set with the command line option '-max-depth = x'. since mishima takes a constant time for each step of finding seeds and dividing the sequences, using a limited depth may improve the performance on some datasets.

the overall flow of mishima algorithm is shown in fig.  <dig> 

assessing the alignment speed and quality
the performance of mishima was evaluated on several example dataset, detailed in the results section. we used several popular aligners for comparison: clustal w, muscle, mafft and mlagan. the details, including version and parameters we used with these methods, are included in additional file  <dig> 

for assessing the alignment quality we used the sum-of-pairs alignment score  <cit> , based on the pairwise sequence identity. the score is computed as arithmetic mean of pairwise identity scores of all sequence pairs in the alignment. pairwise identity score is computed as following: sij = 100*mij/l, where mij is the number of alignment positions where sequences i and j have the same nucleotide , and l is the length of complete multiple alignment .

while a simulated dataset is necessary for a serious discussion of alignment quality, we believe that using the sum-of-pairs score is sufficient for evaluating mishima's performance, for the following reasons:  <dig>  our main focus is on comparing mishima+external aligner combination to external aligner alone.  <dig>  we are mainly interested in verifying whether mishima would not produce particularly big misalignments; the simplified score is sufficient for this purpose.  <dig>  mishima is targeting rather closely related sequences, which means that the correct alignments produced by almost any method will have about the same score, and misalignments will result in a noticeable score decrease.

the sum-of-pairs identity score was computed for all example alignments produced by mishima and other methods.

RESULTS
human mtdna genomes
complete human mitochondrial dna  genome sequences were used as an example of very closely related sequences. as of july  <dig>   <dig>  a total of  <dig>  complete human mtdna genome sequences are available in ddbj/embl/genbank international nucleotide sequence database. mishima was able to successfully align hundreds of such genomes. computation results can be seen in additional file  <dig> 

we measured the time mishima takes for aligning the whole and partial sequences and compared it with three popular aligners: clustal w, muscle and mafft . partial sequences  and complete ~ <dig>  kb sequences were used to see the effect of sequence length on computation time. a fixed length fragment was cut from the beginning of each sequence to produce partial sequences. note that mishima was using an external aligner  to align the regions between the anchors, and their running time is counted in the mishima total running time.  <dig>   <dig>   <dig> and  <dig> sequences were compared separately in order to examine effect of number of sequences on computation time. nucleotide diversity for these datasets is within the range of  <dig> - <dig> .

clustal w is the slowest among the compared aligners, even with  option, followed by muscle . fig.  <dig> shows that computation time of mishima and mafft increases much slower than that of clustal w and muscle. clustal w took 90- <dig> times longer time for complete mtdna sequence alignment than it took aligning  <dig> kb fragments for all four cases. mishima alignment time increased only  <dig> times between  <dig> kb and full length datasets. muscle was unable to complete the alignment of  <dig> and  <dig> complete sequences. we used two kinds of option sets for mafft;   and  . both option sets showed much faster results than either muscle or clustal w. mishima is as fast as mafft set at faster option  either when clustal w was used  or when mafft was used  as the external alignment program when  <dig>   <dig>  and  <dig> human mtdna sequences were compared.

when  <dig> human mtdna sequences are compared, mafft set at faster option is clearly faster than mishima. however, if we add  option to mishima , both aligners finish in similar time. because human mtdna sequences are quite similar with each other, good seeds are found in abundance in the first iteration of dictionary analysis, and average block length  is already short enough for clustal w usage. therefore, we recommend using this option when aligning closely related sequences, such as sequences from same species.

while the performance of mishima is similar to that of mafft, it should be noted that only high similarity regions are aligned by mishima, and the lower similarity parts are aligned with an external alignment program - either clustal w or mafft. this means that when clustal w is used as an external aligner, the alignment quality will be close to that of the clustal w complete alignment. fig.  <dig> shows sum-of-pairs alignment score  <cit>  of all alignments compared. mishima alignment tends to have higher score than alignments produced by clustal w or mafft alone.

we also tried mlagan  <cit>  on these datasets, but it did not work well, either producing corrupted alignment, or taking very long time . therefore mlagan results are not included in the figures.

mammalian mtdna genomes
mishima is powerful for aligning closely related nucleotide sequences, as shown in the above section. if more divergent sequences are compared, how does mishima perform? we thus retrieved complete mtdna genomes from various order of mammalian species. average nucleotide difference for these datasets are within the range of  <dig> - <dig> .

alignment times taken by mishima , clustal w, muscle, and mafft for aligning datasets of  <dig>   <dig> and  <dig> complete mtdna sequences are shown in table  <dig>  scaling of the alignment time depending on sequence length is shown in fig. 4: mishima+mafft combination is the fastest, followed by mafft alone, mishima+clustal w, muscle. clustal w is much slower. muscle could not complete the alignment of  <dig> complete mtdna genomes. mlagan results are not shown, since it was not able to produce the alignment even for  <dig> kb fragments.

if we compare computation times shown in table  <dig> and table  <dig>  those of mishima are certainly much slower for different mammalian species than for human individuals;  <dig> times and  <dig> times more for  <dig> and  <dig> sequences, respectively. this is because the number of good seeds for anchoring is reduced as sequence divergence increases. computation results can be found in additional file  <dig> 

mishima greatly accelerates the aligner that it is using: mishima+clustal w is much faster than clustal w alone, mishima+mafft is faster than mafft alone. is this achieved by sacrificing the alignment quality? fig.  <dig> shows the alignment score of all finished alignments. mishima+clustal w score is about the same with the score of clustal w alone, also mishima+mafft score is similar to score of mafft alone. thus, the improvement in computation time is not achieved at the expense of alignment quality.

complete genomes of four different strains of bacteria
we are now moving from mtdna genome sequences to much larger bacterial genomes. we first chose  <dig> strains of streptococcus pyogenes as example. each genome is about  <dig> mb, so the total dataset size is about  <dig> mb. clustal w could not produce the alignment even after  <dig> days, and computation was aborted. in good contrast, mishima+clustal w could successfully produce the multiple alignment of these four genomes in less than three hours. in the course of alignment the dataset was separated into  <dig> segments, divided by  <dig> seeds. the average length of one segment was about  <dig> kb, which is much easier to align than complete sequences.

we used various values for  option: unlimited depth ,  <dig>   <dig>  and  <dig>  computation times for each case are shown in table  <dig>  adding  reduced the computation time in the case of  <dig> human mtdna genome sequences , while activating this option for alignment of  <dig> genome sequences of streptococcus pyogenes strains resulted in  <dig> times slower computation. as the number of iterations increased, computation time was reduced, and the computation time for the case of  was slightly shorter than default unlimited depth . with  , some sequence blocks may be quite long, and external aligner may take very long time aligning such blocks. computation results can be seen in additional file  <dig> 

in any case, it is clear that mishima can be used to align multiple complete bacterial genomes, as long as they are closely related. the alignment of four strains of streptococcus pyogenes revealed several large insertions and deletions, some more than  <dig> kb long. if one is interested in quickly finding such large deletions, we recommend use of  option, so that the external aligner is not activated. in this way only seed anchoring is conducted, which is much faster  than producing full multiple alignment.

complete genomes of six different strains of bacteria
we then moved to a more complex dataset -  <dig> complete genomes of different strains of helicobacter pylori - each about  <dig>  mb. average nucleotide difference of the dataset is  <dig> . neither clustal w, mafft nor muscle were able to complete the alignment even after  <dig> days. therefore we decided to also run tests on fragments of these genomes; we chopped  <dig> kb from the beginning of every sequence, then  <dig> kb,  <dig> kb, and so on, until  <dig>  mb. the computation time of various methods can be seen in fig.  <dig>  and alignment score is shown in fig.  <dig>  muscle was unable to align even the  <dig> kb dataset, while clustal w took  <dig> hours. mafft managed it within  <dig> minutes - faster than mishima which took  <dig> minutes with default settings. however, as longer fragments were attempted, mafft was slowing down rapidly. with  <dig> kb fragments mishima was  <dig> times faster than mafft at its fastest setting . at the  <dig>  mb length, mishima was more than  <dig> times faster compared to mafft . this time we could also use mlagan, which performed very fast in this dataset. the alignment score of mishima is comparable to that of other methods .

mishima+mafft combination performed well on this dataset. adding options  and  allowed to save time and improve the alignment score with this dataset.

larger datasets
we tried mishima on larger bacterial datasets. we used fragmented sequences of  <dig> genomes of staphylococcus aureus. clustal w, muscle and mafft were taking too long time to align this dataset. mlagan was still able to produce the alignment. mishima was operating with the default options in this example, using mafft as an external aligner. mishima and mlagan alignment times are comparable to each other with partial datasets. when increasing the alignment length, mishima+mafft combination becomes faster .

interestingly, mishima alignment time starts to decrease when the sequences are getting closer to complete genome. the reason is that more complete sequences allow for the construction of a set of anchors that leaves smaller unaligned sequence block in the end. this effect is particularly apparent with  <dig> sequences .

the alignment score does not show any big problems with mishima alignment . so we can conclude that mishima does not sacrifice the alignment quality for fast computation.

at present, we were not able to compare the time and score of mishima+mafft with just mafft, however from previous examples we have seen that mishima+mafft alignment is similar in quality to mafft alignment. thus by using mishima we can produce mafft-quality alignment even for large datasets where we can't use mafft alone.

this example shows that mishima is effective on large datasets, including those that are very difficult to align with other programs.

random sequences
multiple alignments should be conducted on homologous sequences. however, there will always be a possibility of non-homologous sequences being erroneously included for multiple alignment. it is therefore ideal to detect such non-alignable sequences instantaneously and mishima can do just that. we demonstrate mishima's ability to detect unalignable sequences by using random sequences. two sets of random sequences  were generated using newly built random sequence generator . we used mishima with  options. as expected, mishima could not detect any seeds, which is a good indication that the data is very hard or impossible to align. quick examination of the result confirmed that the sequences could not be aligned. mishima took only  <dig> and  <dig> seconds to produce the results for  <dig> and  <dig> sequences, while clustal w took hours trying to align these random and non-homologous sequences .

we therefore recommend to first try using mishima with  options on new data. by this way you will be able to quickly know if there are any problems with the sequences. if mishima was able to find seeds and produce a rough alignment, then the sequences are fine and normal mishima alignment procedure can be used. in fact, computation times using this option set for human complete mtdna genomes are 00:00: <dig>  00:01: <dig>  and 00:14: <dig> for  <dig>   <dig>  and  <dig> sequences, respectively. when  option is also activated, computation times for  <dig> and  <dig> sequences are 00:00: <dig> and 00:02: <dig>  respectively. these are much shorter than mishima default for producing full multiple alignments.

CONCLUSIONS
we developed a heuristic method for multiple sequence alignment that can greatly speed up alignment of large datasets - those consisting of hundreds of sequences, as well as those with very long sequences, such as complete bacteria genomes.

mishima depends on either mafft , or clustal w  for aligning the areas between the "seeds" to produce the complete alignment. however even without other programs mishima can be used to quickly produce a rough alignment where only seeds are aligned. this can be very useful for quick evaluation of a new dataset before applying a more thorough alignment procedure.

given the rapidly increasing number of sequenced organisms, there is a growing interest in comparing closely related species. mishima is particularly helpful for such analysis due to its ability to quickly locate homology signals.

availability and requirements
project name: mishima

project home page: http://esper.lab.nig.ac.jp/study/mishima/

operating system: windows

programming language: c .

other requirements: for using with clustal w, clustal w version  <dig> . <dig> or later should be installed. for using with mafft, mafft version  <dig> b or later should be installed.

license: binary is free for any use.

any restrictions to use by non-academics: none.

authors' contributions
kk came up with the idea of heuristic homology search, developed the software, ran tests and drafted the manuscript. ns contributed to the design of the algorithm, did additional testing on large datasets, and improved the manuscript. both authors read and approved the final manuscript.

supplementary material
additional file 1
index of/study/mishima/supplementary-data

click here for file

 acknowledgements
this work was supported by grants from ministry of education, culture, sports, science and technology. kk was additionally supported by human genome network project led by drs. takashi gojobori and kazuho ikeo.
