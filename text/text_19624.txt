BACKGROUND
next-generation sequencing technologies have developed rapidly in recent years mainly in two regards. on the one hand, the throughput potential is tremendous. for example, a system consisting of a set of  <dig> hiseq x ultra-high-throughput instruments  can deliver over  <dig>  human genomes per year. on the other hand, the cost of whole genome sequencing is decreasing steadily. currently the cost of whole genome sequencing for an individual or patient stands at roughly $ <dig> . it seems likely that this trend will continue, and sequencing costs will continue to fall. this allows access to genome sequencing for a large percentage of the population. all of these changes have led to a sharp increase in the amount of sequence data and pose a new challenge to sequence analyzers.

usually, the data produced by a sequencing platform is not a single sequence with all dna information, but consists instead of a large number of short subsequences, called reads, with partial dna information. read alignment is then required to map reads to a reference genome and identify the coordinate of each individual read on the reference. the past few years have witnessed the appearance of diverse read alignment tools, which can be roughly divided into two categories: tools based on hash table and tools based on prefix/suffix trie  <cit> . a tool from the first category usually builds a hash table for the genome reference, which enables a shorter part of the read  to be mapped to the genome in constant time. then, the coordinate of the read is determined from the result of seed extension at each of its mapping locations. representatives of this category are blast  <cit> , soap  <cit>  and maq  <cit> . a tool of the second category, on the other hand, usually searches the prefix/suffix trie of the genome and then calculates the coordinate of each individual read with the help of burrows-wheeler transform  <cit> . representatives of this category include bwa  <cit> , bowtie  <cit>  and soap <dig>  <cit> .

read alignment is usually the first and most time consuming step of genome sequence analysis. although some existing tools are widely used with great success, their speeds cannot keep up with data increases. the very widely-used bwa definitely has many advantages and achieves relatively accurate results, but its speed is not as fast as could be desired. several new versions of bwa such as bwa-sw  <cit>  and bwa-mem  <cit>  are still limited by low speeds. for example, using bwa-mem to process a read alignment on a whole genome  usually takes ~70– <dig> cpu hours when running on a single core. this means that 16– <dig> cpu cores are required to ensure the speed of read alignment can keep up with the speed of data generation for a hiseq x <dig>  other tools have no prominent advantage in speed at the same accuracy level as bwa. thus, speeding up the read alignment is of vital importance and can significantly improve the efficiency of sequence analysis.

to accelerate the speed, researchers have tried many methods, such as seeking assistance from gpu  <cit> , cloud computing  <cit>  and distributed computing  <cit> . but these methods usually have a high requirement for hardware and often cannot be implemented due to resource limitation. naturally, improvement in algorithm is a better option, such as in subread  <cit> . subread is also based on hash table, which adopts a seed-and-vote strategy instead of the extending step of usual hash-table methods. like many read alignment tools, subread first builds an index for the reference genome, which enables a subread, a subsequence of a read, to identify its coordinate on the reference genome in constant time. then, it extracts multiple subreads from each individual read, gets the coordinate of the subread on the reference genome, and uses the coordinates of the subread to vote the final mapping location of the read. although the seed-and-vote strategy is time-saving, the mapping accuracy of subread is rather unsatisfactory in practice. besides, our tests on real data show that subread does not work well with large sequence data . it produces one third of the output only, while running for more than  <dig> cpu hours.

in this study, we propose a new read alignment tool, fast seed-and-vote aligner , which is ~5– <dig> times faster in running time than bwa-mem while keeping a similar mapping accuracy as bwa-mem. in practice, for a whole genome read alignment , fsva costs ~10– <dig> cpu hours on a single core and ~4– <dig> cpu hours on four cores. the respective time cost of bwa-mem in the same scenario is about ~ <dig> cpu hours and ~ <dig> cpu hours. this advantage of speed makes fsva a promising read alignment tool for big data.

fsva, borrowing the seed-and-vote strategy, builds a hash table for a reference genome and extracts seeds from the read to vote the coordinate. compared with subread, the main improvement of fsva lies in the longer seed, which allows improved running speed and accuracy. while a longer seed cannot be represented as an integer in many programming languages, we avoid this problem by expressing the seed as a large prime number, guaranteeing a seed can be represented as an integer, and the size of hash table is not too big. this specific method is introduced in detail in the methods section. experiments on simulated data and real data illustrate the great advantages of fsva on time saving and present the alignment accuracy of fsva as close to that of bwa-mem, which is shown in the results section.

methods
fsva, based on a seed-and-vote strategy, extracts seeds from a read and makes them vote the coordinate of the read. this method includes two steps: indexing and voting. the detailed methods are described in following sections. one point to note is that in the methods section, our algorithm is introduced based on 150 bp reads, which is the read length of hiseq x <dig>  in the actual situation, our tool fsva can automatically adjust its parameters to fit various read lengths.

building the index
building the index refers to the building of a hash table for a reference genome sequence. in our hash table, the key is a 32bit unsigned integer converted from a subsequence of the reference genome. the value is a vector of 32bit unsigned integer, representing the location of the reference which a seed can be exactly mapped to.

calculating keys
a dna sequence, which usually contains only  <dig> characters , can be converted to a quaternary number. thus an n-long dna sequence can be converted to an unsigned quaternary integer with n digits, or an unsigned binary integer with 2n digits, namely, a 2n bit binary integer. here we extract 31 bp subsequences from the beginning of a reference genome as keys, and the size of a sliding window of each pair of neighbor subsequences is set as 8 bp. to store a 31 bp subsequence, a 62bit unsigned binary integer is needed. for the sake of memory saving, we designate the 62bit unsigned integer modulo a large prime number m. herein m is no bigger than the maximum of a 32bit unsigned integer. thus the 62bit unsigned integer is converted to a 32bit one, which is the final key utilized in the hash table. if a 31 bp subsequence extracted from the reference genome consists of other characters , it is dropped without key calculation. the process of key calculation is shown in fig.  <dig> fig.  <dig> the process of key calculation. xi is a 62bit unsigned integer calculated by converting a 31 bp subsequence into a binary number. keyi is a  <dig> bit unsigned integer, calculated by xi modulo a large prime number m




building the hash table
the hash table has m pairs of key and value, with the key coming from the modulo operation. since the key is calculated from converting an unsigned 62bit integer into a  <dig> bit one by modulo m operation, different subsequences may be given the same key. therefore, a vector is utilized to store coordinates of the subsequences with the same key, and the value of the key is this vector. if no value exists for a key, we mark null. figure  <dig> shows an example of the hash table.fig.  <dig> an example of our hash table. vectorn represents the vector of coordinates of subsequences with the same key n. if no subsequences with the key m, the value of m is null




generating seeds and voting
we treat each one 31 bp subsequence extracted from a read as a seed. thus an n-long read can generate a total of n- <dig> different seeds. the process of key calculation of hash table building is utilized also to get the key of a seed. then, by querying the hash table with the key of a seed, the location of the seed can be located. the seed set with size n- <dig> of an n-long read can search out its corresponding coordinate set with at most n- <dig> vectors. coordinates recorded in these vectors vote the coordinate of the read, and the one with most votes is selected. herein, the vote counting is based on a block. a block is an interval of the genome reference with same length of the read. figure 3a shows the process of generating seeds and voting.fig.  <dig> 
a the process of generating seeds and voting. a, b, c, d and e are determined by the same method of key calculation and used to query the hash table. if the value is not null, we can get a vector which stores some coordinates and then votes on each one of all coordinates in the value vector. after voting from all seeds is completed, the block with the most votes is selected as the mapping coordinate of the read. b in case of more than  <dig> mismatches between a read and its mapping block , the mapping block is extended towards bi-directions with 36 bp, then smith-waterman algorithm is applied on the read and the extending block. in this figure, green, red, and blue represent a match, a mismatch, and the 36 bp upstream and downstream of the extending block, respectively




in practice, the final mapping block should have at least two votes; otherwise, we believe this read cannot be mapped to the reference genome. second, if more than one block is tied for most votes, we choose one randomly and set its mapping quality as  <dig>  third, if the alignment has more than two mismatches, we perform a smith-waterman dynamic programming between the read and the extending block. the extending block is the block with the most votes extending towards upstream and downstream with 36 bp . fourth, our experience from a range of experiments shows that 99% seeds will vote less than ~ <dig> coordinates on the default condition , and if a seed votes more than  <dig> coordinates, we believe this seed is unrepresentative. these unrepresentative seeds are dropped to avoid time-wasting. this threshold of how many coordinates at most a seed can vote is a tradeoff between time cost and alignment accuracy. in our tool, this threshold can be set by users for specific requirements.

in our experiment, we set the length of seed as 31 bp, and for hash table also 31 bp subsequences are extracted from the genome reference to calculate keys. if seed is shorter, many seeds will be generated from a read and much more coordinates could be selected as candidates waiting to be voted upon. consequently, the time cost is higher. on the other hand, if the seed length is larger than 32 bp, it cannot be converted to a 64bit integer and cannot be represented in most programming languages, which will introduce trouble on the programming side.

besides, in our algorithm, 31 bp and not 32 bp is selected as seed length is to avoid an unwanted situation where a perfect match block gets less votes than a block with mismatches. in general, we should ideally prefer the perfect match block. we give an example in fig.  <dig>  assuming the read length is 150 bp without loss of generality, if the seed length is 32 bp, a block with  <dig> mismatches, shown in fig. 4a, could get one more votes than a perfect match block shown in fig. 4b. the voting strategy selects the block with most votes, while in most situations, the perfect match block shown in fig. 4b is preferable to the block with mismatches shown in fig. 4a. generally, with a read length of  <dig> and a seed length of  <dig>  the read with the first seed starting at the first seven base pairs or at the eighth base pair will gain at most  <dig> and  <dig> votes, respectively.fig.  <dig> 
a a block having  <dig> mismatches. in this situation, the first seed of the read voting to the block starts at the first base pair. since the gap between two neighbor seeds is  <dig>  the seeds start at the 1st, 9th, 17th, 25th, 33rd, 41st, 49th, 57th, 65th, 73rd, 81st, 89th, 97th, 105th, 113rd base pair voting to the block, and totals  <dig> votes. b a block having  <dig> mismatches. in this situation, the first seed of the read voting to the block starts at the 8th base pair, and the 8th, 16th, 24th, 32nd, 40th, 48th, 56th, 64th, 72nd, 80th, 88th, 96th, 104th, 112nd base pair voting to the block, totaling  <dig> votes. here, green color stands for a match and red color for a mismatch




in this way if the seed length is set as 31 bp, the abnormal voting results caused by 32 bp seed length could be avoided. each case of exact match will be given  <dig> votes. if the read length is not 150 bp, fsva can automatically adjust the seed length to fit the read length by default. users can manually set seed length also in our tool configuration.

mapping quality
for each alignment, fsva calculates a mapping quality score by comparing the votes of the optimal and the suboptimal blocks. specifically, the mapping quality score is calculated as following: mapq=minoptimal−suboptimal× <dig>  


where optimal and suboptimal represent the number of votes of the block with the most votes and second most votes, respectively. obviously our mapping quality score is a multiple of  <dig> and no more than  <dig> 

RESULTS
to study the performance of fsva, we compared fsva with subread, bwa and bowtie <dig>  <cit> . bwa is a widely-used read alignment tool based on prefix trie and performs well in practice. bwa has three modes: aln/samse/sampe, bwasw and mem. here we chose mem, because mem is the best choice for no time cost concern and alignment accuracy for reads with a length more than 100 bp  <cit> . bowtie <dig>  a tool from the hash table category, locates a seed using the hash table and implements a single-instruction-multiple-data-accelerated dynamic program to extend the seed. both bwa and bowtie <dig> are very popular in read alignment. subread is the tool closest to fsva in its methodology.

in our experiments, when running bwa-mem, subread and bowtie <dig>  all options are set as default. although fsva can run on multiple threads, to simplify the comparison of time cost, we ran the test only on single thread for both the simulated data and the real data.

evaluation on simulated data
simulated dataset
our simulated data is produced by wgsim, a tool provided by samtools  <cit> . with the help of wgsim, we can get a set of reads from the reference genome sequence. as the reads are fetched from the reference, we know the exact coordinate of each individual read. thus, we can compare the predicted location by each alignment tool and the real location to evaluate their accuracy.

here we use wgsim to fetch  <dig> million simulated reads from the whole genome sequence hs37d <dig>  some arguments in wgsim are set to simulate the properties of reads. to simulate the real situation, we allowed the base error rate be  <dig> % and the mutation rate be  <dig> %, in which the rate of snp mutations is  <dig> %, and the rate of indel mutation is  <dig> %. to study the effect of read length, we generate 125 bp and 150 bp reads respectively in the simulation test.

results on simulated data
as the read is taken from the reference, we know its exact coordinate. if the distance between the real read and the predicted one from a tool is no more than 30 bp, we treat it as a correct alignment.

first, to evaluate the influence of seed length on the final alignment results in fsva, we did a test on 150 bp reads and 125 bp reads using seeds with different length, and table  <dig> shows the result. the comparisons are based on three aspects, time cost, confident mapping percent , and error rate. obviously, with the seed length increase, the cost of time also increases, and the performance of fsva is first improved then reduced. to guarantee the speed of fsva, at the same time taking the situation described by fig.  <dig> into consideration, we decided to use 31 bp seed and 30 bp seed respectively when processing 150 bp reads and 125 bp reads, and the test shows fsva has the best performance using these parameters. here, 32 bp is the max value of seed length in our program, and users should avoid setting a seed length bigger than  <dig> table  <dig> evaluation using seeds with different length

176
 <dig> 
 <dig> 
152
 <dig> 
 <dig> 
rl represents read length, sl represents seed length. all the experiments run on a single core of intel xeon cpu e5- <dig>  <dig> @  <dig> ghz.the bold texts represent t﻿he best performance﻿ on different read length




the overall performances of bwa-mem, subread, bowtie <dig> and fsva on simulated data are shown in table  <dig>  these tests are implemented on 125 bp and 150 bp reads respectively, where the number following the tool name indicates the read length. in regards to time cost, obviously fsva holds great advantage. the time cost of fsva on either 125 bp or 150 bp reads and either single-end reads or pair-end reads is much lower than other tools. concretely, fsva runs 3– <dig> times faster than bwa-mem, 1– <dig> times faster than subread and 4– <dig> times faster than bowtie <dig>  in time cost considerations, indexing time is not included, because all four tools need to index, which thus is not a main factor for whole genome data processing. compared with bwa-mem, fsva performs a little worse on confident mapping percent and error rate. the difference is not high, 1–2% in confident mapping percent and about  <dig> % in error rate. excluding time cost, on single-end data, performance of fsva and bowtie <dig> are very close, while on pair-end data, the error rate of bowtie <dig> is a little higher,  <dig>  vs  <dig>  and  <dig>  vs  <dig> . subread is closest to fsva in methodology, however the performance of subread in our test is comprehensively behind fsva, especially the error rate of subread, which is too high to be satisfactory.table  <dig> evaluation on simulated data

except in the ‘tool’ column, the left three columns starting with ‘s’ represent the performance on single-end data, and the right three columns starting with ‘p’ represent the performance on pair-end data. ‘stime’, ‘sconf’, ‘serr’ refer to time cost, confident mapping percent and error rate correspondingly for single-end data. in the first element of each row, such as ‘bwa-mem-125’, the number  <dig> following the tool name bwa-mem represents read length for this test. all the experiments were run on a single core intel xeon cpu e5- <dig>  <dig> @  <dig> ghz




consistent with intuitions, longer reads lead to better confident mapping percent, error rate and higher time cost for all tools. for fsva, the time increase caused by long reads could almost be ignored, which means fsva will be more competitive with the trend of reads becoming longer and longer.

figure  <dig> shows the relationship between unmapped percent and error mapped percent on pair-end data for the 125 bp reads ) and 150 bp reads ). obviously bwa-mem has the best performance, with both error mapped percent and unmapped percent being very low and varying in a small range. our fsva performed a little worse than bwa-mem and much better than subread and bowtie <dig>  when the mapping quality threshold is low, in the range of 1– <dig>  fsva has a relatively low unmapped percent and a high error mapped percent. with the quality threshold increasing, fsva’s unmapped percent rises while error mapped percent declines. noticeably, there is no difference in the error rate between bwa-mem and fsva when the mapping quality threshold is higher than  <dig>  this means we can confidently set the threshold as  <dig> in practice. under this condition, fsva has the almost same accuracy as bwa-mem, except for a slightly lower confident mapping percent. subread and bowtie <dig> show a similar trend as fsva, but have worse performance than fsva both on unmapped percent and error mapped percent. besides, comparing the results on 125 bp vs 150 bp for each tool, we find the performance of all four tools are improved with the increase of read length increases, especially for fsva. fsva is more applicable for long reads since longer reads means more seeds, and consequently less uncertainty on voting. due to this feature, the performance of fsva is improved further when the read length increases.fig.  <dig> the variety between unmapped percent and error rate on the 125 bp reads  and the 150 bp reads . these two figures show the unmapped percent and error rate at each mapping quality level from  <dig> to  <dig>  the two figures show a similar trend in that the error mapped percentage declines with the rise of unmapped percentage. this is because with the rise of mapping quality threshold, the number of alignments with a mapping quality below the threshold increases, and the alignment with a high mapping quality is less likely to be an error mapping




although fsva is not superior to bwa-mem in terms of mapping accuracy, its advantage of time saving is extremely significant. in some cases, read data needs to be processed in a short time and the requirement of accuracy is not very stringent, and for this fsva is undoubtedly the best choice. in next section, tests on real data proves that the difference of accuracy between fsva and bwa-mem does not have much influence on downstream variant calling.

as for storage memory, bwa-mem, fsva, subread and bowtie <dig> need  <dig> ,  <dig> ,  <dig>  and  <dig> gb respectively, on both 125 bp and 150 bp reads. this level of memory cost can be tolerated by a modern personal computer, let alone a server. thus, memory cost is not a major concern of read mapping tools.

evaluation on real data
real dataset
five real whole genome sequenced datasets from illumina hiseq x <dig> were used to evaluate these four tools. all the reads were 150 bp and the number of reads varied from ~ <dig> to ~ <dig> million. the library sizes of these five datasets are shown in table  <dig> table  <dig> library size of the five datasets




results on real data
table  <dig> presents the time cost of bwa-mem, subread, fsva and bowtie <dig> on five real data sets shown in table  <dig>  as in the simulation tests, fsva is the most time-saving method and this element of fsva is much more significant  when compared with bwa-mem. for a genome sequences dataset with a library size of ~300 gb, fsva requires less than 1 day while bwa-mem requires almost 6 days. the time cost of fsva is also much less than that of bowtie <dig>  almost  <dig> times less. on the real dataset, subread cost more time than bwa-mem and bowtie <dig>  let alone fsva. and in the case of big sequencing data , after over  <dig>  min  subread had only processed one third of the input data. therefore, it was dropped by us for big data.table  <dig> time cost on real data

time cost  of bwa-mem, subread, fsva and bowtie <dig> on real data. these tools all ran on a single core intel xeon cpu e5- <dig>  <dig> @  <dig> ghz




to study the influence of alignment results on downstream variant calling, we rely on samtools. samtools is a suite of utilities for interacting with high-throughput sequencing data. one of its utilities is taking output generated by short read aligners like fsva and bwa-mem, and calling variants. for each of these four tools, fsva, bwa-mem, bowtie <dig> and subread, we consistently utilized samtools as variant caller. then, with some statistical factors of the called variants starting from each aligner, we compared the performance of the four tools. these statistical factors included the number of variants and ti/tv ratio. for variants, ti/tv is a ratio of the number of transition to transversion substitutions. recent human studies particularly from the  <dig> genomes project have been showing that for whole human genome, this ratio should be around 2- <dig> . since subread did not complete output in the big sequencing datasets , the following analysis is based on dataset <dig> of table  <dig> 

in fig.  <dig>  relationship between ti/tv ratio and called variant quality is shown. in general, higher variant quality means less variants and bigger ti/tv ratio. the curves of ti/tv to variant quality of fsva and bwa-mem are closest, and are in the middle of the curves of bowtie <dig> and subread. in the very low quality region, the ti/tv ratio of fsva is still above  <dig>  better than those of other three tools.fig.  <dig> the ti/tv ratio of the variants called by samtools using the result of bwa-mem, subread, bowtie <dig> or fsva. the x-axis stands for the quality of the variant, and the y-axis for the ratio of ti/tv




figure  <dig> shows the number of called variants by samtools given the alignment results from bwa-mem, subread, bowtie <dig> and fsva, respectively. if the variant quality threshold is set as  <dig>  the number of called variants is in the range of  <dig> – <dig> million, and it decreases to around  <dig> million if the quality threshold is above  <dig>  overall, for variant calling, bwa-mem is the tool with most sensitivity, and consequently false positives of bwa-mem may be more frequent than fsva and bowties with high probability. to further illustrate the confidence of called variants of each tool, we present a venn diagram of the number of variants with high quality  in fig.  <dig>  for fsva, the vast majority of the variants can also be called from the alignment output of at least one of the other three alignment tools. specifically,  <dig> % is identified from the results of all the four tools and only  <dig> %  cannot be called via any one of other three tools. for bwa-mem, the corresponding two numbers are  <dig>  and  <dig> %. for bowtie <dig>  these numbers are  <dig>  and  <dig> % correspondingly. the total number of called variants based on subread is very close to bwa-mem, while the number of called variants only via subread is highest, and at  <dig>  it is much higher than that of bwa-mem . only  <dig> variants are called only via fsva. this may demonstrate the specificity of called variants based on fsva is best when compared with the other three tools. the difference between fsva and each one of the other three tools is studied even further from the point of frequency of called variants in cohort, shown in fig.  <dig>  variant frequency is extracted from cohort studies including  <dig> genome project, exac and charge. we can infer that over one third of the variants called via fsva but not called via bwa-mem or subread have a frequency higher than 10%, and almost two third of the variants called via fsva but not called via bowtie <dig> have a frequency higher than 10%.fig.  <dig> number of variants called by samtools using the result of bwa-mem, subread, bowtie <dig> and fsva separately. the x-axis stands for the quality of the variant, and the y-axis for the number of variants called by bwa-mem, subread, bowtie <dig> and fsva


fig.  <dig> a venn diagram of the number of variants with high quality


fig.  <dig> frequency distribution of the differential variants between fsva and other tools. the red line, blue line and green line represent variants called via fsva but not called via bwa-mem, subread and bowtie <dig>  respectively




according to the results of the real data, it is reasonable to say that the read alignment performance of fsva can be used to get a high-quality variant set. the tests on real data supplement evidence from the simulation test, and show the higher performance of fsva. considering both the simulation test and real data test, we believe fsva is a very competitive read alignment tool.

discussion
fsva utilizes the seed-and-vote strategy. like most read alignment methods, it builds a hash table for a reference genome first. then it extracts seeds from each read and searches the hash table to find the location of the seed in the reference genome. the coordinate of a read is voted by the seeds of the read.

the most significant advantage of fsva is its time saving potential. for a whole set of human genome sequencing data, the time cost of fsva is one sixth or one seventh of bwa-mem or bowtie <dig>  for example, for a sequencing library with 200g size, time cost of fsva is  <dig>  h while bwa-mem costs 74 h, and bowtie <dig> requires  <dig>  h on a single cpu core. this impressive feature makes fsva very competitive in short read alignment with large size, especially for cohort study.

the accuracy of fsva is illustrated here both on simulation data and real sequencing data. experiments on simulation data show that the alignment accuracy of fsva is almost good as bwa-mem, especially when the mapping quality is selected as over  <dig>  the difference on error rates between bwa-mem and fsva is very small, which can basically be ignored. on real sequencing data, since we do not know the correct coordinate of short reads, and usually the main focus of a pipeline for whole genome sequencing data analysis lies on variant calling, read alignment is just the first step. we explored the influence of four mapping tools, fsva, bwa-mem, bowtie <dig> and subread, on variant calling. in most cases, variants called based on the results of bwa-mem are highest,  <dig> – <dig>  million more than that of fsva and bowtie <dig>  for variant calling, bwa-mem may be the most sensitive, while fsva appears to have the best specificity. about  <dig> % of variants called base on fsva also could be found based on other short read alignment tools, and  <dig> % of variants called based on bwa-mem could be identified based on fsva. for a cohort study, where the data involved is almost a tsunami and the accuracy for an individual is not critical, fsva is a good choice.

fsva is not suitable for very short reads. as fsva uses the seed-and-vote strategy, if the read is too short, the extracted seeds should be shortened, otherwise the number of seeds is not enough to vote. but very short seeds are unrepresentative and will introduce too much noise. we suggest fsva being applied to any library in which read length is over 100 bp. fortunately, the trend in biotechnology development is towards reads becoming longer and longer.

fsva is sensitive to snp and error base. for fsva, the location of an snp or an error base will affect the voted output. it is clear enough that an snp or a base error appearing in the head or tail of a read exists in less seeds than one appearing in the middle of the read. this means an snp or a base error in the middle of a read will introduce more wrong-voting than if it were at the end of a read. if an indel exists in the middle of a read, the wrong-voting is again worsened. for whole genome sequencing or whole exon sequencing, this problem is alleviated since the reads are randomly cut and an snp or an indel will be in the middle of some reads and will be at the end of other reads. for amplicon sequencing, since there are a lot of replicate reads, fsva should be considered more before use.

CONCLUSIONS
in this paper, we proposed a new short read alignment algorithm, named fsva. fsva adopts the seed-and-vote strategy, achieving a significant improvement on speed over existing methods. in some cases, reads have to be aligned in a short time and requirements of accuracy are not very stringent. in these incidences, fsva would be a good choice.

abbreviations
bwaburrows-wheeler alignment tool

fsvafast seed-and-vote aligner

