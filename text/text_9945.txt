BACKGROUND
the diversity of the chains of immunoglobulins  or antibodies and t cell receptors  depends on several mechanisms  <cit> : first, combinatorial diversity, which is a consequence of the number of variable , diversity  and joining  genes in the ig and tr loci  <cit> , second, exonuclease trimming of v, d and j nucleotides and third, addition at random of nucleotides at the v-j and v-d-j junction .

these processes together create a huge diversity in v-j and v-d-j junctions as exemplified by the rearranged ig and tr sequences from imgt/ligm-db  <cit> . in addition, rearranged v-j and v-d-j genes from ig  are specifically submitted to the mechanism of somatic hypermutations  <cit>  . the number of different antigen receptors  per individual is estimated to be  <dig> ×  <dig> in humans and the only limiting factor seems to be the number of b cells  and t cells  which is genetically programmed in a given species.

trimming by exonuclease occurs at the ends of the 3'v-region and 5'j-region  <cit>   and at both ends of the d-region, present in the ig heavy , tr beta  and tr delta  loci  <cit> . little is known about the mechanisms that regulate trimming of v, d and j genes during v-j and v-d-j rearrangement. given the importance of trimming in the creation of the vast diversity of v-j and v-d-j junctions, it is of great interest to better understand this process.

based on the imgt-ontology axioms and concepts of classification   <cit> , description   <cit>  and numerotation   <cit> , on-line tools have been developed by imgt®, the international immunogenetics information system®,  <cit> , for the standardized analysis of immunogenetics data.

among them, imgt/v-quest is the highly customized and integrated imgt system for the standardized analysis of rearranged ig and tr sequences  <cit> . imgt/v-quest identifies the v, d and j genes in rearranged v-j and v-d-j sequences. imgt/v-quest integrates imgt/junctionanalysis  <cit>   to provide a detailed analysis of the observed v-j and v-d-j junctions. as bioinformatics tools become higher-throughput , data representing variables such as number of trimmed nucleotides and n-region length  can be obtained  <cit> . however, these numbers represent what is observed in the final "output" but do not necessarily represent the extent of the "true" trimming or nucleotide addition processes. indeed, randomly trimmed nucleotides can be replaced by identical randomly added n region nucleotides. as a consequence, the number of trimmed v or j nucleotides  will sometimes be underestimated.

there is therefore a need to quantify this bias if we want to investigate the underlying processes. the goal of the present article is to explore this possibility using tra and trg trimming processes, where only v and j genes are involved  <cit> .

our strategy is the following: given an imgt/v-quest+jcta standardized output, we aim to calculate the probabilities of all possible trimming events that are consistent with this output. then, using many such outputs, we aim to probabilistically transform the set of tool "output" data into a representation of the "true trimming process" . this probabilistic framework appears naturally by first taking the "output" dataset and simply calculating the empirical probability that the tool "output" shows that  <dig> , <dig> .. nucleotides were trimmed. then, understanding how the tool works, we aim to "correct" these empirical probabilities with respect to the tool's biases. a comprehensive introduction to probability distributions  can be found in  <cit>  and a simple introduction to bernoulli and poisson distributions is included in supplementary data .

a first-order model is presented in results, along with statistical tests on the transformed probability distributions. a proof of the first-order model and a proposed second-order model  can be found in supplementary data .

RESULTS
a first-order model
figures  <dig> and  <dig> show histograms of the number of trimmed trav, traj, trgv and trgj nucleotides obtained from  <dig> trav-traj and  <dig> trgv-trgj junction sequences analysed by imgt/v-quest+jcta and whose results were agreed upon by experts.

as potentially more nucleotides are trimmed in the "true process" than appear to have been trimmed according to the tool "output," we would like to transform the "output" data into "true process" data.

a factor also to take into consideration are the quantities of data at zero , which do not match the relatively smooth form of the tool "output" data distributions . this may be evidence of a two-step process: either the trimming process is activated, or not. if activated, it follows some as yet unknown law. if not, no trimming occurs. obviously, if the unknown law also takes the value zero, the fraction of data that takes the value zero would then have two sources . thankfully, as will be shown under the following first-order model, probabilistically transforming the "output" distribution towards the "true process" distribution  does not cause further complications. indeed, the transformed masses  above zero do not depend on the original fraction at zero. this means that performing maximum likelihood estimation of the parameters of a two-step process is well-defined on the transformed data.

recovering an estimation of the true process probability distribution
here we introduce a mathematical result that allows us to recover an estimation of the true process probability distribution of the number of trimmed v nucleotides. this result is almost  valid for the true process probability distribution of the number of trimmed j nucleotides. the potential problem is that imgt/v-quest+jcta selects the j gene after the v gene , thus there is a non-zero chance that 5'j-region nucleotides will accidentally be included in the v gene prediction when there has been no n region nucleotide addition. after reanalyzing the data, we found that in the trav-traj dataset, this happened at most  <dig> times and thus was rare enough to be ignored. however, for the trgv-trgj data, this potentially happened quite often, so estimated probability distribution results for the trgj trimming process must be used with caution.

let ℙ {b = k} mean 'the probability that k 3'v-region nucleotides are trimmed under the  true trimming process distribution fb'. we want to estimate this for k ≥  <dig>  let ℙ {f = i} mean 'the probability that i nucleotides appear k to have been trimmed.' that is, the random variable f represents the 3'v-region trimming distribution of the tool "output." we do not know the distribution ff of f exactly, but through our datasets we have an empirical estimate of it.

the goal is to use this empirical estimate of ff to estimate fb. to begin, theorem  <dig>  shows that under some simple hypotheses , there is an explicit link between the law of the observed 3'v-region tool "output" trimming distribution and the "true"  process distribution. indeed, for any k ≥  <dig> we find:

 ℙ{b=k}=43ℙ{f=k}−13ℙ{f=k+1}, 

and for k =  <dig> we find:

 ℙ{b=0}=ℙ{f=0}−13ℙ{f=1}. 

we call this the  rule. supposing the first-order hypotheses are correct, we would have for example that the bias-corrected probability that  <dig> v nucleotides were trimmed is equal to  the probability the tool "output" gives  <dig> trimmed nucleotides minus  the probability it gives  <dig> trimmed nucleotides. we see indeed that under these hypotheses, transformed fractions of data at each data value above zero do not depend on the original fraction of data at zero.

we remark that it is unlikely that the probabilities of appearance of a, c, g and t nucleotides in the n region are equal , nor in the 3'v-region or 5'j-region. a second-order model, giving much more freedom to possible a, c, g and t frequencies  can be found in supplementary data . in brief, we find that the first-order model approximates well the more general second-order model. thus for simplicity, the first-order result can be used in the place of the second-order result to form hypotheses on trimming processes.

testing the transformed v and j trimming distributions
under the hypotheses of the first-order model, we transformed the tra and trg tool "output" data following the law ff into probability distributions following the law fb.

remarking that apart from at zero, these transformed results often resembled poisson laws, we attempted to formally test this. more precisely, we supposed that we were dealing with a bernoulli process  followed by a poisson process  if the bernoulli process gave a success. this meant a density function of:

 f=1{x=0}+pe−λλxx!,x= <dig> , <dig> ... 

maximum likelihood was then performed in order to simultaneously estimate the parameters p and λ, this being necessary to subsequently test the hypothesis that we are dealing with a two-step bernoulli-poisson process having parameters p and λ.

given data x <dig>  x <dig> ..., xn, it is easy to show that maximum likelihood estimation gives the equations g = )c - mλ =  <dig> and p = m/n) to be solved, where m is the number of xi >  <dig> and c the sum of the values of the xi >  <dig>  as m and c are thus constants given any dataset, we see that resolving g =  <dig> for λ then allows us to solve for p in the second equation. upon performing the first-order transformation, we found  = , , ,  for the trav, traj, trgv and trgj datasets, respectively.

to see that g =  <dig> has a unique solution  here, we first remark that for each of these m, c >  <dig>  limλ→ <dig> g' >  <dig> and g'' <  <dig> for λ >  <dig>  limλ→∞g' = -m <  <dig>  and g' is a continuous function for λ >  <dig>  thus, by the intermediate value theorem, there exists at least one λ >  <dig> such that g' =  <dig>  and since g'' <  <dig> for λ >  <dig>  there is in fact a unique solution, which can be easily found numerically for each given m, c >  <dig>  indeed, we find  = , , ,  for the trav, traj, trgv and trgj datasets, respectively.

we found χ <dig> =  <dig> ,  <dig> ,  <dig>  and  <dig>  for the trav, traj, trgv and trgj trimming distributions, respectively. for trav, traj and trgv, we find that at all standard values of statistical significance , the null hypothesis is not rejected, and thus it is plausible that the empirical results follow a bernoulli-poisson-type law. however, for trgj, the null hypothesis is rejected at all of the same values of statistical significance. thus, as it stands, the bernoulli-poisson law hypothesis would seem unlikely for the trgj trimming process.

CONCLUSIONS
exploiting standardized "output" datasets of imgt/v-quest+jcta, we have shown how to recover, under several hypotheses, a representation of the probability distributions of the "true"  trav, traj, trgv and trgj trimming processes.

we proceeded by constructing a simple first-order model, known as the  rule, followed by a second-order model  which had more general hypotheses. it it clear that the first-order model is a good approximation to the second-order model. we then showed that a kind of two-step bernoulli-poisson distribution could plausibly explain the transformed trav, traj and trgv trimming distributions.

we remark that for the tra and trg data available to us, the first-order model is "close" to the original imgt/v-quest output data. this is partially due to the relatively smoothly varying data distributions being only slightly modified by performing the operation 4/ <dig> ℙ {f = k} - 1/ <dig> ℙ {f = k + 1} . an implication of this, for biologists, is that when hypothesis testing on tra and trg data sets, as long as the data is relatively smoothly varying from one value to the next, there should be no problem using the imgt/v-quest+jcta output data, without transformation. indeed, for our  <dig> data sets, the same hypothesis tests gave the same statistical result both on the imgt/v-quest+jcta output data as well as the first-order transformed data.

the statistical analysis of tr and ig junction sequences is a very young field due to the need of having large, clean datasets, unthought-of until recently. since processes such as the trimming process examined in this article are very little understood from a physical point of view , we see this work as opening a window to making hypotheses about the very nature of these physical processes and eventually improve our understanding of the complex molecular mechanisms of v--j recombination  <cit> . imgt® standardized criteria will eventually enable dealing with datasets numbering in the thousands or millions, impossible to deal with by hand. under this framework of much larger datasets, we hope the present work will inspire improved models that eventually allow a series of specific, testable hypotheses to be made.

