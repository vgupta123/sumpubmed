BACKGROUND
during evolution, genomes are subject to genome rearrangements, which are large scale mutations that can alter the ordering and orientation  of the genes on the chromosomes or even change the genome content by inserting, deleting, or duplicating genes. because these events are rare compared to point mutations, they can give us valuable information about ancient events in the evolutionary history of organisms. for this reason, one is interested in the most "plausible" genome rearrangement scenario between two genomes. more precisely, given two genomes, one wants to find an optimal  sequence of rearrangements that transforms this genome into the other. in the classical approach, each gene has exactly one copy in each genome, and only operations that do not change the genome content are considered. these "classical operations" are nowadays a well-studied subject, where the most important operations are reversals , where a section of the genome is excised, reversed in orientation, and reinserted, and transpositions, where a section of the genome is excised and reinserted at a new position in the genome. while the problem of sorting by reversals can be solved in polynomial time  <cit> , and the reversal distance can be determined in linear time  <cit> , the problem gets more complicated if one also considers transpositions, and there are only approximation algorithms known  <cit> . to simplify the existing algorithms, yancopoulos et al. invented the double cut and join operator, which can simulate reversals and block interchanges , resulting in a simple and efficient algorithm  <cit> .

however, restricting the genes to be unique in each genome does not reflect the biological reality very well, as in most genomes that have been studied, there are some genes that are present in two or more copies. this holds especially for the genomes of plants, and one of the most prominent genomes is the one of the flowering plant arabidopsis thaliana, where large segments of the genome have been duplicated . there are various evolutionary events that can change the content of the genome, like duplications of single genes, horizontal gene transfer, or tandem duplications. for a nice overview in the context of comparative genomics, see  <cit> . from an algorithmic point of view, the existence of duplicated genes complicates many existing algorithms, for example the problem of sorting arbitrary strings by reversals  <cit>  and the problem of sorting by reversals and duplications  <cit>  have been proven to be np-hard. so far, most of the existing algorithms restrict duplications to have a fixed length  <cit> , or simulate duplications by arbitrary insertions  <cit> . even with these restrictions, it is hard to solve most of the problems exactly, and heuristics have to be used.

related work
while genome rearrangement problems without duplications is a well studied subject, considering genomes with duplicated genes is a rather new field of research. one of the first works on this topic was done by sankoff  <cit> , where the following problem was examined. given two genomes with duplicated genes, identify in both genomes the "true exemplars" of each gene and remove all other genes, such that the rearrangement distance between these modified genomes is minimized. this approach minimizes the number of classical rearrangement operations, but not the one of duplications and deletions. in the work of el-mabrouk  <cit> , for a given genome with duplicated gene content, one searches for a hypothetical ancestor with unique gene content such that the reversal and duplication distance towards this ancestor is minimized. bertrand et al.  <cit>  developed an algorithm for the following problem. given two genomes with duplicated gene content, find a hypothetical ancestor such that the sum of the reversal and duplication distance of both genomes to this ancestor is minimized. however, in this work, duplications are restricted to have the length of one marker, i.e. a duplication can only duplicate segments that are identical in the initial genomes. therefore, this approach is disadvantageous if large segmental duplications happened during evolution. fu et al. extended this approach to the greedy algorithm msoar for assigning orthologous genes, which works well in practice  <cit> . other approaches  <cit>  simulate duplications by arbitrary insertions. recently, yancopoulos and friedberg provided a mathematical model of a genome rearrangement distance for genomes with unequal gene content  <cit> , combining the dcj operator  <cit>  with arbitrary but length-weighted insertions and deletions. another field of research is the "genome halving problem", where a rearrangement scenario consists of a whole genome duplication followed by a series of classical rearrangement operations. it has been studied first for reversals and translocations  <cit>  and recently has been extended to the double cut and join operator  <cit> .

to the best of our knowledge, the only approach that creates a rearrangement scenario between two genomes, consisting of duplications of arbitrary length and classical genome rearrangements, is the one of ozery-flato and shamir  <cit> . they use a greedy algorithm that starts with one genome and in each step applies the simplest and most evident operation that brings this genome closer to the target genome. if there is no evident operation, the algorithm aborts. although this approach fails on complicated rearrangement scenarios, they were able to find rearrangement scenarios for more than 98% of the karyotypes in the "mitelman database of chromosome aberrations in cancer"  <cit> .

our contribution
in this paper, we will focus on the following problem. given an ancestral genome with unique gene content and the genome of a descendant with arbitrary gene content, find the shortest sequence of reversals, block interchanges, tandem duplications, and deletions that transforms the ancient genome into the one of the descendant. in contrast to most of the previous works, tandem duplications and deletions can be of arbitrary length. we developed a lower bound for the distance, and a heuristic greedy algorithm based on this lower bound. the approach can be extended to also include general duplications and insertions of single elements, as described in section "discussion". experimental results on simulated data show that our algorithm works well in practice.

RESULTS
preliminaries
a genome π =  is a string over the alphabet { <dig>  ..., n}, where each element may have a positive or negative orientation . an augmented genome is a genome where the element π <dig> = 0→ is added at the beginning and the element πn+ <dig> = n+1→ is added at the end. as our algorithm works on augmented genomes, we will use the term "genome" as short hand for augmented genome. the genome  is called the identity genome id. the multiplicity of an element is the number of its occurrences  in π. two consecutive elements πi πi+ <dig> form an adjacency if πi = x→ and πi+ <dig> = x+1→, or if πi = x← and πi+ <dig> = x−1←. otherwise, they form a breakpoint. a segment πi ... πj  of a genome π is a consecutive sequence of elements in π, with πi as first element and πj as last element. a genome rearrangement problem is defined as follows. given two genomes π' and π and a set of possible operations, where each operation is assigned a weight, find a sequence of minimum weight that transforms π' into π. this minimum weight will be denoted by d. in our algorithm, we will restrict the set of operations to reversals, deletions, tandem duplications , and block interchanges , as defined in the next subsection. for simplification, we will also assume that π' = id, i.e. we search for a sequence of operations that transforms the identity genome into π, and we write d as short hand for d.

operations
in our algorithm, we will restrict the set of operations to the following four types of operations. a reversal rev  is an operation that inverts the the order of the elements of the segment πi ... πj- <dig>  additionally, the orientation of every element in the segment is flipped. a block interchange bi  is an operation that changes the positions of the segments πi ... πj- <dig> and πk ... πl- <dig> in π. a tandem duplication td  is an operation that adds a copy of the segment πi ... πj- <dig> before the element πj. a deletion del  cuts the segment πi ... πj- <dig> out of π.

we will use the double cut and join operator  to simulate reversals and transpositions. a double cut and join dcj  cuts the genome π before the elements πi and πj , and rejoins the cut ends in two new pairs. if x = +, we rejoin such that the elements πi- <dig> and πj- <dig> as well as the elements πi and πj- <dig> become adjacent. this is equivalent to a the reversal of the segment πi ... πj- <dig>  i.e. dcj = rev. if x = -, we rejoin such that the elements πi- <dig> and πj as well as the elements πi and πj- <dig> become adjacent. this cuts the genome into the linear genome π <dig> ... πi- <dig> πj ... πn+ <dig> and the circular genome ... πj- <dig> πj- <dig> πi πi+ <dig> .... this circular genome can be absorbed by applying another dcj with one cutting point in the linear genome and the other cutting point in the circular genome. depending on how we rejoin, those two dcjs are equivalent to either two consecutive reversals or to one block interchange. thus, we can reduce the set of operations to dcjs, tandem duplications, and deletions, provided that we demand that circular genomes must be absorbed in the next step.

the breakpoint graph
our main tool for visualization is the breakpoint graph. this graph has been introduced by bafna and pevzner to solve rearrangement problems on genomes without duplicated genes  <cit> . we extend this graph such that it can also be used for genomes with duplicated genes. the breakpoint graph of a genome π can be constructed as follows. first, we write the set of vertices {+ <dig>  - <dig>  + <dig>  - <dig>  + <dig>  ..., -n, +n, -} from left to right on a straight line. second, we add a reality edge ) for each i ∈ . third, we add a desire edge  for each i ∈ , where v = +πi if πi has a positive orientation, v = -πi otherwise, and v' = -πi+ <dig> if πi+ <dig> has a positive orientation, v' = +πi+ <dig> otherwise. for better readability, we draw reality edges as straight lines and desire edges as arcs. for an example, see fig.  <dig>  in contrast to the original breakpoint graph, each vertex can be the endpoint of several desire edges. in fact, the number of desire edges connected to a vertex +x or -x is exactly the multiplicity of the element x in π. the multiplicity of an edge  is the number of desire edges between v and v'. a desire edge  is called a loop. let s denote the number of vertices with a loop. two vertices v, v' are in the same component of the graph if and only if there is a path  from v to v'. let c denote the number of components in the breakpoint graph of π. an edge is called a 1-bridge if the removal of this edge increases c. a pair of edges is called a 2-bridge if none of the edges is a 1-bridge and the removal of both edges increases c.

note that two different genomes can have the same breakpoint graph, like e.g.  and . however, this will not cause problems in our algorithm, because we use the identity genome as ancestral genome, which has a unique breakpoint graph.

a lower bound
instead of searching for a sequence of operations op <dig>  ..., opk that sorts id into π, one can also search for the inverse sequence opk− <dig> ⋯,op1− <dig> that sorts π into id. this is more convenient, as it is easier to track the changes in the breakpoint graph caused by the performed operations . thus, we only apply inverse operations, i.e. we sort a genome π into id by dcjs, inverse tandem duplications, and inverse deletions. note that the inverse of a dcj is still a dcj, while an inverse deletion is an insertion. to keep our original problem in mind, we will use the term "inverse deletion" and not "insertion".

lemma  <dig>  the breakpoint graph of the identity genome has n +  <dig> components and no loops. no breakpoint graph has more components. if a breakpoint graph has n +  <dig> components, it is the breakpoint graph of the identity genome if and only if it has no loops.

proof. the first statement is easy to verify. as each vertex is connected with another vertex by a reality edge, the maximum possible number of components in the breakpoint graph is n +  <dig>  if a genome is not the id, it must contain a breakpoint. the desire edge corresponding to this breakpoint either is a loop, or it connects two vertices that are not connected by a reality edge. in the latter case, the breakpoint graph contains a component with at least  <dig> vertices and therefore cannot have n +  <dig> components.   □

we will now examine how an operation can change the number of components and loops.

dcj
a dcj cuts the genome at two positions, and rejoins the cut ends. this has the following effect on the breakpoint graph. two desire edges  and  are removed, and w.l.o.g. the desire edges  and  are added to the breakpoint graph. this can increase c by at most  <dig>  if one of the removed edges is a loop, all three vertices are in the same component after the operation, i.e. c will not be increased by this operation. as a dcj removes only two edges, s can be decreased by at most  <dig> 

inverse tandem duplication
an inverse tandem duplication deletes the following desire edges.  edges that are inside the duplicated segment. all these edges have a multiplicity ≥  <dig>  thus deleting these edges neither changes c nor s.  the edge between the last node of the segment and the first node of the copy. this can increase c by  <dig>  or decrease s by  <dig> .

inverse deletion
an inverse deletion splits the genome at one position and adds arbitrary elements. in the breakpoint graph, one desire edge is removed and several desire edges are added. therefore, a deletion can increase c by at most  <dig> or decrease s by at most  <dig>  as c can only be decreased if the removed edge is a 1-bridge, an inverse deletion cannot increase c and decrease s.

theorem  <dig>  a lower bound lb of the distance d is

 d≥lb=n+1−c+∑components⌈si2⌉ 

where si is the number of vertices with a loop in component ci.

proof. operations that increase c by  <dig> or decrease s by  <dig> decrease the lower bound at most by  <dig>  for an operation that remove two loops, there are two cases.  it acts on two loops of the same component ci. this decreases si by  <dig> and the lower bound is decreased by  <dig>   it acts on two loops in two components ci and cj. this can decrease two of the summands by  <dig>  but the components ci and cj are merged and c is decreased by  <dig>  thus the lower bound is decreased by at most  <dig>    □

the algorithm
the algorithm uses a greedy strategy to sort the genome. in each step, it searches for operations that decrease the lower bound, i.e. we search for operations that increase c or decrease s, and check their effect on the lower bound. if there is no such operation, we will use additional heuristics to search for small sequences of operations that bring us closer to our goal. the main idea behind these heuristics is to reduce the number of missing elements and duplicates and to create adjacencies.

operations that decrease the lower bound
as a dcj removes two desire edges and rejoins the endpoints with two new desire edges, it can only increase c if the removed desire edges are a 2-bridge, or two 1-bridges in the same component. if the dcj rejoins the endpoints such that we get a linear and a circular genome, we need a lookahead to search for another dcj that absorbs this dcj. those two dcjs are directly merged into two reversals or one block interchange with a weight of  <dig>  inverse tandem duplications can only remove one desire edge with a multiplicity of  <dig> , thus an inverse tandem duplication can increase c only if this edge is a 1-bridge. additionally, one has to check whether the segments on both sides of the cutting point are identical. inverse deletions just remove one desire edge, thus also an inverse deletion can increase c only if the removed edge is a 1-bridge. additionally, one has to check whether there is a segment that can be inserted such that no desire edge in the inserted segment merges two components. although there is such a segment in most cases, practical tests have shown that it is better to only insert segments that have no breakpoints, i.e. we perform only an inverse deletion if the breakpoint is of the form x→y→ or y←x← with x <y. in summary, the main task in finding operations that increase c is to find 1-bridges and 2-bridges in the breakpoint graph, which can be done very efficiently by following the algorithm devised in  <cit> .

finding operations that decrease s is rather straightforward, as we just have to scan the breakpoint graph for loops with a multiplicity of  <dig> and find the corresponding position in the genome. an operation that decreases s can be an inverse tandem duplication or an inverse deletion that removes this loop, or a dcj that removes two loops with a multiplicity of  <dig>  or a dcj on a loop and another desire edge of the same component.

heuristics for the remaining cases
if there is no operation that decreases the lower bound, one heuristic would be to decrease the number of duplicated elements without increasing the lower bound. if there are two consecutive copies of the same segment, we can remove one of them by an inverse tandem duplication. as an inverse tandem duplication only removes desire edges, it can never increase the lower bound. this is different in the general case of an inverse duplication, where the duplicated segments are separated by a non-empty segment in the genome. in this case, the removal of one of these segments  creates a new desire edge between the last element before the removed segment and the first element after the removed segment. if the corresponding vertices in the breakpoint graph are in different components, or if they are identical and the new desire edge would increase ⌊si/2⌋ of this component, the operation would increase the lower bound, i.e. we cannot easily provide a sequence of operations that removes one of the duplicated segments and does not increase the lower bound. however, the situation is different if we have at least three copies of the segment.

lemma  <dig>  if there are three identical copies of a segment that are maximal , then there exists a sequence of operations that removes two of these copies and does not increase the lower bound.

proof. let a be the vertex corresponding to the leftmost element of the segment, and let b be the vertex corresponding to the rightmost vertex of the segment. there are reality edges  and ,  and , and  and  . because the segment is maximal, we can assume w.l.o.g. that w <dig> ≠ w <dig>  as v <dig>  v <dig>  and v <dig> are all adjacent to a, they must be in the same component, as well as w <dig>  w <dig>  and w <dig>  by deleting the first two segments, we remove the desire edges , , , and , and get the new desire edges  and . if this merges two components, the new desire edges are a 2-bridge, and we can apply a dcj that replaces them by the desire edges  and . if v <dig> = v <dig> this can create a new loop. this loop can be removed by another dcj between the edges  and  . in fact, the operations of the sequence can be arranged such that all dcjs are reversals, so we do not have to find appropriate follow-ups. an illustration of the sequence is depicted in fig.  <dig>    □

we will now examine what we can do with elements with a multiplicity of at most  <dig>  a first strategy would be to create adjacencies wherever this is possible without creating loops . as a precondition, there must be a reality edge  and the desire edges  and  with c ≠ d.

if there are no further adjacencies to create, and all elements have a multiplicity of at most  <dig>  all the possible cases for a reality edge and its adjacent desire edges are depicted in fig.  <dig> 

lemma  <dig>  if all elements in π have a multiplicity ≤  <dig>  and there is no dcj that creates an adjacency without creating a loop, then there is a reality edge with adjacent desire edges corresponding to case a, b, or c in fig.  <dig>  for these cases, there is an operation or a sequence of operations that removes this configuration.

proof. if a reality edge and its adjacent desire edges correspond to case d or e, then the reality edge starting at vertex a must correspond to case c . now, let us assume that all reality edges correspond to one of the cases f and g. the elements adjacent to a reality edge of these cases occur either twice in the genome, or they do not occur at all. as we work with augmented genomes, there must be at least two elements that occur exactly once in the genome and have a breakpoint, otherwise the genome is the id. this is a contradiction to our assumption, therefore there must be at least one reality edge corresponding to cases a, b, or c. we will now provide sequences for these cases.

case a
the genome is of the form π=, the element x -  <dig> is missing. let y be the largest element <x that is not missing. we apply an inverse deletion of the elements y+1→ to x−1→ between a→ and x→, i.e. π becomes . the desire edge  is removed, the inserted desire edges are the edge ) and some adjacencies. the reality edge , -x) is split from the component, the edge ) may merge two components, so the overall number of components cannot be decreased. as the element y +  <dig> was not present in the original genome, the edge ) cannot be a loop.

case b
x is in a duplicated segment, w.l.o.g. the segment is left-maximal. we extend it to the right until it is also right-maximal. nevertheless, we will denote the leftmost vertex of the duplicated segment by -x and the rightmost vertex by +x, i.e. π= . as the segment is right-maximal, -b ≠ -c or the segments have different orientation and touch each other, i.e. π=. in the first case, we remove the copy of x that is not adjacent to x -  <dig>  i.e. we remove the desire edges  and , and create the new desire edge . if +a = -c, the loop can be removed by a dcj on this edge and the edge . in the second case, we remove the copy of x that is not adjacent to x -  <dig>  i.e. we remove the loop and the desire edge , and we create the desire edge . in both cases, the desire edge , -x) is split from the component, and adding one new desire edge can merge only two components, so the overall number of components does not decrease. additionally, also s cannot increase.

case c
the genome is of the form π=. we remove the second copy of x. this removes the desire edges  and  and adds the desire edge . if this has merged two components, then  and  are 1-bridges with disjoint endpoints ), so a dcj on these two edges splits the component again. if +b = -d, we have a loop, so we will not apply this sequence. instead, we use the symmetrical case in which we remove the first copy of x. if both +b = -d and +a = -c, we can remove the loop  by applying a dcj on it and the desire edge . note that there is the possibility that the first dcj creates a circular genome that cannot be absorbed in the next step. in this case, we can apply the sequence for case a twice, i.e. we add the same elements before both copies of x.   □

completeness of the algorithm
whenever the algorithm cannot apply an operation that decreases the lower bound, it searches for sequences that remove duplicated segments, for operations that create adjacencies, and for sequences according to the cases a to c in the previous subsection. then, one of these sequences is selected and applied to the genome. the pseudocode of the algorithm can be seen in fig.  <dig> 

to prove the completeness of the algorithm, we need the following lemma.

lemma  <dig>  let m be the number of missing elements in π, let r the number of elements that have to be removed from π  = |π| + m - ), and let a be the number of adjacencies in π. then, τ: = a - 2· + r) is maximal if and only if π = id.

proof. let π be a genome such that τ is maximal. if m >  <dig> we could transform π by adding all missing elements without increasing r or decreasing a. this would be a contradiction to the fact that τ is maximal, therefore m =  <dig>  now, let us assume that r >  <dig>  and let x be the smallest element that is duplicated. as x -  <dig> has a multiplicity of  <dig>  there is at least one copy of x that has a breakpoint. removing this copy decreases r by  <dig> and a by at most  <dig>  while m remains unchanged. this would increase τ, and lead to a contradiction. therefore, π is the genome without duplicated or missing elements with the maximum number of adjacencies, i.e. π = id.   □

we are now ready to prove the following theorem.

theorem  <dig>  the algorithm terminates after a finite number of steps. when the algorithm terminates, the genome π is transformed into id.

proof. as none of the operations and sequences of operations discussed above increases the lower bound, and the lower bound is minimized for id, only a finite number of operations that decrease the lower bound can be applied. as we have shown in the last subsections, the algorithm always finds a sequence of operations as long as π ≠ id. table  <dig> shows the changes of τ when applying these sequences. as all sequences increase τ, only a finite number of those sequences can be applied between two operations that decrease the lower bound. therefore, the algorithm must terminate, and π is transformed into id.   □

testing
we used simulated data to assess the performance of our algorithm. we generated test cases by creating the identity genome id of size n and applying random sequences of αn operations for different values of n and α . for each value of n and α, we created  <dig> test cases. the operations of the sequences are independently distributed, with tandem duplications and deletions having a probability of  <dig>  reversals having a probability of  <dig>  and block interchanges having a probability of  <dig> . once the type of an operation was determined, the operation was selected uniformly distributed among all operations of this type. as long deletions can cancel the effects of previous operations, deletions were restricted to have a length of at most  <dig>  times the current genome length. to keep the size of the genome approximately constant, also tandem duplications were restricted to have a length of at most  <dig>  times the current genome length.

we then calculated the lower bounds of the test cases, and used our algorithm to reconstruct the sequence of operations. the results of these experiments can be seen in fig.  <dig>  on average, our algorithm finds good sequences  as long as the lower bound is close to the number of operations used to create the test case. as this coherence lessens for increasing values of n and α, the length of the calculated sequences increases. however, even for higher values of n and α, the calculated distances are still a good approximation for the original distance. if one examines the frequency of the different types of operations, the number of performed duplications and block interchanges approximately fits the expected values. the algorithm tends to overestimate the number of reversals and underestimate the number of deletions, especially for higher values of n. for details, see fig.  <dig> 

discussion
in the following, we will discuss how the set of operations could be extended.

duplications
while our algorithm only considers tandem duplications, one might also be interested in including arbitrary duplications. this would be rather easy if tandem duplications have weight  <dig>  and all other duplications have weight  <dig>  since the general case of a duplication can change the lower bound by at most  <dig>  however, weighting all duplications equally is a more complicated subject. if all duplications have weight  <dig>  this could be disadvantageous in detecting tandem duplications, as an inverse tandem duplication can decrease the lower bound by at most  <dig>  on the other hand, if all duplications have weight  <dig>  duplications would be favored over dcjs. this could lead to sequences of many small duplications, instead of first merging the segments and then just applying one big duplication.

insertions
insertions of single elements could be easily included in our algorithm, because the inverse of this operation decreases the lower bound by at most  <dig>  insertions of arbitrary length are more complicated. on the other hand, allowing insertions of arbitrary length are neither biologically meaningful nor do they make sense in combination with arbitrary deletions, because one could solve every sorting problem by just one deletion and one insertion step. thus, further research in including insertions should also include a reasonable length depending weighting of the insertions.

CONCLUSIONS
we presented an algorithm that works well for smaller genomes and distances. although our results are promising, this algorithm should be seen as a first step in handling duplications of arbitrary length. further research could improve the algorithm itself by finding closer lower bounds and better heuristics, or extend the algorithm such that it considers more different operations  and can also handle multichromosomal genomes.

competing interests
the author declares that they have no competing interests.

authors' contributions
mb designed the algorithm, implemented it, performed the tests, and drafted this manuscript.

