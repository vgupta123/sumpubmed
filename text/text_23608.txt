BACKGROUND
dna-dna hybridization  is a wet-lab method currently still used as the taxonomic gold standard for species delineation in archaea and bacteria. if the genomic dna of two respective organisms reveals a ddh similarity of below 70% this is the main argument to regard them as distinct species and vice versa  <cit> . ddh is widely considered as tedious, laborious and potentially rather error-prone  <cit> . moreover, in contrast to genome sequencing it does not return more information than the ddh value itself and, as a consequence, it is impossible to work incrementally by re-using data.

the ddh technique is currently established in only a few specialized labs  and, because it is prone to experimental deviation, requires several experimental repetitions to determine the statistical confidence of that experiment. for instance, regarding species delimitation in microbiology, the relevant question is whether or not the ddh value is significantly below or above 70%. this is particularly important in the context of a polyphasic approach, in which the evidence from ddh has to be traded off against other criteria such as phenotypic measurements  <cit> . ddh experiments can be omitted in descriptions of novel species only if the 16s rrna sequence similarity is below a certain threshold, indicating that ddh values above 70% cannot be expected  <cit> .

the increasing availability of genome sequences thus triggered the development of computational techniques to replace wet-lab ddh  <cit> . these were expected to provide the deepest possible resolution for differentiation, to ensure much higher reproducibility of the results and to allow incremental work by filling databases with type-strain genome sequences  <cit> . but unless high correlations with wet-lab ddh, and precise models for estimating ddh or at least ddh-analogous species boundaries from genome-to-genome comparisons, were available, the newly calculated values were not comparable to the previous ones and could yield largely deviating species-boundary estimates and, thus, an inconsistent microbial taxonomic classification. hence, for obvious reasons the literature on in-silico replacements for ddh considered correspondence with wet-lab ddh values as optimality criterion. as a consequence, regression and/or correlation analyses with wet-lab ddh values were used throughout for the calibration and optimization of the in-silico replacement methods  <cit> .

in view of the technical problems and progress the relation between the wet-lab ddh procedure and digital estimation of ddh equivalents reminds very much to what happened some  <dig> years ago when dna:rrna cross-hybridization melting curves  <cit>  were replaced by 16s rrna sequences, which supported a significant progress in microbial phylogeny  <cit> .

the genome blast distance phylogeny approach  was originally devised as an approach for the inference of phylogenetic trees or networks from a given set of wholly  sequenced genomes  <cit> , and was subsequently revisited and enhanced  <cit> . the underlying principle is as follows: in the first step two genomes a and b are locally aligned using tools such as blast <cit> , which produce a set of high-scoring segment pairs . in the second step, information contained in these hsps  is transformed into a single genome-to-genome distance value by the use of a specific distance formula. phylogenetic trees can then be inferred from such distance matrices using standard techniques such as neighbour joining  <cit> . these methods are robust even in the presence of a significant amount of paralogous genes, large repeats and reduced genomes  <cit> , as well as low complexity-regions within the sequences  <cit> . gbdp could also be applied to proteomic data  <cit>  and even to single genes  <cit> .

a further use of gbdp was recently evaluated, namely to infer digital equivalents for ddh values  <cit> . these turned out to successfully mimic the wet-lab hybridization results, providing higher correlations with an empirical set of ddh values than antecedent genome sequence-based methods  <cit>  and being able to deal with rather incomplete genomes  <cit> . microbiologists can make use of gbdp by means of a free web service at http://ggdc.dsmz.de <cit>  for submitting genome pairs and receiving ddh analogues as well as model-based ddh estimates. such values on the original scale of wet-lab ddh measurements have the practical advantage that the well-known 70% threshold can still be applied  <cit> , even though they are mathematically equivalent to in-silico ddh analogues that use a scale of their own, and accordingly represent a novel species-delimitation threshold  <cit> .

the first goal of the present study is to improve ddh estimation from genome-sequence comparisons by using a more comprehensive empirical database and by considering a broader range of numerical data transformations and statistical models. previous studies were limited to regression models of the untransformed data and thus presupposed a linear relationship between wet-lab ddh and the results of genome-sequence comparisons  <cit> . but this assumption might be unjustified, and the inspection of more complex models  and distinct data transformations  <cit>  has frequently been recommended. in addition to suboptimal fits, linear models can lead to ddh predictions below 0% and above 100% similarity if the underlying similarity values are close to the upper bound  or the lower bound , respectively.

the second goal of these examinations is to obtain confidence intervals for in-silico ddh values – an indicator showing taxonomists how uncertain a reported value is, especially if it is close to the 70% boundary. even though it is safe to assume a priori that digital ddh values display much less variability than wet-lab ddh experiments given the high sequence coverage that can be obtained with state-of-the-art sequencing technology  <cit> , it is of interest whether, and how, confidence intervals can be calculated for the in-silico replacement methods, too. hence, gbdp was further extended by integrating resampling techniques for calculating a confidence interval per point estimate . bootstrapping  <cit>  and jackknifing  <cit>  are well-known and robust resampling techniques for estimating the variance of a sample. but uncertainty might additionally or mainly be caused by the empirical modeling of the relationship between ddh and genome-sequence comparisons , and the relative proportions of resampling- and model-based confidence intervals need to be assessed. confidence intervals would render gbdp the first in-silico procedure to infer ddh analogs that can be statistically evaluated, which is particularly important in the context of the polyphasic approach to microbial taxonomy .

the third topic of this study is to broaden the range of considered gbdp distance functions. whereas  <cit>  already investigated a much more diverse range of in-silico ddh analogues than previous publications on ddh-replacement methods  <cit> , here gbdp is further enriched with so-called “coverage distances”  <cit>  . the performance of the novel gbdp implementation could thus be assessed under  <dig> distinct settings , requiring a total of  <dig> million individual genome comparisons , and the overall best-performing settings determined. the effects of the parameters used for the calculation of intergenomic distances on the resulting correspondence with the ddh values was also investigated in detail using multiple regression.

the results of this study are thus likely to contribute toward progress in using the comprehensive information encoded in entire genomes for the taxonomy of prokaryotes.

methods
extended benchmark data set
the ddh benchmark data set was extended compared to previous studies aiming at an increased precision and significance of the ranking of the genome-to-genome distance methods and the models for the conversion to ddh values. in detail, the here used data set  comprised  <dig> unique genome pairs along with their respective ddh values:  <dig> from goris et al.  <cit> ,  <dig> from the gold database  <cit> , and  <dig> from richter et al.  <cit> . only the first two sources had been considered in a previous publication on gbdp as ddh replacement  <cit> .

if several ddh/anib/anim/tetra values were present for a single genome pair, they were averaged. a single genome pair showed a ddh value above 100% similarity . as it biologically made not much sense this value was set to 100% to maintain proper input data for some of the statistical models . another genome pair  had a contradicting relation between its ddh value  and the genome based distance/similarity measures  on the other hand  <cit> . following  <cit> , this questionable data point was excluded from the correlation analyses. the full list of genome pairs used in this study is found in the additional file  <dig> 

to detect significant deviations, if any, between the new and the previous gbdp implementation, the data subset “ds2” was created, containing only the previously available data points  <cit> . for comparing gbdp with the first ani implementation, data subset “ds3” comprised the  <dig> data points in common between  <cit> ; for comparison with the jspecies study, subset “ds4” contained only the  <dig> ddh values in common between  <cit> .

the gbdp principle, and its technical update
to motivate the upcoming changes such as the addition of support for blast+ <cit>  and the completion of the implementation of the so-called “coverage” algorithm  <cit> , the major steps within the gbdp pipeline  <cit>  are summarized in the following.

the pipeline is primarily subdivided into two phases. first, a genome x is blasted against a genome y and vice versa . blast+ has recently been added to the list of available programs, because it provides substantial speed improvements for long queries and database sequences  <cit> . the alignment process is done in one pass using all the available sequence information of both respective genomes, i.e., gbdp does not require the sequences to be artificially cut into pieces as do other approaches  <cit> .

the resulting matches between both genomes are called high-scoring segment pairs  and represent local alignments that are considered statistically significant if the associated expect value  is sufficiently low  <cit>  .

in the second phase, these matches are transformed to a single distance value d by applying one out of ten available distance formulae d <dig> to d <dig>  to describe these, the following definitions are required: 

  xy:=blastrun using genomes x and y  

  ixy:=sum of identical base pairs over all hsps 

  hxy:=total length of all hsps 

  λ:=sum of both genomes’ lengths 

  λmin:=twice the length of the smallest genome 

the web service at http://ggdc.dsmz.de makes use of these formulae; the other gbdp formulae are minor variants  <cit>  and are found in additional file 3: 

  d0=1−hxy+hyxλ 

  d4=1−2·ixyhxy+hyx 

  d6=1−2·ixyλ 

each was devised to consider distinct aspects of intergenomic relationships. formula d <dig> preserves most information, because it is some kind of combination of d <dig> and d <dig>  it also performs best in a phylogenetic context  <cit> . however, d <dig> is immune against problems caused by incompletely sequenced genomes, as it does not consider the genome lengths  <cit> . it follows from d <dig> to d <dig> that similarity instead of dissimilarity  values could be easily obtained by subtracting the distances from  <dig>   <cit> . this would be mathematically analogous for the subsequent correlation analyses because only the sign changed, but using distances is more convenient for inferring gbdp trees  <cit> .

however, in practice, at least some hsps from a blast run between x and y are very likely to overlap , mainly because paralogous genes can be present  <cit> . with respect to definitions  <dig> and  <dig> these overlapping segments would introduce a bias in the resulting distance value, because they would be considered more than once in the calculations. for this reason, gbdp includes three distinct approaches for filtering hsps and, thereby, resolving these conflicts before one of the aforementioned  <dig> distance formulae is applied: “greedy”, “greedy-with-trimming” and “coverage”. for technical reasons, “coverage” was previously  <cit>  only available in connection with distance formulae d0- d <dig>  but the missing ones were implemented in the course of this study. greedy if two or more hsps are overlapping in a specific segment, this algorithm omits all hsps except the largest one . some information is thus lost, but this approach is computationally faster than the next one.greedy-with-trimming here, only the overlapping parts of hsps in either genome are removed, as, for instance, segment “c” in figure  <dig>  this preserves more information and proved to be valuable in phylogenetic inference from genomes with large numbers of repeats  <cit> .coverage the content of the hsps is mapped to two vectors  representing the positions within either genome. concerning definition  <dig> this was implemented by considering a position within a genome as covered by an hsp if there is at least one hsp that covers it  <cit> , whereas definition  <dig> in conjunction with coverage works by assigning the highest identity among all hsps that overlap in a certain genome segment to the positions within this segment. in detail, a coverage vector, as a vector of length |g|, is  computed for each genome as follows: let h be a matching segment of an hsp and i its proportion of identical characters. then segment h beginning at position i and with length l can be displayed as a vector of length |g|  as follows: 

  h= 

each vector position which is not covered by h is set to  <dig>  while each covered position is set to i. further, let pmax  be a function that reduces a list of equal-length vectors to a single vector by determining the maximum for each vector position: 

  pmax:=,…,max)fora=andb= 

in the case of the coverage vectors n equals the length of the respective genome . then, the coverage vector vg = pmax is calculated by applying pmax to the list of k hsp segments within genome g. analogously, a second vector is calculated for the hsp intervals within the second genome. the numerator of the identity- based dissimilarities  can then be calculated by using the sums over all vector positions: 

  ixy=∑vx+∑vy <dig> 

analogously, the number of genome positions covered by hsps, hxy and hyx, as used in formulae  <dig> and  <dig>  is calculated by counting all non-zero positions in the vectors vx and vy, respectively. the coverage method is faster than the other two approaches, because there is no overhead caused by hsp sorting and/or trimming algorithms.

conducting genome comparisons for the correlation analysis
a correlation analysis was conducted to show the overall performance of the gbdp method and to yield the best gbdp parameter setup. six local-alignment tools were tested for genome comparisons: blast+ <cit> , ncbi-blast <cit> , mummer <cit> , blat <cit> , wu-blast <cit>  and blastz <cit> . these were not only used to conduct six genome comparisons per available genome pair but were also applied in several passes, each time changing a chosen parameter, presumably affecting the local alignment and thus potentially improving the correlation with ddh. a special focus was on finding influential parameters for blast+ because it is one of the technically most advanced local-alignment tools available  <cit> . moreover, all available distance functions and hsp filtering approaches were applied to each genome comparison . thus, all in all  <dig> distinct settings were investigated, i.e., the product of  <dig> alignment settings, three algorithms for dealing with overlapping hsps, and ten distance formulae. this resulted in a total of  <dig> million gbdp-based genome comparisons which had to be conducted. additional file  <dig> provides a complete overview on these settings and numbers.

in general, studies of that kind are computationally challenging, because a huge number of input and result files need to be processed. this gave rise for equipping the method with an extension allowing it to be executed on compute clusters  <cit> .

analyzing correlations between intergenomic distances and ddh values
according to  <cit>  pearson’s ρ  and kendall’s τ  were computed for all distinct gbdp settings and the ddh values, respectively. the necessary analysis pipeline was implemented as an r <cit>  script and applied to the previously described main data set ds <dig> and its subsets ds2- <dig>  as gbdp-derived values are distance measures, whereas ani values are similarities between genomes, the correlation coefficients’ sign had to be inverted accordingly to allow for the direct comparison of the performance of gbdp, ani and jspecies. the full list of the genome pairs and their associated ddh and ani values is found in additional file  <dig> 

for gbdp, the influence of four predictor variables was tracked by means of multiple linear regression : alignment method, algorithm for treating overlapping hsps, distance formula and e-value filter method . the identification of the least complex model that still explained most of the variation in the data was conducted with the r package mass <cit>  under both forward variable selection and backward elimination. for an interpretation of possible interaction effects, the effects package  <cit>  for r was used. additionally, each predictor variable’s relative importance index  <cit>  was computed.

gbdp bootstrapping and jackknifing
to obtain confidence-interval  estimates, gbdp was augmented with sampling with replacement  and sampling without replacement , by default using 50% deletion in each replicate. for the variants of gbdp that use a filtering approach for removing overlapping  hsps, namely the “greedy” and “greedy-with-trimming” algorithms , the implementation treats individual hsps as the observations for resampling. after applying the filtering step, the original distance  is inferred from the set of filtered hsps using one of the distance formulae described above. then the set of hsps is bootstrapped or jackknifed, and in each replicate a distance value is calculated using the same formula. for the “coverage” algorithm , resampling was implemented by bootstrapping  the constructed coverage vectors. for the evaluation of the methods,  <dig> replicates were used throughout.

the dependency of the resulting bootstrapping and jackknifing cis on each genome pair’s original distance  was investigated, as well as the effect of the gbdp method on the replicates’ variation. the latter was assessed via the median of the variation coefficients  <cit>  calculated for each genome pair and its respective replicates. it was also evaluated to what extent the ci width was affected by the distribution of both numbers and sizes of hsps derived from a respective pair of genomes. this was done for a selected, well performing blast+ method . to assess the amount of uncertainty indicated by the bootstrap/jackknife cis in ddh prediction, the bounds of each ci were transformed to ddh  using the investigated prediction model. thus, the relationship between  the model’s ci estimating the uncertainty of ddh prediction, as detailed in the next section, and,  the bootstrap/jackknife ci translated to ddh scale was assessed for each observation.

ddh prediction using sophisticated statistical models
the problems caused by linear models  for predicting ddh via intergenomic distances can be solved by more sophisticated statistical models such as generalized linear models   <cit> , generalized additive models   <cit>  and loess models  <cit>  and identifying the one that provides the most robust predictions . all models used ddh values as response variable and the corresponding intergenomic distance values as predictor variable. among the novel models assessed in this study, glms came to our attention for several reasons:  they do not require the response variable to be normally distributed,  the response predictions  would be bound within a fixed interval between a  <dig> and 100% and  the predictor variable does not need to have a constant variance. the latter was especially important because, in our case, as usual for proportion data, the distance values are strictly defined in a range between  <dig> and  <dig>  thus leading to a decrease in variance when approaching these boundaries, causing a dependency of the variance on the mean .

glms belong to the parametric modeling techniques and make assumptions about the underlying distribution. for proportional response data as present here a binomial distribution is recommended . one benefit of such a logistic regression is the variance-stabilizing effect on the response variable . ddh response data was appropriately converted to represent the number of failures and successes of an event . another special type of glm was constructed by changing the response from ddh proportions to a binary response variable . for any given intergenomic distance such a model yields the probability of whether or not it corresponds to a ddh value ≥ 70%. finally, a non-parametric loess smoother was also evaluated, as well as generalized additive models  <cit>  but neither yielded better results than the glms .

to assess whether the fit of the overall model  could be further improved, a log transformation was applied to the explanatory variable  <cit> , and/or a variance-stabilizing arcsine transformation was applied to the response variable . in standard linear-regression models the coefficient of determination  provides a measure of how well future outcomes are likely to be predicted by a certain linear model. as glms do not provide r <dig> for model diagnosis, we checked for potential overdispersion   and where applicable used the akaike information criterion  <cit>  to measure the relative goodness of the glm fits. graphical evaluation of the model fits was done using the r package ggplot <cit> .

the performance of the model types and data transformations was also assessed by computing error ratios in ddh prediction. for each of the  <dig> gbdp settings we calculated the models  and compared the ddh predictions with the respective wet-lab ddh value. in a second pass, we calculated the model’s error ratio by assessing the number of false positives  and false negatives  relative to the total number of observations. to investigate the impact of the extension of the empirical data set, we chose the best-performing gbdp method from this study and fitted the aforementioned models to both the full data set  and the reduced one .

RESULTS
performance of methods and settings in mimicking wet-lab ddh
juxtaposition of ddh correlation values for best-performing gbdp methods as well as  ani  <cit>  and  jspecies <cit>  implementation . the content of the respective data sets ds1-ds <dig> is described in materials and methods, whereas the full table with all correlation results is found in additional file  <dig>  for convenience, the correlation coefficients’ sign of the ani values is inverted to allow for the direct comparison toward gbdp . abbreviations used: wl  and mr . the listed blat runs were all conducted under the same settings . gbdp-based correlations surpass any of the ani implementations throughout the respective data sets.

the most influential gbdp parameters were assessed via a multiple regression under two types of model selection  with both resulting in the same full regression model. the latter contained the independent variables “alignment tool”, “distance algorithm” and “distance formula” and all possible interaction terms between them . that is, only “e-value filter method” got eliminated as it could not explain a significant amount of variation in the data. this was confirmed by the relative importance index  and an anova . additional file  <dig>  shows the interaction effects and the impact of all independent variables.

confidence intervals via bootstrapping or jackknifing
the effect of the gbdp settings on the bootstrapping and jackknifing confidence intervals as assessed by the aggregated coefficients of variation  is shown in figure  <dig>  the “coverage” algorithm resulted in low cvs with little variation , while the variation induced by “greedy-with-trimming” was the most pronounced one, and the cvs were on average much higher, but only slightly higher than for the “greedy” variant . the effect of the distance formulae on the cvs is shown in figure 3b. the median cv was around 1% for formulae d <dig> - d <dig> and around  <dig> % for the remaining ones. bootstrapping and jackknifing had nearly the same effect on the cv’s distribution .

the relationship between the intergenomic distance and the underlying set of hsps obtained by comparing the respective pair of genomes is presented in additional file  <dig> . in brief, the lengths of the hsps are more unevenly distributed for more similar genomes .

models for ddh prediction and species delineation
the results for the glms using wet-lab ddh values as response variable are shown in figure 6; additional file  <dig>  contains the results for the linear models. each part of these figures is a superimposition of two models, based on either ds <dig> or ds <dig>  <cit> . the best correlating blast+ method  was used throughout. both the standard glm and its logarithmized variant showed a substantial amount of overdispersion, as the residual deviances  were significantly greater than the degrees of freedom . . the log-model had a smaller aic  and thus performed slightly better according to the models’ diagnostics than the standard glm, in accordance with figure  <dig>  the standard linear regression models  yielded an adjusted r <dig> of  <dig>  for ds <dig> but only  <dig>  for ds <dig>  indicating that the linear model fits the data less the more observations are provided. the model for the arcsine- transformed data  yielded approximately the same outcome . additional file  <dig> holds ddh predictions for a series of distance values between  <dig> and  <dig> for all discussed models.

in table  <dig> the error ratios of selected gbdp methods and distinct models are shown. generalized linear models with log transformation provided the lowest mean error ratio and had the lowest deviation over distinct gbdp settings. the error ratios in predicting ddh at the 70% threshold subject to correlations between intergenomic distances and ddh are shown in additional file  <dig> . all subplots reveal a similar kind of structure. moreover, as expected we observed that the error ratios increase with an increase in the correlation . the log-glms, however, resulted in a higher correspondence between error ratio and kendall correlation than the other types of models.

error ratios under different models and the full empirical data set. the here presented gbdp settings are a well-performing selection . the models are: linear, lm; linear with arcsine transformation, lmas; generalized linear, glm; glm with log-transformation, glmlog.

discussion
bootstrapping and jackknifing gbdp
with only minor differences between bootstrapping and jackknifing, the use of different algorithms had an obvious impact on the cis of the resulting distances. the full implementation of the “coverage” algorithm allowed its application in connection with distance formulae d <dig> to d <dig> . its cvs and, accordingly, its cis and those of the resulting ddh estimates, were two orders of magnitude smaller than the ones resulting from the “greedy” and “greedy-with-trimming” algorithms . the cis of the ddh values estimated with the “coverage” algorithm showed a typical quality of proportion data , i.e., very high as well as very low distances between two genomes cause a reduced amount of variation between the resampled distances.

that the “greedy” and “greedy-with-trimming” algorithm yielded substantially higher cvs and cis, as well as an increase of cvs and cis with decreasing distance  is most likely caused by the fact that here sets of hsps, not genome positions are resampled. the observed methanococcus genome pair confirms this, as its comparison yielded few very long hsps with a high impact on the resulting distance values during resampling . indeed, we observed the overall tendency that more similar genomes not only result in more hsps but also in hsps with much less equally distributed lengths. hence, for “greedy” and “greedy-with-trimming”, the resampling of hsps probably leads to an overestimation of the uncertainty in distance estimation and thus cannot be recommended over the resampling in conjunction with the “coverage” algorithm, which by construction is not depending on hsps during the resampling phase. the relative sizes of the cis also implies that those from “coverage” bootstrapping or jackknifing are by an order of magnitude smaller than the cis from the ddh prediction via models, and that those from “greedy” and “greedy-with-trimming” resampling are by an order of magnitude higher than the model-based cis.

for practical purposes this indicates that in conjunction with “coverage”, bootstrapping and jackknifing gbdp can safely be omitted because the uncertainty it indicates is always substantially lower than the one indicated by the cis of the model. this result is as expected, because genome sequencing results in a large number of characters – actually, the largest possible number of characters that can be sampled from an organisms –, and, if appropriately calculated, the uncertainty of intergenomic distances should be much lower than the one of wet-lab ddh experiments. experimental errors in determining ddh in the wet lab, however, are likely to be responsible for the uncertainties in fitting the models for predicting ddh in silico.

models for ddh prediction
all previous studies  <cit>  were based on simple linear regressions models  and did not consider data transformations. we observed that error ratios in ddh prediction, provided by lms, are generally higher than those of the generalized linear models . in addition to mere statistical parameters , the size of the error ratio in predicting whether ddh values are above or below 70% is of particular interest. as expected, it was revealed that the smaller the correlation between ddh and intergenomic distance, the higher the error ratio .

moreover, the glms combined with the log-trans- formed explanatory variable yielded a higher consis- tency between the correlation coefficients and the prediction success at the 70% boundary, and the better correlating gbdp methods had, on average, a lower error ratio if combined with these log-glms instead of any of the other models. additionally, glms, if applied as shown, guarantee that even ddh predictions based on extreme distance  values are between 0% and 100%. for obvious reasons, models for species delimitation should be as exact as possible and, thus, lms here at least be considered as problematic. the overdispersion detected when diagnosing glms was presumably due to distinct pairs of strains sharing identical intergenomic distance values but at the same time showing distinct ddh values. this effect is called “unmodeled heterogeneity” and could also result from clustering of the ddh measurements , as observed in our data set. a switch to a “complementary log-log”-link function, as suggested in , didn’t further improve the model .

the enlarged data set provided a globally increased significance of the inferred results. the comparison of selected gbdp methods applied to either the old  <cit>  or the new data set, however, revealed only minor differences in the parameters estimated by the statistical models we tested.

both theoretical and empirical results thus favor glms over standard linear-regression models for obtaining in-silico ddh replacement methods. its improved ddh prediction capabilities offer gbdp as a quick and now even more reliable alternative to the ddh wet-lab technique, thus moving further forward within the transition process to a genome-based taxonomic gold standard.

the recommended gbdp method
in principle, multiple optimality criteria could be applied for selecting a gbdp variant that works best in ddh prediction, depending on the users’ priorities. the newly completed “coverage” algorithm, however, can unanimously be recommended, because it yielded the best correlations for both formula d <dig>  and formula d <dig> . when dealing with incomplete genomes it is highly recommended to use formula d <dig>  as it is independent of sequence length, and thus not directly affected by the removal of hsps due to the removal of parts of the genome  <cit>  . here, formula d <dig> resulted in worse kendall correlations but better pearson correlations and error ratios at the 70% boundary than d <dig>  as observed earlier  <cit> .

regarding local-alignment programs, only blat performed better than blast+ combined with optimized settings, and only slightly so. blast+ ’s optimal initial word length setting of  <dig> or  <dig> allows for comparatively quick genome-genome comparisons, because the intergenomic search space is significantly reduced compared to the default value of  <dig>  resulting in a lower execution time. a higher initial word length results in a lowered sensitivity of the local-alignment program, which had a positive effect on the correlation outcome, as previously reported  <cit> . this is in agreement with the fact that blat, which is a considerably less sensitive alternative to blast, overall performed best  <cit> . all in all, we conclude that the default setting for the novel gbdp implementation should be blast+ combined with d <dig> and the accordingly optimized settings regarding word length and e-value filtering, and that the corresponding log-glm model should be used for predicting ddh including cis. moreover, the situations in which a user might instead favor blat over blast+ and/or d <dig> over d <dig> are straightforward to identify. all these recommendations can now be directly utilized via our updated web service  at http://ggdc.dsmz.de.

beyond pairwise distances
since the dawn of computer-based approaches to phylogenetics, researchers were trying to devise solutions for assessing statistical support of the inferred phylogenies  <cit> . apparently, branches lacking sufficient support should not be overestimated regarding the explanation and visualization of evolutionary events. particularly bootstrapping and jackknifing  <cit>  are widely-used solutions for this kind of question and can be applied to both aligned molecular sequences  and matrices of phenotypic characters. here, resampling is applied to the characters, usually present as columns of a matrix whose rows represent the organisms, phylogenetic inference is applied to the resampled matrix, and finally a majority-rule consensus tree is calculated from the trees from all replicates  <cit> . if distance methods for phylogenetic inference such as neighbour-joining  <cit>  are used in such a scenario, within each replicate a distance matrix is computed from a character matrix that has been resampled at once.

in contrast, distance methods that avoid the construc- tion of a character matrix would need to apply boots- trapping or jackknifing to each pairwise comparison independently. for instance,  <cit>  developed a method that relies on pairwise sequence alignment only; here, maximum-likelihood distances are inferred, and bootstrapped independently, from the alignments of all pairs of sequences. to highlight the conceptual difference, the procedure was called “pseudo-bootstrapping”, and it was demonstrated to be conservative compared to bootstrap analysis of multiple sequence alignments  <cit> .

apparently, gbdp’s new bootstrapping and jackknifing facilities would also yield such a pseudo-bootstrapping approach if it was applied to phylogenetic problems. in particular, sequence comparison is conducted independently for all genome pairs involved, and the sets of hsps or the coverage vectors – on which each pairwise comparison is based – never form a common character matrix  <cit> . bootstrapping and/or jackknifing would just add the individual resampling of these independently constructed sets of hsps or coverage vectors. an advantage to phylogenomics provided by gbdp-bootstrapping over supermatrix approaches  is that the calculation of bootstrapped or jackknifed distances could be done incrementally, and only the phylogenetic inference from all formed distance matrices would need to be done after each update of the set of organisms of interest.

for this reason, gbdp with resampling could be a faster and resource-saving alternative to more compute-intense phylogenomics methods, particularly because gbdp can as well be applied to sequences from proteomes  <cit> . besides, it easily copes with various phylogenetic problems such as paralogous genes  <cit> , low-complexity regions  <cit>  and unbalanced genome/proteome sizes  <cit> . however, whereas this study already presented evidence that “coverage” should be preferred over “greedy” and “greedy-with-trimming” if coupled with bootstrapping or jackknifing, it is a partially open question whether, and under which conditions, resampling proteome-based gbdp <cit>  should be preferred over analyzing nucleotide sequences this way  <cit> . even though it is likely that the deeper branches of the phylogeny can only be resolved based on amino acid sequences gbdp <cit> , in-depth comparisons of the performance of gbdp- bootstrapping/jackknifing with more common phylogenomics methods, as well as similar methods that are also based on resampling hsps  <cit> , are still needed.

nevertheless, that a single method can be applied to both genome-based species delimitation and phylogenomic inferences at other taxonomic levels, and that it can be coupled with the assessment of statistical significance at one level, already strongly indicates that gbdp is an important tool in the transition process to genome-based gold standards at all taxonomic levels. a tighter coupling between phylogenetic inference and the assignment of taxonomic ranks might also help to overcome what we regard as the most severe theoretical limitation of the ddh 70% rule: that a taxon defined as all organisms whose similarity to a type organism is above a certain threshold is never guaranteed to form a monophyletic group  <cit> . the practical limitations of the wet lab-based ddh, however, already seem to have been overcome.

CONCLUSIONS
this update on the gbdp method is an important enhancement, not only because existing features of the software have been improved but particularly because novel features have been added. whereas gbdp was already shown to yield better correlation results in ddh prediction than ani  <cit>  in an earlier correlation study  <cit> , we can also confirm this with respect to the jspecies <cit>  implementation. since taxonomists generally consider these approaches as potential “next generation” replacements for the traditional and currently still dominating wet-lab method  <cit> , up to now these approaches could not be used to determine the ci of intergenomic distance measures, thus rendering the latest installment of the gbdp method to be the first one supporting that feature. this is crucial because numeric estimations from empirical data  always yield a certain degree of uncertainty, and it is thus commonplace in statistics to provide measures of variation and confidence.

by introducing  bootstrapping and jackknifing to the gbdp approach,  better performing ddh prediction models and the cis they provide, and,  direct calculation of the probability that an intergenomic distance yielded a ddh larger than 70%, the here presented methods provide an attractive alternative to the wet-lab ddh for current taxonomic techniques. the addition of novel distance functions  was also beneficial here, particularly in conjunction with the novel resampling techniques and with respect to the resulting correlations with ddh.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
jmk participated in the design of the study, carried out the experiments, performed the  analysis and wrote the manuscript. afa contributed software methods to this study and helped carrying out the experiments. mg and hpk designed and conceived the study. mg also participated in writing the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
empirical data sets. csv file holding the empirical data sets used in this study. the file can be accessed with spreadsheet programs  or any given text editor.

click here for file

 additional file 2
overview on the examined gbdp input parameters. pdf file holding a table about all gbdp settings/input parameters tested in this study.

click here for file

 additional file 3
all distance formulae used by gbdp. pdf file holding a table with all distance formulas implemented in gbdp.

click here for file

 additional file 4
results of the correlation analysis. spreadsheet in open document format  that can be accessed via common spreadsheet programs . the spreadsheet contains several tabs, each one holding the results for the data sets ds1-ds <dig> .

click here for file

 additional file 5
input for the multiple linear regression analysis. spreadsheet in open document format  that can be accessed via common spreadsheet programs . the spreadsheet contains the input data required for reproducing the multiple linear regression analysis.

click here for file

 additional file 6
additional figures. pdf file holding all figures that did not fit in the main manuscript, although these help to further elucidate the study and its results.

click here for file

 additional file 7
ddh prediction-based intergenomic distances. csv file holding sample ddh predictions under different statistical models as analyzed in this study. the file can be accessed with spreadsheet programs  or any given text editor.

click here for file

 acknowledgements
cordial thanks are addressed to marek dynowski and werner dilling, both zentrum für datenverarbeitung, university of tübingen, for granting access and for their technical support related to the compute clusters of the bwgrid  <cit> . the authors thank lea a.i. vaas, cbs, utrecht, for helpful suggestions.
