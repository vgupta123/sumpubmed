BACKGROUND
in order to gain an insight into the appropriateness of argumentation for the wider biological world this work concentrates on employing argumentation within the field of in situ gene expression. this section contains a short introduction to argumentation, followed by a description of the relevant previous work. initially the focus is on in situ gene expression information and the areas in which argumentation may be fruitfully applied.

gene expression information, inconsistency, and incompleteness
gene expression information describes whether or not a gene is expressed  in a location. broadly speaking there are two types of gene expression information: one that concentrates on where the gene is expressed, and a second whose primary concern is the strength of expression. throughout, this work focuses on the former category, in particular a technology called in situ hybridisation gene expression.

information on gene expression is often given in relation to a structure - or it may be given for a point in 3d space - in a particular model organism. here the model organism of interest is the mouse. this organism is studied from conception until adulthood. the time window is split into twenty-eight theiler stages. each stage has its own anatomy, and corresponding anatomy ontology called emap  <cit> . the first twenty-six stages cover the developmental mouse: the mouse from conception until birth. stage  <dig> is the new born mouse, and  <dig> the adult.

the result of an in situ experiment is represented as an image displaying a slice  of a mouse  in which some subsections of the mouse are highly coloured. areas of colour indicate that the gene is expressed in that location. in addition to showing where the gene is expressed, the image provides some indication of the strength  of expression. the more intense the colour, the stronger the expression.

result images are analysed manually. a human expert determines in which structures the gene is expressed, and at what level of expression. as strength information is not the main focus of the experiment, its description is often vague using loose natural language terms such as strong, moderate, weak or present. for example, the gene bmp <dig> is strongly expressed in the future brain at theiler stage  <dig> 

once an in situ gene expression experiment is completed the experiment may be published in a traditional journal. regardless of whether or not this is true, the experiment usually will be published by one  online resources. two of the main resources in the current domain of interest are emage  <cit>  and gxd  <cit>  - both use the emap anatomy ontology. these online databases publish so-called annotations that contain particular types of information: provenance, details of the technique used, analysis of the result, and perhaps some indication of how reliable the resource believes the experiment to be. it is possible to supply information directly to the resources, and thus omit the traditional journal publication. often such a route is favoured by large-scale projects that conduct a great number of experiments.

although emage and gxd are substantial resources they cannot be considered complete  <cit> . there is a range of reasons for this phenomenon including: some large scale projects publish their own results in a proprietary database, some experiments are deemed of insufficient quality by the resource curators, and others simply 'slip under the radar'. consequently, in order to build as complete a picture of the domain as possible, it is necessary to consult multiple resources.

in addition to being incomplete, online biological resources are often inconsistent  <cit> . in terms of gene expression, this means that the same resource publishes one annotation suggesting the gene is expressed in a particular structure, and a second annotation suggesting it is not. as biologists treat absent and expressed as mutually exclusive, this implies an inconsistency. due to the complexity of the underlying experiments there are a number of possible reasons for the different results, including: differences in the interpretation of results, unrecognised differences in the experiments, and human error . as discussed previously, it is necessary to use synchronously multiple resources, but doing so raises the prospect of inconsistency between those resources, in addition to the inconsistency inside each resource.

although there are many different methods to tackle the issues described, the use of argumentation  <cit>  is considered in this work.

argumentation
argumentation  <cit>  is a multidisciplinary field that studies arguments and arguing.

an argument is a reason to believe something is true. this may be a formal proof, or a piece of natural language: for example, a reason to carry an umbrella. the crucial attribute of an argument is its defeasibility: an argument may provide a reason to believe something is true, but it does not prove it is definitely true.

defeasibility is important because arguments can be in conflict: one argument may suggest a conclusion is true whilst a second argument intimates that the same conclusion is false. often it is desirable to make a decision about the validity of a conclusion. as the evidence and corresponding arguments change over time, it may be necessary to revise that decision. defeasibility allows this to happen.

arguing  is the process of using arguments to justify a point of view. this process may take place between multiple agents  inside a debate, or it may be carried out by a single agent: e.g. a political speech justifying the government's decision to increase taxes.

the subdomain of computational argumentation involves the use of computers for constructing and using arguments. there is a wide range of domains in which argumentation has been applied including: artificial intelligence & law  <cit> , ontology matching  <cit> , medical decision support systems  <cit> , and agent communication  <cit> .

argumentation in relation to biology is surprisingly rare. jefferys et al.  <cit>  use argumentation to analyse the output of a protein prediction tool. however, most work involves pedagogical efforts to improve the construction of natural language scientific arguments by students, e.g.  <cit> .

although argumentation is rare in biology, it is common within the medical world and is employed in a wide range of decision support systems, for example  <cit> . additionally, argumentation can be used to persuade patients to change their behaviour  <cit> , and generate pamphlets to explain complex issues to patients  <cit> . although clear differences exist between medicine and biology, there are many parallels. if argumentation has been shown to work well in medicine, it should be relatively successful in similar domains, e.g. biology.

arguing over gene expression information
initial attempts to tackle the twin problems of inconsistency and incompleteness are documented in length in previous work  <cit> ; a brief reprise is given here.

mcleod et al.  <cit>  describe an earlier system designed to tackle the above problems. in effect, this system allows a user to enquire if a gene is expressed in a particular anatomical structure from an individual theiler stage. doing so causes the system to generate a number of arguments, evaluate those arguments, and present the results  and associated evaluation) to the user.

for the arguments to be meaningful, they have to be based on expert knowledge - the expert employed for this task was the senior editor of emage. expert knowledge is captured in a series of natural language inference rules, so-called argumentation schemes  <cit> . essentially, these schemes provide a natural language if-then  rule and an associated series of questions that can be asked to ensure the rule's application is suitable in the current context. additionally the expert assigns a degree of confidence to each scheme.

the natural language schemes are converted into a logical form using the method described by verheij  <cit> . the logic in question is a prolog-like logic employed by the aspic argumentation engine  <cit> . this tool provides a means to generate arguments, and conduct a virtual debate between two agents in order to determine which argument is strongest. arguments are created by the aspic argumentation engine using the rules, and biological facts .

the rules inherit the confidence values of their schemes, and likewise the arguments take the confidence values from their rules. when two arguments are in conflict, the engine compares the confidence values, and declares the argument with the higher value to be valid.

in mcleod et al.  <cit>  the arguments are converted back into natural language and presented to the user, the presentation mechanism experts deemed most suitable. figure  <dig> part a shows a screen shot of the results page: two arguments are displayed using one of aspic's in-built presentation mechanisms. both arguments are undefeated, which means the system believes them to be true. unfortunately, the default presentation style uses a mixture of natural language and logic rendering it unsuitable for use by the expected user group.

subsequent work concentrated on the development, and evaluation of an improved interface . this time the arguments are presented entirely in natural language, and preceded by an image that summarises the argumentation, which is below a single line conclusion .

sutherland et al.  <cit>  discuss the inclusion of this work in a semantic web browser for the life sciences.

full details of the evaluation can be found in ferguson et al.  <cit> ; though, mcleod et al.  <cit>  publish the key findings. in particular, the following issues are prominent:

subjectivity - this affects the final conclusion , the schemes, and their associated degrees of belief;

breadth of resources - there is a need to include other gene expression resources for the developmental mouse, preferably featuring different techniques for measuring gene expression.

RESULTS
in  <dig> work began to generate a real world tool to help tackle the issues of inconsistency and incompleteness in relation to in situ gene expression data for the developmental mouse - this work is undertaken as the argudas project. argudas is an evolution of the work described previously, and in  <cit> . in the former work two prototypes were developed, see figure  <dig>  during argudas, the system has been refined further through the development of two additional implementations; each with a similar, yet distinct, user interface.

this section will explore the knowledge gained from the previous work, and the ways in which argudas is informed by those examples. the discussion starts with an examination of the issue of subjectivity, then progresses to investigate the breadth of the resources included in argudas, and concludes with a review of the issues highlighted by the first argudas prototype.

tackling subjectivity
one of the problems with the previous implementations is the notion of subjectivity. during the evaluation it was clear that each subject had their own approach to interpreting the information contained within emage and gxd. accordingly, users disagreed with the system in three key areas: the argumentation schemes, the confidence values assigned to those schemes, and the final conclusion the system presented. each area of conflict will now be addressed in turn.

CONCLUSIONS
at the top of a results page, e.g. figure  <dig> part b, the second prototype presents a conclusion as to whether or not the gene is expressed, e.g. "the arguments appear to suggest the gene is expressed". each time a conclusion is presented some users agree with it, yet others disagree. a plausible reason for this is discussed by jeffreys et al.  <cit> :

different researchers interpret data in different ways, and even the same researcher may make inconsistent interpretations, adding an unreliable and non-uniform element to data processing.

the phenomenon of subjectivity is explored, in relation to argumentation, in the philosophical writings of perelman and olbrechts-tyteca  <cit> . perelman and olbrechts-tyteca introduce the notion of an audience to capture the idea that each person has their own reasoning process, and thus each member of an audience will judge the same argument differently. this means that there is little point in the system trying to decide whether or not the gene is expressed. instead the system must generate arguments for and against the gene being expressed, and allow the user to evaluate these arguments in order to reach their own decision. in effect, the system should aggregate and evaluate data, presenting the relevant data to inform the user's decision making process.

for this reason, unlike earlier work, argudas does not decide whether or not a gene is expressed. instead it aggregates and interprets information before summarising the material for the user.

argumentation schemes
the argumentation schemes are derived from expert knowledge of how to interpret and evaluate the information in the emage and gxd resources. the expert used for this task was responsible for curating the information in emage, and thus interpreted and evaluated the information in these resources on a daily basis.

unfortunately, there is little published research on the creation of argumentation schemes by a domain expert with which to compare the current work. the available literature on schemes often focuses on the dialogue and natural language aspects. the work of silva et. al.  <cit>  is interesting because it starts with already documented "reasoning templates"  and asks a domain expert to use those to explain his reasoning, customising them if necessary, during case-based reasoning. the process of customising the schemes provides a mechanism to help the expert describe his knowledge. however, no such pre-existing schemes exist for the current domain.

shipman and marshall  <cit>  discuss the problems of working with a number of knowledge formalisms, including argumentation. although their work does not focus directly on the application of schemes, a number of their ideas do transfer over, for example:

tacit knowledge is knowledge users employ without being conscious of its use  <cit> . tacit knowledge poses a particularly challenging problem for adding formal structure and content to any system since, by its very nature, people do not explicitly acknowledge tacit knowledge.

the notion that experts are not aware of all their own knowledge presents a massive impediment for all knowledge-based approaches. similar ideas can be found in the work of bliss  <cit> , which suggests that experts develop mental models of concepts and processes that can be very hard for them to access. such knowledge, referred to as deep knowledge, is unarticulated. knowledge that has never been articulated can be extremely difficult for the expert to recall  <cit> . the biological expert, used in this work, was being asked to provide tacit knowledge, and struggled to do so.

degrees of confidence for argumentation schemes
in order for the argumentation engine to argue, it requires not only a series of logic rules  but additionally that each rule has an associated confidence score. these scores, so-called degrees of belief, should record the expert's confidence, in each rule, in such a way that rules with a high value produce better arguments than rules with a low value. the argumentation engine uses the scores to settle conflict, decreeing that when two arguments oppose one another, the argument with the higher value wins.

during the evaluation of the second prototype the biological expert disagreed with his own assignments, which raises questions over the reliability of their capture. however, this is not the only plausible explanation, bliss  <cit>  suggests that mental models naturally evolve as the person gains experience and knowledge, or as a result of the person consciously thinking about their processes and knowledge. consequently, the act of asking an expert to document their knowledge can change that knowledge, and thus render the assigned confidence value redundant.

a further issue to be aware of when dealing with experts, is the reliability of the individuals themselves. as walton  <cit>  notes, the users of expert opinion often are not capable of judging the quality of that opinion and thus simply apply it:

it is quite common for presumptions to be based on expert opinion where the person who acts on the presumption - not being an expert - is not in a position to verify the proposition by basing it on hard evidence within the field of expertise in question.

walton  <cit>  goes on to caution that such experts, or authorities, may not prove to be reliable:

but even when they are nonfallacious, as used in a dialogue, appeals to authority are generally weak, tentative, presumptive, subjective, and testimony-based arguments. they are inherently subject to critical questioning, or even rebuttal, on various grounds - especially on grounds relating to the reliability of the source cited.

walton's opinion is backed by a number of scientific studies, as hansson  <cit>  reports:

experimental studies indicate that there are only a few types of predictions that experts perform in a well-calibrated manner. thus, professional weather forecasters and horse-race bookmakers make well-calibrated probability estimates in their respective fields of expertise  <cit> . in contrast, most other types of prediction that have been studied are subject to substantial overconfidence. physicians assign too high probability values to the correctness of their own diagnoses  <cit> . geotechnical engineers were overconfident in their estimates of the strength of a clay foundation  <cit> .

these quotes combine to illustrate the difficulty in using expert opinion - often it is not correct, and the user has no way of knowing what he can trust. as such, it is conceivable that the expert did not assign the correct confidence values. furthermore, the authors were incapable of verifying the quality of the assignments.

argudas' solution
the previous work demonstrates a need for an improved set of schemes and confidence values. two significant barriers appear to exist: the difficulty in the expert articulating his own knowledge, and the reliability of an expert's opinion. instinctively, the solution to both of these issues is the inclusion of further experts in the process. having a second expert allows ideas to be communicated and evaluated by people in different ways. thus helping to reduce both the subjectivity and the workload on an individual expert making it easier for them to generate the required output.

regrettably, argudas did not have the resources to restart the full scheme generation process. consequently, as described later, argudas employed two experts to review the schemes and assign new confidence values. these amended schemes and values are used in argudas' first prototype .

clearly having two experts means the possibility of two different points of view. hence, when they both agree, the probability of the degree of confidence being accurate increases. disagreement is beneficial too, because through it new insights are discovered. it seems obvious that if the schemes had been produced by multiple experts the range and diversity of the schemes would have been broader. furthermore, if two experts have to agree the natural language used to document the schemes, the number of ambiguous phrases should be reduced.

yet working with multiple experts can cause a number of difficulties. expert biologists are often geographically disparate. this in conjunction with their workload means it may be difficult to bring the experts together. furthermore, there is an obvious requirement for a formal resolution process to help dissect and settle differences of opinion. finally, it must be acknowledged that not all disagreements can be rectified, and that a mechanism for incorporating differences of opinion must exist. these issues point to the requirement for a framework that enables biologists to work together in order to generate the schemes. lindgren  <cit>  is developing such a framework for the use case of dementia care; however, it is still at an early stage, and cannot be employed here.

extending argudas for richer argumentation
argudas aims to improve on previous work with the integration of further resources - more resources means extra information and potentially richer arguments. initially the microarray data contained in the arrayexpress  <cit>  resource was targeted. this highlights a number of integration issues that have yet to be resolved.

firstly, the arrayexpress resource does not use the emap anatomy ontology. secondly, accessing the data held by arrayexpress was difficult as they did not provide a direct programmatic access to their database. instead access was via a restful web service, which provided limited functionality and did not allow access to the data required by this work. initially it was impossible to ask for all the genes expressed in a healthy mouse's pancreas at stage  <dig> because arrayexpress did not compute multi-factor statistics. that is, they computed which genes were expressed in the pancreas and which genes were expressed in stage  <dig> separately and there was no way of presenting the intersection at that time. the team behind the resource were working on improving this interface and claimed that such functionality would become available in the future; however, the delay was problematic because of the time constraints associated with argudas. finally, arrayexpress had less data for the developmental mouse than expected: only three stages were covered. comparing the costs and benefits it was decided not to pursue this integration further.

as work on arrayexpress stopped an investigation of the allen brain atlas   <cit>  and gene expression nervous system atlas   <cit>  began. both of these resources are databases of in situ experiments focusing predominately on the adult mouse's nervous system, i.e. brain. the latter project makes available a full database dump. the former supplies an extensive range of restful interfaces that provide access to the desired information.

however, bringing the data from these two new resources into argudas is problematic. neither resource uses the emap anatomy - as these resources focus on the brain they have a far finer granularity for the brain structures than emap. hence it is necessary to attempt some form of mapping from their respective anatomies to emap. secondly, these resources use their own measures to describe the level of expression, gensat natural language terms and aba floating point numbers, which also must be mapped across to the corresponding emage/gxd terminology. emage and gxd use very similar labels, for example when a gene is not expressed in a particular structure gxd describe the gene as absent whereas emage use the term not detected.

mapping between the different anatomy ontologies employed by the resources is based on a series of alignments produced by jim√©nez-lozano et al.  <cit> . as both gensat and aba have a finer granularity than emap, mapping from those resources to emage/gxd commonly results in a loss of precision.

the second task is straightforward for gensat as their choice of labels is similar to emage's, and in turn gxd's. whereas emage has not detected, detected, weak, moderate, and strong gensat has not done, undetectable, weak signal, and moderate to strong signal.

mapping emage/gxd expression levels to aba is a more complex task. there are three different measures of expression level published by aba. firstly there is the raw experimental information, secondly there is the average information , and finally there is a mathematical aggregation of the expression level and expression density. for current purposes, the first class of information is most suitable. subsequently, the aba generated expression level mappings must be applied. these mappings are a series of cut-offs that determine whether the expression level is not expressed, weak, moderate or strong. there are different limits for different parts of the brain. in order for the limits to be applied to the structures lower down in the anatomy hierarchy, the limits need to be propagated through the brain in a similar manner to the gene expression information.

when this work has been done it is necessary to determine what level of integration is appropriate for these resources. at the most basic level it would be possible to merely report the results contained in aba and gensat. if either of these resources agreed with an annotation from emage/gxd, it would increase the confidence in that annotation. fully integrating aba and gensat would require the generation of schemes for these resources, which is substantially more work and would necessitate involvement from a resource expert. in the case of aba such an approach may not be fruitful; aba does not publish all the data it collects, accordingly some of the attributes provided by an expert may be hidden from the public, and thereby argudas. the restricted resources of argudas meant that only the former option was realistic.

although an interested biologist may raise a number of concerns regarding the anatomy and expression level mappings described above, currently there is no better way of aggregating data between the four resources of interest.

further issues identified during the development of argudas
the first argudas prototype tests using the revised schemes and confidence values. for the reasons discussed before, it removes the final conclusion and instead presents a list of textual arguments. when this initial system was demonstrated to a group of end users they raised concerns over the number of arguments shown and the style of their presentation. these topics shall be explored in greater depth before a possible conclusion is presented.

too many arguments
during argudas' development it became evident that the number of arguments generated varies enormously. for some queries there are no annotations and therefore no arguments. with other queries over ten annotations are retrieved from emage and gxd, accordingly a large number of arguments are produced: arguing for bmp <dig> - future brain in stage  <dig> generates two hundred and fifteen arguments. clearly, no biologist will read all the arguments, hence there can be no guarantee that he will read all the important information. this realisation led to the conclusion that the potential number of arguments is too high, and steps were taken to reduce it.

although all of the arguments are unique in terms of their content  semantically several arguments seem to duplicate one another. identifying semantically equivalent arguments is not a minor task. the definition of equivalent seems to depend on the individual using the system and the biological task they wish to perform.

there are a number of common interpretations and actions that are not appropriate for certain biological tasks, and which individual biologists may, in general, reject. for example, the emap anatomy ontology is defined using part-of relationships. consequently, positive levels of expression are routinely propagated up the ontology to higher level structures; for instance, if bmp <dig> is weakly expressed in the telencephalon, it is normally correct to say that bmp <dig> is weakly expressed in the future brain. nevertheless, many biologists prefer direct annotations over propagated ones, thus if a second annotation suggests bmp <dig> is not detected in the future brain, the second annotation would take precedence.

likewise, there is a similar problem with the granularity of information desired. finding two distinct annotations with the same conclusion is a powerful argument for trusting the conclusion. however, the granularity of information desired affects the decision as to whether or not the annotations are in agreement. imagine there are two annotations: one annotation suggests bmp <dig> is strongly expressed in the future brain, and a second annotation demonstrates bmp <dig> is weakly expressed in the future brain. if the biologist is attempting to determine if the gene is expressed or not expressed, then these annotations may be taken to agree. yet, if the aim is to determine the level of expression, these annotations are conflicting.

the goal of reducing the number of possible arguments is further hindered by a request for more positive aspects to be highlighted. for instance, although an argument is created when the probe  information is absent for an experiment, no argument is created when it is present.

in summary, argudas' users appear to wish for a broader range of prospective arguments, and yet a smaller number of realised arguments. reconciling these competing aims seemed improbable, until someone remarked that the problem was not the volume of the arguments but the amount of text to be read. it transpired that a significant number of potential users wanted to scan information rather than read it.

the notion of argument reconsidered
previous work, and the initial version of argudas, use the aspic argumentation engine to generate and evaluate arguments inside a virtual debate. these arguments are presented to the user as a natural language paragraph - this display mechanism is chosen as it is the preference of the original expert. however, feedback suggests this choice is subjective  <cit> . furthermore, the potential for a large number of arguments to be generated appears to imply that the original approach is sub-optimal. there is a clear need to find an alternative method for displaying arguments.

during internal discussions it was proposed that the argumentation mechanism should be reconsidered. this approach was based on the belief that users wanted quick access to certain key attributes of the annotation. theoretically there is no need to employ the argumentation engine to create and evaluate arguments. instead, the most important schemes , should be the basis for a range of key attributes that describe the annotation. the schemes indicate whether or not the information stored in emage/gxd for a particular annotation should increase or decrease a user's confidence in that annotation. as such, argudas should extract information from the resources and present it, with associated key highlights, to the user. it then becomes the duty of the user to evaluate the information.

in order to test this hypothesis two mock interfaces were created and evaluated. there are three steps to each interface. the first two steps are the same: select a gene and/or structure of interest; report on the available annotations and allow the user to ask for more information if desired. figure  <dig> shows both of these: initially the query is bmp <dig> - future brain in all stages; the query causes all combinations of the gene and structure to be displayed in a table. the table presents all relevant annotations, summarises what each annotation shows, and provides a link to the resource's web page for that annotation.

in some situations the table in figure  <dig> will be enough to resolve a biologist's question; i.e. it is clear that bmp <dig> is expressed in the future brain in stage  <dig>  on the occasions when the table is not helpful, or does not provide enough information, clicking the argue button provides a range of arguments.

in the first mock interface a number of textual arguments are displayed - in a similar manner to figure  <dig> part b. the second interface can be seen in figure  <dig> - the 'arguments' are now a list of key attributes such as multiple annotations agree. whether or not an attribute should strengthen a user's confidence in the annotation is indicated with a tick or cross. the attributes are divided into two layers - firstly by expression level, and then by annotation. for each level of expression there are three attributes that indicate how likely that level of expression is. asking for more information causes the second layer of attributes to appear. this allows the user to evaluate the annotations individually, and collectively as a group that promotes a specific expression level.

argument presentation reconsidered
the mock interfaces, shown in figures  <dig> and  <dig>  were explored with the help of two expert users. during this small informal exercise , the test users were kept apart; however, they reached identical conclusions:

 <dig>  the revised interface, using key attributes rather than textual arguments, is an improvement;

 <dig>  a further improvement could be made by placing the content of figure  <dig> into a table.

separately, the experts both had the idea contained in figure  <dig>  effectively, the important schemes become columns, with individual annotations represented as rows. ticks indicate positive aspects that suggest the annotation is more likely to be correct, with crosses having the opposite semantics. blue dashes inform the user that a piece of information is unavailable.

enacting these recommendations has a major side effect: there is no need to perform computational argumentation. as such, there is no need for the argumentation engine. instead the user now performs the argumentation themselves using the aggregated and curated information presented by argudas.

this change helps address the issue of subjectivity, because the user is now able to apply his/her own confidence values and criteria to the decision making process. furthermore, because the user is able to build his/her own arguments, argudas becomes a more flexible tool. for example, it is now possible to use argudas to decide where, and at what level, a gene is expressed.

implementing these changes results in the second, and current argudas system . one addition to the advice offered by the expert users is the inclusion of mouse-overs to present extra information. discrete dots underneath a tick/cross, or column title, indicate that more information is available if the user hovers his/her mouse over the dots. for example, hovering the mouse over the dots in the cell for 'strong - multiple annotations agree'  reveals the experiment that suggest the gene is strongly expressed.

the evaluation of this system, described later, demonstrates that the changes are both appropriate and effective because they allow users to quickly scan the important information.

future work
this work set out to model expert knowledge and use it to reason with information sources available through the internet. in the current use case, this appears to be beyond the scope of what a typical end user wishes. however, the current use case is relatively constrained, and thus contained: it focuses on one kind of gene expression information for one model organism. extending the use case to include different types of biological information, for example gene regulatory networks, makes the use case considerably more complex. as the intricacy of the biological investigation increases, the need for user support likewise increases. argumentation is one possible support mechanism.

another avenue for future work relates to cubist, combing and uniting business intelligence with semantic technologies, an eu fp <dig> project that aims to combine the essential features of semantic technologies, business intelligence, and visual analytics. data from both unstructured and structured sources will be federated within a business intelligence enabled triplet store, before visual analysis techniques such as formal concept analysis  <cit>  are applied. one of the project's three use cases involves the gene expression data described in this paper. although it is early in the life of the cubist project, a semantic extract transform load  <cit>  process that includes computational argumentation may be envisioned. the inclusion of argumentation may provide an intelligent transformation of data, and a user-friendly explanation of the transformation.

