BACKGROUND
the availability of whole genome assemblies  <cit>  and the development of bioinformatics tools and interfaces  <cit>  for their analysis, enable data-mining and comparison of these large genomic datasets. ultraconserved elements are nucleotide or protein sequences with 100% identity  in the same organism or between two or more organisms. a recent comparison of several vertebrate genomes demonstrates that, in addition to coding, non-coding sequences can be highly conserved between species  <cit> . approximately 5% of the human genome is under negative selection, indicating conservation of sequence due to functional necessity. these functional regions are conserved since random mutations that would negatively effect functionality would be rejected by natural selection. consequently, orthologous functional regions would be more similar between different genomes. genome comparison has become a vital tool in the identification of these conserved functional elements. of the 5% of the human genome that is under selection, only  <dig> % codes for protein sequences  <cit> . surprisingly, the most conserved regions amongst vertebrates are in non-coding sequences. these ultraconserved elements are significantly more conserved  than would be expected across genomes. though these ultraconserved elements seem to be specific to vertebrates, they are present in other metazoan genomes as well  <cit> . studies indicate that these conserved regions are associated with micro rnas, mrna processing, development and transcription regulation  <cit> . the identification and characterization of these elements among genomes is necessary for the further understanding of their functionality. multiple organism whole genome comparative analyses such as these would elucidate the fun! ction of these conserved elements and their importance in various evolutionary lineages.

in this paper, we describe an algorithm, a set of freely available software programs, and a workflow for finding ultraconserved elements for multiple organisms. we explicitly design our algorithm to take both computational time and memory space in to account such that it can handle genomes of any size as well as take advantage of parallel computation on typical computer clusters. our algorithm and workflow provides a case study showing that by focusing on a specialized task, computation can be orders of magnitude more efficient; this allows previously impractical, but highly desirable, whole genome comparative analysis to be performed.

RESULTS
algorithms that perform multiple organism comparative genomics must explicitly deal with the issue of large genomes. most vertebrate genomes, such as human, mouse, chicken, etc., approach the maximum size of addressable memory space for 32-bit processors . 64-bit processors which have sufficiently large addressable memory space  generally do not have more than 16– <dig> gb of actual physical memory due to technology limitations or because the cost is prohibitive. this physical memory bound will increase in the future as memory capacities increase, but for the near future, the generation of genomic data is increasing at an even faster rate. even if all the genomic data can be brought into memory, there still needs to be space for data structures that organize the data for algorithmic purposes; these data structures are often multiple times larger than the raw genome data.

algorithm
our algorithm for finding the longest ultraconserved sequences for multiple organisms breaks up the task into smaller, manageable subtasks which in most cases can all be run in parallel on a computer cluster. the first step is to prepare the genome data into appropriate size data files. if the genome has already been mapped to chromosomes then the individual chromosome sequences provide a natural division; otherwise, the genome is probably provided as one large fasta file containing all of the assembled scaffolds. the next step is to generate a suffix array data structure for each data file. the main algorithm then takes two suffix arrays and produces a list of maximal common prefixes  which correspond to ultraconserved sequences; this is done in a pair-wise fashion for all suffix arrays of the two organisms. the union of the mcp files produces the final mcp file for the two organisms. the workflow is summarized in figure  <dig>  for multiple organisms, each final mcp file for a pair of organisms is intersected within another final mcp file for a different pair of organisms; this is repeated in tournament-style fashion producing an mcp file for all the organisms. lastly the mcp file can be trimmed to remove overlaps and retain just the longest sequences. figure  <dig> shows the tournament-style intersection for multiple organisms. we describe each step in detail in the following sections.

prepare genome data
the genome data for each organism can be split into appropriate size sequence files to facilitate parallel execution of many of the programs in the workflow. if the genome has already been mapped to chromosomes then the individual chromosome sequences provide a natural division. otherwise, the genome is probably provided as one large fasta file containing all of the assembled scaffolds; in which case, that single file may be split into several smaller files. we provide a simple utility, split_fasta, which will split up a fasta file containing numerous sequences into several files. the largest memory requirement is incurred when the genome data is processed into suffix array data structures. the following calculation can be used to determine how much memory will be required for each strand:

 memory = sequence_size + ) 

where sequence_size is the number of nucleotide base pairs in the sequence. the size of the integer data type, ), on most platforms is  <dig> bytes. the first term is the size of the sequence, and the second term is the size of the suffix array which is doubled for temporary storage required by the sorting process. for example, human chromosome  <dig> is ~ <dig> mb which gives the memory requirement:

 memory =  <dig> mbp +  =  <dig>  gb 

when constructing the suffix array from the sequences , any size sequence file can be accommodated regardless of the available physical memory up to the virtual memory limit of the machine. however if the memory usage as required by the above equation exceeds physical memory then construction of the suffix array will take longer, so to maximize efficiency the size of the sequence files can be reduced so that the processing of each individual file fits completely into the physical memory of the machine. this memory usage is only for the initial creation of the suffix array; afterwards the suffix array is stored on disk and memory requirements are minimal for the additional processing steps of the workflow. there is no limit to the number of individual files that can be processed with the algorithm, so it is better to have many smaller files than just a few large files.

generate suffix array data structures from genome data
we process each sequence file into a specialized data structure called a suffix array. a suffix array  <cit>  looks at the sequence as one long string, takes all possible suffix strings, and sorts those suffix strings in lexicographical order. a suffix is a trailing end substring, so a string of length n has n unique suffix strings of length n, n- <dig>  n- <dig>  etc. we use an implementation for computing a suffx array by sean quinlan and sean dorward  <cit>  that can sort all of the suffix strings in-place using offsets into the full string thus making for efficient sorting, though the whole suffix array does need to fit into memory. if the sequences and resulting suffix array are too large for the available physical memory, then multiple suffix arrays are created on partitions of the sequence data, and those multiple arrays are merged together to form a single result file. the resultant suffix array is a list of offsets into the original sequence string. figure  <dig> shows an example string with its corresponding suffix array. we use a suffix array instead of a suffix tree because it is more amenable for storage and access in a file; thus we do not require the whole data structure to be present in memory.

we provide two programs for constructing the suffix array for a sequence, construct_sarray and construct_prot_sarray. the first program takes a nucleotide sequence, concatenates the sequence for the reverse strand, computes the suffix array, and then writes the suffix array along with some reference data to files. the other program takes a nucleotide sequence, translates codons according to a user-specified genetic code translation table into amino acids for the three reading frames on the forward strand and the three reading frames on the reverse strand, computes the suffix array, and then writes the suffix array along with some reference data to files. both programs take into account repeat masked sections of the sequence, whether masked with n's or lower case nucleotide letters, by removing suffix strings that start with repeat masked nucleotides or by not translating those sections.

each individual sequence file is processed into its own suffix array, so the human genome with  <dig> chromosomes plus the x and y chromosomes would be processed into  <dig> separate suffix array files. if the human genome was also translated then there would be another  <dig> suffix array files for the translated nucleotide sequences. the primary tradeoff is between the use of disk space in exchange for lower memory requirements, but with appropriate data structures the file-based computation is very efficient. there is no dependence between suffix arrays for different sequence files, so all of the suffix arrays can be computed in parallel on a typical computer cluster.

compute maximal common prefixes between two organisms
the maximal common prefix  for two lists of strings is the problem of finding a string in one list that has a common prefix with a string in the other list, and that common prefix is the maximum length for all possible choices of strings. a naive implementation would require each string in one list to be compared to all strings in the other list resulting in quadratic o running time. however with our strings in sorted order in a suffix array, we can compute the maximal common prefixes with just a linear scan through both suffix arrays. such an algorithm can find not only the single mcp but all mcp's for all strings.

we provide the program, mcp_sarray, which given two suffix array files will output all maximal common prefixes greater than or equal to a user-specified length. these maximal common prefixes correspond to ultraconserved sequences as each mcp is an equivalent sequence that appears in both organisms. the program works for both nucleotide and protein sequences. in the first step, the organism's genome was separated into appropriate size sequence files, so we have multiple suffix array files for the organism. finding all of the mcp's requires running the program in a pairwise fashion with all suffix array files for both organisms. for example, the human genome has  <dig> suffix array files and the mouse genome has  <dig> suffix array files, so pairwise application will generate  <dig> resultant mcp files. the program, union_mcp, will combine together these individual files into a single resultant mcp file. one may consider combining all of the suffix array files for an organism into a single suffix array so that the mcp_sarray is just run once; however there are considerably fewer mcp's making the mcp files much smaller than their corresponding suffix array files, so it is more efficient to combine results afterwards rather than before. furthermore, there are no dependencies between the pairwise mcp_sarray programs, so they can be executed in parallel on a computer cluster which is faster than just two large suffix array files. the pairwise mcp files can be considered temporary files and deleted once they are combined together into a single file. however, care should be taken when running many programs in parallel as disk i/o can quickly become a bottleneck if the suffix array files are stored on shared disk space. efficiency can be increased by copying the suffix array files to local disk on each compute node, and the mcp_sarray program attempts to bring in sequence data into memory as a tradeoff between memory and disk i/o.

compute maximal common prefixes for multiple organisms
computing the mcp's for multiple organisms involves taking the mcp file for two organisms and intersecting it with the mcp file for two other organisms, thus producing the maximal prefixes common to all four organisms. this can be repeated in a tournament-style fashion for any number of organisms. similar to the algorithm for computing the mcp's between two organisms, the intersection algorithm for multiple organisms can be performed with just a linear scan through both mcp files. we provide the program, intersect_mcp, which given two mcp files will output all maximal common prefixes that are present in both input files.

the format of the mcp file is similar to a suffix array but requires additional data to support any number of organisms. this additional data includes references to a particular sequence for each organism, an offset into each of those sequences, and a length corresponding to the current maximal common prefix. the sequences in the mcp file are maintained in sorted order. intersection of two mcp files proceeds with a linear scan through both files, determining the maximal common prefix for each entry along with a new possibly shorter length, then combining all of the organism sequence data together into the output mcp file. because an mcp is common to all of the sequences, only one sequence file needs to accessed for each mcp file; however all of the additional data for the multiple organisms is carried forward for easy reporting of final results.

trim maximal common prefixes to produce final report
if our focus was purely on comparing two organisms then the algorithm to compute mcp's can retain just the longest sequence found. however with multiple organisms, when intersecting two mcp files, resultant mcp's may be shorter or substrings of the those in each individual file. therefore, not only is the longest mcp stored in the file but also all of its suffix sequences down to the minimum length. if the intersection of two mcp files produces a match that is shorter, the suffix sequences maintained in sorted order allows the intersection process to be performed efficiently. at the end of the workflow, when all of the organisms have been intersected together, the shorter suffix sequences are no longer required. we provide the program, trim_mcp, which given an mcp file will produce an output mcp file with all shorter suffix sequences removed, leaving only the longest mcp's corresponding to the longest ultraconserved sequences shared by all the organisms; it also sorts the mcp's by length so that the longest sequences are displayed first. the program, print_mcp, will print out all of the mcp's in a given file in either a compact summary form, in fasta format conducive for input into other programs, or in comma-delimited format for easy input into a spreadsheet program.

testing
we ran our algorithm and workflow on a number of example biological case studies to characterize the scalability and efficiency at which multiple organism whole genome comparative analysis can be performed. we describe some of the current algorithmic techniques and tools available, and compare our algorithm against three tools: blast, mummer, and vmatch. our algorithm outperforms or is similar to them in computation time and memory usage while providing additional benefits for multiple organism analyses not provided by any of the tools.

ultraconserved sequences for  <dig> vertebrate genomes
we used our algorithm to reproduce the results of bejerano et al. <cit>  by finding the 100% conserved nucleotide sequences between human, mouse, and rat. our algorithm successfully found the  <dig> segments longer than  <dig> bp and  <dig> segments longer than  <dig> bp in one day of computation on a modest computer cluster .

since the report by bejerano et al., additional vertebrate genomes have been sequenced and assembled; currently there are  <dig> vertebrate genomes available . we used our algorithm to find the 100% conserved nucleotide sequences common to all of the genomes. the programs took about a week to run on our computer cluster producing  <dig> sequences longer than  <dig> bps with the longest being  <dig> bps mapping to the 40s ribosomal protein s <dig> .

all vertebrate genomes with the given assembly data where downloaded from the ucsc genome bioinformatics site  <cit> . sizes from ncbi genome project web pages or approximated from downloaded assembly data.

ultraconserved sequences within the human body louse and between three insect species
the human body louse, pediculus humanus humanus, is an insect with a recently assembled genome of size slightly larger than  <dig> mbp. as part of initial analysis of the genome, we ran our algorithm for the genome against itself and pairwise against three other insect genomes including drosophila melanogaster, anopheles gambiae, and apis mellifera. we found  <dig>  ultraconserved sequences that occur in multiple places within the louse genome; 85% of these correspond to repeat and low complexity regions which were not masked in the genome. we separated out the repeat sequences and separated out those sequences which overlapped the initial gene build for the genome, leaving  <dig>  ultraconserved sequences longer than  <dig> bps. the longest sequence is  <dig> bps with thousands of sequences that are longer than  <dig> bps. work is continuing to also separate transposable elements from the list to get an accurate identification of non-coding ultraconserved elements in the body louse genome.

pairwise comparative analysis with the three other insects produced  <dig> total ultraconserved sequences between  <dig> and  <dig> bps in length. after separating sequences that overlapped both with the louse gene build and the other insect's genes, we are left with  <dig> sequences. the total analysis for comparing the body louse against itself and against the three insects took just one day of computation time . the speed with which our algorithm performed clearly shows that such comparative analysis can be easily performed as part of the initial analysis of newly assembled genomes to identify potentially functional non-coding elements.

comparison to other methods
comparison of genomic data is typically phrased in the terminology of aligning sequences whether this be local alignment versus global alignment, pairwise alignment for two sequences, or multiple alignment for multiple sequences. furthermore, comparison is often in the context of protein coding sequences which implies that approximate computation techniques are required to find matches in the face of nucleotide mutations and degeneracy in the genetic code. comparison techniques vary with tools like blast  <cit> , blastz  <cit> , blat  <cit> , and patternhunter  <cit>  allowing for relatively short query sequences to be aligned against a sequence database containing one or many large sequences; the focus of these tools are concerned primarily with performing adequate approximate matching in reasonable computation time. they tend to ignore multiple organisms and whole genome comparison requires the genome to be chopped up into small query sequences. whole genome alignment tools like waba  <cit>  and lagan  <cit>  including those that support multiple alignments like multi-lagan  <cit>  and mavid  <cit>  adequately consider handling large genome data; however because of their approximate matching techniques, they are computationally expensive for the simpler task of finding ultraconserved sequences. for this reason, tools like mummer  <cit> , reputer  <cit>  and its successor vmatch  <cit>  use data structures such as suffix trees and suffix arrays that allow for efficient computation of exact matches. regardless not all of these tools  handle sequences in a memory-efficient manner and none provides direct support for a multiple organism whole genome workflow.

blast
with careful selection of parameters, specifically by disabling gaps, setting a large mismatch penalty, and optionally turning off masking of low complexity regions, blast  <cit>  can be used to find 100% matches between two sequences. we ran some tests to compare the results, efficiency, and ease of use of our algorithm with blast. all tests were run on an idle powermac dual g <dig>  <dig>  ghz with  <dig> gb memory. we used  <dig> as the lower length bound for sequences.

for the first test, we compared chromosome  <dig> of the arabadopsis thaliana nuclear genome to its chloroplast. a blast database of chromosome  <dig>  was made and the chloroplast genome  was the query sequence. both blast and our algorithm performed the comparison very quickly, under one minute. our algorithm returned  <dig> sequences while blast returned  <dig> sequences; the discrepancy in the results involved the forward and reverse strand. generally one expects double the number of results because an ultraconserved sequence on one strand will also have an ultraconserved sequence on the complement strand, so the  <dig> sequences found by algorithm are  <dig> sequences on one strand and  <dig> on the complement. blast actually found all  <dig> sequences in a mix of forward and reverse strand results, but for some reason only managed to find  <dig> of the  <dig> complement sequences. it is possible that the results for the two strands can be different, when the ultraconserved sequence occurs in multiple places, as the leading and trailing ends may provide a longer match in one place versus another, so searching for matches on just one strand is technically not accurate though it is a good approximation. this situation did not occur with this test.

for the second test, we compared chromosome  <dig> of the chicken genome  with chromosome  <dig> of the human genome . a blast database was made with the human chromosome and the chicken chromosome was the query sequence. blast ran for approximately two hours before crashing with an out of memory error, no results were reported. our algorithm ran for  <dig> minutes and produced  <dig> sequences.

to resolve the memory issues with blast programs, the query sequence can be split into small segments, woolfe et al. <cit>  used  <dig> mb segments for comparing fugu rubripes to human, which are individually aligned and the results combined. difficulties with this approach include providing sufficient overlap of segments so sequences on the border are not missed and the large number of results to be combined. regardless this approach is computationally very expensive, we estimate blast would require around  <dig> hours to compare chicken chromosome  <dig> with human chromosome  <dig>  and numerous result files would need to be parsed and combined to produce the final results.

handling multiple organisms with blast is not implicitly supported but could be handled in a similar tournament-style fashion as with our algorithm. the results from running blast for two organisms would be combined together and formated into a blast database; the results from two other organisms are then used as query sequences against that database. this process would be repeated for as many organisms as desired. this would require an additional program to be written to parse and process the blast output; an additional step not required by our algorithm.

mummer  <dig> 
mummer  <cit>  is a set of programs for rapidly aligning genomes that is similar to our algorithm in capability. it constructs a suffix tree data structure  <cit>  for a given reference sequence, then the tree is traversed for a given query sequence to produce maximally match; these matches correspond to ultraconserved elements. suffix trees are similar to suffix arrays but slightly more efficient because they can be constructed in o time of the sequence length versus o for suffix arrays due to the sorting process. mummer's disadvantage is that it requires the whole tree to be present in physical memory. mummer  <dig>  with its efficient suffix tree implementation requires  <dig>  bytes of memory for each base pair, so the largest human chromosome  <dig> at  <dig>  mbp requires  <dig> mb of memory; the very edge of addressable memory for 32-bit processors. with the finalized sequencing of the human genome filling in the gaps, chromosome  <dig> is now the largest at  <dig> mbp and is too large for mummer to fit in  <dig> gb of memory. various projects  <cit>  have explored the issues of organizing suffix trees on disk.

we performed similar tests with mummer as with blast, comparing chromosome  <dig> of arabadopsis thaliana to its chloroplast. with the nuclear genome as the reference sequence, mummer took  <dig> seconds while with the chloroplast genome as the reference sequence, it took only  <dig> seconds. our algorithm was also sensitive to which sequence was used for the reference sequence taking  <dig> seconds for the nuclear genome and  <dig> seconds for the chloroplast genome.

for comparing chromosome  <dig> of the chicken genome to chromosome  <dig> of the human genome, we tried each as the reference sequence to mummer but in both cases the program aborted with an out of memory error  during the construction of the suffix tree. therefore, we ran our tests on a 64-bit sun dual opteron  <dig>  ghz with  <dig> gb memory. execution speed for mummer and our algorithm were about equal, both taking  <dig> minutes to compare the two chromosomes irregardless of which chromosome was used as the reference sequence. from watching the programs using the unix top command, our algorithm used only  <dig> mb memory  while mummer used  <dig>  gb memory.

as expected, the results show that if sufficient memory is available then the suffix tree data structure is faster than the suffix array. for large genomes, the difference is less apparent and our algorithm provides comparable execution time with a much smaller memory footprint. the lower memory requirement allows us to run two instances of our algorithm on each cluster machine taking advantage of both available processors. this can be especially important in the future with newer processors having multiple execution cores.

our algorithm requires that the sequence data first be processed into a suffix array file. for arabadopsis thaliana this was immediate for the chloroplast genome at under one second and took  <dig>  minutes for the nuclear genome. chicken chromosome  <dig> took  <dig> minutes to build the suffix array and  <dig> minutes for human chromosome  <dig>  we point out that the cost for generating the initial suffix array is amortized across all of the comparisons that use the array, as the generation is only performed once; furthermore, multiple suffix arrays can be generated in parallel using a computer cluster.

vmatch
vmatch  <cit>  is full-featured suite of tools that subsumes reputer  <cit>  and has improved time and space complexity through the use of enhanced suffix arrays  <cit> . vmatch offers a number of matching and post-processing options for finding equivalent string matches between sequences including maximal unique , tandem, supermaximal, and complete matches, while post-processing provides inverse output , masking, and clustering. it operates similar to blast and other tools whereby a database  is constructed from one sequence set then query sequences are matched against that database. vmatch can also perform matching of a sequence against itself. like our algorithm, vmatch uses suffix arrays which have better memory efficiency than suffix trees.

vmatch was efficient in the construction of its database files requiring just a second for the arabadopsis thaliana chloroplast,  <dig> minute for chromosome  <dig> of arabadopsis thaliana,  <dig> minutes for chicken chromosome  <dig> of the chicken genome, and  <dig> minutes for human chromosome  <dig>  however, we note that vmatch only indexes one strand of the sequence and does not appear to allow both strands to be indexed together in its database files; though the reverse strand can be indexed into its own database. analysis of ultraconserved elements requires finding matches against the forward and reverse strand separately, as well as provide forward and reverse query sequences, then combining the result files together. in our tests, use of the reverse strand index file produced no results requiring us to construct a reverse complement sequence first then create a database file with it, so in subsequent analysis, we just performed a single computation on the forward strand.

vmatch took  <dig> seconds to find the  <dig> ultraconserved sequences for the forward strands of the arabadopsis thaliana chromosome  <dig> with its chloroplast. vmatch also showed different running times depending upon which sequence was used as the query versus the database. for comparing chromosome  <dig> of the chicken genome to chromosome  <dig> of the human genome, vmatch took slightly over  <dig> minutes with the human chromosome as the query sequence and almost  <dig>  minutes with the chicken chromosome as the query sequence. after considering the strand combinations to get full results, vmatch is faster than our algorithm. vmatch used about  <dig>  gb memory while processing the chicken and human sequences which is less than required by mummer but more than our algorithm at  <dig> mb.

vmatch is more efficient than our algorithm for constructing the suffix array files and similar but faster in execution speed for finding ultraconserved sequences. one possible reason for the speed difference is our algorithm produces more output in anticipation of the next stage of the workflow. the primary disadvantage, which is seen with all the tools we reviewed, is the lack of explicit support for multiple organisms; output results are not provided in a format allowing for a simple workflow provided by our algorithm in figure  <dig>  additional scripts are required to parse the output results, extract the relevant substrings from the sequence, generate a new set of database files for the next stage of the workflow, and with every stage maintain substring metadata so that final results can be correlated back to the original sequences. all tasks that are handled automatically by our algorithm.

CONCLUSIONS
with more genomes becoming available at a faster rate, whole genome comparative analysis for multiple organisms is both feasible and desirable in our search for biological knowledge. we argue that bioinformatic programs should be forward thinking by assuming analysis on multiple  genomes, then consider both memory and computational complexity during algorithm design to maximize scalability. our algorithm provides a case study for how a compromise design with a trade-off of disk space versus memory space allows for efficient computation while only requiring modest computer resources. the key advantages of our algorithm are:

• it produces output files that can be directly used as input for the next stage of the computational pipeline. this completely eliminates the need to write additional "glue" programs to parse output files into a format required for later stages of the pipeline.

• by focusing on a specialized task, computation can be orders of magnitude more efficient than more general algorithms, e.g. blast, making what use to be unreasonable analysis, quite possible. this also frees up more time to analyze the results instead of waiting for the results.

• the algorithm works for any number of organisms. by maintaining the result files from previous analysis, incorporating new organisms into the analysis is quickly and easily done without requiring all of the previous analysis to be repeated.

while our focus has been on finding the longest ultraconserved elements among multiple genomes, in the process we discovered that the genome in the suffix array data structure can be used for other tasks. one example is searching for class ii transposable elements which have the structure of inverted repeat sequences flanking both ends of the transposase sequence. by performing the search for mcp's for an organism on itself then restricting the results to be within a certain distance apart , the inverted repeats can be quickly found throughout the whole genome.

in future work, there are some enhancements we would like to make. if we relax the restriction of 100% identity then our algorithm tends to approach blast in capability by incorporating mismatches and gaps, where at some point it is better to just use blast. however, a simple relaxation like allowing for a user specified identity level will allow for more matches to be found. we also intend to explore additional comparative analyses that can take advantage of the genome in the suffix array data structure, thus providing a suite of comparative tools for use with multiple organisms.

availability and requirements
we have provided our algorithm in the biococoa library which is the de facto objective-c bioinformatics framework instead of starting a new software package. the main functionality is provided in two objective-c classes, bcsuffixarray and bcmcp, which supports operations on suffix arrays and mcp files respectively. by putting the functionality in general purpose classes, we hope this provides added utility as other programs can utilize those classes for different bioinformatic analyses. the command line tools described in this article directly utilize those classes and are provided as a biococoa application.

• project name: biococoa

• project home page: 

• operating system: mac os x, gnu/linux

• programming language: objective-c

• other requirements: gnustep for gnu/linux systems

• license: creative commons share-alike attribution version  <dig> 

• any restrictions to use by non-academics: none

authors' contributions
sc designed and implemented the algorithm. sc and nl discussed and performed the biological analyses. sc, nl and gm wrote the paper.

