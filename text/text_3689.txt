BACKGROUND
the relevance of genomic structural variation  to human medical disorders is well-known  <cit>  and efforts are starting to focus more systematically on characterizing sv and its implications  <cit> . recent advances in technology  <cit> , combined with the availability of the human genome sequence  <cit> , are now opening dramatic new avenues of sv research  <cit> . these developments collectively point to the pending feasibility of investigating sv over its entire size spectrum. the most comprehensive projects will locate and identify variants, sequence them, and finally establish their statistical characteristics within a population  <cit> .

broadly speaking, sv encompasses translocations, inversions, and copy number variations and other types of inserted and deleted sequences . here, we focus on the last category, which is believed to occur the most frequently  <cit> . historically, cytogenetic techniques were used to examine instances of sv that were sufficiently coarse so as to be visible under a microscope  <cit> . array technologies were later used heavily, but these platforms were still not able to reliably capture alterations well below  <dig> kb  <cit> . more recently, volik et al.  <cit>  proposed a procedure based on paired-end sequences that can detect much smaller variants, depending upon the type of sequence insert one employs. the scheme is remarkably straightforward in concept, relying on the fact that if the subject genome contains an insertion or deletion structural variant , the length statistics of any paired-ends aligned to a reference genome will differ from those of the progenitor library. specifically, inserts would appear to be longer and shorter on average, respectively, for dsv and isv . the method basically furnishes a metaphorical caliper for observing the tell-tale length discrepancies that characterize sv.

although investigators are actively pursuing this technique  <cit> , it is still somewhat new and its conceptual simplicity actually belies a number of latent complications. alignment tasks are not trivial  <cit> , nor are accurate descriptions of a host of statistical issues. for instance, breakpoint localization has only been examined under the idealization of constant insert lengths  <cit> . gaussian length distributions provide a much better empirical fit. indeed, projects routinely invoke precisely this assumption, subsequently exploiting elementary gaussian thresholds to define their sv detection framework. for example, a common rule has been to declare sv if the aligned average length differs from the library average by at least  <dig> standard deviations  <cit> . this threshold implies a confidence interval of slightly better than 99%, or equivalently, the chance of committing a false positive classification error of α < 1%. other procedures call for considering inserts more than  <dig> deviations from the average  <cit> .

in actuality, the statistical aspects of this problem are rather more complicated than what the above practices would suggest. one of the outstanding issues is coverage, which current theory ignores entirely  <cit> . traditional fixed-length processing models  <cit>  are not particularly useful here because the local covering dynamics will depend upon the variation of insert lengths in the library. while the role of variability has actually been recognized for some time  <cit> , it has not been formally investigated much beyond the elementary uniform distribution model  <cit> . consequently, there is little understanding of how the main statistical classifiers, α and β , are affected by gaussian variance through the mechanism of coverage. a subtext to this point is that the statistics of isv versus dsv are not symmetric, as is commonly assumed  <cit> . finally, it appears that there have been no comprehensive studies related to the statistical power of the method or to how the spectrum of sv sizes can be effectively managed in a project.

h
t
h
m
all of these issues have important implications for the broader enterprise of sv research, from project planning and optimization to defining detection rules within sv algorithms. here, we report the mathematical analyses that lead to a general a priori statistical characterization of isv and dsv when using the length-discrepancy technique in conjunction with gaussian libraries. we describe several novel aspects of sv detection revealed by this theory and comment on their implications for pending sv projects.

RESULTS
alignments to the reference genome for which one or more inserts seem either abnormally long or short may represent instances of sv . in the theoretically ideal case of identical insert sizes, the task of sv identification is elementary. a single spanning insert is sufficient for an un-ambiguous assignment, i.e. α =  <dig>  and the detection power is simply the local coverage probability,  <dig> - e-ρ, where ρ is the redundancy  <cit> . the actual problem is rendered more difficult by the natural length variability present in all libraries; it tends to obscure the ability to differentiate true sv-related length differences from those arising merely from the population sampling effect  <cit> . although methodology-related artifacts can also arise, e.g. anomalous mappings of chimeric or extremely small reads, we do not explicitly investigate these second-order effects.

consider a library in which the insert length ℒ is gaussian  distributed  <cit>  as   

where λ and σ are the average and standard deviation of length, respectively, and exp is the exponential function. end-pair alignment of randomly selected inserts yields a sample whose members span a candidate sv region having a magnitude at least δ = |x <dig> - x1| . the associated length statistics that we wish to describe are functions of several variables  and are governed by what we call the sampling and covering problems. their solutions can subsequently be combined in a number of ways to represent the various possibilities of considering sv.

the sampling problem
let random variables  and ℳ represent the number of randomly selected inserts that span a candidate sv site and the mean length of these inserts, respectively. the sampling problem requires the determination of confidence intervals on ℳ with respect to the size of sv being examined. that is, if the probability of ℳ being within λ ± δ by virtue of random sampling effects is very high, then any actual average falling outside these limits would be significant. such an observation would strongly imply an instance of sv of size at least δ.

theorem  <dig> 
define the null hypothesis, h <dig>  as the absence of sv of size δ. given k inserts spanning , the two-tailed test for h <dig> is   

where erf is the gaussian error function  <cit> . eq.  <dig> gives the sampling probability that a perceived event of isv ∪ dsv will actually fall within the established confidence interval due to random sampling effects. conversely, the sampling probability of either type of sv event considered alone, i.e. as a one-tailed test is   

where these represent isv and dsv, respectively.

the covering problem
the covering problem addresses the question of how many inserts span  and is complicated by several factors. first, there are non-trivial difficulties in the alignment process  <cit> , necessitating several simplifying, but reasonable assumptions. given fixed sequencing read lengths of r , we do not consider cases in which a read, rather than its progenitor insert, intersects the boundaries of . such instances would lead to read-splitting in dsv and read-truncation in isv. this scenario is actually the basis of an altogether different kind of detection technique  <cit> , which we do not discuss here. so-called "singleton reads" can arise if the missing mate originated entirely within an isv, and we briefly discuss this possibility further below.

the second complication is that the statistics of isv and dsv are not actually symmetric, as is commonly presumed  <cit> . for example, in the strictest case, we do not admit singleton reads in isv, meaning the placement constraints are much more restrictive than for dsv. lastly, the gaussian model itself introduces certain mathematical difficulties.

lemma  <dig> 
consider the event s in which a single insert of length l  spans a site of sv. let hm and ht represent the status of the sv as homozygous and heterozygous, respectively. if the genomic source has a haploid length g, the bernoulli probabilities of s are   

where   

is the minimum admissible insert size and l ≥ m .

if singleton reads are allowed for isv, then m would actually be smaller than the expression given in eq.  <dig>  however, there is no reliable basis for measuring insert length in such cases, even for perfect, un-ambiguous alignments  <cit> . consequently, there is an irreconcilable mismatch between the numbers of covering and sampling inserts. in the interest of being able to make direct comparisons to the established symmetric models using a two-tailed test  <cit> , one could simply, though somewhat erroneously, take m = 2r for these cases.

theorem  <dig> 
 is poisson distributed  <cit>  with a rate   

where   

is the general rate expression and n is the number of inserts that have been processed.

statistical models of sv
the classification problem for sv is characterized by the probabilities of incorrectly calling out an instance of sv  or overlooking a true instance of sv . such scenarios are described by the traditional inference probabilities α and β, respectively  <cit> , which can be constructed from the above components. we discuss some of the various combinations here, again taking the null hypothesis, h <dig>  as the absence of sv of size at least δ.

theorem  <dig> 
let c be the event that the sample average length falls within some specified confidence interval, then   

is the probability that differences between the sample and library average lengths are attributable to the sampling effect  <cit> . instances falling outside the interval imply rejecting h <dig> and these are significant at a level of α =  <dig> - p. this statement immediately implies the following corollaries.

corollary  <dig> 
the standard model recognizes the inherent asymmetry between isv and dsv, employing separate one-tailed tests for each. specifically c represents intervals λ - δ ≤ ℳ and ℳ ≤ λ + δ for isv and dsv, respectively, where the variable m embedded in these equations assumes appropriate values, as given by eq.  <dig>  homozygous and heterozygous configurations are differentiated according to eq.  <dig> 

corollary  <dig> 
the symmetric model disregards the asymmetry between isv and dsv and implements classification according to the two-tailed test, where c is the interval λ - δ ≤ ℳ ≤ λ + δ and where m = 2r for both isv and dsv. homozygous and heterozygous configurations are again differentiated according to eq.  <dig>  this model can be regarded as a direct extension of previous work  <cit> , though it harbors the singleton anomaly discussed above.

theorem  <dig> 
let d be the event that an instance of sv is provisionally detected , as defined under the following conditions: it is spanned by at least one insert and the length of at least one of those spanners is different from the library mean by a specified threshold amount τ. we have   

where t = τ/. consequently, the detection power p, sometimes written more traditionally as  <dig> - β, is given by eq.  <dig>  where c is replaced by d.

discussion
here, we examine some of the consequent properties of the so-called length-discrepancy method that will have implications for future projects. this discussion is framed in terms of several insert types that are in either experimental or common use . we concentrate largely on the most conservative event from the standpoint of detection: heterozygous insertion.

a representative of <  <dig> kb inserts, see e.g. refs.  <cit> 

b insert length representative of extended chemistry protocols for 2– <dig> kb inserts, see e.g. ref.  <cit> 

c library parameters reported in ref.  <cit> 

d experimental, not currently in routine use

e library parameters reported in ref.  <cit> 

f primary breast tumor library b <dig>  see ref.  <cit> 

overview of trends for false-positive calls
thus far, concerns have predominantly focused on the rate of false-positive sv declarations. the general methodology has been one of assuming symmetric behavior of isv and dsv and subsequently employing elementary gaussian thresholds, usually ±  <dig> σ, to control false-positive errors  <cit> . for example, tuzun et al.  <cit>  aimed to identify sv of size δ ≥  <dig> kb using fosmids. fig.  <dig> revisits this aspect of the problem with respect to heterozygous sv for the edge cases of illumina genome analyzer  short inserts and large-insert fosmids . here, we demonstrate that false-positive behavior is more complex than what simple gaussian thresholds are able to capture.

the asymmetry between isv and dsv is quite clear. statistical significance of dsv detection is basically governed by a single minimum size, δmin, for each specific case. for example, the significance level α = 1% for fosmids is realized for roughly all δ ≥  <dig> kb at ρ = 8× physical coverage. while matching the δmin =  <dig> kb calculated by tuzun et al.  <cit> , this datum is purely a coincidence. asymptotic α is actually highly dependent upon the physical coverage, as shown in the figure, while δmin is only weakly dependent upon ρ.

the situation for isv is quite different in that there is not only a δmin, but also a maximum size, δmax. the latter arises because placement constraints for fixed λ increase with isv size. physical coverage is again tightly coupled with these limits. increased ρ not only renders the better  values of α more accessible, but also widens the range of acceptable δ. for example, heterozygous isv using fosmids does not register at the 1% significance level even at 8× physical coverage. however, at 12×, events are significant roughly in the range  <dig>  ≤ δ ≤  <dig>  and at 16× in  <dig>  ≤ δ ≤  <dig> . the upper limit on δ increases somewhat faster with redundancy than does the lower limit for isv.

while trends are similar for illumina ga short inserts, there are notable differences with respect to physical coverage. in particular, isv at 1% significance does not become feasible until roughly 24×, and even then its range is quite small. the main parametric differences from fosmids are that the variance is higher here, 12% coefficient of variation  vs. 7% reported in ref.  <cit> , and that read length is a much greater fraction of insert length, i.e. r/λ = 20% here vs. about  <dig> % for fosmids. increased variance obviously degrades the statistics, but ironically, so do "better" read lengths. the latter phenomenon arises because of the requirement that reads lie outside the insertion, which implies fewer placement possibilities for covering . these observations indicate that short-read data will generally have to be generated at much deeper redundancies than large-insert clones  and also raise the issue of optimal read lengths. however, the latter depend on all the methods one might use to find variation, so it cannot be settled on the basis of the length-discrepancy approach alone.

all isv curves approach the asymptote α = 50% as δ increases as a consequence of vanishing covering probabilities. inferences in these regions are no better than a coin flip. notice that the fosmid curves also approach the 50% asymptote as δ →  <dig>  the underlying factor here is a vanishing precision, not unlike that experienced when evaluating the small difference between two increasingly large numbers.

remarks on detection power
the concept of detection power is never entirely precise because it requires adoption of ad hoc alternatives to the null hypothesis  <cit> . ours rests on a simple, but intuitive presumption: detection is only possible if at least one insert spans the sv site, and if at least one of these spanners has a length sufficiently different from the library average, as specified by τ. for instance, for isv, at least one covering insert must be shorter than λ - τ, meaning its aligned length will be less than λ - τ - δ.

thresholds can be specified in numerous functional ways, each of which has certain implications for finding sv of different sizes. fig.  <dig> examines the idealized scenario of τ =  <dig> for heterozygous isv. each curve is asymptotic for δ ≪ λ, but rapidly decays as sv size approaches the insert size.

asymptotics are readily shown as a special case of the model to be   

in fact, eq.  <dig> also represents the  power for dsv. here again, we see that inserts having relatively long reads, i.e. larger r/λ, are penalized, but increased redundancy can compensate for the shortfall. for instance, asymptotic power for illumina ga data could be made equal to that for fosmids if its redundancy were roughly 60% higher than fosmid redundancy. note the trend for each insert's power curve to more visibly resemble a unit step function as redundancy is increased. in comparing α to power, it appears the latter is quite acceptable. for example, 16× fosmids are upper-bounded at roughly  <dig> kb for α = 1% significance , for which the corresponding power is still about 85%. this is notable because the choice of τ =  <dig> is not especially sensitive for large insertions.

defining rigorous detection rules is a challenging task for algorithm developers. again using isv as an example, the aligned length of the shortest covering insert, ls - δ, will be known, although ls and δ themselves generally are not. however, one can readily quantify power versus the aligned length of the average insert, for example using some τ = δ/c. this information might serve as a basis for constructing detection rules by correlating a priori known theoretical entities with other heuristic information.

the nature of library variance
library variance is conventionally thought of as something that should be minimized to the greatest extent possible in order to improve sv detection  <cit> . this view actually comes with some caveats, as illustrated by fig.  <dig> for heterozygous isv detection using  <dig> kb fosmids. fosmid libraries can routinely achieve cov of around 7% because of packaging constraints inherent to the vector  <cit> . yet, α is largely constant for cov ≤ 7% over a wide range of redundancies, implying that special efforts aimed at further reducing fosmid library variance would be unwarranted. while some sensitivity is actually realized for very small isv, i.e. less than 10% of insert size, the limit on precision mentioned above renders these instances irrelevant.

another curious phenomenon associated with isv lurks in eq.  <dig>  its exponential and error function terms have leading coefficients σ and λ - m, respectively. the second term represents the familiar "lander-waterman" type of covering mechanism which should ideally provide the bulk contribution, but its potency drops considerably for larger δ via m . this reflects the simple fact that the average insert will not cover very well under these circumstances. performance can be recovered in a seemingly counter-intuitive way by increasing the library variance, which implies there are more longer-than-average inserts in the library. fig.  <dig> confirms this effect, although it is evidently not substantial enough at reasonable redundancies.

the situation is appreciably different for illumina ga inserts, where α rapidly becomes responsive to library variance over slight changes in m/λ . it is mildly sensitive at m/λ =  <dig>  , meaning that there would be some level of improvement if library standard deviation could be reduced. however, for a small decrease to m/λ =  <dig>  , the situation worsens in two ways. not only does α become appreciably more sensitive, but attempts to compensate with increased redundancy are less effective. for example, at 12% cov we could reduce α by 68%  by doubling ρ from 12× to 24×. if illumina ga libraries were improved to fosmid-level 7% cov, we would instead see α reduced by 90%, from  <dig> % to  <dig> %. yet, the curves show that still more resolution could be wrung out, all the way down to about 4% cov . this overall behavior is again a consequence of the relatively long read lengths, which drive down δ for a given m/λ ratio. although these observations suggest investing more effort into reducing σ for illumina ga libraries, the balance against economic considerations has not been conclusively established.

the sv spectrum problem in project design
sv projects are becoming both more routine and more focused on characterizing the entire sv size spectrum. one of the more pressing design questions with respect to length-discrepancy analysis is how to a priori specify the combination of insert types and corresponding redundancies that will best characterize the relevant fraction of the spectrum  <cit> , roughly  <dig> ≤ δ ≤  <dig>  bp.  investigators will typically want to capture all types of sv, implying a project's design will be governed by the most difficult type: heterozygous insertions. we examine the problem primarily on this basis, but emphasize that the findings presented below must be interpreted within the larger context of a sequencing project whose considerations are not limited strictly to sv, much less a single method of detecting it.

fig.  <dig> addresses the design issue from the standpoint of the "spectral chart", which is readily plotted from the theory. in particular, the solid curves represent the loci of points at which the desired α, in this case 1%  <cit> , is realized for the conventional inserts listed in table  <dig>   the dashed curves denote improved performance of hypothetical illumina ga libraries whose covs are one-third lower than conventional values.

medium and large illumina ga inserts, combined with fosmids, readily handle sv above about  <dig> bp using assorted physical coverages in the 15× – 20× range. although the small insert library does cover much of the neighborhood on the lower end, there is a conspicuous gap between  <dig> and  <dig> bp, precisely where many variants could be expected  <cit> . redundancies for both the small and medium insert libraries would have to increase to roughly 90× to close this gap, an obviously undesirable requirement. yet, matters would improve considerably with a few design adjustments.

let us assume hypothetical illumina ga libraries whose covs are reduced by one-third of their conventional values. also, replace the  <dig> kb illumina ga library with a  <dig>  kb ga library. these two modifications largely erase the gap, i.e. curves for adjacent libraries now intersect at roughly 30×. switching to the  <dig>  kb library is primarily responsible for this closure, although reduced covs further improve spectral coverage on the lower ends of the respective libraries. this effect is especially relevant to extending detection range for the smallest svs.

spectral charts are useful for designing projects according to the requirement that no gaps remain in the sv detection spectrum for a desired α. fig.  <dig> shows two such designs. the first proposes roughly ρ = 30× for  <dig> bp and  <dig>  kb ga libraries and ρ = 12× for  <dig> kb ga and  <dig> kb fosmid libraries, netting sv within approximately  <dig> ≤ δ ≤  <dig>  the second prescribes 50× for the  <dig> bp ga library and 18× for the remaining  <dig> libraries, widening the range to roughly  <dig> ≤ δ ≤  <dig>  although many other designs are clearly possible, these illustrate some of the interesting trade-offs faced by the investigator. for example, maximally extending the range raises the possibility of including additional insert types, e.g. a bag library. costs are also vastly different over the various insert types, with small ga being the cheapest, larger ga being more costly because of library inefficiencies, and fosmids being the most expensive according to direct library and sequencing costs. however, these issues are tempered by the fact that long inserts efficiently leverage sequence redundancy,  <dig> ρ r/λ. for example, 20× fosmid physical coverage translates to only about  <dig> × sequence redundancy  <cit> . the second design in fig.  <dig> is probably superior to the first from this standpoint.

CONCLUSIONS
our theory describes sv statistics in more general terms than currently available, though it still depends upon a number of idealizations for the sake of tractability. specifically, the covering process is taken to be independently and identically distributed. we also presume genome size is known a priori, which may not be the case for tumors. finally, we do not account for mapping or sequencing errors, library complexity, the ability of algorithms to distinguish between the reads covering both alleles of a heterozygous sv site  <cit> , instances of singleton, split, and truncated reads, etc. the strength of any predictions should be taken with these limitations in mind.

design optimization is clearly important and we have only considered it from the rudimentary perspective of eliminating gaps in the sv size spectrum. indeed, there are numerous possible designs for any spectral chart, as well as numerous charts that could be plotted by varying the types and numbers of libraries, values for α, etc. it would be quite useful to optimize on the basis of total project cost, but variation in library production and sequencing costs among different insert types, lab environments, production methods, bio-economical feasibility of minimizing cov for inserts, etc. places this goal beyond the present scope. results shown here suggest an intermediate number of libraries, i.e. three to five, will typically give the best results. too few will incur extremely large per-library redundancies because of spectral gaps and the asymptotic nature of spectral coverage , while too many will result in unacceptably high production costs. very roughly speaking, each insert type should probably be responsible for up to an order of magnitude of sv size.

aside from the project design aspects, our theory should also be useful for sv algorithms. many algorithms still follow the symmetric assumption where dsv and isv are considered to be simple opposites  <cit> , though they clearly are not. bayesian classifiers might invoke thm.  <dig> in calculating prior probabilities. the theory might also be useful in helping to pick optimal threshold values, for example with respect to detection power, as mentioned above. finally, the overall design space is enormous and certainly worth further exploration, especially in conjunction with better bio-economical information on the feasibility of reducing cov for illumina ga libraries. future projects could carry out such investigations for neighborhoods of interest in a fairly mechanical fashion.

