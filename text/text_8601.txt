BACKGROUND
embryos undergo a temporally ordered differentiation process, starting as basic undifferentiated eggs. through the process of differentiation, gene expressions take on increasingly complex patterns. transcriptional regulation of the fruit-fly drosophila melanogaster is one of the best understood examples of the regulatory networks that govern gene expression patterning. an understanding of the regulatory networks responsible for gene patterning in drosophila embryos has been aided by digital images produced via in situ hybridization  <cit> . these images document the spatiotemporal dynamics of differentiation found in drosophila embryos. a comparative analysis of these images is beneficial for the understanding of functions and interactions in gene networks  <cit> . to facilitate these discoveries, tools have been developed to searching for images based on keywords that describe embryonic structures  <cit> , and searching for images based on gene expression patterns  <cit> . images for these tools have been obtained from databases of drosophila embryonic images, e.g. the berkeley drosophila genome project , and they are annotated with a controlled vocabulary   <cit>  . the cv terms describe the developmental and anatomical properties of gene expression during embryogenesis  <cit> . currently, groups of bdgp images are manually annotated with cv terms. this is done collectively so that not all images in a group necessarily correspond with each cv annotation. the manual nature of these tasks puts an inordinate burden on biologists as the collection of drosophila gene expression patterns are growing rapidly  <cit> . it is therefore imperative to investigate efficient and effective computational methods to automate this task  <cit> .

image annotation and image retrieval problems have been studied extensively in computer vision and machine learning. however, natural images are the most common subjects of study for image annotation and image retrieval problems; and commonly-used annotation and retrieval techniques may not be effective for our task. for example, unlike most natural images, bdgp images have all been aligned and scaled semi-automatically. the binary feature vector  representation have been developed correlate pattern similarities between images  <cit> , however the bfv representation is not robust to distortions; there were also some studies which tried to use robust descriptors to represent the bdgp images  <cit> , however they have not exploited spatial information. it is desirable to represent images in a way that takes advantage of the spatial properties of image features, while at the same time being robust to image distortions. in our annotation problem, we are interested in collectively annotating groups of images, with each group annotated with multiple cv terms. previous studies have revealed that ignoring group memberships can be detrimental to annotation performance  <cit> , and formulating the task as learning the function between local input patterns and cv terms lead to significant performance improvement  <cit> .

in this article we propose a novel approach for the automated annotation and retrieval of drosophila melanogaster images. we present an image representation model that takes advantage of the spatial information provided by the bdgp images while at the same time being more robust against distortions. we also take advantage of a state-of-the-art learning model in order to boost the performance of our tasks. our feature representation framework is inspired by the spatial bag-of-words  approach for image representation. the bow approach involves first extracting features from local patches on images. these patches are then quantized to a visual word that has been determined by a pre-computed codebook. our approach involves extracting these local patches from each image in a group, while maintaining a record of the locations where features are extracted. thus, our bag-of-words method is essentially a spatial-bag-of-words method. as previous experiments have discovered  <cit> , using only one codebook word to describe a local patch does not capture the slight differences between a word and the actual feature. therefore, we have adopted a sparse learning framework in order to take advantage of multiple codebook words that show varying levels of similarity to a single feature, leading to a “visual sentence” representation of the image patch.

we have tested our methods on bdgp images from the flyexpress database . annotation results from our study show that the spatial-bag-of-words approach consistently outperforms the non-spatial, bag-of-words approach as well as the binary feature vector approach. results also show that incorporating the sparse learning framework into our representation model further improves performance. while for the image retrieval task, experiments show that utilizing the sparse representation alone is sufficient.

methods
in this section, we describe the bag-of-words  and the sparse learning representations for gene expression pattern image annotation and retrieval.

the bag-of-words approach
the bag-of-words method was originally used for text classification problems where each document is represented as a feature vector indicating the frequency of each word in the document. such feature vector representation is used to classify documents into one or more categories. this text categorization approach has been adapted to image analysis  <cit> . specifically, images are represented as a collection of “visual words”, based on features extracted from the images  <cit> .

in the bow approach for image representation, invariant visual features are usually extracted from a subset of images  <cit>  to produce a visual codebook using a clustering algorithm, though a recent study shows that the clustering process is not really essential  <cit> . here the cluster centers are considered to be visual words. from this codebook, each feature from an image patch is quantized to the closest visual word in the codebook. a histogram is then created to represent the number of occurrences of each word located in an image. this histogram is a global representation because it only tracks the number of occurrences of each word in an image but not the location of those words, thereby the spatial layout of local image features is not captured. this is considered as one of the major drawbacks of the bow model  <cit> . next, we discuss each step involved in the bow model when applied to fruit fly images in details.

feature detection
feature detection involves locating regions in an image to serve as representative boundaries for visual words. we are using images that have been properly scaled and aligned semi-automatically. we use a series of overlapping circles to represent areas where feature information is extracted to construct a single visual word. an example of these overlapping circles is shown in figure  <dig>  in our experiments, the radius of the patches are set to  <dig> 

feature description
based on the regions described above, a local feature is extracted from each of the overlapping circle. because of its robustness against variations in image scale and rotation, we use the scale-invariant feature transform  descriptor  <cit>  for representing each local patch. thus, each image consists of a collection of feature vectors.

codebook generation
the codebook is constructed by obtaining a collection of representative vectors from the extracted features. we use the common generation approach of selecting a subset of images and then using the k-means algorithm to cluster their sift feature vectors  <cit> . the number of cluster centers which represent the visual words can be set manually. for our image annotation and retrieval problem, we have set this number to  <dig>  the sift feature vectors can then be quantized to the closest codebook centers in order to form a visual word representation for each image.

once the codebook has been created, we can assign codebook words to features extracted from image patches. formally, assume the number of patches  for a given image is i and the size of the codebook is j. define eij= <dig> if the ith feature vector is assigned to the jthcodeword, and  <dig> otherwise. then the given image can be represented as h=h <dig> h <dig> …,hj where 

  hj=∑i=1ieij. 

the spatial bag-of-words approach
a major limitation of the bow approach is that the spatial information of local image features is not encoded, as the bag-of-words representation is an un-ordered collection of visual words. a previous study on a bag-of-words approach  <cit>  for automated annotation of drosophila embryo image groups showed encouraging results, and a recent study  <cit>  showed that using spatial information together with visual information is better than using only visual information. we expect the performance can be further improved by taking advantage of the spatial information, i.e., the location where visual words are found within images. intuitively, the additional spatial information of visual words within images may facilitate the classification of images when the discriminant features are restricted to a certain region, which is the case for our cv terms. this can be implemented by adopting a method similar to the spatial pyramid matching scheme  <cit> .

our approach for image representation is based on an implementation of the spatial bag-of-words method. like the bow method, the spatial bow method creates a histogram for each image, counting the number of times each word appears in an image. additionally, the spatial bow tracks the position where each visual word is located. therefore, the spatial bow method benefits from the robustness of the bow method while also taking advantage of the spatial properties of images.

a spatial bag-of-words is much like a normal bag-of-words except that it is represented by a larger feature vector. while a histogram of an image is represented by a non-spatial bag-of-words, h, a spatial bag-of-words consists of multiple non-spatial bags, concatenated. specifically, for each image with n spatial sections, a spatial bag mn can be represented as mn=h <dig> h <dig> …,hn, where each hicorresponds to a non-spatial bag-of-words for a particular spatial section. thus we have n bags-of-words from n spatial sections on each image that are concatenated to form mn. this way, different sections of a spatial vector represent different sections of an image. our automated annotation representation is created by partitioning feature patches into  <dig> by  <dig> sections on each image. this representation creates a multiple of  <dig> in added dimensionality to a non-spatial representation of the same visual words. for each image group in the study we also create a global bag-of-words representation to test the differences in annotation performance that are seen between the global and the spatial approaches. figure  <dig> shows a global bag-of-words representation, a  <dig> by  <dig> spatial bow representation, and a  <dig> by  <dig> spatial bow representation below the circular feature representations of two separate images.

the sparse spatial representation
the original bow representation, as applied to image analysis, assigns each feature vector to the closest visual word in the dictionary. denote the feature vector obtained for a given patch as y∈rd and the dictionary matrix as d∈rd×c, in which each column is a centroid . then, the assignment of an image patch to a visual word can be written formally as the following optimization problem: 

  mine12∥de−y∥22s.t.ei∈{ <dig> },∑i=1cei= <dig> 

clearly, the constraints enforce that only one element in the solution e will be set to one, which corresponds to the visual word most similar to the image patch y. in this case, relationships between a feature vector and other visual words are discarded. this would not be a problem if a feature vector is an exact match with the visual word that it is assigned to, as in the case of text classification. however for images, a feature vector may be close to multiple visual words. in such cases, the relationship with the closest word would be overestimated and the relationships with the other similar words would be lost, leading to degenerated representation accuracy.

the sparse approach for bow representation addresses this problem by assigning feature vectors to multiple visual words simultaneously. we seek to represent the local patch using “visual sentence” with a set of “words” instead of a single one. besides the selection of visual words to form this sentence, we also need to evaluate the “contributions”. a commonly used approach is to formulate this problem as a sparse learning problem, which can be solved by state-of-the-art algorithms.

mathematically, the generalization from visual word to visual sentence can be done by relaxing the constraint in . we construct the representation vector x∈rc, such that for the ithentry, i= <dig> …,c, xi=wi when the ith keyword is selected with contribution wi, and  <dig> when the keyword is not selected.

in order to make x sparse , an ℓ1regularization is imposed, resulting in the following optimization problem: 

  minx∥dx−y∥2+λ|x|1s.t.xi≥ <dig> i= <dig> …,c 

 in which |·| <dig> is the ℓ <dig> norm and λ is a parameter that controls the sparsity. in our experiments, λis fixed to be  <dig> . this problem is closely related to lasso <cit> , and can be solved by many existing software packages, such as slep  <cit> .

the comparison between “visual word” and “visual sentence” for image representation is illustrated in figure  <dig>  as shown in the figure, the sparse learning provides more smooth representation.

integrating the spatial and sparse approaches into the bow representation model is therefore expected to produce a more accurate description of drosophila images. we have created both sparse and non-sparse versions of both our global and spatial bag-of-words representations, and compare different combinations of approaches for image annotation and retrieval. detailed performance evaluation can be found in the results section.

RESULTS
data description
the drosophila gene expression pattern images used in our study are obtained from the flyexpress database, which contains standardized images obtained from the berkeley drosophila genome project . in bdgp, the drosophila embryogenesis is divided into six stage ranges . the first stage range is not included in this study because of the small number of cv terms used to describe its images. images from the remaining stage ranges are annotated separately in their respective groups because the majority of terms are stage range specific. the second through sixth stage ranges consist of  <dig>   <dig>   <dig>   <dig>  and  <dig> image groups, respectively. the last two stage ranges contain the largest number of lateral images as well as the highest counts of cv terms.

evaluation of annotation performance
we employ the one-against-rest support vector machines  to annotate the gene expression pattern images, where the svm builds a decision boundary between image groups that contain a particular term and the remaining image groups. we employ the libsvm package  <cit>  and the linear kernel is used. the regularization parameter is set to  <dig> in all cases. our proposed method combines both the spatial and sparse approaches and is denoted by svmspatial+sparse. we compare our method with those that utilize only sparse, only spatial, or global bag-of-words approaches. these approaches are denoted by svmsparse, svmspatial, and svmglobal, respectively. the performance comparison of the four representations in terms of auc and macro f <dig> scores is summarized in tables  <dig> and  <dig>  respectively.

since most cv terms are stage-range specific, we annotate the image groups according to their stage ranges separately. the numbers and proportions of positive samples for the  <dig> most frequent term in each stage range are summarized in table  <dig>  for each stage range, we begin with the  <dig> terms that appear most frequently, and then we add additional terms in the order of their frequencies with a step size of  <dig>  this results in different numbers of data sets in each stage range, depending on the total number of cv terms in that stage range. the extracted data sets are randomly partitioned into disjoint training and testing sets using the ratio 1: <dig> for each term. for each data set, we generate  <dig> random partitions and the average performance is reported. because our method models each individual term separately, we can compare the results of our method against the results of the other method on a term-by-term basis. for example, we can compare annotation results of our method with the non-spatial method in stage range 13- <dig>  term by term, where  <dig> cv terms are used. in this comparison, of the  <dig> terms being studied,  <dig> saw an average increased auc performance and  <dig> saw average increased f <dig> score  performance. due to space limitation, we will not show each individual term by term comparison. instead, we show the results for each stage range where various numbers of cv terms are used.

we have observed that there were significant differences in performance increases between earlier stage ranges where drosophila embryos were less developed and later stage ranges where embryos were more developed. we also observe that there are certain terms that benefit far greater from a spatial bag-of-words approach than other terms. for example, mesectoderm anlage in statu nascendi, central brain anlage, crystal cell specific anlage, hypopharynx primordium p <dig>  procrystal cell, and crystal cell are all stage dependent terms that showed the most dramatic increases in annotation performance. these increases in performance were consistent across multiple stage range tests, where the number of terms being annotated varied. there are also a number of terms such as pole cell, mesectoderm primordium, foregut primordium, germ cell, embryonic central brain neuron, embryonic central brain glia, and lateral cord glia that showed good performance across multiple stage ranges, where various numbers of cv terms were annotated. we included detailed performance evaluation of individual terms in  <dig> different data sets in figures  <dig> and  <dig> 

there are pioneering works on constructing feature representations for drosophlia gene expression image annotation. zhou et al.  <cit>  applied multi-resolution 2d wavelet discrete transform followed by min-redundancy max-relevance feature selection. puniyani et al.  <cit>  proposed an automatic system named “spex2” that performs pattern extraction using markov random field and further extracts features using the sift descriptor and singular value decomposition. using the top  <dig> most frequent terms  <cit>  in the bdgp data set, zhou’s system achieved an average f <dig> score of about  <dig> , while puniyani’s method achieved about  <dig> . for comparison purposes, we extract the individual f1-scores for the same terms. our sparse + spatial representation yields an average f1-score of  <dig> , which outperforms both methods.

comparison of different classifiers
since the main focus of this section is to demonstrate the performance of various image representations, we fix our classifier to be svm with linear kernel for its effectiveness in high-dimensional data. however, it will also be interesting to investigate how different classifiers perform in this task. as an illustrative example, we use stage range 11- <dig> with sparse representation and test the classification performance of three different classifiers including svm, logistic regression and ridge regression. the performance in terms of sensitivity and specificity is reported in table  <dig>  for all three methods, we apply 4-fold cross validation for parameter selection. as we can see in table  <dig>  the three classifiers achieve comparable overall performance, and svm achieves slightly higher sensitivity.

sparse feature is used and the classification performance on stage range 11- <dig> is reported. three different classifiers are applied for comparison, namely, svm with linear kernel , logistic regression  and ridge regression .

performance of over-sampling
as we can see in tables  <dig> and  <dig>  when the number of labels is large, the average sensitivity as well as f <dig> score is quite low. this is due to the dramatic lack of positive samples for some labels. for example, in stage range 11- <dig>  when we use  <dig> labels, the proportion of positive samples in these  <dig> labels can be as low as  <dig> %. in this subsection, we present some preliminary results on tackling this problem with over-sampling.

the over-sampling method works as follows. before training a classifier for a particular label, we first do random sampling on the positive samples with replacement, so that the number of positive samples is equal to the negative ones. then, we train the classifier using the balanced samples. we test this method using the same setting as in the previous subsection, and the classification performance is presented in table  <dig>  as we can see in tables  <dig> and  <dig>  the over-sampling method provides promising improvements in this example, especially when the number of labels is large. for example, when using the logistic regression on annotating  <dig> labels, the over-sampling improves sensitivity from  <dig>  to  <dig> . exploring methods such as over-sampling to further improve the classification performance will be an interesting future direction.

sparse feature is used and the classification performance on stage range 11- <dig> is reported. three different classifiers are applied for comparison, namely, svm with linear kernel , logistic regression  and ridge regression .

evaluation of retrieval performance
based on the proposed image representations, we obtain the pair-wise similarity for every two images in the database, which can be used for image retrieval. in our study, the representative images for different views and stage ranges from the well-known interactive fly websitea are used as queries. then, for a given method and a query image, we select  <dig> images with the highest similarity values to obtain a set of query results. note that the query images are removed from the results since they are always the one with highest similarity. sample query results from different views and stage ranges are presented in figures  <dig>   <dig>   <dig>   <dig> and  <dig> 

first, we will compare different methods by visually inspecting the images retrieved for each query. the first conclusion we can draw from the figures is that the methods based on the bag-of-words  generally outperform the one that utilizes the binary representation only. for example, for the stripe patterns such as those in figures  <dig> and  <dig>  the bfv method retrieves less than  <dig> similar images in its top  <dig> matches, and in figure  <dig>  even the best match looks quite different from the query image. also, we can observe that among the three proposed methods, the sparse representations generally yield more satisfactory results, particularly when the layout of the pattern is subtle, such as the ones in figure  <dig> 

we also give brief interpretations of the retrieved images by analyzing the functions of the corresponding genes in the biological process annotated in the gene ontologyb. figure  <dig> shows a stripe pattern expressed by gene odd, obtained from the dorsal view, in stage range 4- <dig>  odd is in charge of the periodic partitioning. the retrieved genes prd and slp <dig> are about periodic partitioning and blastoderm segmentation, respectively. both of them are closely related to the query gene. we also observe that several other retrieved genes, such as comm, comm <dig>  run, trn and alhambra, are not directly related to the segmentation process. however, they are all involved in the development of the nerve system. it will be interesting to examine how these two functions are related.

CONCLUSIONS
this article presents computational methods for annotating drosophila gene expression pattern images, and identifying similar images based on gene patterning. in both tasks, images are represented as bags-of-words. the size of the bags is determined by the spatial properties of a representation. for both applications, a sparse learning framework was used. results on the flyexpress database indicate that the proposed annotation method outperforms the non-sparse, non-spatial bag-of-words method, as well as approaches that would use either a sparse or spatial framework.

in our study, the bag-of-words representations were created by partitioning image features with local feature patches. terms that saw the greatest increases in annotation accuracy may only reside in specific regions of drosophila embryos during a given stage of development. one promising direction is to create local bag-of-words from these regions in order to eliminate some of the noise created by other unrelated regions, when searching for specific embryonic structures. this technique is commonly referred to as region of interest . we plan to explore this in the future.

endnotes
ahttp://www.sdbonline.org/ﬂy/aimain/1aahome.htmbhttp://www.geneontology.org/

authors’ contributions
all authors analyzed the results and wrote the manuscript. sj and jy conceived the project and designed the methodology. aw and ly implemented the programs and drafted the manuscript. sj, yj, z, sk, and jy supervised the project and guided the implementation. all authors have read and approved the final manuscript.

