BACKGROUND
segmenting words from fluent speech is a mandatory first step when learning a new language. the difficulty of the task lies in the lack of clear information indicating where a word begins and ends. following the paradigm introduced by saffran et al.  <cit> , we exposed adult volunteers to an artificial language while recording event-related brain potentials. after this learning phase, participants were asked to recognize the nonsense words of this artificial language. a specific feature of this language was that no pauses or other potential cues signaling word boundaries were provided. indeed, the only way to identify the embedded words from the continuous speech stream was by tracking the regular positions of each syllable along the sequence, a computational process  which is operative at the early age of  <dig> months  <cit> .

statistical learning is a domain-general mechanism that is involved in a diverse set of sequential situations, such as learning a small artificial grammar  <cit> , sequences of tones  <cit> , and nonsense words from continuous speech  <cit> . moreover, this learning mechanism appears to be functional not only in audition but also in other sensory modalities such as vision  <cit>  and touch  <cit> . all in all, the computation of distributional regularities seems to be important for encoding the temporal order and learning the relationships of elements within sequential input.

word-stress represents another useful cue when learning a language. it can be defined as an abstract phonological property of a word that is more salient or prominent than the other syllables and is established via several different prosodic features, such as pitch, syllable duration and syllable amplitude. in language with a fixed stress pattern, such as finnish, where stress always falls in the word onset position, stress becomes a reliable segmentation cue  <cit> . in english, the preponderant word-stress pattern involves stress in the initial syllable of words used in conversational speech; listeners seem to take advantage of this property and apply a segmentation strategy  that assumes that every strong syllable in the signal corresponds to the onset of a word  <cit> . when we address this issue in other languages like spanish, in which stress takes a wider range of positions within words, it is less clear whether or not listeners would benefit from the native language word-stress pattern to segment novel words from a fluent unknown language. in spanish lexical stress is unpredictable  <cit> : though penultimate stress is predominant, about one quarter of polysyllabic words have final or antepenultimate stress  <cit> . as in other languages, lexical stress is signalled by three simultaneous acoustic cues: fundamental frequency , duration and intensity  but no vowel reduction is required  <cit> . several authors have provided evidences for pitch being the best cue for lexical stress with duration following and, in last place, intensity  <cit> .

recently, erps have been used to investigate language segmentation  <cit> . specific erp components, such as the n <dig> or the n <dig>  have been proposed as speech segmentation indexes  <cit> . the n <dig> seems to be sensitive to word onset perception, whereas the n <dig> has been considered to indicate the identification of recently segmented words and consequently, it could be understood as the first prelexical brain signature detectable after the speech segmentation task is accomplished. in a previous study we corroborated the finding that the n <dig> is involved in speech segmentation  <cit> , and in addition, we found that the amplitude of the p <dig> component was enhanced when a prosodic cue was added on the first syllable of each nonsense word in the stream. importantly, this increased p <dig> appeared only for potentially segmented units  and not for syllables presented in random order, even though stress was analogously placed on every third syllable. this distinction clarified the nature of the p <dig> component in this learning segmentation task reflecting that it was sensitive to an acoustic property  of a unitary group of syllables. to prove the validity of this component in relation to word-stress, further research needs to be conducted in which the placement of a prosodic cue varies within the words.

the main purpose of the present study was to explore the influence of stress as a segmentation cue, its interaction with transitional probabilities, and crucially, the influence of stress on the p <dig> component when stress is placed on different syllables within a word. if the p <dig> component is sensitive to the properties of stressed syllables, we should be able to identify the component regardless of the position of the stressed syllable within words.

RESULTS
behavioral performance
for the three different conditions, the percentage of correctly segmented pseudo-words was: initial stress:  <dig>  ±  <dig> %; medial stress:  <dig>  ±  <dig> %; and final stress:  <dig>  ±  <dig> % . all mean percentages were different from chance   =  <dig> , p <  <dig> ; medial stress: t =  <dig> , p <  <dig> ; and final stress: t =  <dig> , p <  <dig> ). the anova revealed that performance in the three conditions differed from each other  =  <dig> , p <  <dig> ; ηp <dig> =  <dig> ). pairwise comparisons showed that the only significant difference was found between the final and medial stress conditions  = - <dig> , p <  <dig> , effect size: d =  <dig> ; initial vs. medial, t <  <dig>  d =  <dig> ; final vs. initial, t = - <dig> , p =  <dig> , d =  <dig> ).

we further conducted a chi-square test in order to ensure that the observed differences among conditions were reliable <dig>  we proceeded by first identifying how many participants in each condition performed better than expected by chance  <cit> . chance level was determined using a binomial test . accordingly, the performance at or above  <dig> % is significantly better than chance. in the initial stress condition,  <dig> out of  <dig>  participants performed above chance; in the medial stress condition, there were  <dig> out  <dig> participants  that performed above chance; and finally, in the final stress condition,  <dig> out  <dig> participants  performed above the chance level. a chi-square test was then calculated to compare between conditions. the results of the chi-square test revealed that the difference between the final and the medial stress conditions was significant  =  <dig> , p <  <dig> ), as well as the difference between the initial and the final stress condition  =  <dig> , p <  <dig> ). finally, the chi-square revealed that the difference between the initial and medial stress conditions did not reach statistical significance  =  <dig> , p >  <dig> ). in sum, the behavioral results showed that participants' performances were better when stress fell on the word final syllable.

erps
grand average erps for the initial, medial, and final stress conditions are depicted in figures  <dig> and  <dig>  the n1-p <dig> complex of the auditory evoked components was clearly identified at central electrodes for each condition . these components were followed by a broadly distributed negativity  particularly for the medial stress condition. moreover, the n1-p <dig> complex was repeatedly observed throughout the epoch starting approximately every  <dig> msec, which corresponded to the syllable duration. the erp waveforms for each condition were characterized by an enhanced p <dig> component elicited by the stressed syllable. the clearest effect of stress on this component was observed mainly for the first and third syllables of the initial and final stress condition, respectively . this positivity was also observed in the medial stress condition for the second syllable, although the effect was less marked because it overlapped with a negative deflection.

the repeated measures anova for the n <dig> peak amplitude including stress-condition  and syllable-position  found no statistically significant differences. however, a significant trend was found for the interaction between the two factors  =  <dig> , ε =  <dig> , p =  <dig> , ηp <dig> =  <dig> ) probably due to the reduced amplitude of the n <dig> in the first-syllable of the final stress condition. further pairwise comparisons applied only to the initial syllable confirmed that the amplitude of the n <dig> was significantly reduced only when the final stress condition was compared to the medial stress condition  = - <dig> , p <  <dig> , d =  <dig> ; initial stress vs. final stress, t =  <dig> , p >  <dig> , d =  <dig> ; initial stress vs. medial stress, t =  <dig> , p >  <dig>  d =  <dig> ). for the p <dig> peak amplitude, an effect of syllable-position was found  =  <dig> , ε =  <dig> , p <  <dig> , ηp <dig> =  <dig> ), indicating that the p <dig> amplitude was larger in the first syllable in spite of the stress condition .

most probably, the reduction in amplitude for the n <dig> component observed in the first syllable  was due to the overlap of the positivity developed in the last syllable where the stress is located. notice that the duration of each word is  <dig> msec, and therefore the pre-stimulus baseline was influenced by the components elicited in the last syllable. in order to avoid the influence of this pre-stimulus baseline in our statistical analysis, we opted for a peak-to-peak analysis , which is independent of the baseline chosen.

when the anova was conducted on the n1-p <dig> peak-to-peak amplitude measure, the statistical results did not reveal a main effect of either the stress-condition  <  <dig>  ηp <dig> =  <dig> ) or the syllable-position  =  <dig> , ε =  <dig> , p >  <dig> , ηp <dig> =  <dig> ), although a significant interaction was found  =  <dig> , ε =  <dig> , p <  <dig> , ηp <dig> =  <dig> ). this effect reflects a selective increase in the amplitude for the stressed syllable in the initial stress condition  =  <dig> , p <  <dig> , d =  <dig> ; see also table 1), as well as in the third stressed syllable in the final stress condition  = - <dig> , p <  <dig> , d =  <dig> ; third-syllable vs. second-syllable: t = - <dig> , p <  <dig> , d =  <dig> ). for the medial stress condition, however, the largest peak-to-peak amplitude was found on the third syllable, but no statistical differences were observed between the first, second and third syllable . this p <dig> component was characterized by a mid-central scalp distribution, which was similar in both initial and final stress conditions .

discussion
in the current study we evaluated the effect of a prosodic cue on speech segmentation using on-line erp measures. we recorded erps to artificial language  with the presence of a stress cue in different syllable positions. in addition, a statistical cue was always present, which corresponded to the transitional probability signaling the three syllable unit .

although a reduction in the n <dig> peak amplitude component was found for word onset in the final stress-condition, this effect should be interpreted only as the result of the overlap of the stress-p <dig> component elicited by the last syllable. due to the specific structure of these continuous sound streams, the pre-stimulus baseline condition is always affected by the components elicited in the last syllable. for example, the same overlap occurs for the n <dig> component elicited in the second syllable in the initial stress-condition in which the amplitude is reduced due to the larger amplitude of the previous stress-related p <dig> component. in this regard, the peak-to-peak measures proved to be a more reliable method for analyzing the current erp data  <cit> .

increased amplitude in the p <dig> component was found for stress syllables in all three conditions, although it was more discernible in the first and third stressed syllable of segmented words. the behavioral results clearly showed that participants segmented the stream better when stress fell on the last syllable than when it fell either on the first or in the middle syllable of the nonsense words. this pattern of behavioral results, however, might have been influenced by the difficulty of the test, as part-word foils were used instead of non-words. besides, in order to match test items, word-stress was removed from all test items <dig>  overall, the statistical power might have been reduced in the test-phase of the present design due to this manipulation. further studies might need to address if the initial stress condition would have yielded a better segmentation rate when using a different type of design in the phase test.

in real language acquisition, segmentation cues are never applied alone. rather, it seems plausible that the combination of several different cues would yield a significant improvement in solving the task of identifying embedded units. few studies have directly addressed the issue of how the combination of different types of cues facilitates speech segmentation and most of those performed have tested different cues during competition between them   <cit> . in a similar experiment to the one presented here, saffran et al.  <cit>  introduced a stress cue  on either the first or the last syllable in two different conditions. a facilitatory effect of stress was observed only when it was placed at the end of the word, suggesting that participants used their tacit knowledge of word-final lengthening to segment the speech stream. in our study, we observed a reduction in the percentage of segmented words when stress was introduced in the penultimate syllable. this result suggests that the non-coincidence of the statistical and prosodic cues might elicit conflict in this condition. in a similar vein, thiessen and saffran  <cit>  found that 9-month-old infants exposed to conflicting statistical and stress information in an artificial language stream were more reliant on stress than on statistical cues in order to segment the words. the infants seemed to treat stressed syllables as word onset even in the case where it led them to mis-segment the words from the stream. importantly, the reverse result was obtained with 7-month-olds, who relied more on the statistical information than on the rhythmic pattern of the language. in a recent study with adults using a crossmodal-priming paradigm, mattys et al.  <cit>  proposed the existence of weights for the different speech segmentation cues and situated stress in the last tier of this hierarchy. as stated in the introduction, spanish does not have a fixed stress pattern, though penultimate syllable stress predominates <dig>  it is therefore possible that listeners in spanish do not take advantage of this cue when segmenting words from an unknown language. if listeners had been able to exploit this medial stress pattern, a larger segmentation rate would have been expected as they might have taken advantage of this prosodic pattern and the statistical information contained in the boundaries. the pattern observed in the present study clearly indicates that they did not exploit this medial stress pattern. hence, it is likely that listeners do not rigidly use the tacit knowledge of their native language about the word-stress pattern to learn words from a new language; rather, they might use word-stress as a cue but in a more flexible fashion . thus, this result might reflect that the learning system is flexible enough to bypass the default stress language pattern and to focus on the convergence of the segmentation cues . in this regard, this result does not agree with a hierarchical system, whereby reliance on some cues is greater than reliance on others. instead, a more flexible and dynamic system might be able to adjust to the most advantageous learning pattern.

the results obtained in the two conditions where word-stress indicates word offset/onset  suggest that the combination of stress and other distributional cues, like transitional probabilities, seems to slightly facilitate the localization of the embedded words in the streams. therefore, the present results are suitable with two possible explanations: either  the listeners are not able to track the different segmentation cues at the same time if those cues are not located in the same syllable boundary  or  there is a strong facilitation when two types of segmentation cues coincide. both interpretations could be unified if we consider that when two segmentation cues coincide, the attentional focus coincides as well . on the other hand, when two cues do not coincide, the attentional focus might be more disperse and consequently the listener's performance might decrease.

additionally, our results corroborate the notion that learners pay more attention to the ends of the words  <cit> . in a recent study with adults engaged in a repetition detection task, endress et al.  <cit>  found that participants only succeeded in generalizing repetition-based structures when the repetitions were located at the final edges of sequences and were unable to do so when repetition was sequence-internal. for the authors, their results reflected a benefit of processing 'perceptual salience' of certain kinds of structures.

a possible explanation for our pattern of results is that the p <dig> component may be mostly related with the distributional properties of stressed syllables. although the placement of the stressed syllable varied in the three conditions of the experiment, all the language streams resembled each other, being made up of concatenating trisyllabic words with the same stress pattern. this produced the critical characteristic that a stressed syllable was heard in every three syllables, regardless of the condition, and therefore, the stress pattern introduced in the stimulation could act as another distributional cue useful for indicating word offset-onset.

considering the previous interpretations, the p <dig> component elicited by stressed syllables could be interpreted as an attentional cue related to the perceptual learning process itself. for instance, the involvement of attention is important in experiments that reported increased p <dig> sensitivity to learning  <cit> . in regard with language learning, de diego et al.  <cit>  observed a increased p <dig> component that positively correlated with listener's perception of initial-final syllable grouping . interestingly, the authors hypothesized that the p <dig> reflected the reallocation of attention, which was postulated as necessary for learning non-adjacent dependencies between syllables.

data obtained from somatosensory and visual modalities in various perceptual discrimination tasks support the view that attention is involved in the neurophysiologic changes observed during learning  <cit> . thus, word-stress seems to serve as a reliable cue in attentional capture for facilitation of speech analysis  <cit> . in behavioral studies, in which a phoneme monitoring task was used, it has been observed that word-stress serves as an attentional factor which facilitates the target  detection  <cit> , which supports the present observations on the p <dig> component and the exploitation of speech segmentation cues. but recently, in a study of auditory scene analysis where attention was manipulated in two different conditions, the authors found that in spite of attention, the amplitude of the p <dig> component correlated with behavioral judgments of streaming, which measures the capacity of listeners to perceive two different pure-tone streams  <cit> . the main finding of this study, however, was the fact that the p <dig> component appeared to be sensitive to auditory perceptual discrimination. other studies have found that pitch modulated the p <dig> component  <cit> , suggesting that pitch, as a stress mark, is used effectively in spoken word recognition  <cit> .

CONCLUSIONS
the present study shows that adults are able to integrate specific prosodic and statistical cues when segmenting speech. the electrophysiological results showed that stressed syllables elicited a larger amplitude on the p <dig> component than non-stressed syllables. finally, the behavioral results showed that segmentation was improved when word-stress was found on the last syllable of a word. we propose that this p <dig> modulation is associated with the underlying learning mechanism in charge of processing the distributional properties of stressed syllables in a continuous speech stream.

