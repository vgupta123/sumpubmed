BACKGROUND
a human genome is a sequence of nucleotides that can differ from one individual to another  due to various reasons, such as insertions/deletions of fractions of the sequence on the genome or mostly the substitution/mutation of single nucleotides on commonly observed sites called single nucleotide polymorphism   <cit> . in most snps only two different nucleotides are observed out of  <dig> nucleotides. the information of nucleotide variations extracted from these snp sites  is encoded as a sequence called “haplotype”. that is, for a particular snp site a notation is used for one of the observed nucleotides  and another notation is used for the other . because of its informative and heredity nature identifying the haplotypes of individuals has been an important subject in various medical and scientific studies, such as gene related disease discovery and drug design  <cit> , population history research  <cit> , etc. nonetheless, current experimental techniques are not low-cost and efficient enough for directly sequencing haplotypes of an individual; thereby identifying them is mostly based on indirect approaches, e.g., using computational methods to infer haplotypes from an alternative cost-effective data called “genotype”.

the entire human genome consists of  <dig> distinct chromosomes each appearing in two copies  except for the chromosome- <dig>  which consists of two copies of chromosome-x in females or one chromosome-x and one chromosome-y in males. each chromosome is a pair of two distinct sequences -haplotypes- inherited from the parents, i.e., one is from the maternal genome and the other is from the paternal genome. the genotype is sequenced by identifying the types of alleles -nucleotide variants- across the snp locations  in chromosomes. in a particular locus of a chromosome if both haplotypes have the same allele we call this site in the genotype homozygous and denote it with the type of alleles in both haplotypes as either common-type or wild-type; otherwise, if both haplotypes have different alleles –one common-type and one wild-type– we call this site heterozygous. when identifying haplotypes for a given genotype, the ambiguity occurs for the heterozygous sites since there is no information about which haplotype has the common-type allele and which haplotype has the wild-type allele. clearly, genotypes are less informative than haplotypes, as they present an ambiguity on heterozygous sites due to possible permutations and computational methods can be employed to identify which allele come from which haplotype. recently, more cost-effective alternative methods have been used for genotype sequencing  <cit> , e.g., widely used denaturing high-performance liquid chromatography   <cit> . by certain applications of such methods one can only determine whether an individual has homozygous or heterozygous allele in a given snp site, but cannot distinguish the type of allele in homozygous sites. the sequenced data is thereby less informative than the regular genotypes as it only represents the differing sites  between the haplotypes. this less informative form of genotype is named xor-genotype. one can solve the haplotype inference problem based on the xor-genotypes, i.e., xor-haplotyping, with a reasonable extra computational effort.

methods for solving the haplotype inference problem given the regular genotypes can be summarized in two categories: combinatorial methods that usually state an explicit objective function and propose methods for optimizing it, and statistical methods that relies on the statistical modeling of the problem. various methods have been published for the haplotype inference problem  <cit> , however the xor-haplotyping problem mostly remained under-investigated. two particular methods are suitable for xor-haplotyping problems: parsimony haplotyping that is based on the maximum parsimony principle, and perfect phylogeny haplotyping that relies on a population genetics assumption called the infinite sites/alleles model <cit> , i.e., it assumes that allele sequences are long enough so that a particular allele will have a mutation only once in the phylogenetic tree. the perfect phylogeny  model utilizes the infinite sites assumption by building a tree of individuals -haplotypes- where all individuals evolve, with no recurrent mutation, from one common ancestor. an approximate solution to xor-haplotyping problem in the case of pp model was introduced in  <cit>  where the xor-haplotype inference was cast as a graph realization problem  <cit> . however, the proposed method  in  <cit>  is not well-suited for the xor-genotypes with large number of snps, i.e., usually limited by  <dig> snps  <cit> , and is not extended to missing data cases.

on the other hand, it is known that in a population of individuals certain haplotypes are frequently found in certain genomic regions  <cit> . this fact leads to the parsimony principle that states that the genotypes of a population of individuals are generated by the least number of distinct haplotypes. identifying such smallest set of haplotypes is called pure  parsimony problem, which is np-hard  <cit> . an integer linear programing method was introduced in  <cit>  that finds a pure parsimony solution to this problem, and in  <cit>  a branch-and-bound method was used to solve pure parsimony problem. in  <cit>  a method called xor-haplogen was proposed for solving haplotype inference problem in the case of xor-genotype data. this method can find accurate solutions for xor-genotypes with large number of snps. another parsimony method was introduced in  <cit>  for xor-haplotype inference by representing it as a graph realization problem called pure parsimony xor haplotyping .

in  <cit>  a novel framework for  haplotyping was proposed by interpreting the parsimony principle as a sparse representation of the genotypes. two approaches are presented: maximizing a sparseness condition on the haplotype frequency vector determined by the inferred haplotypes, and casting the sparsity of this frequency vector as a sparse dictionary selection problem. the latter approach is based on an efficient greedy method shsd where haplotypes explaining the given genotypes are determined according to a sparse selection from the set of compatible haplotypes. the method constructively determines the solution of each individual while selecting the haplotypes from this set, and it has the convergence guarantee.

for the xor-haplotyping problem, there is an increased ambiguity due to the xor operation between haplotypes, i.e., the process of xor-genotyping that determines whether the type of alleles in both haplotypes differ in a particular site  or they are the same . however this ambiguity can be resolved with the assistance of regular genotypes. regular genotypes can either be used as post-processing inputs for eliminating set-equivalent solutions of a particular inference, or they can be used to refine inference while constructing the solution.

tractability of the maximum parsimony haplotyping problem in the xor-genotype case is still open  <cit> . in this paper, we propose a modified version of shsd —xhsd—, that can efficiently find a solution for maximum parsimony xor-haplotyping problem and resolve the ambiguity with the help of a small number of regular genotypes. for a given set of xor-genotypes the haplotype pairs for each individual are selected from the set of compatible haplotypes by a sparse dictionary selection method. the selection of dictionary columns from the set of compatible haplotypes and the sparse representation of xor-genotypes is formulated as a joint combinatorial optimization problem. the objective function of this problem maximizes a variance reduction metric over all individuals. our algorithm is a low-complexity greedy method that terminates once the solution is fully determined. to resolve the ambiguity and to improve the inference accuracy, we employ a small number of regular genotypes as constraints for the set of compatible haplotypes to help resolve the type of homozygous alleles.

the remainder of the paper is organized as follows. in preliminaries, we introduce the xor-haplotype inference problem. in methods, we formulate the xor-haplotype inference as a sparse dictionary selection problem and present an efficient greedy method for solving this problem. we also discuss the use of regular genotypes to resolve ambiguity. in extensions section we discuss how the algorithm deals with long sequences and data with missing sites. in results and discussion, we present the experimental results on synthetic and real data sets under various conditions. finally, the conclusions section is given in the end.

preliminaries
in an snp locus only  <dig> nucleotides are observed, and a single bit is sufficient for the representation of nucleotide variants such that  <dig> encodes the major allele and  <dig> encodes the minor allele. the haplotype of an individual can thereby be represented with a binary vector that shows the snp variants across the individual’s chromosome. the genotype can then be thought of as a ternary vector where a  <dig>  indicates that the site is homozygous and both haplotypes have major 0/ <dig>  alleles, and  <dig> indicates that the site is heterozygous and the haplotypes have different alleles 0/ <dig> or 1/ <dig>  notice that when encoding homozygous and heterozygous sites we used a different notation from the literature in order to express a genotype vector as the sum of two haplotypes: a minor-homozygous snp is encoded with  <dig> and a heterozygous snp is encoded with  <dig>  so that a  <dig> in the genotype is given by  two minor alleles, and a  <dig> in the genotype is given by  one major and one minor allele.

in general, given a length-l genotype vector, k ≤ l of the loci are heterozygous and thereby ambiguous, in each of the k sites one haplotype can take two values – <dig> or 1– and the other haplotype takes the complement value. considering all k heterozygous sites, one haplotype can then be one of the 2k binary sequences, and the other haplotype will be the complement  of that sequence. therefore, for solving a genotype with k heterozygous sites, the pair of haplotypes is drawn from a set of 2k distinct binary vectors of length-l.

on the other hand, in xor-haplotyping problem the conflated data — xor-genotype — is less informative than the regular genotype with respect to the information loss about the type of allele in homozygous sites. the xor-genotype is itself a binary vector, where for a given site,  <dig> indicates heterozygous snp where both haplotypes have different alleles for this given site. the xor-genotype can be represented by the xor sum of two haplotypes, likewise, for a given site  <dig> indicates a homozygous snp where the haplotypes have the same allele but without any distinction whether the type of the allele is major or minor. that is, the xor-genotype contains the information whether a particular snp site has homozygous alleles, but the type of alleles for those homozygous sites is not identified. every site of an xor-genotype is ambiguous, and each site of the corresponding haplotype can take two values. therefore, a length-l xor-genotype can be explained by a pair of haplotypes that are drawn from a set of 2l distinct binary vectors of length-l. hence, because of the additional ambiguity on homozygous sites, the number of possible solutions for an xor-genotype is significantly  larger than that of a regular genotype of the same size.

besides the xor-haplotyping problem is np-hard, there is also no unique solution to this problem. the nature of the xor operation results in a phenomenon called bit flip degree of freedom <cit> , i.e., for a particular solution set h consisting of length-l haplotypes, one can produce equivalent solution sets by inverting a certain snp i ≤ l  in all haplotypes of h. notice that inverting  an snp across all haplotypes has no effect on the xor-genotypes they generated, because even the alleles explaining homozygous sites of xor-genotypes are not distinguished . more specifically, assume that hi1∈{ <dig> } and hi2∈{ <dig> } represent the haplotypes of i-th individual in the ℓ-th snp and they generate that individual’s xor-genotype xi such that xi=hi1⊕hi <dig>  then the complemented snps of haplotypes also explain that snp of the same xor-genotype, i.e., xi=hi1¯⊕hi2¯. it then follows that for a particular set h of length-l haplotypes that solves a given set of xor-genotypes, there are at most ∑i=1lli=2l− <dig> equivalent sets hi′,i= <dig> …2l− <dig> to h where each equivalent set hi′ also solves that given set of xor-genotypes.

problem definition
for each snp ℓ= <dig> …,l, the xor-genotype is given by the xor-sum of two haplotypes such that, 

  xi=hi1⊕hi <dig> ℓ= <dig> …,l, 

where xi ∈ { <dig>  1} is the xor-genotype of the i-th individual in snp ℓ, and hij∈{ <dig> } is the j-th haplotype of the i-th individual in snp ℓ. let xi=t be the xor-genotype of the i-th individual, then  can be written as 

  xi=hi1⊕hi <dig> 

where hij=t is the j-th  haplotype of the i-th individual consisting of l snps. in this representation, we say that the xor-genotype of the i-th individual xi is phased by the haplotype pair {hi <dig> hi2}.

in regular haplotyping, a putative haplotype z ∈ { <dig>  1}l is called compatible with a genotype g ∈ { <dig>   <dig>  2}l if  ∈ { <dig>  1}l, and such a haplotype is a possible solution that can explain that genotype. that is, the haplotype pair {z,} is one of the possible solutions to the genotype g. therefore, for every given genotype gi it is essential to determine a set of compatible haplotypes hi when searching for possible solutions. the union of the sets h <dig> …,hn for n individuals forms the matrix z ∈ { <dig>  1}l × m where m is the total number of distinct compatible haplotypes.

in xor-haplotyping, on the other hand, it is trivial to see that any haplotype z ∈ { <dig>  1}l is compatible  with any xor-genotype x, i.e., x = z ⊕ z′ since there always exists a haplotype z′ ∈ { <dig>  1}l such that z′ = x ⊕ z. therefore, the set of compatible haplotypes hi for a given length-l xor-genotype xi consists of all binary vectors of length-l, i.e., h1=h2=⋯=hn={ <dig> }l×2l≜z.

because of this compatibility between the xor-genotypes and candidate haplotypes an snp site can always be explained by either of the two alleles, and thus unambiguous snps do not exist anymore. notice that, in particular, an xor-genotype with all-homozygous snps is still ambiguous and requires to be solved up to bit flipping. however, we know that such an xor-genotype is always explained by a pair of identical haplotypes which correspond to the same column of z. on the other hand, if there is at least one heterozygous snp in the xor-genotype then its phasing haplotypes are not identical and correspond to the different columns in z.

the xor-genotype of i-th individual is expressed as 

  xi= <dig> 

where  <dig> represents the component-wise modulo- <dig> operation, and vi ∈ { <dig>   <dig>  2}m, 1tvi =  <dig>  is the sparse vector indicating the haplotype locations as the indices of the matrix z of consistent haplotypes. notice that the modulo- <dig> operation in  is equivalent to the xor operation between the two haplotypes selected by vi.

given z, finding the indicator vector vi for an individual is equivalent to inferring its haplotype pair {hi <dig> hi2}. the maximum parsimony principle suggests that a given set of xor-genotypes should be explained by the smallest number of distinct haplotypes. therefore, given the set of xor-genotypes for n individuals {xi,i= <dig> …,n}, one needs to infer the haplotype pairs for each individual {hi <dig> hi <dig> i= <dig> …,n}, so that the union of all inferred haplotypes forms the smallest set as possible. in other words, the xor-haplotyping problem is to infer vi, i =  <dig>  …, n, given z while selecting as few columns of z as possible.

methods
xor haplotyping by sparse dictionary selection 
if an  xor-genotype is explained by only one haplotype, i.e., xi = hs ⊕ hs, where the haplotype hs is the s-th column of z, then the indicator vector multiplies that haplotype by  <dig>  i.e., v =  <dig> and v =  <dig> for j={1…2l}∖{s}. otherwise, if the xor-genotype is explained by two different haplotypes xi=him⊕hin, m ≠ n, then they are indicated by the vector v such that v = v =  <dig> and v =  <dig> for j={1…2l}∖{m,n}. hence, we can rewrite  in the following more compact form 

  xi= <dig> 

where ai is a set of indices corresponding to the nonzero elements of vi, zai is the submatrix of z consisting of the columns indexed by ai, and vi~ is the non-zero elements of vi.

for each observed xor-genotype xi, the phasing haplotypes are located in columns of z indexed by ai. the union of these column indices, i.e., d=∪iai, forms the dictionary of the haplotypes that suffices to construct all given xor-genotypes. the maximum parsimony principle then dictates that the dictionary d should contain the least possible number of elements that can reconstruct all observed xor-genotypes. the set of haplotypes indicated by such a sparse dictionary d is given by h=zd, where zd is the submatrix of z consisting of the columns indexed by d. then h is a solution set to the maximum parsimony haplotyping problem for the given set of xor-genotypes {xi,i= <dig> …,n}.

to solve the xor-haplotyping problem, we choose the sparse dictionary d to minimize the average distance between the observed xor-genotypes and the closest approximations constructed by the haplotypes in zd. since there is no prior information about the dictionary d and the indices ai for proper reconstruction of each xor-genotype, determining d and ai leads to a combinatorial problem. this joint-optimization problem can be efficiently solved by a greedy method that we will explain next.

for an observed xor-genotype the reconstruction accuracy can be interpreted as the euclidean distance between the observation and its closest approximation, i.e., 

  li=minv~i∥xi−zav~i2∥ <dig>  

where a represents the indices of haplotypes in z used to approximate xi. notice that an exact solution will satisfy li= <dig>  for a given dictionary d, the indices ai for reconstructing each xor-genotype will be determined by restricting ai to be a subset of d such that 

  ai=argmina⊆d,|a|≤2li. 

the individual cost function in  is then translated into a fitness function associated with a given dictionary d, i.e., 

  fi=∥xi∥2−mina⊆d,|a|≤2li. 

finally, the fitness value of d is averaged over all individuals to measure the overall reconstruction accuracy 

 f=1n∑i=1nfi. 

for a given cardinality  of n, the best dictionary is therefore given by 

  dn∗=argmax|d|≤nf, 

and the sparsest dictionary that is sufficient to reconstruct all observed xor-genotypes is determined by 

  d∗=minndn∗:f=1n∑i=1n∥xi∥ <dig>  

notice that determining both d for a given n in  and a for a given d in  is a combinatorial problem. in  <cit> , it is shown that such combinatorial problems can be approximately solved efficiently by a simple greedy method if the objective function satisfies a fundamental property called submodularity. in  <cit> , it is shown that the dictionary selection problem for  haplotype inference has a cost function that is approximately submodular, and when a greedy method is used to optimize this cost function it can efficiently find an approximate solution with a theoretical guarantee  <cit> .

for xor-haplotype inference, on the other hand, the problem is fundamentally different. that is, the submodularity property may not hold for the cost function in  due to the xor operation, and thereby the theoretical guarantee does not hold either for the greedy method. nonetheless, we still use the similar greedy heuristic as shsd in  <cit>  in order to maximize the variance reduction metric in  over the set of observations.

in our algorithm xor haplotyping by sparse dictionary selection , we start with an empty dictionary set d1=ϕ. then at each iteration ℓ, among the consistent haplotypes that are not already in dictionary dℓ− <dig>  i.e., in z∖dℓ− <dig>  we iteratively add the haplotype that contributes to the dictionary dℓ− <dig> with the maximal marginal gain. that is, at iteration ℓ the haplotype hm∈z∖dℓ− <dig> is added to dℓ− <dig> if it satisfies 

  m=argmaxk∈{1…2l}∖dℓ−1f. 

to compute  requires solving  and  for each k. in  for each individual i, ai is found by computing the euclidean distance  between xi and the possible reconstructions given by the pairwise xor-sum of all columns in zd, and picking the columns that minimize . whenever indices ai yield zero in  we can explain that individual with the corresponding haplotypes in z, i.e., xi= <dig>  the dictionary dℓ keeps growing until all xor-genotypes are explained, i.e., f=1n∑i=1nfi=1n∑i=1n∥xi∥ <dig> 

notice that in xhsd algorithm the number of compatible haplotypes |z| exponentially increase in comparison to regular haplotyping problem with shsd. however, –when available– we can reduce z with respect to regular genotype information via utilizing them in the cost function . the necessary modifications are discussed in the next section xhsd with regular genotypes. another fundamental difference in xor-haplotyping is that the xor-genotypes do not provide unambiguous genotype information which one can initialize the dictionary with corresponding haplotypes and improve the reconstruction accuracy. nonetheless, with a bias weight, the modified cost function can exploit the available regular genotypes even when they are not unambiguous.

summary of xhsd algorithm:

• initialization. 

– z={ <dig> }l×2l.

– n ←  <dig> 

– dn−1∗=ϕ.

• iterate until all xor-genotypes are explained, i.e., fdn∗=1n∑i=1n∥xi∥ <dig>  

– perform the greedy search.

∗ for ∀j∈ <dig> …,2l∖dn−1∗, compute fdn−1∗∪{j}.

∗ let j∗=argmaxj∈{1…2l}∖dn−1∗fdn−1∗∪{j}.

set dn∗=dn−1∗∪j∗.

∗ check if any xor-genotype is explained by the addition of the new element hj∗, i.e., if  is zero. if so, the inferred haplotype pair for the individual with such an xor-genotype is .

– n ← n +  <dig> 

given the xor-genotypes of a set of individuals, this algorithm finds the haplotypes of each individual based on the maximum parsimony principle.

as an example, consider the following demonstration. let x <dig> x <dig> and x <dig> be the xor-genotypes of three individuals each corresponding to three snps, i.e., 

 x1= <dig> x2= <dig> x3= <dig>  

the set of compatible haplotypes for these individuals will consist of all length- <dig> binary vectors, i.e., 

 z= <dig>  

after initializing z, and starting with empty dictionary d <dig>  the algorithm performs the greedy search by adding one haplotype from z  at a time. at iteration n =  <dig>   calculates m = arg max and m is randomly picked as  <dig> among the equal maximum values, then the corresponding haplotype z <dig> =  <cit> t is added to the dictionary, i.e., d1= <cit> , and zd1= <dig>  similarly, at n =  <dig>  m = arg max =  <dig> is calculated and the haplotype z <dig> =  <cit> t is added to the dictionary, i.e., d2= <cit> , and zd2= <dig>  x <dig> is explained by the addition of new haplotype, i.e., x <dig> =  <cit> t ⊕  <cit> t, yet the other xor-genotypes are not explained. at n =  <dig>  m = arg max =  <dig> is calculated and the haplotype z <dig> =  <cit> t is added to the dictionary, i.e., d3= <cit> , and zd3= <dig>  other two xor-genotypes are explained by the new addition, i.e., x <dig> =  <cit> t ⊕  <cit> t, x <dig> =  <cit> t ⊕  <cit> t, and the algorithm converges at n =  <dig> via calculating f=13∑i=13fi=43=13∑i=13∥xi∥ <dig> 

this simple example demonstrates how the proposed greedy approach can efficiently construct sparse solutions, where three xor-genotypes are explained by only three haplotypes within three iterations. nonetheless, the solution set has the ambiguity of being one of the equivalent sets of the true solution due to the bit flip degree of freedom which should be resolved.

resolving bit flip degree of freedom
in  <cit>  it is shown that the xor perfect phylogeny problem can be solved up to bit flipping based on the characteristics of the given xor-genotypes. let x ∈ { <dig>  1}l × n be the xor-genotypes matrix of n individuals such that x=. denote χi as the set of heterozygous loci for the i-th individual, i.e., χi = {ℓ:xi = 1}, where xi is the ℓ-th snp in xi. if there exists a set of individuals i⊆{ <dig> …,n} whose xor-genotypes have empty intersection, i.e., ∩iχi=ϕ, then with the knowledge of regular genotypes gi∈{ <dig> ,2}l×|i| of those individuals one can remove all bit flip degrees of freedom. the empty intersection indicates that an snp will have homozygous allele in at least one of those individuals and therefore that snp can be resolved by revealing the type of allele at the corresponding regular genotype. following this, a post-processing method is suggested in  <cit>  that can remove the bit flip degree of freedom across the loci where a set of xor-genotypes have empty intersection.

by bit flipping on a given solution h, one attempts at choosing among the set-equivalent solutions hi′,i={ <dig> …,2l−1} and this choice is decided by the given regular genotypes .

however, this post-processing method have certain limitations. notice that, for large l the set-equivalent solutions are highly specific to the infererred h, e.g., for a given set of xor-genotypes it is very likely that any two different inferences h <dig> and h <dig> —which are not set-equivalent— can have very different set-equivalent solutions. bit flipping on different inferences likely leads to different results, and thereby the bit flipping accuracy largely depends on the initial inference h which is made by avoiding the prior knowledge on homozygous snps, i.e., regular genotypes. besides, –when available– utilizing more regular genotypes in post-processing does not necessarily improve the bit flipping accuracy. basically, to decide among the appropriate bit flippings for a particular locus requires the knowledge of that homozygous snp from a regular genotype. intuitively, to reveal a set of homozygous snps by employing the least number of regular genotypes, e.g., provided by the mti method, will be necessary and sufficient for removing the bit flip degree of freedom across those snps. on the other hand, a larger number of regular genotypes will not be any more informative due to possible inconsistencies on the type of homozygous allele for an snp site across the given regular genotypes.

furthermore, notice that flipping the bits on some loci across all the haplotypes in h does not affect the parsimony of the solution. the final solution h′ will have the same parsimony with h regardless of the set of loci that are flipped. from the maximum parsimony point of view, refining an xor-haplotyping solution via bit flipping method does not necessarily lead to global optimum unless the initial inference is a set-equivalent of the global optimal solution.

therefore, instead of using regular genotypes to post-process a solution, a more intuitive way could be to aim at resolving the bit-flip degree of freedom while constructing the solution. in particular, regular genotypes can be used as constraints when solving the homozygous sites of an xor-genotype. in this sense, given a set of individuals’ xor-genotypes we determine the individuals that have the most informative regular genotypes and pre-process the data set by replacing with the regular genotypes for those individuals. the mti algorithm  <cit>  is useful for finding the least number of such individuals that will be adequate to reveal the homozygous alleles for each of the l snps. in the proposed xhsd framework, we employ the mti method to find which individuals should be replaced with regular genotypes and after replacing them the new data set is presented to the xhsd algorithm .

in most cases the xor-genotypes in x has empty intersection and for each run mti outputs  <dig> or  <dig> individuals, i.e., |i|≤3; then gi has at most  <dig> regular genotypes. one can obtain a larger gi by performing multiple runs of mti with x and collecting the distinct regular genotypes given by mti.

next we explain the necessary modifications to the xhsd algorithm for utilizing the regular genotypes.

xhsd with regular genotypes
the information provided by regular genotypes is used to reveal the type of allele in homozygous sites of an individual so that we can improve the reconstruction accuracy in , and build the dictionary d with more reliable haplotypes. that is, when a regular genotype gi is observed in the i-th individual we employ the variance reduction metric that is given for regular genotypes such that 

  li=minv~i∥gi−zav~i∥ <dig>  

where z is the set of haplotypes that are compatible with the i-th individual’s genotype gi, and a contains the indices of the haplotypes in z that are used to approximate gi. in this representation the approximation accuracy is potentially higher when compared to the xor-genotypes, since the homozygous snps in gi are unambiguous. the haplotypes that are used to approximate those snps will be more reliable candidates when building the dictionary d.

to exploit this fact, we can introduce a weight bi in the cost function li so that the algorithm will give a higher priority on the variance reduction of those individuals that are given by regular genotypes, and the dictionary will more likely grow with the haplotypes that are compatible with the given regular genotypes. the biased variance reduction metric for each individual is then given by 

  li=biminv~i∥gi−zav~i∥ <dig> givengiminv~i∥xi−2∥ <dig> givenxi. 

the weight parameter bi could be set as proportional to the average rate of homozygous snps per genotype, assuming that the more homozygous sites the regular genotype contains the more informative it will be. we experimentally set bi= <dig> as it yielded good performance with both synthetic and real databases.

extensions
long xor-genotypes
note that the size of z grows exponentially with the length-l due to the compatibility between haplotypes and xor-genotypes. that is, finding the solution of a length-l xor-genotype requires to perform the greedy search over z that consists of 2l haplotypes. to mitigate the computational complexity we employ the partition-ligation method  <cit>  as in  <cit>  where the block partitioning is based on identifying the recombination hot spots  <cit>  existing between the haplotype blocks  <cit> . after partitioning the snp sequences will be divided into blocks where within each block the haplotype diversity is as low as possible.

the haplotype diversity of a given block is measured by its shannon entropy. the block partitioning by minimizing the total shannon entropy proceeds as follows. let h~1lm…h~k~lmlm be the k~lm haplotypes that explains all the xor-genotypes xilm,i= <dig> …,n in the block that starts at locus l and ends at locus m, i.e., 1 ≤ l ≤ m ≤ l, and let f~lm=f~1lm,…,f~k~lmlm be the haplotype frequency vector for this block. each f~klm,k= <dig> …k~lm is represented by the density of the nonzero values of the indicator vectors {v1lm,…,vnlm} for the given block xilm, i.e., 

 f~lm≜12n∑n=1nvnlm. 

the entropy of the haplotype block h~klm is then given by 

 e=−∑k=1k~lmf~klmlogf~klm, 

and the total entropy of q blocks, where each block ,q= <dig> …q has an upper bound of length w, i.e., mq − lq + 1 ≤ w, is given by 

 e=∑q=1qe. 

to determine the initial and ending loci of each block ,q= <dig> …q that minimizes e we use the recursive method explained in  <cit> , i.e., for each ending locus 1 ≤ m ≤ l we determine the block , with m−lm∗+1≤w, that contributes with the lowest entropy and then backtrack the best initial points lm∗ for each consecutive block by starting with the block .

missing data
genotyping errors often occur when the observed genotype of an individual differs from the original sequence for various reasons  <cit> . a particular type of genotyping error is the case when some loci are not observed/missed during sequencing or other application processes. although methods dealing with some type of errors were proposed, often erroneous genotypes are produced with significant missing/error rates  <cit> . therefore, it is of high importance for an xor-haplotyping technique to be adaptive for resolving such databases with missing sites. we next present a modification to xhsd in order to perform xor-haplotyping for the individuals exposed to missing data conditions.

let g~i be the incomplete genotype of the i-th individual where the loci with missing information in gi are removed. similarly, let x~i represent the xor-genotype of the i-th individual where the missing loci are removed. as the rate of missing loci increases the sequences become less informative. following the suggestion in  <cit> , we introduce another weight w to give less weight to the less informative individuals when evaluating  in order to improve the reliability of haplotype inference, i.e., 

  li=wbiminv~i∥g~i−z~aiv~i∥ <dig> giveng~iwminv~i∥x~i−2∥ <dig> givenx~i. 

where z~ai is the matrix z with the rows corresponding to the missing loci of the i-th individual removed. the weight is selected as a nondecreasing function of the total information content in the sequence such that 

  w=dim <dig>  

where dim gives the dimension of x~i.

different weight functions could be employed to exploit the distribution of missing sites. since, in our experiments, the missing sites are uniformly distributed across the snps and individuals the function in  gave a good performance.

the proposed method does not account for the direct inference of the missing sites, i.e., imputing missing genotypes  <cit> . however, the missing values in each xor-genotype can be recovered from the solution by simply looking at the haplotype pairs which are specifically inferred for each individual. since the proposed method has robust performance against missing data, as presented in the next section, the inferred solution will be sufficient to type missing genotype sites. an implementation of the proposed method –with aforementioned extensions– is provided in “additional file 1”.

RESULTS
we tested the performance of several xor-haplotyping methods with a number of metrics. first we measured the probability of error , i.e., the percentage of individuals whose inferred pair of haplotypes are different from the original pair. this measure is sensible for assessing the inference quality in regular haplotyping problem since the alleles corresponding to homozygous loci are known and only the heterozygous loci are ambiguous thereby performance depends on the inference accuracy on heterozygous loci. nonetheless, in xor-haplotyping there are a large number of equivalent solutions to original one up to bit flipping and thereby it is very likely that a solution set differs from the original phasing on at least one snp. in particular, for a given xor-genotype even if there is a single snp difference  between the pair of inferred haplotypes and the pair of haplotypes that originally gave rise to that xor-genotype, it is counted as mis-inference. a more sensible metric, therefore, would take into account the percentage of such snps where the inference differs from the true phasing. in that sense, the switch error rate   <cit>  is a proper metric that counts the minimum amount of required switches for heterozygous loci to change to the correct alleles of the original haplotypes. it gives a sense of how closely the inference was made, i.e., as a ratio of total mis-inferred heterozygous loci missihet in all individuals i={1…n} to the worst-case number of switches , i.e., 

 swr=∑i=1nmissihet∑i=1nχi <dig>  

moreover, to assess the accuracy on homozygous sites, we employ prediction error rate   <cit>  computed as the fraction of incorrectly predicted hidden-homozygous sites out of all hidden-homozygous sites, i.e., 

 errp=∑i=1nmissihom∑i=1n. 

we performed xor-haplotyping on various data sets, with and without missing information on loci: synthetic data sets with different recombination rates simulated by a coalescence based program of  <cit> , a database consisting of the snps in the cftr gene that is associated with cystic fibrosis  disorder  <cit> , and another database  containing the snps that have relatively lower linkage disequilibrium . we tested different xor-haplotyping methods that are based on different assumptions including the parsimony graph realization model ppxh  <cit> , the parsimony genetic search model xor-haplogen   <cit> , the graph representation model greal  <cit> , and an integer programming approach poly-ip  <cit> . among the four methods the last two were ineffective for practical reasons. greal failed at finding solutions for data sets with reasonably long sequences , and poly-ip method is often computationally inefficient when solving even a simple problem .

synthetic data
based on different recombination rates three different scenarios are considered in synthetic data sets: no recombination , and recombination with rates r= <dig> and r= <dig>  respectively. the recombination rate is the rate that the haplotypes of an individual exchange the sequence fragments due to several reasons such as crossing-over events. this fact is simulated by a model given in hudson’s software  <cit> . for each scenario we generated  <dig> different data sets by random pairing of a set of simulated haplotypes of different lengths  for a given population size. this is repeated for different population sizes as well, n ∈ { <dig>   <dig>   <dig>   <dig>  50}.

in figure  <dig>  the performances of different methods on short data sets  are displayed which is based only on xor-genotypes. the quality of inference is exhaustively determined after removing all bit flip degrees of freedom by looking for the best equivalent set of a particular inference, i.e., performing an exhaustive search to find the best bit flipping that gives a result closest to the true phasing of xor-genotypes. such evaluation shows the best inference performance of different methods without the help of regular genotypes. compared to other methods, xhsd can potentially resolve a set of xor-genotypes with comparably low error rates. moreover, xhsd achieves the lowest switch error rates, especially for large datasets, indicating a better accuracy  for the initial inference given only the xor-genotypes.

to evaluate the inference quality when regular genotype data are available, we first determined only a limited number of regular genotypes by the mti method, i.e., the smallest set of regular genotypes that have empty intersection on the heterozygous snps, then resolved the ambiguity by bit flipping on the initial inference according to these regular genotypes . this test evaluates how methods can deal with bit-flip degree of freedom under very limited regular genotype data that –in theory– suffice to resolve all snps. given the long xor-genotype data sets , block partitioning is applied in xhsd by limiting the maximum block size to w= <dig> snps. from figure  <dig>  we can say that xhsd has the best potential to make an inference with high accuracy when the regular genotypes are introduced. we also applied the proposed xhsd framework represented in figure  <dig> to the same dataset where  <dig> xor-genotypes are replaced with the regular genotypes. note that the proposed xhsd achieves a significant decrease in pe rates despite the small augmentation of data by only  <dig> regular genotypes, compared to using them in the post-processing, i.e., xhsd .

it is worthy of noting that the algorithms based on segmentation may deteriorate when processing long xor-genotype sequences, especially with increasing recombination rates where the detection of haplotype blocks is complicated  <cit> . we used block partitioning  in xhsd to reduce complexity when processing long xor-genotype sequences. in figure  <dig> the segmentation effect is noticeable particularly in very high recombination rates, i.e., r =  <dig>  however, in general scenario, i.e., r ≤  <dig>  we can say that the segmentation effect is not significant for the proposed method’s performance, and it outperforms xor-haplogen in most data sets containing typical recombination rates.

for more practical results we added regular genotypes in each method with different percentages of the population and allowed the methods to remove ambiguity by their own, except for ppxh. since ppxh cannot make use of regular genotypes directly, we applied bit flipping using the mti solver to remove ambiguity for this method. to regularly genotype a given percentage of the population, the regular genotypes are determined by running the mti method several times until the number of distinct regular genotypes obtained achieves the given percentage of the total number of individuals.

missing data
we investigated capability for dealing with missing data under different circumstances by various methods. since the methods performed similarly under zero recombination rate we used the same data sets with no recombination to generate the database with missing entries. an snp site of an individual is defined as “missing” with a probability of pmiss and the data sets for different percentages of missing snps are generated accordingly. ppxh method is excluded since it cannot handle missing data. in xhsd the block partitioning is applied as before with a maximum block size of w =  <dig> snps.

figures  <dig> and  <dig> show the performances in different scenarios of partial regular genotyping under different rates of missing data. as in the previous plots, each point represents the average value of the corresponding metric over  <dig> realizations– <dig> different sets of varying snp sizes between  <dig> and  <dig>  in most cases, xor-haplogen and xhsd are insensitive to the increased number of missing sites. xor-haplogen is more accurate for small group of individuals. nonetheless, when more individuals are available in the database  xhsd displays a better performance in all circumstances.

we examined the dependency of methods on percentage of the missing data rate for a population with large number of individuals. that is, we used the xor-genotypes from  <dig> individuals and replaced 30% and 50% of the population with regular genotypes, and performed xor-haplotype inference under different missing data rates ranging from  <dig> % to 5%. as seen in figure  <dig> both methods are robust against missing data. on the other hand, xhsd is less dependent on regular genotypes and it can achieve better error rates than xor-haplogen by employing even less number of regular genotypes. xor-haplogen needs approximately 20% more regular genotypes to reach the same pe level with xhsd, e.g., regular genotyping by 30% in xhsd is comparable to that of 50% in xor-haplogen.

cftr gene database
cystic fibrosis  is an autosomal recessive disorder caused by mutations in the gene that encodes the cystic fibrosis transmembrane conductance regulator protein . in  <cit> , various mutations on  <dig> polymorphic locations from the chromosome  <dig> are detected as the disease loci for cf. we used this database corresponding to  <dig> distinct haplotypes to generate random xor-genotypes. by combining the haplotype pairs at random we generated the xor-genotypes for a given number of individuals n, and repeated the process for different population sizes, i.e., n ∈ { <dig>   <dig>   <dig>  400}. in this database, the data sets with small number of individuals present high haplotype diversities, i.e., many of the distinct haplotypes are only used once in the generation of individuals. therefore, the larger data sets that have low haplotype diversities are expected to be solved with higher accuracy by biologically-oriented methods, such as xor-haplogen which obtains its inference according to a multi-locus linkage disequilibrium -based block identification model.

we tested the performance of each method on this database with/without missing sites { <dig> %}. ppxh method was excluded from the missing data analysis since it cannot deal with missing data. xhsd is applied with block partitioning and the maximum block length of w= <dig> snps as before. it is seen in figure  <dig> that xhsd out-performs for various population sizes with significantly low error rates. as the xor-genotypes are taken from more individuals, the inference accuracy is immediately improved in xhsd and xor-haplogen, whereas ppxh do not have this ability to benefit from the additional data.

typing errors
combinatorial optimization techniques are known with their sensitivity to genotyping errors  <cit> . thereby, we tested the effect of typing errors on the proposed algorithm using cftr gene database. we defined a snp site of an individual as erroneous with a probability of perr, and typed the site as either homozygous or heterozygous with equal probabilities. we then run the algorithms without providing the knowledge of erroneous site positions. we excluded ppxh method due to its low performance on the cftr database. figure  <dig> illustrates the algorithms’ performance on typing errors with perr = 2%. it is seen that xor-haplogen is a more robust method against typing errors because of its statical nature. nonetheless, the proposed xhsd algorithm can deal with erroneous data containing ∼2% typing errors, with a small increase in the error rates compared to the results without typing errors.

anril database
the performance of haplotyping methods can deteriorate on databases with decreasing linkage disequilibrium  rates. a snp database with low pairwise-ld scores are investigated in an association study given in  <cit>  for their susceptibility to certain types of leukemia. this database includes  <dig> snps from the chromosome 9p <dig> associated with several diseases and a snp locus encoding for anti-sense non-coding rna in the ink <dig> locus  <cit> . we used the corresponding haplotype data from hapmap database  collected from  <dig> european individuals. we generated the xor-genotypes for the individuals by using their haplotype pairs and tested the algorithms on this database. it is seen from the figure  <dig> that the algorithms deteriorate when inferring the haplotypes with low-ld snps. xhsd shows very similar performance with xor-haplogen, and both methods over-perform ppxh on this database.

notice that the algorithms cannot mitigate the error rates with increasing number of individuals. this can be explained by the occurrence of very high haplotype diversity in corresponding low-ld snp regions. the number of distinct haplotypes explaining the given number of individuals presumably remains at high diversity as the number of individuals grows, whereas the methods based on maximum parsimony principle fail to incorporate this fact. they are tend to find parsimonious  solutions in all population sizes, with a decreasing ratio  of “total number of distinct haplotypes explaining the given set of individuals” to “total number of given individuals” as the population size grows. it is worthy of noticing that, in xhsd results in figure  <dig> , we observed that such ratio decreases as ρ =  in respect to the populations with  <dig> , <dig> , <dig> individuals; whereas the same ratio for the true phasing  is in fact much higher, i.e., ρ = , respectively, thereby causing the parsimony-based haplotyping methods to deteriorate on this database. on the other hand, in high-ld cftr database, the same ratio for the true phasing is very low due to low haplotype diversity, i.e., ρ = , in respect to the populations with  <dig> , <dig>  individuals, and the xhsd method is good at achieving very similar rates,i.e., ρ = , respectively.

CONCLUSIONS
in this paper, we have presented a new xor-haplotyping method xhsd based on the maximum parsimony principle that infers the haplotype pairs for each member of a group of unrelated individuals by observing their xor-genotypes. a dictionary selection method is utilized to find the smallest set of haplotypes selected from a candidate set that can explain the given set of xor-genotypes. the proposed approach requires regular genotypes from only a small percentage of individuals for the removal of ambiguity across all snps of the inferred haplotypes. the smallest subgroup of individuals having the most informative regular genotypes are efficiently determined by the minimum tree intersection algorithm. although the inference accuracy was proportional to the percentage of the individuals given by regular genotypes, xhsd shows less dependency on regular genotypes compared to other methods. experimental results have demonstrated that xhsd is a reliable method for xor-haplotyping under all circumstances including missing data and typing error cases. low rates of missing values  on the xor-genotypes has often insignificant contribution to the error rates, and the proposed method can deal with ∼ 2% typing errors. particularly for large databases, xhsd produces the most accurate solution with significantly low error rates compared to other low-complexity xor-haplotyping methods. experiments with cftr gene database also proved that our approach can perform effectively on real data sets with/without missing sites. another database with particularly lower ld rates indicates that the proposed algorithm can achieve the best performance with the state-of-the-art algorithms. we expect that xhsd can serve as a practical tool for xor-haplotyping on real-world large instances, as the large data collections become more available in the era of next-generation dna sequencing.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
xw and gj conceived of the project. ae, gj and xw participated in the design of the method. ae performed the computer experiments and contributed in the writing of the draft. all authors read and approved the final manuscript.

supplementary material
additional file 1
matlab implementation. this file includes the matlab code of the proposed algorithm, and an implementation with the example database, cftr.

click here for file
