BACKGROUND
the automated extraction of gene and/or protein interactions  from the literature is one of the most important targets of biomedical text mining research. from a biological standpoint, there are several distinct sub-types of gpi, including the direct physical interactions between proteins, protein-dna interactions , and the encoding of a given protein by a specific gene. however, in the biomedical text mining community it is common practice to treat all these interactions as belonging to a single task – here termed the gpi extraction task.

by providing a publicly-available mechanism for converting multiple gpi corpora to a common format, pyysalo and co-workers have made it comparatively easy to undertake rich analyses of the performance of gpi extraction tools  <cit>  comparable to those already undertaken for named-entity recognition tools  <cit> .

included within the paper by pyysalo and co-workers is an assessment of the performance of the relex gpi tool  <cit>  and a simple co-occurrence based method. although the analysis is both interesting and instructive, it is primarily intended to shed light on the differences between gpi corpora and does not, we believe, provide a realistic assessment of gpi tool performance for the following reasons:

• the analysis uses the gold standard named entities  as annotated within the corpora, and hence does not take into account the effects of named entity recognition  errors.

• the authors re-implemented relex , whereas a typical non-specialist user will prefer off-the-shelf tools that are relatively easy to install and use.

the use of gold standard annotations in the evaluation of gpi methods is commonplace. this includes shared tasks such as the lll challenge, for which the exact location of the target entities within the text was provided in advance  <cit> . for the gpi pairs sub-task at biocreative ii, a list of relevant gene mention symbols and synonyms was provided in advance  <cit> .

however, this evaluation protocol is potentially highly misleading, as the performance scores it awards to gpi algorithms are rather inflated. our previous analyses of gene/protein tagger performance have shown that exact matching to the boundaries of the manually-annotated entities in a range of corpora is around 50–60% with the best performing taggers  <cit> . an evaluation of the impact of tagger errors on the performance of gpi methods is one of the main goals of this paper.

here we present an evaluation of several gpi methods coupled with a state-of-the-art entity tagger on five gpi corpora. in addition to assessing the performance of each method, we consider how much effort is involved in setting it up and using it, as we believe this is a key issue for most non-specialist users. taken as a whole, we believe our analysis represents the first realistic evaluation of gpi extraction methods, and sheds light on their performance in a way that is directly relevant to both users and developers.

methods
gpi corpora
the five gpi corpora used in this evaluation were: the aimed corpus  <cit> , the bioinfer corpus  <cit> , the hprd <dig> corpus  <cit> , the iepa corpus  <cit> , and the lll training corpus, a gpi corpus produced for the lll challenge  <cit> . here we provide a short summary of the five corpora. for a more detailed comparison, see  <cit> .

the aimed corpus contains  <dig> abstracts manually annotated for interactions between human genes and proteins. most of the abstracts contain interactions, but a significant percentage  do not, and were deliberately added to provide negative examples. the hprd <dig> corpus contains  <dig> abstracts in which human gene and protein names were automatically identified using the prominer protein and gene name tagger  <cit> . the iepa  corpus contains  <dig> abstracts from pubmed, each containing a specific pair of co-occurring chemicals obtained using  <dig> queries chosen to represent diverse biological research topics. the lll corpus was created as the shared dataset for the learning language in logic  <dig>  challenge and contains  <dig> sentences. the domain of lll is gene interactions of bacillus subtilis. the bioinfer corpus consists of  <dig> sentences from pubmed abstracts that contain at least one pair of interacting genes or proteins. all protein, gene and rna entities were manually annotated, together with all interactions between these entities, including static relations. each interaction is mapped to the bioinfer relationship ontology, defined especially for this purpose. bioinfer permits the annotation of relationships with a complex structure, such as relationships between relationships, or relationships of more than two entities.

these corpora differ significantly in their working definitions of the concept "gene/protein interaction". for example, in the iepa corpus an interaction is a "direct or indirect influence of one on the quantity or activity of the other"  <cit> , whereas bioinfer additionally contains so-called "static" entity relationships, such as family membership. nevertheless, an analysis by pyysalo and co-workers has shown that "a clear majority of all interactions ... correspond to events occurring as part of biochemical processes in living cells", as opposed to static relationships  <cit> . a more recent paper by pyysalo and a different set of co-workers advocates addressing the extraction of static relationships as a distinct subtask  <cit> , but this is not tackled by existing publicly-available tools.

for our analysis we converted all five corpora to a unified format using the conversion software provided by pyysalo and co-workers  <cit> . to simplify our analysis, all  <dig> sentences in the bioinfer corpus that contain at least one discontinuous entity were discarded. for example, in the phrase 'myosin heavy chain and light chains', the annotated entities are 'myosin heavy chain' and 'myosin light chains', although the latter does not appear as a continuous string in surface text.

gene/protein taggers
in two earlier papers we concluded that the version of abner  <cit>  trained on the biocreative corpus  <cit>  was the best performing tagger on a range of biomedical corpora  <cit>  and on a new corpus – immunotome – consisting of ten full-text immunological articles  <cit> .

however, since the publication of those papers, we have evaluated banner, a new biomedical named-entity recognition system implemented using conditional random fields  <cit> . banner exploits a range of orthographic, morphological and shallow syntax features, such as part-of-speech tags, capitalisation, letter/digit combinations, prefixes, suffixes and greek letters. as with the best-performing version of abner, banner was trained on the biocreative corpus.

as shown in tables  <dig> and  <dig>  banner consistently outperforms abner on the same corpora used in our earlier evaluations , and has therefore been used for the analysis of gpi methods we present here.

abbreviations are as follows: y = yapex; j = jnlpba evaluation corpus; p = prospectome; i = immunotome; b = biocreative.

abbreviations are as follows: y = yapex; j = jnlpba evaluation corpus; p = prospectome; i = immunotome; b = biocreative.

gpi extraction methods
a number of different gpi extraction methods have been published in the literature , with some  <dig> teams submitting runs for at least one of the gpi annotation extraction tasks at biocreative ii  <cit> .

however, our purpose here was to undertake an evaluation of the state-of-the-art in gpi extraction relevant to potential non-specialist users. in contrast to entity taggers, a number of which are easy to install locally or can be accessed directly via the web, none of the gpi extraction methods are trivial to install and use. this is partly a consequence of the complex, modular nature of a typical state-of-the-art gpi method that combines third-party components  with a machine learning or rule-based algorithm for identifying possible relationships within a given parse. as noted in  <cit> , the vast majority of such gpi methods are currently not publicly available.

here we focus on four gpi methods: akaneppi, whatizit, opendmap, and a simple benchmark approach that we developed ourselves using perl regular expressions. one system we have not evaluated, even though it is designed primarily for non-specialists and is easy to use, is ihop  <cit> . ihop is a dictionary-based system that uses genes and proteins as hyperlinks between sentences and abstracts in order to navigate information in pubmed. when it comes to gpi, for every gene detected in a query, there is a link that leads to sentences  which describe interactions of that gene with other genes. however, ihop does not accept text submitted by the user, making it unsuitable for the analyses we undertook for this paper.

akaneppi  <cit>  is a state-of-the-art gpi method for which the c++ source code is publicly available. akaneppi combines the version of the deep syntactic parser enju that has been retrained on the genia corpus  <cit>  with a shallow dependency parser  <cit> . a support vector machine with tree kernels  <cit>  is used to extract rules for identifying pairs of interacting genes/proteins from a training corpus. here we used two versions of akaneppi, the original, distributed version ) trained on the aimed corpus, and a second version ) we retrained ourselves on the bioinfer corpus. the authors report an f-score of 52% for gpi extraction from unseen abstracts  <cit> .

opendmap  <cit>  is a general-purpose parsing and information extraction platform that provides an open source java api. it was adapted to perform gpi extraction for the protein interaction pairs subtask at biocreative ii  <cit> , where it outperformed other participating systems, achieving precision of 39% and recall of 31% when scores were averaged over articles  <cit> . opendmap uses a rule-based approach. for biocreative ii, patterns were devised manually from the biocreative, picorpus  <cit>  and prodisen  <cit>  corpora in consultation with biologists. these patterns have been made available for download together with the main distribution and have been used here.

whatizit  <cit>  is a modular text processing system available through the ebi website. of the wide range of text mining services on offer, here we focused exclusively on the protein interaction pipeline.  the pipeline begins by mapping gene/protein names to uniprot identifiers using dictionary look-up. it then attempts to identify relationships between any successfully mapped names using three approaches of decreasing precision, but increasing coverage: natural language processing ; the co-occurrence of two gene/protein names with an interaction verb ; and the co-occurrence of two names without an interaction verb . the abbreviations here are the ones used on the protein corral website.

finally, we developed our own simple baseline method using perl regular expressions. every time two gene/protein names occur together within a sentence and have an interaction keyword between them they are predicted to be an interacting pair of genes/proteins. a minority of the interaction words were inherited from two earlier projects – gift  <cit>  and graphspider  <cit> . the former derived its verb list from flybase  <cit>  and the latter from the lll training corpus. the remaining verbs were obtained semi-automatically using the cluetype event attribute in the genia event corpus  <cit> . our list of interaction keywords and perl script are available as supplementary material .

in addition, we compare the performance of our baseline method with the simpler co-occurrence baseline previously used by pyysalo and co-workers, which predicts an interaction between every pair of genes/proteins co-occurring in a sentence irrespective of whether an interaction verb is present  <cit> . to easily distinguish between these two baseline methods within this paper, we call our keyword baseline method baseline and the simple co-occurrence baseline method of pyysalo et al. baseline.

evaluating performance
with the unified corpus format used here, all interactions are both undirected and binary. hence, when the gold standard named entities are used, scoring is straightforward – either a given interaction has been predicted, or it has not. however, when a tagger is used to identify putative gene/protein entities, there is more than one way to score predicted interactions.

elsewhere we have argued that "sloppy" matching criteria  provide a fairer evaluation of tagger performance than "strict" matching criteria , as the latter is more sensitive to the essentially arbitrary choices made when drawing up annotation guidelines for the evaluation corpora – for example, whether the word "mouse" is part of the protein name in the phrase "mouse oxytocin"  <cit> .

in the context of gpi extraction, the use of "sloppy" matching criteria is somewhat more complex, as the following exemplar sentence from the lll corpus illustrates:

three new sigmab-dependent genes  encoding proteins with still unknown functions were also described.

in the lll corpus, three interactions are annotated within this sentence – sigmab with ydae, ydag and yfkm respectively. however, we found that banner tagged the phrase "sigmab-dependent genes" as a single entity, an entity that has no interactions with ydae, ydag and yfkm . in this example, it would be misleading to penalize the gpi extraction method for failing to identify the three interactions in the lll corpus if it was using the entities identified by banner.

the question arises, therefore, whether the use of sloppy matching criteria for gene/protein names is, on balance, more informative than strict matching criteria in the context of gpi extraction. to investigate this issue, we manually inspected those interactions extracted by banner and akaneppi from the two smallest corpora that count as misses using strict criteria but hits using sloppy criteria. out of  <dig> such interactions from the lll corpus, we judged  <dig> to be valid interactions and six to be erroneous. of these six, two had erroneous names "), whereas in the remaining four the names were valid, but the interactions invalid. for example, for the sentence from lll given in the preceding paragraph, akaneppi wrongly predicted that the tagged entity "sigmab-dependent genes" interacts with "ydag" and "yfkm". out of  <dig> such interactions from the hrpd <dig> corpus, we judged  <dig> to be valid interactions and four to be erroneous. of these four, all are attributable to invalid names being tagged by banner .

to quantify this effect, we calculated the corrected f-score for the two corpora by taking account of the false positives uncovered during our manual analysis. for lll, the corrected f-score was  <dig> percentage points lower than the sloppy criteria f-score and  <dig> percentage points higher than the strict criteria f-score. for hprd <dig>  the corrected f-score was  <dig> percentage points lower than the sloppy criteria f-score and  <dig> percentage points higher than the strict criteria f-score. consequently, we conclude that sloppy matching criteria are significantly more informative than strict matching criteria, and it is the results for sloppy criteria that we report in this paper.

RESULTS
ease of usage
we begin by considering how easy each of the major gpi tools discussed in this paper – akaneppi, whatizit and opendmap – is to install and use.

setting up the akaneppi system involves downloading and compiling the c++ source code. on running the make script provided with the distribution, several components are retrieved from their respective websites, notably the enju parser  <cit> , tinyxml  <cit> , svmlight with tree kernels  <cit> , and standoff manager  <cit> . we found akaneppi easy to build and run on the pre-parsed example supplied, but installing the enju parser was non-trivial owing to problems encountered with environment variables and library dependencies. enju only claims linux compatibility, consequently we were unable to build it on a mac, despite akaneppi advertising mac compatibility.

akaneppi is supplied with a configuration file tailored to the aimed corpus. although this could be modified to improve the performance of akaneppi on different data, most of the settings require linguistic expertise to understand how this might be achieved. for this paper we have used the aimed configuration file with only trivial modifications , as we are primarily interested in evaluating tools from the perspective of the general user.

via its web interface, the whatizit protein interaction pipeline is easy to use, but the output does not specify what interactions are present , nor does it specify interaction confidence levels. however, both of these output features are available when the pipeline is used as a webservice or servlet. the example java client provided is straightforward to adapt given moderate proficiency in writing java. output from the pipeline is in xml .

although arguably sensible for some applications, the requirement that gene/protein names are mapped to uniprot identifiers makes the whatizit protein interaction pipeline different to the other systems evaluated here. it is not possible to use pre-tagged entities with the pipeline, be they the gold-standard corpus entities used in our evaluation, or those generated by a state-of-the-art tagger such as banner. moreover, in the case where the same protein name occurs more than once in a single sentence, the pipeline does not specify which of the names it has identified as providing evidence that an interaction is occurring.  this may be of little practical significance, but it does necessitate the use of weaker scoring criteria than are generally applied to the evaluation of gpi extraction methods. consequently we excluded whatizit from most of the comparative evaluations undertaken for this paper, although we do separately assess its performance on our chosen gpi corpora.

it is also worth noting that, as the whatizit protein interaction pipeline is a remotely-hosted service , the user is dependent on the reliability of third-party service provision. our experience was that the service was unavailable for several days during our evaluation period lasting several weeks.

installing opendmap was reasonably straightforward but not entirely trivial. it is supplied as a tarball containing the java source, technical documentation, etc., plus a pre-compiled binary. as well as the main distribution, the user must obtain a pair of jar files from the protégé  <cit>  distribution, and for gpi extraction, a set of supplementary patterns  that are provided separately. to configure opendmap to use these patterns also requires an xml configuration file, but this is not supplied with the patterns, so we wrote our own by reference to one of the examples provided with the main distribution.

submitting arbitrary text with marked-up entities is not a trivial task either, and we were able to achieve this only with help from the authors, including some sample code custom-written for our requirements. our experiences are consistent with the fact that opendmap is not designed as a ppi/gpi application for biology or bioinformatics researchers, even with the availability of the biocreative project files, but rather as an extensible tool for nlp research and language engineering.

of the tools evaluated for this paper, the web interface to the whatizit protein interaction pipeline is by far the easiest to use, but also somewhat restricted; it will not suit all potential users and applications. in the case of all other tools, none proved entirely trivial to download and use. indeed, in every case we found it necessary to contact the authors in order to get the tool to work properly. our conclusion is that, with the notable exception of whatizit, the vast majority of biologists will not be able to install and use these tools – in spite of the fact that biologists are one of the most important groups of potential users for this kind of tool.

the performance of banner
before moving on to evaluate the performance of the gpi extraction tools on the gpi corpora, it is useful to consider how well the gene/protein name tagger banner performs on these corpora. the performance of banner is summarized in table  <dig>  here banner is shown to perform best on bioinfer, the corpus with by far the highest annotation density  <cit> . the poor performance on iepa is attributable to the fact only a subset of the gene and protein names in this corpus are annotated , whilst the labelled entities in this corpus include cholesterol, gibberellins, and flavonoids in addition to genes and proteins .

corpus abbreviations are as follows: a = aimed; b = bioinfer; h = hprd50; i = iepa; l = lll.

this level of performance is broadly comparable to that for standard gene/protein ner corpora, as reported in table  <dig> 

the performance of gpi extraction methods with gold-standard entities
the performance of our chosen gpi extraction methods on the five gpi corpora with gold-standard named-entity annotations is summarized in table  <dig>  in terms of f-score, the key features of these results are as follows:

the figures for relex and baseline are taken from pyysalo et al. .  figures are given in brackets where a corpus was used to develop a given method. corpus abbreviations are as follows: a = aimed; b = bioinfer; h = hprd50; i = iepa; l = lll.

• the best method is the rule-based relex, which is not, however, publicly available.

• although opendmap has the highest precision, it has by far the lowest coverage leading to the worst over all performance on all corpora. given that its coverage is so low , we have largely excluded it from our subsequent analyses. its poor performance is most likely attributable to the biocreative pattern set being optimized specifically for protein-protein interactions , rather than being a fundamental characteristic of the underlying approach – there are no patterns in this set based around words like "transcribed", "express" or "induction" which are very common in sentences describing gene regulation events. however, opendmap's biocreative entry showed highly variable results even on this more constrained topic, where its interaction extraction f-score on its own training data was only  <dig> %  <cit> .

• akaneppi trained on bioinfer performs significantly better than akaneppi trained on aimed. in this context it is worth noting that our earlier experience with the tagger abner indicates that the choice of training corpus can have a significant impact on the performance of a text-mining tool  <cit> .

• our simple keyword-based baseline approach, baseline, performs surprisingly well. it out-performs the standard version of akaneppi  on four of the five corpora. it also out-performs the simpler co-occurrence baseline approach, baseline, on three of the corpora.

• the performance of all gpi extraction methods except opendmap is broadly correlated across all corpora. scores are consistently worst on bioinfer and aimed, and significantly higher on the other three corpora.

however, this is only part of the story, as the gpi extraction methods differ significantly in their relative performance with respect to precision and recall. broadly speaking, the two baselines are high-recall, low-precision methods, whereas opendmap and akaneppi are low-recall, high-precision. only the two methods that are not publicly available, akaneppi and relex are reasonably balanced in their precision/recall performance.

finally, we wished to estimate the significance of removing the  <dig> sentences containing discontinuous entities from the bioinfer corpus. to do this we evaluated the performance of akaneppi on the entire bioinfer copus. the result was that the f-measure dropped by  <dig>  percentage points. this suggests that removing these sentences does not have a large impact on the validity of the reported scores.

joint error analysis
to understand the nature of the gpi extraction errors and whether they are correlated between different tools, we undertook a joint error analysis for akaneppi, akaneppi and baseline.  given that the two versions of akaneppi were trained on aimed and bioinfer respectively, whereas baseline incorporates keywords from the lll corpus, we have performed our analysis on the only corpora that represent unseen data for all three methods: hprd <dig> and iepa.

of the  <dig> interactions in the hprd <dig> corpus,  <dig> were detected by all three methods and  <dig> missed by all three. there were  <dig> false positive interactions common to all three methods. the number of true positive interactions identified by each method alone were  <dig> ),  <dig> ) and  <dig> ) respectively.

of the  <dig> interactions in the iepa corpus,  <dig> were detected by all three methods and  <dig> missed by all three. there were  <dig> false positive interactions common to all three methods. the number of true positive interactions identified by each method alone were  <dig> ),  <dig> ) and  <dig> ) respectively.

these results show that the correlation between the predictions of the different methods is relatively modest and that there is, as a consequence, significant scope for improving performance by combining the methods in a single predictive system. for example, it would be relatively easy to develop a high-recall system by naively combining akaneppi, akaneppi and baseline , although it would also generate large numbers of false positives.

we undertook a manual analysis of these results and identified the following key points:

• easy interactions . as expected, the interactions correctly identified by all three systems consist of relatively simple sentences containing an interaction verb that is on the baseline list, for example the interactions between a   and pip2-plc in the following sentence from the iepa corpus: "moreover, a  significantly decreased the basal activity of the pip2-plc in spm and the enzyme activity regulated through cholinergic receptors."

• illusory interactions . of the  <dig> false positive interactions for both corpora,  <dig> are attributable to negation, i.e. where the sentence says that two genes/proteins do not interact. for example, the interactions between a beta and pi-plc, and a beta and pip2-plc in the following sentence from the iepa corpus: "moreover, a beta 25– <dig> had no effect on basal pip2-plc activity and cytosolic pi-plc and pip2-plc."

the other joint false positive interactions may be attributable to sentence complexity. hence, for example, the predicted interaction between leptin and npy in the sentence: "significantly increased leptin and galanin levels in postmenopausal obese women coupled with decreased npy levels revealed some changes in the neuropeptides regulating eating behavior, which may be the reason for the onset of postmenopausal obesity."

• elusive interactions . manual analysis of a subset of the jointly missed interactions indicates that a large proportion are associated with sentences describing a specific set of processes that includes cross-linking, immunopercipitation with antibodies, cross talk and immunolocalisation. for example, all the tools missed the interaction between cb <dig> and orexin  <dig> receptor and/or ox1r in the sentence "in the present study, we observed evidence of cross-talk between the cannabinoid receptor cb <dig> and the orexin  <dig> receptor  using a heterologous system."

• the strengths and weaknesses of baseline. the baseline algorithm often correctly retrieves interactions from complex sentences that the other methods failed to parse successfully, for example the interaction between clip- <dig> and phospho-lis <dig> in the sentence "overexpression of clip- <dig> results in a zinc finger-dependent localization of a phospho-lis <dig> isoform and dynactin to mt bundles, raising the possibility that clip- <dig> and lis <dig> regulate dynein/dynactin binding to mts." on the other hand, baseline is prone to generate false positives whenever there are many entities in a sentence, as it predicts an interaction between every pair of entities that are separated by an interaction keyword.

the effect of ner on gpi extraction
the effects of using the banner gene/protein tagger on the performance of akaneppi and akaneppi are shown in tables  <dig> and  <dig> respectively. these results show that using the banner tagger rather than the gold-standard entities leads to a significant drop in performance, with a fall of around  <dig> percentage points being commonplace. the negative impact of using banner is broadly correlated with the performance of akaneppi on a given corpus; the better its performance using gold-standard entities, the greater the negative impact of using banner.

corpus abbreviations are as follows: a = aimed; b = bioinfer; h = hprd50; i = iepa; l = lll.

corpus abbreviations are as follows: a = aimed; b = bioinfer; h = hprd50; i = iepa; l = lll.

intuitively we expected the use of banner to have a greater impact on recall than precision on the grounds that akaneppi would not be able to compensate for missed entities , but would frequently be able to reject erroneous entities , as we expected the latter would often not be engaged in an apparent interaction. however, this was not the case in practice. indeed, with akaneppi the drop in precision was always significantly greater than the drop in recall, with even a slight improvement in recall being experienced with the lll corpus.

of the  <dig> false positive gene and protein names identified by banner in the lll corpus,  <dig> are involved in  <dig> relationships that akaneppi identifies as gpis but which are not annotated as such in the corpus. our manual analysis of these relationships shows that they fall into  <dig> main categories:

 <dig>  invalid interactions involving invalid names. only two of the  <dig> putative names tagged by banner are not the names of genes or proteins: "shaa mutant" , and "asia form bacteriophage t4" . these two names participate in five of the  <dig> false positive interactions.

 <dig>  invalid interactions involving valid names. in these cases banner tagged a valid gene or protein name that had not been annotated in the lll corpus, but one or more interactions identified by akaneppi involving this gene or protein are nevertheless invalid. this situation accounts for  <dig> of the  <dig> false positive interactions. here is an example :

dnase i footprinting showed that spoiiid binds strongly to two sites in the cotc promoter region, binds weakly to one site in the cotx promoter, and does not bind specifically to cotb.

in this case banner additionally tagged "dnase i" and akaneppi identified an erroneous interaction between "dnase i" and "cotb".

 <dig>  more-or-less valid interactions involving valid names. this situation accounts for  <dig> of the  <dig> false positive interactions. in these cases the lll corpus has failed to annotate an essentially valid interaction because:

• there is an alternative, more specific name available. take for example the phrase

the sigma w regulon includes a penicillin binding protein  and a co-transcribed amino acid racemase ...

here banner tags "penicillin binding protein" instead of "pbp <dig> *" and an interaction between "sigma w regulon" and "penicillin binding protein" is annotated by akaneppi.

• a gene or protein name is deemed to be too general by the lll corpus. for example,

our data demonstrate that the ctsr protein acts as a global repressor of the clpc operon, as well as other class iii heat shock genes...

here banner additionally tags "class iii heat shock genes".

• the gene or protein is arguably involved in an uninteresting, though valid, interaction. for example:

in contrast, sspj is transcribed in the forespore compartment by rna polymerase with the forespore-specific sigmag and appears to give a monocistronic transcript.

here banner tags "rna polymerase" akaneppi identifies an interaction between "rna polymerase" and "sspj", an interaction that lll presumably ignores because it is uninteresting.

the fact that only  <dig> of the  <dig> interactions that arise from false positive names are, in fact, valid suggests that over-prediction by gene/protein name taggers is potentially a serious problem. an analysis by pyysalo and coworkers  <cit>  appears to shed light on this effect. their analysis suggests that the performance of gpi extraction methods is correlated with the i/ep of a given corpus, where i is the average number of interactions per sentence and ep is the average number of entity pairs per sentence  <cit> ; roughly speaking, the smaller the value of i/ep, the more difficult the corresponding gpi extraction task. they go on to point out that, "as more proteins are annotated, we would not expect i to grow more than linearly, while ep grows quadratically". in this context, the potentially damaging effect of false positive predictions by a name tagger such as banner is clear  – the growth in the number of entity pairs makes the gpi extraction task more difficult.

it is worth noting that in the vast majority of these cases where the entities tagged by banner lead to akaneppi extracting a false positive interaction, the use of gold standard entities does not lead to the detection of the correct interaction, even though akaneppi is able to avoid tagging the incorrect interaction. in other words, entities tagged by banner are primarily responsible for increasing the numbers of false positives, and not for preventing the detection of true positives with akaneppi. this is consistent with the full set of results for akaneppi on multiple corpora in table  <dig>  which show a modest drop in recall with entities tagged by banner, but a large drop in precision.

one additional result that requires further explanation concerns the performance of akaneppi on the lll corpus; akaneppi had higher recall with tagged entities than with gold standard ones  – in spite of the fact that banner missed  <dig> genes/proteins . on manual inspection it was apparent that the improved recall is attributable to banner's tendency to tag longer versions of gene and protein names than appear in the gold standard annotations. for example, banner tags "cotx promoter" instead of "cotx" and, more dramatically, "ifn alpha tyrosine kinase tyk-2" instead of "tyk 2".

this is potentially significant because gpi prediction methods typically inherit a single token instead of the original, potentially multi-word name from the tagger; hence in cases such as these, akaneppi is making predictions for sentences with a somewhat simpler – and sometimes much simpler – structure than that of the original text. in specific cases , this enables both versions of akaneppi to make correct predictions where they are unable to do so when the sentence is in its original, more complicated, form. however, the drop in recall for akaneppi is consistently high, so in practice this effect does not appear to be highly significant.

finally, the effect of using the banner ner tagger on the performance of our simple baseline algorithm is shown in table  <dig>  as with akaneppi, the smallest drop in performance is on aimed and bioinfer, for which the original performance was lowest. it is also worth noting that recall is on average no worse than that for akaneppi, even though there is no scope for elongated name tagging to be beneficial with our benchmark algorithm.

corpus abbreviations are as follows: a = aimed; b = bioinfer; h = hprd50; i = iepa; l = lll.

performance of the whatizit protein interaction pipeline
to evaluate the whatizit protein interaction pipeline, we adopted a slightly different approach for the reasons given above. we used un-annotated text as input and then scored the putative interactions generated by the pipeline against the gold standard entities and interactions in the evaluation corpora. in the case of sentences containing multiple occurrences of the same protein name, the pipeline was given full credit for identifying the correct name even though the precise context was not specified.

results for the three whatizit interaction detection methods – ppi, co <dig> and co – are given in tables  <dig>   <dig> and  <dig> respectively. these results indicate that the ppi method produces very low recall, ranging from  <dig> % on iepa to  <dig> % on aimed . although the co <dig> approach is very similar to our baseline algorithm, it achieves significantly lower performance. this is attributable to the much lower rate of entity detection with whatizit, a consequence of its requirement that only names it is able to map to uniprot identifiers are tagged.

corpus abbreviations are as follows: a = aimed; b = bioinfer; h = hprd50; i = iepa; l = lll.

corpus abbreviations are as follows: a = aimed; b = bioinfer; h = hprd50; i = iepa; l = lll.

corpus abbreviations are as follows: a = aimed; b = bioinfer; h = hprd50; i = iepa; l = lll.

of the three approaches, the simple name co-occurrence  approach gives the best results. in the case of co, the low rate of whatizit entity detection actually proves beneficial, as it reduces the number of false positive interactions that would otherwise be detected using this naïve approach.

CONCLUSIONS
in this paper we have uncovered some sobering facts about the current state of automated gpi extraction. firstly, in spite of all the research that has been undertaken to develop relatively sophisticated gpi extraction methods using grammatical parsers, we have concluded  that, with the exception of the whatizit protein interaction pipeline accessed via its web interface, none of the tools are easy to install and evaluate. although akaneppi performs significantly better than opendmap, what is most startling is that both tools generally perform worse  than a simple keyword-based approach using regular expressions.  better-performing tools have already been developed , but are not available to the vast majority of potential users.

secondly, we have quantified the effect of using a state-of-the-art ner tagger on the performance of three gpi extraction methods when evaluated on standard corpora. a drop in f-score of over  <dig> percentage points occurred in  <dig> out of  <dig> cases. moreover, when the performance of the gpi extraction method is at its best, the typical drop is even greater – a point that is worth bearing in mind when reading reports of high-performing gpi methods. for example, if we take the two best f-scores for each of the extraction methods, the average drop in f-score with banner is more than  <dig> percentage points.

these results emphasize two points – the urgent need to make the best tools publicly available, and the need to carry out realistic evaluations of tools using name taggers and multiple corpora.

finally, this paper has identified three areas that may prove fertile for additional research. firstly, given the significant impact of gene/protein tagging on gpi extraction performance, we think it worth investigating whether other gene/protein name taggers produce less significant drops in performance than banner. although we show that banner is the best available tagger in terms of general performance on the gene/protein named entity recognition task, this does not necessarily mean that it is the best tagger for gpi extraction.

secondly, our joint error analysis of three tools shows that very high levels of recall  would be achievable using a system that combined all three of them, suggesting that hybrid systems may prove highly effective. 

thirdly, our baseline approach using perl regular expressions was deliberately simple. given that it performed surprisingly well compared to more sophisticated tools and that it correctly identifies many interactions not found by other methods, we believe that it would be worthwhile devising and evaluating more complicated approaches that exploit simple regular expressions.

authors' contributions
rk designed and wrote the scripts used in the experiment, analyzed the results and drafted the manuscript. abc devised the keyword-based benchmark algorithm and set up and executed opendmap. ajs participated fully in the experimental design and data analysis, and contributed to the writing of the manuscript. all three authors co-edited, read and approved the final manuscript.

supplementary material
additional file 1
gpi keywords. a text file containing the list of interaction keywords used by the baseline method.

click here for file

 additional file 2
baseline k perl script. a perl script that implements the baseline method. to be used in conjunction with the keyverbs.txt file. zipped pl file

click here for file

 acknowledgements
we would like to thank rune sætre for helping us install akaneppi locally at birkbeck, william baumgartner for supplying us with custom-written code that enabled us to evaluate opendmap, and nigel ferguson at birkbeck for tagging our corpora using banner. the research of r.k. was supported by the european commission under fp6-2004-ist- <dig> contract no.  <dig>  and the uk orsas scheme.
