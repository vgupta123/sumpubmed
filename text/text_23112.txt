BACKGROUND
in the field of cancer research, classical clinical variables have long been used as prognostic markers. indeed, many strong clinical determinants that explain most of the prognosis have already been identified. nevertheless, certain characteristics of cancer are still poorly understood, and these need to be elucidated to improve treatment. thus, cancer research is making use of new technologies, especially microarrays, and survival analysis methods have been extended to take into account the potential information from microarray analysis with the hope that this transcriptomic information will supersede clinical data. for example, shipp et al.  <cit>  showed that a 13-genes-signature-based outcome predictor provided additional information not reflected in the clinical prognostic model based on the international prognostic index. today, cancer clinicians would like to combine genes and classical clinical variables in the same models to improve assessment of cancer prognosis. in the recent years, some authors addressed this question in two ways. the first way is to involve classical clinical and transcriptomic variables in the same models  <cit> . however, not all authors took into account the particularity of each type of variable. the second way is to consider the additional predictive value of genes. in this context, tibshirani and efron  <cit>  warned in  <dig> against too premature conclusions regarding the predictive power of transcriptomic variables. using the breast study from van't veer  <cit> , those authors showed that the effects of genes were overestimated with regard to the classical clinical variables like tumor grade or size. they proposed a "pre-validation" strategy to correct the artificial importance given to genes.

in the very recent literature, few authors joined those two ways in unique models. thus, binder and schumacher  <cit>  proposed an offset-based boosting approach in the context of survival data. this approach allows also answering the question whether prediction is improved or not by adding transcriptomic variables to classical clinical variables in the same model. shortly later, boulesteix et al.  <cit>  proposed a more general approach than that of binder and schumacher, in the sense that it was not limited to survival data. this approach is based on pls, random forest, and the pre-validation strategy suggested by tibshirani and efron.

the interesting thing in the two above approaches is that they allow simultaneously to construct a classifier combining both types of variables and to determine whether microarray data present additional predictive value.

in agreement with this consideration, we think it is of major importance when designing statistical models to keep in mind that the characteristics of transcriptomic variables are completely different from those of clinical variables. specific clinical variables have already been validated in many large studies. thus, most of these clinical variables are no longer included in the selection process . in contrast, the selection step is still needed for genes, and various issues are still a matter of debate. first, fewer studies have been conducted on transcriptomic variables than on clinical variables; thus, there are fewer datasets available to repeat the analyses and validate the relationships. in most cases, genes selected in a single study are assumed to have a general prognostic value. this selection is presented as a benchmark for the disease without external validation studies on new datasets. second, whenever available, these datasets are rather small compared to the number of genes under study. considering the high number of variables and the relatively low number of observations, microarray data can easily lead to a high number of false-positive variables. by chance alone, many genes may be found significantly associated with the outcome even though most of them may not actually be linked to prognosis.

some publications have clarified certain additional issues related to the selection process in microarray analysis. eindor et. al showed that the final gene signature depends highly on the subset of patients used for gene selection  <cit> . later, the same team pointed out that the reproducibility of a signature depends on the number of samples used for the analysis  <cit> . other teams were interested in the false discovery rate ; that is, the expected proportion of false positives among the genes declared as significant. when looking for differential genes, pawitan et. al showed that the fdr is mostly influenced by the proportion of truly differentially expressed genes and by the sample size  <cit> .

the same problems are met in survival studies where the construction of transcriptomic models raises simultaneously the problem of restricting the number of genes to include and that of validating the selected genes. when a model is too complex – i.e., the number of free parameters to estimate is too high given the information contained in the data – the strength of that model will be exaggerated due to overfitting. some conclusions of the analysis may be due to noise or to some spurious associations between the covariates and the outcome. in this case, the model has high adequacy and predictive accuracy for the dataset on which it was built but is not able to accurately predict the outcome with new datasets. thus, the ability of the model to predict outcome with new datasets is overestimated: this is called optimism. the main objective of this article is to quantify to what extent the optimism of transcriptomic models induced by the selection process may lead to overestimation of the contribution of transcriptomic variables to prognosis, especially in comparison with clinical variables when they are included in a single model. to be able to control the contribution of the two types of variables, the following study was conducted on simulated datasets.

methods
to compare optimism relative to clinical and transcriptomic models within the context of survival, the study was based on simulated datasets that included both clinical and transcriptomic variables. we wanted to simulate the "real situation" faced by clinicians and statisticians: classical clinical variables are already validated, whereas transcriptomic variables are still in the selection and validation process. regarding clinical variables, only validated ones are considered to build combined classifiers, while regarding genes, many of the considered ones are still superfluous noise genes.

to analyze one dataset, the following procedure was employed:  the variables of interest were first identified;  once clinical variables or genes had been chosen, cox proportional hazards models including both types of variables were constructed;  to measure the predictive information contained in each survival model, the ρ <dig> measure of dependence from kent and o'quigley was used so that optimism for both types of variable could be compared  <cit> .

r and s-plus codes used in our analyses are available at .

simulation of the datasets
a classical way to link variables to censored survival data is to use the cox proportional hazards model. let us define x an  matrix of m variables for n individuals. for each of the n patients, the follow-up times were noted t <dig> ..., tn as were the event-indicators d <dig> ..., dn with di =  <dig> if the event occurred and di =  <dig> if it did not occur. at time t, the cox proportional model is given by:

  λ = λ0exp 

where λ <dig> is a baseline hazard function, β = {β <dig> ..., βm} is the vector of parameters and x <dig> ..., xm are the vectors of length n describing each of the m variables for the n patients.

through this model, we simulated a virtual population of size n in which the m variables consisted of both clinical and transcriptomic variables. the simulation process was inspired from the simulation study from gui and li  <cit> , which r code is publicly available.

more precisely, each patient was described by two clinical variables, p genes and survival information. the aim was to estimate in a single model the relationship between the two types of variables and survival times.

clinical variables were simulated using binomial distributions with probabilities  <dig>  and  <dig> , respectively, as parameters for success . normal distributions n were assumed for the transcriptomic variables. a weibull distribution with shape parameter  <dig> and scale parameter  <dig> was used for the baseline function. for censoring times, a uniform u was used, leading to about 40% censoring. the underlying model was:

  λ=λ0exp⁡ 

where xc and xt were respectively the matrix describing the clinical and the transcriptomic variables.

as there were two clinical variables, xc was an  matrix. as for genes, p were under study, leading to an  matrix xt. only p <dig> of the p genes were considered as related to survival; the p <dig> remaining genes were under the null hypothesis h <dig> of no association with survival. note that p = p <dig> + p <dig> and m = p +  <dig>  the relevance of most of the clinical variables had already been established through several studies. the two clinical variables were then considered significant, and coefficients for both of these variables were set at  <dig> : βc, i =  <dig> , i =  <dig>   <dig>  coefficients for transcriptomic variables related to survival were set at  <dig> : βt, i =  <dig> , i =  <dig> ..., p <dig> and the remaining p <dig> were set at 0: βt, i =  <dig>  i = p <dig> +  <dig> ..., p.

for a fixed set of parameters p and p <dig>  r =  <dig> training sets of n patients were simulated according to the design described above. for each of these training sets,  <dig> corresponding test sets were drawn following the same design. this overall process was performed varying n, p and p <dig> sequentially. the number of patients n was considered in { <dig>   <dig>   <dig>  400}, p was considered in { <dig>   <dig>   <dig>  4000} and p <dig> was considered in { <dig>   <dig>  20}.

a single cox proportional hazards model involving both clinical and transcriptomic variables could then be estimated for each of these simulated datasets, as described below.

variable selection and model construction
cox proportional hazards model
in the traditional cox model, the vector of parameters is such that it maximizes the following cox partial likelihood :

  pl=∏k∈dexp∑j∈rkexp 

where d is the set of indices of the events and rk is the set of indices of the individuals at risk at time tk.

in our case, there were p +  <dig> parameters to estimate. this leads to a huge number of variables in comparison with the number of individuals; the high dimensional space of the transcriptomic predictors thus precludes the use of the standard maximum cox partial likelihood method to estimate the parameters. several methods have been proposed in the literature to deal with this high dimension issue in survival models involving genes.

adaptation to high-dimensional data
the first solution aims at selecting a lower subset of genes according to the relevance of each gene taken separately. this approach takes each feature in an univariate way and also does not take into account the interactions between genes. we used the log-rank statistic to order genes; the number of genes to be involved in the model was chosen a priori. we retained the  <dig> genes with the highest statistical values, so that the number of selected genes was in the same order of magnitude as in the approach which follows. the second solution is a multivariate selection method that simultaneously selects genes and estimates their effect on survival. one way to do this is to maximize the partial likelihood under constraints using l <dig> or l <dig> penalization. contrary to the l <dig> penalization  <cit> , which uses all genes in the prediction, only some genes are used in the prediction with the l <dig> penalization  <cit> . the threshold gradient descent  method proposed by friedman and popescu  <cit>  allows a compromise between the l <dig> and l <dig> penalizations. through the choice of a defined threshold, it approximates the l <dig>  and l <dig>  penalized estimations. gui and li  <cit>  extended the tgd to the survival model and demonstrated the ability of their approach to select relevant genes and to provide good predictive performance. we therefore used this model to select the genes to include in our models.

briefly, the tgd method is based on the gradient method, which is classically employed to determine the minimum of a loss function. with this method, the parameters vector is derived in a sequential manner following the direction of the negative gradient of the loss function, here the partial log-likelihood noted l and defined as l = -logpl. the negative gradient is defined as: g = -∂l/∂βt starting with β^t =  <dig>  the vector of estimated parameters β^t is then updated at each iteration:

 β^t=β^t+Δν.h 

the parameter ν, which begins at zero, controls the number of iterations. Δ controls the incremental movement along the gradient. h is defined as: h={fj⋅gj}1p, with fj = i , i  being the indicator function, and τ ∈  <cit>  a user-defined constant. through f, only coefficients for which the gradient exceeds the threshold determined by τ are updated at each step. the final model is given by the value of ν which minimizes the cross-validated partial log-likelihood .

the final vector of parameters β^t has only one piece of non-null coefficients that corresponds to genes that are relevant to predict survival.

the number p <dig> of non-null coefficients depends on the choice of τ. note that the set of the p <dig> genes selected by the tgd may differ from the initial p <dig> set of simulated genes we considered linked to survival. with τ =  <dig>  all genes are kept in the final model. thus, all the predictive variables are selected but, in return, all the noisy variables are also selected. in contrast, with τ =  <dig>  only one gene is kept at each iteration. this time, only a restricted number of genes is selected. among these selected genes, the majority is actually predictive but, in return, some important variables are missed. we have chosen τ =  <dig> , which allows finding a compromise between the two extreme situations obtained with τ =  <dig> and τ =  <dig>  this choice leads to a limited number of selected genes: between  <dig> and  <dig> genes with our simulated datasets, which is a reasonable number of selected genes regarding the number of genes simulated under h <dig> 

although the tgd method combines selection of genes and estimation of their effect on survival, we used it only for selection purposes; that is, to select genes irrespective of their estimated coefficients.

thus, we used two approaches to select genes: a univariate approach using the log-rank, and a multivariate approach based on the tgd. as for clinical variables, they were considered as validated and, thus, directly included in the final model.

the optimism arising respectively from the clinical and the transcriptomic variables was then estimated and compared as follows.

comparison of the contribution of the variables to the prognosis
ρ <dig> as a measure of explained variability
different criteria allow selection and comparison of models based on their capacity to predict the outcome of individuals who did not participate in the model building. among them, explained variability reflects the robustness of the model, and its efficiency in predicting outcomes on new datasets. it measures the information given by some variables involved in a specific model.

in linear regression, the coefficient of determination r2quantifies the proportion of variability in a data set that is accounted for by a statistical model. kent and o'quigley proposed rewriting the r <dig> based on the kullback-leibler information  <cit> , which quantifies the information gain brought by variables involved for example in a cox proportional hazards model. this measure is defined as:

 ρ2=1−exp 

where  <dig> - i) quantifies the difference between information from the model with β^ estimated vector and the null model with no covariables. ρ <dig> is comprised between  <dig> and  <dig>  with ρ <dig> =  <dig> for the null model and ρ <dig> →  <dig> when all parameters tend to infinity.

application
we used this measure to quantify the optimism arising from the clinical and transcriptomic variables. for this, three ρ <dig> with different meanings were computed: ρpop <dig>  ρtr <dig> and ρte <dig>  these values were computed respectively for each type of variable in the model involving both of them.

ρpop <dig> reflects the information accounted for by the variables in the virtual global population. it is computed using the βt vector of coefficients defined earlier in the simulation process. only the p <dig> non null parameters contribute to its computation. ρpop <dig> therefore only depends on p <dig> and has the same value whatever n and p. in contrast, ρtr <dig> and ρte <dig> take into account the selection and the estimation processes. more precisely, ρtr <dig> reflects the information accounted for by the variables selected on one specific training set sampled from the global population. it is computed using the p <dig> coefficients of the model estimated on the training sets. ρte <dig> reflects the information accounted for on test sets by variables selected on the training set. it is computed using coefficients estimated on the test sets. we used ρ¯te <dig>  which is the mean of the ρte <dig> computed on the  <dig> test sets associated with each training set.

as for ρte <dig>  coefficients of genes selected on the training set were estimated on the test set. in fact, the computation of ρ <dig> only requires the knowledge of the coefficients of the cox model and the distribution of the corresponding predictive variables. by re-estimating the coefficients of the cox model on the test set to compute ρte <dig>  we are able to evaluate the predictive information actually given in the test set by the genes selected on the training set. a lack of re-estimation of the coefficients would have assumed that the predictive power of the genes selected on the training set-given by the coefficients of the cox model estimated on the training set- is equal on the test set. the selection process introduces much optimism. the latter must be taken into account by re-estimating the coefficients on the test set. by doing so, the predictive information of the selected genes will be closer to their effective predictive information in the test set.

these measures were estimated for both the clinical and the transcriptomic variables.

optimism quantification
optimism was quantified by computing relative differences between the various ρ <dig>  as described in equations  <dig> to  <dig>  by comparing ρ <dig> values estimated in the training and the test sets, Δtrte  shows the error made by considering that the signature given by one dataset is the real signature and delivers the same information on other datasets. in other words, it gives the difference between the predictive information anticipated on one dataset and the effective information on another.

Δtrpop  and Δtepop  compared respectively ρtr <dig> and ρ¯te <dig> with ρpop <dig>  both measures quantify the relative difference between effective predictive information in one dataset and the detected one. comparing ρpop <dig> to ρtr <dig> quantifies how the predictive information thought to be contained in one training set moved away from information contained in the whole population it is sampled from. it allows the validation process to be evaluated. comparing ρpop <dig> to ρ¯te <dig>  allows the selection process to be evaluated.

the three following measures were computed for each i =  <dig> ...,  <dig> training sets simulated with each combination of the parameters p, p <dig> and n.

  Δtrte,i=ρtr,i2−∑j=150ρte,ij250=ρtr,i2−ρ¯te,i <dig> 

  Δtrpop,i=ρtr,i2−ρpop,i2ρpop,i <dig> 

  Δtepop,i=1ρpop,i2=ρ¯te,i2−ρpop,i2ρpop,i <dig> 

RESULTS
results are shown through boxplots. we found this way of representation well-suited to show the distribution of the various differences obtained over each set of  <dig> training datasets. each point contributing to the boxplot corresponds to one measure of Δ, one Δ being computed for each of the  <dig> training sets.

number of patients
results are shown in an example with p =  <dig> genes. for the clarity of the figure, only results obtained with p <dig> =  <dig> are shown. they remain the same with p <dig> =  <dig> or p <dig> =  <dig> 

regarding clinical variables, Δtrte varied around zero and overall did not depend on the sample size. regarding genes, the difference decreased with increasing sample size; the difference never reached zero in the multivariate case. the decreasing effect was even stronger with the univariate selection method. this can be explained by the fact that the number of selected genes depended on the test set for the multivariate method whereas it is fixed a priori in the univariate method; in the former, the number of selected genes also varied and we observed that this number increased with the number of patients. the tgd selected genes that were not truly related to survival  contributed to the computation of ρtr <dig> although they were noise. as a consequence Δtrte had higher values for the transcriptomic model in multivariate selection than in univariate selection, even for n =  <dig> patients. these results show that transcriptomic and clinical variables have different behaviors. the predictive power of genes selected on one dataset is overestimated with regard to the predictive power they would have with other datasets. on the contrary, the predictive power of clinical variables is the same with both training and test sets. as a result, the two types of variables cannot be interpreted the same way.

regarding clinical variables, their predictive power was clearly underestimated. this observation was amplified with high values of p <dig> . this can be explained by the bias due to missing covariates when a model is badly specified. this phenomenon is an illustration in the high-dimensional setting of a known bias adjustment phenomenon demonstrated by chastang in  <dig>  <cit>  in the classical context of n >> p. through formulae and simulations studies, the author showed that when explanatory variables are omitted in a non-linear model-multivariate exponential cox and weibull survival models- the effect of non-missing covariates is underestimated. this is typically what happens when selecting genes on the training set and when some relevant genes are not detected by the method. because the tgd method selects nearly the same number of genes whatever p <dig>  the higher p <dig>  the greater the number of genes missed at selection. note that, on the contrary, including non-relevant genes in the model does not bias the estimation of the other covariates.

regarding genes, Δtrpop tended to zero when the number of patients increased; the higher the number of patients, the nearer the adjusted ρtr <dig> was to the expected ρpop <dig>  when the sample size is too small, the highly predictive information assumed to be given by the selected genes is far from true information. we were also interested in differences between adjusted ρ¯te <dig> and ρpop <dig> . regarding the clinical variables, the results were the same as for the previous differences.

regarding genes, by using the ones selected with the training set in the test set, the differences Δtepop were mainly negative when the sample size exceeded  <dig> patients: this means that the selected genes were not able to report the true information contained in the dataset. in other words, the genes selected on the previous dataset had no predictive power on other datasets because they were not relevant.

when selecting the p <dig> genes, these can be either really related to survival  or not . the former are true positives , and the latter false positives . to study the influence of the tp on optimism, we compared the evolution of ρ <dig> due to the tp on the one hand, and to all selected genes on the other hand. this was done with the training and the test sets in the case of multivariate selection of genes. the left panel of figure  <dig> shows that increasing n, ρtr <dig> remained of the same order for all selected genes whereas the ρ <dig> due to the tp increased. in contrast, the right panel of figure  <dig> shows that ρ¯te <dig> for all selected genes or for tp only evolved in the same way. in cases with  <dig> or  <dig> patients, there were no tp; ρtr <dig> was also only due to noise; this cannot be seen when using only one dataset for a study and may lead to incorrect interpretation of noise as information. the high values observed for all the genes for the  <dig> patients are due to the fact that there were too few patients to get good estimations.

total number of genes
the need for a lot of individuals is valid whatever the number of genes. however, for a fixed number of individuals, the total number of features under study also has an impact on optimism. results are shown in an example with p =  <dig> patients. figure  <dig> shows the values of the differences between ρtr <dig> and ρ¯te <dig> in the transcriptomic model. when the total number of genes increased, Δtrte increased too. when there were too few genes of interest, it was difficult for the selection method to find the relevant ones . genes selected on the training set had no predictive power on the test sets, which can be explained as follows. the more genes there are, the higher the optimism: the greater the number of genes under study, the more overestimated is the predictive power of the transcriptomic model. the high value of ρtr <dig> is due to noise and not to real information. the study of Δtepop, indicated that genes selected on the training set are not able to relay the predictive power really contained in the test set when the number of genes truly related to survival is too small relatively to the total number of genes. indeed, differences were negative, and increased with increasing p <dig> 

discussion
genes are not yet validated as predictors of outcome. they are selected on a single dataset and assumed to have the same predictive power with other datasets. but results show that this predictive power is overestimated in the case of genes. this overestimation is even more significant when the number of patients is low and the total number of genes high. this is due to two phenomena which overlap: the gene selection mechanism and the power problem. when there are too few patients the genes that are selected are not the true ones due to a lack of power. this problem is not encountered for clinical variables, for which the selection process is over because they have been already validated. the same problem arises when there are too many genes relatively to the number of genes truly related to survival. this problem is difficult to solve in real life studies and must be kept in mind. indeed, the number of genes of interest is not known in advance.

many papers dealt with the choice of the best method for gene selection. our aim was not to study a specific method but rather to study what happens once genes have been selected. however, some comments may be made as to the tgd. first, from one dataset to another, there is considerable variety in the number and identity of genes in a selected set. nevertheless, the number of true positives, which increases with the number of samples, is more stable. note that the number of selected genes depends on the choice of the parameter τ. with a fixed value of τ and p <dig>  the conclusions would be the same whatever the choice of this parameter: optimism increases with the number of patients and decreases with the number of genes involved in the study. second, one point that appears to us as a drawback of this method is that it gives very low coefficient estimations, even for true positives. in our case, we only used the tgd for selection purposes, and coefficients of selected genes were re-estimated in a new cox model. however, we noticed that some of these genes had very low estimated coefficients on the training sets, even more so with a low number of samples. we may wonder why these genes were selected.

to answer the question of comparative optimism of clinical and transcriptomic variables, we worked on simulated datasets reflecting the real situation encountered by clinicians and statisticians. thus, we simulated only two true clinical variables, but many superfluous genes. our conclusions depend on this choice of the simulation setup. it is not the nature  of the two types of variables that explains the difference in the introduced optimism but their status : few validated variables for clinical variables and many variables under selection for genes, of which noisy variables. it is clear that with  <dig> clinical variables and only  <dig> biologically pre-selected relevant cancer genes, the situation would be reversed.

concerning the simulation process, as the real correlation structure of genes is not well known, we chose not to include it in this work. moreover, clinical variables and gene-expressions were all modeled to be independent. future studies will aim to model dependence structures between the two types of variables and genes.

CONCLUSIONS
by comparing the predictive power and optimism from clinical variables with genes two phenomena have to be taken into account: overestimation for genes due to the selection process and underestimation for clinical variables due to the omission of relevant genes. by including clinical and transcriptomic variables in the same model these results must be kept in mind. the predictive power of the clinical variables must not be neglected. in comparison with genes, their importance is not overestimated, which gives the feeling that they have less influence. in reality, part of their impact is hidden by the optimism encountered for genes.

authors' contributions
ct wrote the computer code for simulations, carried out the analysis, analyzed the results and drafted the manuscript. dm-b contributed to the design of the study, interpretation of the results, and contributed with pr to writing the manuscript. all authors read and approved the final manuscript.

