BACKGROUND
metagenomics is a new field that studies the environmental microorganism populations using culture-independent sequencing technologies. it provides revolutionary and unprecedented view of the identities, dynamics and functions of microbial communities in various environments such as marine  <cit> , human gut  <cit>  and many others  <cit> .

the recent advances in next-generation sequencing technologies  <cit>  such as  <dig>  illumina, solid and heliscope significantly promoted the development of metagenomics by offering low-cost and ultra-high throughput sequencing. huge amounts of available metagenomic sequence data create tremendous challenges in data analysis. some challenges are computational and result from the huge quantity of sequence data. it can easily consume 104~ <dig> cpu hours to query a regular  <dig> sample with  <dig> reads against ncbi's non-redundant  database using blast  <cit> . other challenges are due to the high complexity of metagenomic sequence data:  a sample may contain hundreds or thousands of species at dramatically different abundance levels;  many species are unknown;  next-generation sequencers produce shorter reads with higher error rate compared to sanger sequencers; and  sequence data contain other experimental bias, artifacts and contaminations  <cit> . to address these problems, many methods have been developed such as taxonomy binning  <cit> , use of simulated datasets  <cit> , diversity analysis  <cit> , orf calling  <cit> , rrna prediction  <cit> , sequence clustering  <cit> , assembly  <cit> , statistical comparison  <cit> , fragment recruitment  <cit>  and so on. for example, megan  <cit>  assigns taxonomic groups to query sequences based on blast search against a reference database, usually the ncbi nr. cd-hit has been used in clustering raw reads and orfs to identify non-redundant sequences or gene families  <cit> . mothur  <cit>  is a software package with several functions such as identification of operational taxonomic units . qiime  <cit>  is another useful package for the investigation of microbial diversity using rrnas. software package rammcap  <cit>  provides a very fast sequence clustering and annotation pipeline.

it is very difficult for common researchers to install and maintain so many software tools needed in metagenome annotation. many users simply do not have the required computational resources to run some of the tools. the available online resources that provide metagenomic data analysis are also limited. currently, mg-rast  <cit>  and camera  <cit>  are the major sites where users can submit datasets for analysis. mg-rast only provides a fixed pipeline and the waiting time for its jobs is often very long . camera offers a list of workflows, but many useful tools are still missing from camera's site. in addition, both mg-rast and camera require user registration and login, so it is difficult to access their web servers using scripts.

in order to provide a fast, easy and flexible solution for metagenomic data analysis, we developed webmga, a web server that allows users to submit metagenomic datasets and to run many kinds of analysis, or to perform a user-customized annotation pipeline. webmga is freely available at http://weizhongli-lab.org/metagenomic-analysis to all users without any login requirement.

implementation
webmga consists of a web user interface, web service interface, server scripts, a mysql relational database, an email server, daemon processes, application software packages, wrapping and parsing scripts and a computer cluster . the webmga web front-end is an apache http server, which accepts jobs submitted through web browsers. webmga's web services, which are implemented with mojolicious software, accept client-side scripts following representational state transfer  protocol. job requests are processed by server scripts, which submit jobs to a queue and return a unique job identifier with a web link for each request. if an email address is provided , the user will be notified by email of job status change. all the job-related data such as job identifiers, status, date and time are stored in the mysql database, and managed by server scripts and daemon processes. the daemon processes handle the job queue, submit jobs to computer cluster and check job status. a user can query the status or retrieve the results of a job, using web browser or scripts, by submitting a job identifier. the latest versions of software packages are locally installed on our computer cluster, which runs linux operating system and sun grid engine job queuing system. we implemented scripts to run these applications in parallel and parse the outputs.

RESULTS
computational tools
as outlined in figure  <dig>  webmga includes a wide range of tools for analyzing large and complex metagenomic sequence datasets. webmga is implemented with many tested tools that can process millions of sequences in minutes to hours. the key features of webmga are:  rapid analysis enabled by very fast algorithms and methods,  a large collection of computational tools,  flexibility to run individual tools or configure a pipeline consisting of individual tools, and  compatibility of application and pipelines with both web browsers and client-side scripts.

webmga currently has  <dig> individual tools that cover the following categories:

•quality control has  <dig> tools to filter or trim raw reads and yield high quality reads. the first tool  takes reads in fastq format and yields high quality reads in fasta format. the second tool  takes a fasta file and a quality score file and generates high quality reads in fasta format. the third tool  trims low-quality tails of inputted illumina reads using solexaqa  <cit> .

•sequence clustering has  <dig> tools: cd-hit-est, cd-hit, h-cd-hit  <cit>  and cd-hit- <dig>  <cit> . the first two take dna and protein sequences as input respectively, perform clustering, and output clusters and non-redundant sequences. h-cd-hit is a 2-step clustering analysis for proteins. the program first performs clustering on the input dataset and the representatives of this step are the input of the second clustering round. h-cd-hit produces a hierarchical structure for proteins; it also maximizes the computational efficiency and the quality of clustering. cd-hit- <dig> takes raw  <dig> reads and identifies the artificial duplicates, which are commonly present in  <dig> pyrosequencing reads.

•rrna identification includes blastn-rrna  <cit>  and hmm-rrna  <cit> . blastn-rrna identifies rrna from dna fragments by querying against 5s ribosomal database, european ribosomal rna database and silva database  <cit>  through blast. despite blastn-rrna shows higher specificity than hmm-rrna for 5s rrna prediction, hmm-rrna, an hmm-based method, has much higher speed and overall better sensitivity. for more detailed comparison between these two tools, please refer to reference  <cit> . both programs take dnas in fasta format and output  <dig> files: predicted rrna sequences in fasta format, a 'tab' delimited text file that lists the rrna type and positions, and a fasta file for the original input sequences with the predicted rrnas masked by letter 'n'. the purpose of the masked file is to prevent false orf calling if it is used for orf prediction.

•trna identification uses trna-scan  <cit>  to identify trnas from the inputted dna sequences. similar to rrna tools, it outputs  <dig> files: the predicted trna sequences, a 'tab' delimited text file, and a masked input file.

•orf calling include  <dig> tools for orf prediction from dna sequences: orf-finder  <cit> , metagene  <cit>  and fraggenescan  <cit> . orf-finder calls orfs by translating all six reading frames, where an orf starts at the beginning of a sequence or the first atg after a previous stop codon and ends at the first stop codon or the end of that sequence. orf-finder covers more true orfs and yields more spurious orfs than metagene and fraggenescan. it is more suitable to use when the inputted dna sequences are below  <dig> bp. metagene is the first ab initio orf prediction program that is designed for fragmented sequences. fraggenescan is another ab initio program that can handle frame-shift errors, which are typical for  <dig> reads. all these three tools take nucleotide sequences as input and output orf protein sequences in fasta format.

•function annotation includes  <dig> tools. we implemented scripts to annotate the inputted peptide sequences against pfam and tigrfam families using hmmer <dig>  <cit>  and against ncbi's cog, kog and prk databases using rps-blast. we output the annotation in 'tab' delimited text files, which include the details of hits of each query against each reference family  and also several derived results. for example, for cog annotation, we also give summarized results of number of hits to each cog family and each class . for pfam search, we also provide gene ontology  annotation through the mapping between fpam and go.

•pathway annotation takes peptide sequences in fasta format as input, searches our curated kegg database with blastp, and generates the pathway annotation in 'tab' delimited text files. the reference kegg database was prepared to speed up the blastp search. we clustered the kegg database at 90% sequence identity, and if the sequences in one cluster all belong to the same ko group, only the representative sequence  of this cluster is used in the reference database. otherwise , all sequences in that cluster are used. compared to the original kegg database, searching the curated database recovers > 99% of the hits and is ~ <dig> times faster.

•sequence statistics has  <dig> tools: fna-stat and faa-stat. they take nucleotide  or protein sequences  as input and output the summary information of the inputted file including length distribution, gc content etc .

•filtering human sequence is a filtering tool for identification of human sequences from human microbiome samples. this tool queries the inputted reads against human genome and mrnas using fr-hit  <cit> . if the similarity between a read and a human sequence meets a user-specified cutoff , this read is filtered out. fr-hit can identify similar number of hits as blastn, but it is about  <dig> orders of magnitude times faster than blastn. this tool produces a file of un-filtered reads in fasta format and a text file that lists the filtered reads along with alignment information to human reference sequences.

•taxonomy binning has  <dig> tools: rdp-binning and fr-hit-binning. the first uses the binning tool in ribonsomal database project   <cit>  to bin rrna sequences. the second tool aligns the inputted metagenomic reads to ncbi's refseq database and then assigns the reads to the taxon that is the lowest common ancestor  of the hits. lca was originally introduced in megan  <cit> , where blast is used for alignment. since blast is too slow for large metagenomic datasets, fr-hit is utilized here.

•otu clustering takes rrna tags and clusters them into otus. the software we used here is cd-hit-otu , which is a clustering program we developed that can process millions of rrnas in a few minutes, while some traditional methods such as mothur  <cit>  and esprit  <cit>  need days for millions of sequences. cd-hit-otu is also more accurate than many traditional methods that tend to overestimate the diversity due to sequence errors.

•file conversion is a tool that converts reads from fastq format to fasta format.

individual web servers
each of the  <dig> tools introduced above was implemented as a standalone web server. as illustrated in the screenshot of webmga web server , each tool has its own web page so that users can upload dna or protein sequences for analysis, e.g. to call orfs from raw reads using fraggenescan. different applications generate different type of files including sequence files in fasta or fastq format , tab delimited text files , graphic files , raw output files and so on. due to the great diversity of the output types, particular visualization pages are not available for all tools. the results produced by webmga and documentation are packed into a zip file for a user to download and analyze at client-side.

interactively perform analysis pipelines
most metagenomic data analysis pipelines include many processes using different tools. figure  <dig> gives a simplified pipeline as an example. with webmga, users can run complex pipelines by interactively using the individual web servers. for example, to run the pipeline in figure  <dig>  a user can upload the raw reads to the quality control tool and then input the high-quality reads into "sequence statistics", "rrna prediction" and "clustering" servers and run them in parallel. once the rrna prediction is completed, the user can download the result and use the masked sequences  as input to run trna prediction followed by orf-finder. when orf-finder is finished, function and pathway annotation jobs can be submitted in parallel using the predicted orfs as input.

client-side scripting
one advantage of interactively running a pipeline is that a user can monitor and control the annotation process, for example, by checking the results and choosing suitable programs and parameters in the next step. but this way may be too tedious for routine analyses. webmga offers restful web services for all the tools through which a complex pipeline can be automatically executed using one client-side script. two template perl scripts, client_submit_job.pl and rammcap_client_submit.pl, are available at webmga web site. a user can straightforwardly use the template perl scripts to configure an annotation pipeline and run it locally.

the first template script runs a single tool: it submits dna sequences in a fasta file to cd-hit-est web service and downloads the clustering results. the second script performs a more extensive annotation using rammcap pipeline  <cit> , which is also used by camera project. this script starts with a fasta file of reads and then runs a list of web services such as sequence statistics, clustering, rrna and trna finding, orf calling, and function annotation and finally downloads all the annotation results.

computational time and throughput
three datasets were used to test the performance of tools in webmga. the first one is a metagenomic sample  selected from a core gut microbiome study  <cit> , which contains  <dig>  reads with an average length of  <dig> bps. the second dataset contains  <dig>  orfs with an average length of  <dig> letters predicted from the first dataset using metagene  <cit>  with default parameters. the third dataset, which contains  <dig> 16s rrna samples from study  <cit> , has  <dig>  16s rrna reads spanning the v <dig> variable region .

the wall time and total cpu time for each tool to process the above datasets are listed in table  <dig>  fast tools like sequence statistics, file conversion, quality control, rrna-scan and orf calling use only one cpu core; clustering tools use  <dig> cores in parallel; other relatively time-consuming jobs use up to  <dig> cores. when our cluster has enough free cores for webmga, about 50% and 75% of jobs can complete within  <dig> minutes and  <dig> hour respectively. all jobs need less than  <dig> hours except the slowest pathway annotation against kegg, which needs about  <dig> hours.

a see text for descriptions of the  <dig> datasets tested.

b daily throughput is calculated as the daily cpu time of webmga cluster with  <dig> cores divided by the total cpu time of a job, assuming  <dig> minutes of administrative cpu cost such as job queuing, file coping etc. for each job.

we allocated  <dig> cpu cores from our cluster for webmga server to use exclusively. with this computational capacity, webmga can process hundreds of jobs with most tools per day . for example, the daily throughput for orf-finder is about  <dig>  based on the second dataset. function and pathway annotations are the bottlenecks, but webmga can still process  <dig>  to more than one hundred datasets  in a day. webmga only allows  <dig> kegg job to run with up to  <dig> cores at the same time so that other fast jobs can be completed quickly.

example
to illustrate the application of webmga, we annotated the first test dataset  using the template script rammcap_client_submit.pl. since this dataset was already filtered by the original authors, we skipped the quality control, duplicates clustering and filter-human steps. the annotation summaries are outlined in table  <dig>  the results are comparable to those published in the reference  <cit> . for example, the relative abundance of cog categories annotated in this example shows no visible difference to that in original literature   <cit> .

total bases:  <dig>  total ambiguous bases:  <dig> 
clusters:  <dig>  size of the largest cluster:  <dig> 
clusters in cd-hit format and in 'tab' delimited text file,
archaeal-16s:  <dig>  eukaryotic-18s:  <dig>  bacterial-16s:  <dig> 
total letters:  <dig>  total ambiguous letters:  <dig> 
clusters:  <dig>  size of the largest cluster:  <dig> 
clusters in cd-hit format and in 'tab' delimited text file,
total alignments:  <dig>  total orfs aligned:  <dig> 
total cog families aligned:  <dig>  total cog classes aligned:  <dig> 
total alignments:  <dig>  total orfs aligned:  <dig> 
total pfam families aligned:  <dig> 
total orfs with go annotation:  <dig>  total go terms annotated: 964
total orfs with ec annotation:  <dig>  total ec terms annotated: 319
total alignments:  <dig>  total orfs aligned:  <dig> 
total pfam families aligned:  <dig> 
total orfs with go annotation:  <dig>  total go terms annotated: 252
total orfs with ec annotation:  <dig>  total ec terms annotated: 57
a detailed parameters are explained at webmga website.

comparison to other web servers
in metagenomics, mg-rast and camera are the dominating web servers that provide online data analysis. both resources have been constantly busy and many jobs submitted to them need to wait long time for completion. for example, we also submitted gut sample f3t1le <dig> to both mg-rast and camera for annotation and it took them  <dig> days and  <dig> hours respectively. webmga used  <dig>  hours to annotate the same dataset using rammcap pipeline. webmga adds additional computational resources for the increasing need in metagenomic data analysis.

compared with both mg-rast and camera, the most important advantage of webmga is the flexibility to run user-customized analysis pipelines with client scripts besides web server interface. mg-rast has a fixed annotation pipeline that users cannot modify, which is essential to compare annotations of different samples. however a fixed pipeline is not suitable for all the diverse requirements in metagenomic studies, where researchers need to use different tools and different parameters. camera has many analysis workflows that can process user-uploaded data. but these tools can only be used interactively by users that are logged in.

mg-rast and webmga share many common procedures such as quality control, filtering and clustering, but they also apply different methods or resources for the same type of annotations. here are some examples:  mg-rast treats the reads whose first  <dig> bases are identical as duplicates, but webmga uses cd-hit- <dig> for this purpose. mg-rast's method is faster but may miss the duplicates with sequence errors  within the first  <dig> bases. cd-hit- <dig> is slightly slower, but is more sensitive and can pick the duplicates missed by mg-rast.  for host associated samples, mg-rast uses bowtie  <cit>  to identify near identical matches to host reference sequences and removes these reads as host contaminations. webmga uses a slower but more sensitive method, fr-hit, for human-contamination removal.  for orf calling, mg-rast uses fraggenescan; while webmga allows users to choose from orf-finder, metagene and fraggenescan.

camera and webmga also have many common methods, mostly because camera also adopted the rammcap pipeline we developed. but webmga has many unique tools such as filter-human, rdp-binning, fr-hit-binning and cd-hit-otu that camera doesn't have.

CONCLUSIONS
in order to assist researchers in the metagenomics field to deal with data analysis challenges, we implemented webmga with very fast algorithms and effective methods. with webmga, users can use many individual tools and assemble the tools into a pipeline for more complicated analysis through web browsers or client-side scripts. we are in the process of developing new tools and validating more public tools so that, in the future, more rapid tools and pipelines will be added to webmga server.

availability and requirements
•project name: webmga

•project home page: http://weizhongli-lab.org/metagenomic-analysis

•operating system: platform independent

•programming language: perl 

•other requirements: browsers

•license: no license needed

•any restrictions to use by non-academics: no restriction

authors' contributions
sw, zz and wl contributed to system concept. sw and zz implemented the system and performed major programming work. lm and bl contributed to the development of cd-hit software and fr-hit software, respectively. sw, zz and wl coordinated this work, contributed the data analysis and wrote the manuscript. all authors read and approved the final manuscript.

