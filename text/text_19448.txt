BACKGROUND
the use of real-time pcr in functional genomics has increased dramatically during the past decade. with this method, the detection of template accumulation in the pcr reaction is based on a fluorescent probe, or a fluorescent dye. the advantages compared to former pcr approaches are many: a: a closed compartment method decreases risk of contamination, as no post-pcr handling is necessary. b: the data used for calculation of quantity are collected as the pcr reaction runs, reducing the time span from pre-pcr procedures to final results are available. c: compared to endpoint analyses of pcr reactions, real-time pcr is unmatched in precision – and d: an extreme dynamic range of 7– <dig> log <dig>  <cit> .

in the software currently available, analysis of real-time data is generally based on the "cycle-threshold"  method. some packages offer curve-smoothing and normalisation, but the basic ct algorithm remains unchanged. threshold fluorescence is calculated from the initial cycles, and in each reaction the ct value is defined by the fractional cycle at which the fluorescence intensity equals the threshold fluorescence. a standard curve can be used for absolute quantitation, or the comparative ct method can be used for relative quantitation  <cit> .

the ct method is quite stable and straightforward, so why try to complicate things? the answer is that the precision of estimates is impaired if efficiency is not equal in all reactions. uniform reaction efficiency is the most important assumption of the ct method. the simplest estimate of individual sample efficiency is calculated from the slope of the first part of the log-linear phase  <cit> , and can be used for identification of outliers or correction of values from individual samples. the sigmoid curve fit or non-linear regression   <cit> , on the other hand, assumes a dynamic change in efficiency and closely resembles the observed course of fluorescence accumulation during the whole reaction. a further advantage of regression analysis is the possibility to generate estimates of initial copy number directly from the regression estimates, eliminating the need for a standard curve  <cit> . in small study series, the standard curve may be the best choice – but in a high-throughput production lab, elimination of the standard curve could liberate time and resources.

the first obstacle to the use of nlr is that the algorithm needs to be automated. the second and more important obstacle is that proper evaluation is missing both of the comparison of nlr with the ct method, and of the performance of nlr with probe-based chemistry. we therefore decided to develop and evaluate an automated regression model, to test if nlr is a real alternative to the traditional ct method.

RESULTS
altman-bland plots were made of the numerical differences between duplicates  vs. duplicate means for each dataset and each regression model and the ct method. these plots showed an increase of error with increasing mean . however, a log <dig> transformation of all final estimated values could resolve this pattern, and the error plots showed independence . the intra-assay variation could then be characterised by a  <dig> percentile of the observed errors. the inverse log <dig> of this percentile can be interpreted as factor variation and recalculated to a percentage, as presented in table  <dig> 

the mean copy number of duplicates was then analysed in plots of differences between nlr- and ct-derived values  vs. means . again, independence could be observed after log <dig> transformation of the copy number values, but not in the raw data. in each experiment there was a relative bias, but when comparing the different experiments the bias was clearly not systematic. in figure  <dig>  the bias of model  <dig> is shown in an altman bland plot containing data from all three experiments. the distribution of the data clouds indicates that each conversion factor varies between experiments in a random manner.

the calculated conversion factors ranged from  <dig> e+ <dig> to  <dig> e+ <dig> copies/fluorescence unit. table  <dig> offers an overview of all models tested and key figures of their performance. the error percentiles stated are calculated on pooled data from all  <dig> assays, and the bias values are means of pooled numerical bias. as can be seen in figure  <dig>  a simple average of pooled values would yield an erroneously low estimate of the bias, so the overall bias of each regression model has been calculated as an average of numerical bias values. for evaluation of the modifications applied, table  <dig> offers an overview of resulting r <dig> mean, error, and bias changes.

discussion
ct method
in the ct method equal efficiency in all reactions is assumed, and the impact of this assumption on final estimates has been underlined previously  <cit> . tichopad  <cit>  presented a standardised, automatable algorithm for estimation of sample specific efficiency, and a similar approach was published by ramakers et al  <cit> . these models calculate efficiency at the early log-linear phase, and assume homogenous efficiency before that. however, calculation of sample specific efficiency was also evaluated by peirson et al  <cit> , who concluded that this approach was good for detection of outliers, but individual efficiency correction did not improve the precision of absolute quantitation.

the ct method has also been combined with curve-smoothing to obtain background correction and data smoothing . the latter approach may produce nice curves, but especially amplitude normalisation will change the slope of the log-linear phase and thereby mask differences in reaction efficiency.

nlr
theoretically, a calculation of template accumulation that mimics the dynamic change in pcr efficiency, and includes a larger array of the collected fluorescence data, could be more precise than the ct method. alternatives to ct-based calculation have been suggested previously  <cit> . one model that assumes a dynamic change in efficiency is the sigmoidal curve fit  <cit> , though limitations apply  <cit> . especially the late plateau phase of the reaction is difficult to fit in this mathematical model. rutledge suggested removal of observations from the late plateau phase to increase goodness-of-fit to the remaining data. principal objections aside, the latter approach is less well suited for automation. to solve this problem we tested weighted analysis, which performed well in automation but unfortunately did not improve the precision of estimates.

automation
algorithms for this type of analysis should be independent on user input apart from the raw data, to eliminate user-dependent bias. in general, "mass production" techniques should be used with caution in complicated regression models, as small errors may impair the precision of the final estimates  <cit> . of the models initially investigated in this study, three produced one or more bad fits when automated – which illustrates a potential disadvantage of nlr when compared to ct analysis. the remaining eight models seemed robust, and could be evaluated more thoroughly.

model evaluation
the r <dig> value can be interpreted as "the amount of observed variation explained by the regression model". the mean r <dig> values in table  <dig> show that all models generated values above  <dig> . obviously, differences in the 3rd decimal place of r <dig> are not a good measure of model performance, so the altman-bland method is more informative.

in the present study, the gold standard ct method has an intra-assay variation  of 24%, which is close to previously reported values  <cit> . this error is a sum of the inaccuracies in fluorescence measurement, thermocycling, pre-pcr procedures, and the ct fractional cycle estimate. most of these inaccuracies are common to both calculation methods. in nlr,  <dig> or  <dig> variables are estimated in each analysis , and each of these estimates contain intrinsic error. thus, the resulting intra-assay variation is a combination of inaccuracies in the pre-pcr procedures, equipment errors, and errors in the variable estimates. thus, in effect at least 35% of the total 59% error in model  <dig> is generated by the mathematical model itself.

of the four different modifications to the original model tested, changes in r <dig> were minute – but in terms of error all modifications tested had a negative impact on the model, probably due to the increased number of variables estimated. model  <dig>  produced marginally lower bias and marginally higher error, and this is the only modification that was not directly harmful to model performance.

the high-performance assays used in this study are an optimal setting for ct analysis, and our evaluation may therefore be quite conservative in terms of demonstrating the advantages of nlr. in assays with varying pcr efficiency, the nlr method may yet prove to be more precise than the ct method. this, however, awaits systematic evaluation.

conversion factor
the use of an absolute conversion factor, or optical calibration, has been evaluated previously in different analysis models  <cit> . the three data clouds in figure  <dig> were generated with separate conversion factors, and their distribution shows a pattern of random variation, underlining that our conversion factor assessment was inaccurate. however, the conversion factor only affects the absolute sample value and not the intra-assay variation, nor the rank position of a sample in the data set.

probe-based chemistry theoretically offers a stoichiometric calibration, as each probe has one reporter and one quencher molecule. in effect, the conversion factor should be universal and independent on the template measured. the conversion factors calculated in this series ranged ×  <dig>  from lowest to highest, and this also indicates that the precision of our conversion factors was less than optimal. stoichiometric calibration was investigated in detail by swillens et al  <cit> , who lowered probe concentration to define probe as the limiting factor of fluorescence accumulation. this approach assumes a precise probe concentration and a 100% conjugation and purity. as the problem of signal to noise ratio is inherent in all probe-based assays, a reduction of probe concentration lowers the detection window even further and may impair precision.

alternative mathematical models
for curve smoothing combined with the ct method, the sigmoidal curve fit may not be optimal – as the gompertz function  <cit>  shows a better fit both with the steep increase phase and the late plateau phase. the gompertz algorithm is not suitable for estimation of initial fluorescence, though .

to calculate the initial copy number accurately, the efficiency of each cycle must be estimated:

n0=ncte0×e1×…×ect−1
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgobgtdawgaawcbagaegimaadabeaakiabg2da9maalaaabagaemota40aasbaasqaaiabdoeadjabdsfaubqabaaakeaacqwgfbqrdawgaawcbagaegimaadabeaakiabgena0kabdweafnaabaaaleaacqaixaqmaeqaaogaey41aqraesojgskaey41aqraemyrau0aasbaasqaaiabdoeadjabdsfaujabgkhitiabigdaxaqabaaaaaaa@44d9@

in theory each of these efficiencies could be measured directly on the fluorescence curve. in practice, however, only a few points on the pcr curve yield workable efficiency estimates because the early plateau phase is dominated by background noise. rutledge recently proposed an alternative model for estimation of maximal efficiency based on the sigmoidal model  <cit> . as the efficiency is directly calculable in the log-linear phase  <cit> , the important extremes of efficiency  can be assessed. further work will show if this model is workable, or if it will fall short on the problem of multiple estimates.

CONCLUSIONS
nlr is automatable and may be a powerful tool for analysis of fluorescence data from real-time pcr experiments. the unfavourable signal to noise ratio of the probe-based assays did not impair nlr analysis. the versatility of nlr depends on the precision needed – but if adaptable, this analysis method may save both time and resources in the laboratory. further work is needed as to improve precision of the fluorescence-copy number conversion factor in order to reduce the bias observed in this study.

it is indeed possible to obtain absolute quantitation from real-time data without a standard curve. in an optimised assay, however, the ct method remains the gold standard due to the inherent errors of the multiple estimates used in nlr.

