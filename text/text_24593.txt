BACKGROUND
the data generated by various scientific and non-scientific fields is growing exponentially with modern technologies. this paper focuses on the exponential growth of data in the life sciences, which has surpassed the rate at which both processing power and storage technologies are growing  <cit> . in this work we focus on bioinformatics, a key area of information processing that has a direct impact on the quality of human life. since the data in bioinformatics is growing beyond the scope of a single computing architecture, we are working on developing efficient, optimized, and highly scalable parallel applications on the latest and next-generation supercomputing architectures to meet the rising demand. the previously mentioned challenges relative to data growth and large-scale knowledge discovery could be addressed in three phases: the first phase is the development of parallel applications to analyze data at a rapid rate; the second is the creation of methods for easy access to these parallel applications; and the third is the development of databases to host analyzed data for quicker retrieval.

development of parallel applications
the software parallelizations that can be explored to address these gigantic problems are data parallelism, functional parallelism, and a combination of both data and functional parallelism. this paper discusses software wrappers used for data parallelization to process large-scale data in less time. these parallel wrappers aid us in parallelizing the most widely used applications in bioinformatics. the development of parallel applications is not the only aspect of the solution required for this data growth problem. after these applications have been developed, creating interfaces that will allow researchers with limited computing expertise to access these tools to solve their data analysis problems will also be essential. for this reason, we are constructing a science gateway with a web interface to expose these parallel applications for easy access. finally, we are also developing a suite of tools to parse the outputs from all of the parallel applications and store the data in databases for faster retrieval and knowledge discovery since few of these tools generate more output data than input data by an order of magnitude or more.

many parallel bioinformatics tools designed for large-scale data analysis exists. these implementations range from clusters to supercomputers and from grids to cloud computers. wide ranges of workflows, gateways, and packages for various bioinformatics tools are also available, but implementations of highly scalable parallel bioinformatics applications with science gateways for large-scale data analysis are few to non-existent.

to build workflows needed by bioinformatics labs in the states of tennessee and south carolina as a part of an nsf proposal, we have identified the most widely used bioinformatics tools for sequence similarity searches, multiple sequence alignments, protein domain analysis, and phylogenetic tree construction. these tools include ncbi blast: basic local alignment search tool  <cit> ; hmmer  <cit>  by hmmi janelia farm; muscle: multiple sequence comparison by log-exception  <cit> ; clustalw  <cit> ; and mrbayes  <cit> . many parallel implementations of these tools focused for clusters exist, such as mpiblast  <cit> , scalablast  <cit> , mpihmmer  <cit> , clustalw-mpi  <cit> , and parallel mrbayes  <cit> . these tools achieve scalability by selecting techniques such as database fragmentation, parallel input and output , load balancing, and query prefetching, but only a few highly scalable implementations capable of using tens of thousands of cores on supercomputers are available, such as blast on blue gene/l implementation  <cit> , pioblast  <cit> , hsp-hmmer  <cit> , and hspp-blast  <cit> . by creating these highly scalable parallel tools and making them available to researchers through a science gateway, we are making a significant contribution to solving the pressing analysis needs in these areas.

science gateways
many research groups have active science gateway portals as part of the state-of-the-art extreme science and engineering discovery environment  nsf program. these gateways focus on fields such as biochemistry  <cit> , biomedical computation  <cit> , protein structure and interaction  <cit> , and systemic and population biology  <cit> . these portals offer multiple tools for their respective areas and run a subset of those tools on machines that are best suited for hpc computational resources. for example, of the dozens of phylogenetic tools available on cipres, versions of mrbayes, raxml, garli, and beast use xsede resources  <cit> , and robetta is designed to run on computer clusters distributed as mirrors  <cit> .

many portals have workflow capabilities for bioinformatics tools, such as galaxy  <cit> . some xsede informatics tools have available workflows, like chembiogrid  <cit> , which has web-based computational workflows built on the taverna  <cit>  workflow tool. currently, no portals exist that combine workflow between highly scalable bioinformatics tools on hpc resources with gateway access, along with tools for output data analysis. our science gateway, the portal for petascale lifescience applications & research , will provide easy access to this unique combination of powerful, highly scalable parallel bioinformatics applications, output analysis tools, and knowledge-discovery resources. our efforts to provide a new solution to this important problem are described in the section on development of parallel applications.

methods
in this section, we focus on describing the process to take a desktop bioinformatics application, scale it to tens of thousands of cores on supercomputers, and expose it to a science gateway for easy access. we look at the complexity of the code and various profiling options. we also identify the computationally intensive and data intensive sections of the code, and examine parallelizing the code with no change in the functionality of the application as described in following sections.

profiling for parallelization
the unique design of petascale supercomputers, with performance exceeding one petaflop, tens of thousands of computing cores, low per-core systems memory, and a reliance on networked distributed filesystems makes the architecture different from clusters and desktop machines. efficient use of these machines requires special operating systems and carefully designed and tuned applications. one possible way to improve the performance of tools that run on petascale computers is by changing the actual algorithms to better suit the characteristics of supercomputers; this approach involves the tedious process of recoding already complex and specialized tools. the alternative approach that we have taken is to profile the code so as to identify the computationally intensive functions and i/o intensive functions.

we use craypat  <cit> , papi  <cit> , gprof  <cit> , and our own timing code to analyze the runtime of the tools and eventually our own optimizations as well. the craypat profiler was useful in determining possible improvements to the data output scheme and improvements to the overall mpi communication. a second profiler, gprof, was used to analyze the runtime of the tools themselves. a great deal of source code is involved with each application; for example, ncbi's blast code base is approximately  <dig>  million lines, hmmer at  <dig> , and muscle with nearly  <dig>  lines. because of the extensive code base of these tools, as few changes as possible should be made to the code so as to avoid the need to maintain millions of lines of forked software.

after profiling and examining thousands of lines of code, we decided not to alter the functional part of any of these tools so we could avoid the time investment necessary for understanding and parallelizing the tool for advanced architectures. instead, we created a software wrapper that requires only a very small number of changes to each tool's original source code. the architecture of the wrapper solution is shown in figure  <dig>  the wrapper is an executable that runs outside of blast, hmmer, and muscle, but serves to handle the i/o of these tools in an efficient manner, leaving the core of the application as an untouched "black box". the main difficulty in scaling these tools is management of i/o access patterns. the wrapper uses shared memory to redirect the tool's i/o to the wrapper process, so that it may handle the i/o more efficiently. we chose shared memory segments as the means of inter-process communication due to its low overhead, the ease with which pseudo file i/o can be implemented, and the optimization opportunities it provides as described later. many supercomputers use a distributed filesystem, such as the lustre distributed filesystem  <cit> , for i/o data storage. each instance of the tool needs to load a copy of the database, but the file system does not perform well if thousands of processes are trying to read the same files from it simultaneously. a better design is to load all of the data needed with one reader and then broadcast the data to all of the worker nodes. this ensures that the data is only read from disk once. in our solution, we load data from the file system to the master node, and use mpi's mpi_bcast function to broadcast data from the master node to all worker nodes that require the input databases and configuration files. this broadcast is more efficient, scaling logarithmically in the number of nodes. additionally, the use of shared memory allows for a single image of the database to be shared by multiple processing cores. the worker nodes prefetch query sequences from the master node in order to hide input latency while achieving dynamic load balancing, which is essential due to the variance in query runtimes  <cit> .

optimization for improved data access
another common issue related to file access patterns is the reliance on sequential loading of files. in this model, a file is memory-mapped into the process's virtual address space and then read sequentially. a page fault occurs each time the program attempts to read data that has not yet been loaded from the filesystem. then this next page of data is loaded into memory. this results in many small requests for data from the networked file system, which is slower than loading larger blocks of data. our solution to this problem is to preload the database into memory with a single system call. the database typically consumes the most memory during a run. the large size of the database could be larger than the area addressable by the system's translation look-aside buffer  when standard-size pages are used. therefore, we increase the page-size of this preloaded region so that it requires fewer entries in the system's tlb, which decreases the number of tlb cache misses. we found that the use of  <dig> mb page sizes instead of the standard  <dig> kb can reduce the processing time of the blast tool by 15% to 60% depending on the parameters used  <cit> .

the performance of output operations was also improved in several ways. originally, data would be written to disk immediately when available, resulting in many individual writes to the file system and plaguing performance. our improvement consists of a two-stage buffering technique. in this scheme, the tools write output to an in-memory buffer instead of directly to disk. the data is flushed from in-memory buffers to disk by a background process when the buffers are nearly full, rather than on demand. this increases the output bandwidth and also results in more uniform output time. writing to disk in the background also reduces blocking in the tool itself. additionally, we distribute the output files across multiple directories to optimize the common scenario of architectures with a distributed file system. this optimization works because it increases the likelihood that the file system will locate the data on multiple hardware devices, resulting in higher bandwidth for parallel i/o. in addition to writing data in blocks, we provide the option of compressing the actual output blocks. allowing output to be transferred and stored in compressed form helps to increase the throughput of output records. compression occurs in the background so that no additional latency is introduced when the tool writes data. these output enhancements have shown an increase in output bandwidth of 2809%  <cit> . all the above optimizations are performed by a single wrapper  <cit> , shown in figure  <dig> 

output analysis tools
we have also developed large-scale output data analysis tools, because the data generated by these massive runs of query sequences could vary from gigabytes to terabytes and sometimes accessing and analyzing such large datasets is prohibitive. we have generated parsers to parse the xml  <cit>  and other output formats into tab-delimited and user friendly outputs, along with generating sql databases for easy retrieval of results. the user will have access to his or her data either in raw data formats that the tools generated or the parsed data formats if desired. all the results will then be made accessible through science gateways. figure  <dig> illustrates this approach, where the user interfaces with the science gateway portal and initiates a job that is run through parallel modules on an hpc resource. based on the user's needs, either the raw results are delivered back to the user, or the output is parsed and prepared for easier use before it is made accessible through the gateway.

science gateway: challenges and solutions
the usefulness of science gateways, which provide the ability to submit jobs to hpc resources remotely via a defined service, is well established  <cit> . by allowing scientists, researchers, and students to use hpc resources in this way, we can provide a user interface that is highly customized to the user's purpose. this saves time and effort on the researcher's part, and by regulating and standardizing the input interface, we use computing resources more efficiently through the streamlining of complex workflows and the reduction of obstacles caused by deficiencies of expertise and experience with computational resources.

in the process of creating the poplar science gateway, we were faced with several design decisions that directly affected the performance and functionality of our end product. to provide a single experience accessible to the broadest audience, we have implemented poplar as a web-based portal, rather than an alternative solution such as a thick client or desktop application. this also allows us to better centralize management and data flow, and to enforce requirements and best practices such as those for xsede resources.

the initial challenge when establishing a portal is the selection of an architecture that can fulfill the requirements of a science gateway and the needs of the project and its users. the different components have mostly out-of-the-box solutions. while many possible options exist, we identified three that most closely met our needs: the hubzero  <cit> , galaxy  <cit> , and cipres  <cit>  platforms. hubzero and galaxy were the first two platforms we examined. they were both attractive solutions for a front-end, because they are feature rich, with built-in capabilities that fulfill many of the xsede gateway best practices , and have powerful administrative interfaces. hubzero is a general science collaboration environment that is very focused on allowing users to contribute tools, whereas galaxy is more focused on biological sciences and has strong workflow construction functionality. both have some version of submitting jobs to remote computational resources. from our perspective, the strength of hubzero and galaxy was also their weakness in that extra development time would be necessary to make them usable for our purposes; and generalizability could detract from our more focused research. the time investment was our primary concern for both relative to adapting and supporting the packages.

the third solution we examined and chose is an adaptation of the cipres science gateway  platform  <cit> . the csg platform combines powerful, built-in capabilities and a focus on computational biology applications in such a way that it meets most of our feature requirements without over-scoping and requiring extensive customization. some of the most attractive aspects of csg included  a focus on hpc applications, specifically adapted to both xsede and academic resources;  a scalable architecture designed for fully exposing multiple applications  to users via an easy-to-use graphical interface; and  total customizability and parameterization of these tools.

csg uses the workbench framework, which is a "generic distributable platform to access and search remote databases and deploy jobs on remote computational resources"  <cit> . the workbench framework implementation provides a scalable mechanism of xml descriptions that map to graphical user interfaces ; and furnishes a schema for constructing command line statements with the user input entered into those guis  <cit> . this approach offers scalability via ease of development, a robust mechanism for specifying, capturing, and error-checking user parameters, and abstracting presentation from content to allow for separate manipulation and development of both.

for those reasons and because csg so closely matches our application needs, we selected it and thus were able to reduce initial implementation time and focus on customizing and extending the framework.

one of the biggest challenges we aim to address is the transfer of data. our system allows for multiple runs of jobs--in parallel--on very large bioinformatics data sets. when using these tools locally on an hpc resource, the resulting output is stored to the file system. the design of this project requires the delivery of that output to the end user who has submitted the job via a web interface that is not physically co-located with the computation machine's file system. therefore, scaling presents major challenges relative to handling large transfers and meeting local storage requirements. the synchronization of other job information with the user, such as job status, completion, errors, and so forth, also presents an issue. we have implemented a system and run jobs of moderate size via poplar, and are continuing our efforts to improve the scalability of the system.

science gateway: architecture overview
poplar, our science gateway, uses the csg framework  <cit>  and incorporates a java struts <dig>  <cit>  based web portal running on linux and apache tomcat with a mysql database and employs python job management scripts on remote computational resources. our implementation supports only registered users and restricts job submission to only verified users with activated accounts. the web interface allows users to upload data sets; and create, configure, and submit jobs using specific tools, with their uploaded input data and tunable per-tool parameter settings. after job submission, the system populates the results into the portal and notifies the user of job completion. screenshots showing the login interface, an example user-configurable parameter setup for the hsp-blast tool, and output after a successful job, are shown in figures  <dig>   <dig> and  <dig> 

the system uses a community account for job submission and allows for individual user registration, authorization and authentication, detailed logging of portal and computation usage, submission of user attributes to compute resources, the ability to restrict or deny access to individual users, as well as system logging and other features. our adaptations after implementation include restriction of access to registered users, the addition of a verification process for account activation, the enforcement of country of citizenship restrictions on access to computational resources, the adaptation of remote resource job maintenance scripts to the national institute for computational sciences supercomputer, and changes in branding. we have incorporated several of our highly parallel scalable tools into the toolkit. we are working to extend the framework by incorporating integrated workflows across multiple tools, adding parsing and analysis tools as discussed above, and scaling data handling to hundreds of gigabytes for i/o.

RESULTS
scaling of parallel applications
kraken supercomputer, an nsf-funded cray xt <dig> machine consisting of  <dig>  nodes with  <dig>  amd opteron compute cores operating at  <dig>  ghz with  <dig> tb of memory, was used for testing our hsp tools. figure  <dig> shows the weak scaling results of running our hsp versions of blast  as compared to the unwrapped ncbi blast. the input database for blastp runs was ncbi's non-redundant  protein database of march  <dig>  this database contains  <dig> , <dig> protein sequences, and the total size of the formatted database is  <dig> gb. the query sequences are all  <dig> amino acids in length, and the number of sequences chosen is equal to  <dig> times the number of cores. the xml output format was used, and all other parameters were set to the default values established by the blastall tool. we tested the unwrapped blast by dividing the set of input sequences into a unique file for each compute core. output is written to an individual file per core, arranged in a directory. figure  <dig> shows the execution time  of hsp-blast compared to ncbi blast as the workload is increased in proportion to the number of compute cores. hsp-blast achieves near linear scaling performance while the ncbi version experiences scalability issues near  <dig> cores. the original ncbi implementation does outperform hsp-blast at low core counts due to the lower startup cost. we also ran scaling studies of our wrapped position specific iterative -blast  <cit> , whose performance was near linear. this is a significant accomplishment, as no parallel psi-blast is currently available that scales to a large number of cores. we have run millions of blast searches in hours using ~ <dig>  cores on the kraken supercomputer that would take weeks on a cluster or in a cloud environment.

the performance of the wrapped version of the muscle multiple sequence alignment tool was also evaluated on kraken. we chose  <dig> input data sets for each core used in the experiment. each data set consisted of a number of sequences returned previously by a blast alignment search. figure  <dig> shows the scaling results up to  <dig>  cores or  <dig> nodes on the kraken supercomputer. we foresee only few users going beyond the core count. we scaled ncbi blast and hmmer to full machine runs on the kraken supercomputer, and we scaled muscle to  <dig>  cores. these tools are accessible through our poplar gateway.

the poplar gateway
we have generated modules for our wrapped tools on the kraken supercomputer along with documentation for submitting jobs through command line. as discussed in the implementation section, we have also developed our science gateway called poplar. we have incorporated our parallel tools into the poplar gateway and plan to add other bioinformatics tools during the development process. we present these tools via a science gateway so that researchers can use web portals to access supercomputers and further develop workflows of parallel tools that could analyze very large-scale life sciences data, without the need to learn command-line scripting. for example, we are developing a systems biology workflow for a bioinformatics lab at the university of tennessee, knoxville , as shown in figure  <dig>  this workflow results from combining hsp bioinformatics tools currently available on poplar to generate novel protein domain models at a massive scale. this workflow illustrates a real-world application that is commonly used by biologists at ut-knoxville, as well as elsewhere, but must be run in a disjointed fashion, one tool at a time, which is difficult and demanding to perform on large data sets. examples of currently implemented tools include blast, hmmer, and muscle.

in this scenario, the researcher has a newly generated set of genomes or sequences to be annotated for domain models that determine the function, structure, and evolution of the proteins. first, the hmmer tool hmmscan is used to identify domain models in those sequences and those without domain matches. ones with domain matches are identified and documented. to find sequences similar to those without domain matches, the next step is to use the ncbi psi-blast tool. if similar sequences are found using psi-blast to contain known domains, the models for those domain matches can be updated; thus, the next time hmmer should be able to identify the proper domain matches. if psi-blast does not find sequences with domain matches, the next step is to build novel domain models. before building a model, the matching domains are aligned with using tool like muscle, which generates a multiple sequence alignment . the msa is then used to build a new domain model with hmmbuild, part of the hmmer package. that domain model is checked against the known nr database using the hmmer tool hmmsearch. if hmmsearch generates domain matches, the existing models can be improved. if not, the researcher has confirmed that the model created is new and can now be added to domain databases around the world. by making a system where the i/o of each tool are reconfigurable, all the bioinformatics tools, result-parsing tools, and data-conversion tools of the workflow could be used in various combinations to solve different problems in biology.

the data transfer in our current system takes place between the end user and the gateway and between the gateway and the compute resource. a typical sequence begins with the user uploading an input set to the gateway. we note here that once uploaded, an input set is saved indefinitely for reuse, so the sequence may also begin with using a previously uploaded data set. then, after the user selects a tool and configures its parameters, the gateway copies the input data to the compute resource using globus tools. after the job completes, the gateway copies the results back, and the end user can view or download the data.

to examine the scalability of data transfer, we conducted a series of tests to determine the current limits of our system. we tested a variety of i/o file sizes, from 100mb to 5gb. our results showed that for input, we are currently limited by our data upload capability; the maximum input size accommodated by our system is approximately 500mb. for output, we tested output sizes incrementally up to 5gb, without finding a size limitation. in the future we plan to address the upload limitation through software and hardware changes. we are also investigating methods to improve data transfer speeds and to eliminate the need to copy output back to the science gateway for user access. finally, we are also adapting the gateway to allow users to easily apply output data as input into another tool.

the poplar gateway provides an easy-to-use graphical interface that is more efficient than using the command line, for example, in several ways. it is worth noting that because the tools being run via poplar and via the command line are identical, there is absolutely no difference from a processing perspective in the efficiency between methods of the tools themselves. rather, the benefits poplar provides are through the ease-of-use of the graphical interface as well as several functional advantages, including:

• the gui replaces the command line interface, which frees the user from needing experience with the command line.

• likewise, knowledge of each specific tool's syntax is not necessary, as each tool's parameters are broken out and presented via a web form with help text.

• the portal presents the user with one location for data , easy organization of data into folders, and a history of all jobs submitted.

• users can submit and move on to other tasks, as notification of a job's completion is sent to the user via email.

• different tools can be configured to run transparently on different computation resources, all made available to the user via the same interface.

• the computation-resource-agnostic interface does not require the user to know anything about the specifics of different systems ; the same tool running on different resources can be configured to present the same interface.

• jobs submitted via the portal do not require the user to have an account on and log in to each computation resource --only a browser is required to access the portal from anywhere.

by addressing the challenges described above and developing a web-portal science gateway for highly parallelized tools for large-scale data analysis, we look forward to having a substantial positive effect on these fields. our approach allows for easy, user-friendly access to supercomputing resources. with that access, users can more easily submit large-scale jobs. by facilitating rapid large-scale analysis, we are able to help fulfill the significant demand created by the growing volume of data analysis needs in bioinformatics.

the system we have implemented is easily extended to incorporate similar highly scalable parallel tools for other domain sciences. by design, tools located on any computational resource can be made available easily and seamlessly through the science gateway. this combination makes our tools-plus-gateway approach powerfully scalable across both computational resources and science application domains.

CONCLUSIONS
this paper provides a model for the development of highly scalable parallel bioinformatics applications on hpc architectures along with an increase in availability and usability through science gateways. the model directly enhances large-scale data analysis and knowledge discovery capabilities. this work revealed the following points:  significant time and skills are required to change the entire code of an application to a specific architecture, and avoiding such an endeavor is the best practice;  a superior approach is to wrap the code of the bioinformatics application without changing functionality, identify the intensive i/o part of the code, and optimize communications to scale the code well, thus generating accurate results;  even though parallel tools are available at supercomputing facilities, many researchers are reluctant to use them due to a lack of expertise operating on the command line;  for these reasons mentioned here, developing science gateways with easy access to the parallel tools is a better way to facilitate and encourage the use of supercomputing resources by biologists. this research will have a direct impact on life sciences data and the rate of knowledge discovery. still, many issues exist--such as job scheduling, load balancing, and fault tolerance--that need to be addressed with science gateways. we want our users to have total control over the type and size of data that can be analyzed, and we are addressing those issues in the life sciences gateway, as well as developing automated workflows for large-scale data analysis.

availability and requirements
project name: poplar

project home page: http://poplar.nics.tennessee.edu/

operating system: web based / platform independent

programming language: java

other requirements: no

license: contact author

any restrictions to use by non-academics: none

list of abbreviations used
csg: cipres science gateway

gui: graphical user interface

hpc: high-performance computing

hsp: highly scalable parallel 

i/o: input and output

mpi: message passing interface

msa: multiple sequence alignment

ncbi: national center for biotechnology information

nr: non-redundant

nsf: national science foundation

poplar: portal for petascale lifescience applications and research

psi: position specific iterative

sql: structured query language

tlb: translation look-aside buffer

ut-knoxville: university of tennessee, knoxville

xml: extensible markup language

xsede: extreme science and engineering discovery environment

competing interests
the authors declare that they have no competing interests.

authors' contributions
br conceived and served as principal investigator for the project, created the layout of the project, analysis tools, gateways and authored sections of the manuscript. pg developed the highly scalable parallel tools and authored sections of the manuscript. cr implemented the poplar science gateway, developed workflow tools, authored sections of the manuscript. all authors read and approved the final manuscript.

authors' information
br is the research scientist at the joint institute for computational sciences  who is developing parallel bioinformatics applications on hpc machines and next-generation architectures, along with collaborating with researchers from various universities on large-scale data analysis in life sciences. pg and cr are graduate research assistants from the electrical engineering and computer science department at ut-knoxville who are participating in br's projects, and both bring expertise from computer science areas to this research.

