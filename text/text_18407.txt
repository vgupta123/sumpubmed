BACKGROUND
from at least as far back as  <dig>  when gregor mendel published his findings on the laws of inheritance based on experiments with pea plants, statistics has been used as an important tool for genetic studies of inherited traits. with advances in technology and the development of increasingly complex models for analysis, the use of computer science has become integral for all such statistical genetic studies.

one research area of interest for geneticists is locating on the genome the gene responsible for inherited traits. this localization is facilitated by linkage analysis, a statistical genetic method. the method relies on violations of mendel's second law of inheritance, which states that genes assort independently, to estimate the proximity of trait genes to known genomic locations . the model-based linkage statistic is the lod score, defined as the log <dig> ratio of the likelihood of the observed data, divided by the likelihood assuming "no linkage." this statistic utilizes genetic map, pedigree, and trait model information  to obtain an estimate of the recombination fraction θ, the traditional measure of genetic distance between two genomic locations.

a typical linkage analysis therefore involves computing likelihoods in several parameters in order to summarize the information regarding θ. however, complicating factors exist. while sex-averaged genetic maps are normally used in analyses, the true male-female maps are of different lengths. this use of sex-averaged maps results in map misspecification. heterogeneity among pedigrees from different populations is another complicating factor. finally, the trait may be complex, possessing a genetic component but not a simple mode of inheritance. for such traits, linkage analysis may involve exploration of a vast multidimensional parameter space.

the posterior probability of linkage, or ppl,  <cit>  is a class of linkage statistics designed to model both sex-specific maps and the trait model complexity represented by the multidimensional parameter space in a mathematically rigorous fashion. it also accommodates both inter- and intra-population heterogeneity by using bayesian sequential updating over data subsets. however, the method requires the evaluation of integrals with no functional form, making it difficult to compute, and thus further test, develop and apply.

this paper describes mlip , a new, dual-language system that calculates, in parallel, two-point lod scores by pedigree and marker over a user-specified range of genetic parameters and male and female recombination fractions. mlip is designed to facilitate genetic linkage analysis  by allowing the coverage of a complex, multidimensional parameter space using partitioning and parallelization of the computation. motivation for the development of mlip derives from the utility of representing the underlying linkage likelihood explicitly as a function of all implicit parameters to allow computation of the ppl.

mlip utilizes nice, the network infrastructure for combinatorial exploration  <cit> , a network computing infrastructure for dynamic parallelization across a grid of processors. while parallelization has been considered previously in connection with linkage analysis , none of these approaches have dealt with computation over the trait parameter space, the focus of the present work.

methods
the simplest characterization of mlip is as a two-layer model: an inner layer that is used to compute each individual lod score, and an outer layer that oversees the systematic exploration of the multidimensional parameter space by dividing it into chunks, managing the distribution of these chunks to other processors, collecting the results, and subsequently writing them to disk.

inner layer: computing the lod scores
the two-point ppl is by definition the definite integral over the interval [ <dig>   <dig> ) of the recombination fraction . in other words, it is the probability that θ < 1/ <dig> given the data. for marker data m, and trait data t, the two-point ppl can be expressed in terms of lod scores as  <cit> :

  ppl=p∫θ∈[ <dig> )brfdθp∫θ∈[ <dig> )brfdθ+) 

where:

  br=∫α∫g10hlodffdα dg 

in these equations, l indicates linkage, g is a vector of trait parameters, θ is the recombination fraction, α is the admixture parameter, and f, f, f are prior distributions for g, θ, and α respectively. br, a function of θ, is the direct analogue of the usual likelihood ratio . since it is also proportional to the posterior density of θ, the br is simply multiplied across the data sets to sequentially update the ppl over multiple sets of data and the ppl is recomputed using eq.  <dig>  <cit> . nuisance parameters are integrated out of the br independently for each set of data, thereby allowing these parameters to vary independently across data sets, while evidence regarding θ is accumulated across data sets. since the integrand for computing the ppl involves several integrals and has no analytical functional form, it is approximated by averaging heterogeneity lod scores  over a discretized parameter space  <cit> .

the inner layer of mlip allowing computation of the lod scores necessary for the ppl is liped  <cit> , jurg ott's fortran implementation of the elston-stewart algorithm  <cit> . liped has been selected as the inner layer of mlip for several reasons. since the program is an implementation of the elston-stewart algorithm, it is capable of handling the medium to large pedigrees regularly encountered in our own clinical collaborations.

not only is liped both trusted and well tested, with the source code freely available, but it also has additional important advantages over some of the other existing elston-stewart implementations. computing the ppl requires the flexibility to include a variety of genetic parameters in the model. liped supports sex-specific recombination fractions; accepts haplotype frequencies in a manner that allows modeling linkage with disequilibrium correctly; handles quantitative traits; models delayed onset; and supports loops, or complex pedigree structures where more than one path may exist between two individuals.

while the code is not new, when compared to more recent elston-stewart implementations , optimized to reduce the cost of a single lod score calculation, somewhat surprisingly, the performance of the original liped code is still quite competitive overall. for some pedigree structures, it even provides better performance than more recent implementations. when tightly integrated into mlip and with just a few small improvements, our modified liped core's performance is quite respectable .

finally, although liped is written in fortran, the code is relatively compact and well documented. since the modifications required to the original liped code did not involve extensive changes to its algorithmic core, our resulting calculations should inspire the same faith and confidence as those produced by the original liped code.

outer layer: partitioning over a cluster
wrapped around liped is a new outer layer, coded in ansi c. this layer is responsible for reading in the problem specification  and checking it for errors; performing simple data transformations in the interests of efficiency ; and breaking the space into appropriate size chunks to assign chunks to itself and other processors. it is also responsible for invoking liped and collecting computed lod scores and storing them on disk. finally, the layer ensures that the entire space is exhausted while taking appropriate measures to allow effective restarting of a previously interrupted computation via checkpointing.

mlip utilizes the nice infrastructure for distributed computing. unlike peer-to-peer infrastructures, nice models the distributed computing resource as a hierarchy of processors . a nice-enabled application communicates with the infrastructure via a set of callable functions contained in the nice applications programmer's interface, or api. this linkable library contains functions that, when invoked, e.g., request that new copies of the application be spawned on the same or other machines. it also contains functions to support communication between these multiple copies of the application once they are established. note that the library does not actually contain code specific to any application, but rather only the handful of functions that are needed to efficiently parallelize appropriately designed serial applications.

once the nice infrastructure has spawned the appropriate configuration of processes over the nodes in the cluster, parallelization by partitioning involves having an idle slave processor ask its master for a chunk of work and then returning the results once the calculation is completed. since a master processor may have multiple slaves, it should be able to partition independent chunks of its own assigned work among them. the model extends recursively, with each slave potentially having slaves of its own, subdividing its own chunk into still smaller ones.

there are several complicating factors. first, this is a mixed-language system , which requires special handling  <cit> . second, the outer layer is responsible for managing the distribution of chunks to other processors. this requires care to keep all processors working at maximum capacity. a good partitioning scheme performs load balancing  and is also fault tolerant . good load balancing in heterogeneous computing environments presents a challenge. differences in processor speed or memory capacity, and thus intrinsic computation speeds, makes it difficult to distinguish an unduly slow slave from one that has failed. furthermore, for hierarchically-organized processors, some processors will appear much faster than others by virtue of the processor available beneath them. finally, in the case of mlip, we have no a priori knowledge of the amount of cpu time required to perform a unit of work. this is due to the fact that not all lod scores are created equal: differences in markers, pedigree size and structure, and pattern of unknown genetic marker and/or trait data, all affect the cost of the calculation.

since load balancing and fault tolerance are intertwined, the load balancing scheme should also handle lost processors in a systematic fashion, "recovering" and reassigning inchoate chunks that had been assigned to slower or unresponsive processors. in this manner, faster processors cover more and more of a slower processor's assigned territory. if a processor has failed, the faster processors will eventually complete its work. if a processor is truly slow and completes its work before the faster processors report back, the redundantly assigned processors can be aborted without prejudice. this strategy implies that some lod scores may be computed more than once to keep all cpus busy and preserve fault tolerance. the key issue here is to tune the partitioning criteria such that all chunks are finished at approximately the same time and little redundant calculation actually occurs.

finally, since the outer layer is ultimately responsible for collecting the lod scores and efficiently streaming them to disk, it must keep up with the high rate with which results are generated by multiple processors. for mlip, the protocol invoked when a slave completes a portion of its work is the most expensive and the number of lod scores returned to the root can be quite large. for example, for  <dig> pedigrees, a typical genome-wide analysis with  <dig> markers over the regular space for the sex-specific ppl  <cit>  entails the calculation, communication, and storage of  <dig>  ×  <dig> lod scores, roughly  <dig> gb of output. this presents a special challenge for distributed computing. typically, distributed applications will require that the input data sets, which can be quite large, be broadcast to individual processing elements at initialization, and that the subsequent results, which are typically small, be collected over the course of the calculation. here, the input data sets, while not insignificant, are trivially small in comparison to the size of the results, which must be transmitted over the network for storage on disk, so that, e.g., the ppl can be computed and recomputed with arbitrary priors.

to minimize communication costs, each slave node, regardless of its relative position in the hierarchy, communicates its results directly back to the root node, circumventing its own master.

RESULTS
the fundamental question addressed here is whether additional processors effectively reduce total computation time. while the use of n processors should, in theory, allow completion of the same calculation n times faster, few, if any, parallel computing efforts attain linear speedup in practice. this is because some of the work does not parallelize. for mlip, all disk-writing costs are incurred solely by the root processor, while network communications are a necessary expense of all parallel systems.

a sequence of empirical tests is used to explore the question "how close can we get?" to linear speedup. since it is well known that pedigree size and structure, presence/absence of marker genotypic information, and number of marker alleles all affect the degree of difficulty and expected computation time for the elston-stewart algorithm, four simulated test sets have been generated from very different parts of the spectrum. we also test performance on a real data set later. to avoid confounding factors due to complex interactions between data features and computation time, the simulated sets are homogeneous, each consisting of many replicates or variants of one of two pedigree structures .

each test set has  <dig> markers. the markers for sets a and b have  <dig> alleles each, while for sets c and d the markers each have  <dig> alleles. the number of pedigree copies and marker alleles in each set reflects the expected complexity and computation time for the configuration. sib pairs with two-allele markers and fully-known parental genotypes  are expected to require the least amount of computation per lod score, while larger pedigrees with more diverse markers and missing genotype information represent the other extreme . since the sets have different intrinsic difficulty, a different number of model parameters  are selected for each set allowing a single processor trial to take approximately the same amount of time across sets. to examine the behavior of the system as the number of processors increases, each set is tested with n =  <dig>   <dig>   <dig>   <dig>   <dig>  and  <dig> processors arranged in a uniform nice hierarchy, where each processor has at most three child processors. the grid size is increased proportionally as n increases. table  <dig> gives the minimum  and maximum  grid size for each test set.

speedup and scalability
clearly, as seen from the plot, linear speedup is not being achieved for each of the test sets. what is interesting here is that the observed performance appears to be clustering by set, with b, c performing better than a, d.

to understand the reason behind this observed performance, we need to take a closer look at computation, disk-writing, and network costs of the root processor. of these three, only the first represents work that can be distributed to other processors. furthermore, while we expect that disk activity should remain relatively constant , we might also expect to see network activity increase, given that the root processor must now receive relatively more results from other machines.

set a, on the other hand, is the least computationally-intensive of the four sets. however, the grid space for this set is at least twice as large as the space for any other set. as the number of processors grows, the amount of calculation performed by the root processor diminishes to the extent that with n =  <dig> processors, almost all of the root processor's workload comprises disk and network activities. test sets b and c represent the most favorable setup here with fairly complex data, and a sufficiently large grid space for distribution.

behavior over time
we next consider the qualitative behavior of an mlip calculation over time. figure  <dig> plots the load or the total number of log-likelihood scores computed as a percentage of the original grid size, versus the elapsed time for data set c, normalized to account for grid size differences.

the single-processor curve shows steady progress through the test, with each regularly-spaced data point corresponding to completion of one of the  <dig> identical pedigrees in data set c. since there is only one processor, there is no additional overhead to complicate matters.

the two-processor system also produces a regular, smooth, line, while working its way through its load at about twice the speed. with additional processors, the slope of the lines in the plot continues to increase, although the relative margin of improvement declines. furthermore, from a qualitative perspective, three striking changes can be seen in the lines generated. first, the spacing of the data points increases and is much less regular than for n =  <dig> and n =  <dig>  second, the slope of the line decreases towards the end of the calculation, forming a pronounced "knee" near the end of the trial. finally, each line generally extends beyond 100% load.

these visual cues tell an interesting story: the increased spacing implies that the root processor is spending relatively more of its time supervising the collection of data from slaves as opposed to making progress on its own calculations. the fact that the spacing is irregular and tends to decrease towards the end of the calculation is because the chunks being handed out to idle slave processors are getting smaller as the remaining workload decreases, leaving less work for the root to complete a pedigree. moreover, it becomes increasingly difficult to find work for idle processors as the size of the remaining calculation decreases, so some processors are simply told to remain idle. since the idle slaves are not producing lod scores, the overall rate at which the load completes is reduced, resulting in a lower slope. finally, as the number of processors increases, proper load balancing also becomes tricky, leading to an increase in the number of redundant lod scores computed notwithstanding the fact that the homogeneity of this data  represents a best-case scenario for load balancing.

reducing network and disk overhead
the time devoted to network communication and disk access represents a significant obstacle to better performance. two design changes are evaluated here to help sidestep these obstacles. in the original liped implementation, lod scores are calculated as double precision floating point numbers. since only  <dig> to  <dig> decimal digits of precision are needed for computing the ppl, the cost of disk storage can be halved and the data rate on the network doubled, simply by casting double precision lod scores to 32-bit floating point.

a second improvement comes from the manner in which these values are written to the disk. the file on the disk is as an ordered repository of lod scores, indexed by pedigree and parameter space values. since each processor generally produces lod scores in sequential order, collections of lod scores can be efficiently streamed to the disk at the same time, thereby amortizing disk access time and producing higher throughput.

improving performance via better load balancing
one of the hardest problems for any distributed system is how to best keep all available processors equally busy. the granularity issue briefly mentioned previously is one incarnation of this more general problem that arises when the space remaining is not easily apportioned to all available processors. for some datasets  the problem is particularly acute, but it arises to some degree with any dataset, albeit perhaps only at nodes that are nested deeply within the nice hierarchy. a related problem occurs when the root processor is unable to keep up with the results being generated by all the slave processors in the hierarchy. in this situation, the slaves queue up, lying idle while they wait for service from the root processor. the problem is compounded because any idle slaves wanting additional chunks must also wait in line for service. finally, inefficiencies arise when the allocation of processors within the hierarchy of processors is suboptimal. unfortunately, what constitutes the optimal allocation depends on the specifics of the problem being solved, and is not always easy to predict when the data are not as well behaved as our simulated data sets.

consider, as an illustration of this last point, the following empirical results . in the results reported thus far, all processors were organized in a hierarchy with branching factor limited to 3; that is, no processor has more than  <dig> child processors. we repeated our test for each of the four datasets with a variety of branching factors, ranging up to a relatively "flat" hierarchy where the root processor had as many as  <dig> immediate descendent processors. for the most part, a "broader" hierarchy led to reduced speedup. some of these reductions  are quite dramatic: using  <dig> processors in a flat hierarchy led to only an 8% improvement in performance over the single processor case. but for other datasets , peak speedup was obtained when each processor was allowed as many as  <dig> children.

the message here is that load balancing is difficult: no single strategy is likely to be optimal for all possible datasets. even with the simple, uniform, datasets used here , effective load balancing is likely to rely on a set of heuristics or a collection of adaptive mechanisms that can help fine tune performance while the calculation is underway.

performance on real data
in preceding sections the performance of mlip was evaluated using simulated test sets with homogenous complexity of each grid point. here, a set of  <dig> real pedigrees are considered next. these pedigrees consist of 4– <dig> individuals, genotyped for  <dig> microsatellite markers, although not all individuals in a pedigree may be genotyped. the grid size  is roughly similar to that used for sets a and b. figure  <dig>  which shows percent load completed against elapsed time for the real data, can be compared with figure  <dig>  which plots similar measures for simulated set c.

the most striking observation is that figure  <dig> is decidedly less smooth than figure  <dig>  this fact is to be expected, given that the computation time of an individual pedigree is likely to vary significantly with real data. in particular, one pedigree encountered about 25% of the way through the serial run required about 10% of the overall runtime . the effect is less noticeable as more processors are added, since the other processors continue to generate lod scores while the one exceptionally expensive pedigree is being handled.

as with figure  <dig>  the increased spacing between points with the addition of more processors reflects a change in root processor workload from computation to disk i/o. however, this change is particularly marked here for the  <dig> processor case. in figure  <dig>  it can be seen that with  <dig> processors, the root processor completes the space for its first pedigree only when over 90% of the total lod scores have already been computed. for both figures  <dig> and  <dig>  spacing between successive data points tends to decrease as the trial completes, indicating smaller chunk sizes. percentage of effort also increases beyond 100% as processors perform redundant calculations in an attempt to ensure fault tolerance. for the real data, however, figure  <dig> reveals that the wildly differing costs of each pedigree contribute to broad fluctuations in final percentage of effort.

CONCLUSIONS
this paper described mlip, a new multiprocessor two-point genetic linkage analysis system that enables statistical calculations based on the full multidimensional parameter space implicit in the linkage likelihood. empirical results on a broad variety of data have also been provided to support the claim that mlip does significantly speed up two-point lod score calculations over the grid space of model parameters necessary for computing the ppl. in keeping with the requirements for the statistic, mlip not only provides needed flexibility in model parameter specification, but it also stores individual lod scores, providing users with the ability to create data subsets and change priors during post processing. obtaining the output in this form provides some additional benefits as well. the likelihood surfaces tend to be irregular with extensive interaction among the parameters. the output produced by mlip can also be used by visualization software to allow the user to interactively explore the likelihood surface  <cit> .

the immediate task is to continue making incremental improvements in the load balancing strategy. the current partitioning strategy is not sensitive to predictable differences in expected computation time required for different portions of the grid space. a more fine-grained partitioning strategy would at the very least postpone the granularity problems described earlier. adaptive partitioning strategies that take cues from observed run times in the course of a calculation are also possible.

while the distributed computing approach adopted here is more coarse-grained, it may be useful to also consider parallelization at the algorithmic level. in such a situation, a hybrid distributed computing infrastructure-allowing both distributed and shared memory-may be a possibility. use of a grid computing type of infrastructure may perhaps facilitate better time results as opposed to a hierarchy of processors.

another area of interest is the definition of the grid space itself. currently, by use of a simple grid specification language, the grid space is defined a priori by the user when the mlip job is started. since much of the space might be expected to be relatively "flat," standard adaptive quadrature techniques from numerical integration might be used to allow mlip to effectively "design its own grid" over the course of an evolving calculation.

finally, it is a fact that the current core is not fully optimized with respect to the actual likelihood calculation. well-established differences exist among linkage analysis algorithms. both these facts suggest that the use of a new computational core, different from liped, could significantly extend the capabilities of mlip. in particular, liped is restricted to two-point analyses. based on what we have learned with mlip, we are working on constructing a multipoint parallel linkage analysis system  <cit>  based on a reimplementation of the vitesse  <cit>  linkage analysis engine.

as work continues to optimize the performance of the system, the "brute force" grid search method to approximate the ppl as implemented in mlip, will continue to serve as a benchmark against which future approximations may be judged. further development and evaluation of fast alternative numerical integration methods is necessarily predicated upon the ability of this program to carry out the calculation based on a full grid-enumeration in an efficient manner.

competing interests
the authors declare that they have no competing interests.

authors' contributions
mg implemented a large portion of mlip and was also responsible for all testing of the application and writing its linux man page. ams supervised the implementation and developed the nice infrastructure utilized by mlip and further enhanced the infrastructure to provide efficiencies in data streaming and partitioning. vjv conceived of the project and developed the linkage analysis statistic  which mlip was written to compute. all authors contributed equally to the conceptual design of mlip and in composing this manuscript.

