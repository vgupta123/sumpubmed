BACKGROUND
computer science is becoming increasingly important in today’s genetic research. the accelerated progress in molecular biological technologies puts increasing demands on adequate software solutions. this is especially true for next generation sequencing  where costs are falling faster than for computer hardware
 <cit> . as a consequence, the accompanying growth of data results in longer execution times of currently available programs and requires new strategies to process data efficiently. the mapreduce framework
 <cit>  and especially its open-source implementation hadoop
 <cit>  has become more and more popular for processing and analyzing terabytes of data: mapping ngs data to the human genome
 <cit> , calculating differential gene expression in rna-seq datasets
 <cit>  or even simpler but time intensive tasks like matching strings in large genotype files <dig> are already successfully implemented scenarios. with mapreduce, a computation is distributed and executed in parallel over all computer nodes in a cluster, allowing to add or to remove nodes on demand . the developer is responsible to write the corresponding map and reduce task and the framework itself is taking over the parallelization, fault tolerance of hardware and software, replication and i/o scheduling. unfortunately, small to medium sized genetic research institutes can often hardly afford the acquirement and maintenance of own computer clusters. an alternative is public cloud computing which offers the possibility to rent computer hardware from different providers like amazon’s elastic compute cloud  on demand.

working with cluster architectures requires a background in computer science for both setting up a cluster infrastructure and executing mapreduce programs, where sometimes a graphical user interface  is lacking at all. to improve usability, different programs
 <cit>  have been developed with a focus on a simplified execution. this constitutes a major improvement for scientists with the down side to implement a new gui for every future mapreduce program. additionally, concatenating different programs to a pipeline is still hampered.

in this paper we present cloudgene, a platform to integrate available mapreduce programs via manifest files and to facilitate the use of on-demand cluster architectures in cloud environments. cloudgene’s biggest advantage lies in simplifying the import and export of data, execution and monitoring of mapreduce programs on in-house  or rented clusters  and allowing the reproducibility of an analysis or analysis pipeline.

implementation
overall design
in order to be used in public and private clouds, cloudgene consists of two independent modules, cloudgene-cluster and cloudgene-mapred. cloudgene-cluster enables scientists to instantiate a cluster on a public cloud, currently applied on amazon’s ec <dig>  the end user is guided through the configuration process via graphical wizards, specifying all necessary cluster information including the complete hardware specification, security credentials and ssh keys. cloudgene-mapred can be seen as an additional layer between apache hadoop and the end user and defines a user-friendly way to execute and to monitor mapreduce programs, providing a standardized import/export interface for large datasets. cloudgene-mapred supports the execution of hadoop jar files , the hadoop streaming mode  and allows a concatenation of programs to program pipelines. one central idea behind cloudgene is to integrate available and future programs with little effort: therefore, cloudgene specifies a manifest file  for every program which defines the graphical wizards to launch a public cluster or mapreduce jobs . figure
 <dig> summarizes how these two modules collaborate together to execute programs depending on the specified cluster environment.

architecture and technologies
both modules are based on a client–server architecture: the client is designed as a web application utilizing the javascript framework sencha ext js . on server side all necessary resources are implemented in java by using the restful web framework restlet 
 <cit> . the communication between client and server is obtained through asynchronous http requests  with json  as an interchange format. cloudgene is multi-user capable and encrypts the transmission between server and client with https . to integrate new programs and describe all properties of a program or program pipeline, the yaml  format is used to define the manifest file. all required metadata is stored in an embedded java sql database h <dig> . the apache whirr
 <cit>  project is used to launch a cluster on amazon ec <dig>  to combine nodes to a working mapreduce cluster and to define the hardware environment of it. figure
 <dig> summarizes the overall architecture.

cloudgene-cluster
after a successful login to cloudgene-cluster, the main window provides the possibility to create or to shut down a public cluster and to get an overview of all previously started nodes . when launching a new cluster a wizard is shown: in a first step the cloud provider, cluster name, the program to install, the amount of nodes and an available instance type  need to be selected . subsequently, the cloud security credentials have to be entered and an ssh key has to be chosen or uploaded . for user convenience, security credentials need only be entered once for every session  and can additionally be stored encrypted in the h <dig> database. a storage of ssh keys is especially useful for advanced users who want to login into a node via an ssh console. in addition, an s <dig> bucket can be predefined for an automatic transfer of mapreduce results. within minutes a ready-to-use cluster is created, where all necessary software is installed and parameters are set. as a final step, cloudgene-cluster installs cloudgene-mapred on the launched cluster and returns the web address for accessing it. cloudgene-cluster provides the possibility to download ssh keys, to access the log with all performed actions during cluster setup, to add new users or to logout from the system.

cloudgene-mapred
the main window of cloudgene-mapred  is structured as follows: the toolbar on top contains buttons for program  submission, data import and program installation. additionally, buttons for changing the account details  and detailed cluster information  are provided. all currently running and finalized jobs including name, progress, execution time and state are displayed in the upper panel. for running jobs, the progress of the map and reduce phases are displayed separately. the lower panel displays the job-specific information including input/output parameters, s <dig> export location, job arguments, execution time and results. the export location is created automatically using the naming convention s3bucket/jobname/timestamp. moreover, the detail view contains a link to the logfile in case of errors.

before launching a new job, data needs to be imported into the distributed file system , whereby the data source has to be selected. currently, cloudgene supports a data import from ftp, http, amazon s <dig> buckets or direct file uploads. a job can be submitted by specifying the previously imported data and the program-specific parameters. after launching a program, the process can be monitored and all jobs including results are viewable or downloadable. as all data on a cluster in a public cloud are lost on shutdown, cloudgene automatically exports all results, log data and if specified also imported datasets in parallel to an s <dig> bucket.

plug-in interface
to integrate new programs into cloudgene, a simple structured yaml manifest file has to be specified including a section for both cloudgene-cluster and cloudgene-mapred. this manifest file needs to be written once and can be either provided to other scientists by the developer or written by any person who is familiar with the execution of a mapreduce program. the manifest file starts with a block containing general program information . in the cloudgene-cluster section the file system image, available instance types, firewall settings, services , installation scripts  and other program depended parameters are specified. the cloudgene-mapred section contains all necessary information that characterizes a mapreduce program including input and output parameters or cloudgene’s step functionality . at start up, cloudgene loads all necessary information from the manifest file and generates the program specific wizards. figure
 <dig> shows the integration of cloudburst into cloudgene-mapred. to simplify the integration process for end users, all currently tested mapreduce programs including working manifest files, a detailed description of available parameters, available instance types and a tutorial on how to set up an ec <dig> security credentials can be found on our website. furthermore, cloudgene-mapred provides with its integrated web repository a mechanism to install currently available programs directly via the web interface.

RESULTS
cloudgene’s overall aim is to simplify the process of executing mapreduce programs including all required steps on private and public clouds . in the following section, we want to show on different case scenarios the diversity and advantage of cloudgene. table
 <dig> summarizes all currently integrated programs.

integrating existing mapreduce programs
as mentioned above, several programs  already exist implementing a mapreduce approach to process data. to demonstrate the benefit of cloudgene, we integrated these programs by writing appropriate manifest files, including sections for cloudgene-cluster  and cloudgene-mapred . in case of cloudburst, a mapreduce job can now be executed graphically and the benchmark tests show that cloudgene is scalable in time and competitive to amazon’s elastic mapreduce platform
 <cit>  regarding to cluster setup and program execution time . for myrna and crossbow, a web interface has already been made available by the authors using hadoop’s streaming mode. nevertheless, by integrating these programs into cloudgene, the users still benefit from  a standardized way to import/export data,  a system which keeps track of all previous executed workflows including the complete configuration setup  and  the possibility to concatenate different mapreduce jobs to pipelines. here, cloudgene’s pipeline functionality  has been used to execute several computation steps of crossbow and myrna. this can be achieved by defining the output directory of step x  as the new input directory for step x +  <dig>  in the manifest file. even if the newly created workflow consists of several steps in the manifest file, the user can start it as one job. additional file
 <dig> includes the corresponding manifest files showing the step functionality in detail.

integrating novel mapreduce programs
for a proof of concept, we implemented a mapreduce pipeline for fastq pre-processing to check the quality of large ngs datasets similar to the fastqc tool <dig>  in which statistics like the sequence quality, base quality, length distribution, and sequence duplication levels are calculated. unlike fastqc, which can be executed on a single computer and where due to memory requirements only a sequence subset is used for quality control computations, our implementation has the advantage that the complete set of sequences is included into the statistical analysis. after processing all sequences, a consecutive program transforms the results into meaningful plots and can finally be downloaded as pdf-files. again, this shows the step functionality of cloudgene-mapred in which different programs can be connected to a pipeline . moreover, fastq input-files can be imported from public amazon s <dig> buckets , which is especially useful in combination with amazon ec <dig> since data transfer from s <dig> to ec <dig> nodes is optimized .

a further scenario for a simple but time-intensive task is the filtering and extracting of certain rows from a large snp-genotype file as usually used in genome-wide association studies with a file size of several gigabytes. again, this case scenario was implemented as a mapreduce job by the authors of this paper and has been integrated into cloudgene.

all integrated and introduced programs are available for download on our webpage or can be installed from the cloudgene repository directly.

launching a stand-alone image
different file system images  exist to provide a convenient way for setting up systems in the cloud: cloud biolinux
 <cit>  is a suitable file system image in bioinformatics including a wide range of biological software, programming libraries as well as data and is therefore an excellent basis for bioinformatic computations in the cloud. rstudio
 <cit>  is a development environment for r, available as an ec <dig> image
 <cit>  and allows the usage of all r programming tools via a web interface. both systems have been successfully integrated into cloudgene enabling scientists to launch these systems via cloudgene-cluster. cloud biolinux has further been used as the underlying image for the integration of myrna and crossbow into cloudgene, since most of the required software is already installed and the cluster installation process is therefore simplified.

launching a web application
besides the mentioned mapreduce scenarios, cloudgene can also be used to host a user defined web application on a public cloud. we demonstrated this on haplogrep
 <cit> , a tool to determine mitochondrial dna haplogroups from mtdna profiles. especially for large input data, haplogrep requires a lot of main memory, thereby making a public cloud node with sufficient main memory an adequate choice. for this purpose a simple shell script was integrated into the setup process to start the haplogrep web server. this shell script can be defined in the manifest file of cloudgene-cluster and is executed automatically on each cluster node.

discussion
although mapreduce enabled a scalable way to process and analyze data, the execution of programs and the overall setup of cluster architectures still includes non-trivial tasks and hampers the spread of mapreduce programs in bioinformatics. we therefore developed and implemented cloudgene, which provides scientists a graphical execution platform and a standardized way to manage large-scale bioinformatic projects.

strengths and limitations
the usage of cloudgene has several strengths:  programs can be executed via one centralized platform, thereby standardizing the import/export of data, the execution and monitoring of mapreduce jobs and the reproducibility of programs or new defined program pipelines;  scientists can decide flexibly in which environment  a program should be executed depending on the case scenario to guarantee an appropriate level of data security and to reduce data transfer times;  an amazon ec <dig> cluster can be launched via the cloudgene web interface, thus simplifying the overall setup and making cloudgene available in a public cloud;  new mapreduce programs can be integrated by using cloudgene’s plug-in interface without changing the program code at all.

however, cloudgene has limitations as well:  since the main focus of our approach lies on a simplified execution of mapreduce jobs, programs using other paradigms  are currently not supported;  cloudgene allows the concatenation of jobs to simple pipelines with the limitation that pipelines have to be executed from start to end and currently does not allow the execution of specific pipeline steps automatically ;  since the amount of included cluster nodes must be set at start, the launched cluster architectures are currently static and are not changeable during runtime.

comparison with similar software packages
to date, several approaches exist to improve the usability of currently available bioinformatic solutions. systems such as galaxy
 <cit> , genepattern
 <cit> , ergatis
 <cit> , mobyle
 <cit>  and taverna
 <cit>  try to facilitate the creation, execution and maintainability of workflows in a fast and user-friendly way. in contrast to these existing workflow platforms, cloudgene’s primary focus lies on the usability of mapreduce jobs in public and private clouds for bioinformatic applications.

galaxy cloudman
 <cit>  is a similar approach to cloudgene-cluster and supports users to set up cloud clusters using amazon ec <dig>  it works in combination with bio-linux  and configures at start up the oracle grid engine
 <cit>  as well as galaxy. unfortunately, cloudman does not support mapreduce by default and therefore a graphical execution and monitoring of jobs is not possible.

another mentionable and useful system is amazon elastic mapreduce 
 <cit> , which provides the opportunity to create job-flows including custom jars, streaming or hive/pig programs. since everything is located on amazon directly, a highly optimized version of hadoop mapreduce in combination with amazon s <dig> is provided and can be executed by a comprehensive user interface. nevertheless, amazon elastic mapreduce can only be used in combination with amazon ec <dig>  sometimes preventing research institutes from using it due to data security rules or the enormous amount of data to transfer <dig> from their own institutional cloud. in contrast, cloudgene allows launching mapreduce jobs both on public or private clouds, thereby enabling the user to define the location of data. since cloudgene does not utilize emr for its job execution, additionally financial costs for emr can be saved. table
 <dig> summarizes the comparison of cloudgene and emr and shows that cloudgene is competitive regarding cluster set-up, job execution and data transfer.

clovr
 <cit>  is a virtual image that provides several analysis pipelines to use on a personal computer as well as on the cloud. it utilizes the grid engine  for job scheduling and plans a possible future integration of hadoop mapreduce. eoulsan
 <cit>  is a modular framework which enables the setup of cloud computing clusters and automates the analysis of high throughput sequencing data. a modular plug-in system allows the integration of available algorithms. eoulsan uses emr for the execution of their mapreduce jobs and has to be executed on the command-line. both systems improve the usage of programs for scientists, but are not focused on a graphical execution of jobs on public and private clusters.

future work
the success of a platform like cloudgene goes hand in hand with the amount of involved users and scenarios. therefore, our short-term focus will be on an extension of cloudgene with new case scenarios, hopefully motivating users integrating their own mapreduce programs or systems. one of the biggest advantages of public clouds is the opportunity to rent as many computer nodes and thus computational power as needed. thus, the next version of cloudgene is conceived to provide functions for adding and removing computer nodes during runtime. furthermore, a simple user interface for hadoop is not only useful for end users but also for developers. it supports them during prototyping and testing of novel mapreduce programs by highlighting performance bottlenecks. thus, we plan to implement time measurements on the map, reduce and shuffle phase and to visualize them in an intuitive chart. additionally, hadoop plans in its future version the support of alternate programming paradigms, which is particularly important for applications where custom frameworks outperform mapreduce by an order of magnitude .

CONCLUSIONS
we presented cloudgene, a platform that allows scientists to set up a user-defined cluster in the cloud and to execute or monitor mapreduce jobs via a dynamically created web interface on the cluster. cloudgene’s aim is to integrate existing and future mapreduce programs via a manifest file into one centralized platform. cloudgene supports users without deeper background in computer science and improves the usability of currently available mapreduce programs in the field of bioinformatics. we think that this approach improves the utilization of programs and the reproducibility of results. additionally, we showed on different scenarios how an integration can be fulfilled without adding overhead to the computation, thereby improving both the development and the usability of a program.

availability and requirements
project name: cloudgene

project home page:http://cloudgene.uibk.ac.at

operating system: cloudgene-cluster , cloudgene-mapred 

programming language: java, javascript

other requirements: java  <dig> , aws-ec <dig> account for public clouds, hadoop mapreduce for private clouds

license: gnu gpl v3

any restrictions to use by non-academics: none

endnotes
1 see
http://cloudgene.uibk.ac.at/usecases.

2http://www.bioinformatics.babraham.ac.uk/projects/fastqc/.

3 amazon web services  provides possibility to ship data on hard-drives.

 <dig> in-house implementation.

competing interests
the authors declare that they have no competing interests.

authors' contributions
lf and ss initialized the project and developed cloudgene. lf, ss, hw and ak-b were responsible for designing cloudgene. ak-b, gs and fk supervised the project. lf, ss, hw and ak-b drafted the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
supplementary material to cloudgene: a graphical execution platform for mapreduce programs on private and public clouds.

click here for file

 acknowledgements
ss was supported by a scholarship from the university of innsbruck . hw was supported by a scholarship from the autonomous province of bozen/bolzano . the project was supported by an amazon research grant, the grant ‘aktion d. swarovski’ and by the Österreichische nationalbank  as well as the sequencing and genotyping core facility of the innsbruck medical university. we thank the open source and free software community as well as the apache whirr mailing list, especially tom white and andrei savu for their great assistance.
