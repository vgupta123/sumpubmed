BACKGROUND
high-throughput microarray technologies are becoming the norm in genetic and molecular research. nevertheless, some steps in the preprocessing of the data prior to main analyses still remain problematic, as there is no universally accepted procedure for background correction, expression-value summarization and normalization. here we are focusing on the normalization step of affymetrix expression arrays, whose main purpose is to remove any systematic non-biological array-to-array variation. it is well known that  a noisy technical variation exists between arrays  <cit>  due to many factors such as mrna quantities, scanner settings, instrument operator, etc., and  the choice of normalization method can make a substantial impact to the final results  <cit> .

currently, the quantile normalization  <cit> , global normalization  <cit>  and loess normalization  <cit>  are among the most commonly used. however, all these methods rely on sensitive assumptions that may be violated in real applications. to illustrate the impact of normalization step on final results, table  <dig> reports the percentages of concordance in the number of genes declared differentially expressed  between different normalization procedure applied to the same expression measure . the gene expression measurements were taken from of skeletal muscle biopsies from  <dig> duchenne muscular dystrophy  patients and  <dig> unaffected control patients  <cit> . the same analysis method  <cit>  was used for all the algorithms. for  <dig> % false discovery rate limit, the number of de genes varied from  <dig> to  <dig>  and the concordance rate goes as low as 70%.

all normalization methods need a reference set of genes that do not vary between samples. most methods in fact use the whole set of genes as the reference set, and this choice is justified with two key assumptions  <cit>  that  the great majority of genes do not vary between samples, and  the distribution of up- and down-regulated genes is approximatively symmetric. under these assumptions, the simple global-mean normalization, for example, involves making all arrays have the same mean. the methods are not robust against violation of these assumptions: when either of the two assumptions is not satisfied, existing normalization procedures are not trustworthy. the problem is that, in practice, these assumptions are rarely checked. furthermore, it is not usually stated at what proportion the 'great majority' should be, but statistically we should probably expect at least 90%. much smaller proportions than that would undermine the methods; for example, if 40% of the genes vary, it is no longer credible that the global mean should be constant across the arrays.

spike-in experiments have been the key tool to establish current normalization schemes. most of these experiments are typically quite simple, involving only a few spike-in genes. for these experiments, most existing normalization methods work well. however, for the so-called golden-spike data  <cit> , almost  <dig> out of  <dig>  genes are spiked, among which  <dig>  are differentially expressed  and  <dig>  are nonde.

the golden-spike experiment is contrived, but it is important in revealing the lack of robustness of the existing normalization methods. because all the de genes are up-regulated, hence violating the balanced regulation assumption, all of the current normalization methods fails with this dataset. in many real studies, unbalanced regulation might reasonably happen  <cit> . this scenario has been already investigated in two-color microarrays  <cit>  and raises some question marks over the existing procedures. a different sensitivity to violation of this assumption might explain differences in performance of the methods. searching for a safer and more robust normalization procedure has been the motivation for this paper.

the so-called housekeeping genes, i.e. genes involved in the basic maintenance of the cells, might be considered a perfect reference set for normalization. in fact, they are used for normalization of pcr assays  <cit> . to survive, every cell is supposed to express them approximately at the same level  <cit> , so we do not expect the expression of these genes to vary between samples. affymetrix arrays contain a set of possible housekeeping genes, usually used for quality control procedures, but also suggested as the optimal reference for normalizing the arrays. nevertheless, a broad body of evidence exits that genes traditionally considered as housekeeping genes are in reality not invariant under a range of experimental and pathological conditions  <cit> . specific examples of the failure of normalization based on a priori housekeeping genes were given in  <cit> .

a data-driven procedure for identifying genes that do not vary across samples, and therefore might be a good reference set for normalization, leads to the so-called 'invariant-set normalization'  <cit> . the procedure selects the set of genes to use as a reference for normalization in a pairwise fashion. this is done by selecting genes whose ranks are invariant between each sample and a reference distribution, e.g. a pseudo-median sample.

our approach here is also based on data-driven housekeeping genes by identifying genes that vary the least between arrays. instead of using pairwise comparison between samples, we exploit the total information from all the samples. the information is extracted from the probe-level data, by partitioning the observed variability into array-to-array variation, within-probeset variation and residual variation. probesets whose array-to-array variability are below a given quantile are called the 'least-variant set' , and they provide the reference set for normalization.

to summarize our novel contribution, we have developed the lvs normalization and studied its performance in several spike-in experiments. we show that lvs normalization  performs similarly to other normalization methods when the standard assumptions are satisfied, but  performs better when the standard assumptions are violated. in fact, in the latter case, lvs performs similarly to the ideal  housekeeping-gene normalization. this means that the lvs normalization is robust against violation of the standard assumptions of normalization, so it is more widely and more safely applicable than the current normalization methods.

methods
the lvs normalization is based on a two-step procedure. the first step operates at the probe-level data in order to estimate the component of variance due to array-to-array variability. this step is a multi-array procedure, using all the arrays in order to identify genes that vary least between the arrays. if the number of samples is large, a proper subset should be used for faster computation. the second step involves a non-linear fit of the lvs genes from individual arrays against those from a reference array, such as a pseudo-median array.

identification of the lvs genes
to identify the lvs genes, we analyse the probe-level data. each probeset may contain from  <dig> to  <dig> pairs of perfect match  and mismatch  probes. first, the pm data is corrected for background; in our examples we use the so-called ideal mismatch   <cit> , but in principle any background correction method may be used. in figure s <dig>  additional file  <dig>  we show that different methods would produce similar lvs genes. then, for each gene, specify the model

  log <dig> = μ + αi + βj + εij, 

where pmij is the background-corrected pm value for the jth probe from the ith array, i =  <dig> ...,n and j =  <dig> ...,j. μ is the grand mean parameter, αi is the ith array effect, and βj is the jth probe effect. this log-linear model was the basis for the rma summary measure  <cit> . it was similar to, but not the same with, the li and wong model  <cit> , which uses a multiplicative term plus noise, so we cannot take log and get a log-linear model.

the model was fitted by a robust m-estimation method  <cit> , already implemented by the r package affyplm  <cit> . the array-to-array variability is captured by the χ <dig> test statistic, computed by

  χ2=α^′v−1α^, 

where α^ is the vector of estimated αi's, and v is its estimated covariance matrix. these quantities are available from the robust linear model fit.

the array effect αi includes both the technical artifact ti and real biological effect bi, so that

  αi = ti + bi. 

the ideal housekeeping genes are those with bi ≡  <dig> for all i, thus allowing for the estimation of the remaining systematic variation that comes solely from technical sources. the lvs genes are the data-based estimation of these housekeeping genes.

suppose for the moment that in model  ti and bi are independent random effects. then, for genes with the same technical variance, the total array-to-array variability for housekeeping genes should be less than that for non-housekeeping genes. this means that when we compare the χ <dig> statistics among the genes, those with smaller values are more likely to come from genes with bi ≡  <dig>  since the value of the statistic is determined by the residual variance, our assessment must also take it into account. the relationship between the χ <dig> statistic for array effects and the residual standard deviation can be seen graphically , hereafter called the 'ra-plot'.

thus, in practice, to determine the lvs genes, we fit a nonparametric quantile function  <cit>  of the array χ <dig> statistic  as a function of the logarithm of residual standard deviation , and declare those genes that fall under the curve as the lvs genes. in analogy with classical linear models where the conditional mean of the response variable is modelled as a function of some covariates, quantile regression aims at modelling any chosen quantile of the response variable a a function of the covariates. in our current application the quantile function is 'nonparametric' in the sense that it is not based on an explicit functional form, but on local smoothing of the data. we need to set a proportion τ, below which we expect genes not to vary between samples. in our experience τ = 60% is a reasonable choice, since we expect no more than 40% of the genes to be vary significantly between arrays. nevertheless, it might be possible to conceive of an experimental situation were a higher proportion of genes are expected to be regulated, so the user needs to tune the value of τ accordingly.

this step works on multi-array basis, requiring all the arrays for the analysis. for a large number of arrays, memory requirement can be reduced by analysing limited number of probes at a time. furthermore, to reduce computational burden, the analysis might also be performed on a random sub-sample of the data.

non-linear normalization on the lvs genes
once the lvs genes are identified, the normalization algorithm works on the individual arrays by fitting a spline smoother between the arrays and an arbitrary reference array. the latter is, for example, a pseudo-median array or any user-specified array. the curve fitted through the least variant genes is then used to map intensities of all the genes in each array to be normalized.

this normalization step might be performed either after expression summarization or at the probe-level, prior to other preprocessing procedures. any expression summary  might be used. for all analyses in this paper, step  <dig> is applied after background correction and expression summarization. finally, note that this step is single-array based, i.e., not requiring all the samples at the same time. this reduces the memory requirement during computation.

competing normalization procedures
the so-called global or constant normalization method is typically used by the affymetrix microarray suite  <cit> . each sample is rescaled to have mean set to some arbitrary target value . this is achieved by dividing each sample values by a scaling factor obtained as the ratio between the target value and the sample mean. while the standard mas <dig> algorithm works on the original scale, in our implementation we work on the log scale with zero target value, simply subtracting each sample by its mean.

while the global normalization assumes a linear relationship between the arrays, the invariant-set normalization  <cit>  uses a non-linear regression to normalize data. a subset of genes is first selected based on comparing the ranks of the expression values in each sample to a reference array. the idea is that invariant genes, supposedly nonde genes, should consistently have low ranks in each sample. a local regression is then fitted on the subset of invariant genes to get a normalization curve.

quantile normalization  <cit>  is a multichip procedure, were the expression distribution  of each sample is forced to be the same as the distribution of a reference sample. the reference might be any of the samples or any derived one, such as the pseudo-median sample.

datasets
three freely-available data sets have been used to evaluate the proposed normalization procedure. all of these data sets are from spike-in experiments, i.e. produced by controlled experiments with known rna intensities or predefined mutual relationships.

golden-spike data
the so-called 'golden-spike' experiment for affymetrix arrays designed by  <cit>  provides a dataset of  <dig>  rna species, where 100– <dig> rnas were spiked in at fold-change  level ranging from  <dig>  to 4-fold, while a set of  <dig>  rna species was spiked-in at a constant  level. data were designed as a two-group comparison, spike-in  versus control  , with overall  <dig> % genes over-expressed in s versus c. out of  <dig>  probesets on this drosgenome <dig> chip,  <dig>  had fc> <dig>  among which  <dig> had fc> <dig>   <dig>  had fc =  <dig> and  <dig>  were 'empty'. the fc <dig> genes are thus the ideal housekeeping genes, and provide the perfect reference set for normalization of this dataset. this dataset is chosen to represent a study where there is an imbalance between up- and down-regulated genes, so the current normalization methods are expected to fail.

affymetrix spike-in data
the two spike-in data sets were produced by affymetrix  <cit>  and are part of the assessment procedure at the affycomp ii website  <cit> . dataset hgu133a is based on a latin-square experiment with  <dig> arrays and overall  <dig> spiked-in genes at various concentrations ranging from  <dig>  to  <dig> pm. each concentration was performed with three replicates, and each array contains  <dig>  probes.

dataset hgu95a spike-in is one of the data used in the original assessment by  <cit> . it consists of  <dig> experiments arranged in a latin-square design, with  <dig> genes spiked-in at  <dig> different concentrations ranging from  <dig>  to  <dig> pm. each concentration has two replicates, and each array contains  <dig>  probes.

RESULTS
using probe intensities summarized according to the mas <dig> algorithm, we performed several normalizations, i.e. global normalization , quantile normalization and invariant-set normalization. additionally, we performed both a global normalization and a loess normalization using the set of genes with known fc =  <dig>  since the fc <dig> genes are the ideal housekeeping genes, these should provide the theoretically best normalization method.

similar results for rma expression values  are given in figure s <dig>  additional file  <dig> 

to show that the lvs normalization procedure had properly removed any trend, we produced paired ma plots  between the array and the pseudo-median array. in each plot we draw a loess curve along the lvs genes.

¿from a practical point of view, the most important property of a preprocessing algorithm is that it leads to downstream analyses with good operating characteristics . in the downstream analysis of this dataset, the genes are ranked based on the standard t-statistic. similar results were obtained using a moderated t-statistic instead  <cit>  . figure  <dig> shows the oc curves for several methods applied to the mas <dig> expression data. curves were drawn up to a maximum of  <dig> false-positive genes. the lvs normalization performed much better than the quantile, invariant-set or global normalizations, and quite close to the ideal fc1-based normalization. the areas under the curve  were  <dig>  for lvs,  <dig>  for global normalization  <dig>  for invariant-set,  <dig>  for quantile and  <dig>  for fc <dig> normalization; in this computation, the oc curves were standardized to have unit maximum.

affymetrix spike-in
the lvs algorithm was evaluated also on the well-known affymetrix spike-in experiments, namely the hgu-95av <dig> and the hgu133a data. the performance of the lvs method was tested within the framework of the affy competition  <cit> , using for expression summarization and background correction the methods adopted by the standard mas <dig> and rma algorithms. the reports automatically created by the affycomp ii website  <cit>  are available in the additional files  <dig>   <dig>   <dig>   <dig> 

because of the small number of spike-in genes and the clean separation between spiked and non-spiked genes , we do not expect a big difference in performance between lvs and other normalization methods. figure  <dig> shows the oc curves for the hgu133a dataset, based on the standard t-statistic and the whole set of  <dig> spike-ins. for rma-based expression values, no big difference was observed among normalization procedures . however, for mas <dig> expression values, a sharp improvement was obtained using the lvs and invariant-set normalization compared to global normalization. given the limited noise present in the affymetrix spike-in data, the selection of the subset of genes for normalization is expected to have little impact.

discussion and 
CONCLUSIONS
we have presented a new algorithm called lvs for normalizing high-density oligonucleotide arrays based on a set of genes with the least variability across samples. array-to-array variation is estimated through a robust linear model fitted at the probe level. in noisy and complex datasets such as the golden-spike data, lvs normalization outperforms other normalization procedure. in simple spike-in experiments where very few genes are expressed or spiked, all methods of normalization should work equally well. in real experiments, the normalization step does make a difference  <cit> , and in this case it is safer to use a more robust method that relies on fewer assumptions.

in the golden-spike data it seems clear that the signal observed for unexpressed genes was due to both experimental artifacts and nonspecific hybridizations. the latter occurs because probes associated with unexpressed genes might bind to other mrna species with high concentration. when these are differentially expressed, the non-specific binding will lead to false discoveries. so, assuming that probes associated with unexpressed genes should be nonde might indeed be wrong; in recent large arrays, these probes can be expected to be the majority. the complexity of non-specific binding also means that it is important to have more realistic spike-in experiments with a large number of expressed genes.

any normalization procedure is supposed to equalize the distribution of non-varying genes across samples, thus correcting for any random or non-biological systematic variation. obviously the determination of the non-varying genes must rely on pre-normalized data. most current procedures make certain assumptions that would allow one to use the full collection of genes as the reference set, and no analysis is needed to identify them. lvs exploits the information in the probe-level data to determine a pre-specified proportion of least-variant features across samples. in contrast with the invariant-set method, which uses pairwise comparisons between each array and a reference array, lvs uses the full collection of arrays. this partly explains the better performance of the lvs compared to the invariant-set method for the golden-spike data.

because of the intrinsic random noise in microarray experiments, a sufficiently large number of genes should be selected for normalization. normalization based on a small set of genes, such as the housekeeping-gene or invariant-set normalization, might be ineffective with noisy data. the lvs algorithm allows a reasonably large proportion of genes to be selected as a reference set; we get a stable result over a range of proportions from 40–60%.

one of the key assumptions in current normalization procedures is that there is a balance between up- and down-regulated genes. this explains the failure of the current procedures in the golden-spike data. in real studies, can we always expect balanced expression? in lab experiments, e.g. with knock-out mice, an unbalanced proportion of over/under-expressed genes may reasonably occur. haslett et al.  <cit> , for example, reported a relevant bias towards over-expression in muscle-related genes . a similar unbalanced pattern was reported by  <cit>  and  <cit> .

the problem is that, in practice, the assumptions underlying the normalization procedures are rarely checked, so it is never certain that the data are properly normalized. for example, even for clinical data with balanced expression levels, we showed in  <cit>  that the commonly-used quantile normalization was biased for low-intensity genes. this means that we need a robust and safe procedure that relies on fewer assumptions. we believe that lvs normalization is a step in that direction.

authors' contributions
sc performed the analysis, designed the package and wrote the paper. yp performed the analysis and co-wrote the paper. dv developed the package and co-wrote the paper. all authors read and approved the final manuscript.

supplementary material
additional file 1
supplementary report normalization of oligonucleotide arrays based on the least-variant set of genes. this file contains some supplementary figures.

click here for file

 additional file 2
bioconductor expression assessment tool for affymetrix oligonucleotide arrays . this report presents the automatic assessment of the lvs normalization method, with mas5-style summarization, based on the affymetrix hgu  <dig> spike-in experiment, generated by the affycomp website  <cit> 

click here for file

 additional file 3
bioconductor expression assessment tool for affymetrix oligonucleotide arrays . this report presents the automatic assessment of the lvs normalization method, with rma-style summarization, based on the affymetrix hgu  <dig> spike-in experiment, generated by the affycomp website  <cit> 

click here for file

 additional file 4
bioconductor expression assessment tool for affymetrix oligonucleotide arrays . this report presents the automatic assessment of the lvs normalization method, with mas5-style summarization, based on the affymetrix hgu  <dig> spike-in experiment, generated by the affycomp website  <cit> 

click here for file

 additional file 5
bioconductor expression assessment tool for affymetrix oligonucleotide arrays . this report presents the automatic assessment of the lvs normalization method, with rma-style summarization, based on the affymetrix hgu  <dig> spike-in experiment, generated by the affycomp website  <cit> 

click here for file
