BACKGROUND
statistical tests are a fundamental scientific tool for contrasting alternative hypotheses by rejecting or not the null one given an a priori fixed significance level. such a methodology may have two types of associated errors: type i error, i.e. the rejection of the null hypothesis when it is true  and type ii error i.e. the acceptance of the null hypothesis when the alternative one is true . most statistical tests traditionally aim to control type i error. however, such a strategy was originally developed to test a single null hypothesis, and an undesirable high rate of false discoveries may be obtained when working with families of comparisons under simultaneous consideration. different strategies have been considered to deal with this problem. the control of the familywise error rate  is performed by bonferroni likewise techniques. the aim of the fwer is to control the probability of making one or more type i errors in families of simultaneous comparisons. alternatively, false discovery rate  based methods aim to control the proportion of false discoveries among the total ones . multitest adjustment strategies have gained attention since the apparition of the so-called high-dimensional biological data as a consequence of the 'omic' technologies. therefore, in some research areas the number of tests accomplished has increased dramatically due to the recent technological improvements  <cit> . for example, in genomic and microarray studies it is becoming common to work simultaneously with more than  <dig>  tests  <cit> , and future studies may produce analysis of complete genomes with many thousands of polymorphisms between a few species  <cit> .

thus, there is an obvious interest to know which of the available multitest adjustments is the most useful. benjamini and hochberg  <cit>  demonstrated that the direct control of fdr increases considerably the statistical power of multitest adjustment. this is expected because any procedure that controls the fwer also controls the fdr being therefore more stringent than the fdr-based methods  <cit> . since then, several variants of the fdr and bonferroni adjustments have been proposed  <cit> , although there is no clear consensus about which is the best multitest adjustment in all conditions  <cit> . in any case, all available multitest adjustments show the inconvenience of decreasing statistical power when increasing the number of tests  <cit> . this occurs because all methods adjust each single test error rate according to the number of tests used. the consequence of this adjustment is that the higher the number of comparisons the lower the chance to detect even one significant  case using any multitest correction. such a conservative control of type i error is often not very useful from an experimentalist point of view  <cit> . in addition, multitest adjustment needs accurate estimates of the p-values  <cit> . for example, we need  <dig> decimal digits to use the bonferroni multitest adjustment with  <dig> tests at a significance level of 5% . furthermore, the use of non-parametric  methods at very small sample sizes  may produce inaccurate probabilities, which makes less effective the multitest adjustment. in such cases, it may be suggested the use of their parametric counterparts, but if the parametric assumptions are not met, this may produce biased probabilities which will be useless under any multitest method  <cit> . due to these and similar problems some authors have been reluctant to use multiple test adjustments indiscriminately  <cit> , or even recommend that multiple adjustments should not be used  <cit> .

ideally, any multitest correction should show a large statistical power and a small fdr under a small number of comparisons, and its statistical power should increase when increasing the number of tests, as most statistical tests do in relation to sample size. as explained above this is not the situation with any of the available multitest adjustments. here, we propose a new multitest adjustment methodology based on a sequential goodness of fit  metatest. this method will help the researcher to decide which of the tests, previously ranked based on their p-values, would be true discoveries. as desired, sgof increases its statistical power when the number of tests increases. in the present work we formalize the method, giving power and type i error expectations. we also perform simulations both via multiple one-sample t and homogeneity tests, to compare sgof with three alternative multitest adjustment methods: bonferroni , sequential bonferroni  and the benjamini and hochberg  <cit>   which is the original implementation to control for fdr.

our results show that sgof can be a valuable approach for multitest adjustments with high-dimensional biological data. under the most favorable conditions  this test can show a statistical power up to two orders of magnitude higher than the bh and bonferroni methods without increasing appreciably the false discovery rate .

RESULTS
definition of sequential goodness of fit  metatest
consider testing a set of s independent comparisons at significance level α, with their respective null hypotheses h <dig> h <dig>  ..., hs. let p <dig> ≤ p <dig> ≤...≤ ps be the ranked p-values associated to each test, and denote by hi the null hypothesis corresponding to pi. let k be the observed number of rejections after performing the s tests individually at level α. provided that the s nulls are true the expected number of rejections  is s × α. hence, the observed value k can be compared to the expectation in order to reach a conclusion about its significance; that is, to check whether the amount of significant tests could be explained by chance. the sgof metatest performs a goodness-of-fit  test of one degree of freedom comparing the observed  and the expected  numbers of rejections on the family of tests accomplished. this goodness-of-fit metatest is defined as an exact binomial test. however, when s ≥  <dig> it is approximated by a chi-squared or a g-test, both of them approximating a chi-squared distribution with one degree of freedom . let kα be the critical value, given s and α, for such metatest; that is, a rejection at level α occurs when k ≥ kα. thus, kα is the 1-α percentile of the binomial distribution . here, "rejection" means that at least one of the null hypotheses is false. more specifically, in the case of rejection, the sgof metatest concludes that the k-kα+ <dig> hypotheses with the smallest p-values  are false. clearly, this is a proper subset  of the initial set of k rejected hypotheses when performing the individual testing. as we state below, this procedure controls for fwer.

consider the following example of application . imagine that s =  <dig>  tests are performed and k =  <dig> are significant at α =  <dig> . in the case of the exact binomial test, the critical value corresponding to significance level α =  <dig>  and s =  <dig>  tests is kα =  <dig>  which corresponds to the 95% percentile of the binomial  distribution. therefore, if we have an observed value of k =  <dig> this means that the 600-536+ <dig> =  <dig> hypotheses with the smallest p-values will be considered significant.

note that, unlike the benjamini and hochberg fdr controlling procedure  <cit> , the proposed sgof test does not decide which hypotheses are false by comparing the attained p-values to some values of reference. rather, the question that is addressed by the sgof statistic is: are there too many rejections  with respect to the expected amount of them? how many among these rejections are not attributable to chance?

now we describe some basic properties of the sgof metatest. property  <dig> implies that sgof controls for the familywise error rate  in the weak sense. property  <dig> evaluates the per comparison error rate of sgof. the error rates in properties  <dig> and  <dig> are analyzed under the assumption that the s null hypotheses to be tested in a simultaneous way are true. finally, we investigate in property  <dig> the power of sgof to reject at least one hypothesis in the case that a portion of nulls is false.

property 1
sgof metatest controls for fwer in the weak sense, that is, under the intersection null hypothesis . this is an immediate consequence of its definition. note that fwer is the probability of committing one or more than one type i error. in our case, this is the probability of k ≥ kα, which, by definition of kα, is smaller than or equal to α. recall that fwer and fdr coincide when the s nulls are true  <cit> , so our method also controls directly for fdr in this situation.

property 2
the per comparison error rate  of the sgof test is

   

where i is the indicator of the event k ≥ kα, having value  <dig> if the assert is true and  <dig> otherwise, which can be evaluated from the null distribution of k. therefore, pcer reveals  the probability of committing a type i error for each individual hypothesis . for example, if the number of hypotheses  is  <dig> , and the significance level  is 5%, pcer approximately equals  <dig> × 10- <dig> . this is about ten times α/s, a fact that explains the higher power of sgof when compared with other fwer tests as the classical bonferroni one . by simulating several values of s, we have estimated that pcer ≈ 4α/s for s =  <dig>  ≈ 10α/s for s =  <dig>  ≈ 29α/s for s =  <dig>  and ≈92α/s for s =  <dig> . therefore, it seems that the sgof per test error rate is proportional to α/s by a factor that increases with the number of tests, s, resolving in this way the trade-off between type i error and statistical power. this means that the higher s the higher the probability that the metatest rejects each null hypothesis .

property 3
the probability that sgof rejects at least one null hypothesis steadily increases up to one as s increases, provided that a given portion of the null hypotheses remains false. to illustrate this property, assume that there is a proportion λ = s0/s  of true null hypotheses among the s being tested, and that the individual probability of rejection at level α of the 1-λ false hypotheses is α <dig> rather than α . then, the probability that sgof rejects one or more than one hypotheses is approximately given by the probability that a standard normal is less than the critical value

   

where zα is the th percentile of the standard normal. note that zβ is the  percentile and 1-β is the power of the test which in the case of sgof, means the power to reject that all nulls are true  i.e. to detect that at least one null is false. this critical value zβ approaches to infinity as s increases. therefore, the power to detect that at least one null is false increases with s. note that the factor α1-α controls the closeness of the alternative hypothesis to the null, so  the power of sgof decreases for close alternatives . as it is known to occur with other adjustment methods  <cit> .

simulations
simulations were run under two different scenarios: 1) the null model is always true , and 2) the alternative model is true in some of the s tests .

null model is always true
expected and detected numbers of false positives were compared under the simulation design . the results were similar for both one-sample t and g homogeneity tests . the mean percentage of false positives  obtained in the simulation was close to the theoretical expectation  in all cases. only the results for sb, bh and sgof are shown because the b method produced exactly the same values as sb. the mean and standard deviations of the detected significant cases are presented  for  <dig> replicates. clearly, the three methods showed rather similar type i errors. sgof had slightly higher variability through replicates but smaller across the set of cases simulated . in any case, all multitest methods maintained low levels of false rejection rates when the null hypothesis was true.

values are averages through  <dig>  replicates and their ± standard deviations. the simulated null models were tested under t or homogeneity g tests . n: sample size. s: number of tests. sα: percentage of significant tests. sb: sequential bonferroni. bh: benjamini and hochberg. sgof: sequential goodness of fit.

alternative models
we studied the ability of different multitest adjustments to detect significant cases when there is an increasing proportion of tests undergoing a true effect . different sample sizes were studied ranging from  <dig> to  <dig> for the one-sample t tests or from  <dig> to  <dig> for the homogeneity tests . the results were similar for both kinds of tests. again, the b method is not presented as it was nearly identical to the sb one. when the effect was weak  bh always shows an equal or higher mean statistical power than sb, although in both cases the detection of true discoveries is extremely poor. when the effect was strong , bh has high power only with the largest sample sizes. a quite important pattern can be followed from these tables. when the number s of tests increases then the power decreases for sb and bh but increases for sgof. the latter occurs as predicted from property  <dig> above.

weak: the alternative hypothesis is weak implying that data come from a normal. n: sample size. % effect: % of real true alternatives. s: number of tests. significant: % of significants before adjustment. detected: % of significant tests after adjustment. fdr: false discovery rate. sb: sequential bonferroni. bh: benjamini and hochberg. sgof: sequential goodness of fit.

weak: the alternative hypothesis is weak. n: sample size. % effect: % of real true alternatives. s: number of tests. significant: % of significants before adjustment. detected: % of significant tests after adjustment. fdr: false discovery rate. sb: sequential bonferroni. bh: benjamini and hochberg. sgof: sequential goodness of fit.

strong: the alternative hypothesis is strong implying that data come from a normal. n: sample size. % effect: % of real true alternatives. s: number of tests. significant: % of significants before adjustment. detected: % of significant tests after adjustment. fdr: false discovery rate. sb: sequential bonferroni. bh: benjamini and hochberg. sgof: sequential goodness of fit.

strong: the alternative hypothesis is strong.n: sample size. % effect: % of real true alternatives. s: number of tests. significant: % of significants before adjustment. detected: % of significant tests after adjustment. fdr: false discovery rate. sb: sequential bonferroni. bh: benjamini and hochberg. sgof: sequential goodness of fit.

to further study the effect of the number of tests onto the power we performed different sets of one-sample t tests, from  <dig> to  <dig>  tests , with 20% of them coming from the weak alternative. as it can be appreciated in the figure the power of sgof  increases with the number of tests while the power of sb and bh diminishes. this same pattern can be observed for any case in the tables  for  <dig>   <dig> and  <dig>  tests.

concerning the magnitude of the deviation from the null hypothesis, the closer the alternative is to the null, i.e. the weaker the effects , the higher is the fdr, and vice versa . sample size is also critical for controlling fdr. when sample size is small  fdr is not being controlled whatever the adjustment used. in the case of sb and bh it seems that the fdr decreases faster than with sgof but this is simply because under sb or bh there are almost no discoveries so that the margin for false ones is reduced. we further studied how sample size impacts onto fdr control. therefore, we performed simulations through a wide range of sample sizes, with a family of  <dig>  one-sample t-tests with 5% of them coming from the weak alternative and computed the % of false discoveries for the different adjustment methods . it can be appreciated that the effect of sample size is important, indeed with a sample size of  <dig> the fdr is almost one order of magnitude away from its nominal value under the bh method . under asymptotic conditions sb and sgof methods reach null rates and bh reaches the nominal value q . for sample sizes comprised between  <dig> and  <dig>  sgof shows higher fdr simply because it is the only one that detects some true discoveries .

in summary, sgof appears to outperform sb and bh when the effects were weak  and affected a high proportion of tests . in fact, bh only behaves slightly better than sgof under the stronger effect and large sample size in the one-sample t tests . in the case of the g test with the stronger effect and larger sample size, both methods behave similarly with slight advantage for sgof . therefore, in tables  <dig> and  <dig>  mean percentages of discoveries are one order of magnitude larger in sgof but false discoveries are in the worst cases only twice or three times higher. the standard deviation through replicates  is, in general, slightly higher for sgof. under the most favorable conditions, sgof shows statistical power  <dig> orders of magnitude higher than the others. for example, under the g tests with s =  <dig>  n =  <dig> and 20% of the comparisons,i.e.  <dig> tests having a true but weak effect, bh identifies  <dig> ±  <dig> discoveries while sgof detects  <dig> ±  <dig> . notably both methods give almost the same percentage of false discovery rate .

to further study how the efficiency of the different methods depends on the percentage of tests in which the alternative is true  we simulated, for both kinds of tests, a case with high number of tests, s =  <dig> , through a wide range of % of effects . the left panel of both figures shows the absolute number of true discoveries, that is, rejections of the null when is false, for the three multitest adjustments studied. the right panel shows the false discoveries, i.e. rejections of the null when it is true. noticeably, for the weak alternatives case, only sgof has power to identify true discoveries. concerning the sgof false discoveries, the fdr is higher under the one-sample t test  than under the g test , maybe due to the smaller sample size of the first . as expected from the term  in equation  the power increases with the percentage of tests having a true effect . for the strong alternatives, bh performed only slightly worse than sgof in the case of one-sample t tests, although sgof performed still better in the case of g tests.

example of application
martínez-fernández et al.  <cit>  accomplished a proteomic study in which they performed  <dig> tests. of these,  <dig> were statistically significant . after correction with the bh method they did not get any significant case. however, using a g-test they rejected at a  <dig> % significance level that the  <dig> significant tests could be explained by chance. if we apply the sgof test on this data set , we find  <dig> significants. notice that the same significance level was used in the family of tests and in the two multitest methods discussed above. thus, all the  <dig> extra significant cases could be hardly considered as type i errors, suggesting that they could be considered candidate genes for future detailed biochemical studies.

implementation of the method
we provide a computer program which allows to obtain the multitest adjustment probability methods used in this work . sgof is calculated by an exact binomial test when the number of tests is lower than  <dig> and by a g test with the williams' correction in any other case. in addition, a more conservative sgof adjustment using the yate's correction is also given.

discussion
the use of controlling fdr based methods for multitest adjustment has implied an obvious improvement by increasing the statistical power in families of comparisons  <cit> . however, such an improvement is far from being useful for experimentalists under all the circumstances, in particular when a relatively small sample size and a high number of comparisons are involved. in such conditions, classical multitest adjustments are known to have low statistical power  <cit> . in fact, a great number of controlling fdr based methods have been proposed trying to further improve its applicability although with moderate results  <cit> . here we suggest a completely different approach, by using a sequential goodness of fit on the set of comparisons, which may help in some of the circumstances where the fdr-based approaches fail to find true discoveries. similar to bonferroni techniques, the sgof metatest controls for fwer . given a number s of tests, in bonferroni technique the error rate per comparison is fixed to α/s. therefore, this value diminishes as the number s of tests is higher. the problem is that the power to detect true discoveries also depends on this error rate. with a very stringent significance level we will have very low power. importantly, in the case of sgof, the per test error rate is proportional to α/s by a factor that increases with the number of tests resolving in this way the trade-off between type i error and statistical power. therefore, the power increases with the number of tests though the family wise error rate is being controlled to avoid a high false discovery rate. as far as we know, there are not other multitest adjustment methods with this desirable property of increasing power with the number of tests. however, as can be expected from equation  and can be seen in figure  <dig> such increase is not lineal. therefore, it could be suggested that increasing the number of tests up to  <dig> or  <dig>  will increase considerably the statistical power of the sgof adjustment, but above  <dig>  the increase will become slighter . this suggests that using more than  <dig>  comparisons could not offer a clear advantage.

another issue concerning the statistical power of sgof seems to be the percentage of tests in which the alternative is true  which has a clear impact onto the discoveries rates. in the case of sb and bh this impact is more difficult to follow from the tables because there is a trade-off with the increasing number of tests . however with sgof the effect is very clear because, as is expected from equation , both the % of effects  and the number s of tests increase the power.

obviously, because sgof does not perform so stringent control neither on the per test error rate nor in the fdr, this implies that fdr is being allowed to be higher than with sb and bh methods. however, given a number k of observed significants, as power increases, fdr is expected to diminish because in such a case the proportion of true discoveries approaches  <dig>  therefore, sgof will attain an indirect control of fdr with large numbers of tests and/or effects involved. that is, sgof will behave especially well compared to the classical methods when the alternative is weak and both the number of effects through the family of tests and the number of tests involved are high. in this case, sgof can be up to two orders of magnitude more powerful than the other methods, maintaining at the same time acceptable fdr values.

we have also observed that if the p-values are not correctly calculated the fdr will be uncontrolled as occurs with any other multitest adjustment method. this is noteworthy because empirical studies do not usually involve large sample sizes within each test. known multitest adjustment methods can have very good asymptotic statistical properties. in fact, both sb and bh have very good power with the kind of tests assayed when the sample size is as large as  <dig> . the problem is that empirical science does not work on the asymptotic arena but on finite sample size. as we have seen, the assumption of controlled fdr fails when sample size is small, at least under one-sample t and homogeneity g-tests. additionally, the classical adjustment methods  have low power when the number of tests is high and/or the effects are weak. indeed in these conditions, sgof should be considered as an interesting method to detect that some kind of true effect exists though we are not confident in that all detected positives are true discoveries. in addition, some uncertainty exists when significant probabilities have exactly the same values. for example, if  <dig> out of  <dig> comparisons have a p-value below  <dig> , say  <dig> , sgof will show that  <dig> can not be explained by chance, but the researcher has no way to choose among the  <dig>  on the other side alternative multitest adjustment methods  cannot find any significant case. nevertheless, from an experimentalist point of view, it will be more useful to know that at least  <dig> hypotheses deserve more detailed studies than to just ignore all of them. in cases like this, under the sgof method, the  <dig> significant tests will be chosen randomly from the  <dig> available.

concerning statistical properties as conservativeness, sensitivity and specificity we have computed the degree of conservativeness  <cit>  and performed roc analysis  <cit>  for the same cases as in figure  <dig> . the results just confirm the good properties of sgof as already expected from the higher per comparison error rates  and the true and false discoveries numbers .

another important topic concerning multiple hypothesis testing efforts applied to high-throughput experiments is the intrinsic inter-dependency in gene effects. we would like to note that correlation can have important effect onto fdr-based adjustment methods  <cit> . however, it is usually considered a kind of dependence in gene effects called weak-dependence which corresponds to local effects between a small number of genes  <cit> . it has been shown that under the assumption of the so-called weak-dependence, the fdr-based methods are still useful provided that the number of tests is large enough  <cit> . sgof does not consider the p-values individually but the proportion of significant ones and this should make it more robust to dependence issues. therefore, we expect at least the same or better performance for sgof than for fdr-based methods when considering gene dependencies. our preliminary results  indicate that dependence has no effect onto sgof power provided that the blocks with correlated genes are small. indeed with blocks as large as  <dig> genes and correlation as high as  <dig>  the loss in power is small. furthermore, short blocks of correlated genes is what is expected in genome and proteome wide studies  <cit> . additionally, we have observed that if the blocks are short the magnitude of the correlation has a minor effect. nevertheless, we think that such topic deserves further study.

finally, we note that we have obtained p-values via simulation from two kind of tests, one-sample t-test which is widely used, and also via homogeneity tests, that are also frequently involved in multiple comparisons  <cit> . in addition, sgof should be of general utility under other families of multiple comparisons, although this should deserve further investigation. the failure of classical multitest adjustments to deal with a huge number of tests  has been considered as a key problem in many omic technologies  <cit> , and so sgof comes to contribute to a well-known need.

CONCLUSIONS
we propose a new multitest adjustment, based on a sequential goodness of fit metatest  which, contrary to other methods, increases its statistical power with the number of tests. therefore, sgof should become an interesting strategy for multitest adjustment when working with high-dimensional biological data. the sgof metatest, jointly with b, sb and bh multitest adjustments, can be easily computed with the software provided.

