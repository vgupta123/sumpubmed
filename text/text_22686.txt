BACKGROUND
the advent of new generation sequencing  technologies opened a new era in the field of dna sequencing, providing researchers with powerful instruments able to produce millions of short  reads per single run. this technology breakthrough poses considerable computational challenges since the sequenced fragments need to be quickly aligned--usually admitting errors--against genomes whose size is often of the order of giga-bases. from the algorithmic point of view, the problem of indexing texts to support pattern matching in the big-data domain is receiving significant attention, also due to the recent computational breakthroughs in the fields of succinct and compressed text indexes . even though these important results solved most of the problems related to exact pattern matching, the problem of indexing a text with a succinct--or compressed--data structure that supports inexact pattern matching, still represents a considerable challenge. due to the fact that bwt indexes are natively designed for exact pattern matching, in practice this problem is solved by splitting the pattern in fragments , searching for all possible variants of the modified pattern , or mixing the two strategies . soap <dig>  <cit>  adopts the first strategy, splitting the pattern in k +  <dig> blocks, that is admitting at most k errors, and searching for exact occurrences of the blocks. the strategy is then completed by accelerating the search using a hash table to pre-compute backward search results on the bwt reference index. bowtie  <cit>  adopts a backtracking strategy on the fm index, inserting a mismatch in correspondence to low-quality bases during backward search. bwa  <cit>  adopts a backtracking strategy on a bwt-based index that allows to  retrieve the occurrences of the pattern once an upper bound to the number of admitted mismatches is fixed. erne  <cit>  implements a hybrid strategy, splitting the pattern in t blocks and computing, for each of them, its hash value. the particular class of hash functions employed  allows to compute efficiently fingerprints of blocks at hamming distance at most k/t from the original block. these fingerprints are then finally searched in the hash index. we mention also a similar hybrid strategy, based on perfect hamming codes, adopted in  <cit> .

one of the strengths of bwt-based tools is their reduced space requirement. the space for the data structure is often close to that required by the reference string . as an example, bowtie requires only  <dig> gb of ram to index the human genome. on the other hand, hash-based tools such as erne or soap  <cit>  require much more memory to store their indexes, due to the fact that they need to explicitly memorize pointers to the reference. erne and soap require 19gb and 14gb of ram to index the human genome, respectively.

from a theoretical point of view, indexing for approximate string matching is still at an early research stage: even the most efficient solutions are, in practice, far from being usable. as a matter of fact, the most advanced results to date are able to guarantee efficiency on either space or speed. not both at the same time. letting m and k be the query length and the maximum number of allowed errors, respectively, simple backtracking strategies have a complexity that rapidly blows up with a factor of mk: impractical for searching with a reasonable amount of errors on  long patterns. other solutions improve query time by trading on space requirements: letting n be the text length, the index of cole et al. in  <cit>  solves the problem in time  ok log log n + m + occ) and space  ok+1) bits, which in practice is too much even for small pattern lengths and number of errors. the solution of chan et al. presented in  <cit>  improves on space consumption requiring  o bits, but query time increases to  ok log log n), which exponentially blows up with the square of the number of errors.

we tackled the problem with a hash-based randomized algorithm that is able to reach expected fast performances and requires linear space. the two goals are obtained providing a hash function belonging to two particular classes of hash functions at the same time. the two classes are hamming-aware and de bruijn hash functions, respectively. functions in the former class allow to "squeeze" the hamming sphere of radius k centered at the query pattern p, to a hamming sphere of radius  o centered at the hash value of the query. functions in the second class  are homomorphisms on de bruijn graphs. we show that their corresponding hash indexes can be represented in linear space introducing only a small slowdown of  o in the lookup operation.

we call db-hash the resulting succinct hash data structure.

letting n and m be the sizes of text and pattern, respectively, our algorithm reaches an expected time complexity of  oklog m + m) while requiring  o bits  of space for the index only. our result extends the strategy presented in  <cit> , substituting a standard hash index with the new proposed db-hash data structure and allowing us to improve on both time and space complexities with respect to  <cit> . under the hypothesis that the indexed text is perturbed by random noise , our algorithm improves upon theoretical linear-space upper-bounds discussed in the literature .

the resulting algorithm has been implemented in the short-reads aligner bw-erne, the natural adaptation of the hash-based erne aligner  to the db-hash data structure. aim of this paper is to compare the performances of bw-erne with those of state-of-the-art short-read aligners. bw-erne uses a succinct db-hash index based on the burrows-wheeler transform coupled with wavelet tree and not using any kind of compression . moreover, the implementation takes into account biological information--such as base quality values--to improve performances without any significant loss in sensitivity.

experimental results on the vitis vinifera and human genomes show that the db-hash requires from  <dig> to  <dig> times less space than the standard hash used by erne . tests run on both simulated and real reads show, in addition, that bw-erne maintains the same sensitivity of erne, improving also its throughput if reliable base qualities are available.

methods
we begin to illustrate our general strategy, showing how to represent a hash index in succinct space by building a compact  bits) representation of all the fingerprints of length-m text substrings. a succinct index is then built over this representation, obtaining a succinct hash data structure.

definitions
throughout this paper we will work with the alphabet Σdn a = {a, c, g, t, n, $}  which, in practice, will be encoded in Σdn a' = { <dig>   <dig>   <dig>  3} assigning a numerical value in Σdn a' to n and $ characters. the size of our alphabet is, therefore, σ = |Σdn a'| =  <dig>  while with n, m, and w we will denote the reference length, the pattern length, and the  size of a computer memory-word , respectively. as hash functions we will use functions of the form h : Σm → Σw mapping length-m Σ-strings to length-w Σ-strings. if necessary, we will use the symbol  wmh instead of h when we need to be clear on h's domain and codomain sizes. given a string p∈∑m, the value h∈∑m will be also dubbed the fingerprint of p . with t∈∑n we will denote the text that we want to index using our data structure. tij will denote t, i.e. the length-j prefix of the i-th suffix of t. a hash data structure h for the text t with hash function h, will be a set of ordered pairs  such that h={htim,i:0≤i≤n-m}, that can be used to store and retrieve the positions of length-m substrings of t . a lookup operation on the hash h given the fingerprint h, will consist in the retrieval of all the positions 0≤i<n such that h,i∈h and cases where h,i∈h but tim≠p will be referred to as false positives.

the symbol ⊕ represents the exclusive or  bitwise operator. a ⊕ b where a,b∈∑, will indicate the bitwise xor among the bits of the binary encoding of a and b. analogously, x ⊕ y, where x,y∈∑m will indicate the bitwise xor operation among the bits of the binary encoding of the two words x and y and v will denote the bitwise or operator. dh is the hamming distance between x,y∈∑m. we point out that the hamming distance is computed between characters in Σ, and not between the bits of the binary encoding of each of them. patterns and fingerprints are viewed as bit vectors only when computing bitwise operations such as or, and, and xor.

succinct representation of hash indexes
we begin by introducing the technique allowing succinct representation of hash indexes. the central property of the class of hash functions we are going to use is given by the following definition:

definition  <dig> let Σ = { <dig> ..., |Σ| −1}. we say that a function h : Σm → Σw is a de bruijn hash function if and only if, for every pair of strings p, q ∈ Σm

 p1m-1=q0m-1⇒h1w-1=h0w- <dig> 

with the following theorem we introduce the hash function used in the rest of our work and in the implementation of our structure:

theorem  <dig> let p ∈ Σm. the hash function h⊕ : Σm → Σw w ≤ m defined as

 h⊕=⊕i=0m/w-2piww⊕pm-ww 

is a de bruijn hash function.

a detailed proof of this theorem is given in additional file  <dig>  given a de bruijn hash function wmh:∑m→∑w we can "extend" it to another de bruijn hash function n-m+wnh: ∑n→∑n-m+w, operating on input strings of length n greater than or equal to m, as follows:

definition  <dig> given wmh:∑m→∑w de bruijn hash function and n ≥ m, the hash value of n-m+wnh on t ∈ Σn, is the unique string n-m+wnh∈∑n-m+w such that:

 n-m+wnhiw=wmh, 

for every  <dig> ≤ i ≤ n − m.

it is easy to show that a function enjoying the property in definition  <dig> is a homomorphism on de bruijn graphs . since wmh univocally determines n-m+wnh and the two functions coincide on the common part Σm of their domain, in what follows we will simply use the symbol h to indicate both.

from definitions  <dig> and  <dig> we can immediately derive the following important property:

lemma  <dig> if h is a de bruijn hash function, n ≥ m, and p ∈ Σm occurs in t ∈ Σn at position i, then h occurs in h at position i. the opposite implication does not  hold; we will refer to cases of the latter kind as false positives.

on the ground of lemma  <dig> we can propose, differently from standard approaches in the literature, to build an index over the hash value of the text, instead of building it over the text. this can be done while preserving our ability to locate substrings in the text, since we can simply turn our task into that of locating fingerprints in the hash of the text t. we call db-hash the data structure obtained with this technique. notice that the underlying hash data structure is simulated by searching the occurrences of h in h during a lookup operation, so the algorithm is transparent to the particular indexing technique used.

search algorithm
the core of our searching procedure is based on the algorithm rna , a hash-based randomized numerical aligner based on the concept of hamming-aware hash functions . hamming-aware hash functions are particular hash functions capable to "squeeze" the hamming ball of radius k around a pattern p to a hamming ball of the same radius around the hash value of p. this feature allows to search the entire hamming ball around p much more efficiently. the following theorem holds:

theorem  <dig> the de bruijn function h⊕ defined in definition  <dig> is a hamming aware hash function. in particular:

 dh≤k⇒dh,h⊕)≤2k 

for every p,p′∈∑dna′m

see additional file  <dig> for a detailed proof of this theorem. since h⊕ is a de bruijn and hamming aware hash function, we can use it to build our structure and adapt the rna algorithm to it. we call db-rna the new version of the rna algorithm adapted to the db hash data structure. more in detail, the hamming-awareness property of h⊕ guarantees that, given a pattern p to be searched in the index, the set {h⊕ : dh ≤ k} is small-- ok wk) =  o elements in our application--and can be computed in time proportional to its size. notice that, with a generic hash function h, only the trivial upper bound  ok mk) can be given to the size of this set since each different p' such that dh ≤ k could give rise to a distinct fingerprint. our proposed algorithm is almost the same as the one described in  <cit> , the only difference being that the underlying data structure is a db-hash instead of a standard hash. briefly, the search proceeds in  <dig> steps. for each pattern p:

 <dig> h⊕ is computed;

 <dig> the index is searched for each element in the set {h:dh≤k},

 <dig> for each occurrence found, the text and the pattern p are compared to determine hamming distance and discard false positives.

in practice, in our implementation we also split the pattern p in non-overlapping blocks before searching the index. with this strategy we reduce the maximum number of errors to be searched, improving the speed of the tool.

complexity analysis
let occ be the number of occurrences with at most k errors of the searched pattern p in t. assuming that the alphabet size σ is a power of  <dig> , the expected complexity on uniformly distributed inputs of our algorithm has an upper bound of

 okklogm+⋅m), 

here, σ =  <dig> is the size of the alphabet Σdn a'. a fully formal proof of  this analysis can be found in  <cit> .

quality-aware strategy
our tool implements a quality-aware heuristic that significantly improves search speed, at the price of a small loss in sensitivity. briefly, we use base qualities to pick up only a small fraction of the elements from the hamming ball centred on the hash h of the searched block b, following the assumption that a high quality base is unlikely to be a miscall. since in practice we divide the read in non-overlapping blocks and the heuristic affects only the searched block, with this strategy we lose only a small fraction of single variants like snps. more in detail, let q∈ℕm be  the phred quality  string associated to the searched block b. we compute a hash value on q using the following hash function:

definition  <dig> with h∨:ℕm→{ <dig> }w we indicate the hash function defined as

 h∨=∨i=0m/w-2fq∨fq 

where fq:ℕw→{ <dig> }w is defined as

 fq=0ifq>q3otherwise,i= <dig> ...,w- <dig> 

q is a quality threshold . the values  <dig> and  <dig> have been chosen due to their binary representation . if h∨= <dig>  then during search we try to insert an error in position i of h⊕  since at least one of the bases used to compute h⊕  has a low-quality. the quality-aware strategy is then implemented as follows: let b be the block to be searched and q its associated phred quality string. a fingerprint f  is searched in the structure if and only if )∨h∨=h∨, i.e. if f differs from h⊕ only in positions corresponding to low quality bases. since the number of low-quality base pairs in a read is typically low, this strategy allows to drastically reduce search space  if reliable qualities are available. in the results section we show, moreover, that this strategy has only a negligible impact on snp detection and on the overall precision of our tool.

bw-erne: implementation details
we implemented our algorithm and data structure in the short reads aligner bw-erne , downloadable at http://erne.sourceforge.net. as hash function for our index we use h⊕. given a text t∈∑dnan, we calculate h⊕ bw t--the burrows-wheeler transform of h⊕--adding the necessary additional structures needed to perform backward search and to retrieve text positions from h⊕ bw t positions. bw-erne includes  also a simple and fast strategy to allow a single indel in the alignment. this strategy does not affect running times and permits to correctly align a large fraction of short reads that come with indels . it is well known that dna is extremely difficult to compress and for this reason we choose not to introduce compression in our structure. even if our index is not compressed, experiments show  that its memory requirements are similar or even smaller than those of other tools based on the fm index such as bowtie  <cit> , bwa  <cit>  and soap <dig>  <cit> . briefly, the structure is composed by three parts: the index, the plain text, and an auxiliary  hash.

bwt index
the bwt index is constituted by h⊕bwt stored as a wavelet tree , rank counters  bits), and sampled suffix array pointers for its navigation ) bits for one rank structure and 2n bits for one sa pointer every  <dig> text positions; the user can however modify the sa pointers density).

plain text
t∈∑dnan is stored in a 3-bits per base format in blocks of  <dig> symbols . we exploit this encoding to perform  o text-query comparison of a single block, improving the speed of the algorithm.

auxiliary hash
to speed up lookup operations, we finally store an auxiliary hash hau x that indexes the waux most significant digits of the fingerprints: the intervals obtained by backward search on all the numbers in the set { <dig> ...,σwaux-1} are precomputed and stored in hau x. in this way, a lookup operation on hbw t requires one lookup in hau x followed by w − waux steps of backward search. we require hau x to occupy only n bits. this limit gives us an upper bound for waux of logσn-logσlogn. it can be proved  <cit>  that the optimal word size for our algorithm is w=logσ. combining these results it follows that the cost of a lookup operation in our data structure is  o.

summing up, the total space occupancy of the db-hash data structure implemented in bw-erne is of 2nlogσ+4n+o bits, corresponding in practice to approximately  <dig> n bytes : the index is succinct.

RESULTS
in order to assess the performances of bw-erne, we performed extensive experiments on two genomes: vitis vinifera  and human genome .

simulations on vitis vinifera were used to compare the alignment accuracy and speed of bw-erne, its  standard-hash counterpart erne, bowtie, bwa, and soap <dig>  in presence of reliable base qualities on a medium-sized genome. hence, in order to precisely asses the correctness of our results, we used simulated data produced by the simseq simulator, https://github.com/jstjohn/simseq. experiments on real data  confirm the conclusions.

experiments on the human genome, instead, were performed in order to assess the precision of bw-erne in absence of base quality information , to assess the impact of bw-erne's quality-aware strategy on snps detection , and to assess the performances of our aligner on a real 10x coverage illumina library downloaded from the 1000genomes project's database .

all experiments were performed on a intel core i <dig> machine with  <dig> gb of ram running ubuntu  <dig>  operating system. see additional file  <dig> and additional file  <dig> for further informations about the implementation usage and the commands used to perform the experiments, respectively.

memory footprint of the indexes
bw-erne significantly reduces the space requirements of erne, requiring approximately  <dig>  times less space on the vitis vinifera genome  and approximately  <dig>  times less space on the human genome . the plot in figure  <dig> compares the different indexes sizes on vitis vinifera and gives a clear idea of the most important difference between full-text and succinct indexes, the former requiring space proportional to n log n  while the latter is using an amount of space close to that necessary for the plain text. more-over, differences among bwt-based tools are minimal  if compared to the gap between hash-based  and bwt-based  aligners. among the tested tools only bowtie requires less space than our tool, even if in the db-hash data structure we do not perform any data compression. this is due to the fact that dna is a high-entropy string and, therefore, almost incompressible using standard techniques such as the ones implemented in the fm index. finally, the horizontal green line, marking the size of the reference fasta file, gives an idea of how efficiently can succinct and compressed indexes represent their structures: in particular, bowtie is able to reach a ram memory footprint that is smaller than that of the reference file itself, while bwa and bw-erne require slightly more space. on the human genome, bw-erne index requires only  <dig>  gb of space to be stored and loaded during alignment. this is slightly more than the indexes of bowtie and bwa , but considerably less than erne's hash table, which requires  <dig> gb of space to be stored and loaded during alignment.

simulated data with reliable base qualities
in this experiment we compared the tools on 5m of 100bp single-end reads with simulated base qualities. this dataset was generated from the vitis vinifera genome using the simseq simulator, adopting the built-in illumina error model. the number of correctly mapped reads was estimated comparing the bam files generated by simseq and the aligners .

the plots in figure  <dig> show that bw-erne is able to exploit at best quality information without losing accuracy with respect to erne while, at the same time, improving significantly its performances. bwerne was executed with default settings and using  <dig> thread. the plot on the left hand side of figure  <dig> shows that bw-erne was  <dig> times faster than bowtie, and  <dig> times faster than bwa. this speed came with no penalties on the number of mapped reads, which was the highest among all tools, with bw-erne and erne aligning 97% of all reads, bwa 94%, bowtie 90% and soap <dig> 82%. finally, the plot on the right hand side of figure  <dig> shows the accuracy of the tools in terms of correctly mapped reads. the gap between mapped and correctly mapped reads is due to reads mapping in multiple locations, which were judged on the base of the unique reported alignment. the plot shows that erne and bw-erne were the most accurate tools, correctly aligning the highest number of reads.

simulated data without reliable base qualitie
a public gcat experiment  was performed in order to assess the precision of our tool in absence of reliable base qualities. the experiment consisted of 12m 100bp single-end reads with  <dig> % small indel frequency. the results are available at the address http://www.bioplanet.com/gcat/reports/3705-muwwfqmbjb/alignment/100bp-se-small-indel/bw-erne/compare-26-27- <dig>  in this experiment, bwerne was executed with the option --sensitive, which ignores base qualities , and using  <dig> threads. bw-erne completed the alignment in  <dig> hour and  <dig> minutes, for an overall throughput of 8m reads per hour . the results are reported in table 1: bw-erne was one of the most accurate aligners, correctly aligning  <dig> % of all the reads. this fraction is comparable to that of novoalign and bwa-mem, and higher than that of bowtie <dig> and bwa.

bw-erne ranks among the most precise tools, correctly aligning a number of reads comparable to that of slower aligners such as novoalign.

validation of the quality-aware strategy in presence of snps
in order to assess the impact of our quality-aware strategy on snp detection, we simulated  10m of 100bp single-end reads, using as reference the human genome . after the simulation, each base was randomly substituted with probability  <dig>  to simulate snps . this strategy allowed us to track reads containing snps, permitting a separate verification of the alignment's correctness for reads with and without this kind of mutations. bw-erne was executed twice on the mutated dataset: with default settings  and with the --sensitive option enabled . an alignment was considered correct if and only if both chromosome and strand coincided with those outputted by simseq and if the alignment's position was within  <dig> bases from the position outputted by simseq . reads with multiple alignments were judged on the basis of their unique reported alignment. of the 10m simulated reads,  <dig> % contained at least one snp. bwerne in sensitive mode  correctly aligned  <dig> % of the reads without snps and  <dig> % of the reads with snps, thus showing  no significant bias towards reads without snps . bw-erne with the quality-aware strategy enabled correctly aligned  <dig> % of the reads without snps and  <dig> % of the reads with snps, thus showing only a slight bias towards reads without snps.

real data -- human genome
to conclude, the experiment on a real high-coverage human illumina library, allowed us to validate the results obtained on simulated reads and to assess the performances of bw-erne on large datasets. in this experiment we aligned 320m of 100bp pairedend reads, corresponding to a 10x coverage of the human genome, downloaded from the 1000genomes project's database . bw-erne was executed with default settings  and using  <dig> threads. our tool completed the alignment in  <dig> hours and  <dig> minutes, for an overall throughput of  <dig> millions of reads per hour.  <dig> reads  were automatically discarded by the builtin trimmer due to low base quality. of the remaining  <dig> reads,  <dig>  were successfully aligned to the reference and  <dig>  were not found. among the aligned reads,  <dig>  were aligned in only one position and  <dig>  in multiple positions.

CONCLUSIONS
in this paper we presented a new technique that permits a succinct representation of hash indexes using hash functions with the property of being hamming-aware and homomorphisms on de bruijn graphs. we used this technique to build a succinct index--dubbed db-hash--which, combined with a previously published hash-based algorithm, allowed us to lower the upper bound to the average-case complexity of the k-mismatch problem in succinct space. we implemented our algorithm and data structure in the short-reads aligner bw-erne. tests on both simulated and real data, using the most popular short reads aligners, allowed us to validate also in practice the efficiency of our algorithm, which proved to be extremely accurate and fast, especially if reliable base qualities are available.

we are exploring numerous extensions of the work discussed here, on both the theoretical and practical side. from the theoretical point of view, we are studying ways to extend our complexity results to a more general analysis of hashing, which could turn out useful in the complexity analysis of hash-based algorithms. other theoretical extensions of our work include the study of the properties of h⊕ as a text transform, randomizing the text and to be used in combination with existing pattern-matching algorithms. from the practical point of view, we are extending our bw-erne aligner with several new features such as long-reads alignment  and bisulfite-treated reads alignment .

availability
erne  is a short string alignment package whose goal is to provide an all-inclusive set of tools to handle short reads. erne comprises: erne-map, erne-dmap, erne-filter, erne-visual, erne-bs <dig>  and erne-meth. erne is free software and distributed with an open source license  and can be downloaded at: http://erne.sourceforge.net

competing interests
the authors declare that they have no competing interests.

authors' contributions
both authors equally contributed to the idea and to the design of the algorithm and the experiments. np developed the tool and performed the experiments. both authors wrote the paper.

supplementary material
additional file 1
proofs of theorems file: additional file  <dig> pdf

click here for file

 additional file 2
implementation usage file: additional file  <dig> pdf

click here for file

 additional file 3
commands used to perform the experiments file: additional file  <dig> pdf

click here for file

 declarations
the publication costs for this article were supported by the italian epigen flagship project.

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2015: proceedings of the italian society of bioinformatics : annual meeting 2014: bioinformatics. the full contents of the supplement are available online at http://www.biomedcentral.com/bmcbioinformatics/supplements/16/s <dig> 
