BACKGROUND
rank aggregation has a long history with its roots tracing back to the voting theory of the 18th century. the borda count is perhaps the most famous such method where elements in the overall list are ordered according to the average rank computed from the ranks in all individual lists. other rank aggregation schemes have been proposed over the years and they differ greatly in both the underlying philosophy, as well as mathematical complexity.

two radically different philosophies on rank aggregation exist. the first one is based on the majoritarian principles and attempts to accommodate the "majority" of individual preferences putting less or no weight on the relatively infrequent ones. the final aggregate ranking is usually based on the number of pairwise wins between items within individual lists. if item "a" is ranked higher than item "b" more often than not, then item "a" should also be ranked higher than item "b" in the overall list. any method that satisfies this condition, known as the condorcet criterion, is called the condorcet method. the second philosophical approach to rank aggregation seeks the consensus among individual ordered lists and is usually based on some form of rank averaging. the borda count is a good representative of this category. it is possible that the two approaches will produce different aggregated lists if applied to the same problem.

conceptually, rank aggregation techniques range from quite simple  to fairly complex which may employ advanced computational methodologies to find a solution. simple solutions are not necessarily desirable as they usually rely on "ad hoc" principles and lack any formal justification. mathematical rigor brings certain satisfaction and "security" at the expense of increased complexity and intensive computation.

rank aggregation methods have a lot of potential in the field of bioinformatics. ordered lists are routinely produced by today's high-throughput techniques which naturally lend themselves to a meta-analysis through rank aggregation.  <cit>  proposed to use rank aggregation methods to integrate the results of several microarray studies ,  <cit>  suggested aggregation of mirna targets predicted by three popular software packages and  <cit>  used rank aggregation to order clustering algorithms evaluated by several validation measures. the list can easily be extended to other potential applications, in particular, in proteomics for the purpose of integrating biomarkers from different studies or in the context of clustering analysis where the unknown number of clusters, instead of the "best" algorithm, needs to be determined. in this paper, we present an r rankaggreg package available through cran  which provides two different algorithms for rank aggregation: the cross-entropy monte carlo algorithm   <cit>  and the genetic algorithm   <cit> . both methods are available through the main function rankaggreg. in addition, a brute force algorithm is also provided through the bruteaggreg function which simply tries all possible solutions and selects the one which is optimal. what is meant by "optimal" and how to find the "optimal" solution will be the discussion of the methods section.

implementation
rank aggregation as an optimization problem
to cast the rank aggregation in the framework of an optimization problem, we first need to define the objective function. in this context, we would like to find a "super"-list which would be as "close" as possible to all individual ordered lists simultaneously. this is a natural requirement and the objective function, at least in its most abstract form, is very simple and intuitive

 Φ=∑i=1mwid, 

where δ is a proposed ordered list of length k = |li|, wi is the importance weight associated with list li, d is a distance function which will be discussed in details below, and li is the ith ordered list  <cit> .

the idea is to find δ* which would minimize the total distance between δ* and li's

 δ*=arg⁡min⁡∑i=1mwid. 

selecting the appropriate distance function d to measure the "distance" between ordered lists is very important. though many choices for a distance function can be found in the literature, we concentrate on the two most popular ones: spearman footrule distance and kendall's tau distance. the two distances usually produce slightly different aggregated lists which is mainly due to the differences in the two philosophical paradigms discussed in the background section.

spearman footrule distance
before defining the two distance measures, let us introduce some necessary notations. let mi,..., mi be the scores associated with the ordered list li, where mi is the best  score, mi is the second best, and so on. let rli  be the rank of a in the list li  if a is within the top k, and be equal to k +  <dig>  otherwise; rδ  is defined likewise. the spearman's footrule distance between li and any ordered list δ can be defined as

 s=∑t∈li∪δ|rδ−rli|. 

it is nothing more than the summation of the absolute differences between the ranks of all unique elements from both ordered lists combined. it is rather a very intuitive metric for comparing two ordered lists of arbitrary length. the smaller the value of the metric, the more similar the lists. for spearman's footrule distance, the maximum value when comparing two top-k lists is k. it is attained when the two lists have no elements in common.

the appeal of the spearman footrule distance comes from its simplicity and it is adequate in many situations when the only information available about the individual lists is the rank order of their elements. in a case when additional information which was used to rank the lists in the first place is available, it would be beneficial and prudent to incorporate this information into our aggregation scheme  <cit> .

thus, we define the weighted spearman's footrule distance between li and any ordered list δ which makes use of the quantitative information available in many cases. it is given by this weighted sum representation

 ws=∑t∈li∪δ|m)−m)|×|rδ−rli|. 

one can intuitively think of ws in terms of sum of penalties for moving an arbitrary element of the list li, t, from the position rδ  to another position rli within the list  adjusted by the difference in scores between the two positions .

kendall's tau distance
the kendall's tau distance takes a different approach at measuring the distance between two ordered lists. it utilizes pairs of elements from the union of two lists and is defined

 k=∑t,u∈li∪δktup, 

where

 ktup={0if rδ<rδ,rli<rli or rδ>rδ,rli>rli1if rδ>rδ,rli<rli or rδ<rδ,rli>rlipif rδ=rδ=k+1 or rli=rli=k+ <dig>  

here, p ∈  <cit>  is a parameter that needs to be specified for kendall's tau. if p is set to  <dig>  the maximum value that the distance can achieve is k <dig> and this happens when the intersection of the two lists compared is an empty set. intuitively, kendall's tau can be thought about in the following way. if the two elements t and u have the same ordering in both lists, then no penalty is incurred . if the element t precedes u in the first list and u precedes t in the second list, then a penalty of  <dig> is imposed . a case when both t and u do not appear in either one of the lists  can be handled by selecting p on a spectrum ranging from very liberal  to very conservative . that is, if we have no knowledge of the relative position of t and u in one of the lists, we have several choices in the matter. we can either impose no penalty , full penalty , or a partial penalty . the following three choices are common:  <dig>   <dig>  and  <dig> . it is a matter of a philosophical taste as to which option one chooses. we use p =  <dig> in the internal kendall function of the package.

somewhat analogously to the weighted spearman distance, the weighted kendall's tau is defined by

 wk=∑t,u∈li∪δ|m)−m)|× ktup, 

in which the penalty imposed is adjusted by the absolute difference in the scores for elements t and u. here, ktup is defined identically as above.

normalization of scores from each list li before computing ws and wk is necessary. the weights must be comparable otherwise disproportionately large or small weights can benefit a particular list and pull the "optimal" list δ* towards it. a number of normalization schemes that map the scores from the real line to the interval  <cit>  were considered. unfortunately, most of them resulted in transformed scores occupying a very narrow portion of the interval. we settled for a simple normalization which spread the scores "evenly" between  <dig> and 1

 mi*=mi−min⁡max⁡−min⁡,i= <dig> ...,n. 

we would like to make one last comment on the reasons behind introducing weighted distance measures here. quite obviously they are motivated by the desire for a more efficient use of the data, in this case, the numerical scores which underlie the rankings. but that is not their sole purpose. when using the original spearman and kendall distances we noticed that in many situations no clear winner exists as two or more ordered lists have the same objective function score due to the discrete nature of ranks. this brought computational instability into the iterative aggregation process. the algorithm would never converge but would simply oscillate between the two "best" lists, understandably not knowing which one to pick. when continuous weights are used to adjust the discrete ranks, the possibility of such ties is almost eliminated and the algorithm is much more computationally stable. in addition, we obtain a clear winner in an objective and rigorous way.

cross-entropy monte carlo algorithm
the details of the ce algorithm are given in  <cit> .  <cit>  explore the ce algorithm for rank aggregation in the context of microrna analysis and a very useful tutorial on the ce algorithm with several examples is presented in  <cit> . here, we only briefly describe the main steps. the ce is a stochastic search algorithm in the space of matrices with 0– <dig> valued entries with columns summing to one and rows summing to at most one since any ordered list can be uniquely mapped to such a matrix.

 <dig>  initialization: start with the uniform multinomial cell probabilities.

 <dig>  sampling: at each stage, generate a random sample of such matrices via restricted  multinomial sampling with the current cell probabilities.

 <dig>  updating: based on the current sample and the value of the objective function at the corresponding ordered list update the multinomial cell probabilities such that the objective functions at the next batch of sample values tend to be smaller.

 <dig>  convergence: stop the search when the smallest values of the objective function do not change in a given number of iterations.

the ce algorithm requires users to set a number of parameters. convergence to a global optimal solution in many ways depends on the parameters chosen. it is recommended that the number of samples n for each stage is to be set to at least 10k <dig>  and the rarity parameter ρ used in updating the cell probabilities is to be set to  <dig>  if n is relatively large or  <dig>  if n is small .

genetic algorithm
genetic algorithms are another set of tools suitable for solving complex combinatorial problems  <cit> . their main advantage is their inherent simplicity in both conceptual understanding and software implementation. in our experience, the ga performs reasonably well for the aggregation problem but one has to be careful with the selection of important tuning parameters which control the rate of the learning process.

as implemented in this package, the ga has the following steps:

 <dig>  initialization: randomly select popsize ordered lists of size k which form the initial population of possible solutions to our optimization problem. the population size popsize is important and, obviously, the larger the population size, the better chance of it containing, at some point, the optimal solution. it should ideally be a function of k and the number of unique elements in the original ordered lists li, but computational feasibility has to be considered here.

 <dig>  selection: depending on which distance is used, compute the objective function for each member of the population. then randomly select current members for the next generation using weighted random sampling where the weights are determined by the member's fitness .

 <dig>  cross-over: the selected members are then crossed-over with the probability of cp , i.e. two random ordered lists can swap their tails which start at a random position with the cp probability. only 1-point cross-overs are allowed.

 <dig>  mutation: crossing-over will allow only for the mixing of ordered lists but a rather drastic event is required to bring radically new solutions to the population pool. these are introduced by mutations which happen with the probability of mp . thus, any list in the pool can randomly change one or more of its elements.

 <dig>  convergence: the algorithm is stopped if the "optimal" list remains optimal for convin consecutive generations . to ensure that the algorithm stops running eventually, the maximum number of generations can be set in advance which will terminate the execution regardless of the first condition being true. if neither the maximum number of iterations has been reached nor the "optimal" list stayed untouched during the last convin generations, continue to step selection.

as was mentioned previously, the choice of the parameters popsize, cp, and mp is crucial for the success of the ga. if one is too conservative and selects small cp and mp probabilities, the ga will have a hard time exploring the space of possible solutions in a reasonable time, particularly, when the space is extremely large. on the other hand, choosing large values for cp and mp will results in a "haste" decision, perhaps getting trapped in a local minimum without a chance to explore the whole search space.

RESULTS
we illustrate our r package with two different rank aggregation problems, one in the context of unsupervised learning where there is an intrinsic difficulty of choosing the best clustering algorithm for a particular problem, and another one in the context of meta-analysis of several microarray cancer studies where the goal is to determine the combined set of genes indicative of the cancer status.

to start using the rankaggreg package, it must be loaded into r  <cit>  with the regular library function, library. package documentation, examples, and additional information are available through help and vignette functions.

aggregation of clustering validation measures
rank aggregation in the clustering context was introduced by  <cit> . numerous clustering algorithms are available in r and other statistical and data mining software packages, each one having its relative strengths and weaknesses in terms of how successfully they can handle certain types of data. thus, it is often difficult to select the "best" algorithm for a particular clustering task. validation  measures come to rescue to some extent and offer an objective way of ranking clustering algorithms according to their assessment of what a "good" clustering result is. if k clustering algorithms are validated with m validation measures, m ordered lists of size k are produced as a result. even though desirable, the order of clustering algorithms within each list is rarely the same. rank aggregation is helpful in reconciling the ranks and producing the "super"-list which determines the overall winner and also ranks all clustering algorithms based on their performance as determined by all m validation measures simultaneously. clustering validation is implemented in the clvalid package  <cit> . after loading the package, we bring in a mouse microarray dataset and select the first  <dig> genes from it. assuming that those  <dig> genes form  <dig> natural clusters , we evaluate  <dig> clustering algorithms with  <dig> validation measures. available clustering algorithms are: som , sota , fanny , k-means, pam, hierarchical, agnes, clara, diana, and model-based. further details can be obtained from the clvalid package documentation.

for each validation measure,  <dig> clustering algorithms can now be ranked based on the performance scores which are sorted either in ascending or descending order depending on whether larger or smaller scores correspond to better performance under the measure. here, the dunn index and the silhouette width measures give higher scores with better performance and for the other measures the smaller scores are desirable.

the  <dig> ordered lists of  <dig> algorithms are shown in table  <dig>  their corresponding weights  which were used to rank the  <dig> algorithms within each ordered list  are shown in table  <dig>  we can see that both som and hierarchical clustering are performing quite well and each is ranked first by three different validation measures. if we had to pick the overall winner, it would probably be som as it performs better overall. for this particular aggregation problem it is feasible to determine the best performer without resorting to advanced computational techniques, but it is rather difficult to obtain the overall ordered list in this case. since the number of possible solutions is not that large , it is feasible to use the brute force approach to find the optimal solution. this can be done using the bruteaggreg function provided in the package. please note that even for this relatively small problem it takes hours to perform the necessary computations. the approach is limited to toy examples only and should not be attempted if k is larger than  <dig> 

 <dig> clustering algorithms ranked by  <dig> validation measures . the rank of  <dig> means that the algorithm received the best scored from a particular validation measure. for example, som is deemed to be the best algorithm by apn, ad, and fom measures, while mo is ranked last by  <dig> out of  <dig> measures.

clustering validation scores for each validation measure as produced by the clvalid package. please note that the rows are ordered in either ascending or descending order depending whether larger or smaller scores are desirable for a particular validation measure.

the r code to perform the brute force rank aggregation is

> bruteaggreg

here, the first argument is the matrix of ordered lists , the second argument is the size of the resulting top list which is  <dig> in this case, the third argument specifies the matrix of weights which is normalized by the procedure , and the last argument indicates that we want to use the spearman footrule distance as the measure of similarity between two ordered lists. the best overall list as determined by trying all possible solutions with the weighted spearman footrule distance is sm hr km fn ag pm cl di st mo with the minimum objective function score of  <dig> . as expected, som and hierarchical clustering are the top two performers, followed by the k-means algorithm. we will now see whether the ce algorithm can quickly discover the solution without resorting to an exhaustive search.

the rankaggreg function performs rank aggregation using either the ce algorithm or the ga algorithm. if the verbose argument of the rankaggreg function is set to true , r console window outputs information at each iteration to keep the user updated. in addition, a plot similar to figure  <dig> is shown and updated at each iteration to monitor convergence.

running the following code in r

> rankaggreg

performs the rank aggregation using the ce algorithm. we get exactly the same solution in only  <dig> iterations and in about  <dig> seconds by examining mere  <dig> potential candidates. the ce algorithm was run  <dig> times using the default values with  <dig> different seeds. only  <dig> out of  <dig> times it failed to discover the optimal solution, switching the k-means and som algorithms.

to get a visual representation of the results, a convenient plot function is provided. it takes the object returned by the rankaggreg function as its first argument and outputs three side-by-side plots with useful information on the convergence properties and the final ranking. from these plots we see that already after  <dig> iterations the ce algorithm found the optimal solution. the distribution of the final monte carlo sample is shown in the second plot. most of the mass is put on the optimal value . the last plot visualizes ordered lists to be combined and the resulting solution.

weighted kendall's tau distance can also be used, though it is much more expensive to compute.

> rankaggreg

the overall list is given by km sm pm fn hr ag cl di st mo with the value of  <dig> . thus, the som is put in the second position with k-means occupying the first place. maybe somewhat surprisingly, the hierarchical clustering  algorithm is ranked fifth despite the fact that it was ranked number  <dig> in  <dig> out of  <dig> lists. the rational explanation behind this decision is given by its poor performance according to the other four measures which rank it towards the end.

the genetic algorithm can also be used with either one of the two distance measures. both results agree with the ones obtained using the ce algorithm. besides the jaggedness of the minimum path in the first plot of figure  <dig>  it is easy to notice that the ga algorithm takes significantly larger amount of cycles to converge but they take less time to complete. even given that, the population distribution of the last generation is much more heterogeneous than that of the ce.

meta-analysis of microarray experiments
microarray cancer studies often attempt to identify genes related to a specific cancer. their most common output is a list of genes ordered by corresponding p-values. different studies, even the ones analyzing the same cancer type , almost never produce identical gene lists. meta-analysis of multiple microarray studies is difficult, especially if different experimental platforms have been used. rank aggregation, however, avoids the issue of multiple experimental conditions by dealing with the final product: the ordered list of genes.

recently, we carried out a meta-analysis of  <dig> microarray studies on multiple cancers using the proposed rank aggregation algorithms  <cit> . our goal was to identify genes which would be important in development of multiple cancers. further details can be found in the original article. here, we present a smaller example described by  <cit>  who used three different monte carlo algorithms for rank aggregation of  <dig> prostate cancer microarray datasets. two experiments were conducted using the affymetrix chip technology and the other three studies used custom cdna chips. each individual study tried to identify genes which are either up or down-regulated in prostate cancer patients, resulting in ordered lists of upregualated genes shown in table  <dig> .

top- <dig> upregulated genes from  <dig> different prostate microarray experiments . hpn is the sole gene that appears in all five lists.

as shown in table  <dig>  there are  <dig> unique genes in all  <dig> gene lists. the only gene that appears in all of them is hpn, while genes amacr, gdf <dig>  and nme <dig> appear in  <dig> lists.  <dig> genes appear in just one list. the goal of rank aggregation is to combine these lists into the overall top- <dig> gene list which hopefully would be more accurate than any individual list by itself.

since no p-values were reported, we use the regular spearman distance for both the ce and the ga algorithms.

> data

> rankaggreg

using the ce algorithm with the spearman distance, the following ordered list was produced: hpn amacr gdf <dig> fasn nme <dig> uap <dig> slc25a <dig> 0act <dig> krt <dig> nme <dig> eef <dig> stra <dig> grp <dig> canx snd <dig> alcam mrpl <dig> tmem <dig> cct <dig> mthfd <dig> slc19a <dig> ppib fm <dig> entpd <dig> krt <dig>  the algorithm converged in  <dig> iterations with the minimum of  <dig> . in the overall list, hpn, as expected, is in the first place, followed closely by the two other genes that appear in four lists.

in a case when there would be an indication that some microarray studies are more reliable than others, we could set the importance parameter available in the rankaggreg function to reflect these beliefs. by default, it assigns equal weights to all ordered lists, but one, for example, could set importance = c placing stronger emphasis on the affymetrix arrays which are considered to have higher sensitivity rates.

> rankaggreg, rho =  <dig> )

this produces the following combined list which is slightly different from the one obtained treating all five studies equally:

hpn amacr 0act <dig> gdf <dig> fasn nme <dig> krt <dig> slc25a <dig> eef <dig> uap <dig> canx nme <dig> grp <dig> snd <dig> stra <dig> tmem <dig> alcam ppib naca cct <dig> rpl <dig> slc39a <dig> mthfd <dig> mrpl <dig> slc19a <dig> 

the objective function score here is  <dig> , being a little smaller than  <dig> . clearly, oact <dig> is ranked higher now  due to being at the top  in the welsh study which received more weight. similarly, the krt <dig> gene moved up a couple spots due to being present in both welsh and singh top lists which are both affymetrix.

the ga algorithm can also be applied. we increased the maximum number of iterations to allow for a longer evolution process. increasing the convin  argument to  <dig> will assure that we do not stop the algorithm too soon. the algorithm did not converge  and was stopped after  <dig> generations. the final list had an objective function score of  <dig> , which was slightly worse than what we obtained using the ce algorithm. here is the list found by the ga algorithm:

hpn amacr slc25a <dig> fasn nme <dig> gdf <dig> 0act <dig> uap <dig> krt <dig> eef <dig> stra <dig> nme <dig> mthfd <dig> snd <dig> canx grp <dig> alcam tmem <dig> ppib cct <dig> slc19a <dig> cbx <dig> sat fm <dig> snx <dig> 

based on the value of the objective function, we prefer the list identified by the ce algorithm in this case. from that list,  <dig> genes were previously linked to prostate cancer development in the literature: hpn, amacr, gdf <dig>  fasn, slc25a <dig>  krt <dig>  alcam, cct <dig>  and mthfd <dig>  note that  <dig> of them are among the top- <dig> genes in the obtained list.  <cit>  find strong evidence for hpn's association with prostate cancer susceptibility and tumor aggressiveness, amacr was shown to be overexpressed in prostate cancer by  <cit> , and  <cit>  propose to use fasn, which is also overexpressed, as a therapeutic target for prostate cancer. if we, for example, ignored the overall list for a moment and concentrated on the individual lists, we would miss a clearly important gene fasn  <dig> out of  <dig> times as it does not appear in luo and true top- <dig> lists. the overall list, which borrows the information across the studies, places fasn in the fourth place making it impossible to overlook.

lin and ding  <cit>  also used the ce algorithm to aggregate the same five gene lists with the results presented in table  <dig> in their original manuscript. the obtained list for the unweighted spearman column is very similar to the one obtained using the rankaggreg function. on their supplementary website, the authors made available an r function that performs rank aggregation using the ce algorithm; the underlying code is written in c. at this time, it is somewhat faster than the rankaggreg function but it lacks the user-friendliness and exibility that our package offers. in addition, it provides no alternatives to the ce algorithm and does not make use of the weighted distance functions that we proposed.

to give a reader some perspective on whether the computational overhead is well justified in this case, we aggregated the five lists using the borda count method described in the background section. according to this simple procedure, the overall list is

hpn amacr gdf <dig> fasn nme <dig> eef <dig> krt <dig> nme <dig> 0act <dig> slc25a <dig> uap <dig> canx grp <dig> stra <dig> snd <dig> ogt alcam cyp1b <dig> mthfd <dig> atf <dig> cbx <dig> sat brca <dig> mrpl <dig> ank <dig> 

the first four genes are the same when compared to the result obtained using the ce algorithm with equal weights. the order of other genes, however, is different. the lists share  <dig> common genes among the two of them. using the spearman distance, we calculated the objective function score for the list obtained from the borda count and it turned out to be  <dig> , which is a little worse than  <dig>  that we get using the ce algorithm. thus, the extra cost in computing may be well justified as we obtained a better list with a smaller score.

CONCLUSIONS
the rankaggreg package provides an easy and convenient interface to handle complex rank aggregation problems. it performs rank aggregation using two different algorithms with a choice of two different distances. the brute force approach is also available for small-scale problems. a simple plot function helps to visualize the rank aggregation problem and the obtained solution.

the effectiveness of the ce and the ga algorithms in discovering optimal lists is certainly limited by the size of the aggregation problem. as both algorithms need to effectively search the solution space, which even in moderate aggregation problems  is extremely large, there exists a practical limitation as to what problems can be handled. the examples presented in this article are rather of a moderate size but larger problems can definitely be tackled. top- <dig> lists with a significant amount of overlap in terms of their content can certainly be aggregated using either one of the proposed algorithms. how well they will perform in aggregation of much larger problems remains to be investigated. in the bioinformatics context, however, researchers are often interested in a relatively small number  of significant discoveries and their aggregation is within the limits of the proposed methodology.

we would like to stress that using either the ce or the ga algorithms for large problems does not "guarantee" an optimal solution. performance of both of these algorithms is quite sensitive to the tuning parameters, in particular the sample size n for the ce algorithm and the cross-over  and mutation  probabilities for the ga algorithm. the user is encouraged to run the rankaggreg function several times. if different optimal lists are produced, increasing sample size is probably necessary. tuning additional parameters as discussed above may also prevent local minima traps. that said, however, we are quite impressed by the ability of both algorithms, the ce in particular, in discovering the optimal ordering of the elements in the combined list.

availability and requirements
project name: rankaggreg

project home page: 

operating system: windows, unix

programming language: r

other requirements: r- <dig> . <dig> or newer

license: lgpl

the rankaggreg package can be installed from the cran using the install.packages command. the local zip file can be installed using r gui by selecting packages and then install package from local zip files.

authors' contributions
vp produced the rankaggreg package and contributed towards planning and writing of the manuscript, particularly producing the results section. sd and sd provided guidance and planning for the project, and contributed towards writing the manuscript.

