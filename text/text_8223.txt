BACKGROUND
agent-based models  offer endless possibilities to explore heterogeneous problems and spatial patterns but come with a large computational burden. abms are increasingly used to model infectious disease transmission, but little attention is given in the literature to model implementation and performance, e.g., in  <cit> . usually the simulation time on large clusters is mentioned, but it is not clear whether computational resources are optimally used. however, computational performance is a significant aspect of a simulators’ usefulness. especially model exploration and sensitivity analysis, which require bulk calculations, benefit from efficient algorithms  <cit> . furthermore, improving model performance facilitates model development and testing on workstation systems.

performance is implementation specific and therefore we compared different close-contact infectious disease simulators starting from two published abms for pandemic influenza: flute from chao et al.  <cit>  and fred  from grefenstette et al.  <cit> . both simulators are written in c++ and are free, open source software  under the gnu general public license and the bsd 3-clause, respectively. the flute population model consists of census tracts with communities of  <dig> residents on average. the simulation runs in discrete time steps of 12-h representing daytime with work, school and day community contacts and nighttime with household and home community contacts. all children go to school in the home community and adults are assigned to workplaces based on employment rates and commuting data. the community is the central unit in flute and one person is assigned to only one community per time step. the implementation of fred is based on specific places for social contacts. different places are used ranging from small households and classrooms to large schools and communities. all members of one place can have social contacts and one person might be assigned to multiple places per time step.

individual behavior, social contact structures and population setup are very important to simulate infectious diseases. abms are suited to model these features because each person can be represented and stored separately. inherent to these models are many checks and data transfers compared to the number of floating point calculations. for many years, hardware developers have been able to increase the central processing unit  performance  <cit> . mass storage and memory subsystems have improved more slowly for cost reasons, which has introduced a performance gap between processing and accessing data. to reduce this imbalance, a hierarchy of small high-speed cache memories has been added to the cpu. instead of fetching data multiple times from the main memory, it is loaded into cache and re-used  <cit> . the processor loads data into the cache in chunks called cache lines, which leads to efficient processing if in addition to one memory location also the nearby locations are referenced in the near future. this memory characteristic is important for the data layout of software  <cit> . for example, if person data is stored jointly in a person object  and next to a person’s age also their gender and zip-code are checked, it will already be available in the high-speed cache. on the other hand, if person attributes are stored in separate containers  and only the ages are checked, many more ages are available in one cache line and less slow memory accesses are required.

high-speed memory and other advances in cpu technology have enabled performance improvements for sequential software with about a factor of two for every eighteen months during a few decades  <cit> . unfortunately, these improvements have now encountered physical limits and processor manufacturers have turned to multi-core and hyper-threading architectures to increase the accumulated peak performance  <cit> . these novel architectures require adaptations of existing software and new programming approaches to fully exploit the performance potential. extra attention is needed for shared resources  <cit>  like population data or random numbers.

random numbers are a key resource of monte carlo methods and the more randomness they exhibit, the better  <cit> . computer algorithms are by definition deterministic procedures. they can only approximate randomness by generating a stream of so-called pseudo-random numbers. the only true randomness in a sequence of pseudo-random numbers is the “seed” value that gets the series started. the complexity increases even more with parallel simulations. some good pseudo-random number generators  lose their efficiency or quality, or even both, when they are parallelized  <cit> . in parallel applications, independent streams of random numbers are required for each thread to prevent latency. different parallelization techniques are used in practice. in “random seeding”, all processes use the same prng but with a different seed with the hope that they will generate non-overlapping series. more robust and versatile is the “leapfrog” method where one prng sequence is distributed .

in this paper, we focus on single- and multi-core performance of discrete-time abm simulators implemented in c/c++ to simulate infectious disease transmission. we used a limited close-contact disease simulator as case study. however, the features that we look into are also applicable to more extensive models or other types of abms. we investigate data management, algorithmic procedures and parallelization. we illustrate good-practice of a prng in a parallel context. the goal of this paper is to formulate recommendations for abm simulators that are straightforward to realize and significantly benefit the performance.

the paper is structured as follows: first, we describe the methods starting with three different implementations of the population based on a general data structure. second, we define an extension by adding a sorting algorithm. third, we specify methods to run simulations in parallel with a shared-memory approach. fourth, we describe the input data, run parameters and the work environment we used. next, the results and discussion section presents all findings. finally, we end with concluding remarks and avenues for further research.

methods
model structure and implementation
we have opted for a model structure consisting of households, schools, workplaces and districts similar to published studies  <cit> . figure  <dig> shows a schematic overview of the locations, which represent a group of people we define as a “cluster”. social contacts can only be made within a cluster. during nighttime, people can have social contacts with members of their household and home district. during daytime, people stay at home or go to a workplace or school depending on their age, which also determines their day district. contact between infectious and susceptible people may lead to disease transmission, which is a stochastic process based on social contact rates, infectiousness and susceptibility.
fig.  <dig> social contact structure. people are member of a household cluster and the corresponding home district at night. during daytime, people can stay at home or go to a school or workplace in a day district.


fig.  <dig> model design: classes and compositions. the digits represent the number of links that are possible. e.g., the area can have  <dig> or many  cluster objects, but a cluster can only be part of  <dig> area. the models differ in the implementation of the grey classes: flute has less cluster types in area and the population in sid does not contain person objects.



we have constructed three implementations for the previous described transmission model: “flute” and “fred” are based on the corresponding open source models and “sid” has a novel data layout. the area in flute contains only home and day district clusters. membership to smaller sub-clusters like households, schools and workplaces can be retrieved from stored cluster ids in person. people in a district that are also member of the same sub-cluster have two opportunities for social contact and transmission. therefore, during the processing of social contacts in the district, sub-cluster ids need to be checked. if two people from a district are also member of the same sub-cluster, we used an aggregated transmission probability instead of performing two random draws. the area of fred and sid has also separate households and day clusters . we illustrate the difference with the following pseudo-code for the social contacts during nighttime with age dependent transmission probability p tr and p tr∗ for one or two social contacts respectively:



population structure
data for an individual is stored as a person object in flute and fred and the population is a container of person entities, stored consecutively in memory. in sid, the population has a different container for each person attribute and the data of one person is always located at the same index in each of those different containers. for example, to access the age of person i in flute or fred we use “population.age” while in sid we use “population.ages”.

population data have been extracted from the  <dig> u.s. synthetic population database  from rti international  <cit>  for brooklyn and nassau county, new york. every county or state from this database can be used to obtain individual age, household, school and workplace data. people of  <dig> to  <dig> years of age with a school and work id in the original database were assigned to the school to guarantee that people were assigned to only one day cluster. to compare different model implementations, we needed an extra social contact layer . we have created home districts by adding households, sorted on id, until a number of  <dig> people was reached. we assumed that household ids are based on geographic proximity and the threshold was adopted from chao et al.  <cit> . the day districts have been created analogously. the nassau population consists of  <dig>  million people in  <dig>  <dig> households and  <dig>  <dig> day clusters. brooklyn has  <dig>  million people and the cluster sizes range from one up to  <dig>  <dig> people. more details on the study populations are listed in table  <dig> population statistics. legend:  and 




the population data file determined the initial ordering of the person data in the population object. we used seven different orderings for the same population details: the original sequence from the rti database, a fully randomized order and population data sorted according to household, day cluster, and both household  and day cluster , and vice versa. to minimize the effect of random draws, we created five different files for each ordering with a random component.

algorithmic extension: sorting
the open source models  <cit>  process disease transmission by looping over all members of a cluster and if a member is infectious, to match them with all susceptibles. to reduce the total number of operations, we introduced a modified algorithm in which the members of a cluster are first sorted according health status before the infectious members are matched with the susceptible members. a newly infected member is moved ahead of the first susceptible. the member list obtains the following structure: first, recovered and infected  members and second, susceptible members. the following pseudo-code shows the sort algorithm for fred and sid .



parallelization: scheduling
the openmp api is often used for shared memory parallel programming in c/c++  <cit> . in this programming model, subsets of a process are managed independently  and share a global address space of a single or multiple processors which they read and write asynchronously. for each cluster type  in an area, a person is a member of only one cluster. therefore, clusters are stored per type so that these containers can be processed in parallel without synchronization. parallel processing within one cluster would lead to synchronization overhead. the workload distribution over the threads can be static or dynamic  <cit> . with static scheduling, a fixed number of tasks are assigned to each thread. in dynamic scheduling, the workload is distributed over the idle threads until all tasks are done. we have used workloads in chunks of one and ten clusters.

inputs and work environment
we used a  <dig> ×  <dig> transmission matrix and assumed that the transmission probability  is doubled for contacts between children   <cit> . similar to the literature  <cit> , we estimated the relationship between p tr and the basic reproduction number r  <dig> by counting the number of secondary cases of one infected in a complete susceptible population with seven p tr values. based on  <dig> realizations with seven p tr, we approximated r  <dig> by exp. the total run time depends on the clinical attack rate  and for this reason, we performed we performed benchmarks for a range of r  <dig> values . each simulation was performed for  <dig> days. to start the epidemic, we infected a random fraction of the population. after testing seeding rates of 1e − <dig>  1e − <dig>  1e − <dig> and 1e − <dig>  we observed limited impact on the number of cases for these ranges and selected 1e − <dig> as baseline setting.

we included the pseudo-random number generator  from an open source software package called trng  <cit> , a portable and highly optimized library of parallelizable generators. to prevent synchronization and latency, independent streams of random numbers are required for each thread. we used the robust and versatile “leapfrog” method where the prng sequence is distributed over p processes by calculating for draw i the i*th number in the sequence. there are no recommendations to select prng seeds to obtain different stochastic results, except that those seeds have to be different. therefore, the run index has been used to seed the prng.

an extended class diagram and the free open source code can be found in additional file  <dig> and additional file  <dig> respectively. additional file  <dig> contains a user manual to make use of the project software. during development, we used the google c++ testing framework  <cit>  to perform detailed tests. these tests were applied in automated fashion with every change in the code base via a continuous integration server  <cit> . the templatized c++ command line parser library  <cit>  was used to transfer configurations to the executable. the project-code is standard c++ <dig> throughout, independent of external libraries and portable over all platforms that have a gnu compiler  available.

timings presented in this paper were obtained from benchmarks on a cluster with intel®; xeon®; e5- <dig> v <dig>  <dig>  ghz cpu’s  from the hpc core facility calcua at the university of antwerp. we confirmed our results with benchmarks on quad-core intel®; xeon®; w <dig>  <dig>  ghz  cpu’s and amd opteron®;  <dig> cpu’s. the gnu compiler  was used in release mode with compiler optimization “-o3”. additional file  <dig> contains more info on the hardware and extra results. the open source tool perfexpert  <cit>  was used for profiling, as installed on the calcua cluster.

we performed additional benchmarks to explore the effect of cluster size, dynamic clusters and increased model complexity on model performance. methods and results can be found in additional file  <dig> 

RESULTS
the number of infected people is the dominant factor in determining the computational workload and the required simulation time. therefore, we needed to incorporate distinct epidemic curves in our benchmarks by using different r  <dig> values. small deviations in the ar were observed for each r  <dig> as a result of different stochastic paths with and without the sort algorithm and given the different processing in flute. to prevent stochastic fade-out, which is not appropriate for benchmarks, we used relatively high epidemic seeding rates to introduce new infected people in the population  <cit> . the benchmarks all report elapsed wall clock times as is appropriate for parallel programs. all results in this paper are based on mean timings from  <dig> runs with a different random number generator seed. with intervention strategies, we expect more stochastic fade-out and would require more realizations. benchmarks are performed on idle computing nodes and results on other hardware can be found in additional file  <dig> 

simulations with the basic models without concerns of the population order clearly required the longest run times. figure  <dig> illustrates the total run time for fred simulations with the nassau population. similar results were obtained with the other models . we observed a large decrease in run time when the population is structured according to day cluster and household. the workload for a cluster of size n with i infectious and s susceptible members can be approximated by n health checks to select the infectious members + i* health checks to select susceptible potential contacts + i*s random draws to match the infectious member with the susceptible members. the number of susceptibles decreases with each new case, which explains the decreasing curve in fig.  <dig> for epidemics with high ar. next, sorting cluster members on health status before processing disease transmission, had a large impact on the performance. the run times for nassau were reduced with  <dig> % to  <dig> % compared to the basic models, depending on ar and population ordering. the algorithm with sorting has overhead because of swapping infected and healthy members but overcomes i* health checks on susceptibility, which explains the reduced run times.
fig.  <dig> run time according to attack rate and population structure for nassau simulations using fred. structured population: sorted according to day cluster  and household .



given the impact of the ar on the simulation time, we needed to monitor the benchmarks closely. the stochastic transmission process is altered by the sorting algorithm, which has limited impact on the ar. also, the population ordering determines the initial sequence of the cluster members and thus the random path of the simulator. figure  <dig> presents the ar from  <dig> nassau simulations using different models and population structures. the ar distributions were overlapping, which suggested similar transmission dynamics and approved run time comparisons. to validate the transmission model presented here, we performed simulations with the open source fred software from greffenstette et al.  <cit>  using population data distributed with the source code for allegheny, pennsylvania. we observed ars of ± <dig> % if r0= <dig>  and ill people could not stay home, which was close to our results.
fig.  <dig> attack rates for nassau simulation using r  <dig> =  <dig>  and seeding rate = 1e − <dig> according to implementation  and population structure . results from  <dig> simulations. the original population structure is used to compare implementations  and fred to compare population structures . box: upper and lower quartile, wisker: minimum and maximum excluding outliers, circle: outliers , : structured population e.g.,  represents populations sorted according to day cluster  and household .



the population ordering appeared to have a large impact on model performance. to examine the effect on an epidemic with r  <dig> =  <dig>  , we used different versions of the population data with and without sorting according to household and/or day cluster. we repeated our benchmarks multiple times and did not observe large differences in ars . table  <dig> presents the mean timings from multiple runs with each population ordering using the three basic models. the randomized populations gave the highest run times for all basic models. using the original structure of the rti population files slightly decreased the run time. sorting the population on household id improved the performance though most optimal was to sort the population on day cluster  and household . with this sorted population structure, we observed a reduction up to  <dig> % for flute and fred compared with the randomized population. the effect of the population structure was less for sid. the original open source flute model  <cit>  uses a population sorted according to household. with our flute implementation, we observed a decrease in run time of  <dig> % by using a population for nassau sorted by day cluster and household. the population of the original fred model  <cit>  follows the structure of the rti population files. a decrease of  <dig> % in total run time can be achieved with our fred implementation by sorting the population file once. the impact of the population ordering was limited for the models with the sort algorithms.timings for nassau simulations with different population structures


results in seconds with r  <dig> =  <dig>  and seeding rate = 1e − <dig> . e.g.,  represents populations sorted according to day cluster  and household 



the general trends from the nassau simulations were also valid for brooklyn. the improvement of the sorting algorithm ranged from  <dig> % to  <dig> %. for brooklyn, we reduced the simulation time by sorting the population once with respectively  <dig> % and  <dig> % compared to the original flute and fred population. the highest improvement with the population structuring was  <dig> %. table  <dig> presents the mean run times from  <dig> brooklyn simulations with r  <dig> =  <dig> . the ranking of the basic models based on total run time differed between nassau and brooklyn simulations due to the different population size and cluster size distribution. for the models with the sorting algorithm in the cluster class, the ranking was independent of the population structure. the extra effort to manage separated household and day clusters in fred and sid improved the simulators’ performance compared to the district-approach from flute. the sid model with the sort algorithm performed best for all benchmarks, especially with the structured population.profiling results for brooklyn simulations


results with r  <dig> =  <dig>  and seeding rate = 1e − <dig> . all metrics, except run time, are given in lcpi: local cycles per instruction. llc: last level cache.  populations are sorted according to day cluster  and household 



on today’s multi-core chips, memory access is a critical performance-limiting factor  <cit> . therefore, we have analyzed software behavior and memory access patterns with a profiling tool for high-performance computing applications, perfexpert  <cit> . we found that the function in cluster to process disease transmission takes on average  <dig> % of the run time. therefore, optimizations in this part of the code can have large impact. since a member cannot be infectious and susceptible at the same time, it is not necessary to check whether a member tries to infect himself/herself. we observed that adding a simple comparison of two c++ pointers or two integer indices in fred and sid respectively, increased the simulation time with  <dig> %. table  <dig> presents a selection of the perfexpert output. flute had the highest penalty for branch instructions , which limits the cpu to pipeline instructions and to execute different stages  at the same time. a mispredicted branch instruction disturbs this optimization. fred and sid required less cycles for branch instructions, especially with the sort algorithm. sorting the cluster members before processing transmission also reduced the data access. regarding the cache-coherency, we have observed that structuring the population according to the social contact clusters decreased the number of last level cache misses. the sorting algorithm disrupts the memory consistency by relocating references to cluster members. by comparing fred and sid profiling results, we can confirm the targeted data management strategy from struct-of-array vs array-of-structs: the sid models have fewer last level cache misses.

processing disease transmission requires many iterations over independent clusters and therefore seems suited for distributed programming. we observed that the effect of parallelization was dependent of the epidemic curve. figure  <dig> presents differences in the speedup using flute with  <dig> threads according to the ar and the epidemic seeding rate . the different rates we used did not have impact on the total number of cases but only on the length of the initial phase with a small amount of infected clusters. simulations with a high epidemic seeding rate and a large ar gave the best speedups using multiple threads. to illustrate the possibilities of parallelization, we compared simulation times using  <dig> to  <dig> threads for epidemics with r  <dig> =  <dig>  and seeding rate = 1e − <dig> . figure  <dig> presents the speedup for sid with basic and sort algorithm using a structured population according to day cluster and household. similar results were obtained for the other implementations and using the randomized population, which can be found in additional file  <dig>  we observed good speedup for all models and scheduling options with  <dig> threads. with  <dig> or more threads, the added value of extra threads decreased due to memory bandwidth saturation. making the clusters more self-contained by replacing the member references by actual person data would reduce this limitation although it requires much synchronization between the clusters and extra memory. all basic models had most benefit of dynamic scheduling with workload chunks of  <dig> cluster. with sorting, fred and sid seemed to operate more optimally with static scheduling or dynamic scheduling with workload chunks larger than  <dig> cluster. for flute, the dynamic scheduling with chunks of  <dig> cluster gave the best speedup. we tested the models on other hardware and observed similar results .
fig.  <dig> speedup according to attack rate and epidemic seeding rate using flute  with  <dig> threads. all simulations were performed with a structured brooklyn population sorted according to day cluster and household using dynamic scheduling with workload chunks of  <dig> cluster.

fig.  <dig> speedup according to thread number and scheduling for brooklyn simulations using sid. results are shown for the basic and sort algorithm with dynamic and static parallel scheduling using workload chunk size of  <dig> and  <dig> clusters. all simulations were performed with a structured population sorted according to day cluster and household with r  <dig> =  <dig>  and seeding rate of 1e − <dig> .



by increasing model complexity, more different cluster types can be used and sorting the population might be less effective. if more person attributes are required for the disease transmission, co-locating these in a person object will be beneficial. on the other hand, the increased amount of person data will reduce the number of persons that fit in the high-speed cache, so more data needs to be fetched with higher latency. we explored these model aspects  and observed that cluster size had a large impact on run time. though, the differences regarding population sorting, model design  and the sorting algorithm scaled with cluster size. to estimate the effect of dynamic clusters on model performance, we implemented a model with changing cluster membership over time. this way, the run time increased but the overall conclusions remained valid. increasing model complexity by adding extra person attributes in flute and fred reduced the impact of the population sorting. the run times for sid remained constant if these attributes were not used, which confirmed the targeted data strategy of struct-of-array vs array-of-structs. the sid design became a disadvantage regarding model performance and workload for the programmer if these extra person attributes were involved in the transmission process.

CONCLUSIONS
abms offer a very powerful and flexible framework to analyze infectious disease transmission. unfortunately they come with a large computational cost. investing time in code optimization and adaptation to hardware innovations reduces time available for adding new features although it can save much time during testing and in production.

we compared different abm implementations for close-contact disease transmission models for two u.s. counties. our abm consisted of household, school, workplace and district clusters and people in a cluster can have social contacts and transmit an influenza-like disease. the transmission probability was assumed to be age dependent. we observed reductions up to  <dig> % by structuring the model population once according to the largest social contact cluster. next, sorting the cluster members based on health status before processing disease transmission appeared also very beneficial for the model performance .

data movement and access require much more cycles than floating point operations and therefore data layout has impact on run time. we compared models that handle the population in large districts with models that also process the household and day clusters separately. the latter seemed beneficial for the performance especially in combination with the sorting on health status in the clusters. the storage of person data in separate containers instead of per person improved the data locality and cache-coherency and reduced modeling time. models that sort cluster members on health-status before processing disease transmission are scalable with multiple threads if the epidemics have a limited initial phase. the parallel scheduling and workload chunk size had significant impact on the simulation time.

increasing model complexity may reduce the impact of the population ordering. we describe the core of the simulator but more research is needed to assess the role of data layout and sorting algorithms together with mitigation strategies. although improving data layout by using a separate container for each person feature might increase the model performance, it is counter intuitive for an abm and requires extra effort from the modeler. the current software does not predict the workload before scheduling the chunks over multiple threads. we believe this scheduling would be a valuable extension to the parallel implementation because the cluster sizes and the amount of infected individuals per cluster can be very heterogeneous. in conclusion, large performance gains can be achieved with limited effort by structuring the population once, adding an algorithm that sorts by health status and selecting appropriate parallel settings.

additional files
additional file  <dig> 
class diagram. schematic overview of the project.



additional file  <dig> 
free open source code. documented c++ code with makefiles.



additional file  <dig> 
user manual. user manual of the project software.



additional file  <dig> 
hardware specifications and extra results. computer hardware details and more benchmarks for nassau and brooklyn simulations.



additional file  <dig> 
model exploration and validation. benchmarks to assess the impact of model complexity on model performance.



abbreviations
abmagent-based model

arattack rate

cpucentral processing unit

prngpseudo-random number generator

competing interests

the authors declare that they have no competing interests.

authors’ contributions

lw, ss and jb conceived and designed the experiments. lw and ss performed the experiments. lw and et analyzed the data. pb, nh and jb contributed materials/analysis tools. lw, ss, et, pb, nh and jb wrote the paper. all authors read and approved the final manuscript.

