BACKGROUND
in the field of life sciences, many studies are performed where the influence of external stimuli is investigated on the organism's gene transcription, protein expression, and metabolism . examples of these stimuli are the administration of drugs or a specific diet. these studies aim at understanding how changes in stimuli can affect an individual's genes and health but also at finding biomarkers in tissues or fluids that predict or influence the onset of a disease, or assesses its incidence or patho-physiological behaviour. measurement of such a marker allows quantification of the extent to which an individual is susceptible to the development of disease.

successfully analyzing life science studies and so-called omics data  requires appropriate bioinformatics tools and the conceptual frameworks for analysis and interpretation of large amounts of data generated.

in general, the process from conducting a study to obtaining systems biology data, and deriving molecular insight from that is a delicate one. it requires a well-thought workflow that translates the research hypothesis to a statistical study design, facilitates the performance of validated sample preparation, data acquisition, and the interpretation of statistical results . another point of attention with this kind of data is that small changes in the stimuli can trigger multiple changes in gene expression, protein and metabolite levels. these changes are usually very small compared to the biological background variation in the data which means that the statistical power of the analysis is low. an additional difficulty in data interpretation is that the data are usually structured. descending from a biological research question, the data often contain underlying factors such as time, dose, diet, groups, or combinations that correspond to different sources of variation. it can be anticipated that if this data structure is taken into account, the data analysis becomes more focused on relevant sources of variation and therefore has more power. however, most of the currently used statistical approaches simply ignore the structure of the data.

analysis of variance  is an obvious method to analyse the data structure by decomposing the total data into different sources of variance. anova is a so-called univariate method which means that it analyses one variable at a time. as such, it has been used for the correction or analysis of high dimensional gene expression and metabolomic data  <cit> . in contrast, it was also combined with principal component analysis  and simultaneous component analysis  to allow a multivariate data analysis while taking the underlying study design into account  <cit> . however, in these approaches, anova was not combined with a supervised analysis such as regression which is a very useful technique in life science studies.

a regression analysis is generally used to determine the relationship between two types of data  obtained from the study subjects. in these studies, the aim of regression is not to predict the value of the phenotype but to derive reliable and validated relationships that can be studied further to select and interpret those genes, proteins, or metabolites that are most important with respect to the phenotype. therefore, this paper aims to show how the quality and interpretability of statistical regression models can be improved by explicitly using the data structure. in the past for the analysis of a chemical process, a like-wise approach was performed by  <cit> , however limited to only within-run and between-run variance.

as a case study, liquid chromatography mass spectrometry  liver lipidomics data have been used. these data are a part of a large-scale nutritional intervention survey performed in apolipoprotein e3-leiden  transgenic mice. in these mice, during the early time points, the time resolved development of diet-induced obesity and insulin resistance was investigated. as a phenotype, the hepatic inflammation marker serum amyloid a  was measured in blood plasma of these mice. the final data set consists of  <dig> mice for which  <dig> lipids and  <dig> free fatty acids were measured  as well as  <dig> clinical inflammation marker. inflammation is believed to play an important role in the development of diet related diseases such as obesity, diabetes type ii, and atherosclerosis  <cit>  together with the liver as key organ and lipids and free fatty acids as important inflammation related metabolites. for optimal data interpretation, based on the underlying study design, data decomposition by anova is combined with partial least squares  regression analysis to define the relationship between lc-ms lipidomics and the inflammation marker, saa. this combination is called anova-pls. for comparison, standard data analysis was also performed using pls on the total data. our findings demonstrate the advantage of incorporating the study design in the data evaluation.

methods
statistical theory
the principles of anova have been used to separate the different sources of variation of the data based on the underlying study design. the resulting independent data blocks and their combinations can be analysed in various ways including explorative analysis  or regression analysis . before explaining anova-pls, this section briefly describes the ideas behind anova, sca, and asca.

analysis of variance
classical anova techniques can be used to distinguish different sources of variation  <cit> . the aim of anova is to separate the sources of variation and to assign them to specific factors. this is done by splitting the variations into orthogonal and independent parts. in this paper, an anova model will be used to describe two main factors  and their interaction :

  xkci = μ + αk + βc + kc + εkci 

where xkc is the data observed for the sample i on levels k and c, μ is the overall offset, αk is the model parameter for the factor time at level k, βc is the parameter for diet at level c, kc is the parameter of time × diet interaction, and εkci the residual.

if the parameters of this model are calculated under the proper constraints, the model uniquely separates the total data into orthogonal  data blocks that represent the known factors from the design. the remaining part of the data equals the difference between the sum of the known data blocks and the total data. that part is called the residual part and contains sources of variation that cannot be attributed to a known factor. these sources of variation can originate from unknown factors such as instrumental drift, batch effects, sample work up errors, or measurement errors. in the present data set, the residual part will largely consist of biological variability. this is caused by the fact that each measurement represents a liver sample of a separately sacrificed mouse. evidently, no true replicates in time are available.

simultaneous component analysis 
sca  <cit>  is an extension of principal component analysis  that can be used in the situation where the same variables have been measured in two or more populations. examples of such populations are groups of mice that have been fed on different diets. similar as with pca, sca analyses data by summarizing and projecting these in a new space. this new space is defined by so-called simultaneous components, which are linear combinations of the original axis  and that meet some specific mathematical demands. these demands are defined in such a way that the first simultaneous component describes the largest amount of variance possible. consecutive simultaneous components are orthogonal to all the previous ones and each one describes less variation than the previous simultaneous component. using the simultaneous components, it is usually possible to describe the data more condensed than originally. this means that a dimension reduction can be performed which facilitates a visual inspection of the data. in a formula, sca can be represented as follows:

  x==pt+e 

where x  is the matrix of the population matrices xi ; ti  are the scores which are the projected original data into the space of the loadings p ; and e  is the residual. the indices n, ni, j, and r indicate the total number of objects, the number of objects in population i, the number of variables, and the selected dimension in which the data are summarized well , respectively. compared to pca, the advantage of sca is that multi-population data are described in one model with one loading matrix containing the simultaneous components. however, if there are no constraints placed on scores ti, the sca model of equation  <dig> is equal to pca on the concatenated matrix x. timmerman & kiers  <cit>  describe some possible choices for these constraints and the resulting consequences for the corresponding analysis.

as discussed before, a drawback of both pca and sca is that the information sources in the models are confounded. this means that the interpretation of the models can become problematic. a way to solve this problem is by analysing separate data blocks that correspond to unique and known sources of variance.

anova-simultaneous component analysis 
in asca, the advantages of sca and anova are combined  <cit> . this leads to a method in which the original data are split into orthogonal data blocks that can be attributed to a specific factor of interest. on these separate data blocks, sca can be applied to analyze them using the concept of data reduction. the multivariate analogue to the univariate anova model  is:

  xkcij = μj + αkj + βcj + kcj + εkcij 

in this equation, the index j is added to account for all the j variables in the data sets that are described. thus, equation  <dig> represents a series of j anovas. subsequently, all the terms in the latter equation can be collected in matrices x with dimensions , leading to equation 4:

  x = 1mt + xk + xc + x + xe 

where  <dig> is a size n column vector and mt a size j row vector containing all estimates of μj. if sca component models are used to approximate the information in the matrices xk, xc, and xkc, the asca model that corresponds to the anova model of equation  <dig> is:

  x=1mt+tkpkt+tcpct+tpt+e 

the matrices tk, tc, t are the sca scores of the factors k, c, and their interaction, respectively; pk, pc, and p are the corresponding loadings, while e is the residual data that cannot be attributed to a known factor. under the proper constraints asca can be calculated very simply. for a balanced design, it has been found that this can be achieved by a proper centring and performing pca on the rearranged data blocks  <cit> . because these constraints are used in this study, in the remainder of this paper the expression 'principal components' will be used instead of 'simultaneous components'.

anova-partial least squares
similar as with asca, anova-pls is the combination of variance decomposition to extract different effects and a subsequent statistical analysis. in this case this analysis is regression with pls. pls has been described extensively by  <cit>  and, more recently, for genomic data by  <cit> . it is a data modelling technique that is used to determine the relationship between a multivariate data set and a univariate phenotype. depending on the fact if the phenotype is discrete  or continuous , this becomes a classification or a regression analysis, respectively. in this paper, only regression is considered. pls is able to analyze large numbers of variables in small sample sizes by reducing the dimensionality of the data. the dimension reduction is achieved by constructing latent components , in such a way that these components have maximal covariance with the outcome variable whereas the latent components themselves are uncorrelated. note that the optimal number of pls factors is a model meta-parameter that needs to be estimated independently from the regression performance. this is explained in the next section.

for asca, each data block was analysed separately by sca to interpret the different effects. anova-pls slightly differs in the sense that different combinations of effects are used to determine the relation between the data types rather than single effects. the advantage of analyzing selected combinations of effects is that certain effects are highlighted or excluded, compared to the total data, which enables a specific zoom into the data. an additional statistical reason to use effect combinations instead of single effects is that the rank of single effects is too low to build a reliable regression model. this originates from the anova principles where effects are represented by corresponding group means instead of individual values.

statistical regression performance and validation
the quality of the regression models is expressed with a q2cv value after double cross-validation  <cit> . the q <dig> parameter is defined as follows  <cit> :

  q2=1−pressssywithpress=∑i=1n2andssy=∑i=1n <dig> 

in this expression, press is the prediction error sum of squares , while ssy is the sums of squared differences between the measured observations and their mean value y¯. note that if the prediction error  becomes larger than the sums of squares , the q <dig> value is smaller than zero indicating a badly predicting model. because the data set contains  <dig> groups , a 12-fold cross-validation is performed where in each step one unique group is left out. in this approach, the first cross-validation loop is used to determine the optimal meta-parameter  and the second one to predict the performance of the model, given the selected meta-parameter. whereas the number of pls factors and the prediction performance are determined with a double cross-validation, the final regression coefficients are determined on the total data, because here the aim is only to find the regression coefficients that belong to the optimal pls model without estimating the prediction performance. the corresponding number or pls factors is determined in a separate single cross-validation. the need for this approach is shown by  <cit> . from the final model, the significant variables are selected based on jack-knifing according to  <cit> . for this reason, the rsd  of the regression coefficients was calculated: the standard deviation divided by the mean. variables which have an rsd <  <dig>  are considered to be significant: their mean is larger than  <dig> times the standard deviation, indicating a 95% confidence interval. next, with the significant and the insignificant variables, so-called informative and uninformative models are made, respectively  <cit> . these models are used to confirm the predictive power of the selected and deselected variables.

experimental
design of the case study
the study design is shown in figure  <dig>  the study involved  <dig> male apoe3-leiden mice with an age of  <dig> ±  <dig> weeks. three weeks before the start of the study, the mice were fed a standard chow diet. at the start of the study,  <dig> mice were switched to a high fat bovine diet ,  <dig>  <dig> mice to a high fat palm oil diet , and  <dig> mice stayed on the chow diet. of each diet group,  <dig> mice were sacrificed at time points  <dig> and  <dig> days, and  <dig> and  <dig> weeks. as a consequence of the study design, important factors underlying the data sets are time, diet, their interaction: time × diet, and individual  variation.

sample preparation and performed measurements
liver and orbital blood was obtained from animals after a four hour fasting period , it was snap frozen in liquid nitrogen and stored at -80°c until processing. liver lipidomics data  were analyzed with the lipid lc-ms  <cit> , and ffa lc-ms tno platforms which can identify and quantify about  <dig> different lipids and ffa. for all detected lipids  and ffa , a relative concentration was calculated . the relative concentrations were corrected for slight differences in liver weight. serum amyloid a levels were measured by elisa specific for saa . furthermore, quality control  analysis of the analytical measurements was performed on:  pooled qc samples,  duplicate aliquots of representative samples, and  all internal standards in all study and qc samples. using these data, no instrumental drift or other systematic errors could be detected and the data quality was considered to be good.

pre-processing and imputation
prior to performing pls regression analysis, the lc-ms variables of both the fatty acids and the lipids were centred  and scaled to unit standard deviation . in this way, the individual fatty acids and the lipids will be comparable and have similar scales. additionally, the saa measurements were transformed by adding the value of  <dig> and taking the log <dig> to remove the wide range of the data. this value was added to avoid complications when taking the logarithm of zero or of values very close to zero because this will either be impossible or result in artificially high  values, respectively. it was verified that this pre-treatment did not lead to problems such as the introduction of a problematic bias or the introduction of very large negative values due to the logarithm of numbers very close to zero .

due to missing samples for both the lc-ms metabolites and saa values, the data are unbalanced. for lc-ms, the data consisted of  <dig> samples:  <dig> groups of  <dig> mice  and  <dig> groups of  <dig> mice. for saa, concentrations were obtained for  <dig> mice: one group consisted of two mice , one group of  <dig> mice , three groups of five mice  and  <dig> groups of  <dig> mice. however, for both two-way asca and anova it is beneficial to have on groups that are equally sized . this ensures the estimation of independent effects that are crucial for a proper interpretation. analyzing unbalanced two-way anova can be done in different ways but this is not trivial  <cit> . furthermore, special methods like reml  might be required. however, the combination of reml with methods for the analysis of high-dimensional data such as sca is a topic of ongoing research and not available yet. therefore, in order to deal with the imbalance in anova and asca, the groups consisting of less than  <dig> mice were completed . for the lc-ms data, imputation was performed by a random draw from a normal distribution defined by the specific group mean and the standard deviation. for saa, conditional estimations were imputed  <cit> :  a pls model was created between all available pairs of lc-ms metabolites and saa values  and  this model was used to predict the missing saa values. consequently, each missing saa value was replaced by a random draw from a normal distribution defined by the pls prediction from step  <dig> as a mean and the overall pls model residual as a standard deviation. in an experiment where each random draw was randomly repeated  <dig> times, it will be shown that this approach consistently leads to the same conclusions throughout the paper. according to rubin & schenker  <cit> , the strategy adopted is a so-called proper imputation strategy assuming ignorable reasons for the missing data. this means that imputation was performed while reflecting the variability in the data while no systematic differences could be assumed between the missing data within a condition. note that the aim of this paper is not to present a final prediction model for the study but to show how different analysis approaches  on the same data can lead to different results.

when applying regression analysis to determine the relationship between liver no imputed samples were used but only those samples that were originally available for both liver lipidomics and blood saa .

software
all statistical data analyses were performed with matlab  <dig> . <dig>  release  <dig> . the used techniques were: pls in a double cross-validation framework, data decomposition, anova and asca. pls was performed using the pls toolbox  <dig> . <dig> . double cross-validation was performed according to the software from  <cit> . data decomposition, anova, and asca were performed according to the software from  <cit> . the software from these references is available at .

RESULTS
the influence of data imputation
in this study, a conditional imputation strategy was performed to balance the groups, partly relying on a random draw of new samples. this random draw can introduce irrelevant and uninteresting variation which is unwanted. therefore, in order to evaluate the influence of the random selection, the complete imputation procedure was repeated  <dig> times. the results were  <dig> balanced data sets, each consisting of  <dig> mice with  <dig> saa value  and  <dig> lipidomics measurements . for each data set, asca, anova and anova-pls models were made. the results are box plots that show the design-related effect-size differences together with the variation induced by the imputation strategy.

in figure  <dig>  it is shown that the overall trend of effect sizes for both asca and anova is clear even though the effects of the individual  <dig> realizations may vary. the q2cv values of figure  <dig> are derived from the pls models based on different combinations of the asca and anova data blocks. figure  <dig> shows a similar situation as figure 2: the overall trend is clear but individual effect sizes do vary. together, it can be concluded that the random component of the imputation strategy does not affect the data analysis. therefore, one of  <dig> realizations was selected for further analysis in this study. this was the realization which was closest to the median results for the  <dig> asca and anova realizations.

from figures  <dig> and  <dig> it follows that the residual part is a very important component of the total variance. the residual part most likely exists of three elements:  lipidomics and saa measurement noise,  anova and asca modelling error, and  biological variation, possibly as a result of epigenetic effects. however, in this case, it is impossible to further identify and quantify these elements of the residual part because no true replicates could be measured . in addition, the underlying factors leading to the biological variation were not known in advance. if these factors would have been known, they could have been treated as the other study design factors, viz. diet and time. this would make the unexplained residual part smaller and lead to a better understanding of the data structure and to more statistical power. the assumption that structured biological variation is important in the total residual part is supported by the observation that the lipidomic and saa measurements correlate: a pls regression model based only on the residual part still performs reasonably well. this is not to be expected if the residual contains only measurement uncertainty in saa and lipidomics measurements. therefore, it is likely that the residual is dominated by:  structured biological variation, and  higher order effects both in saa and lipidomics.

anova and asca for explorative analysis
as described above, the steps of variance decomposition are used to separate the total data set into blocks that can be attributed to known effects arising from the underlying study design. the advantages of performing regression on the decomposed data are investigated in this paper. however, before doing so, it is very informative to investigate the different effects using anova for the univariate saa measurements and its multivariate counterpart, asca, for lc-ms lipidomics.

when comparing the asca and anova results, the diet effect shows a very similar behavior between the two data sets, and it is the only effect that is significant for both data types. the other effects display different behavior and are not significant for both data types simultaneously.

regression analysis with pls and anova-pls
the main goal of this study was to investigate the relationship between liver lipid metabolites and plasma inflammation markers by exploiting the underlying study structure in a multivariate analysis. table  <dig> shows the regression results on the basis of different combinations of data blocks where each data block contains data for one effect. in table  <dig>  the correlation coefficients are shown for the regression coefficients from the different models.

the q2cv values are presented for the predicted saa values and the measured ones . the prediction of the saa values was done with double 12-fold cross-validation. the q2cv value represents the percentage of variation of saa that is explained by the variation in metabolites.

the squared correlations are shown for the regression coefficients of the different models. all correlations have a p-value <  <dig> .

from these tables it can be concluded that different models are generated when making models on basis of parts of the data. these differences are evident from differences in prediction quality  and from the correlations between the models. from the correlations, two groups of models appear:  the highly correlated models  <dig>   <dig>   <dig> and  <dig>  all containing the diet effect; and  the highly correlated models  <dig>   <dig>   <dig> and  <dig>  all without the diet effect. note that models  <dig>  and  <dig>  are not strongly correlated, indicating that the effects of time, diet, and residual part have an important role. among these effects, the diet effect is the most important one because it leads to a separation of the models into two groups each containing models that correlate well with each other: the ones with and without this effect. a high correlation can be observed from the correlations between model  <dig>  and models  <dig>  and  <dig> . these models are strongly correlated  which implies that adding time and/or interaction to this model does not result in different relations. however, the correlation between these models and model  <dig> is small  which indicates that the modelled relation changes due to the diet effect. this conclusion is supported by the asca and anova results: the diet effect is similar in structure and significant for both the lipidomics and saa measurements. it is important to note that even if the modelled relations strongly correlate, this does not necessarily imply that the predictions by the models are similar. for example, models  <dig>  and model  <dig>  highly correlate , but model  <dig> performs better . a similar conclusion can be deduced from comparing models  <dig>  and  <dig> : the correlation is high  but the latter model is superior .

taken together, the best performing models are obtained by removing the interaction effect while maintaining the diet effect. including or removing the time effect does not affect the prediction performance. therefore, the relation between liver lipidomics and saa is determined mostly by the diet. this can also be seen from the anova and asca results. removing the interaction might be beneficial for the pls prediction because this is a higher order effect. in contrast to linear relations, higher order effects cannot be modelled well by pls. also, our findings clearly demonstrate that the residual part contains an important fraction of the variation that is required to find a good relationship between the lipidomics and the saa data. it is very likely that this variation contains structured biological variation and/or higher order effects.

for comparison, also univariate correlations have been calculated for all individual metabolites and the saa measurements, taking into account the design structure in the same way as for the multivariate analysis. the overall most significant correlation  was  <dig>  . this indicates that the frequently used one-metabolite-at-a-time approach  in this case finds weaker relations between metabolites and saa compared to a multivariate approach in which the correlations between the separate metabolites are taken into account. therefore, univariate correlations are not taken into account further.

in the remainder of this paper, only two models will be compared: model  <dig>  and the original model . this comparison will demonstrate the differences when interpreting a model that is dedicated towards a specific effect and a model containing all effects. it will be shown that the former model is more reliable and leads to a better interpretation than the original model.

for the two models, the performances and the final regression coefficients are shown by figures  <dig> and  <dig>  respectively. together with table  <dig> it is shown that the similarity is very alike between the two models. however, some differences exist. the confidence intervals of model  <dig> are smaller than those of model  <dig> . furthermore, model  <dig> leads to  <dig> significant metabolites  while model  <dig> leads to  <dig> significant metabolites. in total,  <dig> unique metabolites were found significant in either of the two models of which the models have  <dig> in common. in addition, the following metabolites were never found significant for any of the two models:  <dig> of  <dig> ffas ,  <dig> of  <dig> lpcs ,  <dig> of  <dig> phcs  and  <dig> of  <dig> tgs . the differences between the significant and insignificant metabolites were not caused by trivialities such as molecular chemical differences as the molecule sizes or the number of saturated bindings. table  <dig> shows the significant metabolites for the best model  and the overlap with model  <dig> 

the significant metabolites and their rsd values on basis of the best regression model . the compounds marked by an asterisk are not found on basis of the original model . additionally, l16: <dig> was found in the original model but not in model  <dig> . the absolute rsd values of the insignificant metabolites ranged from  <dig>  to  <dig> .

finally, for each of the two models, two new models were made to investigate the robustness of the significant metabolites  <cit> . for the first new model only the significant metabolites are used , while for the other new model only the insignificant metabolites are used . for the informative and uninformative models based on model  <dig>  the performances decreased to q2cv =  <dig>  and q2cv =  <dig> , respectively. for model  <dig>  the informative model performed similar to the original model  while the performance of the uninformative model decreased . this means that the significant metabolites found from model  <dig> are robust while the insignificant ones are indeed uninformative. for model  <dig>  all metabolites are needed to enable a reasonable model, which means that this model is not suitable to gain interpretation from the models because the reliability of differences between significant and insignificant metabolites is low.

collectively, our analyses demonstrate that a regression analysis can benefit from data decomposition on basis of the study design which is used to select specific sources of variation  on which a model is built. on the one hand, this can provide more insight into how different effects relate to a phenotype. on the other hand, the regression analysis can improve by better statistics . it also appears that due to the improved model reliability, more significant variables can be found  which potentially leads to a better understanding of the final model.

biological interpretation
after establishing a statistically validated model, the next step is to interpret the model and its important metabolites from a biological perspective. if parts of the model are confirmed by existing knowledge, it becomes more likely that the unconfirmed parts might indicate useful leads that deserve further investigation. however, the interpretation of the most important metabolites from table  <dig> is limited by the identifiers that were used. these identifiers are based on the molecular element composition which was again based on the exact mass. most of these identifiers require additional analysis to uniquely determine the corresponding metabolite name. however, some of the identifiers did allow a unique association with a metabolite name and consecutively lead to a plausible biological interpretation.

f20: <dig> and f22: <dig> are known to be unique identifiers for omega- <dig> fatty acid epa and omega- <dig> fatty acid dha, respectively. moreover, it is known that f22: <dig> and f24: <dig> are also omega- <dig> fatty acids. table  <dig> and figure  <dig> show that these fatty acids are negatively correlated with the used phenotype: the inflammation marker saa. this again corresponds well with the fact that omega- <dig> fatty acids are known to be anti-inflammatory  <cit> . note that, omega- <dig> fatty acids epa and dha  are only found in model 4: the model that finds the best and most reliable relation with the phenotype.

CONCLUSIONS
regression analysis is a statistical tool that can uncover relationships between two types of data sets. once reliable regression models are derived, regression coefficients can be used to derive knowledge regarding the two types of data. however, when studying complex biological systems, the data comply with a study design. the goal of a study design is to generate relevant information while diminishing the unwanted variation. knowledge about the study design can be used to decompose the total data into data blocks that are associated with specific effects. subsequent analysis can benefit from this decomposition if these are applied on selected combinations of effects. in this way more focus can be put on specific blocks and disturbances can be minimized.

this paper shows that combining anova with pls regression leads to models that differ in structure and statistical quality. the regression coefficients of these different models can then be used to study specific effect related relations between two types of data. additionally, removing specific effects from the relation can lead to statistical models that are better, more robust and with more reliable important variables. the biological interpretation shows that reliability of the most important variables is important to avoid missing useful information. this is especially the case for nutritional studies where subtle effects are expected.

a potential drawback of this approach is that unbalanced multi-way anova  models are difficult to interpret. in this paper, this problem was solved by a conditional imputation strategy. it was shown that this strategy leads to consistent overall conclusions and therefore did not affect the analysis.

it was also shown that the often used univariate approach  did not lead to models that were competitive to multivariate regression. the ultimate best univariate model performed much worse than the multivariate one, even though the analyses were performed on the same selection of data sets.

importantly, the presented approach of pls regression on selected data blocks is not limited to only metabolomics data. it is applicable to all types of data with a known underlying structure. moreover, it can be used in studies where one of the data sets is continuous , for data that contain a subdivision in groups  and for explorative analysis of the data .

abbreviations
anova: analysis of variance; apoe3: apolipoprotein e3; asca: anova-simultaneous component analysis; cv: cross-validation; ffa: free fatty acids; is: internal standard; lc-ms: liquid chromatography mass spectrometry; lpc: lysophosphatidylcholines; pc: principal component; pca: principal component analysis; phc: phosphatidylcholines; pls: partial least squares; rsd: relative standard deviation; saa: serum amyloid a; sca: simultaneous component analysis; tg: triglycerides.

scalars

x: data sample; μ: overall offset; α, β: anova parameters; c: factor of diet; c: subscript on scalar, vector, or matrix indicating the factor of diet; ε: residual; j: number of variables; j: subscript on scalar indicating a variable j specific value; k: factor of time; k: subscript on scalar, vector, or matrix indicating the factor of time; ni: total number of objects ; press: prediction error sum of squares; r: reduced dimension in sca ; ssy: sums of squared differences between the true and their mean value; yi: measured value for saa for sample i; y⌢i: predicted value for saa for sample i; y¯: mean value of all yi;

vectors and matrices

1: column vector of size n with the value of one; e: residual; m: row vector of size j containing estimates of μj; p: loadings; ti: score of population i; xi: data matrix of population i; x: data matrix of population matrices xi

authors' contributions
ut defined the topic of the manuscript, performed statistical analysis and drafted the manuscript. sw assisted with the metabolomics analysis and wrote specific sections. saavdb performed the mice study and wrote specific sections. ib was responsible for the lc-ms data measurements and wrote specific sections. rk, tk, and kwvd were responsible for the mice study and the saa measurements. tk and bvo supplied conceptual and biological feedback on the manuscript. aks supplied conceptual and statistical feedback on the manuscript. all co-authors reviewed draft versions.

