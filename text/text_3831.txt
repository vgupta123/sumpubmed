BACKGROUND
the recent next generation sequencing  breakthrough and the consequent tremendous increase in data production, have been accompanied by the appearance of a multitude of pipelines able to assemble the  short sequences  produced by state-of-the-art sequencers.

in the last two years more than  <dig> new assemblers  have been proposed, more than doubling in size the population of the assemblers designed for long sanger reads. despite the practical and theoretical problems involved in assembling complex genomes using only short sequences  <cit> , several de novo assembly projects based exclusively on ngs data have started. among the most popular ones we mention the panda genome project  <cit> , the assembly of specific human individuals  <cit>  , and several other species  <cit> .

while several tools became publicly available and several projects based on such tools started to appear, a very lively discussion on how to validate new assemblies and, in general, on how to estimate assemblers' output started. as noticed in  <cit> , all assembly tools are based on a small number of algorithms and differ from one another only in matter of details that, very often, relate to how they deal with errors, inconsistencies, and ambiguities. as a consequence, an increasing number of studies is now being published aiming, on the one hand, at evaluating de novo assemblers and assemblies, and, on the other hand, at criticising the results achieved so far.

assemblathon  <cit>  first and second editions, dngasp  <cit> , and gage  <cit>  try to assess the performances of existing tools triggering an assembly evaluation competition among several bioinformatics groups. even though these competitions succeeded in giving a fairly complete overview of the assemblers' potentialities, they are almost always based on specific  genomes or on simulated data, leaving open the question of whether the same tools would have had the same performances when run on different datasets .

recently proposed assemblies carried out using ngs data only , are at the center of a lively debate. alkan in  <cit>  criticised two of the major late ngs achievements: the assembly of the han chinese and yoruban individuals  <cit> , both sequenced with illumina reads. for example, alkan identified  <dig>  mbp of missing repeated sequences from the yoruban assembly and estimated that in both assemblies almost 16% of the genome was missing.

some studies started to criticise the way in which the evaluation of assemblies and assemblers is carried out: standard statistics like the mean contig length and the n <dig> emphasize only length and nothing, or almost nothing, is said about contigs' correctness  <cit> . evaluations of simulated data are inherently biased by the capabilities of the read simulator to faithfully reproduce error schemata  <cit> .

more than three years after the so-called ngs revolution started, it is extremely clear that de novo assembly needs extensive and standardized validation steps. ngs breakthrough allowed to sequence a number of new species and individuals thought to be impossible only few years ago. while, on the one hand, an increasing number of people keeps sequencing and assemblying genomes using available assemblers and short reads, on the other one, day after day, a larger community criticises and casts doubts on assembly achievements.

at the peak of this difficult moment we try to go back to basics and propose a new tool, dubbed gapfiller  <cit> , able to generate small but correct and certified contigs, that can be used either in a first step of an assembly project, or in numerous downstream analyses strongly depending on sequencing and aligning. the innovative feature of gapfiller is the possibility to produce a highly reliable output that, having been certified correct--and hence needing no further validation--, can be used, for example, to improve or validate a whole genome assembly.

our method is based on a seed-and-extend schema aimed at closing the gap between the two mates of a paired read. similarly to other seed-and-extend-based tools like ssake  <cit> , sharcgs  <cit> , qsra  <cit> , and taipan  <cit> , gapfiller selects one read and tries to extend it using reads that overlap for a significant region. the main drawback of seed-and-extend assemblers is their inherent incapability to cope with complex  genomes. gapfiller does not aim at producing a de novo assembly, but only concentrates on closing the gap within paired reads. the advantages of our method lie in the generation of correct and certified contigs and, as a by-product, in the identification of "difficult" areas , thus avoiding the production of wrong contigs. the assembler taipan  <cit>  is implemented to stop its extension phase in presence of a repeat; however, like all other full-fledged assemblers, it is not designed to return certified contigs as output.

closing the gap within paired reads is a strategy already used by software packages like shera  <cit>  and flash  <cit> . however, these tools are able to work only with "overlapping libraries", that is, libraries whose fragment size is shorter than twice the reads' length. gapfiller solves a more challenging problem, aiming at producing filled paired reads of higher length.

we will show how the contigs produced by our method, despite being of sanger-like length or slightly longer , are highly reliable and correct. moreover, the sequences produced generate a genome coverage consisting of evenly distributed long contigs. such contigs can be used to feed another assembler  or to identify and--most importantly--to reconstruct insertion and deletion events in resequencing projects.

on a more technical ground, our algorithm is based on a carefully chosen hash function together with a set of heuristics able to avoid or detect errors, as well as on a test for establishing the correctness of a sequence, that allow us to create a set of certified contigs.

methods
gapfiller is a local assembler based on a seed-and-extend schema  <cit> . seed-and-extend assemblers repeatedly pick up a seed  and extend it using other reads. this procedure is realised by computing and analysing all--or almost all--the overlaps between seed's tips and the remaining available reads. the reads used for an extension are those with the highest alignment score. it is clear that the seed-and-extend assemblers' computation bottleneck is their capability to quickly cope with all the alignment scores to be determined.

gapfiller begins by storing all useful reads in a memory efficient data structure that allows to readily compute overlaps between the contig under construction and the remaining available reads. in a second phase each seed read  is selected one after the other and used to start an extension phase. such phase halts when a stop condition is reached. depending on the stop condition, the contig produced is labelled as trusted or not trusted .

definitions
let Σ be an alphabet and Σ* be the set of the words from Σ. for every s ∈ Σ∗ we will denote with |s| the number of characters of s and with s the sub-sequence of s starting in p ∈ { <dig>  . . ., |s| - 1} and of length l ∈ { <dig>  . . ., |s| - p}. we will refer to s as prefix if p =  <dig>  suffix if p + l = |s|, and as the p-th character of s if l =  <dig>  and we will simply write s.

in order to quickly identify overlaps between the contig under construction and the reads' tips, we use an approach closely related to the one presented in  <cit>  based on an hamming-aware hash function. the idea is that, by representing a string of length l as a base-|Σ| number, one can often replace expensive char-by-char comparison by fast integer  comparison. however, for practical values of l, the integers to be compared would not fit in a memory word. for this reason, as in the classical karp-rabin exact string matching algorithm  <cit> , we can work with numbers modulo q considering equality modulo q only as an indication  that pairs of strings may be the same . policriti et al. in  <cit>  proposed an extension of the approach by karp and rabin, introducing a technique to deal with mismatches, based on the idea of replacing simple fingerprints comparison with a more articulated test. in particular they noticed that, by choosing q to be a mersenne  number , to check whether two strings align against each other at a small hamming distance can be implemented in average linear time.

given a string s ∈ Σ* and its base-|Σ| numerical representation s ∈ ℕ, let us define the hash function fh:∑*→{ <dig> ...,q-1} as

  s↦fh:=s modq, 

where q is a  number of the form q = 2w-  <dig>  for some w ∈ ℕ. the value fh  is called the fingerprint of the sequence in s ∈ Σ* coded with s.

in our context, the use of fh significantly reduces the size of the set employed in the search of the overlapping reads. every read r, as well as its reverse-complement, is indexed by the fingerprint of a substring of length b, starting at a fixed position x in r . formally, given a set of reads ℛ, a sequence  s, a maximum allowed hamming distance k, the set z of the witnesses , a fixed value b for the length of the substring on which the fingerprint is computed in r, and two positions x and y, the following set:

  r:={r∈r|-fh)modq∈z} 

contains at least all the reads r ∈ ℛ such that the hamming distance between r and s is not greater than k. false positives can be present but, as showed in  <cit> , their amount is limited. on this ground the search for reads overlapping  s can be restricted to those belonging to r, for some x, y ∈ ℤ.

as far as gapfiller is concerned, we set k =  <dig> as default, meaning that we search for exact b-length substrings in the reads . as a consequence, better quality output will be obtained if we select a position x in r such that the average base quality is expected to be the highest possible. this point will be further discussed in the section specifically addressing data structures' design and implementation.

dataset preparation
in order to avoid the generation of wrong contigs, it is of utmost importance to use only correct reads over the entire extension phase. several tools are available to perform error correction on illumina data using the so-called "read spectrum" . other tools discard reads or try to improve their reliability using quality information .

our approach, when we are given raw data, is to first trim  the reads on the ground of quality information using a specific rna option , and to subsequently correct them with an error correction tool like quake  <cit> .

another important way to assess a dataset's global quality is to plot the reads' k-mers distribution. this can be easily done using jellyfish  <cit> . if the genome has been sequenced tens of times, then two peaks are expected: one in correspondence of the expected coverage and one in correspondence of coverage one. k-mers composing this second peak are likely to be sequencing errors. as a rule of thumb, a low number of k-mers occurring only once suggests that the dataset has a good global quality.

contig extension
in the contig extension phase, each read is selected in a loop and used as seed in order to create a new contig. once a seed read is selected, the suffix-prefix overlaps with other reads are computed and, if a sufficiently high level of global similarity is reached, they are clustered in a consensus string, which is subsequently used to perform further extensions. the procedure continues while some overlapping reads exist and the consensus string is highly representative of the clustered reads. if either one of the previous two conditions is not met, the extension phase stops, the current sequence is returned in output, and the loop continues.

before the extension phase some parameters are set: the minimum overlap length l and the maximum shift Δ: an overlap between the current contig's suffix and the read's prefix is considered only if the overlap length l belongs to the interval .

gapfiller builds a cluster every time a contig is to be extended with the overlapping reads. in particular, gapfiller uses only those reads aligning against the contig's suffix with at most δ mismatches  is a function of the overlap length l) and requires at least m reads in order to compute a consensus string. notice that b ≤ l ≤ l holds, hence suffix-prefix overlaps might occur with more than k =  <dig> mismatches .

let ℛ be the set of the input reads for gapfiller and r <dig> ∈ ℛ be a seed read. at step i =  <dig> the current sequence is initialized with the seed s <dig> : = r <dig>  denoting by si the current contig at the generic i-th step of the algorithm, the procedure to build si+ <dig> is described below:

step <dig> reads are selected according to their similarity with the current contig si . at this point, every read overlapping si for l ∈  characters with at most δ mismatches is selected.

step <dig> the reads are clustered and a consensus string is computed. every character of the consensus string is assigned a flag indicating how it is representative of the reads from which it is built. more precisely, for every position j, gapfiller selects the most occurring character in the considered reads, and the majority consensus string c is computed . depending on two parameters t <dig> and t <dig> such that t <dig> <t <dig>  we say that a position j is non-represented, low-represented, or high-represented if the representation rate of the corresponding character in c is lower than t <dig>  lower than t <dig>  or higher than t <dig>  respectively.

step <dig> the reads used to build the consensus c are filtered and trimmed, depending on the presence of low-represented and non-represented positions, respectively. the idea is that on low-represented positions we need a minimum percentage of reads matching the consensus string, and that on non-represented positions the extension in considered to be unsafe. reads differing from c in correspondence of low-represented positions are discarded and the remaining ones are also trimmed if a non-represented position occurs .

step <dig> a new consensus string cnew is computed, considering only the reads obtained at step  <dig>  and possibly the current contig is extended . the extension is done only if the number of reads is at least m and the consensus cnew exceeds si's right end: in this case, a new contig si+ <dig> is built and the procedure restarts. otherwise the algorithm stops and the contig si is returned.

the adopted strategy is aimed at either avoiding errors and overcoming the problems arising when gapfiller attempts to cluster reads that are different from each other. in the last part of this section we will discuss in more detail how the algorithm works. the reader who is not interested in the technical formalism might skip this part and move directly to the subsection stop criteria.

step  <dig>  overlapping reads selection
let us denote with r the set of the putative overlapping reads with respect to the l-suffix of si, selected by their fingerprint values , with y = |s| - l + x, for some values of x ∈ { <dig>  . . ., l - b}). for every fixed value of l, the set of the reads overlapping the l-suffix of si with at most δ mismatches is defined as

  r^:{r∈r:dh≤δ} 

where dh :Σl × Σl → ℝ+ is the hamming distance. the set of all the overlapping reads will be denoted by

  r^:= ⋃l=ll+Δr^. 

given a read r∈r^, we define its starting and ending positions as

  i:=|si|-lf:=i+|r|- <dig>  

i and f represent the position of the read r with respect to the current contig si, therefore we set i =  <dig>  for instance, in the case depicted in figure  <dig>  we have r^={r <dig> r4} and r^={r <dig> r <dig> r <dig> r4}, i =  <dig> and f =  <dig> 

step  <dig>  reads clustering and consensus string computation
the subsequent phase consists of the computation of the consensus string obtained from the set of reads r^ ). notice that, in order to compute reliable extensions, we require the number of reads to be at least m, a parameter that may depend on the dataset used. if there exists no l such that the l-suffix of si is covered by at least m reads of r^, then the procedure stops. otherwise, the starting and ending positions of the consensus string c with respect to si can be computed, thanks to . in practice, we let the consensus string start from the leftmost reads, i.e., those covering the longest suffix of si  and end at the rightmost position in which the number of reads is at least m. more precisely, the starting and ending positions of c are defined as

 i:=min{i:r∈r^};f:=max{f:r∈r^Λ|{r′∈r^:f≥f}|≥m}, 

respectively. if f > |si|- <dig> the procedure continues, otherwise it stops as si cannot be further extended. looking at figure  <dig> we have i =  <dig> and f =  <dig> and the procedure continues since f >f  =  <dig> 

the consensus string c is then computed by selecting the most represented character at every position. for every x ∈ Σ and for every j = i, . . ., f we define the number of occurrences of the character x in position j with respect to si as

 σ:=|{r∈r^:i≤j≤fΛr=x}|. 

the consensus string c is defined, for every j = i, . . ., f , by setting c equal to the highest occurring character, i.e., the x ∈ Σ with the highest number of occurrences in position j

 c:=argmax x∈∑σ. 

loosely speaking, the character selected on a particular position of the consensus string is the most occurring character in the reads on that position; hence σ], j) is the number of occurrences of character c on position j.

step  <dig>  consensus-based reads selection
as above mentioned, in order to check, on the one hand, whether a read r is highly representative of the consensus c and, on the other hand, if the extension is "safe", it is important to introduce the notion of non-represented, low-represented, and high-represented characters in the consensus string. we simply define the representation rate of the position j as

  π:=σ],j)|{r∈r^:i≤j≤f}|. 

hence we fix two threshold values t <dig> and t <dig> such that  <dig>  ≤ t <dig> <t <dig> <  <dig>  ∈  as |Σ| = 4) and we distinguish three types of positions in the consensus string:

 jisnon-represented⇔  π≤t1jislow-represented⇔t1<π≤t2jishigh-represented⇔π>t <dig>  

the idea is to discard those reads that "differ from c" and to cut them out, as there is not sufficiently high evidence that gapfiller is extending correctly. in practice, we do not consider a read r if it does not match the consensus string on a low-represented position, i.e., r ≠ c, for some j such that π  ≤ t <dig>  clearly, this applies to non-represented positions as well. then, we trim every read overlapping any non-represented position of c. more precisely, if jnot is the first non-represented position occurring in r  ≤ t1), we consider r instead of r.

after unsafe reads are discarded and the remaining ones are trimmed, a new set of reads, that can be denoted by r^new, is finally obtained . every read in r^new is both matching the consensus string c on each low-represented position and not covering any non-represented one. using this mechanism we take into account only the most representative reads and do not extend the contig with a consensus character when its representation rate is too low.

step  <dig>  final consensus string computation and contig update
after previous step, the new set of overlapping reads r^new is obtained. a new consensus string cnew can be computed as c was before. if f > |si| -  <dig> the extension is performed, the current contig is updated

 si+1:=si.cnew 

and the -th extension phase restarts from si+ <dig> 

stop criteria
the algorithm described in the previous section may potentially extend a contig for an arbitrarily large number of times, without checking any "global" properties of the current sequence. with our method the extension phase halts if at least one of the following conditions is met:  the available overlapping reads for the consensus c are less than m;  the available overlapping reads for the new consensus cnew are less than m;  contig's length exceeds the maximum length;  the seed-mate has been found.

let si be the contig obtained at the i-th step, starting from the seed read r <dig>  criterion  applies when the consensus string c does not exceed the current contig. this means that there are no more than m -  <dig> overlapping reads, or that they are too short. in such a case, the contig produced is labelled as no_more_extension.

criterion  applies when the consensus may have been produced as consequence of the presence of reads belonging to different genomic locations. more precisely, this situation is likely to appear when the consensus extension is "trying" to exit from a repeat. in this case, either too many reads are discarded  or a significant trimming of them has been performed . in such a situation, the extension is halted and the contig is labelled as repeat_found.

criterion  is satisfied as |si+1| >lmax, where lmax is fixed at the beginning of the algorithm and is usually set to the maximum insert size, plus a tolerance value. in such a situation, we could have been able to continue the extension but, however, we could not find the seed-mate. this suggests that the contig produced may be wrong or, at least, that it contains a high number of unreliable bases. when the maximum allowed length is exceeded, the computation is halted and the contig, labelled as length_exceed, is returned.

criterion  is used to stop the extension as the mate r ˜ <dig> of the seed r <dig> is found. at the generic i-th step, every p∈{ <dig> ...,|si|-|r ˜0|} is checked to see whether the following condition is satisfied

  dh≤m, 

where m is the maximum number of mismatches allowed between r ˜ <dig> and si. inequality  is satisfied if and only if the mate is found in si at position p with no more than m mismatches. this control is performed on-the-fly and hence the positions already checked at the i-th step will not be re-checked. the mate-check criterion is used as a guarantee of correctness of the whole contig. this is in contrast to previous criteria, which are used to detect and prevent errors introduced in the extension phase. from this point of view, criteria  and  can be seen as strictly local, since no information collected during previous steps is used. in this last case the contig returned is labelled as mate_found.

data structures
in this section we will take a closer look at the data structures designed for our algorithm and at their implementation. gapfiller's core is the module working during the extension phase. at this point, we assume that the set ℛ has already been trimmed and possibly filtered.

the basic idea is to pre-compute as much as possible of the useful information on the reads, in order to speed up the computation of the overlaps needed to perform the extension phase. suppose that gapfiller is working at the -th step of an extension, with i ≥  <dig>  and let si be the current contig. when constructing the consensus string c  we are always interested in obtaining overlaps between suffixes of si and prefixes of reads belonging to ℛ.

in order to compute overlaps, gapfiller employs a hashing schema based on the one implemented in rna  <cit> ; in particular, a data structure similar to the one proposed in  <cit>  is built. a simplified schema of gapfiller's data structure is presented in figure  <dig>  the basic idea behind gapfiller is the possibility to obtain in a fast and efficient way the set of reads whose prefixes overlap a suffix of the partial contig under construction. therefore we used the rna hash function to find reads that are likely to overlap a suffix of si; those reads are subsequently checked to see if they actually overlap si or not.

obviously, all the data must be stored in the main memory, thus requiring a careful data structures' engineering. it is clear that, since overlaps between reads and the the current contig can take place on both strands, reads must be stored together with their reverse complement.

with the goal to save as much memory as possible, reads are represented as arrays of integers, so that a base needs  <dig> bits instead of  <dig> . the data structure used to compute overlaps and to construct contigs is built from the reads. three arrays are used to represent in a compact way the reads stored in ℛ and to compute overlaps among them:

 <dig>  hashcounter: it is an array of pointers to hashvalues. in position i it stores the first position in hashvalues such that a read r or its reverse complement has a prefix whose fingerprint is i.

 <dig>  hashvalues: each array entry stores the read's location in the array reads together with a boolean value indicating whether the fingerprint has been computed from the original read or from its reverse complement. for this reason the size of hashvalues is twice the number of reads in ℛ;

 <dig>  reads: this array stores the reads and other useful informations, like paired read location, paired read order , and read status .

the overall memory requirement for gapfiller depends on the size of hashcounter and on the number of reads. as for rna, a reasonable value for q is  <dig> - <dig>  such a number guarantees a reduction of the number of false positives . as far as the number of reads is concerned, we can limit q, without loss of generality, to 231: with state-of-the-art illumina technology, such a number of reads represents approximately a 70× coverage of the human genome. an illumina read of length  <dig> bp requires two memory locations in hashvalues of  <dig> bytes each  and one entry in reads of  <dig> bytes . in total the amount of memory required is 4q +  <dig> * 4|ℛ| + 9|ℛ| = 4q + 17|ℛ| bytes.

the reads' fingerprint is computed on a precise substring of length b ). as pointed out in section definitions, the fingerprint of r ∈ ℛ should be computed on the position x such that the  average base quality is as high as possible and the substring r falls into the contigs' suffix, independently on the overlap length l. for these two reasons, having the illumina error-profile in mind, we choose x =  <dig> if r is considered on its original strand, x = l - b if r has been reverse-complemented .

in order to compute the overlaps between the current contig si and the reads, one has to compute the fingerprints of the substrings of length b starting from y, for every y ∈ {|si| - l - Δ, . . ., |si| - l} if original-stranded reads are searched, and for every y ∈ {|si | - Δ - b, . . ., |si| - b} if reverse-complemented ones are to be extracted. let us indicate with sy the fingerprint computed from si . gapfiller uses this number to retrieve reads whose l-length prefix  is likely to match a substring of si close to the sequence's end. in particular gapfiller accesses all hashvalues positions between hashcounter and hashcounter and, subsequently, accesses reads to identify the set of candidate overlapping sequences ℛ . finally, the set ℛ, the set of real overlapping reads. this is done by checking all candidate reads singularly. due to the fact that only a limited number of mismatches is allowed in this phase and that the employed hash function guarantees a low false positive rate, this step is extremely fast.

RESULTS
gapfiller outputs a set of labelled contigs. the label describes the level of reliability of the sequence, in particular we divide gapfiller's output in two sets: positive/trusted contigs are those labelled mate_found, while negative/non-trusted contigs are those labelled no_more_extension, repeat_found, length_exceed. trusted contigs are those that we consider certified correct and can therefore be used in subsequent analysis. non-trusted contigs are defined in this way because we were not able to find the seed-mate and hence we have no way to estimate their correctness.

we decided to perform experiments on both simulated and real data. despite being aware that results on simulated datasets are strongly connected with the ability of read simulators to successfully reproduce realistic error schemata  <cit> , we are also conscious that they are the only way to precisely estimate the reliability of assembled reads. in contrast, experiments on real datasets are necessary in order to test the applicability of our tool.

we simulated ngs experiments on five bacterial genomes, producing four coverages for each of them, in order to show how gapfiller's performances scale at different coverages. moreover, in order to test correctness, we aligned each output contig against a precise region of the reference, as seed reads' coordinates and orientation are known.

the experiments on real datasets were performed on public data, for which the results obtained by various assemblers are public as well. in this case, we first checked the correctness of gapfiller's output contigs and then used them as input for an assembler for long reads.

dataset
the reference genomes used for simulated experiments were downloaded from ncbi website  <cit>  and we used simseq, the reads simulator employed in assemblathon  <dig>  <cit> , to generate paired reads coverages. more specifically, we performed our experiments on five bacterial genomes . we generated a library constituted by  <dig> bp-length paired reads, with insert size  <dig> ±  <dig> bp, using error profiles provided by simseq for reads  <dig> and  <dig>  respectively. in particular, we obtained  <dig> simulated datasets generating, for each organism, four paired-ends coverages: 30×, 50×, 70×, and 90×. the reasons behind this choice lie on the fact that, on the one hand, we need at least a 30× coverage in order to provide gapfiller an adequate reads distribution, and, on the other hand, we noticed that coverages equal or higher than 100× do not appreciably increase gapfiller's performances.

the real datasets were dowloaded from gage website  <cit>  . fragment  and short jump  libraries are available, and corrected data are provided as well. for both datasets, we combined the two libraries in two ways: in a first attempt we ran gapfiller using only reads from the fragment library, while in a second experiment we used both libraries, but we selected seeds from the short jump dataset only, creating in this way contigs of average length  <dig>  kbp.

as far as the experiments on real data are concerned, it is important to notice that the datasets provided by gage, together with the assembly results described in  <cit> , represent the first available benchmarks that can be used to evaluate new instruments like gapfiller.

using a specific rna option, each simulated dataset was filtered to prune and trim reads on the basis of their quality information. for the real datasets, instead, we chose to use the allpaths error-corrected reads, hence there was no need to trim them.

design of experiments
we used simulated data in order to evaluate gapfiller's ability to correctly reconstruct the gap between two paired reads and to assess the reliability of the output classification . in particular we used these datasets--easy to build and validate--to explore how coverage affects gapfiller's extension phase. results on real datasets have been used instead to evaluate gapfiller's potential when its output is used as an input dataset for an assembler for long reads. however, the capability of producing correct contigs is a fundamental feature when gapfiller is used in this context.

gapfiller's performances rely on the choice of three crucial parameters: the minimum overlap length l, the slack Δ, and the length b of the substring on which the fingerprint is computed. we decided to set l =  <dig> and Δ =  <dig>  as reads' length is approximately  <dig> bp for every library used for the experiments. the value of b identifies the length of a substring on which we  require an exact matching between read and contig , due to the fact that the employed hash function has a low false-positives rate ). we set b =  <dig> because we observed that a greater value of b  dramatically prevents gapfiller to find even few-mismatch-affected overlaps.

the parameters t <dig> and t <dig>  necessary to discern among high/low/non-represented positions in the consensus string , are set to t <dig> =  <dig>  and t <dig> =  <dig> . recall that when a position in the consensus string has a representation rate lower than t <dig>  all the reads are trimmed on that position; instead, if the representation rate is lower than t <dig>  only the reads not matching the consensus string are dropped. the value of m, the minimum number of reads required in order to compute the consensus string, has always been set to  <dig>  we chose not to let m depend upon coverage, since the number of reads after step  <dig> strongly depends on the parameters used .

we set the maximum length of a contig to be much greater than the expected mean insert size, i.e.,  <dig> bp for simulated data,  <dig> bp and  <dig> bp for gage fragment and short jump libraries, respectively .

we allowed for the presence of mismatches when looking for the seed-mate in the contig being constructed with parameter m. in all the performed experiments we set m =  <dig> . this choice is justified by two reasons: the first one lies in the fact that the data simulated with simseq have a quite high amount of low-quality bases even far from the rightmost positions within the reads; the second one is that, on real datasets, lower values of m  do not increase output quality. the value of δ, representing the maximum number of mismatches allowed when computing overlaps, depends on the overlap length l and was set to ml / |r|, where |r| is the average read length.

analysis
the post-processing phase of gapfiller's output is aimed at both quantitative and qualitative analysis. the first is focused on evaluating the amount of trusted contigs our tool is able to produce, the second on results' validation. the main goal is to compare the performances on different input datasets and coverages.

due to their nature, experiments on simulated data allow to precisely estimate correctness by aligning a contig in the exact place where it is supposed to occur in the reference genome. more precisely, we used the smith-waterman alignment algorithm  <cit> , assigning a score of  <dig> to a match, - <dig> to a mismatch, and - <dig> to an indel. for instance, let us consider a contig s generated by extending a seed read r, and suppose that r has been extracted from the genome g at position x, on the forward strand. to test its correctness, s is aligned against g, where g is the maximum number of allowed indels, depending on a user-defined threshold for the alignment score. we say that s is correctly aligned if and only if the ratio between the best alignment score of s against g and |s| is at least  <dig>  . for this particular choice of the alignment score, g is fixed to be ⌈3|s|/200⌉.

alignments performed in this way allowed us to divide contigs in four subsets: true and false positives and true and false negatives, depending on the contigs classification and correctness . this gave us the possibility not only to estimate the percentage of correctly reconstructed contigs, but also to evaluate gapfiller's ability to discern between trusted and not trusted ones.

when using a real dataset reads provenance is unknown, so in this case we tested output correctness by aligning the contigs against the reference genome using blast. we set the percentage of identity to be at least 95% and the hit length to be 100% of contig's length, in order to accept an alignment. in real cases it is interesting to extract two pieces of information from alignments: the number of  contigs that correctly align against the reference, as in the simulated case, and the coverage profile, as it is useful in order to estimate the percentage of genome reconstructed by gapfiller .

the experiments performed with both short jump  and fragment libraries are done by picking the seeds from the short jump library only. we state that a contig is aligned against the reference if the alignment is a single hit covering 100% of contig's length and the percentage of identity is at least 95%. the statistics are computed on trusted contigs.

thanks to the presence of theoretical optimal assemblies for the two real datasets  we evaluated the performances of gapfiller with respect to other assemblers. in particular, we extracted a set of contigs corresponding to a fixed coverage  and assembled it with phrap  <cit> , a well known overlap-layout-consensus assembler. we produced a set of statistics representing the correctness of our assembly using the same scripts used in  <cit>  and available for download at  <cit> .

discussion
all the experiments were performed on a 8cpu  and 32gb ram machine. all of them required no more than ~  <dig> gb ram memory. see table  <dig> for the time requirements and for the output coverage produced for each experiment.

experiments performed on simulated datasets show how gapfiller's performances improve as coverage increases . from the histograms in figure  <dig> we can clearly appreciate how the number of true positives  increases with coverage, reaching an average value of 99% when coverage is above 50×. in a specular way, we can see that the number of false negatives decreases as coverage increases. table  <dig> shows how a higher input coverage allows us to produce a higher output coverage composed by trusted reads.

the simulated datasets allowed us to show how gapfiller is able not only to correctly reconstruct the gap between paired reads, but also to correctly flag the generated contigs as trusted  and non-trusted . going into more detail, we observed that the majority of non-trusted contigs are labelled no_more_extension, meaning that gapfiller stops a contig extension depending on some input dataset features . another possible scenario is the one in which gapfiller computes a wrong consensus without recognizing it.

another important result obtained from these datasets is that the percentage of uncovered bases is negligible, being strictly less than  <dig> % even with low input coverages .

on the basis of the results obtained on simulated data, we tested gapfiller on real data. we decided to use two datasets provided by gage  <cit> . we opted for these data because they represent state-of-the-art illumina sequences, they are freely available, and they come with a reference sequence, a set of assemblies obtained with state-of-the-art assemblies, and with a set of evaluation scripts.

in order to proof gapfiller's capabilities when used on real data, we extracted a random 10× coverage from the set of s. aureus output contigs  and a random 15× coverage from r. sphaeroides output contigs . both coverages have been assembled with phrap with default parameters and the results have been compared to the ones presented in gage  <cit> . it is worth noting that the assemblies presented in gage should be considered the best achievable assemblies with the employed tools. in order to obtain a comparison as fair as possible we employed the same scripts used by salzberg and colleagues in  <cit> . it is important to say that the presence of a reference sequence for both the assembled genomes allows us to compute the real number of errors and mis-assemblies.

tables  <dig> and  <dig> show the most important statistics obtained in the validation phase. for what concerns s. aureus assemblies, we can see that our assembly has a connectivity level  higher than that of many other widely used assemblers , moreover the number of small contigs , and the number of wrongly assembled repeats  is always comparable and often better than the other assemblies . the most important columns, however, are the last four, showing the number of errors . gapfiller+phrap not only is one of the assemblies with the fewest number of indels, but is also the one having less relocations  and inversions . these latest two types of errors are the most dangerous ones, due to the fact that they are the result of merging two completely different genome areas.

results showed in table  <dig> for r. sphaeroides are similar: this time gapfiller+phrap has a lower connectivity level . also in this case our assembly is not seriously affected by indels . concerning inversions and relocations, gapfiller + phrap's performances are comparable to that of the other assemblers.

CONCLUSIONS
gapfiller is a local assembler based on a hashing technique. indeed, on the one hand, it boosts the extension phase by reducing the search space and hence allows an exact computation of overlaps, and, on the other hand, it allows to store in an efficient and compact way all the needed information.

gapfiller is a tool able to provide certified contigs, in the sense that those labelled "trusted" are  correct. this statement is sustained by various simulated experiments, as well as by two real ones. gapfiller does not try and does not aim at assembling a genome but, instead, it aims at providing as output a set of sanger-like-length reads certified correct. in a de novo assembly project, gapfiller can be used in two modalities. it can realize a preprocessing step, as the produced trusted contigs can be used as input meta-reads for an assembler for long reads; as an opposite strategy, it can be used to join the contigs produced by a de novo assembler in a scaffolding-like phase or to  assemble structural variations within an ngs resequencing project.

in this paper we proved the effectiveness of the first application. we showed how the sanger-like-long reads can be used to feed another assembler  in order to obtain a standard assembly. this assembly is similar and often better than assemblies generated by state-of-the-art assemblers. in order to proof this we compare the results of our tool with the ones recently obtained by gage.

gapfiller's strength lies, on the one hand, in the ability to produce an output that does not need validation, and, on the other hand, in being a local assembler, making it useful when studying limited regions of a genome.

gapfiller's applications to structural variations analysis include indels detection and validation; in particular, it can be used to assemble insertions occurred in a sequenced organism, with respect to a reference genome. it is of primary importance to notice how, while there is a large number of tools able to identify structural variations, so far there is no widely accepted strategy to reconstruct structural variations in re-sequencing projects. we believe that the localized gapfiller strategy can be used in order to "fill this gap" and move several approaches from identification to reconstruction.

availability and requirements
gapfiller can be freely downloaded from http://users.dimi.uniud.it/~francesco.vezzi/software.php. it has been tested on linux operating systems only . it has been written in c++.

abbreviations
ngs: next generation sequencing.

competing interests
the authors declare that they have no competing interests.

authors' contributions
fn, fv, and ap equally contributed to the idea and equally contributed to the design of the experiments. fn and fv developed the tools and fn performed the experiments. fn, fv, and ap wrote the paper.

appendix
simseq can be freely downloaded from https://github.com/jstjohn/simseq.

command lines for read simulation:

java -jar -xmx2048m simseq.jar - <dig>  <dig> - <dig>  <dig> --error hiseq_mito_default_bwa_mapping_mq10_ <dig> txt

--error <dig> hiseq_mito_default_bwa_mapping_mq10_ <dig> txt --insert_size  <dig> --insert_stdev 200

--read_number pair_number --reference reference.fasta -o output.sam;

java -jar samtofastq.jar input=output.sam fastq=reads_ <dig> fastq second_end_fastq=reads_ <dig> fastq

include_non_pf_reads=true validation_stringency=silent

kmercounter can be freely downloaded from its git repository git clone

http://git://git.code.sf.net/p/kmercounter/code kmercounter-code. command line for kmercounter:

./kmers_count --input reads_ <dig> fastq --input reads_ <dig> fastq --threads num_threads

--output 16mer_profile.txt 

command line for gapfiller:

./igaassembler --k  <dig> --output output.fasta --statistics output.stat --overlap  <dig> --slack 40

--short- <dig> seed_reads.fasta --short- <dig> seed_mates.fasta  --short-ins avg_insert_size --short-var insert_size_st_dev

--read-length avg_read_length --global-mismatch  <dig> --extthr  <dig> --limit num_seeds_to_extend

--no-read-cycle --max-length max_ctg_length

