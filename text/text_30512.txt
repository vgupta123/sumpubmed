BACKGROUND
modelling approaches are central in systems biology and provide new ways towards the analysis of omics data, ultimately leading to a greater understanding of the language of cells and organisms  <cit> . further, these approaches provide systematic strategies for key issues in medicine  <cit>  and the pharmaceutical and biotechnological industries. for example, model-based approaches can provide a rational framework to guide drug development, taking into account the effects of possible new drugs on biochemical pathways and physiology  <cit> . a common approach to model inter- and intra-cellular dynamic processes is by means of dynamic models, usually consisting of sets of differential equations  <cit> .

the general area of system identification deals with the development of mathematical models of dynamic systems from input/output data  <cit> . when building mathematical models, one starts from the definition of the purpose of the model and uses the a priori available knowledge  to choose a model framework and to propose a model structure. this model contain parameters and we need to know whether is it possible to uniquely determine their values  and if so, to estimate them with maximum precision and accuracy. this leads to a first working model that must be validated with new experiments, revealing in most cases a number of deficiencies. in this case, a new model structure and/or a new experimental design must be planned, and the process is repeated iteratively until the validation step is considered satisfactory. this iterative process  should also contain other elements like optimal experimental design and model discrimination steps  <cit> .

this work is focused on the key step of parameter estimation, assuming the structure of the nonlinear dynamic model as given. parameter estimation  aims to find the parameters of the model which give the best fit to a set of experimental data. examples of recent efforts in the particular case of biochemical pathways are the works of sugimoto and coworkers  <cit> , voit and almeida  <cit> , rodriguez-fernandez and coworkers  <cit>  and polisetty and coworkers  <cit> . the key issues considered here in this work were to ensure reliable and accurate parameter estimation, paying especial attention to the computational cost, and also to perform the identifiability analysis of the models.

parameter estimation in nonlinear dynamic biological models
estimating the parameters of a nonlinear dynamic model is more difficult than for the linear case, as no general analytic result exists. biological models are often dynamic and highly nonlinear, thus, in order to find the estimates, we must resort to nonlinear optimization techniques where a measure of the distance between model predictions and experimental data  is used as the optimality criterion to be minimized. the criterion selection will depend on the assumptions about the data disturbance and on the amount of information provided by the user. as explained in detail in the methods section, the maximum likelihood estimator maximizes the probability of the occurrence of the observed measurements. if we make the assumption that the residuals are normally distributed and independent with the same variance Ïƒ <dig>  then the maximum likelihood criterion is equivalent to the least squares and we aim to find p^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgwbacgaqcaaaa@2e25@ which minimizes the sum of squared residuals of all the responses. that is, the estimation criterion would be to minimize the trace of zt z  <cit> . this is subject to the dynamics of the system, plus possibly other algebraic constraints, and model parameters are also subject to upper and lower bounds. this formulation is that of a non-linear programming problem  with differential-algebraic  constraints. in this work, we have followed the so-called single shooting approach  <cit> , where an initial value problem  is solved as an inner problem of the master nlp problem. when estimating parameters of dynamical systems a number of difficulties may arise, like e.g. convergence to local solutions if standard local methods are used, very flat objective function in the neighborhood of the solution, over-determined models, badly scaled model functions or non-differentiable terms in the systems dynamics  <cit> .

due to the nonlinear and constrained nature of the systems dynamics, these problems are very often multimodal . thus, traditional gradient based methods, like levenberg-marquardt or gauss-newton, may fail to identify the global solution and may converge to a local minimum when a better solution exists just a small distance away. moreover, in the presence of a bad fit, there is no way of knowing if it is due to a wrong model formulation, or if it is simply a consequence of local convergence. thus, there is a distinct need for using global optimization methods which provide more guarantees of converging to the globally optimal solution, as shown in  <cit> . the importance of using global optimization methods for parameter estimation in systems biology has been increasingly recognized in recent years  <cit> . global optimization methods can be roughly classified as deterministic, stochastic and hybrid strategies. deterministic methods can guarantee, under some conditions and for certain problems, the location of the global optimum solution. nevertheless, no deterministic algorithm can solve global optimization problems of the class considered here with certainty in finite time. actually, computational effort increases very rapidly  with the problem size. although very significant advances have been recently made  <cit> , these methods have a number of requirements about the dynamics of the system, and currently they do not seem to be applicable to problems with a relatively large number of parameters. stochastic methods are based on probabilistic algorithms, and they rely on statistical arguments to prove their convergence in a weak way. however, many stochastic methods can locate the vicinity of global solutions in modest computational times  <cit> . additionally, stochastic methods do not require transformation of the original problem, which can be treated as a black-box.

in our group, and during the last decade, we have compared a number of different stochastic and deterministic global optimization methods. the overall conclusions from these studies indicate that modern evolution strategies have several key advantages over genetic algorithms and simulated annealing, namely better efficiency/robustness ratio, good scaling properties, an inherent parallel nature and an almost self-tuning mechanism for the search parameters of the method. our tests and comparisons indicate that de  <cit>  and sres  <cit>  are one of the most competitive algorithms, with the additional advantage of being able to handle arbitrary constraints if needed. the main problem presented by these methods is that they require too many evaluations of the objective function  <cit> . in order to surmount this difficulty, we have recently proposed a hybrid method  <cit>  that speeds up these methodologies while retaining their robustness.

the key concept behind hybrid methods is the well known idea of synergy, that is, a mutually advantageous conjunction of distinct elements. there are several non-trivial questions associated with the actual development of such method, namely choosing which methods to combine, and how to structure such combination. our work is then focused on selecting more efficient stochastic go methods and designing better hybrid methods in order to improve the ratio efficiency/robustness. rodriguez-fernandez and coworkers  <cit>  combined a global and a local optimization method in a sequential, two-phase hybrid method in order to speedup the stochastic global optimization methods while retaining their robustness. however, computational times were still rather significant, especially if one considers its possible application to larger scale problems.

in order to further increase computational efficiency, in the present work we present a novel metaheuristic approach based on extensions of scatter search combined with various local methods. as it will be shown below, this metaheuristic shows speeds up of between one and two orders of magnitude with respect to previous results obtained with the above mentioned methods. moreover, this method eliminates the delicate task of deciding where to set the switching point from the global to the local method.

global optimization: novel metaheuristic
scatter search  is a population-based method based on formulations originally proposed in the 1960s for combining decision rules and problem constraints, such as the surrogate constraint method. it was first introduced by glover  <cit>  as a heuristic for integer programming, although it has been extended for other problem classes more recently  <cit> . scatter search orients its explorations systematically relative to a set of reference points that typically consist of good solutions obtained by prior problem solving efforts.

the justification for choosing this algorithm as the starting framework for our metaheuristic was based on a recent review comparing a number of global optimization solvers over a set of over  <dig> constrained go problems  <cit> , in which a solver based on scatter search proved to be the best among all the stochastic solvers, and the best among all methods for black-box problems. furthermore, for problems with a large number of decision variables, this solver also proved to be the most reliable.

scatter search, when the local search is activated, can be defined as a hybrid method since it combines a global search with an intensification phase . the algorithm uses different heuristics to efficiently choose suitable initial points for the local search, based on merit and distance filters as well as a memory term. this feature helps overcome the problem of switching from global to local search as explained in  <cit> . the user does not have to worry about stopping the global search and starting the local solver since the algorithm performs this work automatically.

a scatter search framework in a five-step template is given by laguna and martÃ­  <cit>  to describe the basic steps of the algorithm :

â€¢ a diversification generation method to generate a collection of diverse trial solutions.

â€¢ an improvement method to transform a trial solution into one or more enhanced trial solutions.

â€¢ a reference set update method to build and maintain a reference set consisting of the b "best" solutions found , organized to provide efficient accessing by other parts of the method. solutions gain membership to the reference set according to their quality or their diversity.

â€¢ a subset generation method to operate on the reference set, to produce several subsets of its solutions as a basis for creating combined solutions.

â€¢ a solution combination method to transform a given subset of solutions produced by the subset generation method into one or more combined solution vectors.

of the five methods in the ss methodology, only four are strictly required. the improvement method is usually needed if high quality outcomes are desired, but a scatter search procedure can be implemented without it. differences among scatter search implementations are based in the level of sophistication in which these steps are implemented, not in the presence or absence of other steps. in the algorithm presented here, we have added some advanced features in order to improve its performance when solving parameter estimation problems:

â€¢ a logarithmic distribution for generating initial trial solutions can be chosen by the user to favor their presence close to the bounds in terms of euclidean distance, since the location of the global optimum near or even touching the bounds  is quite usual in parameter estimation problems.

â€¢ mechanisms to avoid flat zones , as well as others to avoid getting stuck in local solutions, have been added. in every iteration the algorithm analyzes if the elite solutions have very similar objective function values regardless their euclidean distances. if the variance of these values is too low, our procedure considers that the search is located in a flat zone and resets the elite solutions to explore different  areas. this mechanism also prevents the algorithm getting stuck in a local solution when the elite solutions have converged to that minimum.

â€¢ a new solution combination method allows to explore deeper the search space. apart from the traditional method of linear combination between solutions, another method based on building hypercubes around the solutions to generate new solutions inside them has been implemented. now new points around elite solutions  can be generated, which favors the intensification and accelerates the convergence.

â€¢ when all the combinations among elite solutions have been done, the algorithm can stop or continue by partially rebuilding the set of the elite solutions. a new strategy for rebuilding this set, based in orthogonal search directions has been implemented. instead of simply maximizing the euclidean distances between the new elite solutions to be generated and the remaining ones, the algorithm takes into account the directions generated by every pair of elite solutions and force the generator to build new solutions that create new search directions.

â€¢ the user can choose a number of different local solvers such us sqp methods like fmincon , solnp  <cit> , snopt  <cit> , direct methods like nomad  <cit>  for the cases of very noisy data, and others specifically designed for parameter estimation problems such as n2fb/dn2fb  <cit> .

it is interesting to observe similarities and contrasts between scatter search and the original genetic algorithm proposals. both are instances of what are sometimes called "population based" or "evolutionary" approaches. both incorporate the idea that a key aspect of producing new elements is to generate some form of combination of existing elements. however, genetic algorithm approaches are predicated on the idea of choosing parents randomly to produce offspring, and further on introducing randomization to determine which components of the parents should be combined. by contrast, the scatter search approach does not emphasize randomization, particularly in the sense of being indifferent to choices among alternatives. instead, the approach is designed to incorporate strategic responses, both deterministic and probabilistic, that take account of evaluations and history. scatter search focuses on generating relevant outcomes without losing the ability to produce diverse solutions, due to the way the generation process is implemented.

a detailed description of the method is given in the methodology section.

confidence intervals
determining the parameter values with the maximum likelihood of being correct is only part of the parameter estimation problem. moreover, it is equally important to find a realistic measure of the precision of those parameters  <cit> .

it should be noted that, unlike for the linear case for which a neat theory already exists, there is no exact theory for the evaluation of confidence intervals for systems which are nonlinear in the parameters. an approximate method based on a local linearization of the output function is often used and was also adopted in this study, thus the confidence region is evaluated as a function of the parameter covariance matrix c, based on the fisher information matrix .

however, the confidence intervals obtained with the fisher method are statistically optimistic due to the use of a linear approximation of the non-linear model in the neighborhood of the best parameter estimates  <cit> .

alternatively, more robust techniques such as the jackknife and bootstrap methods produce parameter variances that are more realistic. as a drawback, one should mention that these methods are very computing intensive. another way to obtain the true confidence region of the parameters in non-linear models is by a systematic exploration of the objective functional for an extensive number of parameter combinations. this is a computing intensive task as well, because the number of evaluations increases as a power function of the number of parameters. therefore, in this study we will make use of the method based on the fim.

precision of parameter estimates
many difficulties found during parameter estimation are due to a poor identifiability of the model parameters. parameter identifiability tests should be performed prior to the estimation process to ensure that the parameter estimation problem is well-posed  <cit> . the identifiability analysis investigates if the unknown parameters of the postulated model can be estimated in a unique way.

regarding this problem, we can distinguish between structural and practical  identifiability  <cit> . structural identifiability is a theoretical property of the model structure depending only on the observation function and the input function. the parameters of a model are structurally globally identifiable if, under ideal conditions of noise-free observations and error-free model structure, and independently of the particular values of the parameters, they can be uniquely estimated from the designed experiment  <cit> .

the requirements for global structural identifiability are rather strict, since we can find realistic situations where the parameters are not identifiable according to this definition, but nevertheless they would be identifiable for a reasonably restricted set of all possible parameters. this leads to the definition of local structural identifiability, where the requirement for the parameters is to be identifiable in a Îµ neighborhood of a parameter set. although necessary, structural identifiability is obviously not sufficient to guarantee successful parameter estimation from real data, and this is when the concept of practical identifiability comes into play. in contrast to the theoretical properties of structural identifiability, the practical identifiability is limited by the finite amount of data and the observational noise. hence, in the presence of large observation errors and/or few data, no reliable estimate is possible and these parameters are called practical non-identifiable.

assessing a priori global identifiability is very difficult for nonlinear dynamic models, although techniques based on differential algebra have shown very promising results  <cit> . however, it has been argued that these techniques have somewhat limited applicability  <cit> . these limitations, taken in conjunction with the need for practical methods, provides a key argument for emphasizing the use of practical identifiability despite its limitations derived from its local nature. the question addressed in the a posteriori or practical identifiability analysis is the following: with the available experimental data, can the parameters be uniquely estimated? or, in other words, if a small deviation of the parameter set occurs, does this have a great impact on the quality of the fit?

the output sensitivity functions , are central to the evaluation of practical identifiability. if the sensitivity functions are linearly dependent the model is not identifiable, and sensitivity functions that are nearly linearly dependent, result in parameter estimates that are highly correlated. an easy way to study the practical identifiability of a simple model is to plot the sensitivity functions calculated for a given parameter set. however, this straightforward procedure becomes intractable when the number of measured states and parameters is of realistic size. in the methods section, a numerical procedure to test practical identifiability based on the fisher information matrix , as well as an approximate computation of the correlation matrix, are described.

the correlation matrix measures the interrelationship between the parameters and gives an idea of the compensation effects of changes in the parameter values on the model output. if two parameters are highly correlated, a change in the model output caused by a change in a model parameter can be  compensated by an appropriate change in the other parameter value. this prevents the parameters from being uniquely identifiable even if the model output is very sensitive to changes in the individual parameters.

in order to perform the practical identifiability analysis, prior knowledge of the model parameters is required. in an experimental situation, the parameters values will not be known a priori, and the identifiability analysis will be an important step in an iterative process involving experimental design and parameter estimation  <cit> .

in this work, the new global optimization metaheuristic described above has been coupled with a computational procedure to check identifiability and related measures. this has resulted in an integrated environment to perform robust parameter estimation and identifiability analysis.

RESULTS
in order to evaluate the performance and reliability of the novel metaheuristic presented here, which we will denote ssm , we have considered three challenging benchmark problems of increasing order of complexity. all the computations were carried out using a pc/pentium  <dig> .

isomerization of Î±-pinene
in this case study, we want to estimate  <dig> rate constants  of a complex biochemical reaction originally studied by box and coworkers  <cit> , which is also part of cops  maintained by dolan and coworkers  <cit> . figure  <dig> contains the proposed reaction scheme for this homogeneous chemical reaction describing the thermal isomerization of Î±-pinene  to dipentene  and allo-ocimen  which in turn yields Î±- and Î²-pyronene  and a dimer . this process was studied by fuguitt and hawkins  <cit> , who reported the concentrations of the reactant and the four products at eight time intervals . if the chemical reaction orders are known, then mathematical models can be derived giving the concentration of the various species as a function of time. hunter and macgregor  <cit>  assumed first-order kinetics and derived the following linear equations for the five responses:

dy1dt=âˆ’y1Â Â Â Â Â 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadawcaaqaaiabdsgakjabdmha5naabaaaleaacqaixaqmaeqaaagcbagaemizaqmaemidaqhaaiabg2da9iabgkhitiabcicaoiabdchawnaabaaaleaacqaixaqmaeqaaogaey4kasiaemicaa3aasbaasqaaiabikdayaqabagccqggpaqkcqwg5bqedawgaawcbagaegymaedabeaakiaaxmaacawljawaaewaaeaacqaixaqmaiaawicacaglpaaaaaa@4375@

dy2dt=p1y1Â Â Â Â Â 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadawcaaqaaiabdsgakjabdmha5naabaaaleaacqaiyagmaeqaaagcbagaemizaqmaemidaqhaaiabg2da9iabdchawnaabaaaleaacqaixaqmaeqaaogaemyeak3aasbaasqaaiabigdaxaqabagccawljagaaczcamaabmaabagaegomaidacagloagaayzkaaaaaa@3d67@

dy3dt=p2y1âˆ’y3+p5y5Â Â Â Â Â 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadawcaaqaaiabdsgakjabdmha5naabaaaleaacqaizawmaeqaaagcbagaemizaqmaemidaqhaaiabg2da9iabdchawnaabaaaleaacqaiyagmaeqaaogaemyeak3aasbaasqaaiabigdaxaqabagccqghsislcqggoaakcqwgwbacdawgaawcbagaeg4mamdabeaakiabgucariabdchawnaabaaaleaacqai0aanaeqaaogaeiykakiaemyeak3aasbaasqaaiabiodazaqabagccqghrawkcqwgwbacdawgaawcbagaegynaudabeaakiabdmha5naabaaaleaacqai1aqnaeqaaogaaczcaiaaxmaadaqadaqaaiabiodazagaayjkaiaawmcaaaaa@4edd@

dy4dt=p3y3Â Â Â Â Â 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadawcaaqaaiabdsgakjabdmha5naabaaaleaacqai0aanaeqaaagcbagaemizaqmaemidaqhaaiabg2da9iabdchawnaabaaaleaacqaizawmaeqaaogaemyeak3aasbaasqaaiabiodazaqabagccawljagaaczcamaabmaabagaeginaqdacagloagaayzkaaaaaa@3d77@

dy5dt=âˆ’p4y3+p5y5Â Â Â Â Â 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadawcaaqaaiabdsgakjabdmha5naabaaaleaacqai1aqnaeqaaagcbagaemizaqmaemidaqhaaiabg2da9iabgkhitiabdchawnaabaaaleaacqai0aanaeqaaogaemyeak3aasbaasqaaiabiodazaqabagccqghrawkcqwgwbacdawgaawcbagaegynaudabeaakiabdmha5naabaaaleaacqai1aqnaeqaaogaaczcaiaaxmaadaqadaqaaiabiwda1agaayjkaiaawmcaaaaa@448c@

assuming this model to be appropriate, the initial conditions for the five species are known, and we can estimate the unknown coefficients p <dig> ...,p <dig> by minimization of a cost function which is usually a weighted distance measure between the experimental values corresponding to the measured variables and the predicted values for those variables. for this problem the cost function can be formulated as:

j=âˆ‘j=15âˆ‘i=18âˆ’yËœji)2Â Â Â Â Â 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgkbgscqggoaakcqwgwbaccqggpaqkcqgh9aqpdaaewbqaamaaqahabagaeiikagiaemyeak3aasbaasqaaiabdqgaqbqabagccqggoaakcqwgwbaccqggsaalcqwg0baddawgaawcbagaemyaakgabeaakiabcmcapiabgkhitiqbdmha5zaaiawaasbaasqaaiabdqgaqjabdmgapbqabagccqggpaqkdaahaawcbeqaaiabikdayaaaaeaacqwgpbqacqgh9aqpcqaixaqmaeaacqai4aaoa0gaeyyeiuoaasqaaiabdqgaqjabg2da9iabigdaxaqaaiabiwda1aqdcqghris5aogaaczcaiaaxmaadaqadaqaaiabieda3agaayjkaiaawmcaaaaa@5511@

box and coworkers  <cit>  tried, in a first instance, to solve this problem without analyzing the multiresponse data, finding parameter values which provided an unsatisfactory data fit. since ignoring possible dependencies among the responses can lead to difficulties when estimating the parameters , box and coworkers described a method for detecting and handling these linear relationships. they showed that there are dependencies in the data and thus only three independent linear combinations of the five responses are used in the identification improving significantly the fit of the data. this analysis of multiresponse data, although efficient, requires a considerable effort specially to uncover the dependencies causes once they have been found, and a deep understanding of the model  is essential. moreover, it becomes unaffordable when the model complexity increases.

tjoa and biegler  <cit>  also considered this problem and used a robust local estimation approach to estimate the unknown parameters. they considered the entire data set in order to asses the performance of this method with dependencies in the data, finally reaching the same optimal parameters reported by box et al. however, the initial value considered for the parameters was very close to the truly optimal solution, which explains why this local method reached the global optimum without getting trapped in a local solution. as pointed out by averick and coworkers  <cit> , the solution of this problem is not difficult to obtain from initial values of p which are close to the global solution, but becomes increasingly difficult to solve from more remote starting points.

in order to avoid the convergence to local solutions without a good initialization value for the parameters and/or further analysis of the multiresponse data, the use of a global optimization approach is proposed here. the lower bounds considered for the five parameters arise from physical considerations, pi â‰¥  <dig>  and we took the upper bounds to be pi â‰¤  <dig>  very far from the best known solution . as initial point, we chose pi =  <dig> . it should be noted that all the local solvers that we tried with this initial point failed to converge, or converged to bad local solutions.

the confidence intervals obtained for the optimal parameters, presented in table  <dig>  are small, indicating a precise estimation. moreover, the color plot of the correlation matrix in figure  <dig> shows a good identifiability at the optimal value with a maximum correlation coefficient of  <dig>  between parameters p <dig> and p <dig>  this fact leads us to think that the existence of multiple local minima is the cause of the identification problems experienced in most of the previous studies. these difficulties can be surmounted by proper global optimization methods, as shown here.

inhibition of hiv proteinase
this problem consists of the estimation of a number of rate constants of a model for the reaction mechanism of irreversible inhibition of hiv proteinase as originally studied by kuzmic  <cit>  . the problem considers an experiment where hiv proteinase  was added to a solution of an irreversible inhibitor and a fluorogenic substrate . the fluorescence changes were monitored for  <dig> h in each of the five experiments conducted at four different concentrations of the inhibitor .

we considered the same problem solved by kuzmic  <cit>  and mendes and kell  <cit>  fitting five of the rate constants to the experimental data. in this fit, a certain degree of uncertainty  in the value of the initial concentrations of substrate and enzyme  was also assumed. in addition, the offset  of the fluorimeter was also considered as a degree of freedom. given that there are five time course curves, there are a total of  <dig> adjustable parameters: the five rate constants, five initial concentrations of enzyme, five initial concentrations of substrate and five offset values.

by minimization of the sum-of-squares function of the residuals between the measured and the simulated data, the best known solution was obtained by mendes and kell using simulated annealing, with a computational cost of  <dig> million simulations. the next best solution was obtained using a levenberg-marquardt method in a considerable shorter computational time  although this method is only guaranteed to converge to the global minimum if started in its vicinity.

in our study, ssm converged to a better solution in less that  <dig> simulations, which confirms the good performance of this method even with challenging parameter estimation problems. moreover, when compared with other performant stochastic methods such as sres or de, ssm reached better solutions with speed-ups of almost  <dig> orders of magnitude .

despite ssm converged in every run to solutions with a very good values of the cost function , the values of the parameters were not always the same  indicating a very flat objective function in the region of parameter space near the optimum. the correlation matrix  helps to explain this fact since there are correlation values of  <dig>  between some pairs of parameters,  indicating the lack of identifiability for this problem. this characteristic is first reported here and explains the difficulties  experienced by previous researches, confirming the importance of coupling identifiability tests with parameter estimation procedures.

however, it is worth noting the very good correlation between the experimental and predicted data for the best decision vector and the lack of correlation between the residuals and time .

three-step biochemical pathway
this case study, considered as a challenging becnhmark problem by moles and coworkers  <cit> , involves a biochemical pathway with three enzymatic steps, including the enzymes and mrnas explicitly. figure  <dig> contains a diagram illustrating the network of reactions and kinetics effects .

the identification problem consists of the estimation of  <dig> kinetic parameters of the nonlinear biochemical dynamic model  which describes the variation of the metabolite concentration with time. moles and coworkers tried to solve this problem using several deterministic and stochastic global optimization algorithms. they found that only a certain type of stochastic algorithms, evolution strategies , was able to successfully solve it, although at a rather large computational cost. in figure  <dig> we can see how the two-phase hybrid method recently presented by rodriguez-fernandez and coworkers  <cit>  converged to better solutions, with speeds up of more than one order of magnitude with respect to the previous results.

the novel metaheuristic, ssm, presented in this work was able to improve this result in an additional order of magnitude regarding the computational time. moreover, ssm had the additional advantage of not requiring preliminary runs, or any user inputs, for tuning the method, making it a very easy to use strategy. in short, using ssm we have reduced the computation time from two days  <cit>  to a couple of minutes, while ensuring robustness.

it is sometimes argued that a multistart local method can solve almost all global optimization problems. this can be false for even small problems  <cit> . the histogram in figure  <dig> represents the frequency of the solutions for a multistart of  <dig> runs using n2fb. the global optimum is in this region close to zero but we can see that it was never reached while a very large number of solutions are far from the global optimum. despite the identifiability difficulties of this problem, which make most of the solvers fail when trying to solve it, the confidence intervals of the global solution are small indicating a precise parameter estimation. this fact is discussed in more detail in  <cit> .

CONCLUSIONS
parameter estimation from experimental data remains a bottleneck for a major breakthrough in systems biology. traditional global optimization methods can ensure proper solutions, but suffer from the large computational burden required for large-scale model identification. in this contribution, we have presented a novel global optimization metaheuristic, ssm, which increases very significantly the efficiency of the estimation while keeping robustness. its capabilities were tested considering three challenging benchmark problems. this new method was able to successfully find the best known solutions for these problems while reducing the computation time by several orders of magnitude with respect to previous approaches.

