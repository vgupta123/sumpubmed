BACKGROUND
a popular class of biological models are differential equations models describing the dynamics of several reactive agents. models of this type often involve a large number of unknown parameters
 <cit>  which need to be inferred from experimental data, a process known as model calibration. this inference problem can be very challenging for many reasons. due to the lack of prior information about parameter values, it is often necessary to search a large region of a high-dimensional space to find parameter values that produce reasonable fits to the data. furthermore, the models are often highly nonlinear functions of the unknown parameters, making it difficult to navigate this space efficiently. this is particularly true for models which are defined as ordinary differential equations. challenges notwithstanding, models of this sort have attracted a lot of interest in the systems biology community and much effort has focused on calibrating these models.

typically, the fitting problem for nonlinear models is very ill-conditioned with large uncertainties in the inferred parameters, a phenomenon sometimes known as sloppiness <cit> . for one model it was observed that inferred parameters had a relative uncertainty of several hundreds of thousands
 <cit> , suggesting that inferring parameters accurately might require unreasonable amounts of data. more recent work applying experimental design techniques, however, has shown that parameters for the same model could be inferred with just a few experiments, provided the experiments probed complimentary degrees of freedom
 <cit> . although the experiments may still require a large amount of data to achieve the desired accuracy
 <cit> , they are nevertheless a dramatic improvement over previous results and the present work is motivated in large part by this approach.

more generally, experimental design has been used extensively in guiding modeling of biological systems, see reference
 <cit>  for a review. interest in experimental design has been further motivated by the need to infer topological relationships among biological agents in protein signalling and gene regulatory networks
 <cit> . in general, the relative complexity of models in combination with the limited amount of quantitative data makes optimal experimental design an ongoing challenge in systems biology
 <cit> .

this work is also motivated by the recent 6th dialogue on reverse-engineering assessment and methods  parameter estimation challenge
 <cit> . this challenge provided three models and required contestants to “purchase” noisy experimental data on a limited budget with the goal of inferring the model parameters and predicting the time series of the protein concentrations after perturbation. in this work we follow the dream <dig> challenge closely, attempting to infer the parameters and model predictions using the same set of perturbation experiments available in the challenge. however, the dream <dig> challenge was a type of meta-optimization problem; contestants were required to balance the costs of different types of experiments with the goal of estimation accuracy. although most real-world decisions will hinge on this trade-off, in this study we do not weigh different experiments by their costs. the problem we address is therefore separate from, but related to, that of the dream <dig> challenge. the work described in this paper was conducted after the conclusion of the dream <dig> challenge.

the main result of this paper is that the fisher information can be used as an effective criterion for experiment selection. the fisher information is a measure of information content based on a local linearization of the model. we show that even when parameter uncertainty is too large to justify the linear approximation, the fisher information is still an effective metric for experiment selection. our method of selecting experiments is therefore computationally efficient since it is based on a sensitivity anslysis at a point estimate of the parameters. it does not require, for example, a sampling of a bayesian posterior or other rigorous methods of estimating confidence intervals in order to select a maximally informative experiment. it is also robust to which parameter values are used to calculate the fisher information. we find that by calculating the fisher information at a local minimum rather than the best fit still produces reliable experiment choices to efficiently find the true parameters. furthermore, our method can be generalized to select experiments that reduce uncertainties in predictions without a need for estimating parameters directly. indeed, we find that model predictions can often be constrained with considerely less cost than the parameters. in real-world scenarios in which costs must be balanced against research goals, we anticipate this approach to be useful.

in the current approach, we assume that the true mathematical form of the model and the distribution of experimental noise are known, while the model parameters are unknown. although such assumptions are generally not true in practice, this problem represents a step toward the more general problem of model inference.

methods
models and data
in this paper, we study three models provided by the recent 6th dialogue on reverse-engineering assessment and methods  parameter estimation challenge. these models describe three hypothetical gene-regulatory networks, implemented as ordinary differential equations that describe the time course of  <dig>   <dig>  and  <dig> dynamical variables . the goal of the challenge is to select a series of experiments to accurately estimate the model parameters, subject to budgetary constraints. although our results are valid for all three models, in this presentation we focus on model  <dig>  whose network structure is given in figure
 <dig>  the precise mathematical form of the model is available in several formats from the website of the challenge and given in the appendix.

the unknown model parameters consist of mrna and protein production and degradation rates, as well as michaelis-menten constants and hill coefficients describing the gene regulation. in our implementation we follow the convention of the dream <dig> challenge and assume that the mrna degradation rates are each  <dig>  which sets the time scale of the experiments, and that the proteins share a common, albeit unknown, degradation rate. with these conventions, models  <dig>   <dig>  and  <dig> have  <dig>   <dig>  and  <dig> unknown parameters respectively.

for each model, we generate a set of parameter values and treat them as the true values in our simulation. we then evaluate the time course of the model according to the true parameter values. when evaluating the model, we assume the initial protein concentrations are each  <dig> while the initial mrna concentrations are  <dig>  we sample the concentrations for  <dig> and  <dig> time points  evenly spaced between t =  <dig> and t =  <dig>  after which, the model has essentially reached the steady state concentration. we then add “experimental” noise to the true time course in the form of both additive and multiplicative gaussian noise. specifically, if v were the simulated value then the observed value would be given by 

  vnoise=max, 

where ξ <dig> and ξ <dig> are gaussian random variables with zero mean and standard deviation of one. we use values c1 =  <dig>  and c2 =  <dig> , following the conventions of the dream <dig> challenge.

the noisy time course for mrna concentrations serve as the startup data, and our goal is to estimate the parameters from this noisy data. however, even knowing the time course for all the dynamical variables of the model is not enough to reasonably constrain the parameters as the parameter can be varied by several orders of magnitude without appreciably changing the model behavior. it is therefore necessary to select new experiments which perturb the model dynamics in order to further constrain the possible parameter values.

cost function and minimization scheme
we define a cost function, 

  c=12∑iri <dig> 

where i labels each measurements and θ are the unknown parameters. the residuals ri are given by 

  ri=yiobs−yipredσi. 

where
yiobs is the i-th experimental observation, measured with uncertainty σi, and
yipred is the corresponding model prediction. the uncertainty is given by
σi=c12+ <dig>  much of the work of this paper involves exploring the dependence of the cost on the unknown parameters θand the choice of experimental data. for a set of data, the parameters θbf that globally minimize the cost function in eq.  are known as the best fit.

in practice, even finding a good fit for a large, nonlinear model such as the ones we consider can be a challenging task. when one possesses little or no prior information about expected parameter values, searching a high dimensional parameter space for the optimal fit can be a daunting task. recent advances have helped to identify the primary pitfalls in finding good fits and suggested methods for finding them more efficiently
 <cit> .

algorithms often fail to converge to any minimum of the cost because they push parameter values to their extreme limits , at which point the algorithm fails since the cost function is very flat in these regions. to overcome this problem we follow the method described by transtrum et al.
 <cit> . first, we augment our cost function with penalties to force the parameters to remain within a reasonable range. these penalties help to guide the algorithm away from extreme parameter values toward ranges where they could be potentially measured by the experiment. specifically, for each parameter θμ we add two additional residuals to our cost function of the form 

  r=whθμ 

and 

  r=wlθμ. 

the former penalty prevents the parameter θμ from becoming too large while the latter prevents it from becoming too small. the weights are chosen to be as small as possible while maintaining a high success rate with the algorithm. we choose wh = wl = w, which places the minimum of the penalty at  <dig>  the natural scale for the problems at hand.

we choose w to be  <dig>  for the degradation rates and hill coefficients. this allows the parameters to vary by roughly an order of magnitude in either direction. while this may seem to be is a tight restriction, it is justified in that the models are insensitive to larger variations in these parameters and it would be impossible to estimate them from the data even if the true values were beyond this range. if the final estimate of the true values for the parameters were to lie near the boundaries set by these penalties, they should not be trusted, and a more accurate estimate would require a different set of experimental conditions.

for the remaining parameters we choose w = 10− <dig>  allowing the parameters to fluctuate by eight orders of magnitude. this larger variation is justified in that the model remains sensitive to these parameters over a larger range. however, as before, if the final estimate of the parameters lies near this boundary, the precise values are suspect.

under a bayesian framework, the penalty terms in eqs.  and  can be interpreted as priors. however, it is not necessary to adopt a bayesian approach to justify including the penalties; their practical utility in helping algorithms find the maximum liklihood estimator also makes them useful from a frequentist viewpoint. when performing a frequentist analysis, one would relax the penalities in order to identify the best fit of the bare cost; however, in practice the penalties are weak enough that they make no practical difference in the values of the final parameter estimates. insetad, their usefulness is in preventing search algorithms from getting lost.

with our modified cost function that includes penalty terms, we search for the best fit parameters using the geodesic levenberg-marquardt algorithm
 <cit> . the levenberg-marquardt algorithm is a gradient search algorithm that interpolates between gradient descent and newton’s method and is usually the most reliable method for nonlinear least-squares minimization. the geodesic levenberg-marquardt attempts to further improve convergence rates by correcting the search direction based on higher order derivative information.

error estimation and the fisher information
in the neighborhood of the best fit there exists a region of parameter values that, although not optimal, are nevertheless consistent with the data within experimental noise and constitute the confidence interval for the parameter estimate. the corresponding variation in the parameter values is known as the uncertainty. if it is known that the set of acceptable fits is sufficiently localized around the best fit, then the uncertainty may be estimated by expanding the cost as a taylor series centered at the minimum: 

  c≈c0+12∑μνδθμhμνδθν, 

where the first order terms have vanished since the gradient is zero. the hessian matrix h contains the second derivatives of the cost with respect to the parameters: 

  hμν=∂2c∂θμ∂θν=∑i∂ri∂θμ∂ri∂θμ+ri∂2ri∂θμ∂θν 

  ≈∑i∂ri∂θμ∂ri∂θμ 

in the final line we have made the common approximation that the residuals ri are small near the best fit and can be neglected. this approximate hessian is the fisher information matrix and its inverse is the co-variance of the inferred parameters in the quadratic approximation, which is our approximate parameter uncertainty. in the dream <dig> challenge, the accuracy of contestants’ inferred parameters was measured by the function 

  dparam=1n∑ilogθiestimateθitrue <dig>  

where n is the number of parameters. because of this, it is advantageous to work in
log parameters, essentially measuring relative rather than absolute uncertainty. additionally, by working in
log parameters we enforce that all our parameters are positive, producing an unconstrained optimization problem. in the quadratic approximation, we can estimate our expected value of dparam as 

  dparam≈1ntracei− <dig>  

where i is the fisher information matrix in log parameters 

  iμν=∑i∂ri∂logθμ∂ri∂logθν. 

eq.  is the average variance of the log parameters, so that a 30% uncertainty in the parameter values corresponds to d =  <dig>  ≈  <dig>  and a 10% uncertainty corresponds to d =  <dig>  =  <dig> . in practice, the fisher information is often ill-conditioned and calculating dparam from eq.  can often produce numerical errors. fortunately, in practice, we are primarily interested in the case for which the fisher information is the most well-conditioned. we can often disregard the cases in which numerical errors pose problems. the stability of the calculation can further be stabilized by noting that since i = jtj where
jmμ=∂rm/∂logθμ, the eigenvalues of i are the squares of the singular values of the jacobian, j. in practice, we therefore use
dparam=∑μ1/sμ <dig> where sμ are the singular values of the jacobian. with this approach our calculations do not suffer from numerical instabilities due to the extreme ill-conditionedness of the problem.

although this approximation of parameter uncertainty is accurate for when the data has sufficiently constrained the parameters, it is not accurate if the uncertainties extend beyond the harmonic approximation or if there are several distinct local minima with reasonable fits. we show, however, that although uncertainties estimated from the fisher information may not be accurate, it provides an effective metric to select experiments.

experiment selection
with the best fit parameters and an estimate of the uncertainty, we next select an experiment to reduce the estimated error. we consider the same set of potential experiments available in the dream <dig> challenge, which consists of a perturbation to the model and a measurement experiment. the perturbations include deleting one gene, over expressing one of the proteins, or down-regulating the mrna production of a single gene. the available measurements included the time series of mrna concentrations  or of two proteins . we do not consider experiments with different initial conditions, nor do we consider multiple perturbation experiments . in addition to time series measurements, we also assume gel-shift assay experiments are available which estimate the true values of the michaelis-menten constants and hill coefficients for a given interaction.

to select an experiment, we simulate all the potential experiments using the current best fit parameters, estimate the parameter error given by eq.  for each experiment, and propose to perform the experiment that reduces the estimated error most. note that when we evaluate eq.  we do not include the contribution from the penalty terms. in this way eq.  only measure the information content of the experiments. noisy data corresponding to the selected experiment is then generated by simulating the model with the true parameters and adding noise according to eq. . with the additional data, the previous best fit parameters will no longer lie at a minimum of the cost function. we therefore repeat the process of minimization and error estimation using the new data. we iteratively select experiments in this way until estimated error is sufficiently small.

our method of selecting experiments is similar to other approaches in the literature
 <cit> . the basic scheme is to first estimate the parameters from the available data, either as a point estimate, as we do, or as a bayesian posterior as done by vanlier et al.
 <cit> . from this estimate, one predicts the outcome of the available experiments and estimates information content of each experiment. finally, the most informative experiment is selected and added to the available data and the processes is repeated. we summarize this procedure in table
 <dig>  one of the advantages of our approach is that our method uses a point estimate of the parameters and so does not require a computationally expensive markov chain monte carlo  calculation at each iteration.

RESULTS
fitting the initial data and estimating uncertainty
by using the methods described above, the fitting process is essentially automated, and we are able to efficiently explore parameter space by searching for local minima of the cost from random initial guesses. with several repeated runs we are able to identify the best fit parameters. there is evidence that the cost landscape for models such as these is very rugged with many local minima
 <cit> . we therefore also search extensively for local minima.

we explore the extent to which our models have local minima in the cost by searching from  <dig>  random starting points chosen uniformly on a log scale in the range corresponding to the penalties described above. the algorithm successfully found a local minimum about 20% of the time for each model. the failures were due to the differential equation solver being unable to accurately integrate the differential equations at extreme values of the parameters. with  <dig>  starting points, our sampling of the entire search space is necessarily not very dense, a limitation due to the high-dimensionality of the search space. notice that the typical number of points per parameter axis for the smallest model is 100001/29 ≈  <dig>  while for the largest model it is 100001/49 ≈  <dig> . however, because such a large fraction of the attempts failed due to extreme parameter values, the sampling is also very diffuse. therefore, our results cannot be attributed to our search being localized to a small portion of parameter space. furthermore, this success rate could be increased by reducing the range of starting points. although the search is sparse and diffuse, if our models had many local minima, we anticipate they would be discovered by our investigation.

among the roughly  <dig> successful attempts of the geodesic levenberg-marquardt algorithm, the majority  correspond to “good” fits of the data, i.e. fits within experimental errors. the remaining “bad” fits correspond to fits that fail to fit one or more qualitative features of the data and had much larger values of the cost function than the good fits.

inspecting the parameter values of the several “good” fits, we find that the parameters vary over a very wide range, suggesting that there are many local minima that fit the data well. however, the eigenvalues structure of the fisher information evaluated at these minima suggests that cost surface has many narrow canyons, i.e. there are many very small eigenvalues. because the cost surface is very flat along the bottom of these canyons, it is possible that these fits actually correspond to the same basin of attraction. indeed, the cost surface is sufficiently flat along these canyons that one would expect numerical noise, such as rounding errors in the differential equation solver, to create artificial local minima along the bottom of the canyon. we therefore cannot use parameter differences as a criterion for distinguishing distinct local minima.

a better criterion to identify unique local minima is to use relative differences in the residual vector, ri as a distinguishing criterion. this is a natural choice as it corresponds to how the search algorithm checks convergence. the algorithm monitors the angle between the unfit residuals and the surface of potential fits in data space, known as the model manifold <cit> . when this angle approaches 90°, the algorithm is near a local minimum as illustrated in figure
 <dig>  furthermore, from the algorithm’s tolerance we can estimate the distance to the true minimum in data space. if two fits are nearer than the distance specified by the algorithm’s tolerance, then we assume that they belong to the same minimum, even though their parameter values may be very different.
|r|cosα are likely to correspond to different minima.

using this criterion, we identify  <dig> distinct minima for model  <dig> that were good fits to the data. closer inspection of the good fits reveals that their residual vectors are separated by a distance only slightly larger than the tolerance specified by the search algorithm. furthermore, a direct line in parameter space connecting the distinct minima reveals they are separated by very shallow barriers in the cost function as we show in figure
 <dig>  these observations leave open the possibility that these good fits are not actually distinct, but belong to the same basin of attraction. however, it is safe to conclude that even if the good fits are distinct minima, they all reside within some broader basin and are separated only by shallow barriers.

finally, we compare the orientation of the unconsrained parameter directions as measured by the fisher information with a principal component analysis  of the collection of good fiits. specifically, consider the two five-dimensional subspaces spanned by the eigenvectors of the least constrained directions for both measures of uncertainty. one can compute the so-called principal angles between these subspaces to measure the extent to which they are aligned
 <cit> . geometrically, these angles are defined by considering the angles between all the vectors contained in the two subspaces; the smallest such angle is the first principal angle. smaller angles indicate that the two spaces are more aligned. for the two five-dimensional subspaces we consider, this angle is 16°. by comparison, the average first principal angle among two randomly generated five-dimensional subspaces is approximately 49°. from this we conclude that the two subspaces are roughly aligned. although our local search method produced many different parameter values suggestive of a rough cost surface
 <cit> , our subsequent analysis suggests that these local minima are separated by very small barriers. we therefore believe that the cost functions of our models are dominated by one large basin that is long and narrow, and that the orientation of this basin is roughly described by the fisher information . therefore, by selecting experiments based on the fisher information using the criterion in eq. , we hope to maximally increase the curvature of the cost function around this minimum and efficiently estimate the parameter values.

this argument should not be misunderstood to suggest that the linear approximation is an accurate estimate of the uncertainty with sparse data. rather we expect the fisher information to be an efficient choice because of the cost function appears to have only one basin that fits the data well. we can therefore select experiments to maximize the curvature of this basin. this argument will be further strengthened in section selecting experiments where we show that the choice of experiment is roughly independent of which point in the basin is used to calculate the fisher information.

selecting experiments
motivated by the results of the previous section, we assume that experiments that minimize the variance described by the fisher information will be maximally informative in constraining the parameters. we therefore select experiments that minimize the error given by eq.  as described in section experiment selection. an example of experiments produced by this method for model  <dig> is given in table
 <dig>  this sequence of experiments was generated by finding one local minimum using the startup data, selecting an experiment using the local fisher information matrix at that minimum, and adding the experimental data for this experiment to the collection of data. we then iteratively repeat the process.

∞
we list the sequence of experiments for estimating the parameters for model  <dig> for a randomly chosen set of true parameters. experiments consist of a perturbation and a measurement. “wild” refers to the original, unperturbed model, while “delete 1” indicates a deletion of gene  <dig>  “over 1” indicates an over-expression of gene  <dig>  “down 5” indicates a down-regulation of protein  <dig>  and so forth. “microarray” indicates a microarray measurement experiment of time series for all the mrna concentrations, while “protein  <dig> and 4” indicates a time series measurement of proteins  <dig> and  <dig>  and similarly for the remaining measurements. gel-shift assay experiments indicate a direct measurement of the michaelis-menten constant and hill coefficients for a specific reaction in the model. the estimated error is calculated from eq.  which is an estimate of the average variance in
log-parameters, so that that 30% accuracy corresponds to an error of  <dig>  and is achieved after about  <dig> experiments. an accuracy of 10% corresponds to an estimated error of  <dig>  and is achieved after  <dig> experiments. after about six experiments, the marginal benefit of additional experiments becomes small. at this point, the experiments are no longer complimentary and most of the benefit is attributed to probing the same degrees of freedom with more data. notice that the estimated error increases at the fifth iteration. this is due to the minimization algorithm not finding the best fit at this iteration. the subsequent success of the method is a demonstration of the robustness of this method for selecting experiments.

naturally, the precise sequence of experiments listed in table
 <dig> depends on at which parameter values the fisher information is calculated. however, we repeated the experiment selection procedure from all  <dig> minima and found that there is much overlap among the selected experiments irrespective of which minimum was used. additionally, we tested our method using several sets of true parameters and found that the selected experiments are also more or less independent of the true parameter values. furthermore, after selecting a sequence of experiments, we repeat our search for local minima. we find that the additional data has successfully reduced the  <dig> minima of the initial startup data into a single good fit, indicating that any of the  <dig> sequences would have also been effective for estimating the parameters. we see this explicitly in figure
 <dig> where we show how the experiments guide the paths of the local minima to the true parameter values. even though the experiments along each path are slightly different, they converge to the same estimate of the true parameters.

it is interesting to compare the uncertainty reduction of our method with that of randomly selected experiments. we find that it typically takes four to five times as many randomly chosen experiments to accomplish a comparable accuracy as that in table
 <dig> 

parameters vs predictions
in addition to estimating parameters, the dream <dig> challenge asked contestants to predict a time series of a perturbed version of the model. for model  <dig> this was a time series for proteins  <dig>   <dig>  and  <dig> with several parameters increased by anywhere from a factor of  <dig> to  <dig>  contestants were then judged based on their score as measured by 

  dpred=1m∑iyipred−yitrue2c12+c22yitrue <dig>  

where m is the number of predictions. if our goal is to accurately predict the time series, rather than estimating the model parameters, we can modify our criterion of selecting experiments to minimize the expected error as measured by eq. . by extrapolating the uncertainty in the parameters to the uncertainty in the predictions, in the quadratic approximation, the estimate of eq.  becomes 

  dpred=1m∑μ,νiμνpredi−1μν, 

where ipred is the fisher information for the predicted time series: 

  iμνpred=∑m∂ympredσm∂logθμ∂ympredσm∂logθν. 

with these modifications, we list a sequence of experiments to minimize the uncertainty in the predictions in table
 <dig>  notice that only  <dig> experiments are needed to reduce the uncertainty to within the experimental noise, even with largely unconstrained parameters. if experiments were chosen to infer parameters it would have taken nearly twice as many experiments to get a comparable accuracy.

we list the sequence of experiments for estimating the predictions for several protein concentrations in model  <dig>  at each iteration we reduce the relative uncertainty in the predictions given by eq.  which is reported in the estimatd error column. for this measure of error, a value of  <dig> corresponds to an accuracy comparable to the experimental noise. notice that the uncertainty in the predictions can be reduced to the experimental uncertainty with just four experiments. even though the parameters are largely unconstrained after four experiments, the model is nevertheless able to make falsifiable predictions.

discussion
note that the success of the fisher information in selecting experiments cannot be attributed to the parameter uncertainty being well-approximated by the linearized residuals. the uncertainties in the inferred parameters after fitting to the startup data are very large and extend well beyond the linear approximation in which the fisher information is valid. this can be seen explicitely in figure
 <dig> in which we show the cost barrier for a straight line path between two good fits. as both endpoints of the path fit the data very well, both points are contained within the confidence region of the parameters. however, since the behavior of the cost along the straight line connecting them is far from quadratic, nonlinearities are clearly important.

the  <dig> distinct minima reported in section fitting the initial data and estimating uncertainty should not be misinterpreted as a sampling of the bayesian posterior distribution. rather, they represent potential local minima of the cost surface. as they each fit the data very well, none of these local minima can be ruled out by initial data and additional experiments are necessary to distinguish among them. although we cannot rigorously identify whether these fits represent local minima or are connected by a flat canyon, by appropriately selecting experiments we were able to successfully distinguish among them. in figure
 <dig> we interpret the effect of the experiment selection on the confidence interval. the initial confidence region is very large and encompasses all the good fits from the initial search. as additional data are added, the confidence interval shrinks and the location of the minimimum is adjusted. after each additional experiment, the new minimimum lies within the new confidence region.

while the precise order of selected experiments varies depending on the minimum used, there are patterns that can be understood ex post facto by inspecting the network topology. for example, among the list of experiments in table
 <dig>  there is a strong preference for perturbations of gene  <dig>  inspecting figure
 <dig> we see that gene  <dig> acts as a type of head node for the network. it is not regulated by any other gene while any perturbation of gene  <dig> expression should effect the entire network. it is natural to suspect that experiments perturbing gene  <dig> will be the most influential. the remaining perturbation experiments involved genes  <dig> and  <dig>  these experiments can be understood by noting that these genes form a negative feedback loop.

similarly, we can also understand the choice of measurements. for example, from figure
 <dig> we see that the protein produced by gene  <dig> does not regulate any other genes. consequently, without a measurement of the concentration of protein  <dig>  the parameter controlling the production of protein  <dig> is completely unconstrained. because of this, the parameter uncertainty with the initial data is infinite , and we always select a measurement of protein  <dig> concentrations as the initial experiment.

to understand the remaining measurements, note that there are two channels through which signals are passed, either through genes
 <dig> →  <dig> →  <dig> or through genes
 <dig> →  <dig> →  <dig>  typically, measurements are selected to observe the effect on both sequences.

the work of apgar et al.
 <cit>  has shown the importance that experiments be selected that contain complimentary information. our qualitative understanding of the choice of experiments in the previous paragraphs reinforces this claim. recent work studying the behavior of large nonlinear models, such as these, from an information geometric viewpoint has suggested that models can be interpreted as generalized interpolation schemes
 <cit> . we offer an interpretation of the experiment choices from this viewpoint.

data leaves parameters unconstrained when it probes fewer effective degrees of freedom than the model has parameters. consider figure
 <dig> where we give sample data for model  <dig>  although the time series corresponding to the concentration of mrna  <dig> has many data points, the model effectively has only two degrees of freedom in such a time series: the magnitude of the equilibrium concentration and the time scale it takes to equilibrate. similarly, the time series of protein  <dig> has two similar degrees of freedom; however, since protein concentration necessarily echoes its corresponding mrna concentration, these degrees of freedom are not independent. furthermore, by noting that gene  <dig> is promoted by gene  <dig> and inhibited by gene  <dig>  we see that the six time series in figure
 <dig> explore many of the same degrees of freedom and are not really independent measurements. in the language of interpolation, the data for gene  <dig> is, in some sense, “between” the data from genes  <dig> and  <dig> and could be inferred by interpolating between the two. quantitative interpolation is precisely the role of the model. indeed, even with largely unconstrained parameters, measurements of time series for gene  <dig> and  <dig> would be sufficient to give accurate predictions for behavior of gene  <dig> 

the choice of experiments in table
 <dig> can be understood in a similar manner to those in table
 <dig>  in this case, the three selected experiments are analogous to the perturbations in the desired predictions. indeed, the primary perturbation in the desired predictions is a massive over-expression of gene  <dig>  consequently, an experiment was selected corresponding to a more mild over-expression of gene  <dig>  as well as experiments to accurately probe the feedback loop that regulates gene  <dig>  the measurement experiments correspond primarily to the three time series to be predicted.

by interpreting the model as a generalized interpolation scheme, we can understand both why the experiments in table
 <dig> were selected, as well has how the model can make accurate predictions without enough data to constrain the parameters. just as the time series for gene  <dig> can be predicted in figure
 <dig> since its behavior is, in some sense, “between” the behavior of genes  <dig> and  <dig>  the desired predictions lie “between” the experimental observations in table
 <dig> 

the fact that the cost of inferring parameters is much larger than that of inferring a few predictions  suggests two differing approaches to modeling and experimental design. while knowing the parameters has the aesthetic appeal of allowing one to know all potential model predictions, if one is only interested in a few predictions, conducting experiments to infer all the parameters is not a cost effective approach. furthermore, in most cases it is unknown whether the proposed model represents the true biological network. in these cases, there will likely be several competing models that can only be distinguished by their ability to make falsifiable predictions. the cost advantage of selecting experiments based on the predictions rather than parameter values is likely to be more dramatic.

one could generalize this method of selecting experiments by choosing alternative measures of information. in particular, our criterion in eq.  is based on a linearization of the residuals, and one could construct a more accurate measure for when the linear approximation breaks down. one approach might be based on an mcmc sampling of the bayesian posterior using eqs.  and  as priors or some other appropriate choice. in fact, we have sampled the bayesian posterior for our models, and find that the allowed range of many parameters is always dominated by the prior for any choice of weights. these parameters can fluctuate to infinity or zero with only a statistically insignificant increase in the bare cost. it is therefore a nontrivial problem how to construct a measure of information that adequately reflects the information content of the experiments without being dominated by the prior.

fortunately, we have shown that eq.  often provides an effective criterion for selecting experiments even when the parameter uncertainties extend well beyond the linear approximation as they do in our case. indeed, the main result of this paper is that the fisher information is efficient for selecting experiments under these conditions. recall that the fisher information describes the curvature of the cost function around a local minimum. since, this curvature describes the parameter uncertainty in the asymptotic limit, selecting experiments to maximize the curvature is actually a reasonable choice. experiments which minimize eq.  can be understood as those that bring us closest to the asymptotic regime. we have seen that this argument seems to hold even when the fisher information is evaluated at different local minima.

CONCLUSIONS
in this paper we have described a method of selecting experiments to infer unknown model parameters. we have shown that when data is sparse, the parameter uncertainty is large and the cost surface has many local minima. in spite of this, by selecting experiments based on the uncertainty estimated by the local fisher information, we are able to reduce parameter uncertainty and constrain the set of reasonable fits to the data to lie within a single region around the best fit. although this method will produce a different sequence of experiments based upon at which minima the fisher information is calculated, we have seen that collection of experiments generated from different minima is in fact very similar.

as we have noted, our method for selecting experiments is very similar to the greedy method described by apgar et al.
 <cit> . our work goes beyond previous results, however, in that we have explored the effect of experiment selection on the global cost surface. since in most cases the parameter uncertainty extends well beyond the harmonic approximation and may even include non-contiguous patches of acceptable parameters around local minima, it is not obvious that a selection criterion that is based only local information  will be effective. our results help to validate the method of previous work
 <cit> , while demonstrating its applicability to additional models.

when selecting experiments, it is important for them to be complimentary and probe independent degrees of freedom of the model. using the proposition that models should be thought of as a generalized interpolation scheme, we have understood that measurements lying “between” observed data is not as effective at reducing parameter uncertainty as measurements that probe independent degrees of freedom. we have shown that we can qualitatively understand which experiments probe these degrees of freedom by inspecting the network topology of the model.

previous observations that predictions are often possible without knowing the parameters precisely
 <cit>  can be understood in this light, as well. indeed, we have shown that uncertainties in predictions can also be reduced by an adequate series of experiments. because reducing the uncertainty in a few model predictions does not generally require all the parameters to be tightly constrained, these predictions can be made with fewer experiments.

appendix
differential equations
in this appendix we give the mathematical form for the first model of the dream <dig> challenge described in this paper. 

  ddt=cod1−mrna_deg_rate 

  ddt=rbs1_strength−p_deg_rate 

  ddt=cod2−mrna_deg_rate 

  ddt=rbs2_strength−p_deg_rate 

  ddt=cod3−mrna_deg_rate 

  ddt=rbs3_strength−p_deg_rate 

  ddt=cod4−mrna_deg_rate 

  ddt=rbs4_strength−p_deg_rate 

  ddt=cod5−mrna_deg_rate 

  ddt=rbs5_strength−p_deg_rate 

  ddt=cod6−mrna_deg_rate 

  ddt=rbs6_strength−p_deg_rate 

where we have used the variables 

  cod1=pro1_strength 

  cod2=pro2_strengthh21+n2×11+n <dig> 

  cod3=pro3_strengthh31+n3×11+n <dig> 

  cod4=pro4_strengthh11+n1×11+n <dig> 

  cod5=pro5_strength11+n <dig> 

  cod6=pro6_strength11+n <dig>  

notice that the mrna concentrations each degrade with the same rate mrna_deg_rate, which we assume is  <dig> throughout this paper. also note that the proteins each decay with the same rate p_deg_rate, which is one of the  <dig> parameters to be inferred. the remaining  <dig> parameters are the production rates for each component: rbs_strength and pro_strength for
j =  <dig> …, <dig> and the michaelis-menten constants and hill coefficients: kj and hj for
j =  <dig> …, <dig>  measurements are made of the either the mrna or protein concentrations at specific time points. that is to say,
yiobs in eq.  can correspond to either mrna or p at a specific time point.

the perturbation experiments modify the above equations as follows: deleting a gene corresponds to eliminating production of both the mrna and protein for the corresponding gene, i.e. pro_strength = rbs_strength =  <dig> for the appropriate gene. we implement mrna knockdown by a five-fold increase in the mrna degradation rate for the appropriate gene. increase of protein expression is implemented by doubling the translation rates, rbs_strength for
j =  <dig> …, <dig> 

competing interests
the authors declare that they have no competing interests.

author’s contributions
mkt designed the experiment selection method, implemented the numerical simulations, carried out the calculations, and drafted the manuscript. pq conceived of the study and participated in its design and coordination and helped to draft the manuscript. both authors read and approved the final manuscript.

