BACKGROUND
this paper describes the methods we used to accomplish entity identification  in the molecular biology domain. entity identification in this domain has been a subject of interest since fukuda et al.'s seminal paper on the proper/kex system  <cit> . the subject is of interest to biologists because it is a necessary first step in many kinds of applications that are of interest to them, including information extraction, information retrieval, and bibliometrics. it is of interest to linguists and computer scientists because it seems to be more difficult than entity identification in "general english" domains  <cit> . in this paper, we show that a stochastic pos tagger performs well as an entity identification system in the molecular biology domain. like tanabe and wilbur  <cit> , we approached the molecular biology entity identification problem as a part-of-speech  tagging task, adding to the standard pos tag set one or more gene tags for genes and gene products. our system replaces the brill tagger with an hmm-based part-of-speech tagger. our experience suggests that the brill tagger is susceptible to specific kinds of performance problems that we hoped to avoid. however, we did not rigorously compare the performance of the two taggers. the main difference between the two systems is our focus on tailoring the post-processing steps for the biocreative task. specifically, we found that understanding the biocreative annotation policies for building the corpora and performing error analysis allowed us to create post-processing rules that were effective in increasing performance.

the goal of biocreative task1a is to assess the ability of an automated system to identify mentions of genes in text from biomedical literature. the corpus used for task1a consists of sentences drawn from medline abstracts and is divided into three sets: training, devtest, and official test. table  <dig> shows the number of sentences and entities for the three subsets. also shown is the distribution of the lengths  of the gene mentions. task1a has two divisions: open and closed. the open division permits systems to use external data resources such as online dictionaries or databases while the closed division does not.

RESULTS
overall
we did five rounds of cross-validation, training on four subsets of the data and testing on a fifth using a combined corpus consisting of the training and devtest data. we evaluated our results using the scoring software provided with the biocreative data. the resulting average precision and recall were  <dig>  and  <dig>  with no post-processing . the resulting average precision and recall with post-processing was  <dig>  and  <dig> , respectively. the averaged results of the cross-validation runs are shown in figure 1a. the results for official test are shown in figure 1b. the term-level score comparison between cross-validation and official scores are shown in table  <dig>  that both sets of results show the same trends shows that our system did not over-train on the devtest corpus and that it performs consistently.

term-level precision and recall
term-level scores  were obtained using the biocreative scoring software. we evaluated performance both with and without post-processing. without performing post-processing, average precision and recall were  <dig>  and  <dig> . when post-processing was applied, average precision and recall were  <dig>  and  <dig> . post-processing improved both the precision and the recall, having a much larger effect on precision than on recall. this tendency is reasonable because our algorithms focus on repairing or removing gene mentions found by the base system and concentrate less on finding new gene mentions that were mistakenly tagged with pos tags such as nn or nns. a dictionary-based post-processing is introduced to help increase recall. however, the dictionary-based approach increased our f-measure by only  <dig> %.

baseline, and normalizing for the difficulty of the task
as a baseline for understanding the difficulty of the task, we measured the performance achieved by simply assigning each word the most frequent tag seen with that word in the training set. this baseline strategy achieved an average precision of  <dig>  and an average recall of  <dig> . for official test the score achieved precision of  <dig>  and recall of  <dig> . these results are considerably worse than even our without-post-processing results.

per-token precision and recall
we then determined the results on a per-word basis. this is equivalent to olsson et al.'s protein name parts metric  <cit> . in this analysis a true positive is a single word that is tagged as gene both in the gold standard and by our system. as would be expected, performance on single words is better than the term-level results, with an average precision of  <dig>  and average recall of  <dig>  without post-processing, and an average precision of  <dig>  and average recall of  <dig>  with post-processing. post-processing yielded a 4% improvement in precision, which was less improvement than was seen for full gene names. recall actually degraded somewhat. these data are consistent with our findings that many of our post-processing steps correct the boundaries of gene mentions at the term level.

per-token performance on unknown words
we use the phrase unknown word to describe a word that was not previously seen in the training corpora. we used the per-token precision and recall metrics described above to evaluate the performance of our system on unknown words. the cross-validation average precision was  <dig>  and average recall was  <dig>  without post-processing. average precision was  <dig>  and average recall was  <dig>  with post-processing. post-processing yielded little improvement in performance for unknown words. in the comparison with overall per-token precision and recall , the precision is 10% and 7% worse for with and without post-processing, respectively.

in order to better characterize the effect of unknown words on the performance of our system, we analyzed false positives that are one word in length. the percentage of false positives that are one word long is 40% and 43% for our system without post-processing and with post-processing, respectively. these ratios are similar to the ratio of one-word gene mentions in the corpus given in table  <dig>  table  <dig> shows the effectiveness of post-processing on one-word false positives with respect to the number of times the words corresponding to the false positives were seen in the training data. this table shows that  <dig> % of one-word false positives that correspond to unknown words were corrected while  <dig> % of one-word false positives that correspond to a word that had been seen twice or more in the training data were corrected. after post-processing is complete, 93% of the remaining one-word false positives correspond to unknown words or words that have been seen only once. this suggests that the lexicon contained in the training data is very important for being able to successfully apply our post-processing steps. we believe that a larger training set covering a larger lexicon would help improve the performance of our system.

effect of term length on performance
 <dig>  recall and precision tend to be better for shorter gene mentions. however precision tends to degrade slightly for gene mentions that are only one word long. as length in words increases there is no drastic drop in performance until length in words reaches five.

 <dig>  post-processing is effective on all gene mentions of any length. however, it seems that improvement in performance is greater for longer gene mentions. this is probably due to lexicon-based post-processing that corrects boundaries.

overall effects of post-processing
the main effect of rule-based and lexicon-based post-processing is an increase in precision. in cross-validation for full gene names, average precision increased from  <dig>  to  <dig> , and average recall increased from  <dig>  to  <dig> . in the official test, precision increased from  <dig>  to  <dig>   and  <dig>   and recall increased from  <dig>  to  <dig>   and  <dig>  . on the level of individual token , post-processing had a much smaller, and not always positive, effect.

the main effect of dictionary-based post-processing is an increase in recall. recall in the open division is increased by  <dig> % from the closed divison recall. table  <dig> shows the individual post-processing effects in our cross-validation testing. it shows that removing rule-based post-processing or removing the lexicon-based post-processing from the post-processing steps has nearly the same effect. removing the abbreviation rules from the post-processing has the least effect, which indicates that it may be less important for the system's overall performance.

discussion
the results of this study raised four questions that we believe should be addressed in the near future â€“ two general to the entire effort, and two specific to our system.

first, it would have been useful to have an estimate of the upper bound on accuracy for any entity identification system trained on the biocreative corpus, which is a function of how consistent and correct that data is. assessing the inter-reviewer reliability for this corpus and/or an assessment of the corpus by independent human judges would be very helpful in understanding the difficulty of the task. our guess is that an f-measure of  <dig> is probably within seven points of the upper limit.

another important question that arises from this effort is to determine the effect of training corpus size on performance. this could be achieved by training on successively bigger percentages of the training corpus. if performance flattens off before the entire corpus is used then simply increasing corpus size may not be useful. however, if the performance has not yet flattened off , then there is hope that our system can be improved simply by training on more data.

there are two aspects specific to our system that we would like to explore. the first has to do with deciding which pos tagger is used. a head-to-head competition between the tnt tagger, the brill tagger, and perhaps others would help determine whether or not the choice of tagger is an important decision. we would look at both the raw performance of each tagger as well as the performance of post-processing rules applied to the results of each tagger. tnt would have an unfair advantage since the error analysis was performed on its output. additional error analysis could be done on the output of the other systems. it may also be useful to combine the outputs of multiple taggers as well.

the second aspect has to do with the use of dictionaries. our system used a simple algorithm to exploit a single data resource from the ncbi. it would be informative to have head-to-head competitions between multiple data resources  as well as compare algorithms for making use of these resources. our study suggests that this would be helpful for improving recall at least modestly.

CONCLUSIONS
the pos-tagging-based approach that we took from the abgene system worked reasonably well. post-processing rules, which included pattern-based rules, rules that used abbreviation recognition heuristics, and lexicon-based rules, worked well to increase both precision and recall. the overall f-measure rose from  <dig>   to  <dig>   on the official test. our use of domain-specific dictionaries was less effective, giving an increase of only  <dig>  in f-measure to  <dig>  compared to the post-processing without dictionaries approach. our conclusion is that either much more sophisticated algorithms that make use of dictionaries need to be employed, or the dictionaries themselves are not sufficient.

