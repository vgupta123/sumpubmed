BACKGROUND
the assignment of international classification of diseases, 9th revision, clinical modification  codes serves as a justification for carrying out a certain procedure. this means that the reimbursement process by insurance companies is based on the labels that are assigned to each report after the patient's clinical treatment. the approximate cost of icd-9-cm coding clinical records and correcting related errors is estimated to be about $ <dig> billion per year in the us  <cit> . there are official guidelines for coding radiology reports  <cit> . these guidelines define the codes for each disease and symptom and also place limitations on how and when certain codes can be applied. such constraints include the following:

• an uncertain diagnosis should never be coded,

• symptoms should be omitted when a certain diagnosis that is connected with the symptom in question is present and

• past illnesses or treatments that have no direct relevance to the current examination should not be coded, or should be indicated by a different code.

since the icd-9-cm codes are mainly used for billing purposes, the task itself is commercially relevant: false negatives  will cause a loss of revenue to the health institute, while false positives  is penalised by a sum three times higher than that earned with the superfluous code, and also entails the risk of prosecution to the health institute for fraud.

the possibilities of automating the icd-9-cm coding task have been studied extensively since the 1990s. larkey and croft  <cit>  assigned labels to full discharge summaries having long textual parts. they trained three statistical classifiers and then combined their results to obtain a better classification. lussier et al.  <cit>  gave an overview of the problem in a feasibility study. lima et al.  <cit>  took advantage of the hierarchical structure of the icd- <dig> code set, a property that is less useful when only a limited number of codes is used, as in our study.

automating the assignment of icd-9-cm codes for radiology records was the subject of a shared task challenge organized by the computational medicine center  in cincinatti, ohio in the spring of  <dig>  the detailed description of the task, and the challenge itself, can be found in  <cit> , and also online  <cit> . the most recent results are clearly related to the  <dig> challenge on classifying clinical free text, some of the systems that have been published so far can be found in  <cit> ,  <cit> ,  <cit>  and  <cit> . here  <dig> teams submitted well-formatted results to the challenge and, among the top performing systems, several exploited the benefits of expert rules that were constructed either by experts in medicine, or by computer scientists. this was probably due to the fact that reasonable well-formatted annotation guides are available online for icd-9-cm coding and that expert systems can take advantage of such terms and synonyms that are present in an external resource . statistical systems on the other hand require labeled samples to incorporate medical terms into their learnt hypothesis and are thus prone to corpus eccentricities and usually discard infrequent transliterations or rarely used medical terms. while the cmc challenge involved a considerable but limited number of codes , the feasibility of constructing expert systems for hundreds or thousands of codes is not straightforward and undoubtedly time consuming if one wants to model all the possible inter-dependencies between labels. thus in our study we examined how well top performing expert rule-based classifiers could be approximated via the extension of basic skeleton expert systems by machine learning methods. such skeleton rule-based classifiers can be obtained automatically or semi-automatically , directly from the publicly available icd-9-cm coding guides. table  <dig> shows how coding guides can be transformed into basic coding systems. thus our goal in this article is to substitute the laborious process of manually collecting rare synonyms , inter-label dependencies and common abbreviations from labeled data with training machine learning models that perform these steps. this approach exploits the advantages of expert systems, is able to handle rare labels effectively , and it is easier to apply even for a high number of labels as the more time-consuming steps of constructing rule-based icd-9-cm coding systems are replaced by machine learning methods.

RESULTS
discovering inter-label dependencies
in order to discover relationships between a disease/illness and symptoms that arise from it, we applied statistical learning methods. for example, the presence of code  <dig> corresponding to pneumonia implied that the patient has certain symptoms like  <dig>  and  <dig>  . since our initial rule-based system that simply implemented the instructions found in the icd-9-cm guide lacked such information, it regularly overcoded documents with symptom labels. this kind of overcoding appeared in the form of false positive symptom labels in the output of the rule-based system.

this overcoding can be overcome by adding decision rules to the expert system to delete some symptom labels when specific labels corresponding to diseases are found. these extra decision rules can be produced manually. we found four rules good enough to worth adding to a rule-based system . these were:

• delete code  <dig>  when code  <dig> is present,

• delete code  <dig>  when code  <dig> is present,

• delete code  <dig>  when code  <dig>  is present and

• delete code  <dig>  when code  <dig>  is present.

deriving such rules based on observations of the data itself is actually quite time-consuming, so we decided to test whether or not such rules could be induced automatically. to do this, we used the labels assigned by the initial rule-based system as features and trained a c <dig>  decision tree classifier for each symptom label, treating the symptom false positive labels as the positive class and all other cases as negative examples. this way the decision tree learned to distinguish between false positive symptom labels and true positive ones. this statistical approach found five meaningful decision rules in the dataset, among which were all four rules that we enumerated above. the new rule was:

• delete code  <dig>  when code  <dig>  is present.

this fifth rule did not bring any improvement on the challenge test set . because the four useful rules and the additional one that brought only a marginal improvement on the training dataset were found via our statistical approach – without inducing any detrimental disease-symptom relationships – we can say that this step of creating icd-9-cm coding systems can be successfully automated.

the modeling of inter-label dependencies brought about a  <dig> % improvement in the performance of our rule-based system, raising the micro-averaged fβ= <dig> score from  <dig> % to  <dig> % on the training dataset and from  <dig> % to  <dig> % on the challenge test set.

collecting synonyms from labeled data
although the available icd-9-cm guides contain many useful synonyms, and incorporating them has the advantage of adding such phrases to the classifier model that are indicators of the corresponding label with a very high confidence, the coverage of these guides is not perfect. there are expressions and abbreviations which are characteristic of the particular health institute where the document was created, and physicians regularly use a variety of abbreviations. as no coding guide is capable of listing every possible form of every concept, to discover these infrequent keywords the examination of labeled data is necessary. the extension of the synonym lists can be performed via a manual inspection of labeled examples, but this approach is most laborious and hardly feasible for hundreds or thousands of codes, or for a lot more data than in the challenge. hence this task should be automated, if possible. the effect of enriching the vocabulary acquired from coding guides is very important, and this step reduced the classification error by 30% when we built a system manually .

since missing transliterations and synonyms can be captured through the false negative predictions of the system, we decided to build statistical models to learn to predict the false negatives of our icd-9-cm coder. a token level vector space representation of the documents  was applied here as a feature set. this way we expected to have the most characteristic phrases for each label among the top ranked features for a classifier model which predicted the false negatives of that label.

training a c <dig>  decision tree for false negatives
we tested a simple approach of building statistical classifiers to predict the false negatives  of the basic rule-based system. of course such predictions can be made by discovering terms that were missing from the synonym lists of the rule-based classifier.

we used a c <dig>  decision tree learning algorithm for this task. the decision tree builds models that are very similar in structure to the rule-based system . thus such learnt models can be directly incorporated into the rule-based system or the classifier can be used in a cascade architecture after the rule-based system has performed pre-labeling.

with this approach we managed to extend the rule-based model for  <dig> out of  <dig> labels. about 85% of the new rules were synonyms  and the remaining 15% were abbreviations . the procedure improved the overall micro-averaged fβ= <dig> scores from  <dig> %  and  <dig> %  to  <dig> % and  <dig> %, respectively. the system yielded better recall  than precision  on the challenge test set.

iterative enriching using maximum entropy classifier
we examined the dictionary enriching task using maximum entropy models in an iterative way. we used the p probabilities for each token level uni-, bi- or trigram feature as an indicator of feature relevance. we ranked all words and phrases according to their relevance on false negative predictions and added the most reliable keywords and phrases to the dictionary of the rule-based classifier. this procedure was repeated until the most significant feature brought fewer than  <dig> additional true positive predictions. with this approach we managed to extend the rule-based model for  <dig> labels. the set of terms acquired by this iterative method is twice as large as that obtained by the decision tree. even so, the difference between their accuracies on the challenge test dataset is unquestionably below the level of significance. this approach improved the overall micro-averaged fβ= <dig> scores from  <dig> %  and  <dig> %  to  <dig> % and  <dig> %, respectively. the system yielded better recall  than precision  on the challenge test set.

discussion and 
CONCLUSIONS
discussion
the cmc challenge on classifying clinical free text using natural language processing demonstrated that expert rule-based approaches are competitive to, or even outperform, purely statistical approaches to the icd-9-cm coding of radiology reports. on the other hand, the construction of systems that use hand-crafted decision rules would become more laborious and hard to accomplish when the number of codes involved in the task is a magnitude bigger than that used in the cmc challenge. to overcome this problem, we examined the possible ways of replacing certain phases of the construction of rule-based systems by statistical methods, while keeping the advantages of expert systems.

our results demonstrate that, after the conversion of icd-9-cm coding guides , the major steps of building a high performance rule-based classifier for coding radiology reports can be replaced by automated procedures that require no human interaction. we studied two aspects of the construction of a purely hand-crafted rule-based system, namely the modeling of inter-label dependencies, which is a special characteristic of icd-9-cm coding and the enriching of the synonym list of the rule-based system with rare transliterations and abbreviations of symptoms or diseases. the results of our experiments are summarized in table  <dig>  a webpage where all the systems described can be accessed and tested online is available at  <cit> .

all values are micro-averaged fβ= <dig> 

the 45-class statistical row stands for a c <dig>  classifier trained for single labels. the cmc challenge best system gives the results of the best system that was submitted to the cmc challenge. all our models use the same algorithm to detect negation and speculative assertions, and were trained using the whole training set  and evaluated on the training and the challenge test sets. the difference in performance between the 45-class statistical model and our best hybrid system  proved to be statistically significant on both the training and test datasets, using mcnemar's test with a p <  <dig>  confidence level. on the other hand, the difference between our best hybrid model  and our manually constructed icd-9-cm coder  was not statistically significant on either set.

to perform these tasks with machine learning models, we trained classifiers to predict the errors of a basic rule-based system which relies just on the knowledge found in the coding guide. we trained c <dig>  decision trees to predict false positive labels using the output of the rule-based system as features to discover disease-symptom relations, using pre-labeled training data. here we found the same dependencies, and got the same improvement in performance, as that of a system with hand-crafted rules for inter-label dependencies.

to enrich the list of synonyms used by the rule-based system with additional phrases and abbreviations, we trained c <dig>  and maximum entropy classifiers to predict the false negatives of the rule-based system using the vector space representation of the texts. these statistical models can be used in a cascade model following the rule-based system, or the most reliable keywords found can be incorporated as decision rules into the expert system. however, the difference in performance between these two different machine learning methods was below the statistical level of significance.

the extracted synonyms and abbreviations correlated well with those phrases added manually to the hand-crafted system. a small percentage of the phrases were clearly noise, that causing the systems to overfit on the training dataset – these systems achieved better performance on the training set than the hand-crafted system and performed somewhat worse on the evaluation set, see table  <dig>  the manual filtering of phrases proposed by the learning models could be performed in a few minutes, and this way more robust  hybrid systems could be built with minimal effort. in our experiments we performed the major steps of the construction of a hand-crafted expert system using statistical methods. evidently, the performance of the hand-crafted system is an upper bound on the performance that can be attained this way. we found that similar results could be achieved via statistical models by improving basic rule-based classifiers like those we obtained by an entirely hand-crafted system. the main contribution of the study described here is that such automatic systems can be constructed at a lower cost, with less human labour.

agreement rates
the results reported here are close to the performance that human expert annotators would achieve for the same task. the gold standard of the cmc challenge dataset is the majority annotation of three human annotators. the inter-annotator agreement statistics are shown in table  <dig>  we should mention here that the human annotators had no access to knowledge about the majority labeling, while models trained on the challenge dataset can model majority labeling directly. this way, human annotator agreement with majority codes should be higher if they had the chance of examining the characteristics of majority labeling. on the other hand, the annotators influenced the target labels as these were created based on their single annotations. this fact explains why all annotators have a higher agreement rate with the majority annotation than with other human annotators. it would be interesting to see the agreement rate of a fourth human annotator and majority codes, given that the human annotator could now examine the characteristics of the majority codes but have no direct effect on their assignment. this statistic would provide a better insight into the theoretical upper bound for system performance  on this task.

a <dig>  a <dig> and a <dig> refers to annotators  <dig>   <dig> and  <dig> respectively. gs stands for gold standard labeling, while basicrb represents our basic rule-based system that models inter-label dependencies. hybrid denotes our hybrid rule-based + maxent statistical model.

the significantly lower agreement between single human annotators shows that different health institutes probably have their own individual style of icd-9-cm labeling. we also listed the agreement rates of annotators and the gold standard labeling with our basic rule-based system with label dependencies. this system can be regarded as a hypothetical human annotator in the sense that it models the icd-9-cm coding guide an annotator should follow, not the gold standard labeling of the data itself. the fact that human annotators agree slightly better with this system than each other also proves that they tend to follow specific standards that are not neccessarily confirmed by official annotation guidelines. it is also interesting to see that majority labling has a significantly higher agreement with this system than single annotators. this observation seems to justify that majority coding of independent annotators indeed estimates icd-9-cm coding guidelines better than single expert annotators.

all the above findings hold when we restrict the agreement evaluation to the  <dig> labels that appear in the gold standard. agreement between human annotators remains comparable to their agreement with the the coding guide . each of the annotators have one preferred partner with whom their agreement is slightly better than with the brb system, and show definitely lower agreement with the other human annotation. the gold standard labeling agrees better with brb than any single annotation by almost 3%, which also emphasises that majority annotation is capable of correcting mistakes and is better than any single human annotation.

error analysis
the current systems have certain limitations when compared to the icd-9-cm coding of expert annotators. take, for example, the following record from the training set:

clinical history:none given.

impression:normal chest.

the annotators – given that the record itself contains nothing of relevance for any icd code – then conclude that this must be report of a routine chest x-ray  as these reports originate from a radiology department. such complex inferences are beyond the scope of automated systems. still, the obvious advantage of automated coding is that it is less prone to coding errors in simpler  cases. some improvement, however, could be achieved by using a more sophisticated method to identify the scope of negation and speculative keywords than we applied here. take, for instance, the following record:

clinical history:cough and fever.

impression:right middle and probable right lower lobe pneumonia.

the use of syntactic structure to determine the scope of negation and speculative keywords would allow the coding of pneumonia here. our current system considers the token pneumonia as speculative, but in the second sentence right middle corresponds to pneumonia as well and is in a non-speculative context.

