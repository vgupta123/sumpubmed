BACKGROUND
experimental results related to molecular biology research are published in the form of journal articles and stored in biomedical literature databases such as pubmed  <cit> . the amount of scientific articles is increasing dramatically. nonetheless, useful information in terms of functional descriptions of proteins and genes, is still extracted from these publications manually by human experts. the extracted information is consequently used to build up annotations within biological databases describing relevant aspects of these proteins. these annotations are also commonly used to automatically infer annotations to other, sequence-related proteins, as often protein sequence similarity can give clues to functional similarity. this automatic sequence-based annotation in some cases can be misleading as even the conservation of function itself is often difficult to estimate  <cit> . as the amount of feasible annotations using manual information extraction is limited, it may not keep up with the pace of article publication and thus represents a severe bottleneck preventing knowledge gain. current estimates are that 10% of protein sequences are annotated from original sources with the remaining 90% being inferred from that 10%.

efficient automatic filtering and information extraction algorithms for biomedical literature are needed to compliment manual information extraction and reduce the human effort needed. moreover, they could aid in maintaining links from database annotations to article sources. in order to speed up the annotation process, a broad variety of methods drawn from the field of text mining and statistical natural language processing  have been applied to common biological problems. some techniques have been applied directly to derive important information for protein annotations  <cit> . other approaches have been intended to complement the analysis of microarrays  <cit>  in terms of the biological background information of the analyzed genes. attempts have also been made to automatically extract protein interactions  <cit>  and to improve protein sequence similarity searches  <cit> . although a considerable number of text mining methods applied to biomedical articles are currently available, common assessment initiatives were missing. this has made it especially cumbersome to compare the relative strengths of different techniques so as to improve future applications.

we present a first version of our approach to extract function annotation passages from free full text articles. this procedure produced the highest number of correct annotation extractions in the biocreative contest   <cit> . the biocreative is a community wide experiment with the purpose of assessing distinct techniques of named entity recognition and gene tagging   <cit>  as well as automatic protein annotation extraction   <cit> .

biocreative task  <dig> , was concerned with the automatic extraction of protein annotations. the evaluation was carried out by expert annotation database curators from the ebi goa team  <cit> , providing a high quality assessment. in this task, the entities involved in the annotation, a protein identifier and a gene ontology  code were provided. go is a dataset containing consistent descriptions of gene products in the form of controlled vocabulary terms. it consists of an ontology with a directed acyclic graph structure. each entry may belong to one of three categories, molecular function, biological process or cellular component and has a unique associated go code. for each go code and protein-identifier pair , given a full text article, the text passage which would be useful to derive a go-based protein annotation  should be returned. those text fragments should thus contain traceable associations between the protein entity and the go-term entity.

in order to extract the required annotation passages, lists of terms and word types for entities involved in the annotation were compiled. for each list, a semi-heuristic scoring scheme was developed, which was used to score sentences after tagging them with the elements recovered from each list. those scores were then used by averaging sentence sliding windows to score each sentence taking into account the context of flanking sentences. the highest scoring sentence window was returned as the annotation evidence text passage.

materials and methods
gene ontology annotation dataset
in order to analyze the associations of proteins with go-terms within scientific articles we used the gene ontology annotation database   <cit> . this database provides annotations of proteins using go-terms through associations derived from the scientific literature. for instance for the swissprot annotation database accession number 'o00115'  the go identifier 'go : 0003677' was annotated using information contained in pubmed document with the pubmed identifier  <dig>  we extracted a dataset of  <dig> human protein- go-term annotations contained in goa and compiled the pubmed abstracts relevant for annotation as our "training" dataset. only goa database annotations with traceable author statements, as provided by the goa-tas evidence code were used. traceable author statement annotations refer to annotations in which the original experiments are traceable through the article by a corresponding author statement. those abstracts served for further statistical analysis of the sub-tag sets of each annotation entity. this dataset was rather noisy.

protein entity tag set
in order to identify functional annotations for proteins, information extraction of text passages relevant to them is crucial. as proteins constituted one of the entities involved in the annotation, we constructed a tool which generates lists of sub-tag sets for a given query protein. each sub-tag set consisted of a list of word types and names associated with the protein query , characterized by a distinct degree of relation with the original query protein.

among the sub-tag sets used for the protein entity class were the original protein name, symbol or identifier, e.g. the swissprot accession number. in the case of this contest the swissprot accession number was provided as the protein query, consisting of a unique identifier for each protein entry in the swissprot database   <cit> . within textual sources, protein symbols or names are often expressed in the form of different typographical variants  <cit> . therefore we derived another sub-tag set containing protein variants which were generated through a rule based pipeline of protein name processing . often different names for a certain protein,  are obtained through cross references to other biological databases.

we developed a database which contained cross linked protein entries derived from several sources, including the hugo, omim, swissprot, unigene and locuslink databases. this database allowed us to extract all the possible naming conventions and definitions for a query protein based on its database identifier. all the protein symbols and names obtained through external links were also incorporated into a separate sub-tag set. for the example presented above, the elements contained in this sub-tag set included: 'deoxyribonuclease ii, lysosomal', 'dnl' and 'dnl2' . the word types forming the gene names also constituted a sub-tag set. for instance, in the case of o <dig> the following word types were part of this sub-tag set: 'deoxyribonuclease', 'ii', 'lysosomal'. the pragmatic context information was also exploited as a sub-tag set to take into account the meronymic relations  for proteins. hence we used terms contained in the global open biology ontologies  dataset. gobo contains structured sets of terms related to different aspects within the field of molecular biology. we used the mutation event and sequence ontology tables, and exploited them as a separate sub-tag set. the presence of such meronymic terms might aid in disambiguating certain protein symbols using context information. examples of the "gobo mutation event sub-tag set" were mutation  and conformational change  and of the "gobo sequence ontology sub-tag set" were est  and transcript .

gene ontology term tag set
as already mentioned go-terms are controlled vocabulary items embedded into an ontological structure. to determine if these terms are suitable for nlp tasks, the lexical properties of go  <cit>  were analyzed. this analysis revealed that most of the go-terms were useful for nlp approaches and some of them are even rather often encountered in free text. nevertheless after a closer look at the go-terms, we decided to construct a sub-tag scheme by analogy to the gene/protein entity . some of the go-terms, especially those which denote more specific features, do not resemble what one would expect in free text. hence it would be rather cumbersome to tag them in biomedical texts. also certain terms within the categories molecular function and cellular component did not seem to correspond to natural language expressions due to the presence of special characters such as the backslashes. some terms indicated the organism source which in principle should not form part of the term itself . from a linguistic point of view, a significant difference between protein symbols/names and the go-terms is that the former are proper nouns while the latter can be considered to be adverbial nouns. therefore, the go-terms are even more diffcult to identify in free text as they often lack morphological characteristics which are present in proper nouns, such as capital letters or special characters as in the case of gene names . to recover some of the go-terms it seemed therefore crucial to process them so that they would resemble their natural language  variants, namely how they might be encountered in free text. we developed for this purpose a rule based system which modifies an input go-term returning several potential nl-variants. some of the processing steps performed by this system were minor typographical changes  and word token substitutions by corresponding synonyms or adjective into noun conversions , collocation shuffing and preposition insertions  are er membrane virus budding and endoplasmatic reticulum membrane viral budding and for condensed nuclear chromosome/perocentric region  one of the nl-variants is pericentric region of condensed nuclear chromosome. among the resources provided by the gene ontology consortium were also a go-term synonym list and links for go-terms to external databases such as the mips database keywords  <cit> . these synonyms and externally linked keywords were included as a separate tag set. finally the word types from which go-terms and the go-term definitions  are formed, were included in two respective tag sets, e.g. for the go-term regulation of mitotic recombination , the word types 'regulation', 'mitotic' and 'recombination' were present in the sub-tag set containing go-term word types. we generated stemmed versions for all the tag sets using the porter stemmer  <cit> .

analysis of sub-tag sets using goa abstracts
after defining the different sub-tag sets for each entity, we determined their utility for extracting annotation passages. therefore we analyzed first the average number of occurrences for each sub-tag using goa abstracts . as already pointed out this dataset was not representative enough in context of annotation text fragments and thus did not constitute a conventional training set. moreover the total number of studied goa abstracts was small and noisy, so it did not satisfy all the needed criteria for a representative annotation text sample. nevertheless, we considered that the average occurrence of each sub-tag reflects somehow its specificity as an evidence item. we thus derived a heuristic weighting factor  associated with each sub-tag which depended on the type of relationship it displayed with the original query and its own average occurrence. more specific sub-tags  were given a higher weighting score than more general sub-tags . the stemmed versions of each sub-tag were scored lower relative to the original  word/s . in the case of the word types forming the go-term, go-term definitions and composed protein names, an extended stop word filtering was performed. we started using several different heuristic weights for each sub-tag. then, using a small set of sample articles, we adjusted the weights until the correct text passages  were returned as the highest scoring text window.

let hi be the heuristic sub-tag score  for each sub-tag i for a given entity, then hi would be calculated by:



where  is the average number of occurrences of elements of sub-tag i in goa sentences and ei corresponds to the relative heuristic weight used for sub-tag i, based on domain expert estimates, the relation to the query term and finally some adjustments based on a small sample of full text articles containing an annotation passage. notice that the heuristic sub-tag weights for the go-term entity were in general higher than for the protein entity.

low level document processing and instantiation of sub-tag elements
the test set provided for task 2a consisted in full text articles from the journal of biological chemistry in sgml format. the first step consisted in previous sgml-parsing, low-level processing and junk formatting. all the analyzed documents were subjected to a rule-based sentence splitting algorithm. section and paragraph information was retained in the form of empty sentences . after performing the low-level processing, we tagged to the document sentences the lists of word types and terms contained in the sub-tag sets for each entity, using an exact string matching algorithm. thus only the sub-tag elements which are matched to a sentence are instantiated. each sentence has thus a set of go entity and protein entity sub-tag elements which were encountered within this sentence. there are also sentences with no matches to sub-tag elements, for instance the empty sentences corresponding to sections and paragraphs. in the case of the following sentence the sub-tag it golgi corresponding to the sub-tag class containing go-term forming words was machted: once fully glycosylated, the enzyme is phosphorylated and released from membranes either in or after the trans-golgi compartment .

trapezoid sentence sliding window
the use of the concept sliding window spans a broad variety of domains, such as information technology where it has been widely used in signal processing for analysis of frequent items in packet streams  <cit> . in bioinformatics, it has been applied to protein sequence analysis such as the prediction of transmembrane protein segments and to generate protein hydropathy profiles  <cit> . sliding windows have also been applied within the field of natural language processing for collocation detection  <cit> . in our case we explored the use of averaging sliding windows for information extraction tasks. we applied a trapezoid sentence sliding window to extract relevant text fragments for biomedical entities  using biomedical literature. the sliding window unit consists of sentences.

let l be the total number of sentences forming the trapezoid sliding window , then the sentence position weight wi within the window was determined by



in this way, we scored the flanking sentences comprising the sliding window lower then the core sentences of the window. this is based on the assumption that the flanking sentences might contain contextual information useful for scoring the sentences relative to the presence of a given entity.

the average sliding sentence window entity score,  was calculated by



where wi is the corresponding sentence position weighting factor and l is the sentence window size. in the case of the entity profiles, l =  <dig> sentences. si is the sum of the scores of the matching sub-tags of a given sentence, n being the total number of matched items and hi the associated heuristic sub-tag score.



entity profiles
the "trapezoid sliding windows" generate average entity scores for each sentence within the document. taking the average sentence scores relative to the sentence number, it is possible to generate a document entity profile for the go-term as well as for the protein. the higher the average sentence score, the more likely it should be that the corresponding text window contains information relative to the entity or to items associated with the entity. these profile scores were used to determine relevant text passages for each entity. the values of the scores can hint at the types of sub-tags being matched to the window, as high average window scores are associated with high scoring sub-tag matches. in general the average sentence scores for proteins are lower then for go-terms, this is due to the overall weighting scheme used for the sub-tags.

annotation profile
after calculating the average entity score for each sentence we had to combine both resulting entity document profiles into a unique annotation document profile . the annotation profile should score every sentence window on whether it contains suitable information for annotating proteins with go-terms. this was achieved using a combining sliding window which, as for the entity sliding windows, averages the sentence scores over a certain window size. the sliding window size used to generate the annotation profile was reduced to l =  <dig> sentences, as larger windows would result in text fragments too cumbersome to be evaluated by the assessor. the sentence position weights wi used in the case of the entity windows were ignored, meaning the flanking sentences had the same weighting as central sentences.

we therefore assumed that semantic information expressing the relation between two entities should be restricted to a distance expressed in sentences.

to calculate the average annotation sentence score , the entity profile sentence scores were used. the average annotation score  for a given sentence window is given by:



where l =  <dig> ,  is the average entity sentence score for the go-term and  the average entity sentence score for the protein. the sentence window with the the highest average annotation score a' was returned: a' = arg max  score as the annotation passage.

RESULTS
the proposed procedure derives individual sentence scores for each entity or class. it uses the weight scores of matched word tokens and entity terms. each weight score depends on the type of tag  the matched token belonged to. every sub-tag set consist of a list of word types or terms with a certain degree of association with respect to the initial query entity. the weight score itself for a given sub-tag is based on its occurrence in gene ontology annotation   <cit>  abstracts and additional heuristic estimates. the sentence window then slides over the whole document generating two entity profiles by averaging over the summed sentence scores comprised in the window. this results in a protein entity profile and a go-term entity profile when considering the average entity sentence score for each of the window positions. these profiles serve as input for a second averaging procedure which combines two entity-profiles to obtain a single document annotation profile using a sentence sliding window. the highest scoring sentence window was returned as the annotation evidence text.

we submitted a total of  <dig> text passages as candidate predictions to provide relevant textual information for protein-go-term annotations. each segment used for annotation prediction consisted of four consecutive sentences extracted from the corresponding full text article. the curators evaluated  <dig> of our submissions on whether they were relevant as traceable annotations . the assessment included separate evaluations of the extraction of each individual entity . three distinct evaluation categories were proposed by the assessors. the category perfect, refers to correct predictions of the annotation textual passages, the category general refers to predictions that are 'in principle correct' but too general for practical use  and finally the category low which in effect refers to a wrong prediction.

due to the fact that a vast amount of data had to be evaluated, a minor fraction of submissions remained without assessment and were returned with the label none.

annotation extraction
we obtained the highest number of correct predictions of annotations for task 2a, with a total of  <dig> correct textual evidences . this means  <dig> % of submissions were correct. nevertheless, with respect to precision, there were groups with a higher precision , but they did not produce results for all the queries. there were  <dig> cases were the go-term was correct and the protein extraction was general . moreover predictions with correctly predicted protein entities and general go-term prediction constituted a total of  <dig> cases . a sample annotation extraction which was evaluated as correct for swissprot accession number  <dig>  and go-id  <dig>  was:

in addition, a single point mutation in the fyve finger motif at cysteine residue  <dig>  is sufficient to abolish its endosomal association. its endosomal localization is also sensitive to the phosphatidylinositol 3-kinase inhib itor, wortmannin. using in vitro liposome binding assays, we demonstrate that myc-tagged endofin associates preferentially with phosphatidylinositol 3-phosphate, whereas the c <dig> s point mutant was unable to do so. we also show that endofin co-localizes with sara but that they are not associated in a common complex because they failed to co-immunoprecipit ate in co-expressing cells.

as can be observed from above the example, the protein name and relevant word types for the go-term are both contained within this textual passage.

entity extraction
with respect to the evaluation of the individual entity extractions, we obtained a total of  <dig> correct evidences for the protein entity, which corresponds to about 56% of total submissions . this seems a satisfactory result, considering that we did not apply any anaphora  resolution algorithms and were only provided with the protein swissprot identifier. we extracted a total of  <dig>  correct go-term evidences. our system thus performed worse for go-term extraction than protein entity extraction. for instance, the example presented above for annotation extraction was also extracted correctly for the protein entity endofin and the go-term entity phosphatidylinositol binding.

annotation extraction relative to go categories
the difficulty of predicting annotations varied with go category . also, the extraction of go-term entities themselves depended heavily on the associated go category. the highest percentage of correct predictions of go-terms came from the category "cellular component", followed closely by the "molecular function" category. terms belonging to the "biological process" category were the most cumbersome to extract. this concurs with previous attempts to map terms to biomedical articles  <cit> , that have also shown that recall was significantly lower for terms from the biological process category. this is explained by the fact that these terms are often expressed colloquially in different ways.

the ranking of correct go-term extraction and of correct go annotation extraction displays a shift in the case of the cellular component and the molecular function groups. the highest number of correct annotations corresponded to the category molecular function instead of cellular component. as word types forming cellular component terms are often used in other contexts , the false positive rate increased.

discussion
our results were more convincing for protein entity extraction than for go-term extractions. this, we suggest, is the result of two principal factors. one is the lexical properties of protein names and symbols considered as proper names, which, often display string features which are easier to detect, such as capital letters and special characters; while go-terms are adverbial nouns, often lacking such characteristics. this could explain the higher entity extraction achieved for proteins. another reason could be the limited number of synonyms and natural language variants of proteins when compared to go-terms. in other words, there are fewer alternatives for expressing a protein entity in free text, while go-terms can be reformulated in a broad variety of ways, often not even in the form of continuous text segments.

our system is especially useful for the extraction of molecular function annotations for proteins; while in the case of biological process annotations it could still be improved, as the extraction of go entities from this category is still rather low.

aside from increasing system speed and offering alternative sliding window sizes, among the potential refinements are the use of different sub-tag scoring schemes. a statistical analysis of the precision of each sub-tag set revealed significant differences in sub-tag precisions depending on go-category . thus a distinct score for each sub-tag based on its precision and depending on the corresponding go category might be useful. for instance, in the case of sub-tag  <dig> , the precision for the category biological process is considerably lower than the other two categories.

as the dataset used to derive the sub-tag scores  was very noisy, it was diffcult to perform statistically significant analysis using common nlp methods. among the sources of noise encountered in this dataset were the different annotation conventions depending on the background knowledge of the annotator. further, major changes in curation conventions over time might have influenced the annotation extraction criteria used for the manual annotations. the most significant problem was the fact that the actual annotations were performed using full length articles. as we only had access to document abstracts, whilst the passage of text relevant for annotation extraction will often be located in other parts of an article , some of the abstracts might even lack the text segments relevant for annotation. therefore some of the sub-tag weights used did not correlate with their precision.

the annotation score may serve to determine the confidence intervals of the distinct predictions . thus depending on the obtained annotation score it is possible to estimate the reliability of the automated annotation extraction. correct predictions correlate with higher annotation scores while bad predictions tend to score significantly lower.

we believe that including contextual sentence patterns could be useful to improve annotation extraction techniques. those patterns are based on verbs which occur in sentences describing functional aspects of proteins. some initial steps have been made in this direction in the form of the automatic extraction of sentence patterns  <cit> , but there was no evaluation using curator based annotations. the sliding sentence window method proposed here was able to score text passages based on contextual information about whether they contain relevant information for a given entity. combining the window scores allows the merging of individual entity extraction into relation  extraction. the use of annotation scores provided by the sliding window could provide confidence intervals in terms of the precision of predicted annotations. the optimization of the scoring scheme based on detailed statistical analysis of each sub-tag set might be useful to enhance this system. also flexible windows size and alternative combinations of the entity scores might improve the performance. we believe that, with such improvements, these preliminary steps could lead to a method useful for real world applications. we are also planning to apply this strategy in the context of gene expression array data.

CONCLUSIONS
our results demonstrate how contextual information could be exploited to extract protein annotations using full text articles through the use of sentence sliding windows. our approach was validated at the biocreative evaluation, which allowed additional performance comparison with alternative techniques. this system performed better for individual entity extraction's  when compared with annotation extraction's .

this sentence sliding window method is able to score sentence windows of full text articles relative to the given query entities such as proteins and go terms, as well as annotations based on those entities in cases were a standard training set is missing.

authors' contributions
mk and mp conceived the initial idea, implemented it and performed the low level processing steps and statistical evaluations and av supervised and coordinated the project. mk and av authored the manuscript.

