BACKGROUND
hydrogen nuclear magnetic resonance  spectroscopy is an emerging tool for metabolomic analysis of cell culture. in contrast to the established use of 13c-nmr for targeted elucidation of intracellular metabolic flux , the quantification of a broader cellular metabolome with 1h-nmr in the context of recombinant protein production has been much more recent . unlike 13c-nmr, which requires relatively expensive 13c labelled compounds and often complex interpretation, 1h-nmr benefits from simple sample preparation and non-selective data acquisition. the result is that a single scan can reveal the concentration of many small molecules in an unbiased manner, with concentration levels reaching as low as the micromolar range. despite the maturity of 1h-nmr technology, the context of cell culture metabolomics offers opportunities for further developments in both acquisition and post-processing of metabolomic time-course data.

quantitative nmr relies on the principle that the integrals of resonance peaks are proportional to the number of nuclei that make up the resonances  <cit> . the absolute area of the integrals is also dependent on spectrometer and sample properties that include the relaxation time of various metabolites, pulse excitation, and broad-band decoupling. while the effect of relaxation time can be ignored with a sufficiently long acquisition time , the effect of other factors is accounted by comparison to a calibration standard. typical calibration standards can be broadly categorized as internal , external , or electronic  . regardless of how the reference signal is generated, metabolite quantification relies on the ratio of target resonance and reference peak integrals. unlike typical measurement variability, error in the generation or measurement of the reference signal will have the same relative impact on all the quantified metabolites and represents one example of a systematic bias.

error related to the reference standard can stem from sample preparation  as well as spectra processing and analysis. although external and electronic standards do not rely on the addition of a chemical standard, the lack of internal standard introduces extra variability from the amount of sample analyzed. proper technique can ensure good reproducibility, but occasional mistakes are nonetheless possible. more importantly, the reference peak is subject to the same variability as any other resonance. phase and baseline correction, which are typically performed on all nmr spectra, are known to have a considerable impact on the accuracy of peak area integration  <cit> . malz and jancke  <cit>  have observed that while routine standard deviation can be reduced to  <dig>  % of mean concentration, the relative uncertainty can be as high as  <dig> % with just “slightly” wrong phase and baseline corrections. other factors may also come into play depending on the quantification method. some commercial packages such as chenomx nmr suite , which has been used in recent cell culture applications , require the user to match the observed internal standard peak to an idealized representation. apart from introducing user uncertainty, this method may be particularly sensitive to line shape variability. discrepancies between the ideal and observed shapes of the internal resonance peak due to imperfect shimming are a likely source of quantification error.

while errors from standard quantification impact practically all nmr samples to some extent, biofluid and cell culture samples are also subject to dilution effects. urine samples vary in their water content, which is corrected by normalization to either total spectrum area or a reference metabolite such as creatinine . the metabolomic analysis of cell lysates, common to many cell culture applications such as drug discovery  <cit> , suffers from similar problems due to the variability of extraction efficiency. the effect of variable solvent concentration results in the same systematic error as from reference quantification – a global underestimation or overestimation in the relative concentrations of all observed metabolites in a given sample.

the application of nmr spectroscopy and other metabolomic approaches to time-course samples presents both a unique challenge and opportunity in dealing with systematic errors. on the one hand, a single biased sample can skew the trends of multiple compounds and suggest false metabolic relationships. on the other hand, the time-course trends of metabolite concentrations have a significant degree of implicit replication that can be exploited through mathematical means. recent work with cell culture  <cit>  and biofluid  <cit>  data has used nonparametric curve fitting techniques to model metabolite concentration trends by leveraging the inherent smoothness of biological trends. this work extends the concept by identifying systematic deviations across a number of metabolites. in the same way that a dramatic deviation from an overall trend of a metabolite’s concentration is identified as measurement error via smoothing spline regression, the deviations of many metabolites in one sample can be identified as the result of reference error or a dilution effect.

in the context of cell culture process monitoring, a subset of compound concentration trends from a batch culture shown in fig.  <dig> illustrates the confusion that can arise from possible systematic errors . the jumps in concentrations of glycine and lysine on days  <dig> and  <dig> correspond with the exhaustion of choline and the peak of o-phosphocholine concentration. the question is whether these deviations from the general trend of the compounds can be interpreted as a physiological shift in cellular metabolism or if they are more likely to be the result of systematic error that is associated with internal standard addition. this work presents a simple iterative smoothing algorithm as a means to address this issue. the method is tested by the stochastic generation of cell culture trends subject to simulated observation error to ensure that identified systematic errors are independent of measurement uncertainty.
fig.  <dig> an example of  <dig> metabolite trends from a metabolic study. jumps in glycine and lysine concentration trends  were hypothesized to be the result of choline exhaustion . time-course data was collected as described in the cell culture subsection of the methods section



methods
cell culture
metabolic data presented in this work originated from an insect cell media supplementation experiment. spodoptera frugiperda  cells were grown in shake flasks at  <dig> °c and  <dig> rpm using in-house supplemented ipl- <dig> media  <cit> . the cells were routinely split to  <dig>  · <dig> cells/ml upon reaching a concentration of  <dig> · <dig> cells/ml, with experiments carried out on cells that have undergone less than  <dig> passages. a  <dig> l mother flask was seeded at  <dig>  · <dig> cells/ml with a working volume of  <dig> ml and grown up to  <dig> · <dig> cells/ml. this flask was used to seed  <dig> ml flasks at  <dig>  · <dig> cells/ml with a working volume of  <dig> ml. cells were counted and sampled for nmr every  <dig> h until reaching their maximum concentration .  <dig> ml samples of cell culture media were collected and centrifuged for  <dig> min at  <dig> g, with the supernatant collected and stored at -  <dig> °c until nmr analysis.

the experimental data used as a template for stochastic trend generation, hereafter referred to as reference data, consisted of  <dig> different carbohydrate supplemented flasks cultured over a period of  <dig> days. the cultures were identical and seeded from the same stock, but with varying concentrations of glucose and maltose.  <dig> compounds were profiled for a total of  <dig> model trends across the  <dig> flasks. although many of the compound concentration trends were similar across the flasks, the use of different conditions resulted in more general trends than would be available from replicates.

nmr
the collected supernatant samples were thawed at room temperature and nmr samples prepared by the addition of  <dig> ul internal standard to  <dig> ul supernatant. the standard consisted of  <dig> mm  <dig> -dimethyl-4-silapentane-1-sulfonic acid  and  <dig>  % w/v sodium azide preservative dissolved in  <dig>  % d2o . the nmr sample solutions were vortexed and pipetted into  <dig> mm nmr tubes . samples were randomized and scanned over a two day period on a bruker avance  <dig> mhz spectrometer with a triple resonance probe . scans were performed using the first increment of a 1d-noesy pulse sequence with a  <dig> s presaturation pulse,  <dig> ms mixing time, and a  <dig> s acquisition. the acquired spectra were re-randomized  <cit>  and analyzed using chenomx nmr suite  <dig>  . phasing and baseline correction were done automatically by the software and adjusted by a human profiler. compound concentrations were calculated using the “targeted profiling” method . briefly, the observed spectra were fit by the overlay of idealized nmr resonance peaks from the software library, with compound concentration quantified by comparison to an idealized fit of the dss resonance peak.

systematic error correction
starting with all compound concentration time-courses from a single cell culture, a nonparametric  model was fit to each time-course. percent deviations from the fits were calculated at each timepoint and for each compound, εtime=i,compound=j=/yi,j,smoothed. a median percent deviation was taken at each timepoint, corresponding to sorting all the deviations at a given timepoint from lowest  to highest , and focusing on the middle  value . if the largest median percent deviation exceeded a specified threshold, it was subtracted from the observed concentrations of all compounds at the corresponding timepoint. the process was repeated until the largest deviation failed to exceed the specified threshold. an overview of the algorithm is presented as a flowchart in fig.  <dig>  an r function implementation of the internal standard error correction algorithm is available in additional file  <dig> 
fig.  <dig> algorithm flowchart. step by step description of the internal standard error correction algorithm. corrected values can be kept or flagged for further investigation/removal



in principle, the algorithm takes advantage of the fact that an error in internal standard addition or quantification will result in a deviation for all quantified compounds relative to their concentration. as the percentage error from measurement uncertainty can be quite high for some media components  <cit> , the median of relative deviations was chosen as a conservative statistic that could still be capable of identifying systematic error. mean values were also tested but found to be more susceptible to random noise. an iterative process was used to account for the effect an erroneous measurement can have on a smoothing trend. once a systematic deviation is identified, the deviating timepoint is corrected and the trend re-smoothed to calculate new deviations. although the elimination of a deviating timepoint would also be suitable, correction has been chosen in this work as it conserves more of the observed data in the form of a consensus between all compound trends.

the choice of smoothing model and median deviation threshold are two important parameters for error detection. a smoothing model should be chosen according to the expected smoothness of compound concentration trends i.e. how likely they are to exhibit rapid fluctuations. a high-density cell culture or one subject to perturbation may require less smoothing to ensure that rapid physiological changes are not mistaken for internal standard error. on the other hand, a slow-growing or continuous culture could use a much greater degree of smoothing. the median deviation threshold represents the minimum amount of deviation that can be attributed to come from systematic error rather than random measurement uncertainty. high measurement uncertainty is reflected in the variability of median deviation, requiring higher thresholds to prevent false bias detection. however, a number of other factors can also have an impact, including the number of observed compounds and the number of timepoints included in the trend. the effect of these factors on the threshold is explored in this work using stochastic trend generation.

stochastic trend generation
the development of a framework for stochastic generation of extracellular compound concentration trends was based on the need to estimate the variability of median relative deviations from a smoothing fit. trend simulation was reduced to four general parameters – overall trend shape, maximum compound concentration, percent change in compound concentration, and measurement variability. the framework was developed around a reference of collected data and consisted of four steps. first, the reference trends were classified as either increasing, decreasing, concave, or approximately constant. a parametric model was chosen for each classification, and representative curves generated with a domain and range of  <dig> to  <dig>  the combination of simulated maximum compound concentrations and percent changes were used to generate maximum and minimum concentration values to scale the trend. finally, measurement variability was simulated and applied to the data. the combination of multiple trends with varying parameters was taken to be a representative of the data one would collect from the time-course of a single culture and is termed “an experiment” throughout the text. r functions used to implement this process are available in additional file  <dig> .

trend classification
initial classification of the reference data identified trends with a net change in concentration greater than  <dig> %. concentrations with changes of less than  <dig> % were taken as having approximately constant concentrations, or “unclassified”. simple linear regression was used to classify trends as either increasing or decreasing if the slope was found to be statistically different from  <dig> at a  <dig> % confidence level using a t-test. compound concentrations that had a statistically significant increase followed by a statistically significant decrease were classified as concave . trends were left unclassified if the classification of a compound differed across the different experimental conditions. this was done to ensure that classification was restricted to general patterns rather than singular observations. in this way,  <dig> compounds were classified as decreasing,  <dig> increasing,  <dig> concave, and  <dig> were left unclassified. to allow changes in the number of simulated compounds, these numbers were reformulated and rounded to  <dig> %,  <dig> %,  <dig> % and  <dig> % of the total compounds respectively.

trend shape
classified reference data trends were smoothed using cubic regression splines with an upper limit of  <dig> degrees of freedom . when normalized to the same domain and range, most of the concentration trends appeared to take very similar shapes. sigmoidal equations  were used to model the increasing/decreasing trends while the concave curves were approximated by a truncated beta distribution density function:
  sigmoidal decrease:y=11+ex−abx∈ <cit>  fig.  <dig> trend shape simulation. a comparison of a observed and b simulated compound concentration trends mapped to a domain and range of  <dig> to  <dig>  observed data has been smoothed using cubic regression splines with a maxium of  <dig> degrees of freedom. the line widths represent estimated standard error ranges from the regression spline model. simulated trend generation is described in the text and the line widths have been set to a constant  <dig> % of maximum value. lines were plotted using a high degree of transparency, with darker areas indicating a higher density of curves passing through them. in b,  <dig> trends were generated for each of the classifications using eqs. 1– <dig> and parameter values listed in table 1




  sigmoidal increase:y=1−11+ex−abx∈ <cit>  

  concave:y=xa−1·b−1x∈ 

the sigmoid functions were defined over a domain of  <dig> to  <dig>  while the beta function’s domain was kept variable. the extra parameters offered greater flexibility in controlling the rate of concentration changes. the y values  were scaled to a range of  <dig> to  <dig> after simulation for easier comparison. unclassified compounds were assumed to follow a linear trend with equal probability of either increasing or decreasing. the linear trend was used to convey a lack of information rather than a strictly linear relationship in compound concentration i.e. the case where a true trend was dwarfed by relative measurement error.

model parameter ranges were selected by trial and error to visually match the observed trends. as the increasing/decreasing trends showed evidence of two distinct patterns each, two sets of parameters were chosen for the sigmoidal curves along with a separate parameter that related the probability of sampling from one population or the other. the parameters in table  <dig> were used to generate the trends in fig. 3b. overall, the simulated trends were highly comparable to the observed ones. although there was less agreement between the concave trends, parameter constraints were kept flexible to account for the low number of concave reference curves.
a
b
a
b
a
b
c
d


trend range
the conversion of idealized trend shapes to realistic concentration time-courses required the generation of minimum and maximum values. the distribution of maximum compound concentrations from the reference data is shown in fig. 4b. compounds increasing in concentration were observed to have lower maximum concentrations than decreasing ones, requiring the simulation to be based on trend classification . on a logarithmic scale, the spread of maximum concentrations was reasonably modelled by a mixture of two normal distributions with means of - <dig>  and  <dig>   and standard deviations of  <dig> . the probability density functions of the resulting distributions can be seen in fig. 4a with the comparison to observed values in fig. 4b. the proportions between the lower and higher concentration clusters were chosen as  <dig> ,  <dig> , and  <dig>  for the decreasing, increasing, and unclassified trends respectively. although a greater degree of fine tuning was possible to achieve better agreement between observed and simulated distributions, the marginal improvement did not warrant deviating from more general consistency.
fig.  <dig> trend scale and perturbation simulation. a probability density functions used to simulate maximum compound concentration distributions. b a comparison of observed  and simulated  maximum compound concentration distributions . c probability density functions used to simulate fraction concentration change distribution. d a comparison of observed  and simulated  fraction concentration change distributions . e probability density functions used to simulate fraction relative standard deviation distribution. f a comparison of observed  and simulated  relative standard deviation. the curves represent kernel density estimates, with the simulated data generated from  <dig> samples per classification from mixtures of two normal distributions . observed data points are shown below the curves



to avoid dealing with the correlation between maximum and minimum concentrations , minimum values were generated from the simulation of net concentration change as a fraction of maximum value. relative concentration changes were assumed to be less dependent than minimum concentrations on maximum values. as compounds with increasing concentrations were generally observed to have an initial concentration of approximately  <dig>  their percent change was taken as  <dig> % for the purpose of the simulation. the distribution of fractional changes for decreasing compound concentrations is shown in fig. 4d. one compound was practically exhausted in all  <dig> of the tested conditions, with the remainder of the compounds being consumed to various degrees but clustering around  <dig> % reduction. no change of less than  <dig> % can be observed as this value had been chosen as a cutoff for separating compounds with a significant trend. the simulation distribution was modelled by a mixture of two beta distributions – one to represent the distribution of non-exhausted compounds  and another to increase the probability of values close to  <dig> and  <dig> , with the proportion between the two set to  <dig>  . the simulated distribution was truncated to the range of  <dig> – <dig>  to reflect the reference data. figure 4d suggests that the simulation was in good agreement with the reference data.

measurement variability
a measurement variability distribution was developed from our previous work on estimating 1h-nmr measurement uncertainty for cell culture applications  <cit> . briefly, a plackett-burman design was used to generate a series of media-like formulations with an orthogonal combination of high and low compound concentrations. in this way, measurement standard deviations for each compound could be estimated independently of other compound concentrations. the result was a collection of relative standard deviations  for all compounds in the media. relative standard deviations for compounds with a statistically significant change in concentration during cell growth were estimated at both high and low concentrations; a single estimate was used for compounds without a significant change.

as the differences in relative standard deviation between compound concentrations were not typically large, all of the relative standard deviations were pooled together into a single distribution of measurement uncertainty . three of the compounds that were particularly challenging to quantify in  <cit>   were excluded as they were not representative of typical quantification – compounds identified to have low concentrations and considerable resonance overlap were not quantified in this work. the resulting distribution took the shape of a bimodal normal distribution  with means of  <dig> % and  <dig> % and a common standard deviation of  <dig> % .

algorithm validation
the simulation framework was applied to answer two fundamental questions. what is the minimum level of bias that can be identified given normal measurement variability? how is bias identification impacted by the choice of smoothing model and experimental parameters? two smoothing models were considered – local linear least squares regression and a cubic regression spline. the former was implemented by the loess function in base r and the latter as a general additive model  provided by the mgcv package  <cit> . both models made use of a smoothing parameter. the loess approach required a span that dictated what fraction of data points to use in local regression. this parameter was varied from  <dig>   to  <dig>  . the gam approach required the choice of basis dimension number, which was varied from  <dig>  to  <dig> . in the text, models are referred to by their smoothing parameter i.e. loess- <dig>  or gam- <dig>  combined with model type and smoothing parameter, the number of quantified compounds  and the number of observed data points  were also seen as important factors that could influence bias detection.

 <dig> experiments were simulated for each factor combination . half of the experiments were subject to normal measurement variability, while half were further perturbed with a systematic bias of  <dig> % at a single randomly selected timepoint. algorithm performance was assessed by smoothing the simulated data using a given model and calculating the median relative deviation of observations from the fit for each timepoint in each experiment. the result was a pool of median values for each timepoint corresponding to a certain factor combination. full r code is available in additional files  <dig> and  <dig> for loess and gam simulations respectively.

implementation
the algorithms and all analysis has been implemented in the r programming language  <cit> . figures were generated using the ggplot <dig> package  <cit> .

RESULTS
application
the correction algorithm was applied to the example data from fig.  <dig> and the results can be seen in fig.  <dig>  although only glycine and lysine results are shown, all  <dig> observed compounds were used in the calculation . the algorithm provided strong evidence that the jumps in glycine and lysine concentration were not due to metabolic shifts but were the result of a systematic error. figure 5a also demonstrates that random measurement error such as the pronounced deviation in glycine concentration on day  <dig> was not impacted by the correction as it was not general to all metabolites. the influence of the correction was most pronounced in the rates of concentration changes calculated as the derivatives of the smoothing curves . as a result of the changes in concentration, both compounds went from being produced then consumed to a steady pattern of increasing consumption. more importantly, the correction of only two points resulted in considerable changes to derivative estimates across all time-points. this can have an important impact on the use of spline smoothing for flux estimation in metabolic flux analysis .
fig.  <dig> correction applied to example data. a white points represent initially observed concentrations that were marked for correction by the algorithm, while black points represent final compound concentrations . smoothing lines were generated using the gam- <dig> model using uncorrected  and corrected  data. time-course data was collected as described in the cell culture subsection of the methods section. b derivatives calculated from the uncorrected  and corrected  smoothed fits



validation
smoothing bias
the smoothing model used in the correction algorithm must strike a balance in having enough flexibility to follow metabolism related changes in compound concentrations while avoiding undue influence from deviating observations. a lack of flexibility can result in systematic deviations from a smoothing fit where no errors are present, while too much flexibility can underestimate deviations due to error. the simulated trends described in the algorithm validation section were smoothed using loess and gam models  and the median deviations from each experiment were averaged to identify overall trends . unsurprisingly, a greater degree of smoothing resulted in less biased deviations i.e. loess- <dig>  and gam-6/gam- <dig> models had practically constant deviations across all timepoints. on the other hand, using an inadequate amount of smoothing generally resulted in an underestimated fit early in the culture  and an overestimated fit later. between the two smoothing functions, gam was found to have a better discrimination of artificially biased timepoints than loess at comparable smoothing levels  – the deviations were more consistent across different timepoints and were not as sensitive to the number of observations. although the jump from loess- <dig>  to loess- <dig>  in fig.  <dig> is quite considerable, further analysis using other span parameters reinforced the observation that gam smoothing is superior for bias discrimination. as gam- <dig> requires less information than gam- <dig>  it can be seen as a good compromise between an unbiased fit and deviation identification. for best results, the smoothing model should be tailored to the data under study.
fig.  <dig> average bias as a function of smoothing model. lines represent averages of simulated median relative deviations from smoothing fits. dashed lines are used to distinguish timepoints simulated with a  <dig> % bias. the gam-6/loess- <dig>  models correspond to a greater degree of smoothing in comparison to gam-3/loess- <dig> . the number of observations corresponds to the number of timepoints in the generated metabolic trends. although the number of compounds per time-course set was varied in the simulation, these were found to have no impact on average trends



apart from smoothing model, the number of observations over the course of a culture was also found to have an influence on deviation estimation . increased sample frequency yielded a more accurate deviation estimate for biased timepoints. however, the net impact of having a greater number of observations remained quite small. for gam- <dig>  for example, a true bias of  <dig> % was estimated as approximately  <dig> % with  <dig> or  <dig> observations and closer to  <dig>  % with only  <dig> observations. further simulations on lower observation numbers suggested that comparable performance could be attained down to  <dig> observations before degrading to a significant degree . as batch processes may be operational for as few as  <dig> days, this translates to a required sampling frequency of two samples a day. since  <dig> h sampling may not always be practical, the effect of a staggered sampling on the correction algorithm was also investigated. with gam- <dig> smoothing, little to no difference was observed between even  <dig> h sampling and a routine where  <dig> samples are taken  <dig> h apart, followed by a break of  <dig> h .

confidence intervals
the variability of median deviations is particularly important for the selection of a correction threshold. the threshold must be high enough to avoid correcting deviations due to random measurement noise while remaining sensitive to systematic sources of error. empirical  <dig> % confidence intervals were constructed from the simulated data by excluding the  <dig> % highest and  <dig> % lowest median deviations at each timepoint . between the number of compounds and the number of observations, only the number of compounds was found to have an effect on confidence interval width. naturally, the observation of more compounds reduced the impact of measurement noise and allowed for a more robust median estimate. however, the simulation of more compounds assumed equal quantification quality. if the number of observed compounds is increased by profiling highly convoluted or otherwise poorly quantifiable compound resonances, the beneficial impact is likely to be limited.
fig.  <dig>  <dig> % confidence interval around median deviations. empirical  <dig> % confidence intervals were constructed from the simulated data by excluding the  <dig> % highest and  <dig> % lowest median deviations at each timepoint. lighter grey colour is used to distinguish timepoints simulated with a  <dig> % bias



based on the results, the observation of  <dig> compounds at  <dig> timepoints  will exhibit a natural variation in median deviation of approximately 2– <dig>  %. thus, deviations beyond this threshold have a high probability of occurring due to a source of bias such as internal standard addition or quantification. the results also show that a  <dig> % bias is more likely to be identified as anywhere between  <dig> – <dig> % , meaning that a subtraction of the estimated median deviation is more likely to dampen the bias, rather than remove it. reduced performance at the end-points reflects the relative lack of trend data and can be ameliorated by adding replicates or extending the observation time beyond the span of direct interest.

simulation extension
to determine how robust the correction method is to changes in the underlying data, four modifications to the simulated data were considered. the ratio of decreasing to increasing trends  was set to  <dig> %: <dig> % as well as  <dig> %: <dig> %. despite these dramatic shifts, both average bias and confidence interval trends remained very similar to those presented in figs.  <dig> and  <dig>  the only exception was at the end points, where lower concentration magnitudes resulted in much more variable relative deviations. since increasing and decreasing trends reach their minimum concentrations at different endpoints, the overall effect on median relative deviations is not pronounced when the two trends are balanced in number. however, the extreme case of a 12: <dig> imbalance between increasing and decreasing trends resulted in larger variability ranges at time-course edges. with  <dig> % of the trends increasing, the bias threshold at early timepoints increased from  <dig>  % to  <dig> %. with  <dig> % of the trends decreasing, the bias threshold increased at late timepoints but did not go beyond the overall average of  <dig>  % . the difference between the two conditions can be explained by the fact that all increasing trends start at or very close to  <dig>  while only some of the decreasing trends reach such low concentrations. since a more balanced proportion of increasing and decreasing trends is expected in real data, the overall effect would be minimal. two other conditions – increasing the net concentration changes of decreasing trends  and increasing the variability of observations  did not appear to have any impact on the threshold calculation. for all conditions, gam- <dig> smoothing remained the best choice.

taken together, these results suggest that a bias threshold of approximately  <dig>  % using gam- <dig> smoothing would be an adequate default choice for diverse data sets. beyond cell culture applications, we predict the bias correction algorithm to be just as useful for other time-course metabolomic data. one such example is biofluid analysis in toxicology. the consortium for metabonomic toxicology  has already established a large collection of time-course urine samples that meet the requirements for systematic error correction  <cit> . while the proposed correction is not designed to replace standard normalization techniques, it can build on the development of recent smoothing spline techniques  <cit>  and serve a complementary role in the identification of spurious results. further extension to mass spectrometry  methods is also likely to be fruitful. techniques such as multiple reaction monitoring  are commonly used for pharmaceutical and toxicological metabolomics  <cit>  and suffer from similar dilution effects as nmr . the correction of systematic biases may serve to reduce the relative standard deviations of quantified compound concentrations.

CONCLUSIONS
the growing popularity of quantitative metabolomics for time-course applications presents a new context for data processing and acquisition. while this work deals primarily with the correction of internal standard quantification in cell culture data, it’s not difficult to imagine similar approaches applied to other analytical methods. improvements in accuracy, precision, and analysis speed can be best achieved by leveraging the replication inherent to the parallel observation of multiple metabolite trends. the algorithm presented in this work took advantage of inherent autocorrelation to identify and correct systematic bias originating from internal standard addition and quantification. the gam- <dig> model was identified as the best smoothing function for the task, with the ability to detect a bias greater than  <dig>  % across most of a culture’s time-course. the simulation framework followed the context-driven approach by capturing the key elements of a cell culture time-course. although the presented validation has focused on trends typically observed in our lab, full code has been provided to allow rapid adaptation to user needs.

availability and requirements
project name: metcourse

project home page:https://github.com/ssokolen/metcourse

operating system: platform independent 

programming language: r 

other requirements: r packages – dplyr , mgcv 

license: apache 

any restrictions to use by non-academics: no



additional files
additional file  <dig> 
internal standard error correction algorithm. r script containing two function definitions – f_smooth() used to generate the smoothing fit and correct_relative_deviation() to perform the corrections. 



additional file  <dig> 
stochastic trend generation functions. r script containing all the functions used to simulate realistic timecourse trends. there is an example at the end of the file that shows how the functions are combined to generate a trend. 



additional file  <dig> 
code used to generate data for loess analysis. r script used in this work for generating loess model data. 



additional file  <dig> 
code used to generate data for gam analysis. r script used in this work for generating gam model data. 



competing interests

the authors declare that they have no competing interests.

authors’ contributions

ss designed and implemented the algorithms and carried out the data collection. ma assisted in data interpretation and manuscript preparation. both authors read and approved the final manuscript.

