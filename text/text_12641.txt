BACKGROUND
accurate measurement of dna using quantitative real-time pcr  techniques is fundamental to many molecular tests with clinical  <cit> , environmental  <cit> , and forensic applications  <cit> . quantitative dna analysis at high target concentration is reproducible  <cit> , but demand for increasing sensitivity requires low levels of dna to be measured with equal reliability. this presents a challenge to analysts as there are numerous problems associated with trace detection and quantification, yet a range of sectors are increasingly reliant on low level analyses. clinically there is growing interest in quantifying levels of circulating nucleic acids for a range of applications including diagnosis and monitoring of cancer patients  <cit> , prognosis for victims of trauma  <cit>  and non-invasive prenatal diagnosis  <cit> . similarly, forensic laboratories are often presented with extremely small amounts of material crucial to a legal case; accurate analysis must then be carried out at the first attempt as often there is insufficient sample for repeated investigations  <cit> . additionally at an international level, legislation governing the limits for genetically modified organisms  in foods necessitates the use of sensitive and accurate methods to detect and quantify trace levels of gmo ingredients  <cit> .

quantification is initially reliant upon having suitable standards in the appropriate concentration range and of the same quality as the samples being analysed. the chosen dna standard must itself be quantified either by spectrophotometer, which has limitations  <cit>  or with an intercalating dye such as picogreen®  <cit>  before being used to prepare standard curves. there are currently no certified reference materials available for dna quantification, so accuracy depends on the way in which individual laboratories measure and prepare the dna standard. dilution protocols involved in constructing standard curves are recognised as contributing to reduced reliability at low template concentrations  <cit>  and there are several causes for this. firstly the stochastic distribution of molecules means that at very low copy number a sampling error can be introduced when pipetting aliquots of dna. measurement variability at low dna concentration has been demonstrated by the observation that the confidence intervals  associated with amplification from small initial copy numbers of template are much greater than those with large initial copy numbers  <cit> . increasing the number of replicate analyses performed on low concentration standards and samples can mitigate inaccuracy caused by this inherent variability, and improve analytical sensitivity  <cit> . a second issue is the apparent lability of dna when stored in solution at low concentrations, which can significantly affect the accuracy of quantification if dilution series are stored for prolonged periods prior to use. a report by teo et al.  <cit>  highlighted the short time scale over which fluctuations in the concentration of standard solutions can occur, due to dna binding to untreated microcentrifuge tube walls. this is of particular significance at low concentrations when there are only a small number of molecules present and has ramifications for the quantification of unknown samples. the result of all these factors is that large measurement uncertainty is associated with trace-level dna quantification  <cit>  and the performance characteristics of pcr-based assays at low target concentrations are ill defined.

the measured instrument response for q- pcr is the ct value, which is the amplification cycle at which the fluorescent signal from each reaction increases above a nominal threshold value. an additional problem when working with q-pcr at trace level is the occurrence of valid negative reactions , which do not have a quantitatively meaningful ct value, as the fluorescent signal does not reach the threshold level during the course of the reaction. these are valid negative reactions, reflecting true absence of analyte because of sampling variability at low target concentration. such negative results are either arbitrarily awarded a value equal to the number of amplification cycles run , or are not allocated a measurable value. assigning a value equal to the total number of cycles run causes difficulties in subsequent data analysis as the value will vary dependent on cycling parameters, and is often identified as an outlier value by normal statistical analysis. the non-normal distribution of results around zero generated by real-time data has also been noted by other researchers  <cit> . a common practice is to exclude all such negative reactions entirely from the analysis, whether obtained from unknown samples or standards. however at trace levels the non-amplification provides accurate information about the low concentration analytes, reflecting both the distribution of molecules and sampling effects. selective omission of negative observations in these circumstances can generally be expected to lead to biased quantification.

the purpose of this investigation was to identify measures to increase the reliability of low level dna quantification and detection by q-pcr. firstly, we performed a series of experiments to assess practical interventions for dna standard preparation, which could potentially reduce the variability of low concentration dna solutions and make construction of calibration curves more accurate. this involved preparing dna dilutions in different types of plastic ware and with a range of diluents. using the results from these experiments, we explored data handling approaches for standard curve construction and sample quantification, with particular attention to the treatment of valid negative observations. finally, routes to assessing detection probability for characterising analytical sensitivity and performance in qualitative applications were examined. logistic regression was applied to determine the detection probability of analyte at a particular concentration, and calculations based on binomial statistics developed to aid prediction of the degree of replication required to achieve a particular level of sensitivity.

RESULTS
effect of standard dna preparation on quantification
the effect of different standard preparation conditions was assessed by analysis of z-scores for estimated concentration, calculated as described below . the estimated concentrations were calculated using calibration curves calculated using the exclusion-by-sample method described below . note that z-scores are expected to have mean  <dig> and variance  <dig>  in the scheme used here, negative z-scores correspond to lower than average concentration compared to the average for the group. when normally distributed, 95% of z-scores are expected to be within approximately ±  <dig> 

one-way anova of the z-scores for between-plate differences showed a strongly significant effect   p-value of  <dig>  × 10-6). this presented a choice; re-calculate concentrations by fitting plate by plate, or, equivalently with no condition/plate interaction term, take advantage of the balanced design and treat plate as a blocking factor. blocking by plate is the preferred option, as it naturally adjusts for the plate effect on the residual degrees of freedom, which would otherwise be slightly overestimated. graphical inspection indicated that the plate effect was not large enough to raise concerns about level-dependent differences in variance, and two-way anova confirmed that there was no significant interaction  between plate and condition, so the analysis reported here used blocking by plate. the relatively small plate effect also permitted continued application of the k-w test as a follow-up test, as the plate effect has the practical effect of increasing within-group variance and consequently will tend to produce more conservative p-values for the k-w test. to check the effect of separate construction of calibration curves for each plate, the analysis was repeated after separate calibration by plate, with no effect on the conclusions and only marginal effects on p-values. we also examined the effect of simply scaling by robust standard deviation at each concentration without z-scoring ; again, this had no effect on the conclusions.

"condition"  was a combination of tube plastic  and solution . initial analysis sought a general condition effect. anova for condition effects, blocked by plate, gave a significant between-condition effect , supported by the k-w test p-value of  <dig>  . homogeneity of z-score variance across conditions was poor, but not grossly so; levene's test gave a p-value of  <dig> , warranting caution, but unlikely to prejudice the anova results greatly.

inspection of the z-scores by condition  showed that conditions 1– <dig> appeared low compared to conditions 4– <dig>  this was potentially important since conditions 1– <dig> used standard plastic tubes, and 4– <dig> used low-retention tubes. otherwise, no specific group stands out. however, given a significant condition effect, plastic and solution effects were investigated further.

tube plastic proved to be strongly significant  . inspection showed, as might be expected, that low-retention plastic ware gave higher observed concentrations than standard plastic . the effect was particularly evident at low concentrations, and essentially negligible over  <dig> g.e., as shown in figure  <dig>  despite some possible differences on visual inspection , , the solution effect proved insignificant at the 95% level of confidence . these observations were essentially unchanged on performing 2-way anova for plastic and solution effects, and there was no significant interaction between the two factors.

these results have practical relevance for performance of trace dna measurements. the significant plate effect was not unexpected, as inter-plate differences are common in q-pcr work. in general, calibration plate by plate is to be recommended; this is, however, normal practice and no additional recommendation is necessary. the strong effect of low-retention plastic is, however, likely to be important for low-level quantification. based on figure  <dig>  standard plastic tubes may give a two-fold reduction in apparent concentration at low levels, compared to low-retention plastic. this result is in agreement with a study by teo et al  <cit> , which concluded that siliconised tubes reduced inconsistencies caused by abstraction of dna from solution to the walls of untreated microcentrifuge tubes. teo et al also found no significant effect on using carrier dna for standard solutions using the lightcycler. given the likelihood that the significant effect of low-retention plastic is related to loss of dna in standard tubes, and the improved accuracy at low concentrations, it seems safe to recommend the use of low-retention plastic when working with solutions at low dna concentrations, here below  <dig> g.e.

construction of calibration curves at low dna concentration
at higher dna concentrations , negative results almost invariably indicate amplification failure and can justifiably be removed as invalid observations. at low dna concentrations, however, particularly below about  <dig> target copies per reaction, sampling issues mean that a significant proportion of the replicate reactions genuinely do not contain any target. the question then arises of how best to calibrate, when data contain a significant fraction of valid negative responses.

exclusion of the negative observations alone, as at high concentrations, introduces a marked downward bias in the average ct value derived for low-concentration standards. this is most clearly seen in the appearance of an upper limit or 'plateau' for mean ct values at low concentrations . this plateau effectively represents ct value for amplification of a single target molecule. this plateau precludes discrimination between samples of different low concentrations. further, the negative bias has adverse effects on the complete calibration curve, the more so as the biased points are at one extreme of the line and consequently introduce a rotational bias; compare the solid and dashed lines in figure  <dig>  thus at very low concentrations, omission of individual reactions with no measurable signal prevents discrimination at low levels and causes inaccuracy at higher concentrations in the same assay. bias introduced by poor reproducibility of low level calibrators has also recently been reported by lai et al  <cit> .

one alternative to omitting negative observations is to include the non-amplification events occurring in low level standards in the calculation of the mean ct values used for constructing the standard curve, using the  limiting value . the change in probability of negative responses then leads to an expectation of non-linear, but monotonic, increase in mean ct with reducing concentration, in principle permitting discrimination between very low level samples. linear regression is clearly inappropriate , but interpolation using an empirical curve fit to such data has been previously reported as successful  <cit> . however, this approach has several weaknesses. it is very sensitive to the proportion of negative responses, so depends on high levels of replication to guarantee near-monotonic change in observed mean value. the interpolation is arbitrary and cannot readily be generalised to wide concentration ranges. the coefficients change with arbitrary changes in the limiting value used. finally, the mean ct value arises from a mixture of quantitative data and arbitrarily coded qualitative responses, making the error distribution complex, seriously compromising the validity of a least-squares approach to interpolation, and making prediction uncertainties very hard to obtain. these shortcomings make the approach unsuitable for routine use.

the approach taken in the present study has therefore been to remove all data for any standard compromised by valid negative responses . the rationale is simply that this eliminates selection bias in the calibration. it is also very simple to implement, so can be used by a routine laboratory with existing software. the results are shown in figure  <dig>  which shows the exclusion-by-sample line for all data  as the solid line.

the approach has the obvious disadvantages of extrapolation to obtain low-concentration values and some loss of precision. however, extrapolation is valid as long as the model is sound. the log-linear model is well supported by the underlying physical process, and effective method validation should in any case confirm linearity over the range of interest before undertaking analysis. the loss of precision is not very marked; recall that the lower concentration samples typically show high variance and should in any case be down-weighted. prediction precision can also be improved by extending the calibration data to higher concentrations as long as continued linearity is demonstrated, and also, in principle, by increasing replication towards the extremes of the calibration range . the most serious practical disadvantage is therefore that it may not be clear in advance which samples will be excluded, resulting in wasted effort and resources and, sometimes, precluding reliable calibration entirely if insufficient high-concentration samples are included. there is also some risk of excluding samples that show negative responses for other reasons.

these latter disadvantages can be ameliorated relatively easily. experience or validation studies will show where the risk of exclusion is sufficiently low to warrant acquisition of data; this is discussed further below in connection with detection probabilities. extending the concentration range upward to provide more precise extrapolation is straightforward, and involves no additional effort; the lowest concentration standards that are at high risk of exclusion can be replaced by higher-concentration standards. it is, however, important to identify and remove aberrant observations at higher concentrations . objective criteria are important when assessing suspect observations, and there is little or no standard practice corresponding to outlier detection in q-pcr work. it is accordingly premature to make firm recommendations here. it may, however, be helpful to note that where a 'true' negative has probability of occurrence of 1% or less, removal of negative observations prior to analysis would have little adverse impact on calibration.

exclusion by sample is nonetheless quite drastic and risks eliminating a proportion of valid calibrant data, as well as incurring uncertainty associated with extrapolation. in the longer term, therefore, there is a need to investigate approaches that use all the valid data and accommodate the negative observations without introducing bias. for example, maximum likelihood fitting is in principle capable of handling arbitrary error distributions and can accommodate censored data. robust regression  <cit>  or median-based methods could be employed to reduce the adverse impact of arbitrary assignment of high values, yet still allow inclusion of all the observations in construction of calibration curves. the ideal regression method would also take into account the fact that part of the error arises from variability in the actual copy number, that is, that there is significant uncertainty in the independent as well as the dependent variable. in the mean time, in the absence of such methods we recommend inclusion of only those standards in which all replicates yield measurable values to ensure that bias is avoided in construction of calibration curves. if higher concentration standards have negative reactions that are identified as outliers, the outliers can still justifiably be excluded.

quantification of unknown samples
in contrast to the situation in constructing calibration curves, exclusion of ct values by sample is not a viable option when an unknown sample is analysed. fortunately, calculations are in any case best carried out in the concentration domain, where one has the option of treating negative observations as zero concentration. two methods for handling negative observations  in the calculation of unknown sample concentration were accordingly compared to determine the most accurate approach. the comparison used the replicate data from the lower concentration standards prepared in low-retention plastic only, with a calibration model based only on standards from low-retention plastic. in one approach, negative reactions were excluded from the calculation entirely, the remaining ct values converted to concentration and the average concentration taken. in the other, negative amplifications were included in the quantification by assigning a concentration value of zero for the observation, and again the resulting individual concentration estimates were averaged. the first of these approaches – excluding negative reactions – is expected to introduce positive bias due to selection for positive reactions only; increasingly so as the incidence of true negatives increases. by contrast, although there may be other sources of bias, assigning a concentration value of zero to non-detects is expected to introduce negligible additional bias as long as the distribution of ct for true positive runs has negligible density above the ct threshold.

the results are shown in table  <dig>  there is a slight positive bias throughout, perhaps because of slight departures from linearity, but the results obtained on excluding reactions with ct =  <dig> clearly gave consistently higher mean sample concentrations than assigning zero concentration to all such values. at  <dig> target copies and above both methods gave the same result as all reactions yielded measurable values. the results obtained on setting negative responses to zero concentration were all closer to the nominal concentration for the standards. this showed that, as expected, excluding negative results leads to undesirable positive bias; more importantly, the bias so induced can be considerable for high proportions of failure.

the practical implication is that in quantification of unknown samples at low copy number, valid negative observations  should be treated as samples with zero observed concentration. thus, to calculate the concentration of a particular sample, the mean of the concentration estimates obtained from ct values via the calibration curve should be calculated, including the negative observations as zero concentration. this allows all available information on the sample to be included in the final result, as also detailed recently by zimmermann et al  <cit> . as table  <dig> demonstrates, this provides better estimates of the unknown sample concentration. the caveat is that negative observations arising through amplification failure rather than genuine absence of target should be excluded or otherwise accommodated. as in the case of calibration curve construction, appropriate criteria should be applied; again, we note that exclusion of observations on the basis of low probability of occurrence  is unlikely to lead to appreciable bias but will avoid the adverse effects of amplification failure on higher-concentration samples.

detection probability and detection limits
estimates of detection probability at low levels are important for several reasons. an understanding of detection probability is useful in determining the expected frequency of detection for a particular concentration analyte, when performing a defined number of replicate analyses. it can assist in deciding whether or not to reject amplification failures at low levels, and aid realistic interpretation of assay results. performance characteristics such as limit of detection  for qualitative pcr-based methods also rely on probability of detection. finally, understanding the detection probability for a single amplification can assist in deciding optimal replicate numbers and in choosing minimum concentrations for reasonable confidence of detection and/or avoidance of detection failures.

to estimate the number of replicate reactions required to achieve a particular number of successful amplifications , the binomial distribution is appropriate. the binomial distribution describes the probability of ns successes in nt trials  with a probability of success  of ps. fortunately, recourse to the distribution is unnecessary for the special cases of ns = nt and ns =  <dig>  which are the only cases required for our purposes. the probability of nt successes in nt trials  is psnt
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgwbacdaqhaawcbagaee4camhabagaemoba42aasbaawqaaiabbsha0bqabaaaaaaa@32b0@; that of no successes is nt
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqadiqaaiabigdaxiabgkhitiabdchawnaabaaaleaacqqgzbwcaeqaaagccagloagaayzkaawaawbaasqabeaacqwgubgbdawgaaadbagaeeidaqhabeaaaaaaaa@364e@. the probability of one or more successes  is then 1-nt
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqadiqaaiabigdaxiabgkhitiabdchawnaabaaaleaacqqgzbwcaeqaaagccagloagaayzkaawaawbaasqabeaacqwgubgbdawgaaadbagaeeidaqhabeaaaaaaaa@364e@. the number nmin of replicates required to ensure a minimum probability pmin of at least one positive is then given by

nmin = ceiling/log)     

where ceiling is the nearest integer greater than or equal to x.

the calculation requires an estimate of the probability of detection at the concentration of interest, usually derived from validation or calibration standard data. given sufficient replication, the observed fraction of positive results at each concentration provides a direct estimate of probability for the particular concentration . use of experimental counts is straightforward. it is, however, somewhat sensitive to random failures, leaves little possibility of interpolation to find probabilities at intermediate concentrations, and typically requires a relatively high number of replicates at each concentration to provide reliable probability estimates.

modelling the probability offers a useful alternative. the detection data is binomial , and logistic regression a widely accepted method of modelling binomial responses. we therefore applied logistic regression to the data set above to determine the probability of target detection across the concentration range.

logistic regression can be biased by errors at extreme concentrations, and pcr data often shows concentrations spanning several orders of magnitude. here, the initial fit against concentration c  was relatively poor due to the amplification failure at  <dig> g.e. . with the amplification failure at  <dig> g.e. omitted, the fit in the concentration domain improved dramatically ; this curve was p = 1/)). the fitted curve is shown in figure  <dig> . using the fitted probabilities in the binomial calculation, replicate numbers at the observed concentrations are  <dig>   <dig>  and  <dig> for  <dig> ,  <dig> and  <dig> copy numbers, and  <dig> thereafter, for 95% probability of at least one positive. this compares well with calculations based on observed fractions.

ideally, aberrant values should be excluded prior to applying logistic regression, as here. if that cannot be done with confidence  we suggest fitting in the log domain for pcr data to reduce the weight associated with failures at high concentration. fitting against log <dig> provided a good fit to the low-concentration data even when the amplification failure at  <dig> g.e. was included . this line was p = 1/))); the curve is shown as a dashed line in figure  <dig>  if anything, this curve appears more realistic on visual inspection, in that it correctly predicts very low probability at very low concentrations, where the concentration domain model apparently predict significant probability of positives. predicted replication numbers using this curve were, however, identical to those previously found.

some caution is necessary in modelling probability. using a log scale for the independent variable, as here, is essentially an arbitrary choice; although ct is expected to be linear in log <dig>  there is no ab initio reason to expect p to be determined by log <dig> rather than c, and the logarithmic spacing of concentrations is driven by practical choice, not necessity. further, logistic regression itself, though a widely accepted and successful methodology for binary response modelling, the logistic model is only one possible model for binary responses  <cit> . wherever it is used, the fitted model needs to be checked for quality of fit before being applied to interpolation, and the most appropriate variant selected. even then, extrapolation to extreme probabilities is particularly dangerous unless there is very strong prior evidence that the model applies.

overall, both observed counts and logistic regression provided broadly similar conclusions in this case. both were somewhat sensitive to aberrant values; logistic regression excessively so in the concentration domain. in the log domain, logistic regression was less affected. the advantages of the logistic regression approach include the general smoothing effect of model fitting; the use of the entire data set in fitting the model, in principle decreasing the number of replicate measurements required for reliable prediction at any one concentration; the possibility of interpolation; and the possibility of estimating uncertainties in the probability estimates from the uncertainties in fitted coefficients.

logistic regression can also be applied to the estimation of analytical sensitivity. in the present study, for example, it was found that the concentration providing 50% probability of detection for a single observation  corresponded, using the log <dig> curve, to a value of log <dig> = - <dig>  and an actual concentration of  <dig>  copies, indicating an impressive likelihood of detectable amplification of a single copy. a similar application, using probit regression, was recently reported by zimmermann et al  <cit> ; in our study, we examined probit regression in the log <dig> domain for comparison, and found that it gave a closely similar result to the logistic regression, as expected. further, logistic regression can assist in estimating the lowest concentration at which amplification failure rate remains acceptable, avoiding wasted resources when employing the recommended 'exclusion by sample' method in calibration. for example, 95% probability of six positives in six replicates requires an individual run success probability of  <dig>  ; based on the log-domain logistic regression for our data, the individual success probability of  <dig>  would be expected at a concentration of about  <dig> g.e. note, however, that lower concentrations also have quite high success probabilities ; it is therefore not particularly surprising that we obtained 100% successful amplifications down to  <dig> g.e.

CONCLUSIONS
tube plastic had a strong effect on measured dna concentration at concentrations below  <dig> g.e., and it is recommended that low-retention plastics be used for dna quantification below this level. neither stabilising solution nor carrier dna had a significant effect under the conditions of this study.

for low-level calibration, it is better to eliminate all ct values in a replicate group for any standard concentration that shows non-detects reasonably attributable  to sampling effects, than to eliminate only the non-detects. there is, however, a need for further work to establish improved fitting methods in the low-concentration region

in calculating concentrations for low-level test samples that show appreciable non-detects, concentrations should be calculated for each replicate, zero concentration assigned to non-detects, and all resulting concentration values averaged. average ct values should not be used to estimate concentrations.

logistic regression was found to be a useful method of estimating detection probability at low dna concentrations, provided that measures are taken to remove, or reduce the effect of, invalid non-amplifications at high concentrations.

