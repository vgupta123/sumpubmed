BACKGROUND
the advent of high-throughput methods to life sciences has increased the need for computer-intensive applications to analyze large data sets in the laboratory. currently, the field of bioinformatics is confronted with data sets containing thousands of samples and up to millions of features, e.g. gene expression arrays and genome-wide association studies using single nucleotide polymorphism  chips. to explore these data sets that are too large for manual analysis, machine learning methods are employed  <cit> . among them, cluster algorithms partition objects into different groups that have similar characteristics. these methods have already become a valuable tool to detect associations between combinations of snp markers and diseases and for the selection of tag snps  <cit> . not only here, the size of the generated data sets has grown up to  <dig> markers per chip. the demand for performing these computer-intensive applications is likely to increase even further for two reasons: first, with the popularity of next-generation sequencing methods rising, the number of measurements per sample will soar. second, the need to assist researchers in answering questions such as "how many groups are in my data?" or "how robust is the identified clustering?" will increase. cluster number estimation techniques address these types of questions by repeated use of a cluster algorithm with slightly different initializations or data sets, ultimately performing a sensitivity analysis.

in the past, computing speeds doubled approximately every  <dig> years via increasing clock speeds, giving software a "free ride" to better performance  <cit> . this is now over, and such automatic performance improvements are no longer possible. as clock speeds are stalling, the increase in computational power is now due to the rapid increase of the number of cores per processor. this makes parallel computation a necessity for the time-consuming analyses in the laboratory. generally, two parallelization schemes are available. the first is based on a network of computers or computing nodes. the idea of such a master-slave parallelization is to parallelize independent tasks using a network of one master and several slave computers. while there is no possibility for communication between the slaves, this approach best fits scenarios where the same serial algorithm is started several times on different relatively small data sets or different analyses are calculated in parallel on the same data set. data set size matters here, as distribution of large data sets is time consuming and requires all computers to have the appropriate memory configuration. the second approach called shared memory parallelization is used to parallelize the implementation of an algorithm itself. this is an intrinsic parallelization via different interwoven sub-processes  on a single multi-core computer accessing a common memory, and requires a redesign of the original serial algorithm.

master-slave parallelization
master-slave parallelization is heavily used by computer clusters or supercomputers. the message passing interface   <cit>  protocol is the dominant model in high-performance computing. without shared memory the compute nodes are restricted to process independent tasks. as long as the load-balancing of the compute nodes is well handled, the parallelization of a complex simulation scales linearly with the number of compute nodes. in contrast to massive parallel simulation runs of complex algorithms, master-slave parallelization is also used for parallelizing algorithms. for this task, a large dataset is usually first split into smaller pieces. the subsets are then distributed through a computer network and each compute node solves a subtask for its subset. finally, all results are transferred back to the master computer, which combines them to a global result. the user interacts with the hardware cluster through the master computer or via a web-interface. however, in addition to hardware requirements, such as minimal amount of memory that are imposed on each compute node, the effort of distributing the data and communicating with nodes of the computer network restricts the speedup achievable with this method. an approach similar to mpi by kraj et al.  <cit>  uses web-services for parallel distribution of code, which can reduce the effort for administrating a computer cluster, but is platform-dependent. a very popular programming environment in the bioinformatics and biostatistics community is r  <cit> . in recent years several packages  have been developed that enable master-slave parallelized r programs to run on computer cluster platforms or multi-core computers, see hill et al.  <cit>  for an overview of packages for parallel programming in r.

shared memory parallelization
today most desktop computers and even notebooks provide at least dual-core processors. compared to master-slave parallelization, developing shared-memory software reduces the overhead of communicating through a network. despite its performance in parallelizing algorithms, shared memory parallelization is not yet regularly applied during development of scientific software. for instance, shared memory programming with r is currently rather limited to a small number of parallelized functions  <cit> .

shared-memory programming concepts like the open multi-processing   <cit>  are closely linked to thread programming. a sequential program is decomposed into several tasks, which are then processed as threads. the concept of thread programming is available in many programming languages like c , java , or fortran  and on many multi-core platforms  <cit> . threads are refinements of a process that usually share the same memory and can be separately and simultaneously processed, but can also be used to imitate master-slave parallelization by avoiding access to shared memory  <cit> . due to the mostly used shared memory concept, communication between threads is much faster than the communication of processes through sockets. in a multi-core parallelization setting there is no need for network communication, as all threads run on the same computer. on the other hand, as every thread has access to all objects on the heap there is a need for concurrency control  <cit> . concurrency control ensures that software can be parallelized without violating data integrity. the most prominent approach for managing concurrent programs is the use of locks  <cit> . locking and synchronizing ensures that changes to the states of the data are coordinated, but implementing thread-safe programs using locks can be fatally error-prone  <cit> . problems might occur when using too few locks, too many locks, wrong locks, or locks in the wrong order  <cit> . for instance an implementation may cause deadlocks, where two processes are waiting for each other to first release a resource.

in the following we describe a new multi-core parallel cluster algorithm  that runs in shared memory, and avoids locks for concurrency control. benchmark results on artificial and real microarray data are shown. the utility of our computer-intensive cluster method is further demonstrated on cluster sensitivity and cluster number estimation of high-dimensional gene expression and snp data.

implementation
multicore k-means/k-modes clustering
clustering is a classical example of unsupervised learning, i.e. learning without a teacher. the term cluster analysis summarizes a collection of methods for generating hypotheses about the structure of the data by solely exploring pairwise distances or similarities in the data space. clustering is often applied as a first step in data analysis for the creation of initial hypotheses. let x = {x <dig>  ..., xn} be a set of data points with the feature vector xi ∈ ℝd. cluster analysis is used to build a partition of a data set containing k clusters such that data points within a cluster are more similar to each other than points from different clusters. a partition p  is a set of clusters {c <dig>  c <dig>  ..., ck} with  <dig> <k <n and meets the following conditions:  

the basic clustering task can be formulated as an optimization problem:

partitional cluster analysis
for a fixed number of groups k find that partition p of a data set x out of the set of all possible partitions Φ  for which a chosen objective function f: Φ  → ℝ+ is optimized. for all possible partitions with k clusters compute the value of the objective function f. the partition with the best value is the set of clusters sought.

this brute force method is computationally infeasible as the cardinality of the set of all possible partitions is huge even for small k and n. the cardinality of Φ  can be computed by the stirling numbers of the second kind  <cit> :  

existing algorithms provide different heuristics for this search problem. k-means is probably one of the most popular of these partitional cluster algorithms  <cit> . the following listing shows the pseudocode for the k-means algorithm:

function k-means

   input: x = {x_ <dig>  ..., x_n} 

         k 

   output: c = {c_ <dig>  ..., c_k} 

         m: x->c 

   initialize c 

   while c has changed

      for each x_i in x

            m = argmin_j distance 

      end

      for each c_j in c

            c_j = centroid  = j})

      end

   end

given a number k, the k-means algorithm splits a data set x = {x <dig> ..., xn} into k disjoint clusters.

hereby, the cluster centroids μ <dig>  ..., μk are placed in the center of gravity of the clusters c <dig>  ..., ck. the algorithm minimizes the objective function:  

this amounts to minimizing the sum of squared distances of data points to their respective cluster centroids. k-means is implemented by repeating two major steps, which reassign data points to nearest cluster centroids and update centroids  for the newly assembled cluster. a centroid μj is updated by computing the mean of all points in cluster cj:  

k-modes clustering for snp data
data from snp profiles can be encoded as a vector of categorical data representing homozygous reference, heterozygous, and homozygous alternative as  <dig>   <dig>  and  <dig>  for instance, a snp s has two alleles a and t. the three possible genotypes are aa, at, and tt. a data point x is represented as a vector of snp values. for measuring similarity of two snp samples, the allele sharing distance  has been proposed  <cit> . recently, it has been shown that asd provides sufficient information for separating subpopulations using snps  <cit> . the allele sharing distance d for calculating the distance between data point x and y is defined as:  

where:  

to incorporate snp data, the centroid update step of the k-means algorithm is adapted to calculate centroids from categorical data  <cit> . cluster centers are now calculated by counting the frequency of each genotype and using the most frequent genotype  as the new value.

parallel k-means/k-modes in shared memory
the k-means/k-modes algorithm is parallelized by simultaneously calculating

 the minimum distance partition  and subsequently

 the centroid update .

that means that the complete data set is split into several subsets , and nearest centroid search is then performed in an individual thread for that subset, effectively parallelizing the minimum distance search. simultaneous write access, to the data structures  containing these data points , which is not possible in a master-slave scenario, is possible through a transactional memory system . centroid update is also parallelized by calculating the new location for every centroid from the previously found minimum distance partition .

instead of explicitly controlling thread concurrency, we here use the concept of transactional memory to indirectly guarantee thread safety . the number of threads used is influenced by two factors: for calculating the minimum distance partition, the number of data threads equals the number of available cpu cores. furthermore, each centroid is managed by its own thread. this means that during the assignment step, data is continually sent to the centroids from all data threads, and the centroid update is performed with k threads in parallel.

transactional memory
in shared memory architectures, there is a need for concurrency control. simultaneously running threads can process the same data and might also try to change the data in parallel. opposed to the low-level coding via locking and unlocking individually memory registers, transactional memory provides high-level instructions to simplify writing parallel code  <cit> . the concept of software transactional memory  that we use here is a modern alternative to the lock-based concurrency control mechanism  <cit> . it offers a simple alternative to this concurrency mechanism, as it shifts the often complicated part of explicitly guaranteeing correct synchronization to a software system  <cit> . the basic functionality of software transactional memory is analogous to controlling simultaneous access via transactions in database management systems  <cit> . transactions monitor read and write access to shared memory and check whether an action will cause data races. the stm system prevents conflicting data changes by rolling back one of the transactions. transactions ensure that all actions on the data are atomic, consistent, and isolated. the term atomic means that either all changes of a transaction to the data occur or none of them does. consistent means that the new data from the transaction is checked for consistency before it is committed. isolated means that every transaction is encapsulated and cannot see the effects of any other transaction while it is running. as a consequence, transactional references to mutable data via stm enables sharing changing state between threads in a synchronous and coordinated manner.

implementations of software transactional memory can be divided into two categories called direct-update and deferred-update stms  <cit> . in our implementation, we use a deferred-update stm . transactions in deferred-update stm systems obtain a copy of the original data and process their changes. before committing the changes to the shared memory, conflicts are checked by the stm system, and conflicting transactions are rejected. as side effects from conflicting transactions do not affect the shared memory, there is no need for restoring a consistent memory state. threads concurrently execute all of their modifications to the shared data without locking other threads. however, before committing the changes, the system checks whether other threads have altered the data in use. if so, the transaction is retried until a consistent commit can be performed. through the use of atomic blocks encapsulating code fragments, parallel code can be implicitly defined without knowledge about locking strategies or thread handling. the stm system guarantees to handle the atomic block correctly.

cluster number estimation
cluster number estimation can be linked to an assessment of the stability of the clustering. this issue is often discussed in cluster analysis, as the number of clusters in the data is usually unknown  <cit> . it has been shown that a repeated cluster analysis with different methods, parameters , feature sets, or sample sizes can help to reveal the underlying data structure. for instance, the bootstrap technique can be used for estimating the number of clusters  <cit> . if the fluctuations among the partitions are small compared to random clustering, the clustering is called robust, and that particular model is chosen. although there are few theoretical findings on the stability property of clusterings, this methodology has proven to work well in practice  <cit> . for stability evaluation of repeated clusterings, methods that measure the similarity of a clustering relative to some instance are used. these methods measure different characteristics of the identified partitions or sequences of partitions, thus implying repeated calculations. they can be subdivided into three groups  <cit> :

 <dig>  internal criteria: measure the overlap between cluster structure and information inherent in the data, for example silhouette, inter-cluster similarity.

 <dig>  external criteria: compare different partitions, for example rand index, jaccard index, fowlkes and mallows.

 <dig>  relative criteria: decide which of two structures is better in some sense, for example quantifying the difference between single-linkage or complete-linkage.

to demonstrate the quality of cluster algorithms, they are often applied to a-priori labeled data sets and evaluated by an external criterion  <cit> . an external index describes to which degree two partitions agree, given a set of n objects x = {x <dig>  ..., xn} and two different partitions p = {c <dig>  ..., cr} and q = {d <dig>  ..., ds} into r and s clusters respectively.

mca cluster similarity index
for the evaluation of the experiments, we here use a measure that is based on the pairwise similarity between set partitions and can be interpreted as the mean proportion of samples being consistent over the different clusterings  <cit> . because this index behaves linearly in the number of data points it offers a better interpretability in terms of proportion of samples moving between clusters. there is no such intuitive interpretability for quadratic validity measures like rand or jaccard index  <cit> . the concept is illustrated in figure  <dig>  in the left part of figure  <dig> two partitionings p and q are compared. the correspondence or similarity  between two clusters ci and dj is given by the size of the intersection set |ci ∩ dj|. the idea of the maximum cluster assignment  index is to find a bijective mapping π: { <dig> ... k} → { <dig> ... k} that maps each cluster from one clustering p to its corresponding cluster in q such a way that the sum over all similarities  is maximized. the bold lines in the right part of figure  <dig> mark the maximum matching nodes in the bipartite graph representation. in this example, the best mapping is a <dig> ↔ b <dig>  a <dig> ↔ b <dig>  a <dig> ↔ b <dig>  the mca index is then defined as the ratio of the number of data points in the intersection sets of the corresponding clusters to the overall number of data points:  

the normalization factor  bounds the index into . this is a well-known problem in discrete mathematics called linear assignment problem . in the current implementation, we use the algorithm by jonker & volgenant   <cit>  that runs in  after building the assignment matrix, which can be done in .

correction for chance
cluster validity indices are used to quantify findings about results of a cluster analysis. they do not include information about a threshold for distinguishing between high and low values. statistical hypothesis testing provides a framework to distinguish between expected and unusual results based on a distribution of the validity index  <cit> . the null hypothesis is chosen to reflect the case of no inherent cluster structure.

different null hypotheses, which lead to different expected values of a cluster validation index and all reflect a specific context of the clustering can be designed. yet, due to the complex characteristics of the baseline distributions and the validation indices it is often not possible to deduce a formula for the expected value of a corrected index  <cit> . in this case, a monte carlo analysis can assist to reveal the distribution of these indices under the chosen null hypothesis. in a monte carlo simulation, several independent test sets are sampled from a given baseline distribution. the chosen validity index is then evaluated for the random data sets. this gives an estimate for the expected value of a validity index under an empirical baseline distribution. then, the baseline distribution is used to correct the validation index i for randomness:  

where imax is the maximum value  and e is the expected value under a random hypothesis.

in the following, we consider three baseline scenarios that we call the random label hypothesis, the random partition hypothesis, and the random prototype hypothesis .

random label hypothesis
the random label hypothesis simulates the worst case behavior of a clustering. each data point is randomly assigned to one of the k clusters such that no cluster remains empty, i.e. ∀xi ∈ x assign xi to cluster cr, with r uniformly chosen from { <dig>  ..., k} and all cr ≠ ∅. the monte carlo simulation for the empirically expected value of the mca index under this baseline is shown in figure  <dig>  for the mca index, the expected value under this hypothesis can also be derived analytically:

• if  is an integral number, the expected value of matching points between partitions is .

• otherwise, there is at least one cluster expected to have more matching data points, i.e. the expected value is .

• e is not monotonically decreasing with n, but has a minimum at , see figure  <dig> 

the expected value of the mca index is:  

the number of matching points between partitions cannot decrease when choosing another baseline hypothesis, i.e. this hypothesis reflects the lower bound of the mca index. due to the limitations of the monte carlo simulation, the expected value of the simulated random label baseline stays constantly above the theoretical limit unless n ≫ k .

random partition hypothesis
the random partition hypothesis simulates the general behavior of randomly clustering a data set. under this hypothesis, every partition of n data points into k clusters is assumed to be equally probable. the number of possible partitions is given by the stirling numbers of the second kind  <cit> : . even for small n and k an exhaustive computation of all possible partitions is not feasible. to give an estimate of the expected value under this hypothesis, a monte carlo simulation can be used .

random prototype hypothesis
in contrast to the previous hypotheses, the random prototype hypothesis simulates the average behavior of a clustering with respect to a given data set. k cluster prototypes cj are chosen randomly, and according to these prototypes, an assignment is performed, e.g. the nearest neighbor rule: ∀ xi ∈ x assign xi to cluster cr if r = argminj||xi - cj|| <dig>  varying the assignment rule enables the simulation of different cluster algorithms . under this hypothesis, the generated partitions are data-driven and best reflect the random baseline clustering for each data set .

choosing the appropriate clustering
with a fast cluster number estimation, a two step procedure can be executed to choose the appropriate clustering. the first step consists of choosing a set of k's that have the highest robustness. for this task we and others propose the sensitivity of the clustering as a measure, see the preceding section  <cit> . robustness analysis is based on the observation that for a fixed number of clusters, repeated runs of a cluster algorithm on a resampled dataset often generate different partitions. the robustness of k-means is also affected by different random initializations. to reduce this effect, k-means is restarted repeatedly for each resampled dataset. only the result with minimal quantization error is then included into the list of generated partitions. in this regard, the median value of the mca index from comparing all generated partitions to another can serve as a predictor for the correct number of clusters. we define the best number of clusters k as the one with maximal distance between median mca index from cluster results and median mca index from the random prototype baseline. statistical hypothesis testing  can be used to rate the significance of the observed clusterings with respect to the baseline clustering and thus can serve to reject a clustering altogether, meaning no structure in data can be found.

in the second step, we choose the partition with the smallest quantization error for the selected k's. as k-means does not guarantee to reach a global optimum, but convergence to a local optimum is always given  <cit> , we use the strategy of restarts with different initializations  <cit> . finally, the result with the minimal quantization error  is selected as the best solution. for extremely large data sets, this strategy requires a fast implementation, as several hundreds of repetitions may be necessary  <cit> .

RESULTS
to illustrate the utility of our multi-core parallel k-means algorithm we performed simulations on artificial data, gene expression profiles and snp data. all simulations of mckmeans were performed on a dell precision t <dig> with dual quad-core intel xeon  <dig>  ghz and  <dig> gb ram. the four cores on each cpu share  <dig> mb of cache. simulations were partly compared to two reference implementations, namely the single-core k-means function implemented in r  <cit>  and the network-based parakmeans  <cit>  algorithm. for the k-means function from r , simulations were also performed on the dell t <dig> 

parakmeans was tested on the web interface at http://bioanalysis.genomics.mcg.edu/parakmeans. some of our larger test data could not be processed due to either a slow data loading routine  or memory limitations on the master computer. these runtime performance comparisons between different implementations  can only illustrate a rough difference between single and multi-core algorithms and should not be regarded as benchmarks.

artificial data
artificial data sets without cluster structure
we generated data sets without imposing a cluster structure. as the k-means algorithm is guaranteed to converge to a clustering, the median runtime of the algorithm on such data sets was used as a performance measure. we generated three simulated data sets . each feature is uniformly distributed over the interval  <cit>  to minimize the effect of random initializations. the performance of clustering the data sets into  <dig> clusters is summarized in figure 5a. each box summarizes the results of  <dig> repeated clusterings . in case of the small data set the computational overhead of the thread management negatively affects the runtime. for the extremely large data set, an improvement of the runtime by a factor of  <dig> can be observed .

the influence of changing the number of threads  for calculating the minimum distance partition  in mckmeans is shown in figure 5c. each box summarizes the results of  <dig> repeated clusterings for a data set . the choice of the number of threads shows best performance if it is in the range of the number of physical cpus to the number of cores, i.e.  <dig> to  <dig> cores.

we also performed a cluster analysis with mckmeans for different numbers of computer cores on a data set . a summary of the experiments using  <dig>   <dig>   <dig>  and  <dig> cores is shown in figure 5d. using  <dig> cores resulted in a runtime improvement by a factor of  <dig> compared to the single-core experiment. with  <dig> cores, the cpu usage rate never exceeded 600%, i.e. not all cores were used during calculations.

artificial data sets with gene cluster structure
we simulated clustered data sets using multivariate normal distributions as the basis for each cluster  <cit> . an artificial microarray experiment consists of n microarrays being composed of p genes. an experiment is sampled to contain exactly k gene clusters. within-cluster variance and between-cluster variance are used to assemble a set of exactly k well-formed gene clusters as follows: at first, k pair-wise equidistant gene cluster centroids μk are drawn from an interval around  <dig> with the variance set to the between-cluster variance . each gene is assigned to one of the k gene cluster centroids. then, a gene-specific mean μg is drawn from a normal distribution with the mean set to the assigned cluster centroid μk and variance set to the within-cluster variance . the variance of an individual gene over n microarrays  follows a χ <dig> distribution with n degrees of freedom. to get an unbiased estimate of the variance, it is divided by n -  <dig>  i.e. , with x ~χ <dig>  <cit> . only a small fraction of genes in the same cluster is set to have a non-zero correlation. the probability of any gene-pair to be correlated is set to c =  <dig> * 10-+2). for each cluster the number of correlated genes is determined by a poisson distribution with mean equal to , where pk is the number of genes in cluster k. if gene gi and gj are correlated, the covariance is calculated from a product of , and the correlation r is drawn randomly from a uniform distribution )  <cit> . the covariance matrix Σ and the gene mean vector μg are then used to simulate the different artificial microarrays. an artificial microarray is calculated from Σ and μg using the triangular factorization method. a matrix Σ can be factored into a lower triangular matrix t and its transpose t', Σ = tt'. it follows that x = yt + μg ~nk, with a matrix y ~nk. the factorization is done with the cholesky decomposition of Σ  <cit> .

we generated artificial microarray experiments with different number of genes, arrays, and clusters . benchmark results for these data sets are given in figures 6a+b. each box summarizes the results of  <dig> repeated clusterings. both mckmeans and k-means r use the mersenne twister to generate random numbers. when started with the same seed value, our implementation of k-means reproduces exactly the same results as computed by the reference implementation in r.

cluster number estimation on artificial data
to further illustrate the need for a high computational speed of cluster algorithms, we performed simulations to infer the number of clusters inherent in a data set. the stability is measured by comparing the agreement between the different results of running k-means on subsets of the data. the agreement is measured with the mca index, and correction for chance is done using the random prototype hypothesis. here, we simulated the clustered data set using separate multivariate normal distributions as the basis for each cluster. we generated a data set with  <dig> cases containing  <dig> clusters in  <dig> dimensions. the data set was resampled  <dig> times leaving out  data points. the effect of resampling on the stability of the clustering can be reproduced on this data. the experiment correctly predicts a most stable clustering into  <dig> clusters. total running time was  <dig>  min. in the simulation  <dig> separate clusterings were performed. we also performed a cluster number estimation for every artificial data set mentioned in this paper. all simulations predicted the correct number of clusters, see supplementary material .

gene expression profiles
smirnov microarray data
we also compared the algorithms on gene expression profiles from smirnov et al.  <cit>  with  <dig> genes and  <dig> cell lines. they used data from cells collected at baseline and  <dig> and  <dig> h after exposure to  <dig> gy of ionizing radiation. we performed two experiments on this data, one comparing runtimes of clustering genes and a second one performing a cluster number estimation for grouping cell lines. results of the runtime experiments are given in figure 6c. here, each box summarizes the results from  <dig> repeated clusterings. our multi-core algorithm performs up to  <dig> times faster than the single-core k-means algorithm included in r. in the cluster number estimation experiment, the objective was to find the best clustering of the  <dig> profiles using all available  <dig> genes. we performed  <dig> cluster runs . the best clustering was found with k =  <dig> clusters. these four clusters do not correspond to the quantitative phenotypes induced by radiation exposure . this suggests and supports the findings of smirnov and co-workers that indicate a highly individual response to the damage at the expression level, and not a uniform mechanism of how cells deal with this radiation exposure.

cluster results for the best clustering with four clusters. each entry shows the number of profiles that are in cluster ci and one of the groups .

single nucleotide polymorphism data
hapmap snp data
for evaluating the performance of mckmeans in clustering snp data, we used the hapmap phase i snp dataset  <cit> . the hapmap project collected snps from  <dig> individuals from four ethnic populations: yoruba in ibadan, nigeria , ceph , han chinese in beijing, china , and japanese in tokyo, japan . for the cluster analysis we only used unrelated individuals. the number of unrelated individuals per group is:  <dig> yri,  <dig> ceu,  <dig> chb, and  <dig> jpt. only snps with a minor allele frequency greater than  <dig>  have been included, which resulted in  <dig> snps.

we performed a cluster number estimation for this data , number of columns  <dig> ). for each k {∈  <dig>   <dig>  ..., 10}, we performed  <dig> runs of clustering on a resampled data set of row size  <dig> . the results are illustrated in figure  <dig>  for each k, two boxplots are shown, one summarizing the mca values from the pairwise comparisons of all cluster results and the other one showing the results of the random baseline. the best clustering  is reported for k =  <dig> . we then computed  <dig> repeated runs of k-means with k =  <dig>  the clustering with the minimal quantization error is given in table  <dig>  the reported clustering essentially coincides with the different populations. all individuals from ceu form a cluster, as well as individuals from yri do. one individual from chb is clustered into the group of jpt, and  <dig> individuals from jpt are clustered into the group of chb. this gives an overall accuracy of  <dig> % for separating the population by clustering the available snp data.

discussion and 
CONCLUSIONS
fast algorithms are increasingly becoming important in the individual laboratory, as the sizes of data sets grow and computational demands rise. we have devised a variant of the popular k-means/k-modes algorithm that effectively utilizes current multi-core hardware, so that even complex cluster number estimations for large data sets are possible on a single computer. computer-intensive bioinformatics software is frequently parallelized using a network-based strategy. such a parallelization can be very efficient when the same algorithm has to be started several times on different data sets of moderate size, or when different analyses have to be calculated in parallel on the same data set. however, this approach also requires additional effort and equipment, like specialized hardware for fast communication between computers, multiple software installations in heterogeneous environments, or compute cluster administration. for multi-core parallelization, openmp and functional programming languages provide a basic parallelization scheme through the parallel execution of loops. more efficient parallelization can be achieved through direct use of threads and locking variables, which requires additional effort for concurrency control as well. we have designed a highly efficient parallel k-means algorithm that utilizes transactional memory, guarantees concurrency, and can serve as a template for other parallel implementations. we achieve a performance increase that scales well with the available resources. an even more dramatic performance gain could be measured in the comparison to the single core k-means implementation: on  <dig> cores, the parallel implementation attained a 13-fold speed increase  for a large data set of  <dig> million cases with  <dig> dimensions. this disproportionately high increase is partly due to different data loading times of the r implementation and our java application. for smaller data sets, the highly efficient r implementation, which calls compiled c code, outperforms both our multi-core implementation and a network-based reference implementation on singular cluster runs. cluster number estimation is often discussed in conjunction with cluster analysis methods, as the number of clusters is an unknown prior  <cit> . for instance, the gap statistic can be used to search for a strong change in within-cluster dispersion across different numbers of clusters  <cit> . such approaches that are based on internal cluster measures favor highly compact clusters. other methods relying on resampling strategies combined with external cluster evaluation measures have been used to additionally incorporate the stability of single clusters  <cit> . consensus clustering assesses the stability as the percentage of object pairs clustered together  <cit> . here, the consensus matrix scales quadratically with the number of objects, and therefore becomes infeasible in clustering extremely large data sets. in contrast, our cluster number estimation method based on resampling and a similarity measure that is linear in the number of objects provides an easy interpretation of the results: instead of considering pairs of objects, we calculate the percentage of objects clustered together across multiple clusterings. for cluster number estimation across repeated runs of an algorithm, fast implementations become even more important, as simulation time can be extensive even for small data sets. for instance, for a cluster number estimation on a data set consisting of  <dig> cases and  <dig> gene expression values, our multi-core algorithm reduces the runtime from  <dig> min  to  <dig> min. for larger data, cluster number estimation now becomes feasible and can give new insights into the data, like suggesting a highly individual radiation-induced response of b cells at the expression level , or showing that a grouping of individuals is actually possible on the basis of single nucleotide polymorphisms . our evaluations of the mckmeans algorithm show that it is fast, achieves the same accuracy as the single-core reference implementations, and is able to cluster extremely large data sets. furthermore, the java implementation is easily deployable on different hardware and software platforms. it runs on a single desktop computer and is able to perform complex cluster number estimation experiments due to parallelization.

availability and requirements
the java software mckmeans supports multi-core and single-core k-means clustering of real valued data, k-modes clustering of snp data, and for both data types cluster number estimation. there are three possible modes of using mckmeans, the graphical user interface , the command line version, and the r-package .

graphical user interface
the graphical user interface  software is available for download from our website. usage of the software is described in the built-in help system. the gui supports clustering of microarray  and snp data. clustering subjects and clustering genes/snps can be done by transposing the imported data set. gene data is visualized as a scatterplot from two selected dimensions. furthermore, sammon's projection method can be performed to show a nonlinear two-dimensional projection of the data  <cit> . results from the cluster number estimation are given as boxplots. we integrated a statistical test  for computing the significance of the best cluster result. the best clustering and the results of the cluster number estimation can be saved for further analysis with statistical software such as r. all plots can be saved as svg files.

command line usage
for running batch analyses, mckmeans offers a command line interface to all functions of the gui version. the command line usage is described on our website.

r package
with the r package "rmckmeans", the multi-core k-means algorithm is fully integrated into the r software framework. rmckmeans is based on the java implementation and is freed from three important limitations in r:  r can only process data sets with up to  <dig> billion entries, while java supports datasets of size  <dig> ×  <dig>   loading time of extremely large datasets in r is extensive, and  r does not yet support true multi-core programs. all available multi-core packages in r cannot share memory and internally have to replicate the data for every used core, resulting in a lower amount of total memory available for computing.

mckmeans is a java program implemented in clojure . mckmeans can also be called from r .

• project name: mckmeans

• project home page: http://www.informatik.uni-ulm.de/ni/mitarbeiter/hkestler/parallelkmeans

• operating system: platform independent

• programming language: java, r

• other requirements: java  <dig>  or higher

• license: artistic license  <dig> 

• any restrictions to use by non-academics: no

authors' contributions
jmk and hak designed the study and wrote the manuscript. jmk implemented the algorithm and performed the experiments. all authors read and approved the final manuscript.

supplementary material
additional file 1
additional cluster number estimation results. cluster number estimation results are given for random data sets with and without cluster structure.

click here for file

 acknowledgements
we thank christoph müssel for r support. this work is supported by the german science foundation , the stifterverband für die deutsche wissenschaft , and the graduate school of mathematical analysis of evolution, information, and complexity at the university of ulm .
