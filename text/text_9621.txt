BACKGROUND
one of the most powerful high-throughput histology related analysis applications of recent years in medical research is the tissue microarray  technique  <cit> . this technique is commonly used for many medical research projects and is well recognized, among others, in the large scale human protein atlas project  <cit> . it is based on the use of formalin-fixed paraffin-embedded  tissue samples, designated donor blocks. cores from the donor blocks are organized as a matrix in a new paraffin recipient block, the tma . the array design and content can be highly customized, according to the aim of the experiments, and may be based on phenotype or genotype features, usually to examine differences among diseases, or to study the effect of drugs on tissues.

the intrinsic high parallelism of a tma block, due to the matrix structure that allows the examination of many different tissues at the same time, leads to several advantages, such as a decrease in the time taken to perform one assay instead of hundreds of separate assays, and a limited reagents cost, as around  <dig> separate experiments can be completed on a single tma. on the other hand the technique presents particular challenges in the input/output data evaluation, including the investigation of the results of hundreds of different tissues at the same time. in addition, tma construction is highly time-consuming because it requires a pathologist to identify, on every donor block, the area suitable to be punched-out and inserted into the array block. this aspect is often underestimated.

since the technique is relatively new, up to now few specific software tools have been developed to support tma construction. to carry out image evaluation, scientists require specific support in two important phases. first, in the evaluation of hematoxylin-eosin  stained slides of donor block tissues , to highlight the interesting areas for tma building. this is a highly time consuming task and in most pathology institutes evaluations are performed completely by hand for each of the hundreds of tissues that will compose the matrix. secondly, the scoring phase on tma samples  may be improved by the use of automatic algorithms. considering that a single tma block may theoretically yield around a hundred of stained sections and that each section may contain hundreds of samples , the need for support tools is evident. therefore, it is clear how the impact of the tma technique may be significantly enhanced by suitable and efficient algorithms to analyse tissue images.

to accomplish this task a number of applications exist. some software tools are commercially available and generally these allow the user to capture spots in images, organize them into a sorted matrix, annotate and analyse them with quantitative imaging, including nuclei counting and signal quantification or some morphology measurements. in addition, for most of these solutions the image software cannot be released as standalone and is usually distributed together with a virtual microscope system. however, these software packages focus on the final analysis phase and are not suitable for aiding automation in the pre-array phase. moreover, they are not very flexible for keeping up with ongoing experimental research and they have not been developed for automatically identifying variations in morphologies, which is the crucial feature in the pre-array phase. the most used tools, listed in figure  <dig>  are provided among others by slidepath, aperio, olympus, dako and beecher instruments.

useful highly customizable software can be found as open source applications  <cit> . most of these rely on web interfaces running on top of local databases to store tma related information, such as patients' clinical data, specimens, donor blocks, cores, and recipient blocks and in some cases also provide the possibility to digitally process microscope acquired images.

all of these packages take into account the post-array images, used in the analysis phase, providing semi-quantitative evaluation tools for the stained core images and assigning to them reaction scores, or exploiting extracted parameters to generate statistical results. nevertheless the identification and localization of suitable areas for extracting tissue cores that are to be used for tma construction represent a major bottleneck in a tma experiment.

in this paper we present an algorithm for efficient tma construction by automatic identification of areas of interest in a tma donor block, which has been included in the tmarepdb  <cit>  web site. in the pre-array phase, the identification of areas that are suitable for extracting the donor cores is not yet covered by any tma-oriented specific automation. this would require software to distinguish normal and pathological morphologies within a tissue. in this context the biological complexity of human tissues represents a challenge. moreover, matters are complicated by the fact that cancer is, by definition, an abnormal and uncontrolled proliferation of cells, further increasing the complexity of the histology. this means that the often heterogeneous areas must be evaluated with different features, and consequently the use of independent algorithms may be required. through a pilot study on tubular breast cancer, the paper shows the feasibility of creating an integrated system to deal with the peculiarities of each different tissue found in donor block sections. the focus of the work is the development of an algorithm able to analyse breast tissue images which provide useful representations of routinely prepared h&e stained sections of tubular breast cancer regions enabling a pre-selection of suitable areas for tma construction.

other works exist which describe automatic approaches to h&e stained tissue slides, to support specifically the field of histopathology and to improve the potentiality of telepathology and virtual pathology. an interesting approach for detecting cancers from digital tissue images has been proposed  <cit>  concerning the identification of the gleason grade, again in the context of prostate tissues. the main steps of the pipeline concern a bayesian classifier to detect gland lumen and the use of a support vector machine  to classify each tissue pathology grade. another work  <cit>  has been proposed to deal with the automatic detection of head and neck squamous cell carcinoma: a machine learning approach has been used, with the development of a svm classifier. image pre-processing has been performed exploiting the density based spatial clustering of applications with the noise  method  <cit> . nevertheless, the svm machine learning approach does not always represent the best choice, because, in our opinion, when investigated features are well defined more robust solutions can be adopted.

for analysing breast tissue, two state-of-the-art algorithms are presented  <cit> . both methods implement the three levels of the nottingham system, which is a scoring method that uses the range 3- <dig> to assess the grade of breast cancers. the former is based on otsu segmentation method  <cit>  and the use of thresholds to detect and score tubule formation, nuclear pleomorphism and mitotic cells, which contribute to the final grading. the latter evaluates two discriminant parameters, the number density of cell nuclei with disperse chromatin and the number density of tubular cross sections, through the exploitation of the lnknet tool  <cit> , developed at the mit . nothing exists in the specific context of tubular breast cancer.

the magic system  <cit>  represents an application in anatomo-pathology field of the baatz and schape method  <cit> , a general purpose image processing for high quality multi-scale segmentation. the implemented system is aimed at detecting histological objects in the prostate tissues by exploiting fully automatic tools for segmentation, classification, and extraction of color, texture, and morphometric features. the exploited method allows the accurate identification of histological classes of stroma, nuclei , cytoplasm, red blood cells, and lumens. this is a valuable system, also enriched by a graphical user interface, with the limitation of being only commercially available. moreover, as many other digital pathology works, it is oriented to h&e images analysis more than to tissue areas localization for pre-array punching purposes.

even other valuable commercial software exists, mainly oriented to tissue analysis in tma and the pathology analysis context: among them the tools implemented by tissuegnostics  <cit> , beecher instruments  <cit>  and aperio  <cit>  companies.

biological context
tubular carcinoma was chosen as pilot subject for the development of the presented tool. it is a type of invasive ductal carcinoma of the breast. it takes its name from its microscopic appearance, in which the cancer cells resemble small tubes. in some cases, tubular cancer cells are mixed with ductal or lobular cancer cells, giving a mixed-tumor diagnosis. tubular carcinomas are often very small, but may show up on a mammogram, as an irregularly shaped mass with a spiky, or starry outline.

it shows morphological features that differentiate it from other types of breast cancer, and that can be used to define parameters for developing an automatic approach for its recognition.

the first feature that has been considered is the presence of a single layer of cells around the lumen of the glands . in normal breast tissue each lumen is surrounded by two concentric layers of cells creating the acini of the lobules, as shown in figure 3a. the second characteristic of tubular breast carcinoma is the random organization of the tubular structures without the typical breast morphology of glands-and-lobules .

methods
dataset
the algorithm has been developed using a primary set of  <dig> tubular breast cancer biopsies provided by the erasmus mc tissue bank hosted at the department of pathology of the erasmus mc in rotterdam . standard  <dig> μm h&e sections were prepared from the ffpe tissue blocks and digitalized using a virtual microscope   <cit> , which allows the acquisition of the whole slide at high magnification through different available virtual objectives. the images were all acquired using a magnification of 200x and having the same dimension . all the h&e images have been valued by two experts in tissues morphology recognition, which discriminate pathological regions from normal ones. in case of contrasting evaluation, which occurs in about 10% of the analysis, the corresponding images have been removed from the dataset. to allow a suitable sampling of the tissue  and according to the defined magnification and dimension, the algorithm automatically subdivided these images using a  <dig> ×  <dig> punching grid which results in a reference dataset composed by  <dig> sub-images. using a classical approach for classification problems, the whole dataset has been separated into three parts: the training set, the tuning set and the validation set. the first set, composed of around 50% of the available sub-images , is used to build the model, in particular to evaluate the parameters suitable for the pipeline implementation; the second set, composed of 25%  of the available sub-images, is exploited to tune the model; the third set, composed of 25%  of the available sub-images, is necessary to measure the performance of the algorithm, holding the identified parameters as constant values. in order to guarantee the reliability of the results a cross-validation is performed, by repeating the classification four times, randomly selecting the images belonging to each group. during each step of the crossvalidation test parameters have been optimized in order to achieve the best possible results. remarkably, considering for each algorithm parameter the interval in which best values are placed for each experiment the range is quite narrow and the variance quite low. in agreement to the achieved results the algorithm default parameters have been selected according to the experiment that provided the best performance in terms of accuracy: data are reported in the following sections. in order to test the effective flexibility of our algorithm, two more datasets of images originated with different resolution and different pixel intensity have been exploited. one dataset includes  <dig> images presenting a resolution of  <dig>  μm/px and a pixel intensity similar to the reference dataset. according to our system, images from this dataset are converted in the preliminary algorithm step in order to obtain images presenting 200x of magnification and  <dig>  μm/px of resolution .

the other dataset includes  <dig> images showing the same resolution of the reference dataset but different pixel intensity. the first step when analyzing these images consists in normalizing to the average pixel intensity of the training set pref =  <dig> . for both sets, images that encountered contrasting tissue evaluation among pathologists  have been removed from the datasets. as reported in the performance section, prediction capabilities of our algorithm was good in comparison with the primary dataset . in both cases by providing a fine-tuning of the algorithm parameters  the performance showed improvements.

algorithms
the workflow for the identification of pathological regions is presented in figure  <dig>  it has been designed and evaluated in collaboration with biologists and pathologists, who deal with h&e slides analysis and manual biopsies diagnosis on a daily basis.

as a preliminary step, each whole image of an h&e stained section acquired through a virtual microscope is normalized with respect to image intensity and magnification, in order to make the images comparable to each other and thus providing algorithm thresholds suitable for all the considered images. each image has then been scaled to specific reference pixel intensity values , in order to eliminate the differences in brightness among images acquired at different times or with different instruments. the transformation is performed according to a well-established colour normalization approach  <cit>  which relies on the evaluation of three specific areas to take as references for a quadratic scaling. in h&e staining context key values are identified as the average intensities of background, nuclei area and cytoplasm regions. for each whole digital slide, these colour-levels have been extracted through segmentation methods and related histograms evaluations, and were then used for data calibration. pre-defined values have been assumed as references. a quadratic mathematical transfer function is used to get a new set of normalized images. for each image, the calibration coefficients a, b, c are automatically calculated based on the areas equalization:

  {yi,=axi,2+bxi,+cyi,=axi,2+bxi,+cyi,=axi,2+bxi,+c 

where xi and yi correspond to the value of the i-pixel of the image, respectively before and after the calibration on the reference values. in particular:

xi, represents the background average value of the image to be calibrated

yi, represents the background average value of reference image;

xi, represents the nuclei area average value of the image to be calibrated

yi, represents the nuclei area average value of reference image;

xi, represents the cytoplasm area average value of the image to be calibrated

yi , represents the cytoplasm area average value of reference image.

for each image, a, b, and c transfer coefficients are automatically calculated based on the previous data. all pixels of images were mathematically converted according to the mathematical transfer function and after the standardization step, images of the same dataset showed the same average colour levels on the background area, on the nuclei regions and on the cytoplasm areas.

when dealing with resolution normalization, images were always scaled to the reference value rref . it must be noticed that the user can intervene on the image normalization phase, in order to better adapt the software to manage images acquired through different technologies, by providing both customized rref and pref values before running the algorithm. this fine-tuning possibility allows the user to redefine the default values coming from the dataset on which this work relies, since performance could slightly suffer the preliminary conversion step of the algorithm. concerning rref a simple re-dimensioning is performed, while regarding pref , even new values of br, nr and cr components are calculated, maintaining the same proportions existing in the original dataset.

before enabling image classification using the designed algorithm, the image is divided into a number of independent sub-images, to be individually analysed and labelled according to the result of the computed diagnosis: the generated grid allows a tissue division where from each area corresponding to one sub-image one tissue punching can be performed randomly.

the successive step of the algorithm consists in discovering if either  the image contains significant information for the segregation of un-affected and affected areas, or  it represents an area where no tissue is present at all, or the tissue does not have the characteristics of affected or unaffected tissue . to perform this classification the average intensity of each image is calculated and compared with a threshold value. if the image is consistent with case , the algorithm must proceed, if the image is consistent with case  the algorithm stops. if the selected image is labelled as informative, the work flow evaluates the first discriminating feature among normal and pathological tissue, which is the random distribution of cells. the basic idea for performing the step arises by observing the whole tissue organization: in affected areas cell agglomerates are randomly distributed within the tissue and are much smaller and more numerous than in unaffected areas. therefore, the designed algorithm relies on object detection and area estimation: areas values are successively evaluated in order to distinguish if they belong to a well organized or a random distributed structure. at first a pre-processing step is required: each image is filtered in a suitable way to obtain a black and white mask where only gland nuclei are highlighted. firstly tissue components had to be separated virtually. a robust and largely used method for performing tissue image segmentation  <cit>  is k-means clustering, an unsupervised algorithm used to group n objects based on attributes into k partitions .

practically, k-means clustering takes pixel intensity values as inputs and considers three random values as starting points. using the euclidean distance it creates three groups that include pixels whose intensity values are close to the three values chosen as starting points. for each group the centroid is calculated, and the three groups are modified considering the centroids as the new starting points.

by applying this procedure to the h&e image and by iterating it, the algorithm approaches the local minimum of the function. when the group variance converges to the minimum each pixel group that has one distinct class label represents one segmented object component. in the studied context it separates the three main different colours of the image generating three result images:

• the white based image, showing the mucus presence

• the pink based image, indicating the basic areas 

• the blue based image, reporting the acid regions 

this method even enables the isolation of the dark blue intensities that characterize nucleic substance images. this is possible by ranking the pixel values by their intensity and just considering the darker ones, that correspond to nuclei, which are the most highly acid substance within the cell. such a processed image is then converted from an rgb  map to gray-scale and the agglomerate shape is highlighted using two morphological filters: dilation and hole filling.

after the pre-processing phase, area detection is performed by including each unknown-shaped agglomerate into fitting ellipses, defined by their semimajor and semiminor axes. if the analysed sub-image can be classified as random-distributed pathological tissue in this step, the work flow comes to an end and labels the image as affected.

if the work flow does not reach an endpoint the presence of a double or a single layer of cells is investigated, using a thickness algorithm based on hildebrand and ruegsegger approach  <cit> . the choice of this algorithm depends on the fact that it is a model independent thickness estimator, which is a very important feature since it has to deal with objects with an a priori unknown structure type. following this approach local widths are obtained by virtually inserting a disk into each image object and identifying the largest disk completely contained within the object structure. analytically, the local thickness τ , where p is an arbitrary point in the structure Ω ⊂ r <dig>  can be defined as:

  τ=2max⊆Ω,x∈Ω) 

where sph is the set of points inside a sphere with center x and radius r. the maximum local thickness is equivalent to the diameter of the largest sphere that completely ts inside the structure:

  τmax=max|p∈Ω) 

the algorithm works on the segmented image, by creating a distance map which assigns the euclidean distance to the nearest edge point to every point within each segmented object. this is equivalent to the radius of the largest sphere centred at the considered point and still completely inside the structure. the inclusion tests lead to the identification of non-redundant spheres, that will define the distance ridge. the thickness algorithm provides a false-colour image, which for each point shows the thickness of the structures by assigning different colours to disks of different diameter: in particular brighter colours are associated to larger disks. the output of the phase is the separation between single-layer pathological tissue and normal tissue. each sub-image is analysed with the described work flow, giving as final output a diagnosis of unaffected, affected, or fat area, indicated by the colour of the border that is overlaid on the original image in the last step of the analysis. the red border means that the considered area is affected, the green border designates unaffected areas. both the green and the red areas may be of importance for selecting cores for the tma. the grey border represents areas without tissue or tissue that is mainly fat. since each image is only a part of the whole slide, in the final step the image is reassembled, thus providing a view that allows the correct choice of the areas to be punched for building the tma block. labelled areas may differ from one region to another, due to the complexity of the tumour tissue mixed with normal tissue. the two biological aspects described, the layers of cells forming the lumen and the morphological organization of the tubular structures, were first considered in separate algorithms, and then combined in the unified work flow.

implementation
the algorithm has been implemented using perl language, embedding external functions based on different software, among which matlab  <cit> , a commercially available computing environment which provides a dedicated toolbox for image analysis , imagej  <cit> , a java based open source application for image management, particularly devoted to bio-images, and command line executables distributed in the imagemagick gpl licensed package  <cit> .

RESULTS
threshold values identification
a crucial role for a successful application of this algorithm is the selection of the parameters that characterize the examined features and the related threshold values for decision making. in the following section the whole work flow will be retraced in order to analyse the parameters used for each conditional step.

pre-processing
pre-processing step includes, other than the normalization phase, the detection of the correct sub-image size, and the identification of a suitable grid to cut the original image. the sub-image dimension is strictly connected to the aim of the proposed work, that is to select punching areas for tissue microarray experiments. therefore, the grid spacing and the sub image size cannot be arbitrarily varied. it depends on image resolution rref and is defined in order to guarantee a correct tile size with respect to real biological object size: the grid must be large enough to provide morphological consistency within each tile, and thick enough to always allow the correct distance between the selected punching areas.

not interesting sub-images detection
concerning the first phase, related to the presence of interesting information inside the considered sub-image, the discriminating factor is identified in the average value of the pixel intensity of the sub-image itself. the threshold value to distinguish between interesting and not interesting sub-images is set to an intensity value identified by analysing a training set of images including significant, void and fat areas. the value correspondent to the intensity that discriminates between brighter and darker images, thus defining those that do not contain informative tissue areas, is  <dig> *pref .

cells distribution evaluation
proceeding with the model investigation, from empirically analysing results achieved on the training set the most reliable parameter for the analysis of the cell distribution appears to be the standard deviation of ellipse area values. when plotting this data in x - y axis a significant difference is observed between tissues with random organization of elements and tissue with the characteristic breast tissue structure. figure  <dig> shows this difference in a common reference scale.

to avoid false positive results which may be caused by the presence of single t and b lymphocytes, which are frequently seen in inflamed tissues but that are not specifically involved in tumour proliferation, area values lower than a fixed threshold were rejected before plotting the data. this size filter has been established to 1/rref2* <dig>  which is consistent with the dimension of a typical lymphocyte  on a digital image.

the considered signal is represented by the distribution of the ellipses areas within an image. the signal standard deviation is computed and, repeating the process for each image of the training set, a biologically consistent value of this parameter has been obtained, which distinguishes between the well organized and the random distributed tissues, thus representing a threshold value. this discriminating value is set to 1/rref2* <dig>  which well distinguishes among normal images, presenting a higher variance of the object areas, and pathological ones, correspondent to lower variances.

cell layers thickness investigation
to answer the final conditional block of the work flow, concerning the thickness of the cell layer around the lumen, a histogram profile is produced for each image resulting from the thickness algorithm application. converting the image from rgb to gray scale, major thickness, related to unaffected double-cell surrounded structures, are dyed in bright colours, while thin objects, mostly present in pathological tissues, show dark colours. in fact, histogram comparison showed differences between the normal and the pathological tissues especially concerning the highest values of intensity, as expected. in this case the analytical parameter chosen to express the observed differences is the sum of the quantity of pixels estimated for brightest colours  in the histogram plot. in fact, according to given representation of the hildebrand and ruegsegger, higher intensity values correspond to major thickness of image objects. the value of this parameter in unaffected areas is always higher than in pathological ones, as shown in figure  <dig> 

a descriptive statistical evaluation has been performed on all data generated from the training set images to infer a parameter useful to distinguish physiological from pathological areas. the quantity of bright pixels within an image has been found suitable to discriminate among normal and pathological regions. the value has been identified, which depends on the rref : images presenting more than 1/rref2* <dig> bright intensity pixels include thick objects within the tissue while images with an inferior number of bright pixels are representative of just one layer of cells around the lumen.

performance
the results performances have been evaluated considering the judgment of two experts, to provide accuracy in histological assessment, in order to reduce any potential intra-observer variability  <cit> . concerning the primary dataset of biopsies, algorithm performances are summarized in table  <dig>  data are reported according to the four performed cross-validations, showing the experiments outcome and the related accuracy. the optimal algorithm parameters have very low variability among the different experiments, which can be attributed to the intrinsic physical meaning of the selected thresholds. according to data presented, the set of parameters chosen as default come up from the experiment 'test2', which provides the best accuracy , considering its values of sensitivity  and specificity . by analyzing in details the error accomplished in experiment 'test2', the origin of false positive and false negative results must be attributed to the misleading view over the tissue caused by the cutting angle on the sample. in fact, a limiting factor in distinguishing the disease-affected areas from the not-affected ones concerns the tissue-cutting plane. since it may intersect the tubule at variable angles, the resulting image does not in all cases clearly reveal the typical tubular morphology. for instance, if the plane runs perpendicularly to the tubule, all the information related to the presence of two layers of cells surrounding the lumen is preserved. on the other hand, if the tissue has been cut along the axis of the tubule it will result in a loss of morphology data . unfortunately this cannot be avoided and an automatic approach can present difficulties in coping with it.

beside the main dataset the algorithm has been tested with two other datasets, in order to test its flexibility while providing as input images which present varied resolution and different pixel intensity. data about these experiments are shown in table  <dig> 

for what concerns the resolution test, five images have been used. the test performed using default parameters resulted in 83% of accuracy, 83% of sensitivity and 83% of specificity. the output of the test involving the fine tuning  retrieved an accuracy value of around 90%, a sensitivity of 85% and a specificity of 94%. algorithm performance is better when enabling fine tuning than when using default parameters. even regarding the pixel intensity test five images have been used. the test performed with default parameters provided an accuracy value of 75%, a sensitivity value of 71% and a specificity value of 82%. the test which foresees the parameters fine tuning  provided an accuracy of 90%, a sensitivity of 82% and a specificity of 97%. even for what concerns software flexibility about pixel intensity, this second test showed better performances than the previous one which implies the use of the default parameters values.

in conclusion, the algorithm shows to be robust since parameters vary slightly in the cross-validation. moreover, the software presents good flexibility whereas it works receiving images of different resolution and pixel intensity as input. finally, the algorithm shows a high level of adaptability, due to the possibility given to users to exploit information about the dataset images to set the correct parameters and to optimize the performance.

the time performance primarily depends on the image size: typical size range goes from 3mb to 8mb. it must be noticed that the time spent to cut the global image in a set of sub-images, and the mirror phase of resembling, have to be considered as an intrinsic sequential part of the code, while the rest of the code  is parallelizable. cutting times go from  <dig> s to  <dig> s, while reassembling times go from  <dig> s to  <dig> s, according to image size. average sub-image processing time is  <dig> s, with a σ =  <dig> s, which takes into account not only the image size but even the variability of the last algorithm step.

a real example
in the following part an example is presented. the whole image was acquired with the previously defined procedure. the result is an image as wide as one third of the whole slide, with pixel dimensions of  <dig> ×  <dig>  using a suitable grid dimension the image was divided into  <dig> sub-images.

the pipeline is applied to each sub-image using the parameters and the thresholds described above. finally all the sub-images are collated, enriched with the computed information, as shown in figure  <dig>  observing the whole output image it can be noticed that two sub-images  are identified as normal  although inside a tumour area. concerning image  <dig> the software correctly identifies a region of normal tissue within the tumour area, as reported in figure  <dig>  this is crucial in the tma experiments punching phase since the random choice of this area of the tissue would lead to a mistake in the data analysis.

on the other hand, the classification of image  <dig> among the normal ones has to be considered a false positive, enforcing the idea that the implemented software represents a first screening and support for the pathologist and therefore should be followed by human evaluation.

CONCLUSIONS
the pilot experiment described here, involving tma support tools, shows that it is feasible to design a program able to distinguish areas in a tissue slide as affected, unaffected and non-informative and localized them. it is currently integrated into the web based tmarepdb database, together with some other basic image processing methods, like sobel edge detection filter and dilation morphological filter . this software has been developed to be used in the pre-array experiment step where, after the collection of donor blocks, affected and unaffected areas need to be identified to create the recipient block with the matrix of tissue cores. this normally needs the support of a pathologist, who marks the glass slide with a coloured waterproof pen where the areas of interest are to be found. as a first step toward process automation slides must be digitalised, in order to be suitable for storing and for using as an overlay on the donor block image, in the semi or automated tma software. the process is further automated with software capable of distinguishing areas of interest within tissues. the generated images may be used to extract the tissue cores by overlapping the image with annotations, through computer software over the image of the tissue in the paraffin block.

the aim of this work is to show that, despite the fact that automation in this context is a very challenging task due to morphological complexity of human tissues, the implementation of a pathologist's support tool, in the tma context, is feasible and the product is reliable. the strength of the computerized method presented here is that once it has been developed and fine-tuned it may be used to analyze numerous images in parallel in a short time.

future developments mainly concerns the possibility of switching the final evaluation of a sub-picture, that should be given to the user to enable modifying the process output. this would result in a completely reliable evaluation of the image, which could be further overlapped to the donor block to map the punching areas on it, with the possibility of performing automatic cores picking. in the authors' plan the described application could be part of a fully tma analysis pipeline: after the tissues collection and the staining process, the procedure would be carried out by automatic or semiautomatic arrayers, guided by software able to detect suitable punching areas, thus allowing a fast tissue microarray building.

in conclusion, although the algorithms can only approximate human competencies and experience, the developed tool represents an important aid for scientists through automation of a laborious and time-consuming job.

availability and requirements
material is available at url: ftp://fileserver.itb.cnr.it/federica/bmctub

software is archived in bmctub.tar.gz; requirements and documentation are provided in the readme file; the exploited datasets are contained in folder datasets.

list of abbreviations
tma: tissue microarray; h&e: hematoxylin-eosin; svm: support vector machine; vm: virtual microscope.

competing interests
the authors declare that they have no competing interests.

authors' contributions
fv and im designed and implemented the algorithm, mt and mdb gave the biological perspective, fb, pr and lm supervised the work from biological and technical points of view. all authors read and approved the final manuscript.

