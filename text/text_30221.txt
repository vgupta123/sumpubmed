BACKGROUND
the completion of human and other model-organism genome projects has provided a sequence infrastructure to allow an improved understanding of the dynamic processes of cellular signaling, regulation, and metabolism. although all cells contain the complete genome, only a fraction of the genes are expressed within a given cell. under different conditions or within different tissues of the same organism, a specific set of proteins are expressed and/or post-translationally modified to carry out the special function of the cell  <cit> . the term 'proteome' is a hybrid of "protein" and "genome" and it refers to the entire protein components, along with all covalent protein modifications in a selected cell. with the arrival of the post-genomic era, functional genomics has become a new focus of biological research, and proteomics has emerged as a promising field for assessing global protein function  <cit> .

in order to understand the roles of different proteins played and to dissect protein-protein interaction networks, high-throughout methodologies are being applied in the emerging field of proteomics. as a result, large amounts of experimental data have been generated by high-throughout proteomics methodologies, such as large-scale two-hybrid systems, high-throughout mass spectrometry technology, and multi-dimensional chromatograph. meanwhile, with the volume of proteomics information increasing rapidly, there has been a great need for a public proteomics repository and for exchanging these raw proteomics experiment data between labs  <cit> . the raw experiment data is usually generated by different instruments, laboratories and methods, thus it is still difficult to exchange the raw proteomics data directly.

recently, a new special organization called psi  was founded at the hupo  meeting in washington d.c. usa to define community standards for data representation in proteomics to facilitate data comparison, exchange and verification  <cit> . since the raw proteomics experiment data produced in our lab and the technologies used in most proteomics labs are still based on the 2d/ms systems, we intended to focus on the exchange of the raw proteomics data produced by 2d/ms systems with general proteomics format.

currently, there have been some xml models developed as standards with relevance to the whole of proteomics such as pedro, hup-ml and agml  <cit> . among these models, pedro and hup-ml are the two popular xml models used to process raw proteomics data. pedro was developed by a group led by prof. norman paton, which takes into account many aspects of gel-proteomics data compared with other xml models, such as mzxml, mzdata and mzident, which emphasize more specifically for mass spectrometry data  <cit> . hup-ml is another proteome- analysis-oriented format based on xml, and it was proposed by kamijo et al. at the  <dig> aohupo xml workshop. the data model of hup-ml is based on the classical 2d/ms systems and it can be used by most labs. here we adopted the hup-ml editor as the data capture software and hup-ml data model for our npc proteomics repository.

npc is one of the most common cancers in southern china and southeast asia, which demonstrates remarkable geographic and racial differences in incidence. public proteome repository is the infrastructure to study the complicated mechanisms of caner. although there are many cancer proteome databases, there has been no npc proteome database to our knowledge. in this paper, we used hup-ml editor to collect the raw npc proteomics data inclued the experiment results and experiment conditions. then, these xml documents were imported into xindice database, and php was used to pass the query request from web clients to database manage system  and to convert the query results back to the clients with html format. the php source codes can be downloaded from our website  to construct a user's own proteome repository.

RESULTS
an example of an xpath query result of the npc gel-proteomics experiment data in the npc 2d/ms repository is shown in figure  <dig>  the architecture of this repository is shown in figure  <dig>  to retrieve the exact information of an identified spot, two choices were provided to query the information. one method is through the text input to query the database with a ncbi accession number, a protein name or synonym name, or gene name. another query method is through clicking the clickable spots on the 2-de gel maps. both query methods were based on the xpath query. the results of an xpath query are returned as a xml document. to display the query results in a readable format, a transform must be done with an xslt processor before output to the client browser. an example of sablotron xslt processor transformed result is shown in figure  <dig>  in the upper right frame, the returned spot result has been shown with red cross on the 2-d gel image, and at the same time the detail protein information of the queried spot is shown in the bottom right frame. another query method is to directly click a spot on the 2-d gel image. if the spot has been identified in the experiment, the detail protein information will be displayed in the bottom right frame. both query methods allow users to access the related functional annotation information of the protein in the ncbi database through hyperlink.

in our npc proteomics repository, peak list of the monoisotopic peaks of every peptide mass fingerprint  maps is extracted with mascot distiller and saved as mgf file. all mgf files have been changed to text files and imported into hup-ml documents. when the users click the hyperlink of ms-map of the identified spot, the dbms queries that node and extracts the monoisotopic peaks from ms_peak_list tag to php, which will then be transformed to a mimic pmf map. by this method, the mimic of raw pmf maps can be shared by everyone without limitation of file format defined by the mass spectrometry manufacturer. figure  <dig> shows the pmf map of an identified protein glutathione transferase omega 1- <dig> generated by monoisotopic peak list.

discussion
there are currently two types of dbms used to store proteomics experiment results, there are relational database manage system  and xml database system. currently, most public 2d/ms databases adopt swiss-2dpage database or free rdbms, such as mysql, to store and manage their data. the swiss-2dpage database is based on the make2ddb software of sib . the backend database system of make2ddb is postgresql rdbms. although the swiss-2dpage database is well established, certain important experimental information and raw data still can not be integrated into database, such as the condition of protein separation and identification, the detailed descriptions of experimental samples, the raw mass spectrometry maps and etc. if researchers use other free rdbms, they have to spend a substantial effort on designing and optimizing the database for the information. the advantage of rdbms is that it can be used easily to store, manage, and query the structural information because of its specially designed structural and relational model. nevertheless, complicated data structure in the proteomics data integrated with hup-ml model makes it difficulty to construct a proteomics repository with rdbms because of some problems in mapping hierarchical structure to relational schema. in addition, if we use rdbms as back-end and map the proteomics data into tables, such dbmss force us to fragment our data into many pieces to satisfy the third normal form requirement. the fragmentation can also impose efficiency problems, as a query can cause the dbms to perform many joins to reassemble the fragments into the original data.

xml technology is the next generation of the internet language. it has powerful capability in exchanging data, and xml technology is particularly well suited to represent biological data and methods and is presently the consensus choice in most areas including proteomics because xml is highly flexible and xml provides an open framework for defining standard specifications  <cit> . as web services grow rapidly, xml flourish more andmore in data exchanging and sharing, and has resulted in two xml-based new database technology: native xml dbms  and xml-enabled dbms . with nxd, there is no need to map the special proteomics schema to rdbms. xindice is an open source native xml database developed by apache, which is a software foundation that promotes the construction of web-based tools and standards. compared with other open source xml databases, such as exist and xmldb, we think xindice may be more stable with better compatibility and technical supports. therefore, we decided to adopt the nxd database xindice to store, manage, and query the collection of raw npc proteomics experiment data.

pedrodb is another new database system for storing, searching, and disseminating experimental proteomics data, and it stores the raw proteomics data as xml format with xindice. pedrodb is a database system based on raw data capture software pedro, which has been developed to encode laboratory data and to generate an xml-based peml  file based on pedro model for local storage or submission to a database. unlike 2d/ms databases based on make2ddb, which emphasizes more on gel annotations, the pedrodb database was designed to provide more information, allowing detailed comparisons of the ways by which the results were obtained  <cit> . however, pedrodb is not available for downloading at least as of our writing.

the hup-ml document uses a flat file structure and it can be treated as a database or a table of rdbms in some sense. the related xml document can be directly put into the same directory and processed by the file manage system. but the functionality of this method is still insufficient, as it cannot provide the merit of a database, such as event security rescue mechanism, parallel control, and high efficient indexing and querying. therefore, by deploying the nxd to process the hup-ml documents, the whole system can be more efficient and secure.

xindice is an open source native xml database, featuring efficient querying based on xpath, xupdate support, and tight integration with existing xml development tools. however, xindice is subjected to the common limitations of nxd because of its short existence compared with rdbms, and not too many nxd-supported technologies and applications have been available.

both pedro and hup-ml represent the current efforts in using xml technology to exchange the raw proteomics data. at present, it is a good choice to use an existing effort, pedro or hup-ml, as a starting point for system design rather than a new one. to choose a raw proteome data capture software, we think that the annotation of gels may be more useful than detail description of experiment conditions. therefore, we select the hup-ml to intergate the different source information of gel-proteomics data.

peptide mass fingerprint map and tandem mass spectrometry are currently the two most commonly used technologies in proteomics for protein identification. because the mass spectrometry used in different labs were made by different manufacturers, the raw pmf maps and ms/ms maps generated by different equipment use different file formats which can only be opened with the special software from the mass spectrometry manufacturers. this has greatly increased the difficulty for exchanging the raw mass spectrometry data. a standard peak list format, such as mzdata, provided by psi requires many agreements from the original software provided by the spectrometry manufactures to third party software developing companies and will be implemented into next version  <cit> . we extracted the monoisotopic peak lists, which comprise the m/z data extracted from the raw maps, and imported it into npc repository. through the monoisotopic peak lists, user could view the raw pmf maps and compare with the user's own ms maps.

although xindice is fit for serving as backend of npc repository, some factors should be taken into account in order to improve the performance of database query. database indexes are a powerful technology to improve the efficiency of database querying. suppose the browsers commonly use the protein name and ncbi accession to query database, here we adopt element "protein_name" and "protein_data" element's "accession" attribute to index the npc collection but it costs almost the same time with no database index. unexpected observation might be due to a bug with xindice or a problem with our implementation. the size of the data file is another affecting factor. now the size of a file integrating all the data of  <dig> spots into xindice is about  <dig> kb, which can be considered as a medium sized file compared to the  <dig> mb xindice file limit. because xindice was specifically designed for managing many small to medium sized documents, it is not a good way to integrate everything into one file even though the present sizes of npc documents are still acceptable. integrating everything into one big file increases the file complexity and the required more time for database query, especially as the identified spots increase. we think one solution is to extract the data of each spot into a single file and import all these files into one collection when the data expand, and this is also an important optimization step involved in constructing our npc repository. although the benchmark test of database has not been performed, it is better to be done before optimizing and tuning the database.

CONCLUSIONS
with our php source code, 2d/ms experimental data can be delivered over the world wide web in an easily understandable format. one advantage of our platform inherent to representing information as alphanumeric strings is that the data can be easily stored and transmitted between different computer platforms and applications using the emerging xml technology, which is particular suitable for the development of proteomics web-services. another advantage of php plus xml is that this platform can be rapidly constructed and it can greatly decrease the efforts on databases designing, storing and exchanging between different labs by using the same standard formats. our website provide information more focused on the results of 2d/ms experiments, such as identified spots, 2-de maps, peak lists.

