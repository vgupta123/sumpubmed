BACKGROUND
prognosis gene signatures for the discovery of biological markers in carcinogenesis studies and the diagnosis of diseases is one of the essential areas in biomedical research. high-throughput screening technologies , such as microarrays, are able to examine more than a hundred thousand of oligonucleotide probes in parallel, which allows the interrogation of thousands of mrna transcripts in a single experiment. to date, transcriptome analysis using hts gene expression profiling has become a useful approach that can provide a stronger predictive power of clinical changes than the diagnostic testing procedures used in pathology  <cit> . out of thousands of interrogated transcripts in the cell of interest, a small subset of genes is assumed to be differentially expressed and is subject to change  <cit> . the exploration of differentially expressed genes that contribute to a better prediction can be referred to as feature selection.

feature selection is a technique of reducing the feature dimension of sample instances, where a subset of features is selected without creating new features from the original form of the features. this technique is widely used in data mining, machine learning and pattern recognition, and has also been applied to the field of bioinformatics  <cit> . known to be an np-complete problem  <cit> , feature subset selection not only finds a subset of relevant features for the use of a model construction but also looks into the minimal subset that optimises the best predictive model. this is actually based on the principle of parsimony  <cit> , i.e., seeking a model that has as few as possible variables to fit the data sufficiently. gene expression microarray experiments are often affected by noise that is caused by the experimental design of the underlying microarray technique, the stages of sample preparation, and the hybridisation processes of oligonucleotide probes  <cit> . several statistical and computational methods have been introduced to cope with the probe level data in recent years . besides the unavoidable technical noise, a typical scenario in the context of discovering gene-expression candidate genes is that there are many thousands of genes to be interrogated, but only tens to a hundred of clinical samples are available  <cit> . the curse of dimensionality makes the process of selecting relevant genes even more challenging.

filter, wrapper, and embedded methods are the three main types of feature selection techniques, where the taxonomy is based on the degree of interaction within a classification method  <cit> . a filter, being either univariate or multivariate, does not use a classifier within its selection scheme and takes only the intrinsic characteristics of sample instances into account in order to quantify the association between features and phenotypes. sam  <cit>  and limma  <cit>  are two examples of univariate filters in the domain of individual selections of differentially expressed genes, based on random permutations  and t-statistics , respectively. on the other hand, a multivariate filter, such as cfs  <cit> , considers feature interactions and therefore does not evaluate features independently, which is sometimes referred to as space search methods  <cit> . a wrapper  measures the predictive power of a feature subset by using a classification model which a repetitive selection scheme is wrapped around  <cit> . due to small sample sizes and an abundance of features, a wrapper is usually prone to overfitting and computationally expensive in spite of the benefit of its multivariate nature. while sequential search  is deterministic  <cit> , simulated annealing and genetic algorithms can be regarded as classical randomised search methods . search procedures embedded into a given learning algorithm where features are ranked or weighted in the context of a classification task are called embedded methods. popular embedded methods are svm-rfe-like  and random-forest  <cit> . both methods interact well with classifiers, are of multivariate nature, and require less computational time compared to a wrapper. nowadays robustness or stability of feature selection are one of the major issues. several techniques  have been devised for making feature selection more stable for biomarker reproducibility. of particular interest is the ensemble approach  <cit> . the approach uses a sampling technique to generate numerous different selectors and combines the components into a consensus ranking list. with regard to the stability of feature selection, we refer the reader to .

recently, feature selection methods using information theory have been devised for feature-to-class relevance and feature-to-feature correlations, including a probabilistic interpretation based upon the conditional likelihood maximisation in order to unify information theoretic feature selection  <cit> . we consider in more detail three information theory-based multivariate filters that exemplify an approximation of higher order gene interactions and aim at the selection of a gene subset. the methods are compared to a new gene-expression candidate gene filter proposed in the present paper. the first method is called the minimum-redundancy and maximum-relevance framework . it uses mutual information to manage the tradeoff between the deduction from redundant features and the gain from relevant features  <cit> . the conditional mutual information maximization  method utilises the so-called minimum operator of conditional mutual information for the evaluation of relevant features that are conditioned on the selected feature subset by using only pairwise feature statistics  <cit> . whereas mrmr and cmim introduce evaluation criteria, the fast correlation-based filter  uses symmetrical uncertainty and designs an efficient backward elimination scheme for the removal of irrelevant and redundant features  <cit> . the three filters all consider feature relevance and feature redundancy, but they still neglect feature interdependence in favour of moderate computational complexity. despite the lesser relevance of neglected features, they could, however, exhibit a strong discrimination when combined with other features and might reveal interactions within a set of candidate genes.

in this paper, we present and fully formulate a new multivariate filter, irda, designed for the exploration of cancer-related candidate genes under a hts gene expression profiling experiment. the filter is based on information theory, approximate markov blankets, and several heuristic search strategies. being a four-step framework, irda takes into account a number of feature properties that include feature relevance, feature redundancy, and feature interdependence in the context of feature-pairs. the irda filter is a data-driven approach that does not employ a priori biological information and the filter can properly tackle interdependent features through the subtle design of the underlying algorithmic procedures. additionally, the filter produces a small number of discriminative genes for improved phenotype prediction, which is advantageous for the domain user since a small number of candidate genes supports greater efficiency of in vitro validation. to demonstrate the strengths of irda, three performance measures, two evaluation schemes, two sets of stability measures, and the gene set enrichment analysis  have all been used in our experiments. its effectiveness has been validated by using eleven gene expression profiling data . the experimental results show that irda is stable and able to discover gene-expression candidate genes that are statistically significant enriched and constitute high-level predictive models.

preliminaries
domain description
in this section, the domain of hts gene selection for phenotype prediction is briefly described. given a gene expression dataset d={x∈ℝm,y∈ℝ}={}i=1n, where d consists of n samples x labeled by a class vector y , and each sample is profiled over m gene expressions, i.e. xi={}i=1n,m≫n . the task is to find a small number of discriminating genes   for clinical classification to be validated experimentally and to identify a gene signature for a specific disease. to address the issue of hts-based gene signatures, one can refer to the task as a feature selection problem. let f be a full set of features  f={fi}i=1m, then feature selection aims at choosing a feature subset g⊂f that maximizes the prediction performance; moreover, if one tries to minimise g, a parsimonious subset is sought for.
fig.  <dig> cancer classification using high-throughput screening technologies. a an example of gene expression profiling data. the experimental dataset contains n samples and each sample has m interrogated genes . b the extracted samples of an experiment are labeled according to their phenotypes or different types of cell lines. both a gene expression matrix and a class vector form the input of gene selection for cancer classification. c a subset g of significant genes is obtained as an output for a certain cancer classification, which is a so-called gene signature



information theory basics
entropy is the rationale behind information theory and is an intuitive measure to evaluate the uncertainty of a random variable. given a variable, the entropy is computed at the level of probability distributions  <cit> . let x be a nominal random variable, then the shannon entropy is defined as 
  h=−∑x∈xplogp, 

where the x denote the values of the random variable x, and p is the marginal probability distribution of x. unlike conventional statistics, an entropy-based measure does not make any a priori assumptions. this differs, for instance, from the student’s t-test, where the values have to be normally distributed. further information quantities can be defined through applying probability theory to the notion of entropy. the conditionalentropy of x given y is represented as 
  h=−∑p∑plogp, 

where p is the conditional probability of x given the observed values of y. this quantity evaluates how much uncertainty of x is left given that the value of another random variable y is known. similarly, the jointentropy of two random variables x and y is denoted by 
  h=−∑∑plogp, 

where p is the joint probability distribution of x and y. it quantifies the amount of information needed to describe the outcome of two jointly distributed random variables. another important information theoretic measure, mutual information, quantifies the amount of information shared by two random variables x and y. the quantity can be defined according to 
  mi=h−h. 

the measure is symmetric and non-negative, and if the value equals zero, then the two variables are statistically independent. the mutual information of x and y can also be conditioned on a variable z as conditional mutual information, which is defined by 
  cmi=h−h. 

the quantity measures the information amount shared between x and y, if z is known. finally, we introduce symmetrical uncertainty, a measure that will be heavily utilized in our gene selection framework throughout the paper. the measure can be viewed as one type of normalised mutual information and is defined as 
  sux,y=2h−hh+h. 

similar to the joint entropy, the joint symmetrical uncertainty can be defined as 
  sux <dig> x2;y=2h−hh+h. 

feature relevance
feature subset selection is to find a subset of the original features of a dataset such that a classifier generates the highest accuracy of classification upon the reduced data that only contains the selected features. kohavi and john   <cit>  addressed the issue of finding a good feature subset and its relation to which features shall be included by partitioning features into three types of strong relevance, weak relevance, and irrelevance. given a class variable c, a set of features f, a feature fi∈f, and fi=f∖fi, the kj feature types are defined by the conditional probability below.

definition <dig> 
kj-strong relevance.a feature variable fi is strongly relevant iff there exists an assignment of values c~,fi~,fi~ for which 
  pc=c~|fi=fi~,fi=fi~≠pc=c~|fi=fi~ 

or 
  p≠pfor short. 

definition <dig> 
kj-weak relevance.a feature fi is weakly relevant iff 
  pc|fi,fi=pc|fiand∃fi′⊂fisuch thatpc|fi,fi′≠pc|fi′. 

definition <dig> 
kj-irrelevance.a feature fi is irrelevant iff 
  ∀fi′⊆fi,pc|fi,fi′=pc|fi′. 

kohavi and john used the above theoretical representations to claim that two degrees of feature relevance  are required in terms of an optimal classifier. the removal of a strongly relevant feature will result in performance deterioration of the classifier. for a weakly relevant feature fi, there exists a subset of features, fi′, such that the performance of the classifier running on fi′ is worse than the performance on fi′ with the inclusion of fi. the loss of discriminative power is reflected by the symbol ≠ in the kj representation  <cit> . in short, the strongly relevant feature is indispensable in the kj sense and cannot be removed without loss of prediction accuracy, while a weakly relevant feature can sometimes contribute to classification performance.

similar to kj definitions, we can define a strongly relevant feature-pair fij given two jointly distributed random variables fi and fj .

definition <dig> 
kj-strongly relevant feature-pair.

a feature-pair fij is strongly relevant iff 
  pc|fij,fij≠pc|fij. 

where fij denotes the feature set f excluding fi and fj at the same time.

a feature-pair is referred to as a united-individual and must be selected together during the process of selection. the strong relevance of a feature-pair will be the basis for the framework presented in our paper for finding hts gene-expression candidate genes.

kj-relevance, correlation, and discretization
kohavi and john proposed two families of feature relevance  and claimed that a classifier should be taken into account when selecting relevant features. therefore, kohavi and john used a wrapper approach to investigate feature relevance by an optimal classifier in practical selection scenarios, such that the prediction accuracy of the classifier was estimated using an accuracy estimation technique  <cit> . on the other hand, correlation is widely used in filter-based feature selection for relevance analysis  <cit>  with the use of a correlation measure. a correlation-based filter employs the following assumption: if a feature variable  is highly correlated with a class variable , then the case of strong relevance is expected  <cit> . a higher correlation value implies a stronger feature relevance.

there are various measures for the correlation between two random variables. a typical correlation measure is relief - it assigns a relevant weight to each feature that represents the relevance of the feature variable to the class variable  <cit> ; the measure has been used in cfs  <cit> . other popular correlation measures are based on the notion of entropy in the context of feature selection filters  <cit> , which is mainly used in this paper and requires the continuous gene expression data need to be discretized for the calculation of entropy-based quantities. here, we discretize continuous features using the scheme presented in  <cit> . given the mean  and standard deviation  of expression data for a gene across all l, any values smaller than μ−σ/ <dig> are substituted by 1; any values between μ−σ/ <dig> and μ+σ/ <dig> are replaced by 3; any values larger than μ+σ/ <dig> are transformed to  <dig>  like other correlation-based filters, a measure to quantify the correlation between two random variables needs to be defined. in the present framework, this measure is called r-correlation and we propose four types of r-correlation, where each type applied to a different stage of our four-step selector of candidate genes.

definition <dig> 
r-correlation.the four types of correlation are: 
 r1-correlation expresses the correlation between the feature fi and the class c, denoted by r;

 r2-correlation expresses the correlation between the feature-pair fij and the class c, denoted by r;

 r3-correlation expresses the correlation between the feature fi and the feature fj, denoted by r;

 r4-correlation expresses the correlation between the feature fi and the class c given a seed feature set ms, denoted by r.



here, r measures the degree of correlation between x and y -), and r quantifies their correlation conditioned on an additional variable z ). based on the generic definition, a number of suitable correlation measures - either linear or nonlinear - can therefore be applied to our framework. in the present paper, we choose the information-theoretic measures of shannon entropy to calculate the four types of r-correlation . the correlation measures are sux,y,sux <dig> x2;y, and cmi ; defined in eqs.  <dig>   <dig>  and  <dig>  the details of how the correlations are calculated and where the four types of r-correlation are applied are shown in table  <dig> 
{Ωk},ms,g⊆∪umsp
ij


r  is used to establish the structure of “relevance-based k-partition” , which is being introduced in definition  <dig>  we also use the r1-correlation for arranging the order of features that form a seed feature set ms  and to aggregate candidate genes g from a set of parsimonious sets msp . the strength of r  is utilised for exploring kj-strongly relevant feature-pairs fij; see definition  <dig>  to estimate the crucial threshold ε in definition  <dig>  r1-, r2-, and r3-correlations ) are required. finally, r  is employed as a conditional independence test for identifying redundant features with respect to a subset of features .

methods
notions and fundamental principles
we introduce a number of fundamental concepts that constitute a filter for high-throughput screening gene selection. in our previous study  <cit> , we have found that feature-pairs would play more important roles than individual features in the context of discovering candidate genes for cancer classification via a “ratio by correlation” plot. by utilising a suitable correlation measure, in general, a feature-pair variable  can be highly correlated with a class variable c if compared to a single feature variable   <cit> . also, feature-pairs having high correlation values are combinations of different types of features in the context of strong and weak correlations; that is, it could be a pair of strongly correlated features; a strongly correlated feature & a weakly correlated feature; or a pair of a weakly correlated features. thus, while searching for strongly relevant feature-pairs, not only strongly relevant features can be selected, but also putative weakly relevant features can be included, i.e., features of weak relevance are sometimes able to contribute to the classification performance when combined with other features. consequently, a feature-pair could have more potential than a single feature when dealing with feature interdependency that takes gene synergy into account. this leads to the following criteria for finding potential feature-pairs, assuming that the more likely a feature-pair fij correlates with a class variable c, the more likely it is kj-strongly relevant.

definition  <dig> 
kj-strongly relevant feature-pairs.for a fixed threshold ε≥ <dig>  a feature-pair fij is considered to be kj-strongly relevant iff 
  rfij,c>ε. 

within our framework, we apply the concept of a markov blanket in order to be able to identify minimal subsets of discriminative features resulting from the exploration of kj-strongly relevant feature-pairs by using the measures of r2-correlation. the concept of markov blankets was introduced in  <cit>  and was incorporated into optimal feature selection by koller and sahami  <cit> , with the assumption that the markov blanket mb of a target variable c is independent of any fi∈f∖mb; the fcbf method  <cit>  extended the approach from  <cit>  to efficiently remove redundant features, based on the search for an approximate markov blanket. additionally, tsamardinos and aliferis  <cit>  considered the connection between kj-relevance and the markov blanket of a target variable in a bayesian network faithful to some data distribution, which aims at building the minimal subset of features according to the following definition:

definition  <dig> 
markov blanket. a markov blanket, mb, is a minimal set of features such that ∀fi∈f∖mb, 
  p,mb)=p. 

tsamardinos and aliferis  <cit>  showed that the blanket is unique and that it also coincides with the case of kj-strongly relevant features under the assumption of “faithfulness” , which can be summarised in the following theorem .

theorem <dig> 
in a faithful bn, a feature fi∈f is kj-strongly relevant if and only if fi∈mb.

since we focus on feature-pairs, we extend the notion of markov blankets accordingly:

definition  <dig> 
markov blanket for feature-pairs.a markov blanket for feature pairs, mfp, is a minimal set of feature-pairs such that ∀fij∈f∖mfp, 
  p=p. 

assumption.
fij∈mfp iff fij is kj-strongly relevant.

typically, there is a huge number of interrogated genes in high-throughput gene expression profiling. therefore, finding an exact markov blanket appears to be impractical. similar to the strategy proposed in  <cit>  regarding the fcbf method, we aim at finding an approximate markov blanket for the problem of discriminative gene discovery. high-throughput gene expression profiling returns only a relatively small number of differentially expressed genes, and the correlation values between a feature variable and a class variable are exponentially distributed. thus, we propose the k-partition of the feature  space with regard to relevance, which is a key component of our framework.

definition  <dig> 
relevance-based k-partition.

given a feature space f and {Ωk}k=1k, where Ωk=fik|i= <dig> ⋯,|Ωk|. if 
  ∀1≤i<|Ωk|,rfik,c≥rfi+1k,c; 

  letΩ~kbe the mean ofrfik,cinΩk,then∀1≤i<k,Ω~k>Ω~k+1; 

  Ωk∩Ωk+1=∅andf=Ω1∪Ω2∪⋯∪Ωk, 

then {Ωk}k=1k is called a relevance-based k-partition of f. note that the symbol |⋯| represents the cardinality of a set.

the proposed partition orders features with regard to the relevance of a class variable within a partition and between partitions. features in the same partition can be viewed as having a similar scale of relevance, while features from two remote partitions do belong to two distinct feature types. for example, if we assume that strongly/weakly relevant features are in Ω <dig> …,Ωk− <dig>  then Ωk can be regarded as the collection of irrelevant features. with the relevance-based k-partition, we are now able to define a seed feature that can provide information about multivariate feature-to-feature relationships.

definition  <dig> 
seed feature. a feature fs is a seed feature if ∀fj∈f∖Ωk,fsj are kj-strongly relevant feature-pairs, where distinct features fj are all coupled to the same feature fs.

for any strongly relevant feature-pair, if the coupled feature is identical, then the other features are dependent on the seed feature and might have interdependence among them to some extent in terms of biological interaction. consequently, seed feature sets are defined for constructing putative markov blankets.

definition  <dig> 
seed feature set.

for a given ε> <dig>  we consider all feature-pairs fsj with seed feature fs that are ε-kj-strongly relevant according to definition  <dig>  a seed feature set, ms, is then a set of features led by fs that has an underlying order of features w.r.t. their r1-correlation: 
  ms=fsj=fs:fj|rfj,c≥rfj+ <dig> c. 

here, ‘ fs:fj’ denotes that the first element in ms is fs followed by its coupled features fj.

thus, a seed feature set consists of features based on kj-strongly relevant feature-pairs that have the same seed  feature fs. we note that when the seed feature set is formed, feature-pairs fsj are decoupled; that is, ms is the collection of single features with an underlying order according to r. furthermore, r≥r is not necessarily true, but, by definition, it is part of all ε-kj-strongly relevant feature pairs fsj. thus, no matter how strong/weak r is, fs is still considered as the first element in ms. hence, ms is called a set of features led by fs. we emphasise that definition  <dig> uses the r2-correlation for strongly relevant feature-pairs as in definition  <dig> with the same coupled feature fs, but the r1-correlation determines the underlying order within ms. the set allows us to look at feature-feature relationships beyond low-order interaction, which leads to the notion of redundant features with respect to a seed feature set.

definition  <dig> 
redundant feature. a feature fi∈ms is redundant iff fi is irrelevant with respect to {ms∖ fi}, i.e., 
  p=p. 

although it seems that every feature within a set led by a seed feature is of relevance , in fact some features may not increase the predictive power with respect to the set. these features are then redundant and should be removed from the set. therefore, given a seed feature set, we need another measure  to assess feature redundancy w.r.t. ms. in the present framework, we use conditional mutual information to calculate how strongly a feature variable is correlated with a class variable conditioned on ms so that the redundant features can be identified, according to the following corollary.

corollary <dig> 
criteria for redundancy.

fi∈ms is redundant iff 
  cmi= <dig>  

proof.
conditional mutual information can be expressed as the kullback-leibler divergence , i.e., cmi =dkl∥pp)≥ <dig>  cmi  is equal to zero iff p=pp for some assignment of values x,y, z. since p=pp, we have p=p, which implies p=p. in terms of eq. , this means that fi is redundant. □

after the removal of redundant features related to the seed feature set, one can eventually build a parsimonious set of features.

definition  <dig> 
parsimony model. msp is called a parsimony model iff ∀fi∈ms,fi is not redundant within ms, which implies msp=ms.

a parsimony model is, therefore, a heuristic approximation of the markov blanket where the existence of least feature redundancy is admitted. initially, strongly relevant feature-pairs with the same seed feature are discovered for a putative blanket ms in a forward phase. out of these coupled features, some features could become false positives from a multivariate point of view. an approximate markov blanket can then be created if these false positives are identified and eliminated from ms. once multiple parsimony models are built, a set of candidate genes for high-throughput gene expression profiles can be selected.

definition  <dig> 
candidate genes.

a set g of features with g⊆∪umsp is called a set of candidate genes such that ∀msp & msp,rfsu,c>rfsu+ <dig> c.

as our original intention is to select gene-expression candidate genes from gene synergy, the parsimony model would not always be the best way to find a suitable size of a gene signature that not only would have good predictive power but also could reveal highly likely regulators or markers regarding a certain disease.

the new filter
a complete framework for finding high-throughput gene-expression candidate genes is presented through algorithm ??. the filter is named irda, an abbreviation for gene selection derived from interdependence with redundant-dependent analysis and aggregation scheme. the framework is based on information-theoretic measures, heuristic search strategies, parameter estimation criteria, a mixture of forward-backward phases, and a gene aggregation scheme. the rationale for devising such a framework is to select a set of candidate genes from gene synergy that could potentially discover genetic regulatory modules or disease-related factors. interdependence between features is, therefore, a matter of concern.



the proposed gene selection method is a four-step framework with a vast body of feature-pairs, including a set of analyses of feature relevance, feature interdependence, feature redundancy and dependence, and feature aggregation. the construction of the relevance-based k-partition is the main objective in the first step. the discovery of {Ωk} plays an important role in exploring strongly relevant feature-pairs, finding a parsimony model, and performing gene aggregation. in our framework, symmetrical uncertainty is used as r-correlation in order to quantify the strength of association between features/feature-pairs and class variables. first of all, for each feature fi,sui,c  is calculated for estimating the degree of feature relevance . this is followed by sorting all of the calculated correlations in descending order ; k-mean clustering is executed on the sorted list of sui,c in order to partition features into five groups that are labelled as Ω <dig> ⋯,Ω <dig> in descending order according to their centroids of sui,c values . these feature types will be passed onto subsequent steps of the framework as indicators for the discovery of seed features, putative parsimony models, and a set of candidate genes.

the consideration of high-order gene interactions could have the potential for a road map of feature interdependence. however, because of the immense complexity of gene regulatory mechanisms, it would not be a good strategy to infer high-order feature interdependence in a direct way, since it is impractical to perform exhaustive search for visiting all feature-pairs if the number of features is very large. in large-scale hts gene expression profiling, differentially expressed genes are biologically assumed to be a small portion of the population and the correlation values between a feature variable and a class variable are exponentially distributed. by using the k-partition {Ωk}, we are able to explore potential kj-strongly relevant feature-pairs whose r2-correlation values are beyond a threshold ε, which is estimated by the following method .

definition  <dig> 
criteria for ε estimation. a feature pair fi,fj∈{Ωk}, where fi is ahead of fj in {Ωk}, is called positive joint feature-pair iff 
  rfij,c>rfi,c;rfj,c>rfi,fj. 

 for given l positive joint feature-pairs, ε is defined by the mean of their r2-correlation: 
  ε=∑l=1lrlfij,cl. 

condition  implies that the feature-pair has a joint effect relative to a class variable that is more significant than the contribution of each single feature, where the contribution is still larger than the correlation between the two features.

in the second step, given a joint random variable of two features fi and fj , joint symmetrical uncertainty sui,j;c is used to measure the strength of correlation between a feature-pair and a class variable. the key idea of interdependence is to generate seed feature sets by using forward selection .

in the forward phase , Ω <dig> is assumed to be a kj-irrelevant-feature subset, while features with kj-strong/weak relevance would exist in the other subsets of the partition. moreover, if we assume that the population of Ω <dig> consists of predominantly strongly relevant features with a minority of weakly relevant features, then one feature from Ω <dig> in conjunction with other features from Ω <dig> …,Ω <dig> might constitute feature-pairs  whose joint symmetrical uncertainty values are greater than the threshold ε . a feature-pair fij having a strong r2-correlation according to definition  <dig> is added to a subset led by a seed feature fi and/or to a subset led by a seed feature fj. thus, fij can lead to two seed feature sets, mi and mj, respectively. due to the structure of the relevance-based k-partition, r is stronger than r. features with the strongest r1-correlation, e.g., f <dig> and the respective pairs f1j, might generate a large number of mj, each consisting only of a few elements, which is too complex to be analysed. moreover, such “fragmented seed sets” might generate noisy data and make gene aggregation extremely demanding. consequently, we propose three selection modes  for the production of probable seed feature sets. in the “greedy” strategy, fij will be added to mi only, and fi is followed by fj. since fi is a seed feature, it will be added just once . on the other hand, fij is added to both seed feature sets mi and mj for the other two selection modes . in case of “semi-greedy” selection, we consider the removal of “fragmented seed sets” that have just two features, fs and fi, inside. if r is weaker than r, the fragment is removed; otherwise, the fragment would still be viewed as a candidate ms . eventually, a collection of non-empty seed feature sets, gpre, is returned . in summary, the “greedy” strategy ignores many probable seed feature sets, but reduces the level of noise when genes are aggregated. the “non-greedy” selection is to fully explore the space of potential ms, and this is especially appropriate for a data matrix where only a few “fragments” are generated. the “semi-greedy” strategy not only allows the presence of some “fragments” , but also takes targets false positives to be removed.



after a seed feature set has been formed, features are analysed with respect to redundancy in conjunction with a given seed features set. thus, the third step in algorithm  <dig> is to identify and remove redundant features with the aim of building a parsimonious set of features . the analysis of redundancy and dependency will be carried out using backward elimination. since there are three modes of selection in the forward phase, two different scenarios are considered in the backward procedure. whereas the “greedy” selection performs two runs of the backward phase with an insertion phase , the other two go only through the backward phase .

algorithm  <dig> shows the details of the backward selection for generating a parsimony model msp . given a collection of subsets gpre, derived from interdependent analysis, the conditional mutual information cmi   of a feature fi and label c conditioned on a subset ms∈gpre is chosen to be the r4-correlation ). the corollary  <dig> reveals how to identify whether or not a feature is redundant with respect to a subst. however, it is inherent to hts profiling that the data exhibit small sample sizes. consequently, it is to be expected that the cmi-based correlation does not accurately express the exact joint distribution of features. therefore, the redundant-dependent analysis of ms will be base upon an approximation of backward elimination as defined below.

definition  <dig> 
approximate backward elimination.we assume that elements of ms are ordered in descending order according to the selected r1-correlation . 
 first seat last check: ∀fi∈ms∖fs, the features are checked for redundancy in ascending order of r1-correlation  using the criteria of corollary  <dig> and fs is checked at last step;

 once fi is removed, fi cannot enter ms again;

 if fs is removed, then ms is discarded.





according to definition  <dig> of a seed feature set, the front features in the seed feature set are of stronger relevance, which implies that they are less likely to be removed when corollary  <dig> is applied. therefore, following definition  <dig>  for any ms ∈ gpre, we test if the value of cmi  is zero for every feature checked as described above . a feature whose cmi-value is zero will instantly be removed  and the next feature will be checked until all features from ms have been tested. if a seed feature is eliminated, the subset ms led by this feature will be discarded ; otherwise, features that remain in the  subset are considered to be dependent with regard to the seed feature. thus, a subset that is not discarded is defined by at least two features .

in the forward phase with “greedy” strategy, a potential feature-pair fsj is not evenly included into seed feature sets. for this reason, we design an insertion phase for re-structuring putative seed feature sets, which is shown in algorithm  <dig>  for any fsj∈gpre′ generated by the first round of backward elimination, we add the pair to the seed feature set led by feature fj, if it is applicable. the move is motivated as follows: if the existence of fj is in multiple seed feature sets after backward elimination, it might imply that fj is likely to be a potential feature such that a seed feature set led by mj might improve the overall performance.



since the insertion phase might create new seed feature sets, a second round of the backward phase is executed . the complete execution of the third step of irda eventually returns a set gpost of multiple parsimony models msp , where we assume that the elements  of gpost are ordered, namely according to the r1-correlation of their seed features that lead the parsimony models msp . the order of msp in gpost can be an indicator for gene aggregation .

wrapper-based evaluation scheme
the underlying paradigm of our method is to provide multiple parsimonious gene sets instead of a unique parsimony model as usually returned by existing feature selection methods. such filters produce candidate genes sequentially one by one, which then extends also to the evaluation process. unlike existing gene selectors, the irda method  selects a candidate gene set g from gpost, which is derived from parsimonious sets msp; i.e., irda aggregates candidate genes sequentially one set by one set, not one gene by one gene. furthermore, the sets msp are ranked according to the r1-correlation of their seed features r .

consequently, in order to cope with single gene vs parsimonious sets, it is imperative to provide the necessary implements for a fair comparison of candidate genes derived from different filters. we propose a wrapper-based evaluation scheme for evaluating different sets of candidate genes from various filters. before presenting the proposed evaluation scheme, we introduce a set of three evaluation measures that are used to assess the classification performance of candidate genes.

definition  <dig> 
performance measures. given a gene expression data set d, a set of candidate genes g, and the knn classifier, the three performance measures of binary classification are denoted by 
 error: generalization error;

 auc: area under the roc curve;

 mcc: matthews correlation coefficient.



the generalization error  is an intuitive judgment about the misclassification rate, but might not present a valid picture if the two classes under consideration strongly differ in size. the matthews correlation coefficient  is generally viewed as a balanced summary statistics that takes into account true positives & negatives as well as false classifications  <cit> . the receiver operating characteristic  curve is a plot of the true positive rate  against the false positive rate  for a given predictor  <cit> ; while a random predictor leads to the auc value of  <dig> , the perfect outcome returns the auc value of  <dig> 

the proposed evaluator is based on a wrapper approach that utilises two performance measures  in conjunction with the k-nearest neighbours classification model . additionally, a search scheme of sequential forward selection  is ‘wrapped around’ the application of mcc, auc, and knn. the evaluator is denoted by ma-knn . if a set of candidate genes g is given, through the evaluation of genes one by one, conducted by a non-parametric classification model knn, the behaviour of candidate genes can be evaluated by dual performance measures and based upon a sequential forward strategy. here, auc and mcc are chosen to find ‘promising’ genes, which are called successive victory genes, as defined below. this way an evaluation profile  of g is generated.

definition  <dig> 
successive victory gene.let o denote the set of previously examined genes; u represents the uniqueness of maxmcc. then g is called a successive victory gene iff 
  g=argmax∀fmcc,u=true,argmax∀hauc,u=false. 

the value of u indicates how many of the genes examined along with o have an identical value maxmcc. in case of u=true, there is a unique gene that dominates the performance measure mcc, and therefore gene f with maxmcc is selected. on the other hand, if u=false, then there are multiple genes along with o that have the same maximum of mcc, and therefore the additional measure auc is invoked. among the genes, a gene h with maxauc is selected, i.e., the selected feature was successively ‘victorious’ in terms of mcc and auc performance.

the ma-knn evaluator, shown in algorithm  <dig>  begins with initial assignments of the examined gene set , the maximum of mcc measures , and the maximum of auc values  . since irda generates candidate genes from gpost, the initial state of o is therefore the first parsimonious gene set, while for other filters the first gene identified by the filter is given to o. we then update g by removing the initial genes and construct an initial evaluation point ei .



the two evaluation measures auc and mcc are then computed for each gene f in g, and a successive victory gene g can be identified out of the remaining candidate genes by using sequential forward selection . the gene g is now removed from g and added to o , and the next evaluation point ei is created for the update of o . this process is iteratively repeated until all genes in g have been examined, which means that an evaluation profile  of candidate genes has been obtained .

RESULTS
cancer benchmark datasets
seven publicly available microarray-based gene expression benchmarks were used  to demonstrate that the proposed framework is potentially capable of selecting the most discriminative candidate genes for phenotype prediction and of finding significant genetic regulation within the selected set of genes. the seven datasets have frequently been used to validate the performance of cancer classification and gene selection 


the brain experiment was designed to investigate whether high-throughput gene expression profiling could classify high grade gliomas better than histological classification. this data set consists of  <dig> samples and  <dig>  probe-sets using affymetrix human genome u95av <dig> array. out of  <dig> high grade gliomas, there are  <dig> glioblastomas  and  <dig> anaplastic oligodendrogliomas . the second experiment recorded embryonal tumor patients in the central nervous system . there are  <dig> patient samples with  <dig> genes. among these samples,  <dig> are survivors  while  <dig> are failures . the colon experiment, introduced by alon  <cit> , consists of  <dig> samples from the patients of colorectal cancer, where  <dig> normal labels are extracted from healthy tissues and  <dig> abnormal biopsies are extracted from colon tumors. out of more than  <dig> genes in the original design of experiment,  <dig> genes were selected to analyze by  <cit> , based on the confidence at the measured expression levels. the leukemia dataset includes gene expression profiles of two classes of bone marrow samples labeled with acute lymphoblastic leukemia  and acute myeloid leukemia . there are  <dig> samples  and  <dig> genes in this dataset. the fifth experiment is about clinically relevant cancer diagnostic tests of the lung. there are  <dig> tissue samples profiled by  <dig>  gene expression intensities. among these observations,  <dig> are of malignant pleural mesothelioma  and  <dig> are of adenocarcinoma . the lymphoma experiment was designed to delineate diffuse large b-cell lymphoma  from a related germinal center b-cell lymphoma, follicular lymphoma , and to identify rational targets for intervention. in this dataset, there are  <dig> observations  with the interrogation of  <dig> probe-sets. the last dataset contains the expression levels of  <dig>  genes for correlates of clinical prostate cancer behavior. there are  <dig> observations in total, from  <dig> tumor patients  and  <dig> non-tumor patients , respectively.

classification performance
to evaluate the effectiveness and characteristics of the proposed framework, three well-known multivariate filters  that utilise information theoretic measures are used for comparison through the examination of the seven recent microarray-based cancer classification datasets. since the sample size is far smaller than the feature dimension in a typical hts gene expression experiment, the conventional training-test data partition of 70– <dig> %  is not very appropriate for the evaluation of gene selection approaches. thus, the procedure of leave-one-out cross-validation  is used in our experiments. we employ performance measures for assessing the gene discrimination of the gene selectors under consideration. the three performance measures were introduced in definition  <dig> , along with the knn reference classifier. the classifier is used to induct candidate genes identified by a filter-based feature selector into a learning process. here, we exploit a non-parametric classifier, knn , for building inductive models from the results produced by gene selectors.

parsimony model
based upon parsimonious models , minimal feature subsets are returned by the irda filter and the three reference filters are evaluated by using the three evaluation measures error, auc, and mcc. in particular, we proceed as follows: the maximum cardinality of msp  that constitute gpost is identified. each reference filter produces genes one by one, and every round the newly produced gene along with previously examined genes are evaluated. the evaluation process stops when max|msp| genes have been generated; if max|msp|< <dig>  the process stops when five genes have been evaluated. for each reference filter and each performance measure , a minimal gene set with the best performance is reported as a parsimony model for the filter and performance measure. out of the msp constituting gpost, irda reports the parsimony model with the best performance regarding the same evaluation measures, and the outcome is then compared to the parsimony models returned by reference filters.

in the respective tables, the best performance  is highlighted in boldface, and the second best evaluation result is highlighted in italics. table  <dig> shows the generalization errors of mrmr, cmim, fcbf, and irda over the seven microarray-based benchmarks. in six out of seven cases, irda returns the smallest number of misclassification, with three rate values identical to mrmr. for irda, the average error rate is  <dig>  %, which is achieved for the smallest average number of  <dig>  genes compared to the other three filters. on the third dataset, irda takes the second place. although fcbf uses only a slightly larger average number of  <dig> genes, its discrimination levels are not as good as irda on all seven datasets. on average, mrmr is ranked at second place with an average rate of  <dig>  %, and the average number of genes  is a bit smaller than that of cmim . both irda and mrmr have no misclassification on the leukemia data, using three and four genes, respectively. all the parsimonious gene sets of four filters in terms of generalisation error rate are provided in additional file 2: table s <dig> 
6
4
4
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
0
 <dig> 
 <dig> 
0
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
%: misclassification rate; #: number of explored genes



the results of the auc performance are summarised in table  <dig>  while irda displays the best results on five datasets, mrmr returns the same value  on leukemia and is better on the lymphoma datasets. in particular, the parsimony model of irda achieves  <dig> % on the leukemia  and lung datasets. on the prostate dataset fcbf has the best auc performance with  <dig>  % while irda and cmim perform almost equally well . overall, irda exhibits on average the highest auc  with the fewest genes . the average auc of cmim and mrmr are  <dig>  % and  <dig>  %, respectively. the parsimonious gene sets of four filters in terms of auc performance are provided in additional file 3: table s <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
100
100
 <dig> 
100
 <dig> 
100
99
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
%: auc performance rate; #: number of explored genes



the results for the matthews correlation coefficient are shown in table  <dig>  while mrmr returns the best results on four instances, irda achieves the best result on cns and leukemia data, with the second best performance on the remaining five instances. however, except for the colon dataset, the difference between the first place and irda on four datasets  is relatively small, with a maximum of  <dig>  %. moreover, with respect to the average value over all seven datasets, irda shows the best mcc performance with  <dig>  %, which is achieved with the smallest average number of genes. we note that the ranking of filters w.r.t. average performance is similar to the one from table  <dig>  which is in line with the general observation that both generalization error and matthews correlation coefficient can exhibit the same overall predictive power. the parsimonious gene sets of four filters in terms of mcc performance are provided in additional file 4: table s4
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
100
 <dig> 
100
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
%: mcc performance rate; #: number of explored genes



gene aggregation evaluation
other than the construction of parsimonious subsets of genes, it is also important to identify candidate genes that could have a high classification performance and therefore are likely to play a role in regulatory modules or as biomarkers. as already mentioned, existing filters produce candidate genes sequentially one by one, and then this sequential order of genes is used to look at their classification performance. in contrast, irda is a filter that produces candidate genes by sequentially aggregating parsimonious gene sets. in this section, we use the sequential ordering of aggregated parsimonious gene sets and compare the classification performance to the three reference filters. for each dataset, we aggregate all of the parsimonious sets in gpost, where the individual sets are dissolved and feature pairs are decoupled, with the resulting set being g . with the known cardinality of g, each reference filter then produces the same number of genes in a sequence. we note that fcbf cannot generate as many genes as |g| for the cns and colon datasets.

figures  <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> display the classification performance of the candidate genes produced by four filters across the seven microarray-based gene expression profiles with regard to three performance measures. for the brain, cns, and colon data, irda produces the best discriminating genes, dominating all three performance measures, followed by cmim and fcbf, while the genes selected by mrmr have a lower level of discrimination. we note that except for irda, no other filter dominates the three measures of error, auc, and mcc. furthermore, while irda achieves for individual numbers of genes and the three measures error, auc, and mcc the levels of  <dig>   <dig>   <dig> %, respectively, on leukemia, lung, and lymphoma data, genes produced by cmim and mrmr reach the perfect level only on leukemia and lung data. fcbf exhibits a slightly worse performance in these datasets. however, since there is only a marginal difference between the filters for all three measures on leukemia, lung, and lymphoma data, the three datasets are apparently more easily to classify w.r.t. the underlying two tissue types. for the prostate dataset, mrmr has the best performance for the error and mcc measures, whereas cmim approaches the best level for auc. initially, irda has the worst performance on prostate data , but with an increasing number of genes its auc performance improves and approaches the levels of mrmr and cmim.
fig.  <dig> classification performance of candidate genes found by four filters upon three measures: brain cancer

fig.  <dig> classification performance of candidate genes found by four filters upon three measures: cns


fig.  <dig> classification performance of candidate genes found by four filters upon three measures: colon cancer

fig.  <dig> classification performance of candidate genes found by four filters upon three measures: leukemia cancer

fig.  <dig> classification performance of candidate genes found by four filters upon three measures: lung cancer

fig.  <dig> classification performance of candidate genes found by four filters upon three measures: lymphoma cancer

fig.  <dig> classification performance of candidate genes found by four filters upon three measures: prostate cancer



in summary, except for the prostate dataset, irda dominates the performance results for an increasing number of genes. on the other hand, the parsimonious gene sets of mrmr can sometimes dominate top-rankings in mcc performance, as discussed in section “parsimony model”, but it seems that its discriminative power does not improve when more genes are selected. furthermore, on the datasets we analysed, the performance of cmim improves with an increasing number of genes, which in most cases eventually leads to better results than those produced by mrmr and fcbf.

evaluation by ma-knn wrapper
in addition to the performance analysis executed in section “gene aggregation evaluation” directly for the three measures error, auc, and mcc, we expose the gene sets produced by the four filters to the wrapper-based ma-knn evaluator introduced in section “wrapper-based evaluation scheme”. here, we are using the same candidate gene sets as described at the beginning of section “gene aggregation evaluation”, but the candidate genes are processed by the ma-knn wrapper, and the outcome is then evaluated by the three performance measures as in section “parsimony model”  and section “gene aggregation evaluation” .

the results are shown in figs.  <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig>  on the datasets for brain, cns, and colon, the four methods demonstrate markedly different capacities of discrimination between binary samples. the new filter irda exhibits on the three datasets overall the best classification results and dominates for most of the gene numbers all of the three performance measures. the second best overall performance is displayed by cmim, which for some gene numbers returns better results than irda. cmim is followed by mrmr and fcbf, although fcbf shows sometimes marginally better results for error and mcc than mrmr on brain data for an increasing number of genes. we note that for cns and colon data, only irda achieves the optimum values of  <dig> and  <dig> % for auc and mcc measures, respectively. on leukemia, lung, and lymphoma data, all of the four filters perform nearly equally and perfectly well. regarding the prostate instance, the differences of performance among the four methods become less obvious than those for the brain, cns, and colon data. however, one can observe that overall irda dominates the the other three filters, followed by cmim, which also reaches optimum levels for error, auc, and mcc. fcbf is ranked third and reaches the optimum level for auc, while mrmr displays on prostate data the least performance. in comparison to the results from figs.  <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig>  irda demonstrates an even stronger overall performance when the ma-knn wrapper is applied. all the candidate genes selected by the four filters are provided in additional file 5: table s <dig> 
fig.  <dig> processing of candidate genes by ma-knn wrapper and evaluation by error, auc, mcc: brain cancer

fig.  <dig> processing of candidate genes by ma-knn wrapper and evaluation by error, auc, mcc: cns


fig.  <dig> processing of candidate genes by ma-knn wrapper and evaluation by error, auc, mcc: colon cancer

fig.  <dig> processing of candidate genes by ma-knn wrapper and evaluation by error, auc, mcc: leukemia cancer

fig.  <dig> processing of candidate genes by ma-knn wrapper and evaluation by error, auc, mcc: lung cancer

fig.  <dig> processing of candidate genes by ma-knn wrapper and evaluation by error, auc, mcc: lymphoma cancer

fig.  <dig> processing of candidate genes by ma-knn wrapper and evaluation by error, auc, mcc: prostate cancer



other disease types
besides the cancer expression profiling benchmarks, we have carried out the gene expression experiments on additional disease types in order to further understand the characteristics of the new filter. the experiments concern a variety of diseases and include larger samples, all above  <dig> and up to  <dig>  and their detailed information is provided in table  <dig>  all the datasets are archived in gene expression omnibus   <cit>  and can be accessed by their gse accession number. the series gse <dig> is to profile multiple myeloma  patients with  and without  bone lytic lesions by mri. gse <dig> is to study cultured skin fibroblasts from marfan syndrome  subjects and unaffected controls of similar age and sex distributions. the identification of gene expression level in different tissues between hiv-positive and hiv-negative patients is represented by gse <dig>  the experimental design of the last dataset is about the alzheimer’s like neurodegeneration , using the anti-ngf ad <dig> transgenic mouse model, which is compared to transgenic vh controls. table  <dig> shows the classification performance of the parsimonious models of genes selected by the four filters over the diseases multiple myeloma, marfan syndrome, hiv infection, and neurodegeneration. cmim dominates the first place in the average of generalisation error rates , while mrmr and irda have the best performance regarding the average of auc scores  and mcc scores , respectively. in terms of the minimal subset of selected genes, the three filters perform not significantly differentially and outperform fcbf.
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
0
0
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
100
100
100
100
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
100
100
 <dig> 
%: performance rate; #: number of explored genes



figures  <dig>   <dig>   <dig> and  <dig> display empirical results about whether or not the performance can be improved if more genes are selected. in the experiments for ms and hiv, irda outperforms the other three methods and achieves the evaluation values of  <dig>   <dig>  and  <dig> % for error, auc and mcc; whereas cmim returns values similar to mrmr, and both are better than fcbf. for the ad dataset, all filters perform equally, no matter what measures are used. for the disease of multiple myeloma, cmim performs worst, and there is no strong distinction between irda, fcbf and mrmr regarding the measures error and auc. finally, the classification performance of selected genes that are evaluated by using the ma-knn wrapper for the four datasets is shown in figs.  <dig>   <dig>   <dig> and  <dig>  the experimental results show that irda outperforms the other methods on all four diseases, while mrmr takes the second place, followed by fcbf and cmim. we also observe that the ad experiment is data set that can be easily classified, and that mm is the most difficult to classify, which can be seen from the mcc performance with a level of around  <dig> . all the candidate genes selected by the four filters are provided in additional file 5: table s <dig> 
fig.  <dig> classification performance of candidate genes found by four filters upon three measures: multiple myeloma


fig.  <dig> classification performance of candidate genes found by four filters upon three measures: marfan syndrome


fig.  <dig> classification performance of candidate genes found by four filters upon three measures: hiv infection


fig.  <dig> classification performance of candidate genes found by four filters upon three measures: neurodegeneration


fig.  <dig> processing of candidate genes by ma-knn wrapper and evaluation by error, auc, mcc: multiple myeloma


fig.  <dig> processing of candidate genes by ma-knn wrapper and evaluation by error, auc, mcc: marfan syndrome


fig.  <dig> processing of candidate genes by ma-knn wrapper and evaluation by error, auc, mcc: hiv infection


fig.  <dig> processing of candidate genes by ma-knn wrapper and evaluation by error, auc, mcc: neurodegeneration




computation time
we conducted an experiment about computation time with regard to the four gene selectors applied to all of the  <dig> gene expression profiling data in the environment of matlab  <dig> , with the hardware being an intel core i7- <dig> cpu of  <dig>  ghz and a  <dig> gb ram. for each of the four filters, we measured the time required for generating candidate genes. the results are summarised in table  <dig>  not surprisingly, improved performance comes at a price: cmim is the fastest method, followed by mrmr, fcbf, and irda. since cmim and mrmr are criteria-based only filters and do not incorporate search-based methods, they are expected to be faster than fcbf and irda. although both fcbf and irda utilise a heuristic search strategy, there are two selection phases  involved in irda. consequently, fcbf outperforms irda in terms of run-time on  <dig> out of the  <dig> data sets considered in our study. moreover, we note that irda generates more viable parsimonious sets to build candidate genes, which, of course, affects the run-time. however, all the irda run-time data are in the region of a few seconds.
unit: seconds



stability performance
in order to assess the robustness of the four feature selection methods, we consider two stability index-based measures with respect to differently sized gene lists. the jaccard index quantifies the amount of overlap between two datasets, ranging from  <dig> to  <dig>  with  <dig> indicating empty intersection and  <dig> indicating that the two sets are equal .

definition  <dig> 
jaccard index. given two gene lists gi,gj,ji a stability index called jaccard index, which is defined as follows: 
 ji=|gi∩gj||gi∪gj|. 

the definition is extended to larger sets of gene lists in the following way:

definition  <dig> 
overall jaccard stability. given a system of l gene lists u, ∀gi,∀gj∈u we define the overall jaccard stability of u as 
 sji=2l∑i=1l−1∑j=i+1lji. 

the jaccard index suffers from the problem of list-size-bias: the more lists approach the size of the total pool of features, the higher the probability of an overlap in pairs of gene lists. to solve the problem, the relative weighted consistency  <cit>  has been introduced based on the relative degree of randomness of the system of lists in the feature selection process.

definition  <dig> 
relative weighted consistency. given a system u of l gene lists gj⊆f, let oj= <dig> denote fi ∈ gj . we set n=∑j=1l∑ioj, which is the total number of occurrences of features in u, and rf=∑j=1loj. the relative weighted consistency of u is then defined by 
 srwc=|f|)−n2+q2|f|−q)−n2+q <dig>  

where q=n and q=n.

we compare the stability of the four filters by using the two stability measures sji and srwc over the eleven datasets . for each dataset, a pool of data samples sj derived from the procedure of leave-one-out  is constructed. for example, the brain dataset consists of 22+28= <dig> samples, which generates  <dig> sets sj of size  <dig>  for each sj, a set of candidate genes gjh is produced by each of the feature selection filters, h= <dig> …, <dig>  for identifying the gjh, as before , irda is executed first, which determines the cardinality of gene lists. the other three methods subsequently generate the same size of gene sets. again, due to the nature of fcbf, the method can sometimes return only a smaller portion of genes. figure  <dig> displays the results for the overall jaccard stability and the relative weighted consistency as average values over the eleven datasets. although srwc provides higher values than sji does, the two box-plots show similar results. for both measures, irda is the most stable with the least variance, followed by mrmr with a smaller median value and a larger variance. fcbf is slightly inferior to mrmr, while the least stable selector is cmim in both plots.
fig.  <dig> box plots of the jaccard index and the relative weighted consistency  to show the stability of the four filters



the details of stability measure results on each dataset are shown in bar-charts in fig.  <dig>  the behaviour of the four filters is less stable on the brain, cns, colon, and hiv datasets, except for irda on brain and hiv data, when compared to the other seven datasets. the gene lists returned by irda perform better than by the other three filters on seven datasets, specifically on brain data. the results for lung data suggest that the gene lists are least varied, such that all methods perform nearly equally well with high index values. for the instances where mrmr dominates the other filters, the difference between irda and mrmr is only marginal. we noticed that there is a large amount of fcbf lists whose sizes are rather small on cns and colon data compared to the other three filters, but, interestingly, this causes fcbf to be the least stable on cns data and the most stable on colon data. surprisingly, cmim appears highly unstable on ad data, whereas the other three filters remain very stable.
fig.  <dig> bar charts of the stability of the four filters across eleven datasets using jaccard index and rwc, respectively



enrichment analysis
whilst a set of genes is selected, it is essential to understand if some genes would interact with other genes in the set. gene set enrichment analysis  is able to provide a good insight into the complex interaction among genes, based on collections of a priori biologically defined and annotated gene sets  <cit> . since its introduction about  <dig> years ago, gsea has become a standard procedure for looking at groups of genes that share common biological function, chromosomal location, or regulation. in the present paper, we utilised for the analysis of candidate gene sets the molecular signatures database  in conjunction with gsea-v <dig> in order to gain knowledge about how many gene sets are statistically significantly enriched. table  <dig> reports  the number of native features  produced by the four filters that were considered for the enrichment analysis over brain, cns, leukemia, lung, lymphoma, and prostate datasets, and  the number of genes  that were actually used by gsea after the process of collapsing original features into gene symbols. we excluded the colon dataset, since it exploited an array of affymetrix hum <dig> where many ests are mapped into the same gene symbol. all the collapsed genes over six cancer benchmarks for gsea are provided in additional file 6: table s <dig> 
n: native features; c: collapsed features



based on the sets of collapsed features, numbers of gene sets recognised by gsea-v <dig> as statistically significant enrichment for the six cancer types are shown in table  <dig>  where gsea employs a false discovery rate to indicate a significance level . the results show that irda produces on five out of the six datasets the largest number of statistically enriched gene sets. cmim occupies the second place in this experimental study, whereas mrmr and fcbf exhibit a similar enrichment performance. of particular interest is that, although, for irda fewer candidate genes are collapsed into gene symbols, the collapsed genes still produced a larger number of enrichment groups. for example, irda has  <dig>   <dig>  and  <dig> enrichment groups based on  <dig>   <dig>  and  <dig> collapsed genes for the brain, lymphoma, and prostate instances, respectively, while there are a fewer enrichment groups identified for the other three gene selectors based upon a larger number of collapsed genes . we note that the lung dataset is the most imbalanced  and that the number of samples  is also relatively larger compared to the other datasets. we found that irda does not perform well on the lung dataset when compared to cmim, although both filters display an almost identical classification performance on this particular dataset. from table  <dig> we see that there is a far greater amount of enrichment groups for leukemia data, independently of the underlying gene selector: gsea returned  <dig>   <dig>   <dig>  and  <dig> statistically significantly enriched gene sets for irda, cmim, mrmr, and fcbf. the details of all the enrichment groups and genes are provided in additional file 7: table s <dig> 

leuk. leukemia, lymph. lymphoma, prost. prostate

fdr < <dig> 



CONCLUSIONS
a new filter, irda, for identifying gene-expression candidate genes for phenotype prediction derived from high-throughput screening technologies is fully introduced in this paper. the filter is able to produce small sets of discriminative genes, either in form of a parsimony model or as a set of candidate genes, with an impact on better phenotype prediction. the output produced by irda meets the demands of a domain user, since a small number of candidate genes is the preferred basis to perform in vitro validation efficiently.

the effectiveness of irda was validated on eleven datasets, including seven well-known cancer benchmarks and four additional disease experiments. based on the transcriptomic profiling data, irda was compared to the three information theoretic filters  in terms of classification performance, stability indices, and the gene set enrichment analysis . according to the experimental results, we conclude that  parsimonious sets generated by irda have good and comparable classification performance;  candidate genes explored by irda dominate the sets produced by mrmr, cmim, and fcbf;  irda exhibits on average the best stability with the smallest variance;  there are more sets of statistically significant enrichment in genes selected by irda than in those discovered by mrmr, cmim, and fcbf. the performance results come at a price in terms of run-time. however, the gene selection is executed on all data sets within a few a seconds on standard desktop equipment. overall, we think that the new irda filter has the potential of identifying genes that might have an inferior relevance, but contribute strongly to interactions between genes. such genes, accompanied by other genes in a signature set, could have a measurable impact on phenotype distinction, which would not necessarily be seen at the level of expression data.

additional files
additional file  <dig> 
table s <dig>  data repositories. the file provides data repositories of seven cancer benchmarks summarised in table  <dig>  



additional file  <dig> 
table s <dig>  parsimonious gene sets of error performance. the file provides all the gene sets of four filters over eleven datasets based on generalisation error rate from tables  <dig> and  <dig>  



additional file  <dig> 
table s <dig>  parsimonious gene sets of auc performance. the file provides all the gene sets of four filters over eleven datasets based on auc measurement from tables  <dig> and  <dig>  



additional file  <dig> 
table s <dig>  parsimonious gene sets of mcc performance. the file provides all the gene sets of four filters over eleven datasets based on mcc measurement from tables  <dig> and  <dig>  



additional file  <dig> 
table s <dig>  candidate genes. the file provides all the candidate genes selected by four filters over eleven datasets to be evaluated in figs.  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig>  



additional file  <dig> 
table s <dig>  collapsed genes. the file is a supplement to table  <dig> to provide all the collapsed genes over six cancer benchmarks for gsea. 



additional file  <dig> 
table s <dig>  gsea results. the file provides the details of all the enrichment groups and genes summarised in table  <dig>  



abbreviations
htshigh-throughput screening

mrmrminimum-redundancy and maximum-relevance framework

cmimconditional mutual information maximization

fcbffast correlation-based filter

irdainterdependence with redundant-dependent analysis and aggregation scheme

aucarea under the receiver operating characteristic curve

mccmatthews correlation coefficient

knnthe k-nearest neighbours classification model

ma-knna wrapper based on mcc and auc in conjunction with the knn classifier

loocvleave-one-out cross-validation

irimbalance ratio

gbmglioblastomas

aoanaplastic oligodendrogliomas

cnscentral nervous system

allacute lymphoblastic leukemia

amlacute myeloid leukemia

mpmmalignant pleural mesothelioma

adcdadenocarcinoma

dlbcldiffuse large b-cell lymphoma

flfollicular lymphoma

mmmultiple myeloma

msmarfan syndrome

hivhiv infection

adneurodegeneration

gsogene expression omnibus

rwcrelative weighted consistency

fdrfalse discovery rate

gseagene set enrichment analysis

competing interests

the authors declare that they have no competing interests.

authors’ contributions

hml developed the algorithms, carried out the computational experiments, and drafted the manuscript. aaa participated in the design of the framework and revised the manuscript. kks conceived of the idea of feature subset selection for candidate genes, supervised the study, and revised & coordinated the manuscript. all authors read and approved the final manuscript.

the authors are grateful to the anonymous referees for their thorough reading of our manuscript and the valuable comments and suggestions that helped us to improve and to extend the presentation of our results.

hml would like to thank king-lian tung and kuo-fu lee for sponsoring and supporting his research.
