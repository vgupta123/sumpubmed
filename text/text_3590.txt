BACKGROUND
haplotypes can give useful information about patterns of inheritance for genomic regions. for each region, the probability of sharing founder genes through segregation in a recorded pedigree can be estimated based on the haplotypes, i.e. identity by descent  probability based on linkage . the probability of sharing the genes from a common ancestor before the recorded pedigree can also be estimated based on the haplotypes, i.e. ibd probability based on linkage disequilibrium  . those probabilities derived from the haplotypes are essential information for linkage mapping and association mapping.

since haplotypes would not be directly observed from genotypic data, they need to be inferred based on observed pedigree information and marker genotypes. this would often result in a large state space of possible haplotype configurations especially with general pedigrees and incomplete genotypic data for multiple markers. exact likelihood methods using pedigree peeling  <cit> , chromosome peeling  <cit>  or a combination of both algorithms often have problems dealing with the large state space and therefore have difficulties in finding the optimal haplotypes.

alternatively, combinatorial optimization algorithms can be used. these are able to deal with problems that are hard to solve in polynomial time . one strategy used in such algorithms is to search for haplotype configurations that require a minimum number of recombination events  <cit>  or no recombination events  <cit> . these approaches are rule-based and do not make any assumptions about genetic distances between markers. another approach is statistically based and would search for haplotype configurations with the highest likelihood, given all observed variables and known marker distances  <cit> .

a widely used statistical approach for haplotype reconstruction is simulated annealing   <cit>  which has been implemented in the linkage software, simwalk <dig>  <cit> . simwalk <dig> uses a random walk approach  <cit>  to find candidates and an annealing process to develop the consecutive solutions to reach the optimal haplotypes. simwalk <dig> constitutes a flexible and efficient algorithm for haplotyping and probably the only one used for a general complex pedigree with incomplete genotypes. however, it needs a very large number of sequential evaluations and it is not always guaranteed that the most likely solutions are found within the arbitrarily determined number of evaluations.

evolutionary algorithms   <cit>  constitute an efficient tool for solving combinatorial optimization problems. a number of parallel solutions are respectively updated  by changing the variables within each solution , or recombining them from different solutions , and the most favorable solutions are selected . compared to sa, ea may be competitive in efficiently finding an optimal solution. an important advantage of ea is its potential to parallellise computations because the algorithm can be divided across multiple cpus. this would substantially reduce computing time. moreover, the search mechanism in ea can make it easier to diagnose convergence compared to that in simwalk <dig>  it is well known that ea can be easily designed and parameterised for a specific problem, and standard values for ea-parameters usually give reasonably good results  <cit> .

in addition, jointly updating the set of dependent variables can increase the computational efficiency although not all sets of dependent variables may be found. it is noted that simwalk <dig> attempts to update multiple variables together, but the set that is updated is randomly chosen . because dependent variables are not necessarily within the same set, the acceptance rates are generally low.

an evolutionary algorithm has not been implemented before in statistical approaches for haplotyping. the aim of this study is to investigate the use of an evolutionary algorithm and joint updating strategy for haplotyping, and compare its efficiency with simwalk <dig> 

RESULTS
likelihood pattern with simulated data
‡number of evaluations of the likelihood function, * <dig> is from the default annealing schedule of simwalk2

¶computing time for simwalk <dig>  ea <dig> and ea <dig> 

when using incomplete genotypic data, the likelihood values are not yet converged after  <dig>  evaluations. after  <dig>  evaluations, the likelihood values appear to be fairly close to the global maximum where the value for ea <dig> is the highest among the three methods . after  <dig> , <dig> evaluations, the likelihood reaches stable values with apparent convergence where the value for ea <dig> is the highest and the value for simwalk <dig> is the worst. it is noted that the convergence patterns and the evaluation numbers are different between complete and incomplete genotypes.

likelihood pattern with real data
¶computing time for simwalk <dig>  ea <dig> and ea <dig> 

computing time for a fixed number of evaluations with  <dig> processors
the relative computing time between simwalk <dig>  ea <dig> and ea <dig> with  <dig> processors varies depending on data structure. when using simulated data with complete genotypes, the computing time completing  <dig> , <dig> evaluations for ea <dig> and ea <dig> was  <dig>  times and  <dig>  times faster than simwalk <dig> . however, when using simulated data with incomplete genotypes, the relative computing efficiency for ea was decreased; ea <dig> was  <dig> times and ea <dig> was  <dig>  times faster than simwalk <dig> . when using the real data, the computing time completing  <dig> , <dig> evaluations for ea  <dig> and ea <dig> was  <dig>  times and  <dig>  times faster than simwalk <dig> . the larger advantage for ea for a larger data set is probably due to the fact that the proportion of transferring time over whole computing time increases when using a data set of small size with a few genotypes, e.g. simulated data with incomplete genotypes.

convergence diagnosis and computational efficiency
discussion
the ea <dig> and ea <dig> reach the global or nearly global maximum quicker than simwalk <dig> both with simulated and real data. this was probably due to the fact that ea <dig> and ea <dig> used a number of parallel configurations which apparently is a more efficient searching mechanism, resulting in a wider range of variables updated during the cycles. using an efficient joint updating strategy combined with the random walk, ea <dig> significantly outperformed simwalk <dig> with a simultaneous use of multiple processors.

convergence for ea <dig>  or simwalk <dig> can be assessed by comparison of the likelihood values between different parts to check if the likelihood reaches a stable value . when the likelihood value is not converged, simwalk <dig>  as currently implemented, requires another complete run with a new annealing schedule which would need to be longer than the previous run. therefore, simwalk <dig> may need multiple runs with a large number of evaluations. however, in the ea, the convergence of the likelihood value can be checked at any time and any point during the run without any rescheduling or rerun. for convergence diagnosis shown in figure  <dig>  simwalk <dig> needed to run more than  <dig> times because of rescheduling of the annealing process for each point . however, ea <dig> and ea <dig> needed only a single run for the same convergence assessment.

the ea population size was arbitrarily set at n =  <dig>  the chance of operating ea-recombination or ea-mutation was randomly equal, i.e. the probability of ea-recombination  was ~ <dig> . different ea-parameters did not dramatically affect the results unless the values were extreme, e.g. outside the range  <dig> –  <dig> for n or outside the range  <dig> ~ <dig>  for the probability of ea-recombination . if there are hundreds of cpus available , then n should not be less than the number of cpus used, to maximize the efficiency of parallel computing. such large values for n for large scale parallel computing should be tested with regard to optimal performance. however, it is possible to set n considerably higher than optimal, and computational efficiency is still increased.

in the real data, we used  <dig> microsatellites on the same chromosome positioned at  <dig> cm intervals on average. however, a much larger number of markers is expected to be used soon, e.g. thousands of single nucleotide polymorphisms  at << ~ <dig> cm intervals on a single chromosome. the ea with hundreds of cpus may be able to parallelize and solve the increased size of the problem within a reasonable time. haplotypes for dense markers might be resolved using population ld without pedigree information  <cit> . however, when population ld is not significant over the average marker distance, the methods may perform poorly  <cit> . therefore, when ld is not sufficient or one may not be sure whether there is enough ld, our approach will be an efficient tool for haplotyping for most types of data.

when using multiple cpus and multiple machines, data transfer between machines took a large part of the overall computing time. therefore, the ea was slightly modified in order to save on transfer time. there were n nodes for ea-members parallelised between machines. we made each node have two ea-members . ea-mutation and ea-recombination were carried out in each node, and the solutions were evolved. one set of solutions in each node was moved to the next node  every n cycles, therefore a complete evolutionary mechanism was performed with periodic isolation of islands . the number for n was chosen as n = total number of iterations/k, which resulted in k transfers between nodes in an analysis. it is noted that n should be a sufficient number for ea-mutation and ea-recombination within each node, and k should be a sufficient number for communicating between the nodes. in this study, we chose a large number k =  <dig> which would be sufficient for n as well because we usually used the total number of iterations ≥ number of meiosis × number of markers ×  <dig>  such a strategy becomes more important when using many different machines.

we used data sets having complex relationships with incomplete genotypes. such data sets are quite common in natural and outbred populations  <cit> . complex pedigrees with incomplete genotypes will generally generate a too large state space for haplotying, which cannot be handled by exact methods  <cit> . however, simwalk <dig> has been successfully and widely used for such data sets although the size of data should be small or moderate. we showed that our approach could handle such data sets, and the computational efficiency for our approach was much higher than that for simwalk <dig> 

CONCLUSIONS
the difference between ea and sa is mainly determined by the parameters used in their search mechanism, which affect the number of configurations considered in the cycles and updating strategies, and how information from different solutions is used to generate new candidate solutions. the ea algorithm has a substantial advantage in convergence assessment and parallel computing, which would much increase the efficiency of haplotype reconstruction . moreover, with the joint updating scheme, ea <dig> significantly outperformed simwalk <dig> . with more cpu, the computational efficiency of ea <dig> would be increased. in addition, our implementation of ea <dig> and ea <dig> for this application is likely to leave much room for increased performance, given the wide range of structural and parameterization strategies that could be invoked. further study would be required to investigate such potential.

