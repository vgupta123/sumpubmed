BACKGROUND
to cope with massive sequence data generated by next-generation sequencing machines, a highly scalable and efficient parallel solution for fundamental bioinformatic applications is important  <cit> . with the help of high performance computing, cloud computing  <cit> , and many-cores in gpu  <cit> , successful scalable examples have been seen in many embarrassingly parallel applications: sequence alignment  <cit> , snp searching  <cit> , expression analysis  <cit> , etc. however, for tightly coupled graph related problems, such as genome assembly, a scalable solution is a still a big challenge  <cit> .

state-of-the-art trials on parallel assemblers include abyss  <cit> , ray  <cit> , pasha  <cit> , and yaga  <cit> . abyss adopts the traditional de bruijn graph data structure proposed by pevzner et. al.  <cit>  and follows the similar assembly strategy as euler sr  <cit>  and velvet  <cit> . the parallelization is achieved by distributing k-mers to multi-servers to build a distributed de bruijn graph, and error removal and graph reduction are implemented over mpi communication primitives. ray extends k-mers  into contigs with a heuristical greedy strategy by measuring the overlapping level of reads in both direction. based on the observation that the time consuming part of genome assembly are generating and distributing k-mers, constructing and simplifying the distributed de bruijn graph, pasha concentrates its effort on parallelizing these two stages to improve its efficiency. however, pasha allows only single process for each unanimous path, and this limits its degree of parallelism. in their experiments, abyss and pasha take about  <dig> hours and  <dig> hours to assembly the yoruban male genome with a coverage of 42x.

to avoid merging k-mers on two different servers, which can result in too many small inter-process messages and the communication latency, yaga constructs a distributed de bruijn graph by maintaining edge tuples in a community of servers. reducible edges belonging to one unanimous path are grouped into one server using a list rank algorithm  <cit> , then these unanimous paths are reduced locally on separated servers. the complexity of yaga is bounded by o computing time, o communication volume, and o) communication rounds, where n is the number of nucleotides in all reads, and p denotes the number of processors. due to the fact that the recursive list ranking algorithm used in yaga has a memory usage of o, this will use large amount of memory and cause low efficiency.

our previous work  <cit>  tries to avoid access collision of merging two neighbor edges. in this work, 1-step bi-directed graph and a computational model named as swap are proposed for edge merging operation. in its experiments, the prototype of edge merging algorithm using swap can scale to  <dig> cores on both yeast and c.elegans dataset. however this exploratory work only focuses on the edge merging operation of genome assembly, some other important problems are not addressed, for example, contig extension, complexity analysis etc.

the scalability of previous assemblers is affected by the computational interdependence on merging k-mers/edges in unanimous paths. sequential assemblers, for example velvet and soapdenovo, process each path sequentially. parallel assemblers can process several paths in parallel, however k-mers/edges sharing one path are merged one by one. swap-assembler resolves the computational interdependence on merging edges sharing the same path with msg. for each path, at most half of its edges can be merged concurrently in each round, and merging multiple edges on the same path can be done in parallel using swap computational framework. in figure  <dig>  the parallel strategy of swap-assembler is compared with other assemblers using an example of two linked paths, we can see that a deeper parallelism on edge merging can be achieved by swap-assembler.

in this paper, we present a highly scalable and efficient genome assembler named as swap-assembler, which can scale to thousands of cores on processing massive sequencing data such as yanhuang . swap-assembler includes five fully parallelized steps: input parallelization, graph construction, graph cleaning, graph reduction and contig extension. compared with our previous work, two fundamental improvements have been made for graph reduction. firstly msg is presented as a comprehensive mathematical abstraction for genome assembly. using msg and semi-group theory, the computational interdependence on merging edges is resolved. secondly, we have developed a scalable computational framework for swap, this framework triggers the parallel computation of all operations with no interference. in this paper, complexity of this framework and swap-assembler is also analyzed and proved in detail. in addition, two steps in swap-assembler are used to improve the quality of contigs. one is graph cleaning, which adopts the traditional strategy of removing k-molecules and edges with low frequency, and the other one is contig extension, which resolves special edges and some cross nodes using a heuristic method. experimental results show that swap-assembler can scale up to  <dig> cores on yanhuang dataset using only  <dig> minutes, which is the fastest compared with other assemblers, such as abyss, ray and pasha. conitg evaluation results confirm that swap-assembler generates good results on n <dig> size with lowest error rate for s. aureus and r. sphaeroides datasets. when processing larger datasets  without using external error correction tools, swap-assembler generates the longest n <dig> contig sizes of  <dig> bp and  <dig> bp for these two datasets.

methods
in this section, our method for genome assembly towards thousands of cores is presented. we first abstract the genome assembly problem with msg. generating longer sequences  from shorter sequences corresponds to merging semi-extended edges to full-extended edges in msg. in addition, computational interdependence of edge merging is resolved by introducing a semi-group over a closed edge set es v  <dig> in msg. the edge set es v  <dig> is proved to be a semi-group with respect to edge merging operation. according to the associativity law of semi-group, the final results will be the same as long as all edges have been merged regardless of the merging order, thus these edge merging operations can be computed in parallel.

in order to maximally utilize the potential parallelism resolved by msg, a scalable swap computational framework is developed. as one edge may be accessed by two merging operations in two different processes at the same time, a lock-computing-unlock mechanism introduced in  <cit>  is adopted for avoiding the conflict. for the problems which can be abstracted with semi-group, the corresponding operations can be done in parallel, and swap computational framework can achieve linearly scale up for these problems.

based on msg and swap computational framework, swap-assembler is developed with five steps, including input parallelization, graph construction, graph cleaning, graph reduction, and contig extension. in the following, we first present msg and the swap computational framework, then details of swap-assembler's five steps will be followed.

mathematical formulation of genome assembly using msg
given a biological genome sample with reference sequence w ∈ ℕg , where n = {a, t, c, g}, g = |w|, a large number of short sequences called reads, s = {s <dig>  s <dig>  ..., sh}, can be generated from the sequencing machines. genome assembly is the process of reconstructing the reference genome sequence from these reads. unfortunately, the genome assembly problem of finding the shortest string with all reads as its substring falls into a np-hard problem  <cit> .

finding the original sequence from all possible euler paths cannot be solved in polynomial time  <cit> . in real cases, gaps and branches caused by uneven coverage, erroneous reads and repeats prevent obtaining full length genome, and a set of shorter genome sequences called contigs are generated by merging unanimous paths instead. our method focuses on finding a mathematical and highly scalable solution for the following standard genome assembly  problem, which is also illustrated in figure  <dig> 

problem of standard genome assembly 

input: given a set of reads without errors s = {s <dig>  s <dig>  ..., sh}

output: a set of contigs c = {c <dig>  c <dig>  ..., cw}

requirement: each contig maps to an unanimous path in the de bruijn graph constructed from the set of reads s.

preliminaries
we first define some variables. let s ∈ ℕl be a dna sequence of length l. any substring derived from s with length k, is called a k-mer of s, and it is denoted by α = ss . . . s,  <dig> ≤ j < l − k +  <dig>  the set of k-mers of a given string s can be written as z, where k must be odd. the reverse complement of a k-mer α, denoted by α′, is obtained by reversing α and complementing each base by the following bijection of m, m : {a → t, t→a, c→g, g → c}. note that α=α′′.

a k-molecule α^ is a pair of complementary k-mers {α, α'}. let .> be the partial ordering relation between the strings of equal length, and α.>β indicates that α is lexicographically larger than β. we designate the lexicographically larger one of two complementary k-mers as the positive k-mer, denoted as α+, and the smaller one as the negative k-mer, denoted as α−, where α+.>α-. we choose the positive k-mer α+ as representative k-mer for k-molecule {α, α'}, denoted as α+, implying that α^=α+={α+,α-}={α,α′}. the relationship between k-mer and k-molecule is illustrated in figure  <dig>  the set of all k-molecules of a given string s is known as k-spectrum of s, and it can be written as s. noted that s = s.

notation suf  , respectively) is used to denote the length l suffix  of string a. the symbol ○ is introduced to denote the concatenation operation between two strings. for example, if s <dig> = "abc", s <dig> = "def ", then s <dig> °s <dig> = "abcdef ". the number of edges attached to k-molecule αˆ is denoted as degree. all notations are listed in table  <dig> 

α^=α+={α+,α-}
1-step bi-directed graph
definition 1: 1-step bi-directed graph. the 1-step bi-directed de bruijn graph of order k for a given string s can be presented as,

  gk1={vs,es1} 

in the rest of the paper, 1-step bi-directed de bruijn graph of order k is abbreviated as 1-step bi-directed graph. in equation , the vertex set vs is the k-spectrum of s,

  vs=s 

and the 1-step bi-directed edge set es <dig> is defined as follows,

  es1={eαβ1=∀α^,β^∈s,suf=preˆ∈∨z))} 

equations  declares that any two overlapped k-molecules can be connected with one 1-step bi-directed edge when they are consecutive in sequence s or the complementary sequence s'. here dα denotes the direction of k-mer α, if α = α+, dα ='+', otherwise dα = '-'. cαβ <dig> is the content or label of the edge, and is initialized with β, that is cαβ1=β, and we have suf=β.

lemma  <dig>  given two k-molecules α^,β^∈s, there are four possible connections, and for each type of connection exactly two equivalent 1-step bi-directed edge representations exist,

 <dig> eα+β+1=,eβ-α-1=

 <dig> eα+β-1=,eβ+α-1=

 <dig> eα-β+1=,eβ-α+1=

 <dig> eα-β-1=,eβ+α+1=

in each type of connection, the first bi-directed edge representation and the second one are equivalent. the first bi-directed edge is associated with k-molecule α^, and the second one is associated with β^. figure  <dig> illustrates all four possible connections. for example in figure 4-, a positive k-mer "tag" points to positive k-mer "agt" with a label "a", and the corresponding edge is etagagt1=.

given a set of reads s = {s <dig>  s <dig>  . . . , sh}, a 1-step bi-directed graph derived from s with order k is,

  gk1={vs,es1}={∪1≤i≤hvsi, ∪1≤i≤hesi1} 

each read si corresponds to a path in gk <dig>  and read si can be recovered by concatenating -prefix of the first k-molecule and the edge labels on the path consisted by s. as an example, an 1-step bi-directed de bruijn graph derived from s = {"t agt cg", "agt cga", "t cgagg"} is plotted in figure  <dig> 

multi-step bi-directed graph and its properties
definition 2: edge merging operation. given two 1-step bi-directed edges eαβ1= and eβγ1= in a 1-step bi-directed graph, if eαβ <dig> dβ=eβγ <dig> dβ and degree =  <dig>  we can obtain a 2-step bi-directed edge eαγ2= by merging edges eαβ <dig> and eβγ <dig>  cαγ2=cαβ1∘cβγ <dig>  using symbol ⊗ to denote edge merging operation between two bi-directed edges attached to the same k-molecule with the same direction, and the 2-step bi-directed edge is written as,

  eαβ1⊗eβγ1=eαγ2oreγβ1⊗eβα1=eγα <dig> 

two edges eαγ <dig> and eγα <dig> in equation  are equivalent, indicating it is same to apply edge merging operation on eαβ <dig> and eβγ <dig>  and to apply edge merging operation on eγβ <dig> and eβα <dig>  figure  <dig> shows an example on edge merging operation.

zero edge  <dig> is defined to indicate all non-existing bi-directed edges. note that 0⊗eαβx= <dig> eαβx⊗0= <dig>  a z-step bi-directed edge can be obtained by,

  eαγz=eαβx⊗eβγy,if∃β,eαβx≠ <dig> eβγy≠ <dig> z=x+yeαβx.dβ=eβγy.dβ,degree=20otherwise 

definition 3: multi-step bi-directed graph. a msg derived from a read set s = {s <dig>  s <dig>  . . . , sh}, is written as,

  gk={vs,es}={⋃1≤i≤hvsi, ⋃1≤j≤g} 

where g is the length of reference sequence w, esij+{eαβj|∀α^,β^∈s}. a msg is obtained through edge merging operations.

given an x-step bi-directed edge eαβx=, if there exists edge eγαy or eβγz satisfying eγαy⊗eαβx≠ <dig> or eαβx⊗eβγz≠ <dig>  then we call edge eαβx as a semi-extended edge, and the corresponding k-molecule αˆ or βˆ as semi-extended k-molecule. if eαβx cannot be extended by any edge, this edge is called as full-extended edge, and k-molecule α^ and β^ are full-extended k-molecules. in figure  <dig> and  <dig>  semi-extended k-molecule and full-extended k-molecule are plotted with different colors , semi-extended edge and full-extended edge are drawn with different lines .

property  <dig>  if the set of full-extended edges in the msg defined in equation  <dig> is denoted as es*, then the set of labels on all edges in es* can be written as,

  ls*={cαβx|eαβx=,eαβx∈es*} 

and we have ls*=c, c is the set of contigs. the proof is presented in appendix  <dig> 

property  <dig>  edge merging operation ⊗ over the multi-step bi-directed edge set es∨ <dig> is associative, and q is a semigroup. the proof is presented in appendix  <dig> 

the key property of 1-step bi-directed graph gk <dig> is that each read s corresponds to a path starting from the first k-molecule of s and ending at the last k-molecule. similarly, each chromosome can also be regarded as a path. however because of sequencing gaps, read errors, and repeats in the set of reads, chromosome will be broken into pieces, or contigs. within a msg, each contig corresponds to one full-extended edge in gk, and this has been presented and proved in property  <dig>  property  <dig> ensures that the edge merging operation ⊗ over the set of multi-step bi-directed edges has formed a semi-group, and this connects the standard genome assembly  problem with edge merging operations in semi-group. according to the associativity law of semi-group, the final full-extended edges or contigs will be the same as long as all edges have been merged regardless of the merging order, thus these edge merging operations can be computed in parallel. finally in order to reconstruct the genome with a large set of contigs, we need to merge all semi-extended edges into full-extended edges in semi-group q.

swap computational framework
the lock-computing-unlock mechanism of swap was first introduced in our previous work  <cit> , where no implementation details and complexity analysis is given. in this section, we present the mathematical description of the problems which can be solved by swap, then a scalable computational framework for swap model and its programming interface are presented. its complexity and scalability is analyzed in appendix  <dig> 

definition 4: small world of operations. semi-group sg is defined on set a with an associative operation r : a × a → a. r is used to denote the associative operation on ai and aj, ai, aj ∈ a. the elements ai and aj, together with the operation r are grouped as a small world of the operation r. we denote this small world as , and  = {r, ai, aj}.

activity act  are given on a semi-group sg as the computational works performed by a graph algorithm, where operation set σ is a subset of r.

in real application, an operation corresponds to a basic operation of a given algorithm. for example, for msg based genome assembly application, an operation can be defined as edge merging. for topological sorting, re-ordering a pair of vertices can be defined as an operation.

for any two small worlds , , where a <dig> ≠ b <dig>  a <dig> i= b <dig>  a <dig> ≠ b <dig>  a <dig> ≠ b <dig>  the corresponding operations r and r, can be computed independently, thus, there exists potential parallelism in computing activity induced from the semi-group sg. we use swap for such parallel computing. the basic schedule of swap is lock-computing-unlock. for an operation r in σ, the three-steps of swap are listed below:

 <dig> lock action is applied to lock r's small world .

 <dig> computing is performed for operation r, and the values of a, b are updated accordingly. in msg, this corresponds to merging two edges.

 <dig> unlock action is triggered to release operation r!s small world .

in swap computational framework, all operations σ in activity act  can be distributed among a group of processes. each process needs to fetch related elements, for example a and b, to compute operation r. at the same time, this process also has to cooperate with other processes for sending or updating local variables. each process should have two threads, one is swap thread, which performs computing tasks using the three-steps schedule of swap, and the other is service thread, which listens and replies to remote processes. in the implementation of our framework, we avoid multi-threads technology by using nonblocking communication and finite-state machine in each process to ensure its efficiency.

the activity act  on set a with operations in σ can be treated as a graph g with σ as its vertices and a as its edges. adjacent list is used to store the graph g, and a hash function hashf un is used to distribute the set σ into p subset for p processes, where σ= ⋃i=0p-1σi, and ai is associated with σi. note that act= ⋃i=0p-1acti, which is illustrated in figure  <dig>  in appendix  <dig>  the pseudocodes of swap thread and service thread are demonstrated in algorithm  <dig> and algorithm  <dig>  respectively.

algorithm  <dig> describes the three-steps of swap computational framework, and algorithm  <dig> on remote processers can cooperate with algorithm  <dig> for running this schedule. the message functions, internal functions, and user-defined functions in algorithm  <dig> and algorithm  <dig> are listed in table  <dig>  where user-defined functions can be redefined for user-specific computational problems.

similar to csma/ca in  <dig>  protocol  <cit> , algorithm  <dig> adapts random back-off algorithm to avoid lock collision. a variety of backoff algorithms can be used, without loss of generality, binary exponential backoff  <cit>  is used in swap thread. note that all collided operations in σi share only one binary backoff, so the cost can be ignored as long as the number of relations in σi is huge.

the complexity and scalability analysis for swap computational framework are presented in appendix  <dig>  when the number of processes is less than the number of operations in σ, which is true for most cases, equation  shows that swap computational framework can linearly scale up with the increasing number of cores. when the number of processes is larger than the number of operations, according to equation  the running time will be dominated by the communication round, which is bounded by log, where dmax is the diameter of graph g.

implementation of swap-assembler
based on msg and swap computational framework, swap-assembler consists with five steps, including input parallelization, graph construction, graph cleaning, graph reduction, and contig extension. complexity analysis of swap-assembler are presented in the end of this section.

input parallelization
as the size of data generated by next generation sequencing technology generally has hundreds of giga bytes, loading these data with one process costs hours to finish  <cit> . similar to ray  <cit>  and yaga  <cit> , we use input parallelization to speedup the loading process. given input reads with n nucleotides from a genome of size g, we divide the input file equally into p virtual data block, p is the number of processes. each process reads the data located in its virtual data block only once. the computational complexity of this step is bounded by o. for e.coli dataset of  <dig> g bytes, swap-assembler loads the data into memory in  <dig> seconds with  <dig> cores while yaga uses  <dig>  seconds  <cit> , and for yanhuang dataset swap-assembler loads the data in  <dig> minutes while ray costs  <dig> hour and  <dig> minutes.

graph construction
this step aims to construct a 1-step bi-directed graph gk1={vs,es1}, where vs and es <dig> are k-molecule set and 1-step bi-directed edge set. in this step, input sequences are broken into overlapping k-molecules by sliding a window of length k along the input sequence. a k-molecule can have up to eight edges, and each edge corresponds to a possible one-base extension, {a, c, g, t} in either direction. the adjacent k-molecule can be easily obtained by adding the base extension to the source k-molecule. the generated graph has o k-molecules and o bi-directed edges distributed among p processors. graph construction of 1-step bi-directed graph can be achieved in o parallel computing time, and o parallel communication volume.

an improvement to our previous work  <cit>  is that the time usage on graph construction is overlapped with the previous step. as cpu computation and network communication can be performed when only partial data are loaded from the first step, they can be overlapped and combined as a pipeline. computation and communication time used in this step are hid behind the time used on disk i/o in previous step.

graph cleaning
this step cleans the erroneous k-molecules, based on the assumption that the erroneous k-mers have lower frequency compared with the correct ones  <cit> . assuming that the errors are random, we identify the k-molecules with low frequency as erroneous k-molecules, and delete them from the vertex set. swap-assembler also removes all edges with low frequency in the 1-step bi-directed graph, and the k-molecules without any attached edges. the frequency threshold can be set by users, or our method will calculate it automatically based on the average coverage of k-molecules. in our case, we prefer  <dig> ~ 10% of the average coverage as the threshold depending on the species.

all the operations in this step can be finished in o parallel computation time, and about  <dig> ~ 80% of the k-molecules can be removed from our graph.

graph reduction
in order to recover contigs, all semi-extended edges in msg need to be merged into full-extended edges. this task can be defined as edge merging computing activity and denoted as act , where the edge merging operation set σ is,

  σ={|eβαv⊗eαγv≠ <dig> eβαu,eαγv∈es} 

in which eβαu indicates an u-step bi-directed edge connecting two vertices β and α. all semi-extended edges of es will be merged into full-extended edges finally.

in order to compute edge merging operations in σi using our swap computational framework, two user-defined functions in table  <dig> are described as algorithm  <dig> and algorithm  <dig> in appendix  <dig>  for each process, the edge merging step has a computing complexity of o, communication volume of nlog)p, and communication round of o)). the proposed methods has much less computation round of o) than yaga with o2)  <cit> . the detailed complexity analysis is provided in appendix  <dig> 

contig extension
in order to extend the length of contigs while maintaining as less errors as possible, three types of special edges and two type of special vertices are processed in our method.

the first type of special edge is tip edge, which is connected with an terminal vertex and has a length less than k, where k is the k-mer size. these tip edges are deleted from the graph. the second type is self-loop edge, whose beginning vertex and terminal vertex are same. if this vertex has another edge which can be merged with this self-loop edge, they will be merged, otherwise it will be removed. the last type is multiple edge, whose two vertices are directly connected by two different edges. in this case the edge with lower coverage will be removed.

in addition, processing two special vertices can help further improve the quality of contigs. the first is cross vertex shown in figure 8-, which has more than two edges on both sides. for each cross vertex, we sort all its edges according to their coverage. when the coverage difference between the two edges is less than 20%, then these two edges are merged as long as they can be merged regardless of other edges. the second vertex is virtual cross vertex shown in figure 8-. we treat edge e <dig> with its two end vertices as one virtual vertex a* and a* has more than two edges on both sides. all its edges are ranked according to their coverage. when the coverage difference between two edges on different nodes is less than 20%, these two edges will be merged with the edge e <dig> regardless of other edges. by processing these two special vertices using the heuristic method, we can partly resolve some of the repeats satisfying our strict conditions at the cost of introducing errors and mismatches into contigs occasionally.

the graph reduction step and contig extension step need to be iterated in a constant number of rounds to extend full-extended edges or stop when no special edges and special vertices can be found. the number of errors and mismatches introduced in contigs can be controlled by the percentage of special edges and vertices processed in contig extension stage. in our method, we process all edges and vertices aiming at obtaining longer contigs. the computing complexity for contig extension step will be bounded by o. as the graph shrinks greatly after graph reduction and contig extension step, all the remaining edges are treated as contigs. the complexity of swap-assembler is dominated by graph reduction step, which is bounded by o parallel computing time, on⋅log)p communication volume, and o)) communication round. according to complexity analysis results of graph reduction step in equation , when the number of processors is less than the length of longest path dmax, the speedup of swap-assembler can be calculated as follows,

  speedup=nruntime=pbllog)+rs+ <dig> 

equation  indicates that, for a given genome with fixed length g, the speedup is proportional to the number of processors; while for a given number of processors p, the speedup is inversely proportional to logarithm of the logarithm of the genome size g. however when the number of processors is larger than the length of longest path dmax, the running time will be bounded by the number of communication round, which is presented in equation  in appendix  <dig>  in either situation, we can conclude that the scalability or the optimal number of cores will increase with larger genomes.

RESULTS
swap-assembler is a highly scalable and efficient genome assembler using multi-step bi-directed graph . in this section, we perform several computational experiments to evaluate the scalability and assembly quality of swap-assembler. in the experiments, tianhe 1a  <cit>  is used as the high performance cluster.  <dig> computing nodes are allocated for the experiment with  <dig> cores and 24gb memory on each node. by comparing with several state-of-the-art sequential and parallel assemblers, such as velvet  <cit> , soapdenovo  <cit> , pasha  <cit> , abyss  <cit>  and ray  <cit> , we evaluate the scalability, quality of contigs in terms of n <dig>  error rate and coverage for swap-assembler.

experimental data
five datasets in table  <dig> are selected for the experiments. s. aureus, r. sphaeroides and human chromosome  <dig>  datasets are taken from gage project  <cit> , fish dataset is downloaded from the assemblathon  <dig>  <cit> , and yanhuang dataset  <cit>  is provided by bgi  <cit> .

s. aureus
r. sphaeroides
scalability evaluation
the scalability of our method is first evaluated on a share memory machine with  <dig> cores and 1t memory. five other assemblers including velvet, soapdenovo, pasha, abyss and ray, are included for comparison. only the first three small datasets in table  <dig> are used in this test due to the memory limitation. the results are presented in table  <dig>  and the corresponding figures are plotted in figure  <dig>  according to table  <dig>  swap-assembler has the lowest running time on all three datasets for  <dig> cores and  <dig> cores, and soapdenovo has the lowest time usage on  <dig> cores and  <dig> cores. according to figure  <dig>  soapdenovo, ray and swap-assembler can scale to  <dig> cores, however pasha and abyss can only scale to  <dig> cores. figure  <dig> also shows that ray and swap-assembler can achieve nearly linear speedup and swap-assembler is more efficient than ray.

s. aureus
r. sphaeroides


the time usage for each step of swap-assembler on the share memory machine is also presented in table  <dig> and figure  <dig>  the input parallelization step is overlapped with graph construction, thus we treat these two steps as one in this experiment. according to table  <dig>  for all three datasets the most time-consuming step is graph reduction, and the fastest steps are graph cleaning and contig extension. figure  <dig> shows that input parallelization & graph construction, graph cleaning and graph reduction can achieve nearly linear speedup when the number of cores increases from  <dig> to  <dig> cores, whereas the contig extension step does not benefit as much as other steps.



to further evaluate the scalability of swap-assembler from  <dig> to  <dig> cores on tianhe 1a, we have compared our method with two parallel assemblers, abyss and ray, and the results are included in table  <dig>  according to table  <dig> swap-assembler is  <dig> times and  <dig> times faster than abyss and ray for  <dig> cores on the s. aureus dataset. on the same dataset, abyss and ray cannot gain any speedup beyond  <dig> and  <dig> cores, respectively. however swap-assembler scales up to  <dig> cores. for the r. sphaeroides dataset, abyss, ray, and swap-assembler can scale up to  <dig>   <dig>  and  <dig> cores, respectively.

 <dig> this table records the time usage on assembling all five datasets with different number of cores, and the length of k-mer is set to be  <dig>  for abyss and ray, the time is recorded until contigs are generated.

 <dig> − denotes assembler with this parameter has not been tested.

 <dig> ∗ denotes assembler with this parameter has run out of memory.

 <dig> + denotes assembler with this parameter has run out of time, the time limit is  <dig> hours.

for three larger datasets, table  <dig> shows that scalability of swap-assembler is also better than the other two methods. on hg <dig> dataset, swap-assembler is  <dig> times faster than abyss, and  <dig> times faster than ray when using  <dig> cores. similar to the results on r. sphaeroides dataset, three assemblers still hold their turning point of scalability at  <dig>   <dig> and  <dig> cores, respectively. fish and yanhuang dataset cannot be assembled by abyss and ray in  <dig> hours, so their running times are not recorded in table  <dig>  for  <dig> cores, swap-assembler assembles fish dataset in  <dig> minutes, while it takes  <dig> minutes with  <dig> cores to assemble the yanhuang dataset. the speedup curves of swap-assembler on processing five datasets are shown in figure  <dig>  it shows that the speedup of assembling two small datasets have a turning point at  <dig> cores, and linear speedup to  <dig> cores is achieved for other three larger datasets. swap-assembler can still benefits from the increasing cores up to  <dig> cores on processing yanhuang dataset.

memory footprint is a bottleneck for assembling large genomes, and parallel assemblers is a solution for large genome assembly by using more memory on the computational nodes. for our case, fish and yanhuang genome assembly needs  <dig> t bytes and  <dig> t bytes memory, respectively. as in tianhe 1a each server has 24g memory, fish genomes cannot be assembled on a cluster with  <dig> servers. the same reasoning applies to yanhuang dataset.

swap-assembler has better scalability compared with ray and abyss due to two important improvements. firstly, computational interdependence of edge merging operations on one single unanimous path is resolved by msg. secondly, swap computational framework can trigger parallel computation of all operations without interference, and the communication latency is hidden by improving the computing throughput. ray and abyss cannot merge the k-mers in a single linear chain in parallel, and pasha can only parallelize the k-mer merging work on different chains, which limits their degree of parallelism.

assembly quality assessment
this part evaluate the assembly quality of swap-assembler. to be compatible with the comparison results from gage, we follow the error correction method of gage. allpath-lg  <cit>  and quake  <cit>  are used to correct errors for s. aureus and r. sphaeriodes datasets. the corrected reads are used as the input to abyss, ray and swap-assembler. in addition, two other sequential genome assemblers, velvet and soapdenovo, are selected in this experiment for comparison, and a machine with 1tb memory is used. the k-mer size for all assemblers varies between  <dig> and  <dig>  and best assembly results from the experiments of different k-mer sizes for each assembler are reported in table  <dig> and table  <dig> 

s. aureus
we also analyzed the contig statistics for three larger datasets and the results are presented in table  <dig>  because these datasets do not have a standard reference set and the original script provided by gage requires a reference set, we wrote a script to analyze the assembly results using the number of contigs, n <dig> size, max length of contigs and bases in the contigs for evaluation. the original data of three datasets are directly processed by five assemblers with a fixed k-mer size of  <dig>  according to table  <dig>  the n <dig> size of contigs generated by swap-assembler is longest for all three datasets. for fish and yanhuang datasets, swap-assembler also performs best in the number of contigs and max length of contigs. however for swap-assembler on hg <dig> dataset, whose reads are extracted from the human dataset by mapping the human chromosome  <dig>  the number of contigs, max length of contigs and bases in contigs have a rank of second, third, and second, respectively. swap-assembler has a best n <dig> size for all datasets. this is because it has efficient graph cleaning and contig extension steps, which can handle sequencing errors efficiently. four other assemblers, without the help from external tools on error correction, are affected by the quality of input reads on larger datasets.

in conclusion, swap-assembler is a highly scalable and efficient genome assembler. the evaluation shows that our assembler can scales up to  <dig> cores, which is much better than other parallel assemblers, and the quality of contigs generated by swap-assembler is the best in terms of error rate for several small datasets and n <dig> size for two larger data sets.

CONCLUSIONS
in this paper, swap-assembler, a fast and efficient genome assembler scaling up to thousands of cores, is presented. in swap-assembler, two fundamental improvements are crucial for its scalability. firstly, msg is presented as a comprehensive mathematical abstraction for genome assembly. with msg the computational interdependence is resolved. secondly, swap computational framework triggers the parallel computation of all operations without interference. two additional steps are included to improve the quality of contigs. one is graph cleaning, which adopts the traditional methods of removing k-molecules and edges with low frequency, and the other is contig extension, which resolves special edges and some cross nodes with a heuristic method. results show that swap-assembler can scale up to  <dig> cores on yanhuang dataset using only  <dig> minutes, which is the best compared to other parallel assemblers, such as abyss and ray. conitg evaluation results confirm that swap-assembler can generate good results on contigs n <dig> size and retain low error rate. when processing massive datasets without using external error correction tools, swap-assembler is immune from low data quality and generated longest n <dig> contig size.

for large genome and metagenome data of tara bytes, for example the human gut microbial community sequencing data, highly scalable and efficient assemblers will be essential for data analysis. our future work will extend our algorithm development for massive matagenomics dataset with additional modules.

the program can be downloaded from https://sourceforge.net/projects/swapassembler.

competing interests
the authors declare that they have no competing interests.

authors' contributions
jt carried out the parallel genome assembly studies, participated in the development of swap-assembler and drafted the manuscript. bq participated in the design and optimization of swap-assembler. yj participated in the development of swap-assembler and modification of this manuscript. sz participated in the design of the study and design of the performance test. pavan conceived of the study, and participated in its design and coordination and helped to draft the manuscript. all authors read and approved the final manuscript.

additional file 1
appendix. appendix  <dig> property proof of msg, appendix  <dig> algorithms for swap-assembler, appendix  <dig> complexity analysis of swap computational framework, appendix  <dig> complexity analysis of graph reduction.

