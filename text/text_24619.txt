BACKGROUND
the problem
high-throughput genome sequencing projects have generated massive amounts of dna and protein sequence data, and will do so more rapidly in the near future. one major challenge continues to be determining protein functions based solely on amino acid sequences. large-scale pairwise sequence comparison directly results in pairwise similarity measures between protein sequences and is an efficient method to transfer biological knowledge from known proteins to newly sequenced ones. the most widely used method to search for sequence similarities is blast  <cit> . three challenges arise:

 <dig>  deriving a quantitative similarity measure from the sequence comparison that models homology as well as possible; frequently this is based on the negative logarithm of the blast e-value.

 <dig>  inventing a clustering strategy that is sufficiently error-tolerant, since experience shows that sequence similarity alone does not lead to perfect clusterings. a common approach is to use a graph-based model, where proteins are represented as nodes and the similarities as weighted edges.

 <dig>  implementing the chosen clustering strategy efficiently.

we note that many approaches do treat the three challenges separately. in this publication,

 <dig>  we use a family of different similarity functions, based on negative logarithms of blast e-values and sequence coverage.

 <dig>  we show that weighted graph cluster editing is an adequate model to identify protein clusters. weighted graph cluster editing has been recently studied in  <cit>  and is known to be np-hard.

 <dig>  we present a heuristic called force to solve the problem. we show that it provides excellent quality results in practice when compared with an exponential-time exact algorithm, but has a running time that makes it applicable to massive datasets. an extended abstract about the force heuristic, including a comparison to other heuristics, was presented at csb  <dig>  <cit> .

to specify the clustering model, we need the following definition: an undirected simple graph g =  is called transitive if

 for all triples uvw ∈ 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqadaqaauaabeqaceaaaeaacqwgwbgvaeaacqaizawmaaaacagloagaayzkaaaaaa@306b@, uv ∈ e and vw ∈ e implies uw ∈ e. 

a transitive graph is a union of disjoint cliques, i.e., of complete subgraphs. each clique represents, in our case, a protein cluster. since the initial graph, derived from protein similarity values and a similarity threshold, may not be transitive, we need to modify it. this leads to the following computational problems.

graph cluster editing problem 
given an undirected graph g = , find a transitive graph g* = , with minimal edge modification distance to g, i.e., where |e \ e*| + |e* \ e| is minimal.

weighted graph cluster editing problem 
to respect the similarity between two proteins, we modify the penalty for deleting and adding edges. first we construct a similarity graph g =  consisting of a set of objects v and a set of edges e : = {uv ∈ 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqadaqaauaabeqaceaaaeaacqwgwbgvaeaacqaiyagmaaaacagloagaayzkaaaaaa@3069@: s > t}. here s: 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqadaqaauaabeqaceaaaeaacqwgwbgvaeaacqaiyagmaaaacagloagaayzkaaaaaa@3069@ → ℝ denotes a similarity function and t a user-defined threshold. the resulting cost to add or delete an edge uv is set to cost : = |s - t|. the cost to transform a graph g =  into a graph g' =  is consequently defined as cost : = cost + cost. as in the gcep, the goal is to find a transitive graph g* = , with cost = min {cost : g' =  transitive}.

it can be easily seen that the wgcep is np-hard, since it is a straightforward generalization of the gcep, where s: 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqadaqaauaabeqaceaaaeaacqwgwbgvaeaacqaiyagmaaaacagloagaayzkaaaaaa@3069@ → {- <dig>  1} and t =  <dig>  the gcep has been proved to be np hard several times, e.g., in  <cit> .

previous work and novel contributions
there are several approaches to cluster protein families. one of the earliest approaches that took the transitivity concept formally into account was proclust  <cit> ; however, the concept of editing the graph was not present in this work. the systers database  <cit> , now at release  <dig>  is based on a set-theoretic systematic researching approach and has existed for some time, but seems to have received little updates since early  <dig>  one of its main features is that it uses family-specific similarity thresholds to define clusters. it does not, however, employ a transitivity concept. in  <dig>  paccanaro et al.  <cit>  presented a comparison of the most popular cluster detection methods, like mcl  <cit> , hierarchical clustering  <cit> , generage  <cit> , and their own spectral clustering approach, which performs best when evaluated on a subset of the scop database. to evaluate our clustering model, we use the same datasets and performance figure. we furthermore include the recently published affinity propagation method in our comparison  <cit> . additionally, we evaluate our approach against the cog database  <cit> .

the weighted graph cluster editing problem was first considered for protein clustering in our extended abstract  <cit> , where we also introduced the basic idea of the force heuristic.

here we present a detailed description of the method and an extended parameter estimation procedure using evolutionary training. our main point in the paper is that the weighted cluster editing problem adequately models the biological homology detection problem if we use appropriate similarity functions and thresholds. the choice of the threshold and similarity function is, of course, critical, and we report the performance for a wide variety of them in the additional files accompanying this paper.

methods
clustering via graph layouting
we present an algorithm called force that heuristically solves the wgcep for a connected component and thus for a whole graph. force is motivated by a physically inspired force-based graph layout algorithm developed by fruchterman and reingold  <cit> . the main idea of this approach is to find an arrangement of the vertices in a two-dimensional plane that reflects the edge density distribution of the graph, i.e., vertices from subgraphs with high intra-connecting edge weights should be arranged close to each other and far away from other nodes. this layout is then used to define the clusters by euclidean single-linkage clustering of the vertices' positions in the plane. to improve the solution, we implemented an additional postprocessing phase. all in all the algorithm proceeds in three main steps:  layouting the graph,  partitioning, and  postprocessing.

layout phase
the goal in this phase is to arrange the vertices in a two-dimensional plane, such that the similarity values are respected. subsets of nodes with high edge-density should be arranged next to each other, and far away from other nodes. to find a layout that satisfies this criterion, we use a model inspired by physical forces, i.e., nodes can attract and repulse each other. starting with an initial layout , the nodes affect each other depending on their similarity and current position, which leads to a displacement vector for each node and a new arrangement.

since this model is only inspired by physical forces without friction, it does not include acceleration. for a user-defined number of iterations r, the interaction between every pair of nodes and thus the displacement for every node is calculated; then all nodes are simultaneously moved to their new position. we compute the displacements as follows: as described in algorithm  <dig> , the strength fu ← v of the effect of one node v to another node u  depends on the euclidean distance d, on the cost to add or delete the edge and a user defined attraction or repulsion factor fatt, frep. more formally,

 fu←v={cost⋅fatt⋅log⁡+1)|v|for attraction,cost⋅frep|v|⋅log⁡+1)for repulsion.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgmbgzdawgaawcbagaemydaunaeyikhwqaemodayhabeaakiabg2da9maaceqabaqbaeaabigaaaqaamaalaaabaacbigae83yammae83ba8mae83camnaemidaqnaeiikagiaemydaunaemodaynaeiykakiaeyyxictaemozay2aasbaasqaaiabbggahjabbsha0jabbsha0bqabagccqghfly1cyggsbabcqggvbwbcqggnbwzcqggoaakcqwgkbazcqggoaakcqwg1bqdcqggsaalcqwg2bgdcqggpaqkcqghrawkcqaixaqmcqggpaqkaeaadaabdaqaaiabdafawbgaay5bslaawia7aaaaaeaacqqgmbgzcqqgvbwbcqqgybgccqqggaaicqqghbqycqqg0badcqqg0badcqqgybgccqqghbqycqqgjbwycqqg0badcqqgpbqacqqgvbwbcqqgubgbcqggsaalaeaadawcaaqaaiab=ngajjab=9gavjab=nhazjabdsha0jabcicaoiabdwha1jabdaha2jabcmcapiabgwsixlabdagamnaabaaaleaacqqgybgccqqglbqzcqqgwbacaeqaaagcbawaaqwaaeaacqwgwbgvaiaawea7cagliwoacqghfly1cyggsbabcqggvbwbcqggnbwzcqggoaakcqwgkbazcqggoaakcqwg1bqdcqggsaalcqwg2bgdcqggpaqkcqghrawkcqaixaqmcqggpaqkaaaabagaeeozaymaee4ba8maeeocainaeeiiaaiaeeocainaeeyzaumaeeicaanaeeydaunaeeibawmaee4camnaeeyaakmaee4ba8maeeoba4maeiola4caaagaay5eaaaaaa@a98c@ 

two nodes attract each other if s > t and repulse each other otherwise. one can see that with increasing distance, attraction strength increases while repulsion strength decreases.

to improve convergence to a stable position with minimal interactions, we added a cooling parameter, also inspired by the algorithm of fruchterman and reingold. in our implementation, this means that if the displacement distance exceeds a maximal magnitude mi in iteration i, which starts at an initial value m <dig> and decreases with every iteration i, the movement is limited to it.

the output of this phase is a two-dimensional array pos containing the x-y-position of each node. additional files  <dig> and  <dig> illustrate the layout process and its convergence for two components with  <dig> and  <dig> nodes, respectively.

partitioning phase
using the positions of the vertices from the layout phase, we define clusters by geometric single-linkage clustering, parameterized by a maximal node distance δ: as described in algorithm  <dig> , we start with an arbitrary node v <dig> ∈ v and define a new cluster cv1
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgjbwydawgaawcbagaemoday3aasbaawqaaiabigdaxaqabaaaleqaaaaa@30c4@. a node i belongs to cv1
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgjbwydawgaawcbagaemoday3aasbaawqaaiabigdaxaqabaaaleqaaaaa@30c4@ if there exist nodes v <dig> = i <dig>  ..., in = i ∈ v with d ≤ δ for all j =  <dig>  ..., n -  <dig>  nodes are assigned to cv1
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgjbwydawgaawcbagaemoday3aasbaawqaaiabigdaxaqabaaaleqaaaaa@30c4@ until no further nodes satisfy the distance cutoff. then the next, not yet assigned, node v <dig> ∈ v is chosen to start a new cluster until every node is assigned to some cluster. we denote with gδ:=∪j=1mcvj
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwghbwrdawgaawcbaaccigae8htdqgabeaakiabcqda6iabg2da9maatadabagaem4yam2aasbaasqaaiabdaha2naabaaameaacqwgqbgaaeqaaawcbeaaaeaacqwgqbgacqgh9aqpcqaixaqmaeaacqwgtbqba0gaesokiufaaaa@3c6a@ the resulting graph obtained by adding all edges between two nodes of the same cluster and deleting all edges between two nodes of different clusters. to find a good clustering we calculate cost for different δ. starting with δ ← δinit : =  <dig> we increase δ by a step size σ up to a limit δmax : =  <dig> 

experimentation shows that it is beneficial to also increase the step size, i.e. to start with σ ← σinit : =  <dig>  and increase it by multiplying with a user-defined factor fσ : =  <dig> . the solution with lowest cost is returned as the resulting clustering. algorithm  <dig> returns the clustering in terms of an n × n adjacency matrix e* ∈ { <dig> }n × n, and the transformation cost c*.

postprocessing phase
although the best clustering is not guaranteed to be the optimal one, we often obtain a close to optimal solution in practice. to further improve the results we use a two-step postprocessing heuristic. we denote with cost the cost to obtain the clustering c.

 <dig>  to reduce the number of clusters and especially the number of singletons, the first step is to join two clusters if this reduces the overall cost:

let c : =  be the clustering obtained from the partitioning phase, ordered by size. for all cluster pairs  <dig> ≤ i <j ≤ n we calculate cost until we find a clustering c' : =  with cost <cost. let 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqadaqaaiqbdogajzaafawaasbaasqaaiabigdaxaqabagccqggsaalcqgguaglcqgguaglcqgguaglcqggsaalcuwgjbwygaqbamaabaaaleaacqwgubgbcqghsislcqaixaqmaeqaaagccagloagaayzkaaaaaa@39f5@ be the sorted vector c' . repeat to attempt joining more clusters until no more join is beneficial.

 <dig>  similar to the restricted neighborhood search clustering , we move a vertex from one cluster to another if this move reduces the overall cost:

as above, let c : =  be the clustering obtained from step  <dig>  ordered by size. for i, j ∈ { <dig>  ..., n}, i ≠ j, and every k ∈ ci, we tentatively move k from ci to cj and calculate cost , until we find the first such modified clustering with lower cost than cost. we sort the resulting clusters again by size and use them as a new start configuration for the next iteration until no more re-assignments are beneficial.

analysis
the worst-case running time of force is given by the addition of those of the three main phases. layouting runs in Θ , where r denotes the number of iterations and n is the number of nodes in the graph. since r is determined by evolutionary training , it might grow with n, but we set an upper bound for r to rmax =  <dig> that in practice suffices even for very large datasets.

partitioning runs in o, where d is the number of different δ-values used. this is seen as follows: each d-value requires the construction of an auxiliary graph in o time, the discovery of its connected components in o = o time, setting e' to the transitive closure of eδ and computing its cost, which is also possible in o after detecting connected components.

during postprocessing, each iteration takes o time, since the number of clusters is bonded by n. the total time is thus o, where p is the number of postprocessing iterations. while theoretically p can grow with n, in practice we observe only a small number of iterations until no more improvement occurs.

thus for all practical purposes, the overall runtime of force is quadratic in the number of nodes.

evolutionary parameter training
there are several user-defined parameters to assign, such as the number of iterations r, the attraction and repulsion scaling factors fatt, frep, the magnitude m <dig>  and the initial circular layout radius ρ. a practical method to find good values is evolutionary training. force implements such a strategy in two different ways.

first, a good parameter combination is determined that can be applied to most of the graphs. this is done during a pre-computation on a training data set. since, however, the optimal parametser constellation depends on the specific graph, we additionally apply such a training algorithm to each graph. force allows to specify the number of generations to train, and thus to adjust runtime and the quality of the result.

training works as follows: first we start with a set of  <dig> randomly generated parameter sets and the initial parameters mentioned above. the parameter sets are sorted by the cost to solve the wgcep on the given graph. for each generation, we use the best  <dig> parameter constellations as parents, to generate  <dig> new combinations. in order to obtain fast convergence to a good constellation, as well as a wide spectrum of different solutions without running into local minima, force splits these  <dig> new combinations into  <dig> groups, with  <dig> members each. the first group consists of parameters obtained only by random combinations of the  <dig> best already known parameter constellations. the next group is generated with random parameters, while the third group is obtained by a combination of the previous methods. to reduce the runtime for small or very easy to compute solutions, we added a second terminating condition: if at most two different cost appear while calculating the  <dig> start parameters, the best one is chosen. no more generations are computed.

datasets, similarity functions, and parameters
here we describe the datasets used for the subsequent evaluation. first the astral dataset from scop, as used in  <cit> , is introduced. we also describe a considerably larger dataset obtained from the cog database. blast is used for all-against-all similarity searches in all datasets. the similarity measure is a function of the blast e-values; we describe three reasonable functions to convert e-values into similarities. the results are used as input for force. all datasets can be downloaded from the force website.

scop and astral95
scop is an expert, manually curated database that groups proteins based on their 3d structures. it has a hierarchical structure with four main levels . proteins in the same class have the same type of secondary structures. proteins share a common fold if they have the same secondary structures in the same arrangement. proteins in the same superfamily are believed to be evolutionarily related, whereas proteins in the same family exhibit a clear evolutionary relationship  <cit> . we take the scop superfamily classification as ground truth against which we evaluate the quality of a clustering generated by a given algorithm, using reasonable quality measures, such as the f-measure . since the complete scop dataset contains many redundant domains that share a very high degree of similarity, most researchers choose to work with the astral compendium for sequence and structure analysis in order to generate non-redundant data  <cit> . astral allows to select scop entries that share no more sequence similarity than a given cutoff, removing redundant sequences.

we extracted two subsets of the astral dataset of scop v <dig>  with a cutoff of  <dig> percent, which means that no two protein sequences share more than 95% of sequence identity. we consider astral <dig> as the best possible available reference for remote homology detection on a structural basis.

the two subsets are exactly those used in paccanaro et al.'s work  <cit> . the first comprises  <dig> proteins from  <dig> different scop superfamilies, namely globin-like, ef-hand, cupredoxins, glycosidases, thioredoxin-like, and membrane all-alpha. we refer to this dataset as astral95_1_ <dig> 

due to the fact that scop is continuously updated, we decided to evaluate both the original data from  <cit>   and more recent data from the current scop version . the novel version is slightly different. for example, the superfamily membrane all-alpha has been removed in the meantime, and most of its proteins are assigned to different superfamilies. also, several other proteins have been reassigned to one of the five other superfamilies. this provides another dataset of  <dig> sequences from the remaining  <dig> superfamilies, which we refer to as astral95_1_ <dig> 

the second subset consists of  <dig> sequences from  <dig> superfamilies, namely globin-like, cupredoxins, viral coat and capsid proteins, trypsin-like serine proteases, fad/nad-binding domain, mhc antigen-recognition domain, and scorpion toxin-like. we refer to this as astral95_2_ <dig> and astral95_2_ <dig> respectively. scop can be found at  <cit> , while the protein sequences are available at  <cit> .

protein sequences from the cog database
the cluster of orthologous groups  of proteins database is a repository whose main goal is a phylogenetic classification of proteins encoded by complete genomes. it currently consists of  <dig>  prokaryotic protein sequences from  <dig> complete genomes distributed across the three domains of life  <cit> . cog contains clusters in which at least three individual proteins , originating from three different species, are each other's best blast hit in both directions. this strategy is believed to generate clusters of groups of orthologous genes.

we consider cog as the best possible representation of orthology detection, based on sequence data alone. we refer to this dataset as the cog dataset. cog can be found  <cit> , while the protein sequences are available at  <cit> .

similarity functions and thresholds
any attempt to  solve the wgcep would be in vain if the target function did not model our goal appropriately. as mentioned earlier, the main challenge is to identify appropriate similarity functions and thresholds. we have used a variety of similarity functions that we describe below.

assume we are given a set of proteins v and a blast output file containing multiple high-scoring pairs  in both directions. for two proteins u and v we denote with i and j, where i =  <dig>  ..., k and j =  <dig>  ..., l, the corresponding k hsps in one and l hsps in the other direction, respectively.

we consider the following three similarity functions.

best hit 
this widely used method concentrates on the e-value of a single hsp: for both directions, one looks for the best hit, i.e., the hsp with lowest e-value. to obtain a symmetric similarity function s: 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqadaqaauaabeqaceaaaeaacqwgwbgvaeaacqaiyagmaaaacagloagaayzkaaaaaa@3069@ → ℝ, the negative logarithm of the worst  of the two e-values is taken as similarity measure between u and v. the resulting symmetric similarity function is then defined as

 s:=−log⁡10i),min⁡j= <dig> ...,le-valuej)}).
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgzbwccqggoaakcqwg1bqdcqwg2bgdcqggpaqkcqgg6agocqgh9aqpcqghsislcyggsbabcqggvbwbcqggnbwzdawgaawcbagaegymaejaegimaadabeaakmaabmaabagagiyba0maeiyyaemaeiieag3aaiwabeaadawfqaqaaigbc2gatjabcmgapjabc6gaubwcbagaemyaakmaeyypa0jaegymaejaeiilawiaeiola4iaeiola4iaeiola4iaeiilawiaem4aasgabeaakiabbweafjabb2catiabbaha2jabbggahjabbygasjabbwha1jabbwgalnaabmaabawaaewaaeaacqwg1bqdcqghqgcrcqwg2bgdaiaawicacaglpaaadawgaawcbagaemyaakgabeaaaogaayjkaiaawmcaaiabcycasmaaxababagagiyba0maeiyaakmaeioba4galeaacqwgqbgacqgh9aqpcqaixaqmcqggsaalcqgguaglcqgguaglcqgguaglcqggsaalcqwgsbabaeqaaogaeeyraukaeeyla0iaeeodaynaeeyyaemaeeibawmaeeydaunaeeyzau2aaewaaeaadaqadaqaaiabdwha1jabgkziukabdaha2bgaayjkaiaawmcaamaabaaaleaacqwgqbgaaeqaaagccagloagaayzkaaaacagl7bgaayzfaaaacagloagaayzkaagaeiola4caaa@850d@ 

sum of hits 
this approach is similar to beh, but additionally includes every hsp with an e-value smaller than a threshold m = 10- <dig>  we use this threshold as penalty for every additional hsp. this leads to the similarity function

 s:=−log⁡10⋅∏i=1ke-valuei),m−⋅∏j=1le-valuej)}).
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgzbwccqggoaakcqwg1bqdcqwg2bgdcqggpaqkcqgg6agocqgh9aqpcqghsislcyggsbabcqggvbwbcqggnbwzdawgaawcbagaegymaejaegimaadabeaakmaabmaabagagiyba0maeiyyaemaeiieag3aaiwabeaacqwgtbqbdaahaawcbeqaaiabgkhitiabcicaoiabdugarjabgkhitiabigdaxiabcmcapaaakiabgwsixpaarahabagaeeyraukaeeyla0iaeeodaynaeeyyaemaeeibawmaeeydaunaeeyzau2aaewaaeaadaqadaqaaiabdwha1jabgcziskabdaha2bgaayjkaiaawmcaamaabaaaleaacqwgpbqaaeqaaagccagloagaayzkaagaeiilawiaemyba02aawbaasqabeaacqghsislcqggoaakcqwgsbabcqghsislcqaixaqmcqggpaqkaagccqghfly1daqewbqaaiabbweafjabb2catiabbaha2jabbggahjabbygasjabbwha1jabbwgalbwcbagaemoaaomaeyypa0jaegymaedabagaemibawganiabg+givdgcdaqadaqaamaabmaabagaemydaunaeyokh4qaemodayhacagloagaayzkaawaasbaasqaaiabdqgaqbqabaaakiaawicacaglpaaaasqaaiabdmgapjabg2da9iabigdaxaqaaiabdugarbqdcqghpis1aagccagl7bgaayzfaaaacagloagaayzkaagaeiola4caaa@8b61@ 

coverage 
the third approach integrates the lengths of a hsp into the similarity function. to determine the coverage, we need the following indicator function:

 iuv:={1if in u the position i is covered by any hsp n= <dig> ...,k or m= <dig> ...,l,0otherwise.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaatuudjxwak1uy0hmmaehbfv3yslgzg0uy0hgiud3bagabaiab=hi8jnaabaaaleaacqwg1bqdcqwg2bgdaeqaaogaeiikagiaemyaakmaeiykakiaeiooaojaeyypa0zaaiqabeaafaqaaegacaaabagaegymaedabagaeeyaakmaeeozaymaeeiiaaiaeeyaakmaeeoba4maeeiiaaiaemydaunaeeiiaaiaeeidaqnaeeiaagmaeeyzaumaeeiiaaiaeeicaanaee4ba8maee4camnaeeyaakmaeeidaqnaeeyaakmaee4ba8maeeoba4maeeiiaaiaemyaakmaeeiiaaiaeeyaakmaee4camnaeeiiaaiaee4yammaee4ba8maeeodaynaeeyzaumaeeocainaeeyzaumaeeizaqmaeeiiaaiaeeoyaimaeeyeaknaeeiiaaiaeeyyaemaeeoba4maeeyeaknaeeiiaaiaeeisagkaee4uamlaeeiuaalaeeiiaaiaeiikagiaemydaunaeyikhwqaemodaynaeiykakyaasbaasqaaiabd6gaujabg2da9iabigdaxiabcycasiabc6cauiabc6cauiabc6cauiabcycasiabdugarbqabagccqqggaaicqqgvbwbcqqgybgccqqggaaicqggoaakcqwg1bqdcqghsgircqwg2bgdcqggpaqkdawgaawcbagaemyba0maeyypa0jaegymaejaeiilawiaeiola4iaeiola4iaeiola4iaeiilawiaemibawgabeaakiabcycasaqaaiabicdawaqaaiabb+gavjabbsha0jabbigaojabbwgaljabbkhayjabbeha3jabbmgapjabbohazjabbwgaljabc6cauaaaaiaawuhaaaaa@acb7@ 

the coverage can now be defined as

 coverage:=min⁡,1|v|∑i=1|v|ivu).
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqqgjbwycqqgvbwbcqqg2bgdcqqglbqzcqqgybgccqqghbqycqqgnbwzcqqglbqzcqggoaakcqwg1bqdcqwg2bgdcqggpaqkcqgg6agocqgh9aqpcyggtbqbcqggpbqacqggubgbdaqadaqaamaalaaabagaegymaedabawaaqwaaeaacqwg1bqdaiaawea7cagliwoaaawaaabcaeaatuudjxwak1uy0hmmaehbfv3yslgzg0uy0hgiud3bagabaiab=hi8jnaabaaaleaacqwg1bqdcqwg2bgdaeqaaogaeiikagiaemyaakmaeiykakiaeiilawcaleaacqwgpbqacqgh9aqpcqaixaqmaeaadaabdaqaaiabdwha1bgaay5bslaawia7aaqdcqghris5aowaasaaaeaacqaixaqmaeaadaabdaqaaiabdaha2bgaay5bslaawia7aaaadaaewbqaaiab=hi8jnaabaaaleaacqwg2bgdcqwg1bqdaeqaaogaeiikagiaemyaakmaeiykakcaleaacqwgpbqacqgh9aqpcqaixaqmaeaadaabdaqaaiabdaha2bgaay5bslaawia7aaqdcqghris5aagccagloagaayzkaagaeiola4caaa@7f9d@ 

in order to obtain a good similarity function, we control the influence of the coverage on the overall similarity function by a user-defined factor f, and set

 s : = s' + f ·coverage. 

here s' : 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqadaqaauaabeqaceaaaeaacqwgwbgvaeaacqaiyagmaaaacagloagaayzkaaaaaa@3069@ → ℝ denotes one of the previously presented similarity functions, beh or soh.

parameter choices
the initial parameters obtained from the pre-processing training are r =  <dig>  fatt =  <dig> , frep =  <dig> , m <dig> =  <dig>  and ρ =  <dig> for the protein clustering problem. furthermore, we apply evolutionary training to each problem instance, as described in the algorithms section.

RESULTS
this section contains three different types of results. first we discuss the appropriateness of the wgcep model for the detection of clusters of homologous proteins using the astral dataset described earlier. next we show that the force heuristic is fast in practice, and compares favorably against an exact  fixed-parameter algorithm in terms of solution quality. we show that force is able to handle very large datasets efficiently, in particular the cog dataset described previously. finally, we have integrated the clustering results of force into the corynebacterial reference database coryneregnet  <cit> .

evaluation of the wgcep model
to show that the wgcep model is adequate for protein homology clustering, we evaluate our algorithm in the same way as paccanaro et al. did in their article  <cit> , using the so-called f-measure to quantify the agreement of force's result with the reference clustering provided by the astral dataset.

we first explain the f-measure, which equally combines precision and recall. let k =  be the clustering obtained from the algorithm and c =  the reference clustering. furthermore, we denote with n the total number of proteins and with ni, nj the number of proteins in the cluster ki and cj, respectively. following this, nij
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgubgbdaqhaawcbagaemyaakgabagaemoaaogaaaaa@30f6@ is the number of proteins in the intersection ki ∩ cj. the f-measure is defined as

 f:=1n∑j=1lnj⋅max⁡1≤i≤m.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwggbgrcqggoaakcqwglbwscqggsaalcqwgdbwqcqggpaqkcqgg6agocqgh9aqpdawcaaqaaiabigdaxaqaaiabd6gaubaadaaewbqaaiabd6gaunaacaaaleqabagaemoaaogaaaqaaiabdqgaqjabg2da9iabigdaxaqaaiabdygasbqdcqghris5aogaeyyxic9aacbeaeaacyggtbqbcqgghbqycqgg4baeasqaaiabigdaxiabgsmijkabdmgapjabgsmijkabd2gatbqabagcdaqadaqaamaalaaabagaegomaijaemoba42aa0baasqaaiabdmgapbqaaiabdqgaqbaaaoqaaiabd6gaunaabaaaleaacqwgpbqaaeqaaogaey4kasiaemoba42aawbaasqabeaacqwgqbgaaaaaaagccagloagaayzkaagaeiola4caaa@5d2b@ 

as mentioned earlier, paccanaro et al. previously compared the most popular protein clustering tools against their own spectral clustering: generage, tribemcl, and hierarchical clustering. since there is no need to replicate existing results, we use the same data . table  <dig> summarizes the results: using force, we obtain slightly better agreements than with spectral clustering. the best similarity function parameters and score threshold for the astral95_1_ <dig> dataset were cov-scoring using f =  <dig> and beh as a secondary scoring function, and t = - <dig> . for the astral95_2_ <dig> dataset, this was cov-scoring with f =  <dig> and soh as secondary scoring function with t = - <dig> .

note that in the present context, we do not consider it as cheating to optimize the similarity function and threshold: we want to check how far the wgcep model can retrieve the biologically correct clustering under ideal conditions. the same kind of optimization was applied by paccanaro et al. in  <cit> . table  <dig> also shows the f-measures for the affinity propagation  approach, which was recently published in  <cit> . we used the same data and also varied necessary input parameters to evaluate against the best possible performance of ap. for astral95_1_ <dig>  this was cov-scoring with f =  <dig> and soh as secondary scoring function with fixed preference pre =  <dig>  and damping factor df =  <dig> . for astral95_2_ <dig>  this was cov-scoring with f =  <dig> and soh as secondary scoring function with pre =  <dig>  and df =  <dig> . for both datasets, ap performs worse than spectral clustering.

we generated images in the same style for all datasets, zipped them, and provide them as additional file  <dig>  we additionally evaluate force with the newest astral <dig> datasets . table  <dig> shows the resulting f-measures for a variety of similarity functions and parameter choices. all of these achieve higher f-measures than spectral clustering, or ap.

in additional file  <dig>  we provide f-measures of force for a wide range of thresholds and coverage factors, for all used datasets and similarity functions. good clustering quality is also reached by using other thresholds and similarity measures for all test datasets. in additional file  <dig>  we give f-measures for a range of thresholds, but with fixed coverage factor f =  <dig>  for dataset astral95_1_ <dig>  and similarity function beh. in additional file  <dig>  we provide f-measures for affinity propagation for a wide range of parameters and coverage factors, for all used datasets and similarity functions.

quality and running time of the heuristic
after evaluating the wgcep as a reasonable clustering paradigm, we address the performance of the force heuristic: we compare the running time and solution quality against a slow exact algorithm on the large cog dataset. a recently developed fixed-parameter  algorithm for the wgcep  <cit>  extends ideas of previously developed fp algorithms for the  gcep by gramm et al.  <cit>  and dehne et al.  <cit> , and has a running time of o, if there exists a transitive projection of cost at most k. this allows us to find the optimal solution for a wgcep, given a graph g =  up to size |v| ≈  <dig> in appropriate time. to our knowledge, the implementation of this algorithm is the fastest available exact wgcep solving program.

in order to compare the two approaches we use the cog dataset, split into connected subgraphs using similarity function soh and a threshold of  <dig>  we extracted  <dig> connected components . for the evaluation, we restricted the maximal run time to  <dig> hours. the fp algorithm thus could only be applied to  <dig> components with |v| ≤  <dig>  for the remaining components, the fp algorithm was terminated unsuccessfully after  <dig> hours. due to the large number of graphs, we abstained from applying fp to graphs with |v| ≥  <dig>  because it is very likely that runtime would exceed  <dig> hours. figure  <dig> illustrates a running time comparison of the fp  and the heuristic algorithm . force has been configured to use one generation of evolutionary parameter training for each graph, as described in the algorithms section. all time measurements were taken on a sunfire  <dig> with  <dig> mhz ultrasparc iii+ processors and  <dig> gb of ram.

one can see that for large graphs |v|·|e| ≥  <dig> 000), force is much faster than the exact fp algorithm. note that the axes are logarithmically scaled. we evaluate the quality of the force heuristic by comparing the relative cost increase of the reported solution, with respect to the provably optimal solution. for  <dig> out of the  <dig> comparable components, the heuristic determines the optimal solution. the optimal cost over all  <dig> components is  <dig>  <dig> , while force finds a solution with a total cost of  <dig>  <dig> , which is a difference of  <dig> %. figure  <dig> illustrates these numbers. note that most of the data points lie on the x-axis and hence indicate that the optimal solution was found.

in addition to the direct running time and quality comparison, we make all connected components and clustering results of the cog dataset available on the force website, using the following similarity functions and thresholds: beh/ <dig>  beh/ <dig>  soh/ <dig>  soh/ <dig>  these choices do not reproduce the original cog clustering; we obtain the following f-measures:  <dig>  ,  <dig>  ,  <dig>  , and  <dig>  . it should be noted that a) the cog clustering problem has very different properties than the scop clustering problem, and b) here we have not optimized in any way the scoring function and threshold. we discuss this further below.

coryneregnet
coryneregnet  allows a pertinent data management of regulatory interactions along with the genome-scale reconstruction of transcriptional regulatory networks of corynebacteria relevant in human medicine and biotechnology, together with escherichia coli. coryneregnet is based on a multi-layered, hierarchical and modular concept of transcriptional regulation and was implemented with an ontology-based data structure. it integrates the fast and statistically sound method possumsearch  <cit>  to predict transcription factor binding sites within and across species. reconstructed regulatory networks can be visualized on a web interface and as graphs. special graph layout algorithms have been developed to facilitate the comparison of gene regulatory networks across species and to assist biologists with the evaluation of predicted and graphically visualized networks in the context of experimental results. to extend the comparative features, we need adequate data on gene and protein clusters. the integration of this information would widen the scope of coryneregnet and assist the user with the reconstruction of unknown regulatory interactions  <cit> .

using force, we calculated protein clusters for all organisms integrated in coryneregnet: corynebacterium diphtheriae, corynebacterium efficiens, corynebacterium glutamicum, corynebacterium jeikeium, escherichia coli, mycobacterium tuberculosis cdc <dig> and mycobacterium tuberculosis h37rv . based on cluster size distribution, we empirically determined a comparatively high threshold of  <dig>  and similarity function soh to create the force input files based on the all-vs-all blast results that are generated during coryneregnet's data warehousing process.

the results computed by force are parsed into the object oriented back-end and further on translated into the ontology based data structure of coryneregnet. we added a new concept class forcecluster and a relation type b_fc , which links the proteins to their clusters. finally, we adapted the coryneregnet back-end to import the new data into the database and the web-front-end to present the clusters.

discussion and 
CONCLUSIONS
we have shown that the wgcep is an adequate model for remote protein homology clustering from sequence-based similarity measures and can outperform existing clustering approaches. part of this effect is certainly attributable to the class of similarity functions that we consider. nevertheless, in this particular application, the wgcep paradigm  even outperforms the affinity propagation approach, for which we use the same class of similarity functions and a similar parameter optimization as for our approach.

we described force, a heuristic algorithm for the np-hard weighted graph cluster editing problem. compared to the currently most efficient exact  fixed-parameter algorithm for this problem, we have demonstrated empirically that force regularly provides solutions that are optimal, although no guarantee is given by the algorithm. in contrast to the exact algorithm, force can solve the problem for graphs with several thousands of nodes in reasonable time.

one of our motivations to develop a rapid and high-quality clustering algorithm arose from the need to extend the data warehouse coryneregnet with protein family information. consequently, the clustering derived by force has been integrated into the system.

we emphasize that force can cluster any set of objects connected by any kind of similarity function using the concept of editing a graph into a transitive graph with minimum cost changes. the integrated evolutionary parameter training method ensures good performance on any kind of data.

several issues remain to be resolved with the cluster editing or transitive projection approach. one disadvantage of the method is that it uses the same threshold for all clusters to determine the cost of adding or removing edges. the authors of systers  <cit>  report an interesting approach to choose thresholds in a dynamical way. finding a way of incorporating dynamic thresholds into cluster editing would certainly enhance its applicability.

the other issue we need to discuss is more global and applies to any clustering algorithm and concerns the choice of parameters. for evaluating the wgcep model with the scop datasets, we have optimized similarity function and threshold  by using the known truth as a reference and thus determined that there exists a  similarity function that models the truth rather well. in practice, given an unknown dataset, we do not know which parameters lead to the unknown truth. therefore we need to find properties of the resulting clustering  that tell us something about the quality of the clustering. for coryneregnet, we were able to use the cluster size distribution, as we had expert biological support. in other cases, it is an open challenge to find properties of the clustering that can be easily verified by knowledgeable experts in the field.

availability and requirements
project name: force

project home page: 

operating system: platform independent

programming language: java 6

license: academic free license 

any restrictions to use by non-academics: license needed. user should contact

jan.baumbach@cebitec.uni-bielefeld.de.

comment: source code, all used datasets, and the clustering results can be obtained from the force project website.

authors' contributions
tw and jb developed and implemented the heuristic force. together with fpl, tw and jb evaluated the data with astral <dig>  jb integrated force into coryneregnet. sr proposed to examine the clustering problem from the transitive graph projection viewpoint, modeled the similarity functions and supervised the whole project. all authors contributed to writing; and all authors read and approved the final manuscript.

appendix
algorithm  <dig> – graph layouting
input: similarity matrix  <dig> ≤ i<j ≤ n with sij : = s - t; circular layout radius ρ, attraction factor fatt, repulsion factor frep, number of iterations r

output: node positions pos = ; each pos  ∈ ℝ <dig> 

1: pos = arrangeallnodescircular ▷ initial layout

2: for r =  <dig> to r do

3:    ▷ compute displacements Δ for iteration r

4:    initialize array Δ =  of displacement vectors to Δ =  for all i

5:    for i =  <dig> to n do

6:       for j =  <dig> to i -  <dig> do

7:          if si, j >  <dig> then

8:             fi ← j = log + 1)·si, j·fatt ▷ attraction strength

9:          else

10:             fi ← j =  + 1))·si, j·frep ▷ repulsion strength

11:          Δ + = fi ← j·/d

12:          Δ - = fi ← j·/d

13:    ▷ move nodes by capped displacement vectors

14:    for i =  <dig> to n do

15:       Δ  = ·min{||Δ||, m}

16:       pos  + = Δ 

17: return pos

algorithm  <dig> – partitioning the layouted graph
input: layout positions pos, initial and maximal clustering distances δinit, δmax, initial step size σinit, step size factor fσ, similarity matrix  <dig> ≤ i<j ≤ n to compute costs

output: best found n × n adjacency matrix e* describing a clustering, associated cost c*

1: δ = δinit, σ = σinit, c* = ∞, e* = n × n

2: while δ ≤ δ max do

3:    construct auxiliary graph gδ =  with eδ : = {uv : d ≤ δ }

 <dig>     detect connected components of gδ 

5:    compute transitively closed adjacency matrix e' from eδ 

6:    if cost <c* then

7:       e* = e'; c* = cost

8:    σ = σ·fσ; δ = δ + σ

9: return 

supplementary material
additional file 1
graph layout i. this file is an image illustrating the layout process of a graph with  <dig> nodes after   <dig>    <dig>  and   <dig> iterations.

click here for file

 additional file 2
graph layout ii. this file is an image illustrating the layout process of a graph with  <dig> nodes after   <dig>    <dig>  and   <dig> iterations.

click here for file

 additional file 3
graphical clustering summary. this zipped file contains images summarizing the force clustering results for the two similarity functions beh and soh, and all four datasets, similar to our figure  <dig>  we used matlab scripts provided by paccanaro  <cit>  to create these images.

click here for file

 additional file 4
quality evaluation for different scoring themes and datasets. this file is tab-delimited and stores f-measures for a wide range of thresholds and coverage factors, for all used datasets and similarity functions. column 1: f-measure, column 2: coverage factor f, column 3: threshold, column 4: dataset/similarity function.

click here for file

 additional file 5
quality evaluation for different thresholds and fixed coverage factor, dataset and similarity function. this file is tab-delimited and stores f-measures for a range of thresholds, and fixed coverage factor f =  <dig>  dataset astral95_1_ <dig>  and similarity function beh. column 1: threshold, column 2: coverage factor f, column 3: f-measure.

click here for file

 additional file 6
quality evaluation of affinity propagation for different scoring themes and datasets. this file is tab-delimited and stores f-measures for a wide range of parameter constellations and coverage factors, for all used datasets and similarity functions. column 1: f-measure, column 2: coverage factor f, column 3: preference pre, column 4: damping factor df, column 5: dataset/similarity function.

click here for file

 acknowledgements
the authors wish to thank andreas dress for helpful discussions. anke truss, sebastian briesemeister, and sebastian böcker generously made available an implementation of their fixed-parameter algorithm for comparison. we are grateful to marcel martin and ralf nolte for expert technical support. further thank to alberto paccanaro, for providing the matlab scripts used for the painting of the graphical clustering summary. additionally, we would like to thank constantin bannert for helpful advice.
