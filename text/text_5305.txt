BACKGROUND
while systems biology techniques—whether experimental or computational – are often developed on simple model systems, their application to increasingly complex model systems is one of the most exciting and promising aspects of modern biological research. however, applying these techniques to complex systems often presents new challenges. for example, systems biology approaches are only recently being brought to bear on non-human primate model systems , which can be critical to translational biomedical research when simpler organisms are not good models of human physiology  <cit> . however, the number of experimental samples possible in these systems is limited: using a large cohort is cost-prohibitive and ethically questionable, and animal welfare considerations limit the volume and frequency of blood or other tissue sampling. also, validation experiments in non-human primates are extremely difficult, which makes it critical that only a small number of high-confidence hypotheses are tested.

learning regulatory networks is a common task in systems biology research  <cit> , and one that is confounded by the restrictions associated with complex model systems. complex model systems usually do not allow for a large number of samples, but robustly learning network structure with few samples is difficult  <cit> . for experimental validation complex model systems require identification of only a few high-confidence connections between variables, but many common network analysis tools instead generate high-connectivity graphs  <cit>  .

given large sample sizes, bayesian networks are effective at identifying a small number of meaningful connections between features. bayesian networks  <cit>  are probabilistic graphical models that account for conditional dependencies when finding relationships between features. these networks do not necessarily reflect causality, but they are typically concise  and allow for easier identification of the most important relationships. however, with small sample sizes learning bayesian networks can be difficult. for example, network learning on systems with as few as  <dig> variables may often be tested using  <dig> or more samples  <cit> . larger and more complex networks may require even more samples for robust inference, which is typically infeasible in complex model systems. bayesian network inference also does not computationally scale well to large numbers of features  <cit> , though analysis of high-dimensional datasets is at the core of systems-scale, “omics” hypothesis-generating research.

classifiers, which are algorithms that predict the category of a sample based on data about that sample, are a class of techniques that can perform their task well even with comparatively few samples  <cit> . this is perhaps unsurprising, since only one feature or value is to be predicted rather than an entire network of connections. this focus only on relationships to one central feature, rather than between all of them, also typically enables classifiers to scale more easily to large numbers of features. however, focusing on just individual relationships to a central feature may ignore information that could provide improved predictions. to this end, bayesian networks have previously been used to create effective classifiers  <cit>  that exploit this information content. in these bayesian network based-classifiers, the actual structure of the network is not viewed as important—it is only a means to an end of correct classification – and they thus are typically not assessed.

we hypothesized that if bayesian network classifiers can be so effective at prediction , then they likely contain useful information about the underlying  structure in the networks being learned for the classification task, even if that is not an intended focus of the algorithms. the selection of nodes for inclusion in the model and the placement of edges between nodes, while intended merely for classification purposes, may in fact capture some of the most informative underlying structure that we would like to learn for biological interpretation. the fact that there is often some observed phenotype  that one would like to explain based on systems-scale data  only further supports the idea of using classifiers as the basis for network learning: the systems-scale data can be used to “classify” the observed phenotype and lead to the learning of a network.

accordingly, we chose to harness a recently-published tree-like bayesian network classifier  <cit>   and modify it to learn regulatory networks from biological datasets with comparatively few observations. these constraints are driven by our work in systems biology studies of non-human primate models of malaria, where the number of samples obtained per experiment is typically not greater than  <dig> but the number of features per experiment is sometimes in the thousands. to our knowledge, the problem of bayesian structure learning under the constraint of extremely small sample sizes has not previously been considered in depth.

we leveraged the extremely effective predictivity of the previously developed tree-like bayesian network classifier  <cit>  by refining it to provide less topologically restrictive learning of network structures. while this approach is most applicable for trees with known roots , here we show that it can also be applied to networks with an unknown root node . we demonstrate the efficacy of this classifier-based structure learning method using simple synthetic models and established reference datasets. we demonstrate that this approach produces reliable and limited predictions of network architecture under constrained sample sizes, with the potential to generate more efficient network models for complex systems. we also apply this methodology to a real complex biological system dataset, analyzing transcriptional data from a non-human primate model of malaria infection to get better insight into the animals’ response to the pathogen challenge.

methods
background and problem specification
a bayesian network is defined as b = <s, Θ>, where s and Θ respectively represent a directed acyclic graph and a set of conditional probabilities associated with the graph. each vertex in the graph is a feature , and a directed arc from vertex i to another vertex j shows a direct dependency relationship of feature j on feature i. feature i is called the parent of feature j, and feature j is called the child of feature i. specifically, in a bayesian network feature j is conditionally independent of all vertices that are not its children, given its parents. in this work, we look to learn s from a dataset consisting of m × p measurements , where m is the number of experiments, p is the number of features , and m < < p.

previous tree-like bayesian network  classifier
tree-like bayesian networks are a subset of bayesian networks: they meet all of the requirements of being a bayesian network, but with the additional requirement that all nodes except for one  have exactly one parent, while the root node has no parents. in recent work, lin et al. developed a classifier that learned a tree-like bayesian network  to perform the classification task  <cit> . they showed that this method performed as well as or superior to three common bayesian network classifiers.

briefly, the bn-tl method constructs a tree by first identifying the feature f* with the most mutual information with the root and places it as a child of the root. it then finds the feature f’ with the most conditional mutual information with f* given the root and places it as a child of f*. it then places all nodes with conditional mutual information sufficiently close to that between f* and f’ as children of f*. if there are any features left, a new branch is established in a similar fashion. this process is repeated until all features are added to the tree.

their method was tested on seven diverse datasets. its classification performance was shown to be comparable to or better than three common bayesian classifiers, including naïve bayes, a general bayesian classifier learned using a k <dig> greedy search strategy  <cit> , and another tree-like algorithm  <cit> . based on the strength of this approach at predicting classifications, we hypothesized that there is likely significant useful information in this classifier's network, even though that was not the stated goal of the classifier. however, the exact topology of the classifier’s network was not likely to be informative: it was flat, with a maximum of three layers and without consideration of potential relationships between features on the bottom layer . accordingly, we sought to harness the predictive power of this classifier with more flexible network construction to facilitate learning of generalized tree-like regulatory networks .fig.  <dig> representation of the topological constraints of two tree-like bayesian networks. a the topology of the previous tree-like bayesian network classifier  was constrained to three levels: a root, children of the root, and the terminal grandchildren of the root . construction of this network did not account for conditional mutual information between siblings. b the proposed tree-like bayesian structure learning algorithm  has no constraints on maximum depth of the network and considers the mutual information and conditional mutual information between siblings when creating the network structure



computational algorithm
here, we have designed a tree-like bayesian structure learning algorithm  that uses an approach similar to the bn-tl classifier algorithm to infer a generalized tree-like network . the goal of this network is to incorporate the most important dependency relationships in the given dataset. the general outline of the algorithm is provided in table  <dig> table  <dig> tree-like bayesian structure learning algorithm 





while the most direct application of this approach is to infer trees that explain the behavior of some specified root node , we have also designed the algorithm in a more generalized fashion to allow for the learning of networks in datasets where there is not an obvious root node. if not otherwise provided, the algorithm starts by selecting a reasonable root feature using statistical methods and dataset resampling . after a root is selected, new branches are extended from the root node by adding children to the root. the first candidate child is the node with the largest mutual information  value with the root, where mi is defined as:  <dig> mixy=∑x,ypxylog2pxypxpy, 

from the root, a minimally significant mi value  is required to allow the establishment of a new branch, helping to filter out features with minimal relationship with the root node if they exist. for the first established branch, child nodes to be added to that branch are searched for iteratively , and are then configured into sub-tree structures  if they do exist. once all children to be added to that branch have been identified and appropriately structured, the remaining unassigned node with the largest mi with the root is considered for addition as a new branch; this process is repeated iteratively until all features have been added . the algorithm then returns the learned tree-like structure.

the boldface words above are three functions used repeatedly in the algorithm. below we provide more details on each function.rootselection: if there is not a specified phenotype to be described with the dataset or an obvious root node for the system, then the first step of the tl-bsla approach is to select a reasonable root node from the given feature set. as illustrated in table  <dig>  this procedure consists of four steps:table  <dig> rootselection subroutine of tl-bsla





create a noisy dataset  matrix), where α is the ratio of synthetic noisy features to real features. a synthetic noisy feature is created by randomly permuting the m observations of a real feature; this is done α times for each of the p real features.

treat every real feature as the root node temporarily, and consider two datasets dreal- <dig> and dnoisy, where dreal- <dig> is the original real dataset with the observations for the current root node temporarily removed. for each feature in dreal- <dig> and dnoisy, calculate its significance level si with the current root node as the following:  <dig> si=mitruefi;current_root−meanmipermfi;current_rootstdmipermfi;current_root, 

where mitrue represents the mi value between feature fi∈ dreal- <dig> or dnoisy and the current root, and miperm represents the mutual information of randomly permuted fi and the root. np permutations are used for each fi , and the mean and standard deviation of the permuted mi are used to calculate si. this captures the relative significance of the relationship between fi and the root given the specific distribution of data in fi.

for each temporarily selected root, compare the s distribution for all fi∈ dreal- <dig> to the s distribution for all fi∈ dnoisy . this captures the overall significance of the root’s relationships with all nodes given the specific distribution of data in dreal.

select the feature with the largest difference between its two s distributions  as the final root. this allows for the network that is inferred to represent the strongest overall relationships in the data.



this function returns the selected root and a revised set of nodes d’ which includes all of the original nodes except for the root.b) findlayernode given the nodes fbottom  at the bottom layer of the current branch, the first step in this procedure is to determine whether there exist any child nodes that could continue this branch in the next layer. a candidate feature fi will be considered as a possible child of some node in the current bottom layer if mi’ ≥ ffc, where ffc is a user-determined parameter, mi’ is the maximum value of mi and the conditional mutual information  of fi and fbottom given the parent of fbottom ), and mi is the maximum value of mi for all fbottom,j∈ fbottom.



instead of mi, the mi’ value is used here because it not only accounts for the direct impact of the parent nodes, but also considers the indirect influence originating from the grandparent nodes. the numerical value of the parameter ffc is a user-determined parameter. here, we use  <dig>  based on empirical experience and suggestions from the previously-developed bayesian network classifier  <cit> . although the selected value of ffc may affect the ultimate inferred structure, parameter sensitivity analysis  has shown there to be a fairly broad interval of ffc around  <dig>  over which the algorithm’s results are insensitive.c) structurelayernode the purpose of this procedure is to arrange the candidate child nodes identified in findlayernode into a sub-tree structure using the nodes currently in the bottom layer of the current branch as potential parent nodes. as schematically described in table  <dig>  the input of this procedure includes the current bottom nodes , which are considered as the temporary roots for the sub-tree, the corresponding parent nodes of the bottom nodes , and the candidate pool for searched child nodes . the configuration starts with the calculation of mi’ for all bottomnodesi∈ bottomnodes and fi∈ child_pool. for each fi, the bottomnodesj with the largest mi’ value with fi is identified as a potential parent of fi; we refer to these pairs as . then, each bottomnodesj* is connected by an arc to the fi* with the greatest mi’ value with bottomnodesj* from among all of its potential children fi*. for bottomnodesj* with multiple potential children, an additional independence test is used to determine if additional children add sufficient independent information to allow their inclusion in this layer: if cmi ≤ ffc_independent , an arc is created between bottomnodesj* and fk. that is, fk is considered to be another child node of bottomnodesj* because it is  conditionally independent of the other children in the layer and thus should not be a child of those nodes. this process is continued iteratively until all nodes returned by findlayernode have been added to the tree.table  <dig> structurelayernode subroutine of tl-bsla







literature and synthetic datasets
four examples were used to evaluate the performance of the proposed tl-bsla. we developed a simple synthetic network  with a true tree structure except for a single node that is not connected to the rest of the graph. in this work we refer to this network as synthetic-tree; the true network is illustrated in additional file 1: figure s <dig>  the other three example networks used in this work are published networks widely used in structure learning literature: the child system, the alarm system, and the asia system . the child system is a tree-like network with  <dig> nodes and  <dig> edges, but is not exactly a tree. the asia and alarm networks are less tree-like networks  used to assess the algorithm’s performance on data drawn from underlying networks more similar to real “omics” data. data was generated based on the probabilities defined by each model network, with all variables being discrete.

experimental data
transcriptional data was used from a recent malaria challenge experiment in five rhesus macaques .

ethics statement
the experimental design of this experiment involving rhesus macaques  was approved by the emory university institutional animal care and use committee  under protocol #yer-2001892-090415ga.

malaria challenge experimental methods
the experimental protocol was similar to that used in our previous malaria challenge experiment  <cit> , with four noteworthy exceptions: there was a longer follow-up period for measurements, complete blood count  profiles were determined every day, there was no biotinylation of erythrocytes, and plasmodium cynomolgi sporozoites were used for the experimental infection. bone marrow aspirates were taken under anesthesia with ketamine at seven time points over the course of approximately 100 days, corresponding to baseline, peak of parasitemia, treatment of blood-stage parasites, and during and after relapse. transcriptional profiles were obtained by sequencing on an illumina hiseq <dig> at the yerkes national primate research center genomics core. additional details on the infection protocol, sampling protocol, and methods for initial processing of transcriptional data are available in additional file 1: supplemental methods.

experimental data processing
since the transcriptional profiles consisted of continuous variables, they were first discretized. this is a common data processing step, as it decreases the computational complexity and the minimum number of samples required for accurate structure learning. we have previously described methods for discretization of continuous data and their potential impact on learned networks during structure learning  <cit> . here, we have taken a simplified approach for our proof-of-principle analysis of a malaria-based dataset, using an equal-quantile discretization to evenly divide the values for each variable into high, medium, and low values. genes describing the recently identified axes of variation across large-scale human population human cohorts were used as a starting point for analysis, to facilitate data interpretation and network construction  <cit> .

comparator algorithm
our main goal in this work was to test the hypothesis that the information contained in a bayesian network classifier would be sufficient to provide informative learning of the actual underlying bayesian network. to provide a benchmark for acceptable performance in network learning, we selected the sparse candidate algorithm   <cit>  as implemented in the causal explorer package  <cit> . numerous algorithms have been published for structure learning of bayesian networks, with no conclusively optimal algorithm. we selected sca as the main comparator because it is widely-used and generally performs well, it is effective at handling reasonably large-scale networks , and it typically provides better positive predictive value in its inferred networks . the avoidance of false positives is particularly important for the design of validation experiments in complex model systems. in previous work  <cit>  we have found that many other algorithms  often learn many more false positives than true positives when sample sizes are limited. sca learns a significant fraction of those true positives with many fewer false positives, making it a desirable choice.

RESULTS
a classifier-inspired algorithm can effectively learn tree-like network structures
as described in greater detail in the methods, we have developed a tree-like bayesian structure learning algorithm  by building off the success of a previously published tree-like bayesian network classifier  <cit> . we removed some topological limitations from the existing bayesian network based-classifier and used conditional mutual information to appropriately arrange the nodes in the network.

four example networks were used to evaluate the performance of the proposed tl-bsla relative to a benchmark algorithm. the networks included a simple synthetic-tree network and three widely used literature networks with tree-like  and non-tree-like  structures. for each example,  <dig> randomly generated datasets were tested to reduce the impact of dataset-specific biases introduced by sampling; each dataset was analyzed using our proposed tl-bsla and the well-known sparse candidate algorithm  structure learning method  <cit>  for sample sizes ranging from  <dig> to  <dig> observations. more detailed justification for using sca is provided in the methods, but its key feature is that it typically provides good positive predictive value .

three metrics were used to assess the accuracy of learned structures for each algorithm:  true positive rate , the fraction of all actual edges that are correctly predicted by an algorithm;  false positive rate , the fraction of all actual non-edges that are incorrectly predicted by an algorithm to be edges; and  positive predictive value , the fraction of predicted edges that are actually edges. worth noting is that ppv is the most relevant metric for the purposes of model validation, as it determines the likelihood of success of often extremely expensive or difficult validation experiments: a low ppv will confound the experimental validation of network structure and have significant costs. for identification of true positives, we considered two cases in all analyses: if a learned edge needed to be the correct direction in order to count as a true positive, or if the correctness of each edge was determined without consideration of directionality. the same root was used for all analyses of a given network in order to provide sufficient consistency for comparison; roots were selected as described in the methods. the results of this evaluation are presented in fig.  <dig> fig.  <dig> tl-bsla performs consistently better than sca in four example systems. the true positive rate , false positive rate , and positive predictive value  are shown for four representative networks. black lines show performance of tl-bsla, blue lines show performance of sca. dashed lines represent calculations without considering the direction of connections when assessing their correctness. tl-bsla is almost universally better than sca, with the exception of tpr for the asia and alarm networks where the directionality is not accounted for in assessing correctness. in these cases, the much higher fpr of sca outweighs its potentially better coverage of true positives, as evidenced in the superior ppv curves for tl-bsla. for ppv, all performance metrics across all networks  are statistically significant  except for the  <dig> and  <dig> sample sizes for the asia network for the undirected case. error bars are one standard deviation



in the synthetic-tree system, tl-bsla correctly recovered almost all of the correct connections even when the sample size was rather small . in comparison, the sca approach achieved a lower tpr than tl-bsla, regardless of whether directionality was considered in assessing the accuracy of the networks. when directionality was considered, sca performed much more poorly than tl-bsla. this was particularly noticeable at low sample sizes: sca recovered no more than 50 % of the true edges when the sample size was below  <dig>  even without considering directionality, the performance of sca on this simple system was still significantly worse than that of tl-bsla. moreover, the average fpr for sca was always greater than 10 %  and 20 % , which was at least 4-fold higher  than that of tl-bsla. accordingly, the ppv for tl-bsla was much better for the synthetic-tree network.

the tpr for tl-bsla was consistently higher than for sca for the child system whether or not the directionality of the learned edges was considered . the magnitude of the difference was also fairly consistent across the range of sample sizes; most importantly, there are significant differences between the two methods at low  sample sizes. the tl-bsla also had a much lower fpr than sca, indicating that fewer incorrect edges were learned by the algorithm. as a result, the ppv of the tl-bsla was again significantly better than that of sca.

we also analyzed whether the networks inferred were sensitive to changes in the input datasets. it has previously been observed in biomarker discovery  <cit>  and in network inference  <cit>  that resampling of data can yield different outputs for machine learning and network inference algorithms. to assess this we followed a previously published approach to assess robustness to resampling  <cit> . from a fixed set of  <dig> samples for the child network, we selected subsets of  <dig> samples; we used tl-bsla to learn networks for  <dig> such resampled sets, with each set having no more than 30 % similarity to any of the others. the average number of connections found per dataset was  <dig>  using the tl-bsla,  <dig> connections were found in at least 60 % of the resampled datasets, suggestive of robust structure learning by tl-bsla.  <dig> of those connections were true positives . these results are consistent with the tpr and ppv performance shown in fig.  <dig>  on the other hand, only  <dig> connections were found in every subsample, and  <dig> connections were found in at least one and at most 10 % of subsamples . thus, while there is variability in the networks found based on the input dataset just from resampling, tl-bsla is capable of consistently picking out a core of true positive connections. this robustness may decrease, though, if real biological samples have significantly greater noise than the synthetic noisy data used here. additionally, we note that in the case where many samples are available, a resampling technique such as this can be useful to identify the highest-confidence and most robust connections for further experimental and computational investigation  <cit> .

a tree-like algorithm can also perform well on non-tree-like underlying structures
as for systems whose underlying structures do not resemble trees, such as the alarm and asia systems, fig.  <dig> shows that the tl-bsla performed competitively with, and in some important respects better than, sca. in both datasets, for the identification of edges with the correct directionality, the true positive rates for the two algorithms were statistically indistinguishable. in both cases the tl-bsla was more robust to the reduction of sample size to small values . when edge directionality was ignored, the performance of sca improved much more than that of tl-bsla, and was statistically significantly better for all sample sizes in the asia system. however, we note that at small sample sizes, the true positive rate ignoring directionality was statistically indistinguishable for the larger, more realistic alarm system.

importantly, though, the false positive rate for the tl-bsla was much lower  than that of sca. this ultimately resulted in ppv performance such that tl-bsla was significantly better at learning connections than sca across sample sizes, with the difference even more prominent when the directionality of edges was considered. taken together, this suggests that the use of a classifier-based bayesian network learning strategy that is computationally efficient may be a viable replacement for existing network learning algorithms.

based on the across-the-board improved ppv performance of the tl-bsla and the details of how it works, it is worth noting that the main benefit of sca  can likely be captured through iterative application of tl-bsla. once a root is set for tl-bsla, areas of the network that are essentially insulated from that root and its subnetwork  will not be considered. this is appropriate for classifier-based tasks, but for the purposes of learning a complete network from large-scale data suggests that by initiating the algorithm with a separate root that was not used in the initial inference, additional true positive edges are likely to be discovered , resulting in even further improved performance of the tl-bsla.

network learning performance is not sensitive to the choice of ffc
we found that the parameter that most directly affected structure learning results was ffc, used to determine which nodes are children of the current bottom layer nodes. it is a user-defined parameter with an optimal value that is possibly data-specific. in our simulations, we used ffc =  <dig>  based on our initial explorations and some empirical experience from previously published work  <cit> . however, it is important to assess the sensitivity of algorithm performance to critical parameters so that over-fitting and inappropriate comparisons are avoided. we assessed the accuracy of tl-bsla when varying ffc values over a factor of  <dig> . for example, for the child system with  <dig> samples, we found that the variation induced by different ffc values  was smaller than the variation induced by different datasets . this ffc-induced variation became even smaller as the sample size increased . under the constraint of small sample sizes, the variation induced by even substantial changes of ffc was thus not substantial. in fact,  <dig>  was not even necessarily the optimum value of ffc , but we used it successfully for four diverse datasets  being studied here. we also performed a similar parameter sensitivity analysis for sca to confirm that the results in fig.  <dig> were not due to poorly chosen defaults. this analysis is presented in additional file 1: figure s <dig>  showing that the performance of sca was not highly sensitive to its parameters and that changing parameter selections did not allow sca to perform as well on the child system as the tl-bsla.fig.  <dig> sensitivity analysis of ffc shows less significant impact than random variability. the child network was analyzed with  <dig> samples,  <dig> times each for ffc parameter values ranging from  <dig>  to  <dig> . the variability induced by changing ffc  is smaller than the variability from different random datasets being used for structure learning . this suggests that there is a broad optimum of ffc values and that the value used in this work is a reasonable one . tpr: true positive rate; fpr: false positive rate. error bars represent one standard deviation



generalization of the strategy by selecting a root
while one of the primary goals of our algorithm is to generate networks that explain some single important phenotype from a dataset , in some cases there may not be an obvious choice for that root node. existing training sets for classification problems typically do not include associated regulatory networks, and thus there is no way to assess the accuracy of networks we would predict for those training sets. instead, we needed to use training sets designed for assessing network learning, which meant that there would not necessarily be obvious roots for the networks. accordingly, we devised a strategy to identify a reasonable root node based on a statistical treatment of the specific dataset being analyzed. our root selection procedure, rootselection , resamples from the existing dataset and uses dataset permutations to identify a reasonable, statistically meaningful root for learning a tree-like bayesian network. the root is identified as the node that has the most significant mutual information with the true features relative to a set of randomly permuted features.

we used the child and alarm networks  to assess the performance of our dataset-specific, unbiased root selection approach. for each example, we considered the impact of varying the number of observations  for the features from  <dig>  to  <dig>  for each number of observations, we used  <dig> different randomly generated training datasets. the selected roots for each example are summarized in fig.  <dig> fig.  <dig> the tree-like bayesian structure learning algorithm can select a root for structure learning in tree-like or non-tree-like networks. roots were selected automatically for two representative networks across a range of sample size limitations: a the tree-like child network and b the non-tree-like alarm network. any node ever selected as a root has a red outline, where increasing line width indicates increasing frequency of selection as a root. nodes never selected as a root have blue outlines of fixed width. c a quantitative summary of the root nodes selected, as a function of sample size. selection from a tree-like structure is straightforward and consistent; from a non-tree-like structure there is increased variability, but reasonable roots  are typically chosen. feature  <dig> was used as the root for previous alarm network learning work. it is worth noting that selection of a better root could improve the tl-bsla’s tpr and ppv even further



the root selection approach performed quite robustly in the tree-like networks. for the child system , node  <dig> was consistently returned as the root for all sample sizes. even for small sample sizes , node  <dig> was selected as the root most of time. while node  <dig> is actually the real root, node  <dig> is obviously a reasonable second option for root selection based on the topology of the network. there was little sensitivity of selected root to sample size, which is particularly valuable for applications to small sample datasets.

for the alarm system, there was not such a strong consistency of root selection, though the results were still fairly insensitive to sample size. for different training datasets with the same number of samples, different root nodes were often selected; as shown in fig. 4b, the roots selected most often across all sample sizes were nodes  <dig>   <dig>   <dig> and  <dig>  only for a sample size of  <dig> was the selection of root nodes particularly variable. since the network topology of the alarm system is not tree-like, there is no single “correct” root that characterizes the entire network, and each of these seems on initial inspection to be a potentially reasonable selection, especially if directionality of edges is ignored. to recover the structure of the whole system, multiple trees with different roots could be combined together in some form . while that is an interesting future direction, here we focus on finding a single strong sub-network with constrained sample size to demonstrate the potential for using classifier-based algorithms to learn network structure.

data reduction for omics-scale datasets
for high-throughput and “omics”-scale studies, such as transcriptomics and metabolomics, datasets typically contain relatively few samples but thousands of features. in general, this can make it harder to identify the  features that are relevant to determining the phenotype or structure of the system because the desired signal may be buried in multivariate noise. for small sample sizes, a further problem is that supervised identification of the important features can often lead to overfitting, where some linear combination of features can explain even random results or classifications. moreover, increasing the number of features also increases the computational cost for essentially all types of data analysis. in order to make our structure learning method useful for such datasets, we developed an additional screening step  to exclude features likely irrelevant to the selected root. this method was not used in the above assessments, and is considered as a useful addition to the algorithm proposed in table  <dig> 

specifically, for each feature in the dataset , it is included in the network if it satisfies si ≥ threshold, where si is as used in the rootselection subroutine and is defined in equation . si represents the significance of the mutual information between fi and the root, given the specific distribution of data in fi. in this work we used a threshold value of  <dig> , based on statistical arguments, previous literature  <cit> , and empirical exploration. the s value is essentially a z-score on the mutual information of a feature with the root based on a background of permuted data for that feature; thus, a threshold of  <dig>  on a zero-mean, unit-variance normal distribution corresponds to a one-tailed significance of  <dig> , where a significance lower than the typical  <dig>  threshold was selected to limit false positives due to multiple hypothesis testing. changes in threshold change how conservative the feature inclusion is, and thus affect the true positive and false negative rates; variation of threshold was not explored due to its statistical interpretation.

we tested the performance of this screening step by adding 10-fold manually-created noisy features to the set of real features in the example datasets. these noisy features were generated via permutation of the observations within each real feature. using the child system as a representative example , with node  <dig> as the root, we found that over 50 % of the real features were included as significant features when the sample size was  <dig>  in contrast, only  <dig>  % of the noisy features were selected as significant. as the sample size increased, the percentage of real features selected for inclusion gradually increased to over 80 %, indicating that most of the real  had been correctly selected. interestingly, the percentage of noisy features remained at approximately 3 % even with an order of magnitude more samples. this again supports the idea that our overall structure learning approach is effective for the case of complex model systems with limited numbers of samples. results for the synthetic-tree and alarm systems  were similar to those of the child system, indicating that our proposed screening step can generally exclude noisy features that are irrelevant to the root across different types of underlying network structures. thus, once the root is determined , we can focus on the features most relevant to the root with a concomitant reduction in computational complexity.table  <dig> features in the child network selected for model inclusion using a dimensional-reduction screening procedure, with node  <dig>  as the root



application of tl-bsla to analyze transcriptomic data
to apply this approach to the analysis of real systems biology data, we used results from a recently-completed experimental challenge of five rhesus monkeys  with the malaria parasite plasmodium cynomolgi. transcriptional profiles were measured from bone marrow aspirate samples that were taken seven times over the course of three months after infection. we used these transcriptional profiles as the basis for the construction of a bayesian network. we used the recently described axes of common variation across large-scale population human cohorts  <cit>  to provide a more focused analysis on transcripts likely to be informative in describing the animals’ response.

figure 5a shows a representative inferred network from the data, demonstrating the flexible nature of the networks that can be inferred using tl-bsla: dependency relationships several levels deep are identified. we began with a simplified analysis using previously defined “blood informative transcripts” from previous work as best describing uncorrelated axes of variation in whole blood transcriptional profiling of healthy subjects  <cit> . there are seven main axes most strongly observed in both human and macaque samples ; we compiled together the ten blood informative transcripts for each of these axes for input to the tl-bsla. the genes in the network were selected using the dimensional reduction scheme described above, yielding a network of manageable size for visual and biological interpretation. the root was automatically selected from the data, using the approach described in the methods. there were two main branches in the tree: one branch almost exclusively consisting of axis  <dig> transcripts, and one that is a combination of multiple transcripts from axes  <dig>   <dig>  and  <dig>  while this network indicated potentially interesting relationships between the axes, it also suggested that deeper exploration by including more genes from each axis would help to better distinguish potential relationships from noise. we thus rebuilt the network from the same root instead using the top  <dig> genes from each axis. this deeper analysis of the axes made the relationships within the tree even more evident : axes  <dig> and  <dig> have a significant interaction with axis  <dig>  which is the root of the tree. each of these three axes has a branch almost exclusively consisting of only members of that axis, suggesting a coherent, significant relationship with the level of the root gene.fig.  <dig> tree-like bayesian networks learned from transcriptional data of a malaria challenge experiment in macaca mulatta. networks were learned using blood informative transcripts  <cit>  to focus on potential axes of variation in the transcriptional data. a using the ten blood informative transcripts as originally published, two branches emerge that best describe the root , consisting of other genes from axis  <dig> and a combination of multiple genes from axes  <dig>   <dig>  and  <dig>  b using the top  <dig> genes from each axis to build a network based on the same root, the relationship between the axes becomes even more evident, as both axis  <dig> and axis  <dig> contribute the dominant genes in parallel branches of the tree, suggesting significant but distinct mutual information with their parent and ultimately with the root. these relationships were not evident using standard multivariate and clustering analyses, and were not expected a priori based on previous descriptions of the axes of variation and the fact that the gene lists were derived from whole blood, not bone marrow aspirate, transcriptional profiling analyses



discussion
network structure learning is a useful tool to help identify unknown regulatory or causal relationships between variables  in a system. with the rise of systems-scale “omics” data over the past two decades, structure learning methods have been applied with increasing frequency to biological systems for the discovery of new modules, pathways, and regulation. however, in many cases the number of samples available in “omics”-scale studies is small, particularly in the case of complex model systems . in addition, while these techniques often include measurements for many variables or features , often only a small fraction of them are directly relevant to the phenomenon being studied. secondary or indirect effects may make many more genes appear to be highly correlated with each other and with a phenomenon of interest, and these indirect effects hinder the identification of the real regulatory relationships in complex systems.

a tree-like network inference approach based on classifier learning algorithms
to address the issues associated with network learning in complex model systems, we hypothesized that bayesian network-based classifiers that have been proven to be effective with few samples and with many features may, within their networks, have the potential to capture important regulatory structure in addition to their classification prediction. accordingly, we created a new structure learning method, the tree-like bayesian structure learning algorithm , which refined a previously demonstrated effective tree-like bayesian network classifier by removing limitations on network topology . we chose to focus on bayesian network structures because they typically provide a more succinct representation of regulatory interactions than correlation-based networks, and because the relationships between features are highly suggestive of direct interaction or regulation, each of which are valuable properties for driving validation experiments or mathematical modeling efforts.

iterative structure arrangement steps enable learning of network connections
in the tl-bsla, we improved upon the previous classifier-based approach in a number of ways. we refined the arrangement of nodes within branches to more accurately reflect the relationships between those nodes as opposed to just their  mutual information with the root node. this entailed developing a strategy to iteratively select nodes for inclusion in a branch and to arrange their topology in a manner reflective of likely interactions based on mutual information and conditional mutual information.

using four different networks as examples, we supported our hypothesis on the utility of network-based classifiers for learning regulatory structure via the effectiveness of the proposed tl-bsla to infer reasonable regulatory networks for a variety of different underlying topologies. for the systems with tree-like structures , even with a limited number of samples , the algorithm could recover most of the correct connections with a lower false positive rate than a commonly used structure learning algorithm, sca.

while it may not be surprising that an algorithm designed to learn trees can perform fairly well for underlying networks that do in fact resemble trees, we posit that it is still surprising that it performs substantially better than an existing, widely-used structure learning approach like sca. the underlying networks are sparse and are ultimately rather simplified bayesian networks; one would expect such networks to actually be fairly easy to infer for an algorithm like sca. it is certainly possible that the constraints of being a tree-like structure contribute to the ability of tl-bsla to infer accurate networks with fewer samples. nonetheless, given that tl-bsla can find more true positives with fewer false positives without any external information about the true root of the system, this suggests its potential wider utility, particularly for fairly simple networks.

in networks without a tree-like structure , the algorithm was still able to recover a substantial portion of the original network. more importantly, we found that the false positive rate of tl-bsla was much lower than sca, which itself typically has a low false positive rate  <cit> . this yielded a better positive predictive value for tl-bsla, which makes it a more effective strategy for learning networks of relationships in biological datasets under the constraint of small sample size.

addition of other functionalities enables broader applicability of the algorithm
we addressed the inclusion of nodes via a feature selection step in the algorithm. the previous classifier excluded features based on a user-defined parameter that eliminated a fixed fraction of the features based on mutual information with the root. while this certainly makes sense for learning a classifier , having a user parameter that so significantly affects the members and structure of a network is undesirable for network learning. we used statistical approaches to identify the features with statistically significant mutual information with the root, which retains most of the relevant features with fairly low inclusion of irrelevant features. as shown in table  <dig>  this screening step shows good performance in separating relevant and non-relevant  features.

we also addressed the issue of root selection to attempt to generalize the algorithm to network learning without a target phenotype to be predicted. we used mutual information-based statistical approaches to identify the best candidates for roots, with robust selection in the case of underlying tree-like structures and reasonable selection  for underlying structures that are not tree-like.

application to a malaria transcriptomics dataset provides leads on biological insight
we then applied this approach to a transcriptional dataset of bone marrow aspirates from a group of five m. mulatta infected with the simian malaria parasite p. cynomolgi. focusing on genes representing common axes of transcriptional variation, we applied all aspects of our network inference approach: selection of significant features based on mutual information relative to resampled and permuted data, identification of a root node based on the significance of the mutual information between the root and the rest of the features, and then learning of tree-like network structure. the algorithm automatically constructed a network that was deeper than it was wide , although multiple independent branches within the network were learned.

an initial network defined by the top ten most informative transcripts from each axis suggested a possible relationship between four of the axes; including more genes in the analysis, it was clear that three of the axes  had a significant relationship. each of them dominated a different branch in the network, showing that they had significant relationships to the root gene , but that the relationships for each axis to the root gene were different .

the axes of variation represent sets of genes that are positively co-regulated in peripheral blood data sets in humans, where each axis tends to capture an aspect of blood and immune system biology. notably here, axis  <dig> is enriched for b-cell signaling, axis  <dig> for erythropoiesis, and axis  <dig> for interferon signaling. these axes are not strongly evident in the bone marrow since the major blood cell types have not yet differentiated, but the tree-like bayesian network nevertheless recovers nascent relationships. it is particularly notable that axis  <dig> falls out as a separate branch, since there is no sign in these graphs of axis  <dig>  which largely captures t-cell signaling, nor of axis  <dig>  which is closely related to neutrophil activity and inflammation. we would thus argue that our algorithm is capable of capturing the earliest stages of cellular differentiation during leukopoiesis when seeded with genes that are markers for the mature cell types.

this finding is particularly noteworthy for a number of reasons. first, the axes of variation as originally derived had fairly low covariation with each other, with a few exceptions. however, none of those exceptions were observed here, and relationships between axes  <dig>   <dig>  and  <dig> were not previously observed. second, the axes were derived from whole blood transcriptional profiling, so there was not an expectation that the same variation should be seen in bone marrow aspirate transcriptional profiling. their observability in bone marrow aspirates supports broader utility of this approach to transcriptional analysis. finally, and most importantly, the relationship structure between axes  <dig>   <dig>  and  <dig> was not identified from standard clustering and statistical analysis of the transcriptional data . based on standard multivariate analyses there was not an obvious relationship between these axes; only through the consideration of conditional mutual information and a network of interactions between genes were we able to identify robust relationships between axes. thus, the bayesian, tree-like network analysis contributed uniquely to understanding and interpretation of the data.

thus, by identifying the likely expression relationships in our experiments of these genes revolving around related themes , the network-based analysis has contributed to interpretation of the data and ultimately to directing future efforts in our studies of the host-pathogen interaction in malaria using non-human primate models.

limitations and caveats
the requirement for our learned structure to be tree-like is an inherent limitation to our approach, as biological networks are not necessarily tree-like. there could be significant cross-talk or combinatorial regulation on a given node in a true biological network. however, the networks learned by tl-bsla are a reasonable approximation even to underlying networks that are not strongly tree-like. moreover, multiple trees inferred starting from different roots could potentially be pieced together to provide a more complex network representative of multiple subnetworks but that is not tree-like.  if nothing else, the tree-like network approach would serve as an excellent starting point for a search-and-score heuristic structure learning algorithm and would help to identify which subset of nodes should be included in such a search.

for our comparator algorithm we used sca, chosen since it is a well-known and widely-used structural learning algorithm with better avoidance of false positives than many other bayesian structure learning algorithms . countless other algorithms could have been used as comparators; nonetheless, the commonality to many of those algorithms is their inability to robustly learn networks under the constraint of small sample size. in this sense, sca is a reasonable representative of existing algorithms, and tl-bsla stands on its own as learning networks effectively under this constraint. moreover, an important goal of our work was to validate the hypothesis that methods developed for classifier learning could have significant potential for learning network structure, which we have demonstrated here even if there are other bayesian learning algorithms that perform slightly better than sca.

the idea that classifier learning could have significant potential for identifying network structure has been hinted at previously; in fact, one of the algorithms that the previous bn-tl classifier compared itself to explicitly notes the potential for identifying valid relationships between features   <cit> . however, this algorithm also restricted itself to a very flat topology making it difficult to find deeper, more complex regulatory relationships as is enabled by the tl-bsla.

we also note that our approach did not exploit the temporal aspect of the samples in constructing the network. this information could potentially enable improved structure learning, whether by exploiting the relationship of consecutive samples or by enabling connections between variables that represent regulatory loops as is possible using dynamic bayesian networks . however, robustly learning dynamic bayesian networks requires even more samples than learning general bayesian networks, which is counter to the goal of the tl-bsla.

finally, we note that for the transcriptional data, since feature selection and root selection are based on permutations and resampling, replicate runs can yield slightly different results. multiple runs were performed for the networks in fig.  <dig>  with the ones presented being highly representative of all of the runs; differences between runs were in the inclusion of a few different genes and resulting slight changes in topology at the bottom of a tree .

CONCLUSIONS
taking together the novel aspects of our tree-like structure learning algorithm with the validation on transcriptional data from a malaria challenge experiment in a non-human primate macaque model system, we have shown that bayesian network-based classifiers can be the basis for meaningful inference of regulatory network structure. the algorithm we designed for this task, tl-bsla, is an effective and useful algorithm for structure learning in systems biology data under the constraint of small sample size and is better than an existing, widely-used structural inference algorithm. we have demonstrated its efficacy for systems exactly meeting its tree-like assumptions, for systems that only slightly deviate from tree-like assumptions, and for systems that deviate substantially from tree-like assumptions. by including data-specific assessment of the significance of mutual information, we have enabled the identification of a reasonable root for an arbitrary dataset, as well as the identification and elimination of spurious features. we believe this approach has particularly significant promise for the integration of different types of datasets, where some molecular-level explanation  is desired that explains some observed phenotype  that can serve as the root of the tree-like structure. this represents a promising addition to the set of tools for probabilistic graphical model and bayesian structure learning, filling a need for high-confidence analysis of complex systems with few samples and many variables.

availability and requirements
project name: tree-like bayesian structure learning algorithm

project home page:http://styczynski.chbe.gatech.edu/tl-bsla 

operation system: platform independent

programming language: matlab

other requirements: developed on matlab r2011a; backwards compatibility unknown

license: freebsd

restrictions for non-academic use: none.

additional file
additional file 1: 
supplementary figures, tables, and methods. 



abbreviations
bn-tltree-like bayesian network classifier

fprfalse positive rate

ppvpositive predictive value

scasparse candidate algorithm

tl-bslatree-like bayesian structure learning algorithm

tprtrue positive rate

competing interests

the authors declare that they have no competing interests.

authors’ contributions

wy conceived of the study, implemented the structure learning algorithm, designed and carried out the computational experiments, and helped draft the manuscript. sg processed the transcriptional data, helped analyze the learned transcriptional network, and helped draft the manuscript. mrg and am designed and supervised the animal experiments providing the samples for the transcriptional dataset. mps conceived of the study, participated in its design and coordination, and drafted the manuscript. all authors revised the manuscript, and read and approved the final manuscript.

