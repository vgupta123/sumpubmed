BACKGROUND
time-lapse imaging of optically sectioned nuclear images has provided an unprecedented opportunity to observe biological processes as they unfold in space and time. using fluorescent proteins such as gfp to label nuclei, one can image the embryogenesis of diverse organisms such as c. elegans  <cit> , drosophila <cit> , zebrafish  <cit>  and mouse  <cit>  with single cell resolution over an extended period of time. given sufficient temporal resolution, individual nuclei can be followed over time, providing a virtually contiguous record of proliferation, differentiation and morphogenesis  <cit> . however, exploiting this source of information is not trivial: such data sets can record thousands of cells over hundreds of frames, adding up to terabytes of files. the data becomes manageable only when tracks of the behavior of individual cells are generated from the raw images.

in response to this, a variety of computational techniques and software packages have been developed to aid in quantitative analysis of images  <cit> . the largest class of image analysis methods focuses on segmenting contiguous regions of pixels, implicitly detecting nuclear locations in the process. the simplest technique, thresholding image intensity, has been supplemented with image processing techniques like smoothing, adaptive thresholds  <cit> , morphological operators, mode finding <cit> , watershed  <cit> and level set methods <cit>  to increase robustness in the face of noise, uneven contrast and touching nuclei. a second class of methods uses matched filters to detect the centers of nuclei. a predefined template of object appearance is compared to every location in an image and local maxima of similarity are assumed to be object centers  <cit> .

the success of these methods is mixed. when nuclei are widely spaced, many methods perform well, with error rates of one percent or less  <cit> . however, as nuclear density increases and nuclei start to touch, which is typical of late embryonic development and adult tissues, error rates rise to from two up to more than ten percent  <cit> . errors are caused by image characteristics that violate the rules or models underlying a detection method. these have their origin in both biology and the imaging process. variation in nuclear fluorescence and shape is the primary biological complication. uneven signal within a nucleus  can result in over segmentation, separate redundant detections of a nucleus on each mode of intensity. elongated and irregular nuclear shapes can also contribute to over segmentation . even if expression is uniform, each end or bump may independently match some computational definitions of a nucleus. biology also contributes to under segmentation or missed nuclei. when nuclei are crowded, nuclei with weaker expression may be masked by brighter neighbors and remain undetected .

imaging distortions and resolution limitations further complicate nuclear detection. an optically sectioned sample yields a series of 2d cross sections, so that nuclei are recorded as slices through bright globular shapes against a dark background. coordinates within the 2d planes are referred to as x and y while z refers to the direction orthogonal to the image planes. optical and physical constraints make resolution along the z axis lower than that within the x,y plane. typically, phototoxicity and image acquisition time also limit the number of optical sections, further reducing the z resolution. in turn, this limits the information available to resolve closely packed nuclei along the z axis . systematic distortions such as fading of signal with depth and stretching of fluorescent signal along the light path  further distort the picture, contributing to error.

all errors are problematic because the quality of information extracted during image analysis limits the kinds of biological questions that can be answered. a low percentage of error, say three or five percent, allows reliable assessment of statistical trends in the behaviors of a homogeneous group of cells, such as drug response in cell culture, or the shape and overall migration of a tissue. on the other hand, this level of accuracy is not always sufficient for detailed analysis of embryogenesis, where tracing the behavior of single cells is often necessary. investigation of many critical developmental processes such as neural crest cell dispersion <cit>  or convergent extension  <cit>  can be most effectively investigated with this kind of single cell record of development. though desired information may be theoretically present in images, it is large-scale annotation of cell behavior that makes systematic investigation possible. tracing the paths of cells as they move and divide demands virtually 100% accuracy, as a handful of misidentified cells per time point can lead to a thoroughly fragmented and scrambled lineage  <cit> . to make use of error filled data, biologists must spend a significant amount of time manually editing the automatically constructed result. even for simple organisms with a few hundred cells, such as c. elegans, this has previously taken up to days  <cit> . in an organism like zebrafish this amount of editing would be an impossible task, with correction of even a sublineage of interest being weeks, or months, of effort. errors quickly make human curation a bottleneck, undermining the premise of high throughput imaging. often, when combined with time constraints, this inability to accurately find cells forces one to detour around a key question, missing an opportunity for a clean experimental design.

to address this challenge we present a detection and segmentation method that is accurate enough to allow high fidelity analysis over a variety of images while remaining fast enough to run in real time, making it practical for use with large data sets.

RESULTS
algorithm design
given that image planes are widely spaced along the z axis, adjacent and similar voxels along the z axis are fairly likely to belong to separate nuclei, frustrating naive detection and segmentation methods. a shape model is necessary to guide segmentation, filling in boundary information that is not locally available in the image. our method views nuclei as a collection of slices and uses a shape model that consists of expectations about slice size, brightness and location relative to other slices .

this basic shape model is sufficient because nuclei have relatively simple, largely convex shapes. pixel level segmentation is done within 2d image planes, a shape model is not necessary because resolution is high . in contrast, the final definition of 3d nuclear extent, which must be done under the constraint of limited resolution along the z axis, is based on pre-segmented slices, allowing computationally efficient use of a model of nuclear shape . this approach avoids the impossibly high computational cost of fitting complex shape models, like the active shape models  <cit>  commonly used to segment noisy volumetric data, to hundreds of thousands of nuclei.

image filtering
our approach begins with image filtering to find an initial set of seed locations for nuclei. volume data is filtered with a 3d difference of gaussian's  filter  <cit> , which can be viewed as a matched filter representing a blurred sphere against a dark background. as in typical use of such filters, 3d maxima of intensity in the filtered image serve as candidate nuclear centers for further analysis. at the same time, the filtering process also highlights information about the individual nuclear slices: 2d local maxima within each image plane represent the centers of nuclear slices, and the edges of nuclei are highlighted as zero crossings of intensity . see additional file 1: supplemental methods section  <dig>  for more details.

slice segmentation
based on the filtered volume data, nuclear slices are extracted as polygonal regions. for every 2d intensity maxima above a base threshold a 2d shape representing that slice through the nucleus is segmented. following our goal of reducing computational complexity, 2d segmentation is reduced to a set of 1d boundary detection problems. sixteen evenly spaced rays are sent out from the 2d maxima. these rays terminate when they encounter a zero-crossings . ray endpoints are then post processed to discard rays that are unusually longer or shorter than their neighbours. these final ray endpoints define a polygonal segmentation boundary that can capture detailed shape, but is computationally very cheap to compute. see additional file 1: supplemental methods section  <dig>  for more details.

nuclear extraction
we then combine extracted slices and the subset of these slices corresponding to 3d maxima to segment nuclei. the slice corresponding to each 3d maximum attempts to claim slices above and below it based on a trained probabilistic model of nuclear shape. the underlying shape model is a set of  <dig> dimensional gaussian distributions, generated using labeled training data, and representing the expected properties of slices within a nucleus and of 'distractor' slices originating from other nearby nuclei. dimensions of this distribution include the relative size, intensity and position of slices in relation to the center slice and the closest intervening slice . training the model is simple, a superset of slices that might be part of the nucleus is generated automatically and extra slices are deleted to create a corrected result . this model is used in a simple maximum likelihood classifier that assesses whether each nearby slice along the imaging axis is more likely to belong to the nucleus or another nearby 'distractor' nucleus. this shape model, though simple, is flexible enough to capture different levels of shape variation, nuclear separation, and optical distortion, making it adaptable to different organisms and microscopy techniques. see additional file 1: supplemental methods section  <dig> - <dig>  for more details.

finding overlooked nuclei
when crowded, nuclei with weaker fluorescence intensity are often overshadowed by brighter neighbours, and so are not 3d maxima. conceptually, we can imagine masking the signal from known detected nuclei. when this is done dimmer nuclei become local maxima. the segmentation of all nuclear signal into the unit of slices provides a practical way to achieve this. all slices claimed by at least one nucleus are marked as accounted for and the remainder examined. in any 3d spherical neighbourhood where multiple unclaimed slices exist, an overlooked nuclear center is seeded at the locally brightest slice. this works because the shape model is accurate enough to prevent the brighter, initially detected nuclei from claiming the slices corresponding to these undiscovered dimmer nuclei. these nuclei are extracted from the full set of slices  subject to the same nuclear shape model above, and this is repeated iteratively until no unclaimed clusters of slices remain. see additional file 1: supplemental methods section  <dig>  for more details.

conflict resolution
false positives come largely from multiple detections of the same nucleus caused by variation of intensity within the nucleus. false positives from noise are rare because of the strong smoothing provided by filtering. most false positive cases are revealed by significantly overlapping claims on slices from multiple  nuclei. these cases are judged by a conflict resolution step. whenever two extracted nuclei both claim a slice, two possibilities are considered, merging the two nuclei, and splitting their overlap between them. these two configurations are scored against the shape model and the option with the best shape score is picked. merging is scored by using the shape model to calculate the total score of all slices in a nucleus formed from the union of all slices in both nuclei, assuming its center to be the slice closest to the geometric middle of the merged set of slices. the second possibility, splitting, is scored by assigning each slice to the nucleus with the strongest claim on it, subject to the constraint of each nucleus being contiguous and adding up the total of the two nuclei's claim on their slices. see additional file 1: supplemental methods section  <dig>  for more details.

image analysis software
matlab source of the image analysis software is available as additional file 2: source code and is distributed under the gnu gpl. the source will be actively maintained; the most recent version is available for download at sourceforge .

though the algorithm contains a significant number of parameters, the set of key parameters is relatively small. it is typically possible to get good results by setting only two intuitive parameters: the diameter of the nucleus in the first frame, used to set the filter size, and the noise threshold used to discard filtered maxima corresponding to image noise. a full catalogue of all parameters  and advice on tuning them for new images is provided in additional file  <dig>  section  <dig> . an example parameter file along with a script and instructions for retraining the 3d shape mode is provided in additional file 2: source code.

evaluation of accuracy
we applied our method to a diverse set of test data, encompassing both confocal and light sheet microscopy, and sampling a range of metazoan model organisms. data sets include in toto c. elegans using laser scanning confocal microscopy  <cit> , in toto drosophila using dlsm-si  <cit> , in toto zebrafish using dlsm  <cit>  and a partial mouse embryo using laser scanning confocal microscopy . for c. elegans, we analyzed  <dig> time points covering roughly five hours of development and ranging from the four-cell stage to about  <dig> cells. for each of the other data sets, sub volumes containing 200- <dig> cells were selected, due to the time constraint of gathering the ground truth by manual identification of nuclei when the guidance provided by an invariant lineage is not available. for drosophila and zebrafish data, which cover an extended period of development, an early and a late developmental stage were tested. we analyzed drosophila stage eight and eleven  and with ~ <dig>  and  <dig>  total cells respectively) and the zebrafish late "1k cell" and "14- <dig> somites" stages . the mouse embryo is analyzed at the late headfold   <dig> ) stage with all non-axial mesodermal nuclei labeled. when selecting the sub volumes, we chose regions with above average difficulties in the respective data in terms of nuclear density, variation of nuclear shape, size and intensity. parameters were tuned on one time point and then tested on three successive time points for the same test sub volume.

all error rates were calculated based on ground truths created by human correction of all discernable detection errors in the computed result. segmentation accuracy was not considered, as our primary goal is localization of nuclei, segmentation is largely a way to increase the accuracy of that localization. however, some segmentation results are shown in figure  <dig> and appear to be of good quality. for c. elegans the detection ground truth can be considered perfect, as the invariant lineage provides a guide in resolving any temporary image ambiguity. for all other data sets the ground truth was generated by examining the volume data slice by slice several times, deleting multiple detections and marking overlooked nuclei. the ground truth was automatically matched against the original result and deviations logged as errors. computed results and ground truths are available as additional file  <dig>  c. elegans image data is available on request because of its large size; images for other organisms are included as additional files  <dig>   <dig>   <dig> and  <dig> this additional data is release under the gnu gpl. average error rates range from near zero for early zebrafish and c. elegans to around  <dig> to  <dig> % in drosophila and late c. elegans . these error rates are about two to six fold lower when compared to previous approaches. for early development in c. elegans, our method achieves  <dig> % error around the 180-cell stage, compared to  <dig> % reported for data of equivalent quality with a mode finding approach  <cit> . our approach also compares favorably with  <dig> % error reported using a graph cut segmentation method  <cit>  on another highly similar dataset. performance on later developmental stages was not reported for either of these methods. for late c. elegans development, our method achieves ~ <dig> % between the 180- to 350-cell stage  and ~3% afterwards . our previous matched filter method  <cit>  yielded ~3% and 12% error respectively at these stages using the identical test data and ground truth. error on the zebrafish data set is a third or less  than that of the adaptive thresholding method originally used on the same data set  <cit> . while our assessment is based on a portion of the whole volume used in prior error analysis the sub volume is one of the most crowded areas in the image.

we further analyzed how different components of our algorithm contribute to the final accuracy of the results. we analyzed one of the data sets that gave the highest error rate, namely the late stage c. elegans embryo from about  <dig> to  <dig> cells. the breakdown in table  <dig> illustrates the contribution of the shape model, both in finding missing nuclei in the form of unclaimed slices, and in merging redundant overlapping segmentations of the same nucleus. these steps reduce both false negatives and false positives by about a half from the initial 3d blob detection.

error rates compared to ground truth for the partial result available at each stage of the algorithm.

in analyzing error rates we found that accuracy correlates well with the separation of nuclei in z across organisms and imaging methods . error is close to zero when the average separation is greater than or equal to one plane. this means that, on average, gaps between nuclei are captured by one or more planes. error rises sharply when the average separation is less than half a plane. in contrast, sampled slices per nucleus, an intuitive measure of visibility, does not predict accuracy . these results emphasize that, because the gap between nuclei is typically much smaller than nuclear size, resolvability  of nuclei bounds performance. this suggests that for quantitative analysis z sampling should be carefully tuned in response to the spacing between nuclei. in spite of this, our experience suggests that experimenters tend to be less attentive to this parameter, compared for example to laser intensity, likely because image quality within a plane is more perceptible. the mouse data is an exception to this trend, z sampling is more than adequate but error is higher than expected because the x,y resolution is unusually low . this causes occasional segmentation errors and somewhat higher than expected error . a qualitative sense of analysis results can be gained from additional files  <dig> and  <dig>  3d reconstruction movies of the mouse and zebrafish embryos generated from the unedited output of our system.

computational cost
to assess running times and memory loads, we tested our method on a whole volume of each data set on a  <dig>  ghz intel core <dig> pc using a lightly optimized single threaded matlab implementation . the runtime consists of several components, with disk access and image filtering scaling with image size while slice and nuclear extraction scale with the number of nuclei present. in larger images computational time is strongly dominated by image filtering, which contributes to 83% of runtime for the zebrafish dataset. for the c. elegans, drosophila and mouse data, the processing speed is about  <dig> seconds per megapixel of image data. for the zebrafish data, the speed is reduced to approximately  <dig> seconds per megapixel. this is likely due to memory management inefficiencies resulting from the need to divide the image filtering into multiple parts that fit within addressable memory on a 32bit architecture.

it is worth noting that, even in this unoptimized implementation on limited hardware, our algorithm is fast and efficient enough for real time analysis of all data sets but zebrafish. as the core image filtering is highly parallelizable and memory management seems to take a large toll in our tests, a parallelized implementation on a fairly typical multicore 64bit workstation with sufficient memory would be qualitatively faster. this should allow the processing of larger volumes such as the zebrafish data in real time with a few cpus. the computational cost of our method compares favourably with previous work. our method takes ~ <dig> seconds per volume to process c. elegans data, well below the data sampling rate of one volume per minute. in contrast, mode finding  <cit>  takes twice as long, on a volume less than half the x,y resolution using comparable hardware. our detection combined with our previous tracking approach  <cit> , takes about  <dig> seconds per volume to detect and track through the  <dig> cell stage, compared to up to  <dig> minutes per volume at the  <dig> cell stage for combined tracking and segmentation with a graph cut approach  <cit> . our method is also efficient in comparison with previous zebrafish analysis methods both of which would require around two hours  <cit> , to process a volume of the size that our method can segment in approximately  <dig> minutes.

3d reconstruction of pharynx development
we demonstrate the potential of our algorithm by analyzing the organogenesis of the pharynx in c. elegans. the pharynx is a prime model for organogenesis and has been widely studied. however, a detailed single cell level record of its formation has never been made. this is because of the image analysis challenge presented by small cells in a crowded configuration during later embryogenesis. with our previous cell detection method  <cit>  high error made this analysis impractical. more accurate detection opens the door not only for this record of wild type development, but also for novel experimental investigations of late pharynx morphogenesis at the single cell level.

the pharynx is a feeding apparatus that ingests and grinds bacteria. it is made of  <dig> cells, derived from different lineages and including multiple tissues such as muscles, neurons and glands  <cit>  . this complex set of tissues with a small set of cells has made the pharynx a powerful model to study organogenesis. genetic and functional genomic analysis have identified the key signaling pathways, master regulators and the molecular cascade underlying pharyngeal development, leading to a substantial understanding of how the cell lineage generates the particular set of  <dig> differentiated cells . however, as in most models of complex organogenesis, how the differentiated cells give rise to the structure of a functioning organ is poorly understood.

we have reconstructed pharyngeal development up to the stage where structures corresponding to the parts of the fully formed pharynx can be visually identified in the embryo . visualized in 3d, early morphogenesis of the pharynx appears to involve two distinct stages. during the first stage, pharyngeal precursor cells are recruited from discrete regions of the embryo to form a coherent structure with an overall left-right symmetry . pharyngeal cells are derived from the ab and ms lineages, with the ms cells born in a contiguous structure and the ab cells assembled piecemeal. in the ms lineage, pharyngeal precursors are born in two rows, one on the left side and one on the right . the two rows are born next to each other and the midline corresponds to the future midline of the organ, around which the ab cells assemble. the ab cells can be further divided into two groups. the right side group is derived from the abara sublineage. cells in this group  are born next to each other  and maintain their relative positions as they move towards the midline to meet the left side group . in contrast, the corresponding cells in the left side group  are born isolated and migrate towards each other to assemble a mirror image of the right side group . in the meantime, the pharyngeal precursors move from the ventral surface to the inside of the embryo. this process starts with the ms cells at around  <dig> minutes pfc . the ab cells first move on the ventral surface towards the midline  to cover the ms cells  before following them inside . this stage of morphogenesis, which we term the assembly stage, ends at ~ <dig> minutes pfc. the end result is a contiguous primordium consisting of two flat sheets of cells and an overall left-right symmetry. this is highlighted in figure 5iv frame a, which marks the correspondences between pharyngeal cells from symmetric lineages.

during the second stage of morphogenesis, which we term the inflation stage, the flat, two-sheet structure swells, similar to the inflation of a balloon, to create a rounded structure. figure 5ii and additional file  <dig>  movie  <dig> illustrate the process with a color scheme that shows the mapping of the primordium to the mature pharynx   <cit> . interestingly, shortly before the inflation starts , the primordium is aligned with the anterior-posterior axis between the anterior end of the embryo and the intestine . from the ventral view, this configuration is similar to that of the mature pharynx, creating a false impression of the mapping to the final structure. as the left view shows, the longitudinal axis of the digestive tract is curved and deviates from the long axis of the whole embryo, that is, the anterior-posterior axis. the inflation starts around  <dig> minutes pfc  and becomes prominent by  <dig> minutes. it starts from the middle of the primordium with no obvious bias towards the future anterior or posterior lobe. the relative position of cells largely remains constant during the inflation. a dramatic exception is the e2v cell , which moves anteriorly from the middle of the primordium to join the other epithelial cells that make the buccal cavity. we also followed the symmetry of the primordium over time. the mature pharynx shows a threefold rotational symmetry, while the cell lineage and the primordium show largely bilateral symmetry. as sulston pointed out, "the third symmetry element arises  by piecemeal recruitment of cells", and the placement of these cells in the lineage does not show any apparent logic or regularity  <cit> . as shown in figure 5iii and additional file 12: movie  <dig>  these cells  are born at or near the midline along the length the primordium, on both the dorsal and the ventral sides. thus, the logic controlling which cells are recruited for the 3d symmetric structure is apparently spatial. additional file 13:movie  <dig> illustrates the final 3d configuration of cells colored by lineage origin, anterior/posterior and left/right fate, allowing their systematic comparison.

more detailed temporal and spatial information is available in movies corresponding to different coloring schemes in figure  <dig> . as individual cells within the organ can be analyzed based on our accurate nuclear identification method, morphogenic behaviours can be dissected at single-cell resolution using mutants, gene expression mapping and other approaches. such studies will ultimately extend the molecular cascade of pharyngeal development from differentiation to morphogenesis, and pave the way for a comprehensive understanding of organogenesis.

discussion and 
CONCLUSIONS
our method is a reliable tool for nuclear identification that, by respecting the structure of the underlying image data, achieves robust and fast nuclear detection and segmentation. three key ideas underlie the strength of our design. first, we separate the problem of 3d nuclear segmentation into 2d slice segmentation and 3d slice grouping. we can achieve reliable 2d segmentation using simple methods because of the high resolution within image planes. we can then efficiently solve the hard problem of 3d segmentation by grouping the relatively small number of slices in the image . second, we employ a trainable, probabilistic shape model based on slices, which allows us to consider the variability of nuclear shape and intensity within an easily manageable framework. third, we use the 3d maxima generated by the dog filter to guide nuclear segmentation. as 3d maxima typically provide >95% accuracy in nuclear detection, they provide a powerful guide for a greedy slice grouping approach. the strategy of segmenting individual slices of volume data is not unknown  <cit> , but our method is unique in its computational strategy and in being general, fully automatic and capable of good results in the crowded images typical of late embryogenesis. the algorithm remains efficient and fast enough for a typical computer to achieve real-time analysis of in vivo imaging of metazoan embryogenesis, in models ranging from c. elegans to mouse.

aside from accuracy and speed, our method has the advantages of easy adaptability and an extensible modular framework. retraining the probabilistic nuclear model for new images provides increased flexibility with minimal effort. since slices can typically be segmented with >95% accuracy after setting only  <dig> simple base parameters, manual labeling involves only pruning a list of nearby potential slices. no hand tracing of image regions is necessary. detailed tuning and training procedures are given in additional file  <dig> section  <dig>  apart from the adaptability of the probabilistic model, our method has the added flexibility of a modular algorithmic design. each sub task is largely independent of the others and can be replaced by other methods independently of the general strategy. for example, if very differently scaled or shaped nuclei were a concern, the dog filter could be replaced by a  battery of oriented or multi-scale filters while leaving the rest of the framework untouched. additional specializations can also be added at different stages to address particular concerns, such as false detections outside the boundary of an embryo or adaptation to fading signal.

currently, our method still produces 2-3% error in the more challenging cases of late embryogenesis. the vast majority of remaining false negatives are crowded nuclei for which secondary recovery failed because a brighter neighbor not only prevented its detection as a 3d maximum but also mistakenly claimed all of its slices. similarly, the majority of remaining false positives are nuclei split into upper and lower halves along the z axis. these were detected twice but did not claim each other's slices sufficiently to be merged. this suggests that the classifier for slice inclusion is likely the weakest link in our system and could be improved, especially for the elongated nuclei with varying spatial orientation that are frequently seen in the drosophila and mouse embryo. however, at least half of these error cases are ambiguous to the eye, and require examination of adjoining time points to determine if they represent one or two nuclei. this suggests performance of our algorithm may be close to the bound set by information in a single image, and large future improvements may be more easily achieved through improvements to imaging and use of temporal tracking.

as our analysis on the relationship between the error rates and nuclear separation  suggests, the most direct imaging improvement for detection in most situations would be an increase in z resolution. additional resolution in the x,y plane or improved signal to noise ratio are always useful, but if x,y resolution is already sufficient for segmentation this will not significantly reduce error and might be counterproductive if, for example, it results in increased phototoxicity due to greater magnification or laser power. our results provide a practical guide for optimizing imaging parameters: ensure a z sample spacing of at least the separation between nuclei. though acquisition speed and other constraints will not always allow a sufficient z resolution, the curve in figure  <dig> allows these factors to be traded off against error in an informed manner. furthermore, because nuclear separation is a universal metric for fluorescence labeled nuclear images, it is useful in comparing image analysis techniques across different image data. for example, this metric highlights the counterintuitive fact that the ~ <dig> hpf zebrafish embryo with over  <dig> cells is about as easy to analyze as the twenty four cell c. elegans embryo.

