BACKGROUND
in recent years, various groups studied the problem of developing classification models based on examples annotated by multiple labellers. the labels we integrate come from not only human beings  but also machine-based classifiers .

from the methodology perspective of the multi-annotator problem, one line of research focuses on annotator filtering by identifying and excluding low-performing annotators  <cit> . the other line of research aims at a single consensus label by aggregating labels from multiple annotators  <cit> . both strategies demonstrate significantly improved performance against single-annotator strategy and majority voting baselines.

learning from multiple annotators is also applied to bioinformatics. for example, manually labelled data is successfully used together with mathematical models to provide annotator-specific accuracy estimates based on multi-annotator agreement  <cit> . in computer-aided diagnosis , many computer-aided image diagnosis systems  <cit>  were built from labels  assigned by multiple physicians who provide their estimations of the gold standard, which can only be obtained from dangerous surgical operations. also, valizadegan et al.  <cit>  developed a probabilistic approach for learning classification models from opinions provided by multiple doctors and applied the approach to heparin induced thrombocytopenia  electronic health records . in the prediction of protein disorder, meta-learning is commonly used . meta predictors are typically developed relying on disorder/order labelled training datasets. these datasets contain a very small number of proteins which have not already been used for development of the component predictors. in addition, there is a potential problem of over-optimization for the meta predictors when combining information from multiple components. in contrast, here a meta predictor is constructed in a completely unsupervised process without use of confirmed disorder/order annotations  <cit> .

in this study, we learn a classification model using multiple noisy labels obtained by multiple annotators. specifically, we address a scenario where novice annotators are dominant. our method for integration of multiple annotators by aggregating experts and filtering novices will be called aefn. based solely on the information obtained from the good annotators, in an iterative process our method evaluates annotators to exclude low-quality ones followed by re-estimation of the labels. in a scenario considered in our study the noisy annotations are obtained by a combination of humans and existing classification models. therefore, the new method is applicable to many biomedical problems.

compared to previous studies, the uniqueness of our study lies in the following aspects:

• the aefn algorithm combines the removal of some annotators with labelling based on consensus of the remaining annotations. this is achieved without using any ground truth information.

• it provides estimates of good annotators' accuracy in addition to removing novice annotators.

• it is applicable in situations where annotators' accuracy varies across the data subsets which are not the case with previously proposed solutions .

• compared to our previous study  <cit> , aefn algorithm is explored in more details by conducting additional experiments on prediction of protein disorder on casp <dig>  data. the new experiments with machine-based classifiers provide a complementary characterization to experiments on human annotators reported at the preliminary version  <cit> . in our solution, a combination of noisy annotations obtained by humans and existing machine-based classification models were integrated. therefore, aefn has the potential to be applied as a solution to many biomedicine and bioinformatics problems.

• based on aefn algorithm, a way of deciding which annotator is more appropriate to label new instances has been investigated in our experiments. this is potentially beneficial in any situation where annotating instances is expensive.

methods
given a dataset d={xi, yi <dig>  ..., yir}, where xi is an instance, yij∈{ <dig> } is xi's corresponding binary label which is provided by the j-th annotator. for multi-annotator problem the task is to get an estimation of the unknown true label yi.

majority voting , a commonly used approach for this problem, has a limitation that the aggregated label for an example is estimated locally by only estimating the labels assigned to that example and not considering the performance of the labels for other examples.

in order to solve that problem,  <cit>  introduced an map-ml algorithm. as  <cit>  proposed "map-ml algorithm models the accuracy of the annotator separately on the positive and negative instances. if the true label is one, the sensitivity  αj for the j-th annotator is the probability that the annotator labels it as one: αj =pr. on the other hand, if the true label is zero, the specificity  βj is the probability that annotator labels it as zero: βj=pr. then map-ml corrects the annotator biases by jointly estimating the annotator accuracy  and the hidden true label." for details of map-ml, please refer to  <cit> .

map-ml implicitly assumes that the performance of the annotators  doesn't depend on the examples. to fix this problem, gmm-mapml algorithm takes into account that the annotators are not only unreliable, but may also be inconsistently accurate depending on the data. as  <cit>  mentioned "gmm-mapml models the annotators to generate labels as follows: given an instance xi to label, the annotators find the gaussian mixture component which most probably generates that instance. then the annotators generate labels with their sensitivities and specificities at the most probable component." for details of gmm-mapml, please refer to  <cit> .

our previous study  <cit>  goes further. as  <cit>  argued "recent experiments show that in some cases, a consensus labelling of a few experts will achieve better performance  <cit> . to further characterize the behaviour of annotators, we define the ranking evaluation score as sj=|αj+βj -1|. random annotations result in sj near zero, while perfect annotations correspond to sj= <dig>  based on the ranking evaluation score, we propose an aefn algorithm by extending the gmm-mapml. in each iteration, ml estimation measures annotators' performance at each mixture component . then, we add a step to filter the low-quality annotators at each gaussian component according to the score : if skj is smaller than a pruning threshold, we filter the j-th annotator from the pool of annotators at the k-th gaussian component. thus, we refit the map estimation with only the good annotators and get the updated probabilistic labels zi based on the bayesian rule." the algorithm is summarized at algorithm  <dig> while details are provided at a preliminary version of this study  <cit> .

algorithm 1: aefn algorithm

input: dataset d={xi,yi <dig> ...,yir}i=1n containing n instances. each instance has binary labels yij∈{ <dig> } from r annotators.

1: find the fittest k-mixture-component gmm for the instances, and get the corresponding gmm parameters and components responsibilities τik for each instance.

2: initialize Λk={ <dig> …,r} the sets of good annotators for each gaussian component k= <dig> ...,k.

3: initialize zi= ∑j=1ryij based on a majority voting.

4: initialize iteration indication iter← <dig> 

5: repeat

6:      

7:      ∀j∈Λk, update the sensitivity αkj and specificity βkj as follows

 αkj=∑i=1nzikyij/∑i=1nzikβkj=∑i=1n/∑i=1n 

8:      update the prior probability pi as σ.

9:      

10:      if iter> <dig> 

11:           for all k= <dig> ...,k  do

12:                for all j∈Λkdo

13:                     update skj=|αkj+βkj-1|.

14:                     if skj<ξ  then

15:                          Λk←Λk-{j}

16:                     end if

17:                end for

18:           end for

19:      end if

20:      

21:      ∀i= <dig> …,n restricted to the annotators in the set Λk instead of integrating all r annotators, estimate zi as follows

 zi=aipiaipi+bi 

where

 pi=pr=σ 

 ai= ∏j=1ryij1-yij 

 bi= ∏j=1ryij1-yij 

 q=argmaxk= <dig> ...,k 

22: iter←iter+1

23: until change of zi between two successive iterations< ξ.

24: estimate the hidden true label yi by applying a threshold  γ on zi. that is, yi= <dig> if zi> γ and yi= <dig> otherwise.

output:

• detected low-quality annotators of all gaussian components in set { <dig> …,r}-Λk.

• good quality annotators of all gaussian components in Λk with sensitivity αkj and specificity βkj, for j∈Λk, k= <dig> ...,k.

• the probabilistic labels zi and the estimation of the hidden true label yi, ∀i= <dig> …,n.

all multi-annotator algorithms are unsupervised meaning that integration of noisy labels is achieved without using true labels. following properties differentiate the proposed aefn algorithm from alternative multi-annotator approaches :  it integrates labels globally ;  it is data-dependent ; and  it filters novice annotators . also we summarize the properties of all multi-annotator algorithms in the table  <dig> 

the comparisons of properties of multi-annotator algorithms are shown. 'y' denotes that the algorithm has the property; 'n' denotes that the algorithm doesn't have the property.

RESULTS
in this section, we intend to validate the proposed aefn algorithm by doing experiments on a biomedical text classification task and a protein disorder prediction task. the protein disorder prediction experiment with machine-based classifiers provides a complementary characterization to the usage of human annotators reported in the biomedical text classification experiments.

biomedical text classification experiment
in the experiment, we used a  <dig> -sentence scientific texts corpus from rzhetsky et al.  <cit> . for details of data pre-processing and experimental settings, please refer to  <cit> .

in the preliminary version of this study  <cit> , we showed that our aefn was slightly better than gmm-mapml, while it significantly outperformed other competitors, when all annotations were from experts. using the same settings, our aefn also selected a three-component gmm model with covariance matrix λdkadktfor the biomedical text data. shown in table  <dig> and table  <dig> are the filtered annotators and estimated sensitivity and specificity of each good annotator on the evidence classification task and focus classification task for each component. for the evidence classification task, annotator  <dig> has been filtered in the 1st and 3rd components, and annotator  <dig> has been filtered in the 2nd component. for the focus classification task, annotator  <dig> has been filtered in all three components and annotator  <dig> has been filtered in the 2nd and 3rd components. the tables show that for different tasks the annotators perform in different manners. for example, annotator  <dig> is good at the evidence classification task, but not at the focus classification task. in addition, we found that the five annotators had comparable overall quality, and on average only one per component was eliminated. these results are consistent with the results of our preliminary version of this study  <cit> .

the estimates by five annotators for three principal components on the text evidence task are shown.

the estimates by five annotators for three principal components on the text focus task are shown.

in  <cit> , we also showed that our aefn has much better aucs than all competitor methods, especially when low-quality annotators dominate . to further characterize our aefn method on annotator-performance estimation, we designed another experiment on the same biomedical text data as follows:  find the fittest k-mixture-component gmm for all instances by using step  <dig> of aefn. as discussed in the previous paragraph, we found a three-gaussian-component model for the text data.  randomly split 40% of instances as training data and the remaining 60% as testing data.  on training data, estimate annotators' performance and identify the best annotator for each gaussian component by using our aefn method. here, we used the estimated ranking evaluation score as the criterion  to choose the best annotator. for the evidence classification task, annotator  <dig> was the best for the first component, annotator  <dig> was the best for the second component, and annotator  <dig> was the best for the third component. for the focus classification task, annotator  <dig> was the best for both the first and the third components, and annotator  <dig> was the best for the second component.  on testing data, we compare three logistic regression classifiers: a) randomly selected annotator that for each training data point used a label obtained by a randomly picked annotator among the five available annotators; b) aefn indicated annotator that for each training data point picked an annotator based on the suggestion from ; c) ground truth that is trained using an approximation of ground truth labels defined by the majority vote of the eight annotators' labels as previously discussed. the accuracies of these classifiers were compared according to 5-fold cross-validation on the 60% testing data. the purpose of using the 40% training data is to obtain annotator suggestion for aefn indicated annotator classifier.

the roc comparisons for three logistic regression classifiers on the evidence and focus classification tasks are shown in figure  <dig> and  <dig>  respectively. the figures show that when using the annotator's labels suggested by our aefn method, a simple logistic regression method clearly outperforms the classifier trained using labels chosen randomly from five available annotators. the results show that our aefn method can rank annotators by instance, and can help decide which annotator is more appropriate to label new instances. this is an interesting and important potential in the situation where annotating instances is expensive.

protein disorder prediction experiment
treating an individual predictor as an annotator, the multi-annotator methods can be used to build meta-predictors for protein disorder prediction. in this section we experimentally validate the proposed algorithm on the casp <dig> protein disorder prediction task. casp <dig> data  <cit>  consists of  <dig> experimentally characterized protein sequences with  <dig>  disordered and  <dig>  ordered residues. to reduce prediction noise due to experimental uncertainty, we didn't consider disorder segments shorter than four residues in the evaluation process. we selected  <dig> predictors developed by groups at different institutions, assuming that their errors are independent. therefore we can treat them as individual annotators.

in the study, a feature vector  of each residue was derived from the subsequence covered by a moving window centred at the current position. of the  <dig> dimensions, the first  <dig> features come from amino acid frequencies composition and the last one is a local sequence complexity feature . for details of amino acid feature vector construction, please refer to  <cit> . in this experiment, we set the size of the moving window as  <dig>  which is based on our previous study  <cit>  as well as the ratio of short  disordered segments to long ones in the data.

comparisons of  <dig> protein disorder predictors, the mv algorithm, the map-ml algorithm, the gmm-mapml algorithm, and our aefn algorithm on casp <dig> data are shown in table  <dig>  methods were evaluated by two measures  <cit> : the average of sensitivity and specificity , and the area under the roc curve . our proposed aefn algorithm significantly outperforms the three competitor multiple-annotator methods  and each individual protein disorder predictor based on both acc and auc scores.

comparisons of aefn vs. alternative multi-annotator methods  and individual casp <dig> protein disorder predictors.

for casp <dig> data, aefn algorithm also finds that a three-component gmm with the covariance matrix λkbk. for each component, estimated sensitivity and specificity of the best predictors, as well as filtered less-accurate predictors using aefn, are shown in figure  <dig>  for comparison, we also plot the actual sensitivity and specificity of each individual predictor at each gaussian component on the same figure. figure  <dig> clearly shows that the individual casp <dig> disorder predictors perform differently at different components. for example, gsmetadisordermd performs well in the first and third components, but it is not among the best in the second component. biomine-dr-pdb performs well in the second component, but it is not among the best in the first and third components. the figure also demonstrates the main benefit of our proposed aefn algorithm: the predictors identified as experts without relying on ground truth were indeed among the best according to their actual prediction performance at each component as verified by labelled data of confirmed order/disorder residues.

for further analysis, we found that the first, the second, and the third gaussian components highly correlate with n-terminus , internal, and c-terminus  of protein sequences respectively. for details of the casp <dig> amino-acid position distribution analysis, please refer to  <cit> . based on the casp <dig> analysis summarized in figure  <dig> and the position distribution analysis, the only reliable predictors for all three regions are prdos <dig> and multicom-refine . for n-terminal, reliable predictors also include zhou-spine-d and gsmetadisordermd while for the internal region we may also rely on biomine_dr_pdb and for c-terminus we may also use mcguffin, mason, and gsmetadisordermd. the experiment provides evidence that aefn algorithm can potentially be used to provide helpful suggestions on choosing the suitable disorder predictors for each region  of unknown protein sequences.

CONCLUSIONS
a probabilistic algorithm  for the multi-annotator classification problem is addressed in our study. without using any ground truth information, the proposed aefn algorithm is excluding lower quality annotations of novice labellers and providing more accurate classifications based on consensus of remaining experts' annotations of higher quality. evaluation on biomedical text classification and prediction of protein disorder provides the evidence of the effectiveness of the proposed method. in our experiments, aefn significantly outperformed alternatives that include the mv and multi-annotator algorithms . it was particularly beneficial when low-quality annotators are dominant. we have also found that aefn algorithm can be used to determine which annotator is appropriate to label new instances. this is potentially beneficial in any situation where annotating instances is expensive. in addition, aefn can be used for developing more accurate patient-specific diagnostic models by identifying groups of competent annotators for specific instances.

list of abbreviations used
ehr: electronic health record; mv: majority voting; roc: receiver operating characteristic; casp: critical assessment of techniques for protein structure prediction.

competing interests
the authors declare that they have no competing interests.

authors' contributions
pz conceived the algorithm, developed theoretical contributions, developed the proposed algorithm's prototype, performed experiments, carried out the analysis, and drafted the manuscript. wc reviewed theoretical contributions, and helped to revise the manuscript. zo inspired the overall work, provided advice, and revised the final manuscript. all authors read and approved the final manuscript.

