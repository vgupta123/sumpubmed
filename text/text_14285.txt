BACKGROUND
inference of population structure has found application in fields as varied as human genetics  evolution and speciation  <cit> , molecular ecology  <cit> , landscape genetics  <cit> , agriculture  <cit> , forest population genomics  <cit> , tree improvement  <cit> , fisheries  <cit>  and many others. the bayesian algorithm implemented in the software structure  is now among the most heavily used methods to infer population structure from genotype data despite the difficulties in making unbiased estimates of population structuring under various models of demograhic history  <cit>  or when not using balanced population sampling  <cit> . the structure algorithm is a model-based clustering method that uses markov chain monte carlo  re-sampling to determine the likelihood of a particular number of hardy-weinberg linkage equilibrium clusters  for a given genotype dataset. once several replicate analyses for a variety of k values are completed, one can then determine the optimal number of inferred genetic clusters  that individuals within a given dataset draw ancestry from. this is most commonly accomplished using the evanno method  <cit> .

due to the underlying nature of the mcmc process, the structure algorithm is computationally intensive while requiring very little computer memory. a single run of the algorithm can take 100% of a processor’s computing power for several hours to complete. this, along with the replication required to generate the tests of likelihoods, can lead to a single analysis taking between several days to weeks to complete, even with fewer-than-optimal number of replicates being performed.

the ever-expanding size of population genetic data sets generated by next generation sequencing technologies and other high-throughput genotyping platforms  presents an additional significant computational challenge for researchers interested in performing structure analyses. phylogeographic datasets, one class of dataset for which structure analysis is common, have shown a drastic increase in size over the past five years, with some projections showing that the median number of single nucleotide polymorphisms  per dataset may approach  <dig>  by the end of  <dig>  <cit> . recently a new method - faststructure  <cit> , was developed to speed up inference of population structure in large genome-scale data sets, but it leverages approximation to make computational gains at the cost of user directed model selection. given the continuing popularity of original structure algorithm, it is highly likely that researchers interested in model selection will continue to use it even with genome-scaled data sets. such analyses would significantly benefit from taking advantage of parallel computing in multi-core computing environments using a streamlined pipeline - beginning with the replicated structure runs and ending in collation of results using the pre-existing script structure harvester  <cit>  that is designed to visualize structure output and to implement the evanno method  <cit>  to determine the optimal number of clusters .

large datasets also make it more difficult to perform exploratory analysis of datasets that inform the full and complete analysis. as the documentation for structure correctly points out, “...some care is needed in running the program in order to ensure sensible answers. for example, it is not possible to determine suitable run-lengths theoretically, and this requires some experimentation on the part of the user.”. in the case of large data sets, this process is often challenging due to the time required to perform a single run of the structure algorithm. tools to enhance the speed of these initial exploratory runs as well as the full structure analysis would be very useful.

while structure can be easily implemented using the back-end command-line interface, it lacks the front-end functionality of setting multiple iterative runs to test k clusters and then collating the data from each run. structure is also not designed to make use of parallel computing, now commonly available on personal computers and high performance computing  clusters. although runs of structure have been parallelized through the r programming language  package - parallelstructure  <cit> , it requires that individual iterative runs for each k cluster being tested be manually specified on the joblist. additionally, some proficiency in the r programming language is required to make use of this package.

to address these issues, we present an updated version of the popular stand-alone, interactive, python program strauto to automate and parallelize structure analysis on macintosh os and the various flavors of unix running on workstations and hpc clusters. the use of the script requires no knowledge of python programming, and only requires basic interaction with the unix command line. this program also includes a secondary script samplestructurefile that randomly samples loci from a structure file so that researchers can use subsets of their data  for initial exploratory experimentation before they commit longer periods of time to the full analysis. we demonstrate the usage and benchmark the results from analysis of two example data sets on a standalone computer and a hpc cluster. strauto version  <dig>  is available for download from http://strauto.popgen.org.

implementation
the strauto workflow is as follows: 
information about the user project and intended analysis, including the fraction  of available processing cores to be committed for parallelization, is collected from a template text file.

using this information strauto outputs a unix shell script , two parameter files required by structure, and if parallelization is chosen, a file with all individual structure commands .

optional parallelization is implemented through gnu parallel  <cit>   which should be installed locally.

upon execution, the shell script ‘runstructure’ runs structure for k clusters over n iterations distributed over x number of processors.

results are compiled into a zip file and fed through structure harvester  <cit> , if available locally.

final output is ready for visualization and inference of population structure.




also included with strauto is the script sample
structurefile, which takes a structure-formatted datafile and randomly samples a given number of loci in to a new file for use in initial exploratory experimentation. more information on the use of this script is available in the strauto user manual.

speed benchmarking trials
in order to benchmark strauto’s ability to speed up structure analysis on parallel computing platforms, it was tested on two different data sets. the first data set  included genotypes at  <dig> nuclear microsatellite loci from  <dig> eastern white pine trees   <cit> , and the second  data set included genotypes from  <dig> nextrad based snp loci from  <dig> individuals of the neotropical malaria vector anopheles darlingi  <cit> . strauto was used to set up the structure analysis with k varying from  <dig> to  <dig>  and  <dig> to  <dig>  and with mcmc chains of  <dig>  million generations with the first  <dig>  generations discarded as burnin. this led to a total of  <dig>  and  <dig>  independent runs of the program structure . to benchmark the computational gain achieved using strauto, we replicated all analyses on two separate systems. a standalone server with  <dig> cores and  <dig> gb of physical memory running ubuntu linux version  <dig>  was set up to incrementally use  <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> cores. a research computing cluster  with multiple nodes, each with  <dig> cores and  <dig> gb of memory running red hat enterprise linux version  <dig>   <cit>  was set up to incrementally use from  <dig> to  <dig> nodes . the ssr data  was run on up to  <dig> cores on the standalone computer and  <dig> cores  on the hpc cluster. the snp data  was run on up to  <dig> cores on the standalone computer and  <dig> cores on the hpc cluster.

RESULTS
in the benchmark trials, there was a drastic decrease in the total analysis time with increasing number of cores applied , regardless of whether the analysis is of a large dataset with a few loci genotyped in many individuals  or many loci genotyped in a few individuals . it is important to note that strauto is not parallelizing the structure algorithm directly, but is distributing the replicate runs of the structure algorithm to different processor units. therefore, any computational gains are determined primarily by the number of cores available and the total length of time required for a single structure run. one might even argue that strauto will potentially lead to more ‘accurate results’ in shorter amount of time merely by empowering users to  run sufficient numbers of replicate runs and  perform an appropriate number of burnin and mcmc sweeps, both necessary for proper inference of the optimal k solution, than if they were limited to running only one instance at a time. strauto allows users to efficiently leverage the computational power of their hardware for this analysis. for instance, analysis of the snp dataset using a single core, which included  <dig> independent runs of the structure algorithm, took  <dig>  h to complete on the standalone computer and  <dig>  h on the hpc cluster . using  <dig> separate cores, one per independent run of structure, the total time to complete the analysis was  <dig>  h on the standalone computer  and  <dig>  h on the hpc cluster . analysis of the ssr data set on the hpc cluster took  <dig>  h to complete using one core and  <dig> min using  <dig> cores . the scaling seen using strauto will be dependent upon the architecture of the computing environment. interdependence of available cores and overhead costs involved in parallel processing will lead to less performance gain than when cores are running independently. other factors such as disk i/o requirements of the program and hyperthreading may also affect the scaling. therefore one should not expect linear scaling of the time needed for the analysis. however, because the runs of structure are independent from one another, one will always see a speedup of total computation time as one increases the number of cores available for computation – until the number of cores exceeds the total number of independent structure runs. this is clearly evident from our benchmarking tests which show no further speedup once the number of cores exceed total number of runs e.g. snp analysis using  <dig> vs  <dig> cores and ssr analysis using  <dig> vs  <dig> cores . on the other hand, it took  <dig>  h of additional computation time to complete analysis of the snp data set  using  <dig> vs  <dig> cores. this is because when using  <dig> cores, the analysis must wait for two cores to become available before proceeding with 49th and 50th independent run . there is no wait when using  <dig> cores because the number of runs is equal to number of cores engaged.
fig.  <dig> speed benchmarking for the analysis of two trial datasets incorporating various numbers of cores on two different computing systems. single nucleotide polymorphisms  data is from  <cit>  and simple sequence repeat  data is from  <cit> . the asymptotic approach to a minimum time represents the time it takes for a single core to compute one complete run of the program structure for the given dataset. a standalone computer. b hpc cluster




a larger dataset from  <cit> , with  <dig>  loci genotyped among  <dig> individuals, that analyzed  <dig> replicates for each k ranging from  <dig> to  <dig> with mcmc chains as defined above, took just over  <dig> days to complete on  <dig> cores using the strauto script. initial exploratory experimentation to determine the mcmc parameters with this dataset was conducted using  <dig> randomly sampled loci, and took ∼ <dig> h with  <dig> cores.

in summary, the time to complete a fully replicated structure analysis is a function of the number of cores available and the time to complete a single run of the algorithm. users with access to smaller numbers of cores may consider using other multi-core or cloud-based computing platforms when analyzing large datasets. as our results show, hpc clusters offer greater scalability for this analysis with up to  <dig> times speedup with our example data than standalone computers which showed upto  <dig> times speedup.

CONCLUSIONS
strauto is the first tool to implement a pipeline approach by  combining structure analysis with downstream collation of results using structure harvester, and  distributing runs over multiple processors using gnu parallel. these functionalities make strauto ideal for deployment on high performance computing clusters and multi-core personal workstations, to reduce the computational time.

abbreviations
gbgiga bytes

gnugnu’

s not unix; gplgeneral public license

hpchigh performance computing

i/oinput/output

macintosh osmacintosh operating system

mcmcmarkov chain monte carlo

nextradnext generation restriction site associated dna sequencing

snpsingle nucleotide polymorphism

ssrsimple sequence repeat

