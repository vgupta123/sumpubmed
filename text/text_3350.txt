BACKGROUND
in the general text classification, effective feature is essential to make the learning task more efficient and accurate. no degree of classifiers can make up for a lack of predictive information in the input features  <cit> . in bioscientific literature, where biological structures and terminologies are described in a large number of features, the situation is more serious: well-chosen features could improve the classification accuracy substantially and decrease the risk of over-fitting  <cit> .

in the early days of biological literature classification study, most of the researchers depended on the domain experts to pick out the informative features. regev et al. used expert-defined rules to extract features from the semi-structure text and figure legends. besides, they utilized external lexical resources and semantic constraints to achieve a better coverage and accuracy  <cit> . min shi et al. employed two types of keywords as feature: one type was from the given evidences and the other type was manually extracted from the training texts by domain experts  <cit> . moustafa m. ghanem et al. utilized expert-edited regular expressions to capture frequently occurring keyword combinations  within short segments of the text in a document  <cit> . all these approaches require the involvement of domain experts in identifying the specific textual objects and the informative templates, so that they cannot easily be automatically extended to an efficient and scale-free model on other biological datasets  <cit> .

recent years, fully automatic and scalable text classification algorithm provides an alternative to the previous methods. wilbur employed unigram, bigram and all of the mesh terms as the set of feature to represent the documents  <cit> . dobrokhotv et al. utilized the words processed by the xerox natural language processing tool as discriminating attributes  <cit> . aaron et al. used “bag of words” model: content was tokenized and stemmed into unigram feature and modelled the samples as binary feature vectors  <cit> .

although all of these features catch some aspects of biological and statistical meanings, they still cannot well and automatically exploit the domain dependent information from the complex biological literature. it becomes a challenge in biological text mining field to automatically introduce higher level domain dependent features into the classification process and integrate with the lower level domain independent features.

in this paper, we investigate the issue of biological literature classification from the perspective of feature selection and integration, which is evaluated by biocreative  <cit> , an international evaluation in biological text mining. in ias  of biocreative  <dig>  participants were asked to classify a given set of medline titles and abstracts, according to whether a document contains at least one physical ppi  or not. this procedure would be extremely useful for facilitating the efficiency of manual curation since it will largely filter out the irrelevant documents. in the evaluation, one of our implemented classifiers achieved outstanding results: the accuracy ranked at the 1st place, auc and f-score ranked at the 2nd place respectively.

although the result is encouraging, the performance has dropped significantly from the 5-fold cross validation on the training set to the evaluation on the official testing set . main differences between these two data sets are: 1) the testing documents are mainly published in  <dig> while the training documents distributes evenly over the past years; 2) the relevant/irrelevant document rate in the training set is nearly 2: <dig> while in the testing set it is 1: <dig>  to statistically analyze the phenomenon, we use the variance of kullback leibler divergence to estimate the distribution of the top  <dig> employed features on the training and testing sets as follows:

 kl`=12∑xlog⁡pq+qlog⁡qp) 

where x is the word and phrase features employed in ias, p and q are the probability of x in the training and testing set respectively.

the result  demonstrates that there is great divergence between the probability distribution of features in the irrelevant document set. and only one thirds of the top  <dig> features selected from the training set accordingly occur in the testing set . it is clear that our previously selected features are limited and sensitive to the data distribution. how to efficiently exploit the domain independent and dependent features in the biological literature and avoid the over-dependence on data distribution motivates us to have an in-depth investigation in this paper.


                           unigram feature
                           relevant probability
                           irrelevant probability




the rest of the paper is organized as follows. we will introduce the detailed description of methodologies proposed in this paper in methods section. in results and discussion section, we will present the experiment results and analysis. in conclusion section, we will summarize our contributions in this paper.

methods
in this paper, we are engaged to investigate the issue from the perspective of feature selection and integration. the main contribution in this paper lies in that: we propose 1). domain independent feature value schema tf*ml and length-fixed string feature 2). domain dependent “semantic template” feature 3). efficient integrations among the features. these methods are described respectively in the following.

probabilistic schema
the traditional tf*idf schema  <cit>  just takes into consideration the occurrences of words in the whole corpus, while discarding the distribution of words in different categories. differently, we propose a novel probabilistic feature value schema tf*ml  to substitute the traditional tf*idf as follows:

 tf  × ml = tf  ×  − log⁡ p) 

where t means the selected feature word, c+ and c− mean the relevant and irrelevant category, p and p mean the probability that t occurs in category c+andc− respectively.

the sign of ml indicates the category relevance of the feature and the magnitude reflects the classification confidence. following the same idea as tf*idf to express the specificities of features in different documents, we also multiply tf by ml.

here we do not depend on the posterior distribution of features to implement the prediction. to explain the reason we could rewrite the formula  as follows:

 ml = log⁡ p − log⁡ p = log⁡ p − log⁡ p + log⁡ pp 

and the posterior distribution is:

 posterior = log⁡ p − log⁡ p = log⁡ p − log⁡ p 

in the ml schema, the relevant/irrelevant document rate pp has been taken into consideration as a compensate factor. but in the posterior probability schema, the impact of relevant/irrelevant document rate is eliminated, according to the independent and identical distribution hypothesis, while it is not tenable in our situation .

the essence of tf*ml schema is to fully utilize the category relevant information from the annotated samples, which cannot be inferred from the tf*idf schema. experiment results  demonstrate that ml steadily improves the discriminative capability of features.

the precision/recall/f-score demonstrate classification capability of the model, and auc  is to evaluate ranking capability of the model.


                              feature value
                              precision
                              recall
                              f-score
                              auc


string feature
in many text classification applications, it is appealing to take every document as a string of characters rather than a bag of words  <cit> . especially in bioscience, the tokenizing and stemming procedure would incur undesired loss of the informative attributions, since many of the semantically related biomedical terms that share the same stem or morpheme are often not reducible to the same stems  <cit> . therefore, we propose to directly utilize the length-fixed strings as feature to exploit most of the informative segments.

to the best of our knowledge, no one explicitly takes length-fixed strings as feature because of the explosion and sparse of feature space. however, the statistical analysis based on formula  demonstrates that distributional divergence between the training and testing set becomes much smaller under the length-fixed string feature . so we turn to take the fixed-length strings as feature: the length-fixed strings are extracted from the whole sequential text without considering the sentence boundaries and strictly consist of  <dig> lowercase english letters ,  <dig> numbers  and a white space. chi-square statistics  <cit>  is employed to select out the significant features and tf*idf is computed to build the feature vector .


                              string feature
                              relevant probability
                              irrelevant probability



                              unigram feature
                              string feature


named entities and semantic template features
both of the above proposed methods are domain independent, which are endowed with well generalization capacity and are not necessarily limited to the bioscience domain. but introducing domain dependent features could greatly filter out the false positive samples and further improve the performance  <cit> . in biological literatures, named entities , such as cdc <dig> , and semantic templates , such as “proteina interact with proteinb”, are the most meaningful concepts in ppi documents and well conserve the syntactic and semantic structures in describing the protein interactions. so we introduce the named entities and semantic templates as feature to exploit the domain dependent information.

with the help of abner  <cit> , a named entity recognition tool,  <dig> types of named entities in a given document could be identified: protein, dna, rna, cell types and cell line. since the recognized entity space is large and sparse, we only utilize their types as feature to decrease the dimension of feature space without losing the universality.

after recognizing the named entities, semantic templates are ready to be extracted from the documents. we propose a novel template extraction algorithm named keybt, i.e. keyword based template extraction algorithm, to extract the semantic templates describing the interaction patterns among all of the recognized entities.

compared to the traditional local alignment algorithm, keybt operates differently: first locate statistical significant words as seeds, and expand the seeds in the contextual environment iteratively, finally preserve the most “powerful” templates as the result.

the flow chart of the keybt algorithm is as follows:

1) locate the occurrences of predefined candidate keywords in each sentence; discard the sentences without any keywords; get the initial candidate sentence set s0;

2) locate each entity type in s0; discard the sentences without any entities; get the initial candidate template set t0;

3) iteratively normalize each template in t0: removing the redundant templates by syntax parsing; get the raw templates set t1;

4) evaluating the templates in t <dig>  filter out the templates of low quality, get the final template set tf.

keybt not only depends on chi-square statistics to select the most distinct keywords but also utilizes ml to determine the category relevance of the keywords, because chi-square does not distinguish the association between features and different categories: a few high quality features of irrelevant category might be overwhelmed in the large amount of features of relevant category. chi-square is employed to select a raw candidate keyword list , and then top  <dig> features from both categories are preserved according to ml respectively.

we use the following formula  <cit>  to evaluate the relevance of templates based on the balance between generality and specificity of the templates.

 s =  × ln⁡ 

where t.pos and t.neg are the positive/negative matching count of template t in the training set, and β is the parameter tuning the positive/negative matching rate.

when we get the final templates set tf, we do not simply depend on the positive/negative matching rate of each template to make the prediction. instead, we use them to build feature vectors and train a classifier.

top  <dig> keybt-extracted templates are illustrated in table  <dig> 

<ptn>, <dna>, <cel> mean protein, dna and cell-line, e* means any words occurrence


                              keybt templates
compared with the local alignment algorithm that depends on the post evaluation to remove meaningless and noisy templates, the potential advantages of keybt algorithm are as follows: 1) keybt utilizes the statistical characteristic of the candidate keywords to largely remove noise before extraction; 2) keybt templates need not to fix the entities' type beforehand, so that it could catch the distribution of templates in both categories to discriminate both of the relevant and irrelevant categories; 3) the heuristic rules applied on the relation of named entities and candidate words  would guarantee the biological meaning of the extracted templates.

feature integration
experiment results of the overlap among the misclassified samples by different features show that there is great complement among different features: in many cases, the false prediction caused by one feature would be treated correctly by another one. and a single type of feature is easy to lead the classifier over-fitting on the data distribution . thus, the integration among different features would be beneficial. in this sense, we propose two kinds of integration from different levels: feature-level and classifier-level to integrate all of above proposed features.

we perform the feature-level integration in a typical way: normalizing each part of features and unifying them into a new feature vector. we do the normalization as follows  <cit> :

 norm_value = unnorm_value
                           −min_value max_value−min_value 

where max_value and min_value are the maximum and minimum values that are actually seen in the input feature set.

but there is an obvious defection in the above method: some lower dimensional features might be overwhelmed by the higher dimensional features . based on this consideration, we turn to perform the integration on the classifier level and propose two different ways to implement the integration. the first one is to integrate the output of each classifier: after training classifiers on different types of features respectively, we normalize and unify the output of each classifier into feature vectors and train a classifier. the other one is adaboost <cit> , a general classifier integration method, which has two major advantages: firstly, adaboost tunes the weight of each classifier according to its performance in each kind of training samples, which could fully utilize the discriminative capability of features; secondly, soft margin of adaboost avoids the risk of over-fitting in the training process. these approaches well overcome the defection mentioned above.

RESULTS
the benchmark corpus is provided by biocreative  <dig>  the training set contains  <dig> relevant documents  and  <dig> irrelevant. the testing set contains  <dig> documents,  <dig> of which are labelled as relevant. all of the proposed features and integration methods are implemented on the linear-kernel svm.

probabilistic schema
in table  <dig>  tf*ml schema improves recall performance by  <dig> % without losing precision compared to the traditional tf*idf schema. the improvement validates the effectivity of exploiting the category relevance information of features and testifies ml to be a more effective and general feature value schema in general text classification applications.

string feature
in our experiment, the best performance is achieved when the string length p is set to  <dig>  in table  <dig>  the length-fixed string feature  gains encouraging recall improvement by more than  <dig> % compared to unigram and bigram feature. but the precision has dropped about  <dig> % as the expense, which can be further compensated by employing tf*ml as feature value. the practical efficiency confirms our statistical analysis of the distribution of features and gives us insight in the selection of lower level features.


                              feature
                              precision
                              recall
                              f-score
                              auc


named entities and semantic template features
in table  <dig>  only depending on a simple criterion that if a document contains at least one protein entity, the document should be judged relevant otherwise irrelevant, we could achieve a very high recall  with an acceptable precision . our proposed template extraction algorithm keybt well captures the complex association between the keywords and named entities and achieves promising performance in term of precision  and a better improvement comparing to our former approach onbires templates  <cit> , which is based on local alignment algorithm.


                              feature
                              precision
                              recall
                              f-score
                              auc
feature integration
in table  <dig>  feature-level integration contributes the improvement in terms of f-score by  <dig> % and auc by  <dig> %; in table  <dig>  integration based on the output of classifier achieves better improvement in terms of f-score by  <dig> % and auc by  <dig> %. the best performance is reached by adaboost: in f-score by  <dig> % and in auc by  <dig> %. advantage of the feature integration is obvious: different types of features are independently selected from the corpus, which focus on the different aspects of feature space and reinforce each other. from the result, it is apparent that different integration methods well leverage the capability of different types of features and achieve promising improvement.


                              feature
                              precision
                              recall
                              f-score
                              auc


integration on length-fixed string feature, entity feature and template feature


                              feature
                              precision
                              recall
                              f-score
                              auc


statistical significance test
since the size of the evaluation corpus is not large enough, it is necessary to perform the statistical significance test to validate the reliability of our proposed features and integration methods. here we employ s-test to evaluate the performance of systems on the pooled decisions on the individual documents/category pairs  <cit> .

in table  <dig>  we can find that the proposed feature value schema tf*ml, length-fixed string feature and semantic template feature are much better than their counterpart , and two different level of feature integrations significantly improve the classification performance .

the null hypothesis is that the performance of two methods is the same; the alternative hypothesis is that the former is better than the latter.

                              string vs. unigram+bigram
                              keybt template vs. unigram+bigram

                              p value
                              feature level integration vs. unigram+bigram
                              classifier level integration vs. unigram+bigram

                              p value
comparison with the state of arts
in table  <dig>  the mean, standard deviation and best performance from biocreative  <dig> are selected from  <dig> runs of  <dig> teams. under our feature selection and integration procedure, the performance outperforms the previous best results .

the best performance from biocreative  <dig> is selected from  <dig> runs of  <dig> teams respectively.

                              precision
                              recall
                              f-score
                              auc
CONCLUSIONS
the experiment results clearly demonstrate that the lower level features are endowed with better generalization capability, but hampered by lower accuracy; higher level features contain rich domain dependent information, with better specificity but poor universality. integration of different level of features would benefit from the different aspects of the feature space, which would reinforce the domain dependent classification and overcome the bias on the data distribution.

main contributions of this paper are as follows:

 propose novel domain independent feature value schema tf*ml and length-fixed string feature;

 introduce domain dependent features  into the biological literature classification, and propose a novel template extraction algorithm keybt;

 investigate the feature-level and classifier-level integration methods to incorporate the information from different levels and perspectives.

now, the proposed methods are being integrated into our online service onbires  <cit>  as a pre-processing module. in the next step, we will be engaged in the aspect of incremental learning to make our approaches portable to different datasets.

competing interests
the authors declare that they have no competing interests.

authors' contributions
hw carried out the main work of the paper, proposed the methods and drafted the manuscript. mh gave directions in the whole process and revised the draft. ds participated in the design and implementation of the experiments. xz supervised the whole work, gave a number of valuable suggestions and helped to revise the manuscript. all authors have read and approved the final manuscript.

