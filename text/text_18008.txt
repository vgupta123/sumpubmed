BACKGROUND
the literature provides information that is not covered in other knowledge resources, for example in thesauri, databases or ontologies. in the biomedical domain the integration and transfer of knowledge from textual sources to these other knowledge sources is ongoing work and requires a lot of effort. an example of such a resource that profits in particular from this transfer is the omim  database  <cit> . the transfer of information though suffers from the high growth rate in literature information and publication numbers that turns manual curation into a rather slow process. hence, researchers in text mining focus on the automation of this process that profits from different analytical steps, for example low level natural language processing or recognition of named entities.

term recognition deals with the identification of relevant terms and term resolution intends to link these terms to entries in existing knowledge sources . although quite some effort has been spent on the identification of protein and gene named entities  in the scientific literature, little work has been invested on the recognition and resolution of other terminologies in the biomedical domain. this is partly due to the fact that annotated corpora are not abundant due to the high costs attached to their generation.

the biocreative challenge i and ii  <cit>  have tackled the gene recognition and resolution tasks apart from the identification of gene ontology  <cit>  terms and for both challenges an annotated corpus has been generated. the nlpba challenge  <cit>  has been based on the genia corpus  <cit>  that contains annotations of concepts from the genia ontology. some work has been devoted to the identification of chemical entities  <cit>  linked to chemical databases like pubchem  <cit>  or chebi  <cit> . all these efforts are tied up with the generation of corpora that help to improve existing named entity recognition solutions that eventually become available to the general public . furthermore, such solutions can be used in other tasks like information retrieval as has been shown in the trec genomics track  <cit> . solutions for the identification of disease named entities have been made publicly available, such as metamap  <cit>  or selected modules in whatizit  <cit> , but any of them still lack proper assessment against a gold standard.

the recognition of disease terms in the literature is relevant to identify known and hypothetical causes of the disease such as genes that are involved in a disease as well as known treatments of a disease such as drugs, chemicals or chemical compounds. omim is an example of a database that is based on information from the literature, i.e. it has been generated by manual curation of scientific papers. an automatic analysis of the literature could expedite the population of this database.

some research work has been done on the identification of diseases: refer to the work of craven  <cit> , krallinger  <cit>  and chun  <cit>  for the identification of relations between disease entities and pgns. in the first work, the diseases where identified automatically using the existing omim lexicon. in the second, the authors provide a set of sentences together with the mentions of genes and diseases but the corpus does not deliver the annotations of the diseases as part of the sentence. in the third work, the disease and pgns were identified in a random selection from a subset of medline using a look-up technique based on the umls, the assessment was done at the level of ner but the resolution of the term into the proper umls entry was not done. as a conclusion, there is no gold standard that can be used to do the preparation and evaluation of existing techniques focused on the identification of diseases in text.

several terminological resources are available that provide disease terms. amongst the most used resources are the medical subject headings   <cit> , the nci  thesaurus  <cit> , snomed ct  <cit>  and finally the umls resource  <cit>  that encloses all the other ones into a single source. mesh and snomed ct have a different scope and fulfill different tasks and none of them has been designed to meet text mining needs. since we are interested on the recognition of diseases in text, umls is a natural choice since it is covering more than the other sources.

in this paper, we benchmark three different methods that make use of the umls metathesaurus for disease entity identification. all measured results are based on an annotated corpus that is publicly available from our web site. our measurements result from complementary methods, i.e. from a lexical approach , from an information theoretical approach  and from metamap . in addition, we measured the performance of combinations of all three methods. these results give a better understanding of the complexity of the task and the appropriateness of the methods applied.

RESULTS
our analysis leads into two different assessments. the first one is based on entity recognition. in this task we analyze how all methods perform on the identification of the boundaries of the disease mentions in the text. this task is similar to the gene mention task in biocreative. the second task focuses on the recognition of diseases at the sentence level. here, we want to assess the correct resolution of disease terms. since the recognition of these terms may not be limited to the boundaries of a noun phrase, we have decided to select the sentence boundaries as the minimum stretch of text that would be relevant for annotations of diseases. this task is similar to the gene normalization task in biocreative. the evaluation of the applied methods requires to define the benchmark that is used for the assessment  and evaluation measures, for example precision, recall and f-measure that are well known standards .

ner evaluation
in our research work we wanted to assess the recognition and resolution of disease named entities from natural language text. we identified the corpus provided by craven  <cit>  as relevant to our evaluation, although it has the limitation of being restricted to omim diseases only.

typically all ner evaluations consider correct matching of both, the left and the right boundary of the named entity as the most precise assessment . others have proposed to deviate from exact matching: as shown in  <cit>  more flexible matching of the boundaries in the evaluation produces better results and may be more appropriate in the evaluation. we assessed our results as well against exact matching as well against the match of the left boundary and right boundary only, respectively. the results are shown in table  <dig>  in all three cases, the dictionary look-up performed best and in particular the assessment against an alignment to the right boundary showed best results, where general disease terms such as malignant cancer were accepted as a correct match against the underlying disease evidence such as familial malignant neoplasm. this example shows the overall better results for right alignment in comparison to the other two assessments in the case of dictionary look-up. our statistical method and metamap operate on regions defined by their nlp processes that do not necessarily comply to a defined chunk such as a noun phrase. this leads to the condition that the boundaries of the text evidence used for the term recognition do not match at either end exactly to the boundaries of the term representation in the terminological resource.

this table shows the result of ner. for each alingment and method, first we show how many diseases are annotated in the corpus  then the total number of annotation done by the method and then how many of them agreed with the benchmark .

the low performance of the applied methods is partly explained by the mismatch between the terminology used by craven to do the annotation of diseases and our terminology . we have identified additional terms that are occurrences of diseases in text but are not identified as such in craven's data set due to missing mentions of the disease in the omim lexicon such as retinoblastoma or terms not annotated because a given disease term represents a term variant in comparison to corresponding entry in omim's such as anhaptoglobinemia, del versus anhaptoglobinemia. in a similar work  <cit>  it has been proved that filtering techniques based on machine learning can improve the performance of look-up techniques. in similar tasks but for different semantic types, as pgns, machine learning techniques like svm or conditional random fields are the state of the art. further research is needed to assess these techniques on disease recognition.

term recognition evaluation
in this task we want to link the diseases with the appropriate concept in the umls data resource. the evaluation consists in comparing the annotations produced by each one of the methods with the annotations in the gold standard. according to our assumptions, the occurrence of a representation of a disease term is limited to the sentence boundaries.

all annotations of disease entities have been collected at the sentence level for all three applied methods to better compare the results. all three methods use different local contexts in their ways to match text evidence to the representation of a disease named entity, i.e. the mention of the disease is not always limited to a noun phrase as used in the dictionary look-up. our statistical approach can be adjusted to analyze a zone that is larger than the noun phrases up to the stretch of a complete sentence. metamap delivers annotations in a local context that resembles the stretch of a noun phrase but tends to be larger than the typical noun phrase definition, for example metamap extends the noun phrase with punctuation extensions. all annotations of all methods have been collected at the sentence level.

as no corpus is available that can serve as a gold standard we have processed the available corpus consisting of a set of sentences. after the annotation of the results with all three methods, two domain experts have analyzed the results. for more details refer to the methods section.

this table shows the total number of diseases in the benchmark , the number of distinct diseases , the number of diseases annotated by the method and the number of unique diseases identified. then we present the result on standard measures like precision and recall.

in addition we combine the results from all three different methods by a voting scheme. in the first approach, we picked up any disease that has been proposed by at least one of all methods. in the second and third approach we required that at least two methods agree or all three of them. the first combination of our methods obtains the highest recall showing that all three methods are complementary. if we require a higher level of agreement between the methods, we see an increase in the precision but at the cost of lower recall .

we found that selected diseases appear at high frequencies in the corpus and are most likely overrepresented. this could be the reason for a bias in our assessment, since the overall measurements could be mainly based on the occurrence of the most frequent diseases in the corpus. this would lead to the consequence that the observed differences between the methods are due to the behavior that they show on the basis of the frequent disease mentions. we defined frequent diseases as the ones with more than  <dig> occurrences in the benchmark corpus. the list of theses diseases can be found in table  <dig>  in order to generate more representative results, we have removed these diseases from our test data and have reassessed the performance of our systems. the results are presented in table  <dig>  as we can read from the result table, we reduced the overall number of disease annotations in our result set by around 36% although we removed only  <dig> diseases . although the figures for precision and recall have been lower now, the differences overall between all three methods is still in the same range.

this table shows the total number of diseases in the benchmark , the number of distinct diseases , the number of diseases annotated by the method and the number of unique diseases identified. then we present the result on standard measures like precision and recall.

discussion
as we can observe from our analysis, the results of the dictionary look-up and from the statistical approach are in the same range as the performance of metamap in terms of f-measure. we can expect that the results could be further improved, if at least the statistical approach is better fitted to the data in the gold standard corpus.

all methods require improvements regarding the recall and looking into the causes for low recall we find the following issues for improvement. recall is limited by the fact that not all lexical variants found in the text are contained in the umls metathesaurus .

complex syntactic variants like coordination  are difficult to capture and are common to the recognition of semantic types such as pgns  <cit> .

low precision is due to false positives that are generated from different findings. some methods have proposed general terms like neoplasm that did not fit the full form in the text. on the other side, we encountered the prediction of very specific terms, for example primary malignant neoplasm versus malignant neoplasm, that were not represented in the text. in addition, our methods proposed related terms that still represented different semantic types such as von hippel-lindau syndrome versus von hippel-lindau mutation or had to cope with ambiguity. for example retinoblastoma was proposed as disease, but was referred to in the text as a gene. this phenomenon has already been discussed by chen et al.  <cit>  in their study about ambiguity of pgns. a very common source of ambiguity is the reuse of acronyms like emd for different senses, for example emery-dreifuss muscular dystrophy and for electromechanical dissociation  <cit> . finally, some redundant and ambiguous terms are contained in the umls metathesaurus lexicon itself, despite all the filtering applied to possible ambiguous terms .

the combination of all three methods to propose annotations leads to modified precision and recall characteristics. if we accept all annotations that have been proposed by at least one method the recall increases. this shows that all three methods are complementary and recognize diseases that have not been recognized by the other two methods. the contrary is true, if two of the three or all three methods have to agree on the annotation. in this case the precision increases and at the same time the recall is lower. this again shows that all three methods are complementary and make different assumptions concerning the recognition of diseases in the literature.

we have further analyzed the results under the condition that two of the methods agree but the third one disagrees. we find altogether  <dig> cases and in  <dig> of these cases the disagreement results from metamap. this shows that the statistical approach and the dictionary look-up have more similar performances than metamap. only in  <dig> cases all three methods agreed and all three methods disagreed in  <dig> cases.

these results can also be read from the measurement of the overall performance of the methods in tables  <dig> and  <dig>  in the dictionary look-up approach terms are matched to the lexicon of the concepts giving priority to the longest span of text that matches the entity; some annotations may refer to occurrences of the term in nested terms. no variation is allowed so it may present higher precision at the cost of lower recall; as we will show with the statistical method. analyzing the results we find that with this simple method we obtain a result that performs better than the other two methods considering the f-measure. this means that use of disease terms in text is well standardized. the statistical approach obtains the highest recall this is not a surprising result since the matching of terms is more flexible than in the dictionary look-up. the recall is not significantly much higher than the dictionary look-up, further supporting the assumption that the disease terminology is quite close to its usage in text. metamap relies on morpho-syntactic processing of terms and text and selects a region of the sentence that is larger than the occurrence of the disease term in text. this is the cause to the finding that in some cases the score provided by metamap is lower than other terms selected by metamap. in addition, at times the occurrence of a disease term exceeds the window selected by the nlp processing tools used by metamap.

CONCLUSIONS
the corpus with the curated annotations is available at  <cit>  and can be used to optimize the parameters of a different system performing disease annotations. the corpus allowed us to assess different methods. it covers already a significant number of disease entities and accounts for a selection of term variability problems that are encountered in the recognition of disease entities in the literature. the corpus enables research teams to develop other systems for the recognition of diseases.

the comparison of the approaches applied on this corpus shows that dictionary look-up provides already competitive results. we read from this finding that the disease terminology is well covered in the available terminological resources and well standardized in the literature. furthermore, we observe and explain phenomena in the identification of disease terms that are better addressed by one or the other applied methods. these phenomena may not apply to other semantic types such as go  terms or pgns. for instance go terms are longer than disease terms and the purpose of go is not meant for text mining, so it is harder to identify them in text as such using a lexical look-up solution. pgns on the other hand can be recognized with high recall in text with a look-up approach but require a special treatment due to the ambiguity of the terms with common english, other semantic types as disease or other pgns belonging to different species.

the results show some directions for further research. concerning the recall, we can see that the lexicon is quite extensive but some lexical variants can be found that are not covered by the current resources.

morphological variants may be considered and may be dependent of the semantic type covered; the same analysis cannot be applied to pgns. on the other hand, syntactic variants like coordination have already been studied  <cit>  but are not intensively considered in the biomedical domain and an interest has already been shown for pgn recognition in the biocreative challenge  <cit> . some tools already exist that do term normalization like the specialist lexical  <cit>  tools available from the nlm. concerning precision we may find common issues concerning the current approaches that relate to the disambiguation of the term to the appropriate semantic type for which resources like the umls project  and the disambiguation of abbreviations that is already one research topic in many research groups.

