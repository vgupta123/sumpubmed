BACKGROUND
complementing functional genomics, proteomics deals with the large-scale analysis of proteins expressed by a tissue under specific physiological conditions. a broad range of technologies are used in proteomics, but the central paradigm has been the use of a method for separating mixtures of proteins followed by identification of protein by mass spectrometry . two-dimensional polyacrylomide gel electrophoresis  very popular, despite the availability of other powerful separation techniques. with 2d page  <cit> , proteins are separated in one dimension according to their molecular mass and in the orthogonal dimension according to their isoelectric charge. in theory, each protein is uniquely determined by its location along the two dimensions of separation. the separated proteins are then stained with fluorescent dyes so that they are amenable to imaging. proteomic differences across multiple samples can be studied by comparing the expression profiles across sets of gels.

the main steps in differential analysis of two-dimensional gels involve image de-noising, spot detection, spot quantification, spot matching and statistical analysis, which were discussed in detail in  <cit> . unlike the analysis of microarray data, the statistical differential analysis of 2d gel images is still in its infancy. the main difficulties are the discrimination between actual protein spots and noise, the quantification of protein expression levels thereafter, and spot matching for individual comparison. although there are commercial software packages for 2d gel image analysis , considerable human intervention is required for spot matching. spot matching is the process by which one maps a spot on a particular gel to the corresponding spots on the other gels so that spots corresponding to the same protein are identified. with a larger number of images, this step becomes increasingly problematic as fewer spots are matched and the analysis is performed on sparser data  <cit> . moreover, in available software packages, the comparison of the quantitative features is based on classical tests, such as the t-test or the f-test. attempts have been made to avoid image segmentation and spot quantification. models based on image pixels  <cit>  are not practical given the huge number of pixels, high variation in the background intensity and sensitivity to misalignment.

recently, academic software was developed to cope with difficulties in the analysis pipeline including protein spot detection, quantification and spot matching  <cit> . to improve the spot-detection results and avoid spot matching, the methods in  <cit>  utilize the mean gel image as the template for locating spots. the pinnacle method  <cit>  uses a fixed window for spot detection, quantification and background separation. the approaches in  <cit>  rely on the watershed transform  <cit>  for spot segmentation and quantification. the regstagel software  <cit>  provides advanced statistical tools. comparison of different software for protein spot quantification is beyond the scope of the current paper. we shall focus on the statistical analysis, assuming that spot quantification has been performed appropriately. for convenience, we employ regstatgel  <cit>  to obtain spot quantification for statistical analysis of the set of gel images considered in this paper.

since hundreds or thousands of proteins are usually featured on a gel, once proteins are quantified, we are faced with a large-scale hypothesis-testing problem. the regstatgel software  <cit>  applies the benjamini-hochberg  procedure  <cit>  in combination with multivariate analysis for identifying significantly changed proteins. the bh-fdr procedure is widely used for selecting the p-value threshold to control the false discovery rate . under the assumptions that tests are independent or weakly dependent and the null distribution of the p-values is uniform, the bh-fdr procedure controls the false-discovery rate at a given level. but in practice, these two assumptions are often invalid. strong dependence usually exists, especially in the field of genomics and proteomics  <cit> , where the dependencies themselves are actually also of interest. considerable effort has been dedicated to the estimation of the proportion of true null hypotheses and of the false discovery rate at a given p-value threshold  <cit> . the empirical bayes methodology and closely related methods exploiting a two-component mixture model  <cit>  represent typical examples of such effort. the two-component eb models assumes that a test statistic follows either the null or the non-null distribution.

it has been commonly assumed that the null distribution of the test statistics follows some distribution theoretically. however, efron  <cit>  pointed out that in large-scale hypothesis testing the theoretical null distribution often does not hold for reasons including incorrect model assumption, unobserved covariates and correlations among test statistics. it is more appropriate to estimate the null density of the test statistics directly from the data instead of using the theoretical null density. using the two-component empirical bayes  model, efron  <cit>  proposed to estimate the mixture density from the entire histogram and the null component from data around the central peak of the mixture density. the two-component eb model aims at separating a small subset interesting cases from a large group of uninteresting cases. efron's innovative concept and estimation approach have been throughly discussed  <cit> . the locfdr r package  <cit>  was developed to estimate the two-component model using poisson regression and computing the local false discovery rate .

two methods  <cit>  were proposed to estimate the null component. one is based on finding an optimal normal approximation to the mixture density around the central peak of the histogram, and the other on maximum-likelihood estimation. in both methods, the mixture density and the null component are estimated separately. the estimation of the mixture density does not take assumptions about the null density into account. thus, there is no guarantee that the estimated null component is no greater than the mixture density over the entire domain. the two approaches may result in the estimated local fdr having multiple peaks or its being greater than  <dig>  <cit> ; neither is desirable. we present a modified estimation method for the two-component eb model: the null and the mixture densities are estimated simultaneously with a necessary constraint, which can be achieved with a constrained iteratively re-weighted least squares  algorithm. the proposed methodology is applied to the analysis of a set of 2d gel images from a factorial experiment. simulation studies are conducted to further validate and investigate the performance of the proposed approach.

methods
data
to investigate the effect of nicotine exposure on the proteome of spleen cells of female and male rats, a  <dig> ×  <dig> factorial design with gender and treatment  factors was used with  <dig> rats in each experimental group. spleen cells from the control and treated rats were harvested on post-natal day  <dig> and then cultured in the presence of convanavalin a. after  <dig> days in culture, cell pellets were lysed and solubilized directly in rehydration buffer. lysates were aliquoted and stored frozen at -80°c. samples were thawed and  <dig> μg protein from each sample applied to a ph 4- <dig> immobilized ph gradient strip  by overnight rehydration. isoelectric focusing was performed using an ipgphor ief system  with voltage increased gradually from  <dig> to  <dig> v and then kept constant at  <dig> volts for  <dig> hours. separation in the second dimension was performed on  <dig> % excel prepared gels specifically made for the multiphor ii apparatus  and run at  <dig> ma for  <dig> minutes followed by  <dig> ma for  <dig>  hours. gels were silver stained  and imaged using a umacs power look  <dig> scanner .

the objective is to find proteins that changed in quantity under exposure to nicotine or show a gender effect. the next steps would be to determine the genomic sequence of the differrentially-expressed proteins by mass spectrometry and to refer these sequences to a database of protein sequences in order to identify them and investigate their functions.

the proteins were detected and quantified using the the default settings of the regstatgel software  <cit> . specifically, the watershed algorithm was applied to the mean image to generate a master watershed map which is then imposed onto each individual gel image. each watershed region contains a single object, either a single spot or an aggregate of two spots 9a seldom occurence). the pixels in each region are then classified as either belonging to the object or to the background using otsu's method  <cit> . the mean intensity difference between the object and background serves as a summary statistic for each region and therefore for each protein , and is used for comparison across images. the regstagel software is fast, easy to use and has comparable performance to commercial software packages  <cit> . note that other free programs such as pinnacle  <cit>  can also be used for protein quantification.

for the dataset under consideration, there are a total of  <dig> watershed regions containing proteins . now, we set up a statistical model for comparing protein quantities across experimental groups. denote the log of the statistic of interest  for protein i, image l, experimental group g by ygli, where g =  <dig>  ..., nc, l =  <dig>  ..., n, i =  <dig>  ..., k. for the dataset described above, we have nc =  <dig>  n =  <dig>  k =  <dig>  and the experimental conditions  correspond to the factorial combinations of treatment and gender. we have the following linear model:

  y1li=μi-τi-γi-i+ε1liy2li=μi-τi+γi+i+ε2liy3li=μi+τi-γi+i+ε3liy4li=μi+τi+γi-i+ε4li 

where τi, γi, i are, respectively, the treatment, gender, and interaction effect for protein i. with the assumption that εgli~n, the test statistic for the treatment effect on protein i is

 ti=ȳ <dig> i+ȳ <dig> i-ȳ <dig> i-ȳ <dig> i2si∕n, 

where si is the pooled sample variance and ti follows the t-distribution with df =  <dig> degrees of freedom under the null hypothesis that τi =  <dig>  the test statistics for the gender and interaction effects follow the same t-distribution under the null hypothesis. let zi = Φ-1), where fdf is the cumulative tdf distribution. theoretically, under the null hypothesis, zi follows the standard normal distribution.

two-component empirical bayes model
the two-component eb model assumes a mixture model for the density of zi,

 f=p0f0+f <dig>  

where p <dig> is the prior probability that zi complies with the true null hypothesis, f <dig>  is the null density and f <dig> is the density under the alternative hypothesis. this model is very popular in the literature on differential analysis of microarray data, where most authors assume the null density is the theoretical null density.

efron  <cit>  defined the posterior probability that zi is from the null hypothesis as the local fdr, which is given by

 fdr=pr=p0f0∕f. 

it can be shown  <cit>  that the relationship of the local fdr to the usual fdr is

 fdr=ef{fdr|z≤zi}. 

to estimate the local fdr, we must estimate the unknown p <dig>  f <dig>  f. theoretically, f <dig> should be the n density. however, for many reasons, this theoretical null density may not be valid in practice. for example, strong correlations among tests or covariates unaccounted for in the model will invalidate the usual assumptions  <cit> . moreover, when the majority of tests show small effects, it is sounder to select the relatively more interesting effects by comparing larger effects to smaller effects rather than to the theoretical zero effects. therefore, it is more appropriate to estimate the null density of the test statistics directly from the data instead of using the theoretical null distribution.

efron  <cit>  assumed the null distribution to be n and estimated the null distribution from the data. the log of the mixture density log) was estimated by fitting a natural cubic spline or high-order polynomial to the log of counts in the histogram bins via poisson regression. indeed, suppose the z-values have been binned and the bin counts are

 mj=#{ziin binj},j= <dig> ,…,j. 

assume the mj's to be poisson counts, i.e.

 mj~po,j= <dig> …,j, 

with the unknown νj proportional to the density f at the midpoint xj of bin j, i.e. approximately

 νj=nΔf, 

where Δ is the width of the bin and n is the total number of tests. log can be modeled using a polynomial function at xj or a natural cubic spline and estimated using standard generalized linear models  for poisson observations.

efron's estimation methods for the empirical null distribution
both the central matching  and the maximum likelihood  methods of estimation are implemented in the locfdr r package  <cit> . mle is somewhat more stable but can be more biased than cme. efron  <cit>  shows that cme yields nearly unbiased estimates.

central matching
when zi is generated from a t-test, the central peak of the histogram is assumed to coincide with the null density. to estimate the empirical null density from the estimated mixture density, a quadratic curve logp0f <dig> ^ is fitted to the central peak of logf^,

 logp0f <dig> ^=β^0+β^1z+β^2z <dig>  

assuming f <dig> ~ n, the log of the null component is

 log)=logp0-12δ2σ2+ log+δσ2z-12σ2z <dig>  

p <dig>  δ, and σ can be estimated from β^ <dig> β^ <dig>  and β^ <dig>  the local fdr at z is then estimated by fdr^=p0f <dig> ^∕f^. the quadratic curve is obtained by finding a least-squares approximation to the estimated logf^ using bins in a selected interval  containing null zi's.

maximum likelihood estimation
an alternative estimation method is based on the maximum-likelihood estimator of the parameters p <dig>  δ, σ. assume that the non-null density f <dig> is supported outside some given interval . let n <dig> be the number of zi in , and define

 p0=Φb-δσ-Φa-δσandθ=p0p <dig>  

then the likelihood function for all the z-values in  is

 fδ,σ,p0∝θn0n-n0∏zi∈ϕδ,σp <dig>  

where ϕ denotes the normal density. the estimates of p <dig>  δ, and σ can be obtained by maximizing this likelihood.

constrained estimation approach
in the procedures described above, the mixture density and its null component are estimated separately. the estimated null component p0f <dig> ^ may be greater than the mixture density f^. thus, there is no guarantee that we will have fdr^=p0f <dig> ^∕f^≤ <dig> for all z. indeed, we may end up awkwardly having that fdr^>1>fdr^ for some z <dig> <z <dig> <  <dig>  as shown in figure  <dig>  where both approaches were implemented on the set of gels of interest.

therefore, we propose to modify the cme approach by estimating the mixture density and its null component simultaneously. the log of the null component is estimated via a quadratic approximation to the central peak of logf^ using bins contained in the interval . we add the constraint that f^≥p0f <dig> ^  while maximizing the poisson likelihood. to solve this problem, we utilize a constrained iteratively reweighted least-squares algorithm, as shown below. we approximate the bin counts of the mixture histogram via poisson regression using a natural cubic spline with d knots. assume the knots are x <dig> = h <dig> < ⋯ <hd ≤ xj, where x <dig> and xj are the two bins at the left and right ends of the histogram. denote the value of the natural cubic-spline function at point x by s, where θ is the unknown parameter vector for the cubic splines. then

 s= ∑d=1dbdθd=b′θ 

where θ = ', b = '. bd are the natural cubic spline basis functions  <cit> :

 b1= <dig> b2=x,bd=ϕd-2-ϕd- <dig> d= <dig> …,d, 

where ϕd=∕ and + =  <dig> if x <hd. we fit the log of the histogram counts using the natural cubic spline assuming

 log= log+ log)=s. 

suppose the non-null density is close to zero in , we have approximately for xj ∈ 

 log≈ log+ log)=q 

where q is a quadratic function with parameter β.

the constraint that log ) ≥ log ) leads to s ≥ q for all xj's. then, we only need to estimate the parameters θ by maximizing the poisson likelihood with the constraint that s ≥ q, which results in solving

  max∑lsubject tos≥q,j= <dig> …,j 

where l = -exp{s} + mj s. l is the poisson log likelihood for bin j, omitting the constant term unrelated to the parameter θ. q is the best quadratic approximation to s based on bins in . to solve this, the parameter β must be expressed as a function of θ. below, we show how to re-write the constraint in terms of the spline parameter θ.

denote the values of the natural cubic spline at all the bins x <dig>  ..., xj as a vector s. we have

 s=θ=Γθ, 

where Γ, a j × d matrix, has entry in row j and column d Γ = bd. similarly, we denote the values of the spline at bins in  in a vector form as s <dig> = Γ0θ, where Γ <dig> is the corresponding sub-matrix of Γ. s <dig> approximates the null component of the mixture density. let q = ω'β be a quadratic function, where ω = ' and β = '. the values of the quadratic function at all bin midpoints can be written in a vector form as q = Ωβ, where Ω is the j ×  <dig> matrix with jth row as ω for j =  <dig>  ..., j. similarly, we denote the values of the quadratic function at bin midpoints n  as q <dig> = Ω0β, where Ω <dig> is the submatrix of Ω corresponding to bins in .

we want to obtain the best quadratic approximation to the natural cubic spline s based on the bin midpoints xj ∈ . the least-squares solution minimizing ' is given by  

thus, maximizing the likelihood  is equivalent to solving

  max∑lsubject to-1Ω′0Γ0)θ≥ <dig>  

the above problem is solved by means of non-linear programming. a simple computational algorithm for estimating the null and mixture densities is to modify the iteratively reweighted least-squares  procedure  <cit>  for poisson regression by adding the constraint to the weighted least-squares regression. the irls algorithm converges very fast, based on our experience.

the pseudo code for the modified irls algorithm is as follows:

/* initialization of deviance dev and olddev */

dev =  <dig>  olddev = 0

/* initialization of estimation of νk */

νj=∕2

where 

{

/* update weights */

wj = νj

m˜j= log+∕νj

/* constrained weighted regression */

θ= argmin∑wj-m˜j)2s.t. -1Ω0′Γ0)θ≥0

νj = exp{s}

/* update poisson deviance */

olddev = dev

dev = 2Σ{mj log - mj -  - νj)}

}

the local fdr can then be estimated using fdr^= expq-s, where .

RESULTS
in this section, we implement the two-component eb model on the set of 2d gel images described previously. both efron's estimation approach and the proposed one will be applied for comparison. these approaches will be further compared using simulations.

analyzing 2d gel images
at first, we analyze the zi values for the treatment, gender, and interaction effects using efron's locfdr r package. the upper panel of figure  <dig> shows the histograms  of the corresponding z-values, the mixture density from the poisson regression, and the null component estimated using cme and mle. for estimation of the null component, we chose the intervals ,  and  for the treatment, gender and interaction effects, respectively. the degrees of freedom of the splines were chosen to minimize the aic criterion  <cit> , which were  <dig>   <dig> and  <dig> respectively. the green solid curves in the upper panel of figure  <dig> are estimates of the mixture densities from the poisson regression. the blue dashed and red dotted curves in the upper panel represent the empirical null component estimated using the cme and mle methods, respectively. the lower panel of figure  <dig> shows the local fdr at different z-values based on the empirical null component from the cme  and mle  methods. figure  <dig> clearly conveys the message that the theoretical null, the standard normal density n, is not appropriate for the proteomic data at hand. taking the treatment effect as an example, the empirical null distribution is n by cme and n by mle with proportions of true null hypotheses close to  <dig> for both, which indicates nicotine exposure effect affects similarly all proteins expressed by spleen cells. clearly, the empirical null density is even further from its theoretical form for the gender effect. the central peak of the z-values is to the left of - <dig> 

comparing with figure  <dig>  we see that the proposed constrained estimation approach yielded results similar to those obtained with cme. however, now, the empirical null component is below the mixture density, and the local fdr estimate is no greater than  <dig>  smooth and non-increasing at both tails. for treatment and interaction effects, the null proportion is nearly one, indicating that there is no apparent differential effect of nicotine exposure. the treatment and interaction effects follow approximately n and n, respectively. the empirical null distribution for the gender effect s n with the null proportion about  <dig> . the results for the gender effect show that we need to interpret results from large-scale hypothesis testing with caution. the bulk of the histogram is centered around - <dig> , indicating that the majority of proteins have higher expression in female rats. the local fdr plot for the gender effect reveals that there is a small group of proteins with higher expression in males. this group of proteins is clearly separate from the rest as evidenced by the small local fdr. the local fdr is therefore more indicative of how different the gender effect is on a protein compared to the majority of the proteome, and less indicative of how significant the gender effect is. should the theoretical null distribution be used, there would be a large number of effects at the left tail. overall, we note that the estimated means of the null components are far from zero, especially for the gender effect, which may indicate the need to further normalize the data to remove some systematic bias.

simulation validations
numerical simulation
in this section, we compare the proposed constrained estimation procedure with the cme approach without constraint using numerical simulations. the simulation model consists of zi ~ n, i =  <dig>  ...,  <dig>  and zi ~ n, i =  <dig>  ...,  <dig>  thus, the first  <dig> zi′s belong to the null distribution and the last  <dig> zi′s to the non-null distribution, and the null proportion p <dig> =  <dig> . the interval  was used for estimating the null component. the estimated mixture density and its null component are displayed in figure  <dig>  with the left column showing the results from the cme approach without constraint and the right column showing the results from the proposed constrained estimation approach. the upper panel shows the histogram of the simulated z-values from one run, the estimated mixture density  and the empirical null component . the lower panel shows the estimated local fdr from each approach.

even when the true null distribution is normal and there is a large number of observations, the unconstrained estimation approach generated undesirable results. the null component is greater than the mixture distribution at some points around the peak of the histogram. moreover, the left tail of the local fdr is close to  <dig>  indicating that some true null values will be declared as non-null depending on the threshold of the local fdr. the estimated null density follows n with the null proportion p^0= <dig> , which is quite different from the values in the simulation model. in contrast, the empirical null density estimated using the constrained estimation approach is more accurate. the estimated empirical null density follows n with p^0= <dig> . the right tails of the estimated local fdr are similar under the two approaches, which indicates that both have similar sensitivity. the left tail of the local fdr has much larger values in the constrained method, indicating a lower chance of a true null value being declared as a non-null.

we performed  <dig> simulations to compare the bias and standard deviation of estimates of the null parameters from both approaches. we chose different numbers of bins  as well as different numbers of observations . table  <dig> shows the mean and standard deviations  of the estimates of the null parameters from both approaches.

from table  <dig>  we see that both approaches yielded estimates that are nearly unbiased. the estimates from the proposed approach have much smaller standard error, especially for σ and p <dig>  the superior performance of the constrained procedure continues as the total number of observation increases. the constrained approach is not sensitive to the number of bins used for estimation when this number is large enough  for the histogram counts to be roughly proportional to the density in the bins. the unconstrained approach is more affected by the number of bins, with a smaller number leading to increased variability for the estimates of σ and p <dig>  the simulation results clearly demonstrate that the constrained approach is better at estimating the null component.

next, we compare the performance of both approaches for estimation of the local fdr at points close to the non-null component. to do that, we choose several z's on the right tail to compare the local fdr estimates with the true values. the results are shown in table  <dig>  the comparison is based on the ratio of the average of the local fdr estimates at a given z to the true value and on the relative sd of the estimates from the two approaches for the  <dig> simulations. the relative sd was computed as the sd from the constrained approach divided by the sd from the unconstrained approach.

validation using simulated gels
to further validate the proposed approach, we analyzed a set of simulated 2d gel images, which was generated by randomly perturbing an actual gel image as described in  <cit> . the  <dig> simulated gels were divided into two groups of  <dig>  to simulate the group  effect, we artificially altered  <dig> manually selected spots such that these  <dig> spots were significantly differentially expressed across groups. figures  <dig> and  <dig> show two simulated gel images from different groups with the  <dig> altered spots circled. the test statistics for the  <dig> spots were obtained using the regstatgel software. we applied both estimation approaches. the results are shown in figure  <dig>  the interval  was used for estimating the null component. the left column shows the results from the cme approach without constraint and the right column shows the results from the proposed constrained approach. the upper panel shows the histogram of z values, the estimated mixture density  and the empirical null . the lower panel shows the estimated local fdr from each approach. the '+' signs in the lower panel locate the observed points. both approaches identified all and only the  <dig> spots. both approaches yield local fdr estimates for the  <dig> spots much lower than for the other proteins. again, the unconstrained approach shows a bizarre local fdr curve.

CONCLUSIONS
similar to microarray data analysis, proteomic analysis leads to large-scale simultaneous hypothesis testing and thus carries similar challenges. the two-component model plays an important role in the microarray literature. we applied a two-component eb model for analyzing a set of 2d gel images. as demonstrated by the 2d gel data, the true null density can be very different from its theoretical form, which supports efron's innovative idea of choosing the empirical null distribution for hypothesis testing. the problem of estimating the null density is important and fundamental in the two-component eb model. efron generalized the theoretical null n to n and proposed two methods, cme and mle, for estimating the null density, which are convenient to use.

however, as shown here, neither method is devoid of problematic results, which are hard to interpret in practice. to improve the estimation of the null density, we proposed a constrained estimation approach based on the central matching method. this novel procedure naturally takes the shape of the null density and its relationship to the mixture density into account for estimation, and explicitly constrains the estimated mixture density to being no less than the null density. both the unconstrained and constrained approaches are nearly unbiased. the constrained method yields more stable and desirable estimation, as demonstrated by our simulation results. it can be generalized to include the situation where the null density comes from a family broader than the normal. the proposed approach can certainly be applied to any context where large-scale hypothesis testing occurs. here, we have constrained the null component to be no greater than the mixture density for the histogram bins. it is a simplified version of the constraint that the null component is no greater than the mixture density over the entire real line, which is much more complicated. we note that, given the smoothness of the mixture density, the simplified constraint suffices in practice. it is reasonable to assume that the local fdr is a non-increasing function near the tail areas where the z-values are farther away from the null component. to impose this non-increasing property on the estimation of the local fdr, the monotone spline regression technique  <cit>  should be utilized. we will tackle this in our future work.

the choice of the interval  may be influential for the estimation, especially if it is misspecified. when it is appropriately specified, i.e., the non-null component is nearly zero in the interval, our limited experience showed that the proposed approach is not sensitive to the choice of . however, how the interval  can affect the estimation in general needs further research.

a quite different method for empirical null estimation is based on fourier analysis  <cit> . rather than modeling the mixture density, an attractive method for modeling the local fdr directly has also been proposed  <cit> . the former is non-parametric and the latter relies on parametric model assumptions. both methods yield good estimates.

we have focused on estimating the local fdr based on test statistics. the two-component eb model is robust to correlation effects among the test statistics. it may be more informative to model the structure inherent in the data, which is certainly a challenging problem and relies on model assumptions. further research is certainly needed here.

we utilized the protein quantifications from software regstatgel with default settings. it should be noted that different software may generate different quantifications  <cit> . it is beyond the scope of the current paper to compare different quantifications.

authors' contributions
fl developed the constrained estimation approach and generated all the numerical results, as part of his doctoral work. fsm and fl wrote the paper together. all authors read and approved the final manuscript.

