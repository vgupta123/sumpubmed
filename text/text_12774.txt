BACKGROUND
polypeptide backbone amide proton-deuterium exchange  analysis provides a powerful approach for understanding protein backbone solvent accessibility and conformational change. despite the availability of nmr-based hdx methods, the ms-based approach is sensitive and amenable to larger molecules . in ms-based hdx, exchange rates are not analyzed in real time. instead, individual timepoints are quenched from a deuterium exchange reaction with cold acid then each is incubated with an acid protease  and the resulting peptides are rapidly resolved and analyzed via lc-ms then identified by correlation with prior lc-ms/ms experimental data. to minimize back exchange  of acquired deuterons with solvent protons during wet sample processing, low temperature and low ph conditions are maintained after quenching, and wet analysis should be rapid.

the generation of potentially very complex peptide mixtures by digestion with a low-specificity protease necessitates lc analysis with dynamic range and resolution, which should nonetheless be speedy in order to minimize be. minimizing potential ambiguities in peptide assignment within mass spectra from complex chromatograms requires accurate mass measurement, ms/ms on the fly, use of the reversed phase retention time parameter and/or fitting of experimental to candidate theoretical isotope distributions. our current path around these challenges has been a combination of the above, namely mass-accurate maldi-tof analysis in combination with the rapid reversed phase lc nano-fractionation of hdx experimental samples. this is preceded by a series of preliminary lc-ms/ms analyses in order to build, then saturate, a non-redundant peptide library . finally, in place of simple lc retention time analysis we correlate "z" number   <cit>  between hdx and ms/ms library experiments, for each peptide irrespective of lc instrument or solvent mixture.

to our knowledge, the use of maldi for hdx work was first reported by the komives lab, with the introduction of features such as immobilized pepsin for digestion and pre-chilled maldi plates  <cit> . additional reports built upon this approach  <cit> . however, these experiments were done with unfractionated peptide mixtures and, to our knowledge, there have been no reports of hdx by lc-maldi. strengths of maldi-tof/tof as a platform for hdx work would be expected to include accurate mass determination , spectral resolution for accurate spectral peak area analysis/centroiding and the absence of a heating step during sample introduction into the gaseous phase. being an offline technique, mutual exclusivity is maintained between sample fractionation and acquisition speeds, and between chromatographic and spectral resolution. this contrasts with some online instruments, in which the slower scan speeds required for good spectral resolution may place a practical limit on both lc gradient times and the high chromatographic peak resolution required to prevent peptide-to-peptide spectral interference and undesirable overlap of the deuterium-shift trajectories of peptides in crowded spectra – with crowding being a particular hazard at the high dynamic ranges achievable with such instruments. finally, tof-based work has the well-known advantage of yielding essentially singly charge ions as the sole species in peptide work, simplifying data interpretation. thus hdx workflows play to many of the well-known strengths of maldi-tof-based technology. for our "hdx by lc-maldi" analysis, we employed the abi  <dig> maldi-tof/tof mass spectrometer for data acquisition. in our hands, samples can be in the instrument within  <dig> min of deuterium uptake quenching . within the gaseous phase within the instrument, in which be is finite but in our hands an order of magnitude slower than at atmospheric pressure, ms data from a set of  <dig> spots can be fully acquired within  <dig> minutes of plate docking.

for hdx experiments, specialized software tools are required to reduce comprehensive datasets to manageable proportions. minimally, target peptides should be identified within mass spectra that may be numerous and crowded, from which the corresponding target 13c/2h isotope clusters must be excised, then the resulting spectral segments averaged across an xic peak and analyzed for deuteric mass shift. tools have been reported which address part or all of such workflows. these include: "autohd"  <cit> , which fits measured isotopic distributions to modeled distributions calculated via fast fourier transform, and includes a module for isotopic deconvolution of mass spectra , correlating clusters with peptides based on accurate mass and isotopic distribution. however, this software is designed for use with infused samples only . more recently, "dex" from the komives lab  <cit>  provides a tool which appears to have essentially the same function as autohd. weis & engen's "hx-express"  <cit>  also covers the latter part of the standard hdx workflow, namely calculating centroid masses for isotope clusters corresponding to chosen peptides, excised from raw spectra in ascii text format, followed by the generation of deuterium uptake rates plots as microsoft excel charts. calculations are based on weighted mean mass of the isotope cluster rather than fitting to a theoretical distribution.

scripps-florida's "the deuterator"  <cit>   was a first attempt to automate the processing of hdx data from scanning mass spectrometers, including steps prior to isotope cluster analysis. the workflow for this software includes the initial curation of ms/ms-confirmed peptide data, the automated editing of lc-ms spectral stacks from hdx experiments as delimited by the positions of hdx isotope clusters predicted from the initial curation, averaging of the stacked spectral segments, and isotope centroiding/fitting to the segment average. in the first step, peptides whose independently acquired ms/ms spectra match the protein of interest in independent database searches  are manually curated  in a listing that includes peptide sequence and a chromatographic retention time range. in the second step, a retention time-range/mass-range "box" enclosing the mass spectral cluster assigned to a peptide sequence in ms/ms experiments guides the automated editing of spectral stacks arising from lc-ms analysis of hdx timepoints. after averaging of the stacked segments and removal of spectral peaks attributable to interfering clusters, the segment average is subjected to deuterium uptake calculations by either least-squares fitting of theoretical isotope distributions to the observed cluster  <cit> , or centroid  calculation  <cit> . "the deuterator" has a spectral viewer that displays peptide information  along with xics and spectral data. deuterium buildup curves can be plotted using software such as ms excel.

at the time thta hdx work was initiated in the authors' lab , no automated tools were available for the processing of hdx data including steps prior to isotope cluster analysis. moreover, as a result of our novel lc-maldi approach to hdx sample analysis, which involves off-line peptide fractionation followed by analysis in a maldi-tof instrument, data acquisition and processing requirements are significantly different from those that are commonly used, in terms of templating of the experiment and the qualities/numbers of spectra produced. we therefore developed a series of macros into a robust toolbox  for hdx analysis via lc-maldi ms, and these are described herein.

RESULTS
raw data format in the ab-sciex  <dig> maldi-tof/tof mass spectrometer
the ab-sciex  <dig> maldi-tof/tof mass spectrometer acquires mass spectra  vs. intensity ) for samples spotted/dried onto stainless steel plates. the standard "opti-tof" plate used by the authors for hdx experiments accommodates up to  <dig> samples deposited within pre-etched spots denoted according to rows  and columns  with each having a unique alphanumeric "spot_label". via the manufacturer's " <dig> series explorer" software, the ab-sciex  <dig> is controlled, and mass spectra are acquired from individual spots then stored in an oracle database . tsquared is routinely accessed via " <dig> series explorer", leading to the display, within a graphical user interface , of a central table , each row of which corresponds to a spot on the sample plate. upon docking a plate that has been spotted for the first time , a new spotset  is initiated.

each spectral acquisition/storage event in the ab-sciex  <dig> generates and stores in tsquared:  a "raw spectrum", ie. a lengthy encoded list of m/z-intensity pairs to be joined in displayed spectral plots,  an "output peaklist", ie. a tabulated listing of the properties of all peaks detected in the raw spectrum  as interpreted by instrument-based spectrum processing methods with user-adjustable parameters. in the " <dig> series explorer" software, each primary peaklist  is treated to reduce clusters of peaks attributable to naturally-occurring isotopes of molecules with a specified molecular formula  at natural abundance, to just one peak, namely the monoisotope. as far as we could ascertain, only the resulting listing, named the "output peaklist" is available to the user, eliminating the opportunity for theoretical isotope fitting to instrument-derived peaklist data during hdx experiments. groups of spectra within a spotset are denoted as linked via a shared five-digit job_run_id , which is unique within tsquared . a new jrid is assigned whenever a new automated  data acquisition session is initiated. thus, in hdx experiments for example, all lc-maldi spots for a particular hdx timepoint are associated with one another via a shared, unique jrid.

preliminary ms/ms experiments and per-experiment ms/ms-confirmed peaklist  files
the flow of experimental data through tof2h is shown in fig.  <dig>  the directory structure employed by tof2h at the time of writing  is shown in additional file  <dig>  in which the experiment series level contains project-specific information relevant to all experiments having a particular target protein/protease combination. file types and functions are indicated in additional file  <dig> 

at the outset of a new experiment series or project, preliminary nanolc-maldi ms/ms experiments are performed  in order to build a non-redundant library of ms/ms-confirmed peptides for the target protein/protease of choice. via tof2h's "mpl curation tools" macro, the database search output from each ms/ms experiment, exported in .csv format, is reformatted to tof2h's "mpl" format , with one mpl workbook per experiment containing one mpl sheet per protein hit in the .csv. to the mpl workbook are attached the original .csv and a manually-filled spreadsheet detailing lc gradient and fractionation parameters for the overall ms/ms experiment. lc fraction numbers are converted between templated and actual spotting patterns where necessary to defeat the more limited range of spot pattern templates available within the mass spectrometer software. a mascot "expect" =  <dig>  cutoff is typically employed for inclusion in the mpl. the resulting mpl workbooks are stored at the "experiment series" level within the directory hierarchy  since they are relevant to all subsequent experiments with specific protease and target polypeptide.

"reference ms/ms-confirmed reference peaklist"  – a "dynamic", non-redundant compendium of ms/ms data for a specific protease/target protein
while mpl files are generated on a per-ms/ms-experiment basis, the refmpl  is a non-redundant compendium of all ms/ms confirmed peptides for a specific protease/target protein combination, derived from multiple mpls. the refmpl is structurally distinct from precursor mpls in that it  contains only mpl sheets , and  is continuously updatable  with mpl sheets from additional rounds of ms/ms analysis of the same specific protease/target protein  controls of additional hdx experiments, as they become available). for a round of updating, mpl files/sheets are selected individually for comparison with all extant data in a chosen refmpl using "tof2h-mpl curation tools"'. a difference or "comparison" sheet is generated, which can be transferred to the refmpl as an "update" sheet. all such operations are automatically time-stamped to preserve the chronology of progressive updating. refmpl contents can be displayed in sequence-contig format using a contig. viewing tool, showing also the % sequence coverage and the positions of any inter-contig breaks.

implementation of an hdx wet experiment, data extraction from tsquared, experiment template file
next, hdx timepoint experiments can be implemented as described in "methods". for each hdx experiment, a unique "experiment template file"  contains an "experiment template"  spreadsheet indicating the overall layout of the lc-maldi hdx experiment . the et sheet indicates, also, all spots that failed internal calibration during data acquisition . information in the et sheet co-ordinates data assembly, largely via the construction/deconstruction of file paths and extracted datafile names. additional spreadsheets within the etf include layout map for all maldi target plates used, and tables for relevant lc gradients.

the complete table contained  <dig> data rows . the et cross-references experiment-specific parameters , with acquisition-specific parameters . a horizontal format  is also available for smaller experiments . column "jr" is currently unused.

after recovery of peaklist and raw spectrum data from tsquared , the etf is built de novo by"tof2h-etf generator". prior to building, user-inputs comprise: experiment series name/experiment name, relevant spotset dumps, etf format , preset maldi spotting pattern, whether spot labels are alphanumeric or numeric, and confirmation of the macro-deduced timepoint/be timepoint corresponding to each job run id. "etf generator" then assesses the contents of the t2de data dump, detecting:  multiple acquisitions of a spot;  aniticpated lc-maldi fractions missing in data dumps; or  multiple jrids per timepoint. thereafter, cross-correlations are deduced automatically between spotset, spot, hdx timepoint and lc fraction ). all fractions whose acquisition failed internal calibration are then marked on the et sheet using, as a guide, an xml-formatted summary of all acquisition parameters for the experiment exported from tsquared. alternative etf generation tools return various levels of manual control to the above scheme .

tof2h-assembler: import and assembly of all ms peaklist data into a single worksheet
etf generation is followed by peaklist assembly. here, "tof2h-assembler" compiles all monoisotopic m/z-intensity values from the set of dumped output peaklist  ascii text files for the experiment into a single worksheet, using the .etf as a guide . each m/z-intensity pair is tagged, on the fly, with lc fraction number, and the resulting data triplets are listed in a single column triplet per timepoint/be timepoint, sorted by ascending mass. column triplets are horizontally juxtaposed by ascending deuterium uptake time . the resulting spreadsheet is supplemented with a marked-up copy of the et sheet, spreadsheets for recording program run parameters and statistical output . the resulting assembler output  file comprises the comprehensive ms peak summary for the experiment .

tof2h-processor: pre-filtering steps 
the assembled peaklist spreadsheet of the .ass file is processed, sequentially, through a series of steps by "tof2h processor", the output of each being preserved as a precursor for the subsequent step . the .ass file spreadsheet is first "pre-filtered", followed by the alignment of equivalent masses between timepoints, followed by a library search for peptides relevant to the target protein and for deuteration-dependent isotopes of library hits. results are written to an output spreadsheet. in our hands, the above steps are able to reveal exchange-active peptides of target proteins from instrument-derived output peaklist data alone . pre-filtration/mass alignment and library search steps may be coupled or run independently.

the goal of the pre-filtering steps was the achievement of just a single data triplet for each peptide detected with accurate-mass in each timepoint. in the initial filtering step, spots failing internal mass calibration  are highlighted then discarded, using the et as an index. this step was imposed to prevent the errant misinterpretation of equivalent peaks as independent ones during the inter-timepoint mass alignment steps , as was found to occur in early versions of tof2h. the second pre-filtering step is a de-isotoping one, imposed in order to pick up deuteration-time-dependent isotope shifts in the final search output, which would otherwise be masked. although output peaklists from  <dig> series explorer software had undergone deisotoping for natural abundance elemental isotopes known to occur within peptides, the superposition of an artificially-induced h-d isotope distribution led to a breakdown of the instrument's deisotoping function, and the consequent retention of many isotopomers in the instrument's output peaklist files . this effect was not unique to the ab  <dig> maldi-tof/tof's instrument software: in our hands, matrixscience's "mascot distiller", for example, was also unable to fully deisotope distributions from artificially deuterated samples . the aim of full deisotoping was not to find monoisotopic peaks, but rather, to reduce the number of list entries per isotope cluster to just one .

the deisotoping algorithm is not a fitting function. instead, it simply detects the inter-isotope mass +/- deisotoping mass tolerance, up to a user-settable maximum number of isotope increments, with "gradient factors" to permit the progressive broadening of deisotoping mass tolerance with number of isotope increments . the discarded isotope mass count increased with experimental deuterium uptake time , validating the algorithm in practice. upon checking a subset of ~ <dig> of the discarded isotope masses manually against the corresponding raw spectra, false positives  and false negatives  were very infrequent . regarding the former, closely-apposed cross-talking mass clusters may be of limited value for hdx studies anyhow, in the absence of theoretical fitting  <cit> . the very occasional occurrence of false negatives could be regarded as an unavoidable statistical outlier phenomenon. since isotopes include induced deuteration at various levels, superimposed over the mixture of 13c, 2h, 15n, 17o, 18o, 33s, 34s and 36s isotopes found in peptides at natural abundance , tof2h-processor can generate dataset-specific frequency distributions of experimental deisotoping mass errors for a given isotope mass difference , then empirically determine the isotope mass difference that nulls the distribution . the resulting value can then be used for deisotoping as an alternative to the standard h-d mass difference ).

in the penultimate pre-filtering step, tof2h-processor highlights then deletes internal calibrant masses. in the final pre-filtering step, xic clusters  are located and, within each xic group, only the data triplet with greatest intensity is retained. mass tolerances used for this and subsequent mass-equivalence determination steps are described in additional file  <dig> 

tof2h-processor: inter-timepoint mass matching and alignment 
mass/height/fraction triplets surviving the pre-filtering steps are subjected to mass correlation between timepoints. this is done by copying values for all timepoints to a single column triplet, which is then searched for mass group boundaries. each mass group is then dispersed back across the timepoint domain after calculating spreadsheet row numbers to bring equivalent masses into alignment.

tof2h-processor: hdx mass matching to ms/ms-confirmed peptide libraries
next , tof2h queries the chosen refmpl files for f1h theoretical mh+ matches to each hdx-f1h monoisotopic mh+ value in turn. in the event of hdx experiments having mixed, multiple-protein targets, multiple refmpl files can be simultaneously searched. to search for cross-matching peptide masses from a non-target protein within a protein mixture, a refmpl may alternatively be selected as a "decoy" and searched only upon finding a hit within the target protein's refmpl. within any chosen refmpl, sheets may be searched optionally . upon finding a match, the entire matching row from the row-aligned-mass table  is reconstructed in the nascent search result table  along with peptide information from the refmpl and search mass error. if the hit is a decoy, this is denoted with specific background coloration in peptide info cell. ambiguous hits  are boxed . theoretical masses corresponding to peptide adducts  may be included in a search.

"isotope-progeny" and "false progeny" check 
at later timepoints in hdx experiments, the shift in a peptide's isotope envelope may be sufficient to drive the abundance of the monoisotopic peak below the s:n threshold employed for peak detection , so that the first isotope listed for an isotope cluster is not the monoisotope. to reveal such clusters, tof2h-processor performs, for each primary hit , an on-the-fly "candidate isotope-progeny check". specifically, all entries in the pre-filtered mass list possessing mass values between the hit mass and "upper bound"  are scanned for matches to isotopic increments of the hit mass. upon finding a candidate isotope progeny hit, the corresponding row of the aligned-mass table is reconstructed in the nascent search result table, directly beneath the primary hit . the search results table thus provides an immediate visual indication of the extent of deuteric shift  for each experimental peptide detected, along with the deuterium uptake timeframe of the shift, prior to any spectrum editing. the absence of progeny does not indicate an absence of deuterium uptake, simply insufficient uptake to send the monoisotopic peak into the noise.

if the progeny hit has a corresponding value in the f1h column, then this mass is marked as a "false progeny" . false progeny almost invariably arise from a closely juxtaposed peptide cluster whose fractional mass places it within the mass tolerance window. such a situation may arise because the  deisotoping mass tolerance is set narrow  over just  <dig> isotope increments, yet progeny may be sought at "inter-plate" mass tolerance  over a "theoretical isotope shift space" that extends to upper bound .

at the conclusion of each processing stage , parameter sheets are time-stamped and updated with user-adjustable settings, pathnames for input files and program versions, and all table contents enumerated. multiple searches can be conducted and stored per processor output file.

tof2h-editor
tof2h-editor is an ergonomic tool for developing deuterium uptake rate plots from raw spectra using the tof2h-processor search results as a guide with real-time spectral segment validation. a four-quadrant multi-tabbed gui was developed whose quadrants  <dig>   <dig>   <dig> and  <dig> contain charts for:  xic;  single spectrum editing,  multi-spectrum edit/sum xic peak/peak-detect/cluster-centroid,  be-correctable deuterium-uptake rate plot/be rate plot. rates plots are built progressively across the deuterium uptake timepoints for each peptide) . editor workflow was streamlined with a target throughput of <  <dig> min processing time per peptide in a 14-timepoint experiment  with real-time validation of all data.

in tof2h-editor, windows are tabbed to allow the viewing of main and alternative charts in each. in the standard workflow, masses corresponding to search hits  are selected from a listing in quadrant  <dig> . for the selected peptide and timepoint, the xic is then displayed in quadrant  <dig>  above-noise xic fractions are highlighted in a listing in quadrant  <dig> , and a spectral segment for the first of these is read from the corresponding raw spectrum ascii and displayed in quadrant  <dig>  "upper bound'  is used as the initial upper endpoint mass for the segment. tof2h-editor recommends acceptance/rejection of the segment and, minimally, the operator visually approves or rejects the segment via buttons under the quadrant  <dig> spectral plot, or allows tof2h-editor to accept/reject without intervention. acceptance sends the segment to quadrant  <dig>  and brings the next entry from the highlighted fraction listing into the quadrant  <dig> plot. spectral segments thus accumulate as an overlay in quadrant  <dig> until the highlighted xic fractions listing is exhausted for a given peptide/timepoint. overlaid spectral segments can then be mass-normalized to the average for all , then spectral peaks in the cluster are detected via a statistical algorithm, followed by weighted mean  calculation from the areas of all detected isotope peaks for each individual segment. the overlaid segments can then be averaged in quadrant  <dig> , followed by spectral peak detection and weighted mean calculation for the averaged peak cluster. a button then sends the weighted means to the developing plot of centroid mass vs. time in quadrant  <dig> , and this operation brings the xic/first spectral segment for the next timepoint into quadrants  <dig> and  <dig> respectively. upon completion of all timepoints, the rates plot data are output and spectral processing for a subsequent peptide can be initiated. all rates data from an experiment  are output to an experiment-specific "rates archive" workbook.

additional features of "tof2h-editor" include: ability to choose from multiple searches in the processor output file and to save/restore partially edited experiments at the point at which editing was interrupted, as well as: quadrant 1:  dual-pass xic construction with user-settable tight error tolerance for internally calibrated spots, and looser error tolerance for spots whose internal calibration had failed .  overlayable xics for isotope progeny .  "absolute" and/or "relative" xics, the latter calculated relative to peak heights for internal maldi mass calibrants . quadrant 2:  semi- and fully-automated decision-making, namely approval/rejection of successive, above-noise segments across a displayed xic, via a statistical algorithm.  option to "snap" spectra to the internally calibrated peaklist-derived mz value .  defcal alert. quadrant 3: rejection of individual segments; grouping of segments for individual centroiding. quadrant 4:  calculation and display of be rates from be timepoints for each peptide in the experiment via linear regression and correction of the main rates plot accordingly.  calculation and overlay of various theoretical horizontal asymptotes, on the fly, on the developing quadrant <dig> exchange rate charts.  overlay of averaged spectral segments, from timepoint-to-timepoint, in a third quadrant  <dig> tab, with save in ascii text format that is compatible with weis & engen's hx-express macro. quadrants 1–3: vertical guidelines over charts marking f1h theoretical monoisotopic mass , current xic fraction or f1h experimental monoisotopic mass , or isotope progeny mass if such a peak is being edited . quadrants  <dig> and 3: quick adjustment of spectral endpoints ; restoration to initial values, single-click zoom-out and restore to check for interfering clusters. all quadrants: single-click chart zoom functions with full functionality in the zoomed state.

tof2h-rates browser
tof2h-rates browser  is a processing and presentational tool for combining comparing rates plots between experiments. it takes, as input, editor "rates archive" data , from which the centroid masses of individual fractions, or scans, across an xic peak are used to generate statistical populations. currently, rates browser can:  simultaneously list peptides from any number of input files, pool listings for replicate experiments under "group tabs", sort listed peptides by mass or sequence position with ability to switch between list-view and sequence-contig-view.  overlay rates plots for any number of listed input peptides with or without be correction and theoretical horizontal asymptotes, with separate be rate plot overlays.  convert y-axis between various scales including centroid mass, % of maximum theoretical deuteration, % of maximum experimental deuteration, # of deuterons, convert x-axis between either log or linear, with selectable plot legend type.  optionally interpolate missingtimepoint values.  generate output chart as mean of all currently displayed input plots  with standard deviation error bars based on the populations for individual fractions, tabulating the resulting data and saving to an output file that includes an operations log. any number of such plots can be overlaid in the output chart.  re-import analyses from an output file for subtraction of pairs of mean rates plots.  show the sequence of a currently plotted peptide marked-up with theoretically exchangeable amide protons and amide protons that are hydrogen bonded within an x-ray crystal structure.  show sasa  <cit>  plot, based on an x-ray crystal structure, of predicted solvent accessibility for each residue in the currently displayed peptide and calculate a mean sasa value across all exchangeable residues of the peptide.  calculate the "transition" timepoint and interpolated "transition" time corresponding to the %deuteration level that attains, or surpasses, a user-defined "contour" value, and pick a corresponding color for 3d visualization, based on the transition timepoint, from a user-defined color vs. timepoint series.  simultaneously "re-contour" all analyses in an output file to find the %deuteration contour that yields an optimum spread of color values for 3d visualization.  plot mean sasa value vs. "transition" time for all analyses in an output file, and "re-contour" this plot.  export gif images of all charts with plots colored by series number or by 3d color value, or flattened to grey shades with selectable line/marker styles.

CONCLUSIONS
the functions described form a core package of interconnected programs  for the semi-automated processing and analysis of hdx data generated via an lc-maldi workflow, from raw spectra and instrument-generated peaklists through be-corrected, combined deuterium uptake rate plots. additional modules and capabilities  have not been described. in the standard tpf2h workflow, an experiment series  initiates with iterative lc-ms/ms analyses of peptidase digests of the protein/under fully protic conditions until growth of the resulting non-redundant library of ms/ms-confirmed peptide masses becomes asymptotic. tof2h then chaperones instrument data from hdx experiments through a series of steps initiating with the generation of an experiment template, assembly of the contents of ~ <dig> or more individual instrument-derived spectral mass/height peaklists into a single data array containing  <dig>  or more masses, then filtering of the array and alignment of equivalent masses, peptide library searching, and systematic processing of spectral segments for each "hit" peptide in turn.

tof2h was designed with the nanoflow rates of lc-maldi in mind. we are aware of just four reports in which hdx has been done at nanoflow rates  <cit>  . if nanoflow approaches grow in popularity, specific issues may come into play such as variablility in chromatographic elution time  due to the amplification of the effects of run-to-run differences in dead volume at low flow rates. this could provide a challenge for the "fixed box" spectral editing approach  <cit>  in which hdx experimental spectra are edited on the basis of library peptide elution times. the ab initio approach employed by tof2h has proven, in our nanolc-maldi experiments, resistant to dead-time effects, especially when combined with additional peptide validation and filtering on the basis of lc elution profile . tof2h is being upgraded for general instrument  compatibility and, in this regard, the lc-maldi approach may be adaptable to the simpler maldi-tof instrument in place of the maldi-tof/tof instrumentation reported here. since tof2h accepts database search results in standard format, ms/ms-confirmed peptide library construction could be performed on any esi instrument in standard configuration followed by hdx experiments via maldi-tof. such a "divorced" analysis may avoid the need for hdx-specific modifications to esi mass spectrometers   <cit> . for this dual-instrument strategy to be effective, however, a maldi-tof with reasonably fast batch-acquisition rates would be required.

a significant amount of functionality is incorporated into the tof2h toolset, whose performance has proven to be quite precise and robust. tof2h matured with some elements in common with "the deuterator" , as may be inevitable due to the systemic nature of segments of the workflow. however, many features seem to be unique: tof2h data processing workflow incorporates real-time verification, via interactive  spectral editing, as opposed to the more fully-automated data processing approach employed by "the deuterator" which then requires manual validation as a follow-up. tof2h takes an ab initio approach to isotope cluster finding in spectra, and xic peak finding in chromatograms, as opposed to boxing predicted positions in the lc-ms spectral stacks . the ab initio approach involves scanning of the spectrometer software-generated, partially declustered peaklists for target peptides of interest prior to any spectral editing operations, then picking extant chromatographic peaks based on an examination of each xic from beginning to end. within the active fractions of an xic, tof2h verifies each spectral segment for the presence of a recognizable, well-segregated cluster prior to sending the spectral segment for summing. thus, every segment that is summed and centroided has already been automatically or visually pre-validated in multiple steps. tof2h is distinct in other ways too: it works through an entire hdx experiment from an experiment template file, it can provide an approximation of the extent of deuterium uptake over the experiment from peaklist analysis alone , and during searches of ms/ms confirmed peptide lists it has the ability to find metal adducts and to reject mass matches that could be spurious matches to non-target proteins present in an experimental mixture. tof2h has the capacity to display and process deuterium uptake plots, and it can calculate and implement be corrections on the fly in a manner that normalizes for the potential influence of lc retention time on be. finally, the tof2h package is portable, available for use on client pcs, in other labs, for desktop use, and is available from the authors at no charge.

in the general sense, hdx data processing strategies are likely to develop in an instrumentation-dependent manner, as a function of:  instrument mass accuracy/resolution of spectral acquisition;  whether or not instrument software parses spectra into peaklists and whether these peaklists are partially or fully declustered;  whether sample input to the spectrometer is in the form of fractions or a continuous sample stream;  whether mass calibration is conferred experiment-wide or on a spectrum-by-spectrum basis;  how spectral resolution may impinge on the method used for centroid mass calculation or theoretical fitting  <cit> ;  whether library hits and spectra are few enough to permit assessment/validation of each spectral segment in real time  and whether validation is automated or done on a sampling basis only;  maldi target plate layout;  chromatographic resolution;  amount of be that must be corrected, etc. we believe that within this landscape, tof2h carves a distinct niche providing, to our knowledge, a number of capabilities that are not otherwise available. tof2h has the potential, nonetheless, to be useful in a cross-platform manner without fundamental re-design.

tof2h was implemented as an integrated series of macros in ms excel/vba  though it is now undergoing conversion to other languages. it runs in ms windows xp or vista environments, and can run either locally or remotely via a lan. data inputs comprise instrument-derived peaklist and raw spectrum files , instrument database xml export files, database  search results exported as csv, and a user-filled lc parameters spreadsheet. intermediate and final data output are in the form of ms excel spreadsheets. gif files of processed deuterium uptake rates plots and other charts can be exported.

