BACKGROUND
the process of whole genome doubling  gives rise to two copies of each chromosome in a genome, containing the same genes in the same order. through an attrition mechanism known as fractionation, one of each pair of duplicate genes is lost over evolutionary time. the rare deletion of both copies of a gene can be excluded from our considerations of the interleaving patterns of deletions from duplicated regions first discovered by wolfe and shields  <cit> . the retention of one copy of each pair is what differentiates the wgd/fractionation model from approaches to gene duplication, insertion and deletion in the study of comparative genomics, pioneered by el-mabrouk  <cit> .

an important biological controversy arises from the question of whether duplicated genes are deleted through random excision - elimination of excess dna - namely the deletion of chromosomal segments containing one or more genes  <cit> , which we term the "structural" mechanism, or through gene-by gene events such as epigenetic silencing and pseudogenization  <cit> , which are "functional" mechanisms. this question is important to evolutionary theory because it speaks directly to the role of wgd, and gene duplication in general, in disrupting gene order, in creating functional innovation, and in the radiation of new species. it is a question of whether selection operates on the level of simply permitting non-lethal deletions or whether more subtle effects are in play, such as dosage balance of interacting genes.

this debate may be formulated in terms of deletion events removing a number x of contiguous genes, where x is drawn from a geometric distribution γ with mean µ. here the one-at-a-time deletion model is represented by µ =  <dig>  while the random number of deletions at a time holds if µ >  <dig>  in the latter case, the possibility of two overlapping events is handled by a biologically realistic additive run-length assumption.

in this paper, we investigate the discrimination problem of choosing between the two models based on deletion run-length statistics . this involves comparing an observed genome containing single-copy genes, originally members of duplicate pairs, to the predictions of the models for µ =  <dig> and for µ >  <dig>  this requires knowledge of the run-length distribution, given a total number of deleted genes and remaining duplicate pairs. while this is easily calculated for the case µ =  <dig>  the the distribution for the opposing scenario µ >  <dig> is not known.

in the first part of this paper, we analyze aspects of the deletion run-length distribution ψ when µ >  <dig> for the deletion-length distribution γ, including some new and surprising analytical results, the clearest of which pertain to a continuous analog of the problem. we then show why it is difficult to describe ψ in closed form or other easily computable format. in the second part, we simulate the distribution and carry out a study of the discrimination problem for various values of µ, genome size n and θ, the proportion of undeleted genes at time t. we conclude with a discussion of the remaining mathematical problems to be solved before the method can be applied to data from real wgd descendants.

RESULTS
the models
for modeling purposes, we consider a doubled genome made up, at the outset, of a pair of identical linear chromosomes each containing genes g <dig>  . . . , gn , where n is large enough so that we can neglect end effects - particular behaviors near g <dig> and gn . at each time t =  <dig>   <dig>  . . ., one such doubled gene gi is chosen at random, and a value x is chosen from a geometric distribution γ with mean µ. if x = a, then gi, gi+ <dig>  . . . , gi+a− <dig> are deleted from one of the genomes - they become single-copy genes - unless some of these are already single-copy. in the latter case, we skip existing single-copy genes and proceed to convert the next double-copy genes we encounter until a total of a double-copy genes have been converted to single-copy. note that this overlapping of deletion events never occurs if µ =  <dig> since, in this case, by definition, exactly one double-copy gene is selected and deleted in each step. for simplicity, we assume all deletions take place from one and the same genome. in a more complete model, deletion events would occur on one or the other chromosome, with probabilities φ and  <dig> − φ  <cit> .

the "skipping" procedure, introduced in  <cit> , is a natural way to model the deletion process, since deletion of part of a chromosome and the subsequent rejoining of the chromosome directly before and directly after the deleted fragment means that this fragment is no longer "visible" to the deletion process. as observers, however, we have a record of the deleted genes, as one copy of each gene must be retained in the genome.

overlapping deletion events and skipping result in the creation of runs of single-copy genes whose length is the sum of a number of geometric variables. the sum of r identical geometric variables produces a negative binomial distribution with parameter r, but the skipping process does not involve the sum of identical random variables, since a deletion with a large value of a is more likely to overlap an existing single copy region than a deletion with small a. thus, at any point of time t >  <dig>  the distribution ψt of single-copy run lengths will tend to contain a higher frequency of runs of length  <dig>  and of very long runs, than would be generated by the negative binomial. on the other hand, the distribution of run lengths of the remaining double-copy genes is geometrically distributed with a probability distribution ρt, where the mean νt decreases with t  <cit> .

analysis of overlap probabilities
an attempt to determine ψt analytically starts with the calculation of how many deletion events have overlapped to form a run of single-copy genes at time t. in  <cit> , we derived a formula to predict whether a deletion event would create a new run of single-copy genes, probability p0; overlap exactly one existing run, thus extending it without changing the total number of runs, probability p1; overlap two runs, producing one larger combined run in place of the two pre-existing ones, probability p2; and so on. other probabilities deal with the events that a run "touches" a pre-existing run without overlapping it. these probabilities all depend solely on γ and ρt. for example, we examine the case of p <dig>  the other probabilities are all formulated in analogous ways.

the proportion of genes in single-copy runs of length l is lρt/νt, where νt= ∑l>0lρt. the probability p <dig> that a deletion event falls within a run of double-copy genes without deleting the terms at either end is

  p0= ∑l>2lρtvt ∑j=2l-11l ∑a=1l-jγ=1vt ∑l>2ρt ∑j=2l-1∑a=1l-jγ=1vt ∑l>2ρt ∑a=1l-2γ 

where j indexes the starting position of the deletion within a run of length l, and a is the number of genes deleted in the event.

this formula requires quadratic computing time, but the pi for higher i, require polynomial time of degree i +  <dig>  here we exemplify with p <dig> to show that these probabilities can in fact be reduced to closed form, so that computing time is a negligible constant. the formula in , when expanded, consists of a number of partial sums of the geometric distributions γ and ρt and means of these distributions, all of which are readily reduced to closed form, plus sums of terms of the form 1 -1μ1 -1vtl and l1 -1μ1 -1vtl, which themselves can be considered in terms of a geometric distribution with mean ζ, where

  1-1ζ= 

then  reduces to:

  p0=νt-12νt  

for large νt, i.e., during the early stage of the process,

  p0≈vtμ+vt1-1vt 

typically, µ is somewhere between  <dig> and  <dig>   <cit> , and νt of the order of  <dig> or  <dig>  thus p <dig> is initially only slightly less than  <dig> but declines rapidly as νt decreases exponentially.

we proceed in an analogous way to derive closed forms for p <dig>  p <dig>  . . ., but it is perhaps more instructive here to present the continuous version of the deletion process. here the two identical chromosomes at time t =  <dig> are linear segments, long enough in comparison with the other parameters of the model so that end effects can be ignored. at each time t =  <dig>   <dig>  . . . , a random point g is chosen on the chromosome, and a value x is chosen from an exponential distribution

  f=1μe-aμ ,a≥ <dig> 

with mean µ. if x = a, then the segment  is deleted from one of the genomes -  becomes a single-copy region - unless part of it is already single-copy. in the latter case, we skip existing single-copy regions and proceed to convert the next double-copy region we encounter until a total measure a of double-copy regions have been converted to single-copy.

in analogy with the discrete model, the combined length of the remaining double-copy segments is exponentially distributed according to probability distribution σt, with a mean νt that decreases with t.

the proportion of undeleted regions accounted for by segments of length ldl is lσlvtdl, where νt= ∫ 0∞lσdl. then the probability p <dig> that a deletion event falls completely within an undeleted segment is

  p0= ∫ l=0∞lσtvt ∫ x=0l1l ∫ y=0l-xfdy dx dl 

carrying out the integrations, we find

  p0=vtμ+vt 

which is reminiscent of the relation  in the discrete case with large νt.

the probability p <dig> that a deletion event overlaps exactly one existing run of deletions is:

  p1=1vt ∫ l=0∞∫ z=0∞σtσt ∫ x=0l∫ y=l-xl-x+zfdydxdzdl 

  =vtμ+vtμμ+vt 

it can be proved by induction that the probability a deletion event overlaps exactly q existing runs of deletions is:

  pq=vtμ+vtμμ+vtq 

thus we have the surprisingly uncomplicated result that the number q of pre-existing runs of single-copy regions overlapped by a new deletion event is geometrically distributed on q =  <dig>   <dig>  . . . with parameter µ/.

on the run-length distribution
although having a closed form for pq constitutes progress towards the computation of the run-length distribution ψt, or eventually towards some analytical results on it, how to find this distribution remains a difficult question. as mentioned in the previous section, long deletion events will be involved in more skipping than small ones. this is illustrated in figure  <dig>  where runs built from a small number of events tend to be composed of shorter deletions, especially when µ is large. had we just added independent samples from a geometric distribution, the curves in the figure would have been horizontal lines.

how to account for the distorting effect of skipping on the run-length distribution will require additional insight and research. in the interim, we may use simulations to study the discrimination problem.

simulations
we first simulated the fractionation process for all combinations of the following parameter values:

• gene number n =  <dig> to  <dig>  in steps of  <dig> 

• µ =  <dig>  to  <dig> , in steps of  <dig> .

• proportion of the genes deleted,  <dig> − θ =  <dig>  to  <dig> , in steps of  <dig> .

for each combination of the parameters µ, n and θ, we calculated the distribution of run lengths l for single-copy regions, and similarly for double-copy regions. the simulation was repeated  <dig> times and the frequencies of length  of runs of deleted genes were averaged over the  <dig> trials to get a reasonably accurate estimate of the cumulative fµ,n,1−θ . similarly we estimated the cumulative gµ,n,1−θ for runs of remaining double-copy genes.

once the cumulative distributions were established, we then carried out the actual discrimination study. for each value of µ and n, we sampled  <dig> new individual trajectories of the deletion process until  <dig> − θ =  <dig> % of the genes were deleted. for each value of  <dig> − θ =  <dig> ,  <dig> , . . . ,  <dig> , we created "bins" corresponding to the fifteen values of µ for which we had constructed cumulatives. then for each sample si, at each  <dig> − θ =  <dig> , . . . ,  <dig>  we counted the frequency of runs of deleted genes of length =  <dig>   <dig>  . . . and constructed a cumulative distribution. we calculated the kolmogorov-smirnov statistic dμ,n,1−θbetween the sample cumulative and the previously established distribution fµ,n,1−θ for each fifteen values of µ and assigned the sample to the bin corresponding to the minimal value of this statistic, which was our estimate μ^ for that sample.

with all four values of n in figure  <dig>  the most accurate inference is made for µ =  <dig>  the gene-by-gene model. this brings us back to the original problem of discriminating between the gene-by-gene "functional model"  and the random excision "structural" model . figure  <dig> shows the frequency with which we estimate μ^= <dig>  for various values of µ and n =  <dig> or  <dig>  as a function of  <dig> − θ, the proportion of genes deleted. the upper curves in the figure show that we can correctly identify the µ =  <dig> model around 70-85% of the time; more for n =  <dig> and less for n =  <dig>  as long as  <dig> − θ < 50%. in other words, the type i error of a test of h <dig> : µ =  <dig> against h <dig> : µ >  <dig> with these parameters and procedures, is about 15-30%. the lower curves show that incorrectly inferring μ^= <dig> occurs around 20% of the time when µ =  <dig> , but very rarely for µ =  <dig>  or even µ =  <dig> , until  <dig> − θ begins to exceed 50%. in other words, if now hm : µ = m, for some constant m >  <dig>  is the null hypothesis and h <dig> is the alternative, then the type i error is very small unless m is very close to  <dig>  or  <dig> − θ is large .

up to now, we have examined only runs of single-copy genes. what of the runs of remaining double-copy genes? figure  <dig> compares some of the results from the same simulations as figure  <dig>  but using the cumulative gµ,n,1−θ for runs of double-copy genes as well as fµ,n,1−θ for runs of single-copy genes. the main observation is that the double-copy approach systematically infers µ =  <dig> with higher frequency for small values of  <dig> − θ, whether or not this inference corresponds to the generating µ. it systematically infers µ =  <dig> with lower frequency for large values of  <dig> − θ, again whether or not this is correct. these simulations establish ranges of values of n, µ and  <dig> − θ for which we can and cannot discriminate between the two models.

CONCLUSIONS
in this work we have made some progress in deriving the run-length distribution ψt for single-copy regions, although this problem is still not completely resolved. from an analytical point of view, it is unexpected and interesting that in the continuous version of the problem, the number of pre-existing runs overlapped by a deletion event follows a geometric distribution.

the simulation study showed the much greater difficulty in distinguishing between the structural and functional models when the mean µ of the deletion size distribution is  <dig>  rather than  <dig> , when n is  <dig> rather than  <dig>  and when the proportion of genes deleted is bigger than 50% rather than less than 40%. the latter effect is also apparent in empirical studies  <cit> .

our simulation results are based on a "binning" strategy for determining μ^ for the purposes of discrimination, rather than an asymmetrical testing approach comparing the hypotheses µ =  <dig> and µ >  <dig>  this is justified by the lack of any biological significance, and high rates of error, in comparing μ=1+∈ and µ =  <dig> for very small  ∈, as well as the global picture it offers of the degradation of discriminatory power as a function of µ, n and θ.

this work has for the first time enabled the systematic discrimination between the two models of duplicate deletion following wgd. future research will continue on the analytical determination of ψt as well as extension to the "two-sided" deletion models proposed in  <cit> . eventually, we will have to allow processes of genome rearrangement to disrupt runs of single-copy genes or double-copy genes, as in  <cit> . it is these kinds of model that will eventually be useful for analyzing data from real genomes.

competing interests
the authors declare that they have no competing interests.

authors' contributions
all authors participated in the research, wrote the paper, read and approved the manuscript.

