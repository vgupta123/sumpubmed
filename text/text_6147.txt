BACKGROUND
high-throughput genomic technologies are revolutionizing biology and medicine and provide new challenges in the way we analyse and interpret these large amounts of data. to this end it is necessary to integrate the acquired knowledge into a flexible data structure and the gene ontology  consortium has provided a widely used solution to this challenge by describing properties of genes using a controlled vocabulary and representing them in a directed acyclic graph, which groups genes in categories  <cit> . consequently, there has been an explosion in the number of methods to investigate large-scale gene expression data in the context of these functional annotations. the principle underlying most of these methods is the identification of an enrichment in particular gene annotations among a selected set of genes compared to a reference set – for example, an enrichment of differentially expressed genes in certain annotation categories compared to all other genes on a microarray. the significance of the enrichment is tested for example using tests based on the hypergeometric or the binomial distribution, fisher's exact test, or a chi-square test. various programs have implemented this approach . ben-shaul and others have argued that in many cases such a discrete distinction between "differentially expressed" and "not differentially expressed" genes is arbitrary and can reduce the power of the study to identify enriched categories . therefore, methods have been proposed that instead use a measure of choice to rank the genes, and then use rank-based tests such as the wilcoxon-rank test or the kolmogorov-smirnov test to test if genes that belong to one category differ in their ranks from genes that do not belong to the category . in both cases – rank-based classification or discrete categorization – and independently of the statistic that is used to define the significance of a single category, one needs to correct for the large number of tests performed in such an analysis. this is a challenging task, since the tested categories are highly inter-dependent: a single gene is usually annotated in many categories, and categories subsume other categories. one approach for the correction is to compute the family-wise error rate , i.e. to estimate the probability that at least one false positive category exists among those that are labelled significant. applications exist that use a simple bonferroni correction or other more powerful fwer correction procedures  <cit> . however, it has been noted that controlling the fwer might be too conservative in many genomic applications and that it might instead be more useful to determine the false discovery rate   <cit> , i.e. the proportion of false positives among all significant features, since a known proportion of false positives can easily be tolerated for an increase of power in these contexts . several approaches to estimate the fdr have been proposed  and some have been integrated in various functional profiling applications . many methods rely on permutations to estimate the fwer or fdr, since the dependency and structure of the annotations makes it difficult to find analytical methods to do the same  <cit> . usually the gene lists are permuted, though some approaches also allow the permutation of sample labels  <cit> , this allows not only a correction of dependence among the categories, but also for dependence among the genes  <cit> .

in this paper we present the program package called func, which includes and extends on the methods described above: it allows selecting among four different kinds of tests, depending on the type of data to be analysed :  a test based on the hypergeometric distribution for analyzing binary associated variables ,  a wilcoxon rank test for a continuous associated variable ,  a binomial test to compare the ratio of two counts per gene in  and  a  <dig> ×  <dig> table test that is suitable for a mcdonald kreitman type test to infer selection on genes from divergence and polymorphism data at two types of sites, like synonymous and non-synonymous sites in a coding sequence  <cit> . the two latter methods have not previously been implemented in any go statistical analysis application, and should be especially useful for analyzing genome-wide dna sequences. these approaches, as implemented in func, have already been used for the analysis of the chimpanzee genome sequence  <cit> . func also uses permutations of genes to calculate for each category, i.e. each p-value cut-off, a fwer and a fdr estimate. in addition, func provides a global test statistic to gauge the significance of the complete data set, which has not been implemented in other programs to date. the global test statistic tests whether the complete distribution of functional annotations differs from a random distribution and in this way allows determining an overall significance for the data set. another method implemented in func is the ability to refine the results by eliminating extraneous categories marked as significant. some categories are significant solely due to the fact that their subtree includes categories that are significant. thus, their significance does not provide additional information beyond that of their descendant categories and their exclusion from lists of significant categories can be helpful for interpreting and representing the results. in summary, func provides a useful and sensitive tool to analyse annotations in the context of a variety of genomic data.

implementation
overview
func is a set of four command line tools that allow the analysis of a set of genes with respect to their annotation . it is particular useful when analyzing ontological annotations such as provided by the gene ontology consortium  <cit>  or evoc  <cit> , but can be easily adapted to any other annotation. each of the command line tools performs a specific statistical test on a certain type of input data. the user can select the ontology or subtree of an ontology on which the test should be performed, and restrict the tested categories to those containing some minimum number of genes. for each category in the ontology the statistical test of the used tool is performed resulting in the "raw p-value" for that category. since many categories are tested and the tests are inter-dependent in a complex manner, func compares the test results to results obtained from random datasets in which the gene-associated variables are permuted. this specifies the null hypotheses, namely that there is independency between the associated variable and the annotation of the genes. these random sets are used to calculate for each category, i.e. each raw p-value cut-off, two corrected p-values: a resampling-based false discovery rate   <cit>  and the family-wise error rate   <cit> . the fdr is an estimate of the proportion of categories that are false positives among all the categories with a raw p-value equal or lower than the given category. the fwer is the estimated probability that at least one false positive category exists among all the categories with a raw p-value equal or lower than the given category. in addition, func compares the distribution of raw p-values of the random sets with the distribution of raw p-values of the data set in order to obtain an overall significance p-value – a kolmogorov-smirnov-type test against a single null hypothesis stating that the gene associated variables are randomly distributed across all the categories. this global test statistic is useful since it is expected to be sensitive even if a weak signal is distributed among many categories. the overall significance value can be used to decide whether the data set is at all differently distributed from random. if this is the case, one can then pick a fdr to decide which of the categories deviate significantly. this procedure might be preferable to a-priori selection of an fdr or fwer significance level or changing them post-hoc.

the output of func is a summary of the above-mentioned statistics as well as a table listing the analysed categories and the associated raw and corrected p-values. after picking a desirable p-value cut-off, the user can run the refinement algorithm to identify those significant categories that provide the most concise information, i.e. to identify those categories whose significance does not depend solely on significant descendant categories.

category tests
each of the four func tools is designed for one of the possible category tests: hypergeometric test, wilcoxon rank test, binomial test and  <dig> ×  <dig> test . for each test, two p-values for both sides of the test statistic are calculated, which allows the detection of an enrichment or a depletion of gene-associated variables among the categories. a detailed description of the algorithms used can be found in the supplement. briefly, the hypergeometric test takes a binary variable  that is associated with each gene  and uses the hypergeometric distribution to calculate for each category the probability to "draw" this many or more  differently expressed genes from the top category of the subtree selected by the user.

the wilcoxon rank test differs from this scheme only in that it takes a floating point variable instead of a binary variable and compares the ranks of the genes in the tested category with the ranks of the remaining genes in the top category. this test is useful when it is not possible to clearly classify genes to two distinct classes – as is often the case in microarray experiments. this kind of test has also been used previously in the comparison between the human and the chimpanzee genome to identify go categories, which contain an excess of fast or slowly evolving genes  <cit> .

whereas the hypergeometric and wilcoxon rank tests compare the distribution of one gene-associated variable among categories, the "binomial test" compares the ratio of two gene-associated counts among categories. each gene is associated with two counts, and the test determines whether the ratio of these counts in a category is significantly different from the ratio in the top category. the binomial test has been used to identify categories containing more amino acid changes on the human lineage compared to the number of changes on the chimpanzee lineage, and to identify categories that have a higher than expected number of amino acid changes between human and chimpanzee compared to changes between mouse and rat  <cit> . this test might also be useful when comparing counts of expressed sequence tags e.g. from two sage  libraries  <cit> .

the fourth test takes a  <dig> ×  <dig> table as the associated gene variables, sums them over each category, and uses a fisher's exact test or a chi-square test  to test whether the two properties  are independent of each other. note that in contrast to the other three tests, the calculated p-value is not dependent on an expectation taken from the top category. this test can be useful to conduct a mcdonald-kreitman type test  <cit>  on go categories. a mcdonald-kreitman type of test compares the number of fixed substitutions and the number of polymorphisms at two classes of sites, such as synonymous and non-synonymous sites. an excess of fixed non-synonymous substitutions can indicate the action of positive selection, whereas an excess of non-synonymous polymorphisms can indicate the presence of slightly deleterious amino acid variants . the  <dig> ×  <dig> contingency table test implemented in func calculates two separate p-values to test for an excess of non-synonymous substitutions and an excess of non-synonymous polymorphisms, respectively . the availability of a large  genome-wide measurement of polymorphisms in humans and other species together with the already available data on substitutions should make this test very useful in the near future.

it is important to keep in mind that for all the four tests the power to reject the null hypothesis differs among different categories since categories differ in their amount of genes and/or their amount of gene associated counts. hence, the category with the biggest size effect is not necessarily the most significant category and vice versa . also note, for the binomial test and the  <dig> ×  <dig> contingency table test, that the null hypothesis that func tests is that the genes and not the gene associated counts are a random sample in a category. as a consequence the raw p-values of these two tests should be considered more like an arbitrary test statistic which is compared to the distribution of p-values obtained by permuting genes and not single gene counts .

correction for multiple testing
when many hypotheses are tested at the same time it is expected that a number will appear significant even if all the null hypotheses hold. therefore, in order to confidently reject some of the null hypotheses, it is necessary to correct for multiple testing. the types of large-scale genomic experiments that have become possible during recent years, in particular microarrays, have revived interest in different statistical methods that deal with the issue of multiple testing . the issue is somewhat complex, since  the tests are interdependent in a complex manner,  the power of each single test is often low,  more than one of the tested hypotheses are usually truly not null and  rejected hypotheses can be regarded as candidates for additional tests, so that less conservative significance levels can be tolerated for an increase in power. all of these issues are also relevant in the context described here, in particular the complex interdependency of the tests. to overcome the interdependency, we chose to use permutations, i.e. the randomization of gene-associated variables, in order to model the distribution under the null hypothesis that gene associated variables are independent of the gene annotation. this permuted data, the random sets, can be used to estimate the family wise error rate, i.e. the probability that among the tests that are declared significant one or more are false positives  <cit> . this approach is more powerful than the conservative bonferroni correction and has, in the context of go analyses programs, been implemented for example in funcassociate  <cit>  and is also implemented in func. for any of the four tests described above one can calculate a raw p-value cut-off for which the fwer is e.g. 5%. this approach is, however, very strict, and a certain  fraction of false positives among the significantly labelled tests can often be tolerated for an increase of power. this is the reason why the false discovery rate  has gained popularity within the genomic community. the fdr is  defined as the expected proportion of falsely rejected hypothesis among all rejected hypotheses. different methods exist to estimate the fdr, differing in how they treat the case that no hypothesis is rejected and in how they estimate the number of falsely rejected hypotheses . several programs that analyse functional annotations have implemented fdr methods , most often using the procedure of benjamini and hochberg  <cit> . in func we implemented a similar method by yekueteli and benjamini that is well-suited for resampling methods  <cit> . although it has been shown that the method also works well under positive dependency among the tests  <cit>  and an easy  correction method can be used under different kinds of dependencies  <cit> , it is not entirely clear whether it is well-suited for the kind of strong dependencies that exist among the functional annotations. further, it is noteworthy that the current methods for computing the fdr are strictly valid only under the assumption of subset pivotality, i.e. under the assumption that the significance of one test does not depend on the significance of another test. however, this assumption is violated for the hypergeometric test, the wilcoxon rank sum test and the binomial test, since the expectation for each category is derived from the top category, which includes the other categories tested. hence, if one or more categories truly deviate from the null hypothesis, this influences the null expectation of other categories. however, there is often no reasonable alternative for estimating the expectations independently. in practice, all these issues will not be relevant for data sets which deviate considerably from a random functional annotation. but for data sets with a weak signal, the fdr might not qualify as a well-suited measurement to determine whether there is any  indication for a non-random distribution of the gene-associated variables among functional categories. therefore, in addition to estimating the fdr and fwer rate of the data set, we developed a method to test directly the "global null hypothesis" that gene-associated variables are randomly distributed among categories.

testing the global null hypothesis
as was pointed out above, when the strength of the deviation may be weak, it is useful to test whether the data shows any deviation from the random distribution before embarking on finding out which categories deviate from others. for this we calculate a p-value to test the null hypothesis that the gene-associated variables are randomly distributed among all categories. this is done by looking over all possible cut-offs of the raw p-values between  <dig> and  <dig> , and for each of them calculating the proportion of random sets that have as many or more categories showing this many or fewer raw p-values. then, in a similar fashion to the kolmogorov-smirnov test, we find the cut-off value for which the deviation from the random sets is maximal. we then do the same test  to every one of the random sets. the overall p-value is then determined by calculating the proportion of random sets that have the same or a larger maximal distance . if this p-value is low , one can reject the null hypothesis that the gene-associated variable are distributed independently of their functional annotations.

this measure should be especially helpful, when the signal is weak and/or is distributed among many categories and should be more sensitive than an fdr estimate . the fdr can be interpreted using an analogy of how much money one would be willing to waste. testing the global null hypothesis can determine whether it is worth spending any money in the first place, and the fdr can subsequently be used to estimate how much money one is willing to waste.

refinement
once one is confident that categories showing enrichment or depletion of the gene-associated variable exist, and after choosing a suitable raw p-value as a cut-off, based on a certain fdr or fwer, it is useful to specify the deviation as precisely as possible. this means one wants to exclude categories that are significant solely because they contain significant descendant categories. the refinement algorithm starts from the leaves , recursively removes the genes annotated in significant descendant categories, and tests the remaining genes in a significant parent category again . in this way the list of all significant categories can be limited to the most specific categories, which make the results more interpretable and manageable. this algorithm is similar to the elim algorithm proposed recently  <cit> . however, in contrast to the elim or the related weight algorithm  <cit>  we interpret the results of the refinement like a post hoc test. consider a hypothetical example where the gene associated variables are significantly overrepresented in the category carbohydrate metabolism, which is due to an overrepresentation in the two descendant categories glycolysis and tricarboxylic acid cycle. it is true regardless of the refinement that genes annotated in carbohydrate metabolism are overrepresented in the data set. that genes annotated in glycolysis and tricarboxylic acid cycle are overrepresented is just the more specific statement. we find it helpful and transparent to distinguish between significant categories and the most specific significant categories and hence find it useful to separate these two analyses.

RESULTS
to demonstrate the usefulness of func we analysed a dataset of  <dig> orthologous genes compared between human, chimpanzee, mouse and rat  <cit> . we asked whether there are go categories that evolve faster than expected in either rodents or primates. for this purpose we added the number of amino acid changes for a given gene between mouse and rat  and human and chimpanzee  and performed the binomial test described above . this procedure treats all genes in a category essentially as a single gene. note that the p-value of this test is only nominal since it assumes independency among amino acid substitutions, but that the global p-value, the fwer and the fdr calculated by func is based on the permutations across genes and hence controls for the dependency of amino acid substitutions within genes. for brevity, we limit the analysis here to the results from the ontology molecular function.

of the  <dig> genes,  <dig> genes have an annotation in the ontology molecular function . the binomial test is two-sided, i.e. for each category it tests whether there are more amino acid changes in primates and tests whether there are more amino acid changes in rodents. the expected value is given by the ratio of amino acid changes of all genes annotated in the ontology molecular function. the global p-value is  <dig>  and  <dig>  for rodents and primates, respectively, since in the  <dig>  permutations performed only  <dig> and  <dig> sets, respectively, had maximal ranks of p-values that were equal or higher than for the data set . this indicates that categories exist, which evolve faster in primates than in rodents and faster in rodents than in primates. this is not due to different mutation rates in rodents and primates among the different categories since changes at silent sites do not show such a significant grouping when tested accordingly . interestingly, none of the categories that evolve faster in primates had a fdr or fwer estimate below  <dig> , exemplifying that the global p-value is more sensitive in detecting a deviation from the null hypothesis than fdr or fwer estimates for single categories. at a fdr threshold of  <dig> ,  <dig> categories evolve faster in primates. in order to get the categories with the most specific annotation, we ran the refinement algorithm at the corresponding raw p-value, which resulted in  <dig> categories . these categories might evolve faster in primates because they experienced more positive selection in primates than in rodents or because they evolved under less constraint in primates than in rodents . olfactory receptors, which are also identified here , are thought to have evolved under less constraint in primates than in rodents since a higher fraction of pseudogenes is found in primates  <cit> , indicating that the more sensitive global test statistic identified biologically relevant categories in the analysed example.

a number of genes analysed in category; b number of amino acid changes between mouse and rat; b number of amino acid changes between human and chimpanzee; b number of amino acid changes between mouse and rat; dratio of primates/rodents; e the ratio in the top category, in this case molecular function, gives the expected ratio.

CONCLUSIONS
we present the software package func, which enhances the ability of researchers to correlate their data with gene annotations that are often provided in the form of ontologies. func currently has two main drawbacks. first, it does not provide any graphical representation of the results such as those provided e.g. by gominer  <cit> . second, it allows no easy way to permute sample-associated variables instead of gene-associated variables. this has been shown to be useful in some cases  <cit>  and has been implemented by some programs  <cit> . however, despite these two drawbacks, func provides considerable advantages over existing tools: it integrates four kinds of tests suitable for the analysis of gene expression data and dna sequence data of which two  are not implemented in other go analysis programs. func also provides two established multiple correction methods  as well as a new overall significance estimate, which should be useful especially for data with a weak signal. further, the implemented refinement algorithm is a useful and transparent method for extracting the most specific biological information from lists of significant go categories. finally, func is available as a well-documented stand-alone tool for unix/gnu linux platforms as well as accessible via a web service, which makes its use more flexible than many other available go analysis tools. thus, func provides flexible, statistically rigorous and novel tools to analyse the functional annotation of a variety of genome-wide data.

availability and requirements
project name: func

project home page: 

operating system: unix/gnu linux

programming languages: c++, perl, bash

other requirements: r mathematical library 

license: gnu gpl v <dig> 

authors' contributions
kp developed and wrote the software, bm, hhd, gw and pk contributed to and conceived earlier versions of the software, er and sp provided resources, ml conceived the statistical analyses, we conceived the study and wrote the manuscript. all authors read and approved the manuscript.

supplementary material
additional file 1
description of used methods and algorithms

click here for file

 additional file 2
input file used for comparing amino acid changes between primates and rodents

click here for file

 acknowledgements
we are grateful to janet kelso for comments on the manuscript. this work was supported by the bundesministerium für bildung und forschung, the max planck society, the european commission's sixth framework programme for new and emerging science and technology  and the deutsche forschungsgemeinschaft.
