BACKGROUND
chromatin immunoprecipitation  is a well-characterized technique for enriching regions of dna that are marked with a modification , display a particular structure , or are bound by a protein , in vivo, across an entire genome  <cit> . chromatin is typically prepared by fixing live cells with a dna-protein cross-linker, lysing the cells, and randomly fragmenting the dna. an antibody that selectively binds the target of interest is then used to immunoprecipitate the target and any associated nucleic acid. the cross-linker is then reversed and dna fragments of approximately 200– <dig> bp in size are isolated. the final chip dna sample contains primarily background input dna plus a small amount  of additional immunoprecipitated target dna.

several methods have been used to identify sequences enriched in chip samples . one of the most recent utilizes high throughput signature sequencing to sequence the ends of a portion of the dna fragments in the chip sample. in a typical chip-seq experiment, millions of short  sequences are read from the ends of the chip dna. the reads are mapped to a reference genome and enriched regions identified by looking for locations with a 'significant' accumulation of mapped reads. calculating significance would be rather straight forward if the distribution of mapped reads were random in the absence of chip . this does not appear to be true. the method of dna fragmentation, preferential amplification in pcr, lack of independence in observations, the degree of repetitiveness, and error in the sequencing and alignment process are just a few of the known sources of systematic bias that confound naive expectation estimates.

several methods have been developed to identify and estimate confidence in chip-seq peaks. johnson et al. used an ad hoc masking method based on their control input data and prior qpcr validated regions to set a threshold and assign confidence in their nrsf binding peaks  <cit> . robertson et al. estimated global poisson p-values for windowed data using a rate set to 90% the bp size of the genome. to estimate fdrs, a background model of binding peaks was generated by randomizing their stat <dig> data and choosing a threshold that produced a  <dig> % fdr  <cit> . mikkelsen et al. took a remapping strategy that involved aligning every  <dig> mer in the mouse genome back onto itself to define unique and repetitive regions. for each chip-seq dataset, "nominal" p-values were calculated by randomly assigning each read to a "unique region" and comparing the observed randomized  <dig> kb window sums to the real  <dig> kb window sums  <cit> . mikkelsen et al. also employed a hidden markov model that awaits description. fejes et al. mention a monte carlo based fdr estimation based on read location randomization in their find peaks application note  <cit> . lastly, valouev et al. use a variety of promising enhancements  to call binding peaks from chip-seq data and estimate fdrs base on control input  <cit> . only the johnson et al. method makes use of input data to control for localized systematic bias. this is unfortunate given the presence of clear systematic bias in chip-seq data, see below. additionally, none of the methods reported evaluation of their confidence estimations using spike-in data or simulated spike-in data where actual fdrs can be compared to estimated confidence metrics. this is critical for evaluating the usefulness of any novel chip-seq peak discovery method.

RESULTS
in this paper we have 1) developed several methods to identify chip-seq binding peaks while controlling for systematic bias 2) examined three methods for estimating statistical confidence in the peaks without prior knowledge 3) characterized these methods using both simulated spike-in data and a reanalysis of a published chip-seq dataset and lastly, 4) created an open source software framework to support the development of next generation sequencing data analysis applications . included in the current useq package are the low level chip-seq analysis applications described here for converting mapped reads into chromosome specific summary tracks and enriched regions as well as numerous high level analysis applications for intersecting genomic regions, finding neighbouring genes, scoring binding sites, etc. a user guide, table of available applications, and other supporting documentation are available on the project website and with this manuscript, .

systematic bias
a visual inspection of several chip-seq control input datasets  <cit>  revealed clear evidence of non-random mapped read enrichment. the bias is in some cases obvious  and worth removing prior to analysis. the bias is also subtle  and not so easily minimized. these false positives are seen in control input data and chip data in both unamplified and pcr amplified datasets, figure 1a. if uncontrolled, the impact of these false positives can be quite substantial. figure  <dig> shows the number of false positives in the johnson et al. control unamplified input data as a function of the number of window reads and bonferroni corrected global poisson p-values. at a conservative p-value threshold of  <dig>  , more than  <dig> false positives are obtained, at a threshold of  <dig> × 10- <dig> , >  <dig> false positives are apparent.

chip-seq peak detection methods
the need to control for systematic bias motivated us to develop and test several methods to minimize the number of false positives using spike-in datasets. spike-in datasets have proven to be instrumental in evaluating novel chip-chip <cit>  and expression microarray <cit>  analysis methods. they provide a known truth. as such, one can ask how many spike-ins and non-spike-ins  are recovered using a particular method at a given threshold. by fixing an acceptable fdr  the method that returns more spike-ins is, by definition, better. spike-in data is also useful for measuring the accuracy and consistency of novel confidence estimators. presently, experimentally derived chip-seq spike-in datasets do not exist. therefore, we generated a close approximation by adding simulated chip-seq reads to experimentally derived input control data. our test datasets included two spike-in simulations that were created by adding  <dig> spike-in regions containing 2– <dig> reads each to human input data from johnson et al. and  <dig> spike-in regions containing 1– <dig> reads to a combination of mouse input data and data showing little to no enrichment from mikkelsen et al. the two datasets were made deliberately different to test the robustness of our methods against different data density, different alignment methods, and different levels of noise. the human dataset  represents one with low coverage;  <dig>  million reads in each of the three samples: input  <dig>  input  <dig>  and input  <dig> + simulated chip reads. the mouse dataset  represents one with relatively high coverage,  <dig> million reads in each sample, and likely more noise due to the inclusion of some low-level chip enrichment.

four different peak identification methods were compared using the two simulated chip-seq datasets. each method made use of a sliding window  to generate summary scores for all interrogated regions in the genome. overlapping windows were combined into candidate binding peaks by merging those that exceed a given threshold. binding peaks were then scored for intersection with the spike-in key and the true positive rate  and the fdr calculated. figure  <dig> shows a plot of tprs against fdrs over a variety of thresholds for each of the four methods. the "sum" method uses no input control data but simply sums the number of reads falling within each window. the "difference" method is a subtraction of the sum of the reads in the chip data minus the sum of the reads in the input control data for each window. the "normalized difference" method takes the difference and divides it by the square root of the sum, an estimation of the standard deviation. lastly, binomial p-values were calculated as described in the methods. of the four, the normalized difference and binomial p-value out performs the others at all fdrs with the normalized difference slightly better in the small dataset. in situations where control data is available, regions in the genome with significant global poisson p-values can be identified and removed prior to windowing. application of this pre filter significantly improved the difference window score making it essentially equivalent to the normalized difference score and slightly improved the performance of the other window tests . however, the risk of picking an inappropriate p-value threshold for pre filtering the data may preclude its usefulness.

the second fdr approximation  uses storey's  <cit>  q-value method to convert window binomial p-values to q-values. at the core of the q-value conversion is the assumption that under the null, input p-values are uniformly distributed. this presents a problem with chip-seq data. for small datasets, a significant number of regions contain <  <dig> reads, their binomial p-values are not continuous but produce discrete values , spikes in the p-value distribution, and a poor q-value fdr estimation, see figure 4b. to more closely approximate a uniform distribution, windows with <  <dig> reads are removed prior to converting p-values. this significantly improves the q-value fdr estimation but does affect the test sensitivity with small datasets, see figure 3a. with both datasets, binomial p-values converted to q-values overestimated the actual fdr by < ~ <dig> fold .

each of the two confidence estimators has its own advantages and disadvantages. the efdr underestimates the actual fdr and requires twice the number of control input reads to generate a null distribution. yet the efdr has no set minimum data size and can thus be used with low read density datasets. the q-value fdr overestimates the true fdr, requires a minimum of  <dig> reads in each window, and half the input control data. it is most useful for high read density data with matching input control data. in practice, we use both fdr estimations to get an approximate range of likely confidence for a given list of enriched regions.

lastly, the viability of using global poisson p-values was assessed with the simulated spike-in data sets. this method assumes random read distribution under the null hypothesis. it does not make use of input control data to down weight localized systematic bias. as a result, it performed rather poorly. real fdrs of <  <dig>  could not be achieved at any threshold due to the presence of high numbers of false positives. it is useful in situations where control data is unavailable but otherwise, it should be avoided.

analysis of the neuron-restrictive silencer factor chip-seq data
using the methods developed here, we reanalyzed johnson et al.'s nrsf chip-seq data. currently, this is the only published chip-seq dataset with control input data and extensive qpcr validated regions. the authors did not perform an input subtraction but used the control data to exclude regions with high numbers of control reads , a hard mask. to set a threshold, they constructed roc curves using  <dig> known qpcr verified nrsf binding regions and  <dig> qpcr negative regions and chose a threshold  that gave high sensitivity and specificity. their pcr amplified and non-amplified datasets were processed independently and those regions common to both were selected to represent likely nrsf binding regions.

to reanalyze the data, we combined the two datasets, ran them through scanseqs with a window size of  <dig> bp and selected a normalized difference score threshold that produced a ranked list of  <dig> regions that were subsequently trimmed to match the number of non redundant regions found in johnson et al.'s supplementary material,  <dig>  intersections were then made between the enriched region lists to evaluate scanseqs performance. the two lists were quite similar  and intersected the same number of nrsf qpcr positives  and negatives . the authors state that their list is likely a conservative estimate of the true number of nrsf binding regions. the efdr and q-value fdr associated with the normalized score threshold used in generating the  <dig> regions,  <dig>  × 10- <dig> and  <dig>  × 10- <dig>  agree with this estimate. to get an idea of the number of regions with an fdr of less than  <dig>  we set thresholds and generated enriched regions using an efdr of  <dig>  or a q-value of  <dig>  to yield  <dig> and  <dig> regions respectively. these data and a reanalysis of mikkelsen et al.'s  <cit>  and barski et al's  <cit>  histone modification, rna polymerase ii, and ctcf data are posted on our das/ <dig> server http://bioserver.hci.utah.edu:8080/das2/das <dig> and best accessed using the integrated genome browser .

implementation
the useq package contains more than  <dig> command line applications written in java for portability, speed, and collaborative development. it makes use of r http://www.r-project.org/ and storey's q-value package http://genomics.princeton.edu/storeylab/qvalue/. in addition to generating standard text based data summary and result file types , extensive use of the affymetrix binary bar file format has been made for direct viewing in igb and optimized distribution using the das/ <dig> protocol and the genoviz das <dig> server, see http://bioserver.hci.utah.edu/bioinfo/index.php/software:igb and http://genoviz.sourceforge.net/. useq is distributed under an open source bsd license. documentation related to analysis usage, available applications, command line menus, and output file type descriptions are included  and on the useq project web site http://useq.sourceforge.net/.

testing
useq has been tested primarily on red hat enterprise linux  <dig> and mac os x with limited evaluation on windows xp. a typical analysis run takes <  <dig> hrs for  <dig> million mapped chip and control reads on a dual processor  <dig> bit hp server with  <dig> gb ram. the majority of time is spent writing out the binary bar graph files. run time without bar file write out is <  <dig> min.

CONCLUSIONS
the ability to control for systematic bias and accurately estimate confidence in chip peaks are two critical steps in generating useful data from chip experiments. here we have developed novel methods to reduced systematic bias by directly comparing mapped enrichment at each genomic loci in the chip data against control input data. this allows proportional scoring of genomic regions without imposing a hard mask. more importantly, we developed and characterized two methods for estimating fdrs associated with chip enrichment that do not rely on extensive qpcr validation. much is unknown about this new chip-seq data type. empirical methods, such as detailed here, are proving to be a good first approximation. as more becomes known, modelling of the chip-seq detection and mapping process may replace the need for input controls. until that point, we strongly recommend the generation of input datasets to control for systematic bias and enable chip-seq peak confidence estimation. these input control datasets are likely reusable barring major changes in the chip sample preparation and alignment methods.

