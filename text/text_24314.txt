BACKGROUND
as the influx of high-throughput sequencing data  <cit>  is imminent, the data management requirements for the analysis packages have changed fundamentally. while, during the days of candidate gene analysis and linkage analysis,”only” up to several thousands of genetic loci had to be stored and loaded into the analysis packages, current genome-wide association studies  provide genetic information on several millions of genetic loci. thus, the typical size of a dataset containing mostly common variants is about  <dig> to  <dig> gigabytes. for high-throughput sequencing studies, the number of genetic loci genotyped increases by several magnitudes, and the file size of such sequencing data can be up to several terabytes. for such large files, the loading process can take up to few hours without counting the time for analysis. this results in great waste of disk space and computation time, which is a problem that is encountered routinely.

one possible solution is to use the general-purpose compression software, such as gzip and bgzip. however, such compression software is not designed specifically for genetic data and its analysis, so the compression rate is relatively low and decompression is always needed before accessing the data. better solutions have been proposed. plink and pbat, which are free whole-genome association analysis toolsets, have introduced binary ped formats  <cit> . this format ensures that only  <dig> bits are required for storing the information of one genotype. it is the most popular compression format used in gwas. however, the compression rate is not sufficient for massive datasets generated nowadays as their compressed datasets could still occupy several gigabytes of the disk space. in recent years, sophisticated compression techniques designed specifically for sequencing data have been proposed. for example, dnazip  <cit>  introduced the idea of storing only the difference between one individual genome data and a reference genome. however, such algorithms suffer the large overhead for storing the reference genome. also, they require substantial cpu-time for decompression.

we propose here a simple and efficient algorithm to store large datasets containing snp data of multiple samples. we show that our algorithm always works better than the compression algorithm implemented in plink or pbat and provides excellent compression rate for sequencing data. also, the compressed data structure provides the potential for efficient implementation of permutation methods and does not require any overhead cpu-time for decompression. we have implemented the algorithm in the gpl licensed c++ library: speedgene. we show that it takes much less time for loading the compressed files than plink using our library. in addition, our c++ implementation supports parallel loading of the genetic information, which further decreases the loading time as the number of parallel jobs increases. the version  <dig>  of the speedgene library is available at http://people.hsph.harvard.edu/∼ dqiao/speedgene.html together with detailed instructions and examples.

methods
the linkage/plink data format
the linkage or plink data format is a commonly used data format for storing snp data in genome-wide association studies. data files in this format are called pedigree files and have”.ped” as the suffix. this format can be converted from or to the vcf format used in  <dig> genome project using vcftools  <cit> . the speedgene library currently only recognizes pedigree files in the linkage/plink format, but the algorithm can be implemented for compressing snp data in the vcf format. the vcf format requires the same amount of disk space for each genotype  as the linkage/plink format, so the compression rate of this algorithm applying on vcf files should be similar to the compression rate for pedigree files. note that vcf files may contain other informations such as indels and whether the genotype is phased or unphased, which could not be incorporated into the linkage format. however, since snp data are very commonly used genetic data in association studies and takes the most disk space, efficient storage of the snp data could still save a lot of resources. in the demonstration of the algorithm and the examples below, we use the linkage/plink format as the input format.

any pedigree file in the linkage format has the same structure, a toy example is shown in figure  <dig>  the first line contains the marker names, separated by a space character. starting from the second line, each line includes pedigree and genetic information for each individual. the first six columns of these lines specify each individual’s pedigree information in the order of pedigree id, subject id, father id, mother id, sex, and affection status. subject id must be unique within one’s family. father and mother id could be  <dig> if this information is unknown, e.g. population-based study of unrelated subjects. sex is  <dig> for male and  <dig> for female. affection status is  <dig> if the subject is unaffected,  <dig> if affected, and  <dig> if the status is unknown. the other columns contain the genetic data for each individual, separated by a space between each marker. two columns are required to represent the information for two alleles, separated by a space. the allele in- formation is coded using  <dig> to  <dig> where 1 = a, 2 = c, 3 = g, t =  <dig> and  <dig> represents missing allele information.

the speedgene algorithm
the speedgene algorithm consists of three different sub-algorithms, which are selected by speedgene based on the minor allele frequency  of the genetic locus to be stored. the space needed for the compressed data is computed for the sub-algorithms beforehand. the speedgene algorithm then selects the best procedure among the three compression methods. the first sub-algorithm is based on the binary format implemented in plink and pbat. it utilizes the fact that the marker information of each marker can be represented using a 2-digit binary number. the second sub-algorithm uses subject indices to indicate heterogeneous, homogeneous and missing genotypes. the third sub-algorithm uses binary digits to indicate heterogeneous genotype and subject indices to indicate homozygous and missing genotypes. a feature of all three compression methods is that the required memory space for storage can be computed prior to compression. thereby, the speedgene algorithm is able to select the optimal method before compressing the data. the three sub-algorithms are described in detail in the following sections.

sub-algorithm i: compression using binary encoding
for any pedigree file, we assume that there are only bi-allelic markers in the file. for any allele of a marker, an individual may only have  <dig>   <dig> or  <dig> of this allele. also, the allele information can be missing for any individual at any marker. thus, the marker information can be transformed into the number of copies of a particular allele. it could be  <dig> , <dig>  or missing and could be converted to a 2-digit binary number. in the compression process, we find the minor alleles at each marker and use  <dig>   <dig>   <dig> to represent zero, one or two copies of the minor allele at one marker.  <dig> indicate that the genetic information is missing at this marker for the individual. thus, one genotype in the original file can be converted into two binary digits, which is  <dig> bits on disk space. four of such 2-digit binary number is  <dig> bits, which equals  <dig> byte. therefore, the genetic information of four markers for one individual can be converted into  <dig> byte in a binary file. this binary encoding is similar to the binary format used in plink  <cit>  or pbat  <cit> .

based on this conversion method, we can compress the genetic information in the pedigree file into a much smaller binary file. as we have seen in the example , the genetic information for four genotypes occupies  <dig> bytes in the original pedigree file, and it is converted to only  <dig> byte in the compressed file, which could save up to a factor of sixteen on the disk space. if there are n subjects in the dataset, the storage requirement for compressing n genotypes for one marker using this algorithm is given by

  2∗n/8bytes 

for the assessment of the performance of the proposed speedgene algorithm, we will use the linkage/plink format and the binary-encoding algorithm described above as the standard approach to which the speedgene algorithm will be compared.

sub-algorithm ii: compression using subject indices
with the binary-encoding algorithm described above, the genetic information of any marker in one dataset is compressed to the same size since the compression algorithm does not depend on the frequency of each genotype. as shown in the results section, the performance of the binary compression is the best we can achieve when the variants are relatively common . however, for snps with small maf, only a few subjects have the heterozygous genotype and, even fewer, have the rare homozygous genotype. thus, it is wasting disk space if the genetic information for all the subjects is recorded, especially for the subjects with the common homozygous genotypes which is by far the most frequent genotype. therefore, we can utilize this feature of snps with small maf, and record only the indices of the subjects with the missing, heterozygous or rare homozygous genotypes for the snp. the common homozygous genotype is the default genotype. since most of the snps of the human genome have small maf  <cit> , the improvements of this approach is substantial compared to the binary-encoding algorithm in the last section.

specifically, suppose we have n subjects in the data, then we need log2n binary digits in order to record the index of any subject. first, the number of the rare homozygous, the heterozygous and the missing genotypes are counted. this information is used to calculate the compressed size and determine whether sub-algorithm ii should be used for the snp. if sub-algorithm ii requires the smallest amount of memory, speedgene will use sub-algorithm ii for the compression of the genetic data for the snp. the indices of the subjects with the homozygous, heterozygous and missing genotypes are transformed into binary digits and are written into the binary file afterwards. since the number of subjects with each genotype varies, the counts, each requires log2n bits on the disk space, are written to the file before the indices of the subjects are outputted to the file. thus, the storage requirement for compressing n genotypes for one marker using this algorithm is given by

  log2n∗#homo+1+#heter+1+#missing+1/8bytes 

where #missing denotes the number of subjects with the missing genotype, #homo denotes the number of subjects with the rare homozygous genotype, and #heter denotes the number of subjects with the heterozygous genotype.

sub-algorithm iii: compression using binary encoding and subject indices
as we will see in the next section, sub-algorithm ii works best for snps with very small maf, but performs worse than sub-algorithm i for more common snps . however, by combining sub-algorithm i and ii, we can create a hybrid approach that performs better than sub-algorithm i and ii for snps whose mafs are somewhere between uncommon and very common.

since the heterozygous genotype is more common for genetic loci that are in the range between uncommon and very common , recording the heterozygous genotype by the indices of subjects is not very efficient. instead we use a binary number of n digits to indicate the subjects with the heterozygous genotype, where n is the number of subjects in the dataset. if subject i has the heterozygous genotype for the snp,  <dig> is put at position i instead of  <dig>  beside this, the indices of subjects with the missing and homozygous minor allele genotypes are recorded in the same way as in sub-algorithm ii. the storage requirement of the marker information for n samples using this algorithm is given by

  #homo+1+#missing+1∗log2n+n/8bytes 

where #homo denotes the number of subjects with the rare homozygous genotype and #missing denotes the number of subjects with the missing genotype for the snp.

for sub-algorithm ii and iii, since the indices of the heterozygous and homozygous genotypes are stored for each marker, this compressed data structure makes computation for permutation methods much convenient.

RESULTS
performance comparison of sub-algorithms
the speedgene algorithm selects for each genetic locus the optimal algorithm in terms of storage space  among the three sub-algorithms as described in the methods section. to assess the performance of the speedgene algorithm, we compare it with the standard linkage/plink format and the plink/pbat compression algorithm. the efficiency of the speedgene algorithm depends on two factors, the genotype frequency of the genetic locus and the number of subjects included in the dataset. assuming hardy-weinberg equilibrium, the first plot of figure  <dig> gives a plot of the compression factor of the three sub-algorithms versus different mafs for a dataset of  <dig> subjects. the second plot shows the number of bits needed per genotype for storing the genotype information of  <dig> subjects at different maf values. the dashed line provides the performance for the speedgene algorithm which is based on the allele frequency and formulas  <dig>   <dig> and  <dig> to select the optimal compression procedure among sub-algorithm i-iii.

as in the plot, approximately, speedgene always achieves a compression factor of  <dig> compared to the standard linkage format for maf >  <dig>  for which sub-algorithm i is used. speedgene accomplishes a compression factor of  <dig> up to  <dig> compared to the linkage/plink format for  <dig>  ≤ maf ≤  <dig>  for which sub-algorithm ii is selected. for rare and uncommon alleles , a compression factor of at least  <dig> compared to the linkage format is realized. with smaller mafs, the compression factor increases rapidly. equivalently,  <dig> bits per genotype would be needed for maf >  <dig> , about  <dig>  to  <dig>  bits per genotype for  <dig>  ≤ maf ≤  <dig> , and less than  <dig> bit per genotype is needed for maf <  <dig> .

the performance of the algorithms also depends on the number of subjects in the dataset. figure  <dig> shows the compression factor of the algorithms for one marker for different number of subjects, at eight maf levels. generally, the compression factor decreases slightly as the number of subjects included increases, but is mostly constant over the range of number of subjects we have considered for different values of maf. in addition to that, the plots give us similar information as the plots above. for example, for snp with maf =  <dig> , sub-algorithm ii is able to compress the genetic information by a factor of at least  <dig>  which is much better than sub-algorithm i and iii. thus, maf is the most influential factor in determining which algorithm is the optimal method among the three sub-algorithms.

the c++ library implementation
we have implemented the algorithm in a c++ library called speedgene. there are two classes in the speedgene library. the first one is the comp class, which is responsible for compressing a pedigree file in the linkage/plink format into a text file that contains the subject information and a binary file that contains the genetic information. the binary file is not human-readable and can only be used by the second class in our library. the compression process requires two scans of the pedigree file to avoid storing all the marker information before compression, which would take a great amount of memory space. the second class is the loadcomp class. as its name suggested, it is responsible for loading the compressed files into the memory, and for processing queries from the user. it provides an option to load the entire pedigree file or to load a section of the file. this partial-loading function ensures that only necessary information is loaded for the jobs that are running in parallel, which greatly decreases the loading time. moreover, the public functions provided by the library allow the user to retrieve any information stored in the original file. this c++ library makes it straightforward for users to incorporate it into their own programs whereas other existing libraries do not offer such capability.

performance
compression rate
we evaluated the performance of the speedgene algorithm on two rare variants datasets. we simulated two datasets with  <dig> subjects from the wright’s distribution  <cit> , which is fp=cpβs−11−pβn−1eσ1−p, where the scaled mutation rates βs =  <dig> , βn = βs/ <dig>  the selection rate σ =  <dig>  and c is a normalizing constant. table  <dig> below shows the compressed file size for the simulated data. for sequencing data, the optimal algorithm is sub-algorithm ii for most of the snps. thus, speedgene is able to achieve a large compression rate. in the simulated data, the compression factor is approximately  <dig>  which is equivalent to  <dig>  bits per genotype, whereas  <dig> bits per genotype is required by plink or pbat. gzip seems to perform much better on rare variant data than on common variant data, however, such general-purpose software takes extra time to decompress the files before loading them into the memory. we have also extrapolated the approximate file size if dnazip is used  <cit> . according to the paper, each snp for one person requires slightly less than  <dig> byte per snp for storage and it requires a reference hu- man genome  and a reference snp map  to retrieve the entire genome data.

compressed file sizes of the simulated datasets using plink, gzip, speedgene and dnazip. each dataset contains  <dig> subjects.

we also applied these methods to two real datasets. one dataset contains the genotype data from the framingham heart study , which includes  <dig> subjects and  <dig>  snps. the other dataset is obtained from the copdgene study on patients with chronic obstructive pulmonary disease . it includes  <dig> subjects with  <dig> snps over the human genome and 77% of the snps in this sequencing data have a maf ≤ 5%. the original file size and the compressed file sizes using different compression methods are shown in table  <dig>  for the fhs dataset, since that most of the snps are common, the compression rate of speedgene is just slightly greater than that of plink. gzip gives a much lower compression ratio of  <dig> here, as for most common variant datasets. the copdgene sequence data contains mostly rare variants, but still includes some common variants, so we observe a much higher compression rate with speedgene than with plink and gzip.

file sizes of the fhs dataset and copdgene dataset, compressed using plink, speedgene and gzip.

loading time
the time for loading the compressed datasets using speedgene and plink on a  <dig>  ghz amd opteron cpu with 128 gb of ram is shown in table  <dig> below. the time to load the entire file using speedgene is less than half of the time needed by plink for the simulated datasets. if the analysis is ran in parallel, the loading time using speedgene is decreased further as the number of jobs ran in parallel increases. for example, if we are loading 1/ <dig> of the dataset with  <dig> million snps in each parallel job, the loading time needed by speedgene is  <dig>  minute.

the cpu time needed for loading the two compressed files using speedgene and plink on a  <dig>  ghz amd opteron cpu with 128 gb of ram.

CONCLUSIONS
to tackle the problem of large file sizes and long loading times of genetic data, we have developed a new compression algorithm - speedgene. the algorithm selects the optimal approach among three methods in terms of the required disk space. we have shown that the algorithm always works better than the compression algorithms provided by pbat and plink, and can reach a compression factor of sixteen up to few hundreds. especially for sequencing data with mostly rare variants, the algorithm is able to compress files of hundreds of gigabyte to hundreds of megabytes. similar compression rate can be reached for the vcf files containing snp data. in addition, the compressed data structure requires no extra time for decom- pression and could reduce a large amount of computation time for performing permutations on the genotypes.

a c++ implementation of the speedgene algorithm is provided and an integration in r is ongoing, but the algorithm could be implemented easily for other data formats and using other programming languages. the speedgene library utilizes the structure of the compressed data and enables direct loading of the genotype data into the memory. moreover, the functions in the loadcomp class of this library allow the user to flexibly retrieve any specified subject or genetic information from the compressed dataset. furthermore, user-friendly parallel-loading function is supported, which in result shortens the loading time greatly when parallel jobs are dispatched in clusters.

to fully utilize the compression algorithm, it needs to be incorporated into other analysis software for association studies, where the genetic information can be loaded using the library and directly sent for analysis in the software. for example, we are planning to include this binary format as one of the standard input format in npbat, which is an interactive software for the analysis of population based genetic association studies. such incorporation would require additional efforts, but with the gain of much more disk space and shorter loading time, it will be beneficial in the long run.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
dq and cl conceived of the project idea, developed the algorithm and wrote the manuscript. dq implemented the algorithm in the c++ library. wky was responsible for preparing the testing datasets. all authors read and approved the final manuscript.

