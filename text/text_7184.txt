BACKGROUND
the mining of high-dimensional data in which the number of features is much larger than the number of samples, has become increasingly important, especially in genomics, proteomics, biomedical imaging and other areas of systems biology  <cit> . the availability of high dimensional data along with new scientific problems have significantly challenged traditional statistical theory and reshaped statistical thinking  <cit> .

the high dimensionality of functional genomic data sets poses problems to build classifiers. because of the sparsity of data in high dimensional spaces, many classical methods of classification break down. for example, fisher discrimination rule will be inapplicable because the within scatter matrix become singular if the number of variables is larger than the number of samples  <cit> .

another problem is caused by the small sample size. the number of samples is usually not adequate to be representative of the total population. moreover classifiers built on small sample sets are often not stable and may have a large variance in the number of misclassification  <cit> . one common approach for this problem is to aggregate many classifiers instead of using a single one. there has been considerable interest recently in the application of aggregating methods in the classification of high-dimension data  <cit> . the most well-known method in this class of techniques is perhaps bootstrap aggregating . breiman found that gains in accuracy could be obtained by bagging when the base learner is not stable  <cit> . however, vu and braga-neto argued that the use of bagging in classification of small-sample data increases computational cost, but is not likely to improve overall classification accuracy over other simpler classification rules  <cit> . moreover, if the sample size is small, the gains achieved via a bagged ensemble may not compensate for the decrease in accuracy of individual models  <cit> .

cross-validation is probably the most widely used method for estimating prediction error. in small sampled high dimension data modeling, k-fold cross-validation is often used  <cit> . the k-fold cross-validation estimate is a stochastic variable that depends on the partition of the data set. full cross-validation, that means performing all-possible ways of partitioning, will give an accurate estimation, but is computationally too expensive. therefore, repeating k-fold cross-validation multiple times using different splits provides a good monte-carlo estimate of the full cross-validation  <cit> . this repeating procedure results in a lot of classifiers.

in this paper, we aggregated the classifiers obtained from principal component discriminant analysis  with a double cross-validation scheme  <cit> . pcda is an adaptation of fisher's linear discriminant analysis  for high-dimensional data. in pcda, the dimensionality of the data is reduced by principal component analysis . in the reduced dimensional space the within scatter matrices is nonsingular and classical lda can be performed  <cit> . a double cross-validation scheme was used to estimate both the number of principal components and the predictor error of the pcda model  <cit> . the classifiers that were obtained from the different cross-validation loops are aggregated to make a single classifier. this approach is tested on simulated data, gene expression, proteomics and metabolomics data. the results obtained from the research may provide insights into the use of aggregating learner in low sample, high dimensional biological data.

methods
pcda
given a high dimensional data set a of size m × n, where m is the number of samples and n is the number of features , classical flda  <cit>  finds the discriminating direction dn ×  <dig> that maximizes the ratio of the between-class scatter sb against the within-class scatter sw.         

here r is the number of classes, and each class has mi samples. mi is the index set of samples in each class i.  and  are the class centroids and the global centroid respectively.

the discriminating direction d is the eigenvector corresponding to the largest eigenvalue of the matrix . because the number of features n is larger than the number of samples m in high dimensional data, the matrix sw is singular. this means that  does not exist and flda cannot be applied directly.

to overcome the difficulties imposed by the singular covariance matrices, the data can be first projected onto a low dimension pca subspace, and lda is then applicable in this pca subspace. the main goal of pca is to reduce the dimensionality of a data, whilst retaining as much as possible of the information present in the original data. this reduction is achieved by a linear transformation to a new set of variables, the principal component  scores. the combination of lda with pca yields principal component discriminant analysis .

aggregating pcda with double cross-validation
the optimal number value of reduced dimensions of pca is usually determined by cross-validation. the simplest form of cross-validation is to split the data randomly into k mutually exclusive parts, building a model on all but one part, and to evaluate the model on the omitted part. this strategy allows for estimating the optimal model complexity; however, the resulting prediction performance estimate is often too optimistic since the same samples were also used to find the best number of pc's and thus they are not completely independent. it is therefore recommended to use a double cross-validation approach  <cit> . as shown in figure  <dig>  first the original data set was divided into two parts, training set and test sets. the test set was not used in double cross-validation scheme and it was employed afterwards to evaluate how good the built classifier really is. the training set was partitioned into k parts. of the k parts, a single part is retained as the outer validation set, and the remaining k- <dig> subsamples are used as inner training data and inner validation set. on the remaining k- <dig> parts, a k-1-fold cross-validation is performed to find the best number of pc components. this is a nested validation scheme. the inner validation set is used to determine the optimal number of principal components, and the outer validation set is used to find the cross-validation error of the method. in summary, the double cross-validation with pcda is summarized in the following pseudo code

divide the training data set into k parts:

for i=  <dig> to k

   for j=  <dig> to k-1

      build pcda models with different pcs

   end

   find an optimal pc number

   build pcda model with the optimal pc number

end

   obtain cross-validation error.

since the cross-validation error accuracy would depend on the random assignment samples, a common practice is to stratify the folds themselves  <cit> . in stratified k-fold cross-validation, the folds are created in a way that they contain approximately the same proportion of classes as the original dataset. with randomly chosen partitions of inner and outer validation set, we can repeat the double cross-validation scheme to produce a lot of pcda classifiers. the multiple versions of the predictors can be aggregated by majority voting, i.e., the winning class is the one being predicted by the largest number of predictors.

data
simulation
the simulated data contain two classes. each class li  consists of  <dig> objects and each object has  <dig> features, and it is sampled from a multivariate normal distribution n  respectively, i =  <dig>   <dig>  here vi is the mean of class li, and Ω represents the covariance of the simulated data. to make the simulation more closely to real data, we constructed the simulated data from the gaucher proteomics data . suppose the means and covariances of two classes in the auto-scaled gaucher data are represented by vector u <dig>  u <dig>  matrix Ω <dig>  and Ω <dig> respectively, and the mean v <dig> and v <dig> and covariance matrix Ω of the simulated data were calculated by the following equations.         

equation  <dig> and  <dig> are to ensure the separability of two classes, and equation  <dig> is to make two classes have the same common covariance matrix Ω.

by following the above procedure, we obtain the simulated data set of size  <dig> ×  <dig>  before building pcda classification model by double cross-validation on the simulated data set, we separated the simulated data set into training set and test set as shown in figure  <dig>  in order to form training sets of differ sample sizes, we randomly selected  <dig>   <dig>   <dig>   <dig>   <dig> objects from  <dig> objects. in the test set,  <dig> objects were random selected without replacement from the data set after removing the training set. the whole selection procedure was repeated  <dig> times randomly. to make a reasonable comparison, we fix the random seeds in each selection procedure. in single pcda, a double cross-validation with ten-fold in the outer loop and nine-fold in the inner loop were used to obtain the optimal pc number and cross-validation error. in aggregating pcda, the pcda approach was repeated  <dig> times with different cross-validation splits to obtain an aggregated classifier. besides, we also constructed a single pcda model with double cross-validation in the simulated data sets to compare the classification performance of pcda with aggregating pcda.

leukemia gene expression data
leukemia data from high-density affymetrix oligonucleotide arrays were previously analyzed in golub and tibshirani  <cit> , and are available at http://www.broad.mit.edu/cgi-bin/cancer/datasets.cgi. there are  <dig> genes and  <dig> samples coming from two classes:  <dig> in class all  and  <dig> in class aml . among these  <dig> samples,  <dig>  are set to be training samples and  <dig>  are set as test samples. the data is mean-centered before classification. it should be noted that the pretreatment step such as mean-centering and auto-scaling was always performed on the training data and then the test data was pretreated with by the mean and standard deviation obtained from the training set. auto-scaling means mean-centering the data and scaling each column by its standard deviation.

gaucher proteomics data
the data consist of serum protein profiles of  <dig> gaucher patients and  <dig> controls  <cit> . serum samples were surveyed for basic proteins with seldi-tof-mss making use of the anionic surface of cm <dig> prtoeinchip. all preprocessing  of the seldi-tof-ms data was performed using ciphergen software. the data set of size  <dig> ×  <dig> is available at http://www.bdagroup.nl/content/downloads/datasets/datasets.php. one gaucher sample  has been detected as an outlier and was removed. the spectra profiles were first normalized by dividing each profile by its median to arrive at comparable spectra. subsequently, the data sets were auto-scaled before classification.

grape extract metabolomics data
the data set is from unilever food and health research, vlaardingen, netherlands, thirty five healthy males were recruited to investigate the effect of grape extract supplementation on vascular function and other vascular health markers. the study has a double-blind, placebo controlled randomized full crossover design with  <dig> treatments, a run-in period,  <dig> interventions- and  <dig> washout periods. 1d 1h nmr spectra of plasma: d2o  samples were recorded on a bruker advance  <dig> mhz nmr spectrometer according to a standard operating procedure with a pulse sequence. all data were processed in bruker xwin-nmr software version  <dig>   and imported in amix software from bruker. due to some missing data, the final nmr data of  <dig> plasma samples were bucketed in the spectral region 0- <dig> ppm using a bucket-width of  <dig>  ppm.

the data set of size  <dig> ×  <dig> of two classes was divided into two subsets,  <dig> samples in training set and  <dig> samples in prediction set, using the kennard-stone method  <cit> . the kennard-stone method was used to select objects to model such that they are uniformly scattered over the experimental space. in the training set and test set, the samples were assigned in such a way that the ratio of class membership is similar to the original data. the data sets were auto-scaled before classification.

RESULTS
when aggregating works
breiman  <cit>  has noticed that the efficiency of aggregating depends on the stability of the prediction or classification rule. each cross-validation pcda classifiers are constructed on different samples, so it is expected that there will be some variance in the prediction error. first, we applied aggregating pcda on the simulated data sets. as shown in figure  <dig>  the classification performance of aggregated pcda is usually better than that of pcda itself. here, the single pcda itself uses ten fold outer cross-validation to determine the cross-validation errors and a nine fold inner cross-validation to determine the optimal number of principal components. the aggregated pcda was constructed by repeating single pcda  <dig> times. the simulation results in figure  <dig> themselves are a pro of aggregating. as the training sets and prediction sets follow same distribution, the cross- validation error and prediction error are quite similar in figure  <dig>  a close look on figure  <dig> also tells us, when the number of sample size is increasing, the classification rate is increased and the variation of the prediction error is reduced.

we further applied pcda and aggregated pcda on three real data sets. figures  <dig> and  <dig> illustrate the variation of misclassification rate of the data sets in training and predictions.

the aggregated pcda was constructed by repeating pcda  <dig> times. as shown in figures  <dig> and  <dig>  the median of the misclassification rate is indicated by the center line, and the first and third quartiles are the edges of the box area, which is known as the inter-quartile range. the extreme values  are the ends of the lines extending from the inter-quartile range. points at a larger distance from the median than  <dig>  times the inter-quartile range are plotted individually as plus sign. due to the low sample size in the gaucher data, a separate test set was not created. there are only two data sets giving the performance of the prediction of the test set in figure  <dig>  obviously, the variations in the error rate of the pcda models are quite large in the data sets, especially when ratio of feature to sample is high. the most stable case is from the grape data, and the ratio of feature to sample is the lowest among all three data sets. table  <dig> and table  <dig> also show that aggregating pcda model often gives an improved performance over a single pcda model in the three real data sets. in table  <dig> and table  <dig>  the performance of a single pcda is represented by the median of the misclassification rate.

the aggregated pcda can make a good pcda classifier better since the variance of misclassification rate can be reduced  <cit> . a heuristic explanation is that the variance of the prediction error of the aggregated classifier is equal to or smaller than the error of the original classifier since majority voting is modeling averaging.

the dimension reduction step by pca can not be guarantied to preserve all directions that contain discriminative information  <cit> . but in an aggregated pcda model, the discarded discriminant information of one pcda model can be re-modeled from other pcda model with different partition of training data sets by cross-validation. so, aggregating pcda itself may contain more discrimination information than single pcda.

we also compared pcda with the support vector machine  classifier  <cit> , and the results are shown in table  <dig>   <dig>   <dig>   <dig>  we found that the single pcda classifier has a comparable result to the single svm classifier. however, aggregating pcda achieves better results than svm, pcda, and aggregating svm classifiers.

when aggregating does not work
aggregating may increase the bias of a learner since only a part of the training data are sampled by cross-validation or bootstrapped for modeling. that is to say, the use of k-fold cross-validation may have a negative effect on the accuracy of individual pcda models. as shown in figure  <dig>  when the sample size is twelve, the performance of pcda classifier is relatively bad and not stable. after aggregating, the classification performance did not achieve expected training and prediction performance yet, since basically in such case more samples are needed to build a precise model. another situation which does not favor aggregating is case of very weak learners. a very weak learner means that the performance of learner is even worse than random guess. aggregating such learner will make prediction even worse because averaging such learners will result in a learner that will give a wrong prediction in all cases. for example, if an observation is classified as a success about four times out of ten. after the majority voting, it will give 100% wrong.

further notes
although the efficiency of aggregating depends on the stability of the prediction, aggregating does not definitely make the predictor stable, and it stabilizes to a certain extent. as shown in figure  <dig>  there is a small margin of sample  <dig> and sample  <dig> of the gaucher proteomics data. the difference between the fractions of times a case is correctly classified and the fraction of times it is incorrectly classified is called the "margin" for that case  <cit> . larger margins are desirable because a more stable classification of that sample is implied. as seen in figure  <dig>  some samples are always corrected predicted and also some samples  are always wrongly predicted. on the other hand, the small margins in sample  <dig> and  <dig> tell us that these two samples have almost half chances to be corrected classified, and half chances to be incorrectly classified. these two "instable samples" result in an aggregating classifier that is not stable. figure  <dig> also supports such findings as the misclassification rates fluctuate greatly with different numbers of aggregation.

another question about aggregating pcda is how many times resampling is enough? figure  <dig> gives the misclassification rate in training with increasing number of aggregation. the number of aggregation starts from  <dig> to  <dig>  and increases by  <dig> each time. we observe in figure  <dig> that the aggregated misclassification rate will keep stable after  <dig> replicas in leukemia and grape data. for gaucher data,  <dig> replicas also give a reasonable estimation. to our experience, 50- <dig> replicas are usually enough to get a stable value. aggregating learner in this paper is obtained from cross-validation, which is resampling without replacement. the conventional bagging is obtained from bootstrapping, which is resampling with replacement. as stated by buja and stuetzel  <cit> , there is an equivalence between bagging based on resampling with and without replacement. so, the conclusion obtained in this paper in our opinion also holds in bagging approaches.

another concern is whether aggregating pcda can apply to multi-classification problem. because the discrimination in pcda is performed by lda, the properties of lda for multi-classes also hold. since the decision boundaries in lda are constructed in a pair wise manner  <cit> , the conclusions drawn in this paper in principle are also valid for a multi-class problem. however, many discriminative methods are often most accurate and efficient when dealing with two classes only, but usually at reduced accuracy and efficiency for multi-classification  <cit> . the effects of aggregating multi-classifier still need further careful studies.

in addition, an interpretable model is usually required as it is important to identify which genes, proteins and metabolites contribute most to classifiers. the pcda model has been already combined with rank products  <cit>  to find important variables. in aggregating pcda, we can repeat the same strategy too. for example, we aggregate  <dig> pcda learners together. as a single pcda yields  <dig> discriminant vectors in a  <dig> fold cross-validation;  <dig> runs gives  <dig> discriminant vectors in total. then for all features the products of the  <dig> ranks are calculated. after sorting, the features with the lowest rank products are the ones with the largest discriminative power.

CONCLUSIONS
the use of cross-validation to study the performance of a classifier is an established method. if performed in a proper way cross-validation provides roughly unbiased estimates of the prediction measures. however, the different partitions in cross-validation can give rise to high variability of the model predictions. in this paper we show a way to overcome the variability by building one aggregated classifier from all the classifiers that were build in the repeating cross-validations.

aggregating learners can have several important benefits. aggregating over a collection of fitted values can help compensate for overfitting. that is, the majority voting tends to cancel out results shaped by idiosyncratic features of the data. one can then obtain more stable and more honest assessments of how good the fit really is.

aggregating learners also have some limits. when the sample size is very small, aggregating learner may have a large bias. so it is important for us to visualize the data to see if aggregating will be helpful or not.

in conclusion, we recommend the use of aggregating learner in high dimensional data analysis, but a careful look on data structure and comparison with base learner result.

competing interests
the authors declare that they have no competing interests.

authors' contributions
all authors conceived the model and the structure of the paper. cjx performed the analysis and drafted the paper. all authors read and approved the final manuscript.

