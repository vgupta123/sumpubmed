BACKGROUND
microarray technology has become a widely used tool in the biological sciences. over the past decade, the number of users has grown exponentially, and with the number of applications and secondary data analyses rapidly increasing, we expect this rate to continue. various initiatives such as the external rna control consortium   <cit>  and the microarray quality control  projects  <cit>  have explored ways to provide standards for the technology. for microarrays to become generally accepted as a reliable technology, statistical methods for assessing quality will be an indispensable component; however, there remains a lack of consensus in both defining and measuring microarray quality.

defining quality in the context of a microarray experiment is not an easy task. the american society for quality  defines quality as a subjective term for which each person has his or her own definition. in technical usage, quality can have two meanings: a product or service free of deficiencies, or the characteristics of a product or service that bear on its ability to satisfy stated or implied needs  <cit> . many other definitions of quality exist but a common theme of most is the dependence of quality on the needs of the consumer. so what do users of gene expression microarrays want? the most common applications appear to be: finding differentially expressed genes between two conditions, clustering genes or samples, and predicting sample types or outcomes.

in our attempt to measure quality we quantify the effect of removing bad quality data on the biological results reported in a publication, which we refer to as bottom-line results. one should note that bottom-line results depend on the application. furthermore, various levels of the data can be considered for removal; we can consider removing: one data point from a feature on one array, all data points from a feature across all arrays, all data from an array hybridization, all data arising from an rna sample, all data arising from an entire batch of arrays, all data arising from an entire experiment/study, or, in a cross-study meta-analysis  <cit> , all data produced from a particular lab. thus, defining quality in the context of microarray experiments is indeed a difficult task. to provide a useful review, in this paper we focus our attention on the removal of all data from a poor quality array hybridization and the subsequent improvement in bottom-line results. because most results from microarray studies combine data from various hybridizations, even one bad array can easily taint final results.

recently, two factors have greatly increased the demand for reliable assessment of microarray quality: microarrays are beginning to be used in clinical settings to aid in diagnosis  <cit>  and researchers are conducting meta-analyses and developing bioinformatic tools to mine the plethora of microarray data made publicly available through geo and arrayexpress  <cit> . in the former case, it is crucial to know whether the data being used to guide patient care is of usable quality. in the latter case, poor quality arrays might taint the results of a large meta-analysis or cause a bioinformatic tool to provide erroneous information.

in this paper, we focus on affymetrix genechip microarrays, but many of the methods and recommendations presented can be extended to other microarray platforms and other high-throughput technologies for which there exists enough publicly available data. in the methods section, we begin by revisiting a widely used statistical model and discussing its implications regarding array quality. we then propose a formal definition of array quality and use this definition to assess the performance of current quality metrics. we find that single-array metrics typically perform poorly, while multi-array metrics perform well. while multi-array metrics are useful in traditional laboratory experiments, many modern uses of microarrays - such as clinical use, large meta-analyses, etc. - would benefit from single-array quality metrics. to address this, we propose a novel single-array quality metric based on one of the best multi-array quality metrics. we demonstrate that this single-array metric performs nearly identically to the multi-array metric on which it is based, except in a specific situation where the multi-array metric fails. finally, because publicly available microarray data is often used to develop and test new algorithms and bioinformatic tools, we use our newly developed metric to assess the quality of publicly available affymetrix microarray data.

methods
to better understand what we are measuring and what we actually observe, we use a relatively simple statistical model. this model has been proposed by various authors  <cit> :  

here ii,j represents the observed intensity for feature i for sample j. kj is an sample/array effect which accounts for the need for normalization, θi,j represents a quantity proportional to the amount of rna hybridized to the array , ϕi quantifies the probe effect, ε represents measurement error and oi,j represent the components of the intensity due to non-specific binding and optical noise. for simplicity, we assume we can correct for the background components, oi,j, and that kj, θi,j, and ϕi are not zero. under these assumptions we can simplify to a linear additive model; this model is extensively used as part of the robust multi-array analysis  preprocessing algorithm  <cit> :   

this parametrization reveals two important facts for quality assessment. first, a feature intensity being larger on one array when compared to another does not imply the level of expression is also larger because k may differ. using the notation above we write yi <dig> >yi, <dig> does not imply θi, <dig> >θi, <dig>  veterans of microarray data analysis know this very well and always perform normalization before making direct comparisons. second, two feature intensities on the same array are not comparable because of the probe effect ϕi. in other words, y <dig> j >y <dig> j does not imply θ <dig> j >θ <dig> j. this fact, although not explicitly explained in most papers, is the principal reason why most publications using microarray experiments base findings on relative or differential expression. using the notation above and assuming we have normalized and removed the k, we can write:  

where δi is measurement error. in this case the probe-effect cancels out and the observed log ratio is a useful estimate of the true log ratio of expression levels.

quantifying quality
we start by defining some notation. let a = a <dig>  ..., an represent the data from n arrays. denote with f the data manipulations that are performed on a to produce a set of results represented by r, i.e. let f = r. let q represent a quantification of the accuracy and precision of r. we define a successful quality assessment procedure as one that prompts us to ignore data from array j, that is Δj = q - q >  <dig>  here a-j represents the data set with the data from array j excluded and Δj the improvement from removing the jth array. a specific example of the above notation is the following: a represents the data from  <dig> arrays , f represents the action of computing the t-statistic for each gene and from this value computing an fdr q-value, r is the list of genes for which q <  <dig> , and q is the percentage of true and false positives on our list. notice that removing an array of bad quality can result in improved accuracy and precision, but removing a good quality array can worsen the results because we lose power by considering less data. it is important to keep in mind that overzealous quality metrics can actually worsen results.

in general, q is not computable. if we had a way to know true and false positives we would not need to run the experiment. however, for the purpose of assessing quality metrics, we need experiments with enough a-priori knowledge that we can define q. it is very important to note that q must be defined prior to observing r, e.g. it is not appropriate to define true positives based on the q-values obtained from r.

review of existing quality assessments
various summary statistics or quality metrics have been suggested for affymetrix genechip arrays. affymetrix's software offers  <dig> quality metrics; bolstad et al. proposed two additional metrics  <cit> . a description of these methods follows.

affymetrix quality metrics
affymetrix provides various quality metrics as part of their mas <dig>  analysis software. of these, the three most commonly used metrics are: average background, scale factor, and percent present. other metrics provided by affymetrix assess the quality of the rna hybridized to the array rather than array quality itself. average background is computed as the 2nd percentile of the feature intensities in a given region of the array. it is intended to measure optical background. affymetrix considers average background values between  <dig> and  <dig> as typical for a good quality array. the scale factor is the median feature intensity on an array. affymetrix normalizes arrays by scaling them based on these values. within an experiment, arrays are expected to have scale factors within 3-fold of each other; arrays whose scale factors are outside this range are considered to have poor quality. the percent present is the percentage of genes called present by affymetrix's detection algorithm  <cit> . these percentages should be similar between replicate samples, and arrays with extremely low values should be considered poor quality. we refer the reader to  <cit>  for a more detailed description of these metrics.

multi-array quality metrics
the first quality metric proposed by bolstad et al.  <cit>  is the relative log expression . these values are calculated by subtracting the median gene expression estimate across arrays from each gene expression estimate, . therefore, the rle for gene i on array j is:  

for a given array, a median rle not near zero indicates that the number of up-regulated genes does not approximately equal the number of down-regulated genes, and a large rle iqr indicates that most genes are differentially expressed. if these indications are not biologically plausible, the array is likely of poor quality.

the second quality metric proposed by bolstad et al.  <cit>  is the normalized unscaled standard error . for a given gene, j, the nuse provides a measure of the precision of its expression estimate on a given array, i, relative to other arrays in the batch. specifically, it is defined as:  

problematic arrays result in higher ses than the median se; therefore, arrays are suspected to be of poor quality if either the median nuse is above one or they have a large iqr.

rle and nuse values can be displayed in boxplots and summarized with the median and interquartile range . both rle and nuse values for any given array depend on the other arrays in the batch; therefore, values from different batches are not directly comparable. also it is important to note that nuse values depend on fitting model  <dig>  also known as the rma model, but rle values do not.

single-array version of nuse
a weakness of the approaches proposed by affymetrix is that the probe-effect, described above, is not taken into consideration. a large proportion of the variation seen across feature intensities can be predicted by the probe-effect implying that the identification of outliers becomes easier when considering this effect. the alternative quality metrics proposed by bolstad et al. do take the probe-effect into account; however, to estimate and adjust for probe-effects, the user is required to analyze multiple arrays simultaneously. such multi-array methods borrow information across arrays which were hybridized under similar conditions allowing the probe-effects to be estimated. while they often provide far better performance, multi-array methods cannot be used in situations where a single array needs to be analyzed. an additional limitation of the methods proposed by bolstad et al. is that they provide a relative measure of microarray quality not an absolute one. that is, rle and nuse values are only able to determine if an array's quality is better or worse than the typical array being analyzed in that experiment or batch. the methods proposed by affymetrix are single-array and do not suffer from these limitations.

in order to obtain a single-array absolute measure of microarray quality, we propose a modification of the nuse metric. we call this new metric a global nuse or gnuse because the quality of an individual microarray is assessed relative to a balanced sample of all publicly available microarray data on a given platform. as such, it provides a global view of microarray quality. specifically, we compute the median se vector from a large biologically diverse data set and use this vector to normalize se values from new arrays. we define a global normalized unscaled standard error  for a given gene, j, on array, i, as:  

where i =  <dig>  ..., i denotes all the arrays in the larger data set. in this paper i =  <dig>   <dig>  as we used the same  <dig>  samples used to create the reference distribution for the current implementation of the frozen robust multi-array analysis  preprocessing algorithm  <cit> . by preprocessing arrays with frma, the values for  are directly comparable across arrays and batches. however, it should be noted that the median se vector is platform-specific.

similar to nuse values, gnuse values can be displayed using boxplots and summarized using the median and iqr with a median gnuse greater than one or a large iqr indicative of poor quality.

RESULTS
assessment of quality metrics
we first evaluate the quality metrics proposed by affymetrix and bolstad et al. based on their ability to provide good bottom-line results for each of the  <dig> primary applications of gene expression microarrays - differential expression, clustering, and sample type prediction. we show that the metrics proposed by bolstad et al. are often able to detect poor quality arrays while the affymetrix metrics typically fail to do so. because in the first three assessments the nuse and gnuse values are nearly identical, we omit the gnuse. in the fourth example, we provide a situation where the gnuse provides more informative results.

differential expression
as our first example we use the data from affymetrix's hgu <dig> spike-in study. in this experiment  <dig> transcripts were spiked in to background rna in such a way that  <dig> arrays were replicated except for these  <dig> transcripts. we selected a subset of  <dig> arrays for which  <dig> sets of  <dig> arrays had identical spike-in concentrations - this is our array data a. we then performed a t-test comparing one group of  <dig> arrays to the other and obtained false discovery rates  - this is the data manipulation f. for various fdr cut-offs we formed lists of candidate genes - our result r. a perfect list will only contain the  <dig> spiked-in transcripts, so we are able to calculate a quantification of accuracy and precision, q. we repeated the analysis, this time removing each array one at a time. based on each of these procedures, we plotted an roc curve . for one particular array , its removal noticeably improved results - a large Δj. the q-values for the true positives further demonstrate the positive effect of removing this array . finally, a residual image shows the array in question has a very strong spatial effect . now the question is: which quality metric detects this array as problematic? the affymetrix quality metrics suggest that the array has similar quality to others ; this is to be expected because affymetrix presumably used their quality metrics to screen these arrays. however, the nuse and rle metrics correctly detect the array in questions as having poor quality.

the q-values for each of the  <dig> spiked in probesets using all  <dig> arrays  and with each of the  <dig> arrays removed . a q-value <  <dig>  denotes a probeset correctly identified as differentially expressed. removing the poor quality array  decreases the q-values, while removing the other good quality arrays increases the q-values.

clustering
for the second example we constructed a data set composed of two replicate arrays for  <dig> different tissues . we then pretended that we did not know the tissues and clustered all the samples using hierarchical clustering with euclidean distance  - this is f. because we in fact know the tissues we can define q as the average distance between replicates. the typical distance between replicates is  <dig> , but one pair of replicate tissues  stands out as clearly problematic . this suggests that one  of the cardiac myocytes arrays has poor quality. the only metric that detects one of these arrays as problematic is the nuse metric . in these data we also noticed an additional sub-cluster ; these arrays are identified by the rle metric as clearly problematic . the nuse and percent present metrics are also able to detect these arrays as being somewhat problematic. the poor quality cardiac myocytes array, detected by nuse, has a strong spatial effect . residual images also show that the sub-cluster of arrays have different expression patterns in specific regions of the array . this must be an artifact; a likely explanation is that affymetrix organizes the probes in rows by sequence properties and sample preparation somehow favored certain probe sequences in the sub-cluster of arrays.

prediction
the final test of a quality metric is whether removing poor quality arrays results in improved inference. to assess this, we considered predicting a clinical parameter, pathologic complete response , based on microarray data provided by md anderson to the maqc-ii project  <cit> .

these data were divided into training and validation sets as part of the original study design. the only modification we made to these designations was to include  <dig> arrays that were flagged as poor quality by the original study participants. this resulted in  <dig> training samples and  <dig> test samples .

to investigate the effect of microarray quality on prediction, we fit a model to the training data using all  <dig> samples and made predictions on the test samples . we then removed the lowest quality array, refit the model, and made a new set of predictions. we repeated this procedure  <dig> times, each time removing one additional array and assessing the prediction by matthews correlation coefficient . for our prediction algorithm, we chose one of the most widely used algorithms - prediction analysis for microarrays   <cit> . this procedure was done for each of the quality metrics described above.

in general, we observed an improvement in prediction when removing the arrays with the poorest quality . however, some metrics did substantially better than others at detecting arrays that negatively affect prediction. in particular, the rle and percent present appeared to perform best, followed by nuse.

gnuse vs. nuse
because most published experiments are composed of primarily good quality arrays, the gnuse and nuse values are often fairly similar. for example, we repeated the prediction analysis above using the gnuse. recall that out of the  <dig> training samples we expect most to be of good quality. the prediction improvement seen using gnuse is nearly identical to that seen using nuse .

however, the gnuse offers two advantages over the nuse. first, gnuse values can be obtained from a single array. second, since the nuse measures quality relative to other arrays in a batch, if most arrays in a batch are of poor quality, the denominator will be inflated and all arrays may appear to be of acceptable quality. the gnuse is not susceptible to such errors because its denominator is computed based on a large fixed sample of arrays. this difference can be seen in boxplots of the nuse and gnuse values for a published data set comprised of a sizable number of poor quality arrays . notice that many of the arrays look acceptable based on the nuse, whereas most appear to be of poor quality based on the gnuse.

assessment of publicly available data
having developed a single-array quality metric  that performs at least as well as the best multi-array quality metrics, we turn our attention to assessing the quality of publicly available microarray data.

geo quality
to assess the overall quality of publicly available microarray data, we computed gnuse values for all affymetrix hgu133a and hgu133plus <dig> miame-compliant arrays available from the gene expression omnibus   <cit>  in december,  <dig>  in total, we assessed  <dig>  affymetrix hgu133a microarrays from  <dig> experiments and  <dig>  affymetrix hgu133plus <dig> microarrays from  <dig> experiments for a total of  <dig>  arrays from  <dig> studies. while most gnuse values are close to one , the long right tails demonstrate that their are some probesets on some arrays that are of very poor quality . in fact, many of these poor quality probesets come from the same arrays .

based on the maqc data described above, we observed that removing arrays whose gnuse median exceeded  <dig>  improved prediction. one can interpret this threshold as filtering arrays whose precision is on average 25% worse than the typical array. based on this threshold, roughly  <dig> % of hgu133a arrays and  <dig> % of hgu133plus <dig> arrays are of poor quality. the distribution of gnuse medians along with this threshold further supports the gnuse median threshold as providing reasonable separation between the majority of arrays with acceptable quality and those with poor quality .

sources of poor quality
we now turn our attention to the potential causes of poor microarray quality. first, we examined  <dig>  microarrays from  <dig> studies consisting of arrays publicly available through arrayexpress  <cit>  or geo for which the lab in which the array was hybridized could be ascertained. we focused on two potential sources of poor microarray quality - the type of sample analyzed or the laboratory in which the sample was analyzed. to investigate these sources, we fit the following random effects anova model:  

with,    

where μ is the overall average gnuse median across all i samples, j sample types, and k labs. sj is the random effect for sample type j, lk is the random effect for lab k, and εi,j,k represents measurement error. we can assess the variability in gnuse medians by comparing the estimated variance of sample type effects, , and the estimated variance of lab effects, . the estimated variance between labs is more than  <dig> times greater than the estimated variance between sample types , suggesting that the lab in which an array was hybridized accounts for more of the variability in microarray data quality than the tissue that was hybridized to the array. figure  <dig> shows the individual lab and tissue effects as well as their estimated variances.

furthermore, we fit a one-way anova model of the gnuse medians  on lab separately for two tissue types analyzed by many labs - bone marrow and brain. table  <dig> shows that most lab effects within each tissue are statistically significant  and practically significant, with estimated lab effects of up to 23%.

anova model effects for gnuse median across labs, for two tissue types - bone marrow and brain. there is a statistically significant difference in quality between labs in both tissue types . a * denotes a statistically significant lab effect.

poor quality studies
finally, we report the overall quality of the  <dig> miame-compliant microarray studies available via geo in december  <dig>  for each study, we report the number of arrays, the average gnuse median, and the proportion of poor quality arrays  in additional file  <dig>  the first result of interest is that array quality does not appear to be correlated with study size .

there are  <dig> studies that are composed of at least half poor quality arrays. some of these extremely poor quality studies can be explained by further examination of the experimental design - for example, gse <dig>  gse <dig>  and gse <dig> hybridized macaca mulatta rna to hgu133a arrays designed to measure human gene expression. however, this only explains a handful of these poor quality studies.

CONCLUSIONS
we have described microarray quality in general and provided the mathematical formalism that permits us to quantify the quality of a microarray hybridization. using this formalism, we have demonstrated how to assess quality based on the  <dig> most common microarray applications and used these applications to describe the strengths and weaknesses of the most common quality metrics used to assess affymetrix genechip microarrays. specifically, we found that the methods proposed by bolstad et al. are often able to detect poor quality arrays while the methods proposed by affymetrix are not. however, the methods of bolstad et al. are inherently multi-array, so we propose a single-array modification of the nuse metric, called the gnuse. we show that the gnuse metric differs substantially from the nuse metric only when the experiment is composed primarily of poor quality arrays.

we then use the gnuse quality metric to assess the quality of publicly available microarray data. we found that roughly 10% of publicly available affymetrix hgu133a and hgu133plus <dig> arrays are of poor quality. we also found that these poor quality arrays are not evenly distributed among labs or studies - that is, some labs are more likely to provide poor quality arrays than others, and some studies are compromised of mostly poor quality arrays.

while the most likely cause of high gnuse values is poor array quality, it is conceivable that a study using a non-standard hybridization protocol or investigating a particularly unusual tissue type might appear to have poor quality. an example of the latter situation is the hybridization of non-human rna to human microarrays. a potential example of the former situation may be the data used to create the biogps webtools  <cit> . the  <dig> arrays used in the creation of these webtools  showed consistently high gnuse values -  <dig> % of the arrays had a median gnuse above  <dig>  and  <dig> % of the arrays had a median gnuse greater than  <dig>  it is difficult to determine whether these arrays are of nearly uniformly poor quality or simply differ from typical arrays in some manner. nevertheless, combining these arrays with arrays from any other experiment would certainly not be advisable.

the greatest strength of the gnuse metric, the ability to assess the quality of a single array relative to overall microarray quality, is also its primary limitation - it requires a sizable number of arrays from different labs and different tissues to assess overall microarray quality. however, with the rapid increase in microarray experiments, this limitation is quickly diminishing, and the advantages of the gnuse metric are growing. while there have been previous attempts at providing array quality metrics coupled with publicly available data sets  <cit>  and at assessing the effect of quality on differential expression  <cit> , these attempts used metrics that could only assess the quality of an array relative to other arrays in the batch or the quality of a batch of arrays relative to other batches of arrays. the incorporation of the gnuse metric in such efforts would allow one to truly assess the quality of publicly available data.

the results presented here are based on the two most widely used affymetrix microarray platforms. as more data becomes available on newer platforms, we look forward to implementing frma and the gnuse on those platforms. we currently have a preliminary implementation of frma on the human exon st  <dig>  array. based on  <dig> publicly available arrays, roughly  <dig> % of arrays have a median gnuse greater than the quality threshold of  <dig>  . this may indicate that newer arrays are of better quality or that the quality threshold needs to be reassessed when measuring exon-level rather than gene-level expression.

while the results presented here focus primarily on affymetrix genechip microarrays, many of the ideas can be generalized to other platforms and manufacturers. specifically, we recommend defining quality in a quantitative manner that focuses on the bottom-line results from common genomic applications.

furthermore, assessing the quality of one sample in the context of the wealth of public data is a powerful technique for developing quality metrics in high-throughput studies. we believe that the ideas and formalism described here can form the basis for future quality assessments of other microarray platforms and even other genomic technologies.

the gnuse algorithm is available as part of the frma r package on bioconductor  <cit> .

authors' contributions
mm helped design the study, carried out some of the analyses, and wrote the manuscript. pm helped design the study, carried out some of the analyses, and helped prepare the manuscript. ml organized and annotated the data. wh helped conceive the paper. ri conceived the study, carried out some of the analyses, and helped write and edit the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
supplementary figures. figures s1-s <dig> 

click here for file

 additional file 2
gnuse by study. table containing the overall quality of the  <dig> miame-compliant microarray studies available via geo in december  <dig>  for each study, we report the number of arrays, the average gnuse median, and the proportion of poor quality arrays .

click here for file

 acknowledgements
we thank catherine ball for motivating us to write a review of quality assessment. we thank the maintainers of geo and arrayexpress for making the data publicly available. the work of r.i was partially funded by national institutes of health . the work of m.m. and p.m. was partially funded by national institutes of health .
