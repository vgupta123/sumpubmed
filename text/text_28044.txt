BACKGROUND
mathematical modelling is becoming increasingly recognized as an important tool in the study of biological systems
 <cit> . databases such as biomodels
 <cit>  provide access to peer-reviewed published models, which can be freely downloaded in the standards-compliant sbml format
 <cit> . model construction, however, is only the first in a number of steps that can be used to gain insight into the function of a complex biological system.

many models consist of a set of ordinary differential equations  describing how the concentrations of various chemical species change with time. the dynamic behaviour of the model can be examined through integration of the odes, a relatively computationally simple task. in many cases, it can be helpful to simulate the model behaviour using a stochastic or hybrid algorithm
 <cit> , which can yield more information than a deterministic solution can provide. solving a model using a stochastic method requires more computational power than solving it deterministically
 <cit> , and must be repeated many times to yield information on the distribution of potential trajectories.

in addition to examining the dynamic behaviour of a model, we may also wish to analyse it using techniques such as sensitivity analysis – examining the influence of specific model parameters  on a systems-level property . other tasks commonly used to examine models include optimizations , or parameter fitting .

copasi
a number of software tools are available for biological systems modelling. some of these, such as matlab, are very flexible, but require users to have a knowledge of programming. other tools, such as copasi
 <cit> , are designed with a user-friendly interface, and do not require any programming expertise, while still providing sophisticated algorithms and analyses.

copasi allows users to construct and analyse models using a graphical user interface . simulations using a deterministic, stochastic or hybrid method can be performed at the click of a button. in addition to time-course simulations, copasi can perform a range of model analysis tasks, including steady-state analysis, sensitivity analysis, parameter estimation, and optimizations.

as models become more detailed and complex, simulation and analysis requires increasing amounts of computational power, potentially requiring more power than can be offered by even the most high-powered desktop machine.

client-server simulation tools
the limitations of using personal computers to perform computationally intensive simulations have led to the development of a number of server-based systems biology tools. these allow a user to set up mathematical models and to display any results on their local machine, but to perform computationally-intensive simulations on one or more remote machines.

one such example is jws online
 <cit> , a modelling tool accessed through a web-based java applet, which performs all simulations on a remote server. the computational capacity is limited by the performance of the server, and so it is not ideally suited for computationally intensive tasks. vcell
 <cit>  is another tool for model simulation and analysis. models are prepared using a local java interface, and simulations are performed on a distributed computing cluster operated by the vcell team, up to a maximum of  <dig> simulation repeats per submission. parameter estimations can also be performed, but these are run locally.

another way of running simulations remotely is to incorporate them into an automated distributed workflow, using a management tool such as taverna
 <cit> . copasi web services
 <cit>  enables copasi simulation and analysis tasks to be incorporated into a distributed workflow by setting up copasi simulation servers to offer computing cycles. using automated workflows allows for very flexible usage patterns, though creating workflows can be a complex process. in addition, users must have the computational resources available for running simulations.

condor
condor is a system for high-throughput computing, allowing computing jobs to be run on a pool of machines
 <cit> . the pool can contain dedicated computers, though one of the main strengths of condor is its ability to utilize non-dedicated machines during periods when they would otherwise be idle. for example, in academic institutions, large numbers of computers are in use during working hours on weekdays, but will sit unused overnight and at weekends. condor is often configured to detect when a machine is not being used by its owner, and can then assign queued computing jobs to be run on it. condor is compatible with most major operating systems, including windows, linux and osx.

in order to submit a computing job to run on condor, a job description file must first be prepared. this file contains information about the executable file to be run, any files that must be transferred to remote machines, and the software and hardware requirements of the job, such as the operating system and minimum amount of memory required. once the job description has been prepared, it is submitted, and added to a queue for resources. a machine known as the master decides when, and on which machine each job is to be run.

condor pools can contain thousands of machines. the computing power of such pools can be most readily exploited when a computing task can be split into multiple, independent, jobs which can be run in parallel. however, the requirement to split large tasks into multiple smaller ones, along with a complex condor job submission process involving command line tools, makes this difficult for many users.

condor-copasi
copasi can be used, without any modification, with high-throughput computing environments such as condor. however, in order to exploit the parallel nature of a condor pool, any simulations or analyses must be manually split into multiple small, independent jobs a task that can be difficult. in addition, condor requires the use of command-line tools to submit jobs and to monitor their status. this can be difficult, and may deter many users from making use of these facilities.

therefore, we developed condor-copasi, a tool which integrates copasi with condor, allowing users to perform a selection of model analysis and simulation tasks in parallel using a condor pool. condor-copasi is designed to be very simple to use, with all user interaction taking place through a gui. the process of splitting tasks into an optimal number of parallel jobs, and submitting them to the condor pool is handled automatically, without requiring user interaction. once jobs have been submitted to the condor pool, condor-copasi monitors their status, automatically emailing a notification to the user upon completion. results can be displayed within the interface in a variety of formats, including tables and interactive graphical displays, or can be exported for further processing.

implementation
condor-copasi is a server-based application, accessed by users through a web interface which is compatible with all modern web browsers. access to the web interface is controlled on a per-user basis; the administrator must create an account for each user of the system. a database is used to store user account information, along with information about the computing jobs each user has submitted, such the submission and end times, the number of parallel jobs used and cpu hours used. historical usage data is stored, and statistics can be displayed though the web interface to monitor how the system is being used. other files, such as copasi models, and outputted results are stored as flat files in a directory of the system.

we designed condor-copasi to run as a server-based application, since the alternative – a local application to be run on each users machine – would require condor to be installed and configured on each individual local machine. installing condor and configuring it to be able to submit jobs is a complicated process, and in many cases impractical. in addition, condor requires that the machine which is used to submit jobs must remain powered on while remote jobs are running; an onerous requirement for many users, particularly when jobs have a long run time and are submitted from a portable machine. running as a server-based application removes this requirement for the individual users, since only the server must remain powered on while jobs are running.

condor-copasi is written in the python programming language
 <cit> , using the django web development framework
 <cit> . it must be installed on a server which has access to a condor pool and permission to submit jobs. additionally a web server must also be installed . finally, a django-compatible database must be available – choices currently include mysql, postgresql, oracle and sqlite.

condor-copasi allows users to submit a number of predefined tasks, each of which is amenable to running in parallel . tasks are submitted by uploading a pre-prepared copasi model using the web interface . condor-copasi then automatically determines the best way to split the task into parallel, automatically creates the necessary files, and submits the parallel jobs to condor.

while the web interface is written in python, all simulation and analysis tasks are performed using the copasi simulation engine. condor-copasi works by automatically modifying the xml-based copasi file format, generating a custom copasi model file for each parallel job. in addition to the model file, a condor job specification file is automatically generated for each parallel job. these files are submitted to the condor pool, along with a copy of the copasi simulation engine  binary for the appropriate machine architecture. the copasi simulation engine carries out all computation on the remote machine, writing output to a text file. these text files are transferred back to the condor-copasi server, where they are processed and collated.

various graphical plots can be produced. static two-dimensional charts such as depicting the dynamics of mean and standard deviation of particle numbers in stochastic simulations are generated using the python matplotlib library
 <cit>  . an interactive bar chart, showing maximal and minimal sensitivity values of various model parameters for different potential parameter sets is also available. a scroll bar can be dragged to change the range of the parameter set, while the chart values update automatically. this feature is provided using the google motionchart api
 <cit> , the chart is displayed using a mixture of javascript and flash; all data is processed and rendered locally, and no data is sent to google.
 <dig> 

RESULTS
available use cases
condor-copasi currently provides access to seven use cases, each of which is likely to require significant amounts of computational power, and can be efficiently split into smaller independent parallel jobs.

global sensitivity analysis
the global sensitivity analysis procedure, as described in
 <cit> , involves performing a number of optimizations – one maximization and one minimization for each parameter. since each optimization can be run independently, the task can be trivially split, with one parallel job for each maximization or minimization. for example, a model with  <dig> parameters for which a sensitivity is to be calculated will generate  <dig> parallel jobs.

copasi provides access to a number of optimization algorithms, including deterministic such as truncated newton
 <cit>  and stochastic such as particle swarm
 <cit> . any of these algorithms can be selected for use in this task. if a user wishes to try more than one optimization algorithm, multiple tasks should be prepared and submitted, each with a different algorithm selected.

results are provided in table format, and through graphical summaries of the importance of each parameter to the target property of the model. charts showing progress of the optimization tasks can also be generated, displaying the best optimization value against the number of steps taken by the algorithm.

stochastic simulation repeat
this task allows for multiple repeats of a stochastic time-course simulation to be performed – a necessary procedure to determine the distribution of the trajectories. the results of each repeat are recorded, and for each time point particle number means and standard deviations are calculated. plots of the results can be displayed in the web interface, and all result outputs can be downloaded as a tab-separated text file. splitting of this task is carried out using the load balancing algorithm described below.

parallel scan
the parameter scan task in copasi automatically scans through various values for one or more parameters, performing a subtask such as steady-state analysis, a time-course simulation, or an optimization for each set of parameters. it can also be used to repeat a subtask a number of times using the same parameter values, or to sample parameter values from a random distribution. condor-copasi can split this task into parallel, using a non-overlapping range of parameter values for each chunk. the number of subtask repeats to perform in each parallel job is determined using the load balancing algorithm.

due to the diverse nature of possible outputs from this task, condor-copasi does not produce graphical plots. instead, a text-based output must be prepared for the task by the user, before the task is submitted. after the task has finished, a collated text file containing the output from all parallel jobs is made available to download; this file is identical to the output that would have been produced had the parameter scan run on a single machine.

optimization repeat
many of the optimization algorithms available in copasi are stochastic, and will return a different solution each time they are run. condor-copasi can repeat an optimization multiple times, using the same algorithm for each repeat, and from these repeats, determine the best objective value and associated parameter set. the number of optimization repeats to be performed for each parallel job is determined using the load balancing algorithm. once the task has finished running, a copasi model file containing the best parameter values can be downloaded. alternatively, the parameter values can be displayed in the browser.

parameter estimation repeat
the parameter estimation task in copasi is able to find the best parameter set to fit experimental data. like the optimization task, many of the algorithms available are stochastic, and each time they are run will return a different solution. condor-copasi can repeat a parameter estimation multiple times, using the same algorithm for each repeat, determining the best solution from all repeats, using the load balancing algorithm to determine how many repeats to perform in each parallel job. once the task has completed, a copasi model file containing the best parameter set found by the parameter estimation can be downloaded, or the parameter values can be displayed on-screen.

optimization with different algorithms
since copasi has many optimization algorithms available, and it is not clear which one works best for each problem, a modeller often wishes to run the problem through several of them. condor-copasi can run an optimization several times using a different algorithm for each one. the user can select which algorithms will run, and can configure all tuning parameters of those algorithms. each optimization is run as a separate parallel job, and after all jobs have completed, the best algorithm are determined. in addition, a copasi model file containing the best available parameter set as determined by the optimization can be downloaded, or the parameter values viewed in the browser.

raw mode
the raw mode task is designed for advanced users, and allows one or more copasi tasks to be repeated an arbitrary number of times. the user is able to specify all command-line arguments for the copasise binary, and must specify any required input and output files. one repeat is performed per parallel job, and any output files generated must be manually collated and processed by the user. this mode extends the applications of condor-copasi to a number of other possibilities, however it requires the user to understand the use of copasi through the command line interface, as well as a basic knowledge of distributed computing.

load balancing
the stochastic simulation, parallel scan, optimization repeat and parameter estimation repeat use cases all involve repeating a particular task multiple times, and can be run in parallel by performing a certain number of repeats per job. for these tasks, the user specifies the total number of repeats to perform, while the number of repeats to perform per job, and subsequently the total number of parallel jobs, is determined using a load balancing algorithm.

the load balancing algorithm constructs the parallel jobs such that they each run for an approximately equal length of time, t, a parameter which is set by the system administrator. the algorithm first measures how long a single repeat of the task to be performed takes to complete. the parallel jobs are then built up with an appropriate number of repeats, such that the total run time of each job is approximately equal to t. if a single run takes longer than t, the algorithm will time out, and assign only one repeat per job.

it is important to choose a good value for t. if it is too small, each task will produce a large number of parallel jobs, each with few repeats. in this situation, the overhead associated with submitting jobs to the condor pool will become a significant factor, and in an extreme situation, running the job on condor could take longer than if all jobs had been run sequentially on a single machine. however, if t is too large, then each task will produce a small number of parallel jobs, each taking a long time to complete. in this situation, the computational capacity of the condor pool will not be fully exploited, and the benefits of running in parallel will be negated. in addition, jobs may run for too long, risking eviction from the machines they are running on – non-dedicated machines in the pool are normally only available when they would otherwise be idle, such as overnight and at weekends – meaning there is often an upper bound on the length of time a job can run for.

to determine the best value for t, we ran an optimization repeat task with t values ranging from  <dig>  to  <dig> minutes . we found that setting t to  <dig> minutes gave a good trade-off between job submission overhead and gaining the benefits of running jobs in parallel. however, factors such as the number of machines in the condor pool, and the speed of network communications between the server and machines in the pool will impact on this value. therefore, we advise administrators to consider adjusting this value if necessary.

we also include an option for applicable tasks to override the load balancing algorithm, and to construct jobs with only one repeat per parallel job. this is useful in situations where the user knows a priori that each repeat is likely to take longer than t, saving them from having to wait for the load balancing algorithm to time out, or in situations where one wishes to make the run-time for each parallel job as short as possible.

error handling
condor handles various types of error – if jobs are evicted from the machine they are running on, they will automatically be re-queued and executed on an available machine. in cases where a job fails , the job will remain in the queue, but will be marked as ‘held’. we note that while condor supports application checkpointing for compatible software, which allows evicted jobs to resume on a different machine without loss of computation time, copasi does not support checkpointing, so all evicted jobs must begin again from the start.

condor-copasi monitors the status of each submitted job – the queue of jobs is periodically polled to check for jobs marked as ‘held’, and the exit status of all completed jobs is checked  by parsing the log files. when an error is detected, condor-copasi will try to determine whether it happened before copasi was executed on the remote machine  or after .

after detecting an error, condor-copasi will email the user to notify them that the task did not complete successfully. the web interface will display the probable cause of the job failure , and allow the user to download a compressed copy of all files generated by condor-copasi, including condor log files, automatically generated copasi model files, and any returned results files. such files can assist the user in determining the cause of the failure, and allow them to manually collate any results generated if they will be useful.

finally, condor-copasi will log all activity to a text file, including any errors and exceptions encountered. examining this file can be helpful to determine the cause of a failure if it is otherwise not clear.

performance
to illustrate the effectiveness of condor-copasi, we collated data from  <dig> months of real-world usage on our installation of condor-copasi; for each task submitted, we recorded the total cpu time used by all parallel jobs, the wall clock time of the task, and the number of parallel jobs used. the cumulative total of cpu time used does not include any time spent queuing for resources to become available, or any other delays caused by the condor job submission process. it is analogous to the time it would take to run each parallel job sequentially on a single-core local machine, and therefore provides a good measure of how long the computing task would have taken to perform without using condor-copasi. the wall clock time represents the total waiting time between submission of the task to condor-copasi and its completion, and includes time where possibly no jobs were running because the condor pool was running jobs for other users. therefore, the total waiting time for each task is not necessarily dependent only on the nature and size of the task. the wall clock times reported come from a production system where condor was shared with other users and are therefore indicative of typical usage.

in all cases, the tasks were run on our local condor pool of approximately  <dig> execute nodes. the most common hardware configuration in the pool is an intel core <dig> quad processor at 3ghz, with 4gb of ram. however, we note that the pool is heterogeneous, with a number of different hardware configurations, and nodes continuously coming online and offline.

to illustrate the improvements in run time using condor-copasi, we calculated the speed-up factor for each task, defined as cpu time/wall clock time. the speed-up factors were plotted against the number of parallel jobs used . in general, speed-up factors of between  <dig> and 103are seen. for tasks except global sensitivity analysis, there is a roughly linear relationship between the number of parallel jobs used, and the speed-up factor achieved. we note that the global sensitivity analysis task cannot be parallelized as efficiently as the other task types, so the highest speed-up factors for this task type were lower than other task types. some tasks achieved a low speed-up factor ; most of these tasks had a low cpu time and used few parallel jobs, so there was little benefit to running in parallel. of particular note are two global sensitivity analysis tasks with speed-up factors of around  <dig>  and  <dig> , with  <dig> parallel jobs each. for these tasks, the cpu time used was very low , while several hours were spent waiting for resources in the condor pool.

specific examples
to further illustrate the benefits of running tasks on condor-copasi, we describe  <dig> detailed examples of tasks that were run , showing the extent to which they were parallelized, and the speed-up factor achieved .

global sensitivity analysis – we ran a global sensitivity analysis on a model of nfκ b signal transduction
 <cit> , examining the control of  <dig> parameters on the frequency of nuclear nfκ b oscillation, using a parameter space consisting of the original parameter values ±20%. the task completed on condor-copasi in approximately  <dig> hours using  <dig> parallel jobs, using a cumulative total of  <dig> hours of computing time, achieving a speed-up factor of  <dig> 

global sensitivity analysis – we ran a global sensitivity analysis on a model of the mapk signalling cascade
 <cit> , examining the control of  <dig> parameters on the concentration of nuclear mapk-pp, using a parameter space consisting of the original parameter values ±50%. the task completed on condor-copasi in approximately  <dig> hours using  <dig> parallel jobs, using a cumulative total of  <dig> hours of computing time, achieving a speed-up factor of  <dig> 

stochastic simulation repeat – we ran  <dig> , <dig> repeats of a stochastic time-course simulation of a three-variable calcium oscillation model
 <cit> . condor-copasi completed the task in approximately  <dig> hours , using a cumulative total of  <dig>  hours of computing time across  <dig> parallel jobs, achieving a speed-up factor of  <dig> 

scan in parallel – we used the parallel parameter scan task to perform  <dig>  monte carlo simulations of an nfκ b signal transduction model
 <cit> . condor-copasi completed the task in approximately  <dig> hours, using  <dig>  parallel jobs, using a cumulative total of  <dig>  hours of computing time, achieving a speed-up factor of  <dig> 

scan in parallel – we used the parallel parameter scan task to perform  <dig> , <dig> monte carlo simulations of a mapk signalling cascade model
 <cit> . condor-copasi completed the task in approximately  <dig> hours, using  <dig>  parallel jobs, using a cumulative total of  <dig> hours of computing time, achieving a speed-up factor of  <dig> 

discussion
performance
condor-copasi enabled us to significantly reduce the run time of many simulation and analysis tasks. in  <dig> months of real-world usage on our installation, we saw tasks running up to  <dig> times faster than if they had been run on a single computing core, with an average speed-up of  <dig> times. this has enabled us to perform model simulations and analyses that would otherwise not have been feasible, with some individual analysis tasks using more than a year of computing time, but completing in less than a day.

in an ideal situation, for most task types, the decrease in run time for running a particular task on condor compared to running it on a single computing core should be proportional to the number of executing nodes available in the condor pool. so, for example, if we have a task that takes  <dig> minutes to run on a single core, and a condor pool available with  <dig> equally fast executing nodes, then the speed increase would be 1000-fold and the task would complete in  <dig> minute.

in practice, various limitations, overheads and discrepancies in the local network architecture and condor pool mean the overall running time for a task will always be more then the theoretical minimum, and it is rarely possible to predict exactly how long a task will take to complete.

there is an overhead associated with submitting and running each parallel job – the condor master must add the job to the queue for resources and assign it to an appropriate executing node when one becomes available. the submitting node must then send the associated model and data files, along with a copy of the copasi binary, over the network before job execution can start. in a situation where we have a small number of jobs to submit, each of which will take a long time to execute, the overhead will likely not be significant. however, if we have a large number of jobs to submit , each of which will can be executed in a short amount of time, then the submission overhead will be more significant, and could become a limiting factor in the execution time for the task. in this situation, it may be preferable to construct the parallel jobs so that they each perform a certain number of repeats, increasing the job execution time while keeping the submission overhead constant, thus reducing the impact of the submission overhead. the load balancing algorithm described above attempts to find an ideal compromise between the degree to which the job is parallelized and the job submission overhead.

another factor that can affect the execution time of our jobs is the potentially heterogeneous nature of the machines in the condor pool – disparities in hardware specification will mean that the job execution time will vary from machine to machine. thus, unless we specify exact hardware requirements for our jobs, their execution time will vary depending on the specification of the machine they are assigned to.

we must also consider that we may be competing for the available resources with other users. therefore, the number of available executing nodes will depend on how many other users are using the condor pool, and our priority within the queue relative to others.

finally, the extent to which we can parallelize our task depends on the task type. where the task involves repeating a subtask a certain number of times, we can parallelize up to the extent where we have one repeat per parallel job. other task types, such as the global sensitivity analysis, can only be parallelized according to the number of parameters we are investigating. for example, an analysis on a model with  <dig> parameters will produce  <dig> parallel jobs . in this situation, having more than  <dig> executing nodes available will not speed up overall job execution any more than having just  <dig> 

in summary, the degree to which running a task on condor will speed up execution compared to running the same task on a single local machine depends on the type of task being performed, and the degree to which it can be parallelized, and will also depend on a number of other factors, such as submission overheads and demand for the available resources. however, in testing, we saw vast improvements in the run-time of all task types .

comparison with existing software
unlike tools such as copasi web services, condor-copasi works as a standalone piece of software, performing all aspects of file preparation, simulation and results processing with a user-friendly interface, requiring minimal user interaction. other tools, such as vcell and jws online, provide user-friendly graphical interfaces, but are less able to fully exploit the parallel nature of a distributed computing pool.

other cluster management tools
condor is one of a number of systems designed for cluster management and job scheduling; other systems with widespread deployment include oracle grid engine 
 <cit> , maui
 <cit> , and pbs
 <cit> . the strengths of condor , make it an attractive choice for academic institutions, particularly those looking to utilize existing hardware. however, we recognize that many potential users may only have access to other distributed computing systems. if demand dictated, it could be possible to add support for other systems at a later date, though differences in job preparation, submission, and monitoring would make this task non-trivial. as an alternative, we note that it is possible to install condor alongside other job schedulers; the condor scheduler will only assign jobs to computing nodes with available computational capacity.

we also note the growing popularity of cloud computing systems, such as amazon’s elastic compute cloud , which allow users to lease any required computational capacity by the hour, without having to invest in dedicated hardware. such systems are particularly appealing to researchers who need only occasional access to large amounts of computational power. it is currently possible to use condor on amazon ec <dig> by running one or more ec <dig> instances as executing nodes. however, setting up such an arrangement is difficult, requiring a detailed knowledge of network configuration, and is likely to be beyond the capacity of most casual users. therefore, a possible future extension of condor-copasi could be to automatically configure and launch ec <dig> or other cloud computing instances to form a condor pool, and to use this to complement, or as an alternative to, a local condor pool.

CONCLUSIONS
condor-copasi enables the use of ubiquitous distributed computing by making it easy to submit systems biology simulation and analysis tasks without requiring any knowledge of programming or managing networks of workstations. the computational power available in such pools of computers can be vast, particularly in institutions such as universities, which have thousands of computers serving as terminals for only a portion of the day. being able to efficiently utilize such a resource can enable large-scale simulations and analyses to be performed that would otherwise require too much computing power to be feasible.

availability and requirements
project name: condor-copasi

project home page:http://code.google.com/p/condor-copasi/

operating system: server-side software: linux; user interface can be accessed on any operating system through a modern web browser

programming language: python and django

other requirements: python  <dig> ; django  <dig> ; a web server with python support such as apache; copies of the copasi simulation engine  binary in 32-bit and 64-bit mode for windows, osx and linux; the lxml and matplotlib python libraries; condor version  <dig>  or higher, a database application – either mysql, postgresql, sqlite or oracle – with an appropriate python wrapper.

license: artistic license  <dig> 

restrictions to use by non-academics: none

full instructions on deployment, and an instruction manuals users are available on the project home page:
http://code.google.com/p/condor-copasi.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
ek wrote the software and prepared the first draft of the manuscript. pm and ek developed the specifications of the software. pm, ek and sh tested the software. all authors read, improved, and approved the final manuscript.

funding
pfizer/bbsrc grant bb/g529859/ <dig>  nih grant r <dig> gm <dig>  bbsrc/epsrc grant bb/c008219/ <dig> , eu fp <dig> grant  <dig> .

