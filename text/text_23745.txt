BACKGROUND
with the development of microarrays technology, more and more statistical methods have been developed and applied to the disease classification using microarray gene expression data. for example, golub et al. developed a "weighted voting method" to classify two types of human acute leukemias  <cit> . radmacher et al. constructed a 'compound covariate prediction' to predict the brca <dig> and brca <dig> mutation status of breast cancer  <cit> . the family of linear discriminant analysis  has been widely applied in such high-dimensional data  <cit> . lda computes the optimal transformation, which minimizes the within-class distance and maximizes the between-class distance simultaneously, thus achieving maximum discrimination. many other works have also extended the lda framework for handling the large p  and small n  problem. for example, shen et al. developed an eigengene based linear discriminant model by using a modified rotated spectral decomposition approach to select 'hub' genes  <cit> . pang et al. proposed an improved diagonal discriminant method through shrinkage and regularization of variance, a method to borrow information across genes to improve the estimation of gene-specific variance  <cit> .

studies have shown that given the same set of selected genes, different classification methods often perform quite similarly and simple methods like diagonal linear discriminant analysis  and k nearest neighbor  normally work remarkably well  <cit> . however, because the data points in microarray data sets are often from a very high-dimensional space and in general the sample size does not exceed this dimension, which presents unique challenges to feature selection and predictive modeling. thus, finding the most informative genes is a crucial task in building predictive models from microarray gene expression data to handle the large p  and small n  problem. to tackle this issue, different clustering-based classification approaches were proposed to reduce the data dimensions.

li et al. developed cluster-rasch models, in which a model-based clustering approach was first used to cluster genes and then the discretized gene expression values were input into a rasch model to estimate a latent factor associated with disease classes for each gene cluster  <cit> . the estimated latent factors were finally used in a regression analysis for disease classification. they demonstrated that their results were comparable to those previously obtained, but the discretization of continuous gene expression levels usually results in a loss of information. hastie et al. proposed a tree harvest procedure for find additive and interaction structure among gene clusters, in their relation to an outcome measure  <cit> . they found that the advantage of the method could not be demonstrated due to the lack of rich samples. dettling et al. presented an algorithm to search for gene clusters in a supervised way. the average expression profile of each cluster was considered as a predictor for traditional supervised classification methods  <cit> . similar idea was further explored by park et al.  <cit> . they took a two-step procedure: 1) hierarchical clustering and 2) lasso. in the first step, they defined super-genes by averaging the genes within the clusters; in the second step, they used the super-gene expression profiles to fit regression models. however, using simple averages will discard information about the relative prediction strength of different genes in the same gene cluster  <cit> . yu also compared different approaches to form gene clusters and the resulting information was used for providing sets of genes as predictors in regression  <cit> . however, clustering approaches are often subjective, and usually neglect the detailed relationship among genes.

recently, gene co-expression networks have become a more and more active research area  <cit> . a gene co-expression network is essentially a graph where nodes in the graph correspond to genes, and edges between genes represent their co-expression relationship. the gene neighbor relations  in the networks are usually neglected in traditional cluster analysis  <cit> . one of the major applications of gene co-expression network has been centered in identifying functional modules in an unsupervised way  <cit> , which may be hard to distinguish members of different sample classes. recent studies have shown that prognostic signature that could be used to classify the gene expression profiles from individual patients can be identified from network modules in a supervised way  <cit> .

in this study, we propose a network modular-based lda  method for improving the prediction performances of dlda, dqda and among others. the major difference between our method and other lda-based methods is that mlda incorporates the gene network modules into lda in a supervised way. we built the mld prediction model using modular-specific features. as a comparison, we also implement a variant of super-gene based regression models  <cit> . we first define super-genes by extracting the first principal component  within the network modules. we then use the super-gene expression profiles to fit a logistic regression  model. we named the method as mpclr.

materials and methods
data sets
three real microarray data sets are used in evaluating the performance of our proposed algorithm and other established classification methods. the detailed description of these data sets is shown in table  <dig>  we got the preprocessed colon cancer microarray expression data from http://genomics-pubs.princeton.edu/. for prostate cancer and lung cancer microarray data sets, we downloaded their raw data from gene expression omnibus  and preprocessed using robust multi-array average  algorithm  <cit> .

seed-based network-module identification
to identify gene modules in a gene co-expression network, we modify the correlation-sharing method developed by tibshirani and wasserman  <cit> , which was originally proposed to detect differential gene expression. specifically, we first use a seed-based approach to identify correlation-shared gene modules from gene network. each of these modules includes a differentially expressed gene between sample classes, which is treated as a seed, and a set of other genes highly co-expressed with the seed gene. the revised approach works in the following steps:

1: build a co-expression network using pearson correlation coefficient   <cit> .

2: compute test statistic tifor each gene i in the co-expression network using the standard t-statistics or a modified t-statistics, such as significance of microarrays   <cit> .

3: rank the absolute test statistic values from the largest one to the smallest one and select the top m genes as seed genes.

4: find the module membership s for each selected seed gene i* in the co-expression network. the module assignments can be characterized by a many to one mapping. that is, one seeks a particular encoder cr that maximizes

  is*=max{0≤r≤1}avei∈cr|ti| 

where cr={s:abs)≥r}. the set of genes s for each seed gene i* is an adaptively chosen module, which maximizes the average  differential expression signal around gene i*. the set of identified genes s should have absolute  correlation  with i* larger than or equal to r.

mlda algorithm
we propose a new formulization of the traditional linear discriminant analysis. specifically, we first use the seed-based approach to identify gene network modules. then we perform lda in each module. the linear predictors in all the identified modules are then summed up. the new modular-based classification approach returns signature components of tight co-expression with good predictive performance.

let assume there are a and b two sample groups , which have na and nb samples, respectively. the data for each sample j consists of a gene expression profile xj = , where xij be the log ratio expression measurement for gene i =  <dig> ,...,p and sample j =  <dig> ,...,n, n = na+nb. we assume that expression profiles x from group k  are distributed as n. the multivariate normal distribution has mean vector μk and covariance matrix ∑k.

in a simplified way, we assume that ∑ = ∑a = ∑b = {σi,i'}i,i' =  <dig> ...,p , whereσii=σi <dig> σii' = σi'i and σii' is the pooled covariance estimate of gene i and gene i' for sample groups a and b. therefore, when ∑^ is a block-diagonal structure, we have

 ∑^=∑^100…00∑^20…000∑^3…0⋮⋮⋮⋮⋮000…∑^cp*p 

where c is the number of blocks  and∑^c is the estimated covariance matrix for block c.

the linear predictor  with block-diagonal covariance structure is given by

  lp= ∑c=1cxc-12μac+μbct∑^c-1μac-μbc 

where xct is the expression measurements of the genes in module c for a new sample to be predicted and μkc and diagonal linear discriminant analysis   <cit>  are the special cases of mlda. that is, when c =  <dig>  lp=x-12t∑^- <dig>  where xt is the expression measurements of p genes for a new sample to be predicted, so mlda is simplified to lda; when c = p , lp= ∑i=1pxi-12t{/σi2}, where xi is the expression measurement of gene i for a new sample to be predicted, so mlda is simplified to dlda.

we estimate the mean vector μkc of the genes in module c asx ¯kc and use the pooled estimate of the common covariance matrix in each module c

  ∑^c=sac+sbcna+nb- <dig> 

where skc={σ^ii′c}, i,i' =  <dig> ...,pc and pc is the number of genes in the module c. σ^ii′c is estimated as

  σ^ii′c=σ^i2fori=i′σ^iσ^i′r ^cfori≠i′ 

where r ^c=median{r ^ii′} i,i' =  <dig> ...,pc and i ≠ i', r ^ii′ is the correlation estimate between gene i and gene i' in module c of sample group k.

∑c is inversible when n ≥ pc, that is,

 ∑-1=∑1-100…00∑2-10…000∑c-1…0⋮⋮⋮⋮⋮000…∑c- <dig> 

however, in some modules , it is possible that n <pc. in this case, ∑c is not inversible. we apply singular value decomposition  technology  <cit>  to solve the problem. assume ∑c is a pc ×pc covariance matrix, which can be discomposed uniquely as ∑c = udvt, where u and v are orthogonal, and d=diag with σ1≥σ2≥,...,≥σpc≥ <dig>  if ∑c is a pc × pc nonsingular matrix ), then its inverse is given by ∑c-1=vd-1ut where d-1=diag.

the rule to assign a new sample j to group k is, thus, based on:lp>=lognbna, sample j is assigned to group a; otherwise, it is assigned to group b.

mpclr algorithm
in order to compare mlda with other super-gens based classification approaches, we also implement a variant of super-gene based regression models  <cit> . mpclr classification algorithm includes three stages: 1) construct correlation-sharing based gene network modules; 2) extract meta-gene expression profiles from the constructed modules using principal component analysis ; 3) classify samples using pca-based logistic regression model. here we briefly described each of the three stages:

stage 1: construct seed-based gene network modules. this can be done using the same approach as used in mlda algorithm described above.

stage 2: principal component analysis of correlation-shared expression profiles: to do this, for each of the seed-based gene network modules, we perform principal component analysis. specifically, for a given gene module with pc genes, we assume xj=x1j,x <dig> j,...,xpcj be expression indices of pc genes in the jth sample. let ∑ be covariance matrix of x with dimension pcxpc. all positive eigenvalue of ∑ are denoted by λ1>λ2>...>λpc . the first pc score of the jth sample is given by xj*=e1txj, where e <dig> is the eigenvector associated with λ <dig>  therefore, we can define the super-gene expression profile for n samples in a seed-based gene module as x*=x1*,x2*,...,xn*. the estimated values for the coefficient e1t of the first pc can be computed using singular value decomposition   <cit> . briefly, assume e be an nxpc matrix with normalized gene expression values of pc genes in a given module, so we can express the svd of e as e = udat, where u = {u <dig> u <dig> ...,ud} is a nxd matrix ), d=diag{d <dig> d <dig> ...,dd12} is a d × d diagonal matrix where dk is kth eigenvalue of et e, a = {e <dig> e <dig> ...,ed} is a pcxd matrix where ek is eigenvector of associated with λk and coefficients for defining pc scores. magnitude of loadings for the first principal component score can be viewed as an estimate of the amount of contribution from the module genes.

stage 3: classification using pca-based logistic regression model: assume y is a categorical variable indicating the disease status . here we only focus on binary classification and suppose that y =  <dig> denotes the presence and y =  <dig> indicates the absence of the disease. therefore, we can have the following supervised pca-based logistic regression model:

  logpj1-pj=β0+ ∑icβi*pc1i*j+εj 

where pj=pr. pc1i*j is the first principal component score estimated from the seed gene module i* for sample j and represents the latent variable for the underlying biological process associated with this group of genes. the model was fitted using glm function in stats r package.

comparisons of different supervised classification methods
we compared the prediction performances of mlda with other established supervised classification methods, which include diagonal quadratic discriminant analysis , dlda, one nearest neighbor method , support vector machines  with linear kernel and recursive partitioning and regression trees . we used the implementation of these methods in different r packages http://cran.r-project.org/, which are sma for dqda and dlda, class for 1nn, e <dig> for svm and rpart for trees. default parameters in e <dig> and rpart for svm and tree were used, respectively. for other methods , there are no tuning parameters to be selected. in the comparisons, seed genes were selected using t-test and sam, respectively. we evaluated the performances of dqda, dlda, 1nn, svm and trees based on different number of the selected seed genes and those of mpclr and mlda based on different number of gene modules, which were built on the selected seed genes.

cross-validation
we performed 10-fold cross-validation to evaluate the performance of these classification methods. the basic principle is that we split all samples in a study into  <dig> subsets of  equal size, set aside one of the subsets from training and carried out seed gene selection, gene module construction and classifier fitting by the remaining  <dig> subsets. we then predicted the class label of the samples in the omitted subset based on the constructed classification rule. we repeated this process  <dig> times so that each sample is predicted exactly once. we determined the classification error rate as the proportion of the number of incorrectly predicted samples to the total number of samples in a given study. this 10-fold cross-validation procedure was repeated  <dig> times and the averaged error rate was reported.

RESULTS
tables  <dig>   <dig> and  <dig> list the prediction performances of different classification methods applied to microarray gene expression data sets for colon, prostate and lung cancers, respectively. here the different number of top seed genes  was selected by t-test. since it is generally time-consuming to search for genes which are not only correlated with a given seed gene but maximize their averaged test statistic value , in order to save time, we only tested  <dig> cutoffs of correlation r from  <dig>  to  <dig>  with interval  <dig> . we observed that the averaged correlation of genes in the identified modules is usually between  <dig>  and  <dig>  with the number of genes in the modules from  <dig> to  <dig>  suggesting that the genes in the modules are highly co-expressed.

as we can see, the proposed mlda has relatively better or comparable classification performances among all being compared classification methods in the three data sets. the performance of mpclr is not consistent in the three data sets. this is likely that the variation in the given data captured by the first pc may be different. other methods with better classification performances are dlda and svm. in general, all these methods except tree works well for both colon and lung cancer data sets. the performances of these methods in prostate cancer data are slightly worse than those in colon and lung cancer data sets, which may be due to clinical heterogeneity among samples.

we also used sam to select seed genes and evaluated their prediction performance using the same procedure as described above. similar prediction results are observed as shown in table  <dig>  overall, the mlda has slightly lower error rate than other being compared classification methods .

in many cases, we found that the simple method dlda works well. its performance is comparable with the advanced methods, such as svm. we also observed that the performances of predictors with more genes are not necessarily better than those of the predictors with fewer genes. for example, when t-test was used to select the seed genes, the best performance was obtained with only  <dig> genes for mpclr and mlda predictors in colon cancer data set ,  <dig> genes for svm predictor in prostate cancer data set  and  <dig> genes for mlda predictor in lung cancer data set . when sam was used to select the seed genes, the best performance was also obtained with  <dig> genes for svm, mpclr and mlda predictors in lung cancer data set .

discussion and 
CONCLUSIONS
in this study we developed a network modular-based approach for disease classification using microarray gene expression data. the core idea of the methods is to incorporate 'essential' correlation structure among genes into a supervised classification procedure, which has been neglected or inefficiently applied in many benchmark classifiers. our method takes into account the fact that genes act in networks and the modules identified from the networks act as the features in constructing a classifier. the rationale is that we usually expect tightly co-expressed genes to have a meaningful biological explanation. for example, if gene a and gene b has high correlation, which sometimes hints that the two genes belong to the same pathway or functional module. the advantage of the method over other methods has been demonstrated by three real data sets. our results show that the algorithm mlda works well for small sample size classification. it performs relatively better than dlda, 1nn, svm and other classifiers in many situations. the modular lda approach induced in the study have the potential to increase the power of discriminant analysis for which sample sizes are small and there are large number of genes in the microarray studies.

our results are consistent with previous findings: the simple methods have comparable or better classification results than the more advanced or complicated methods  <cit> . this is likely due to the fact that there are more parameters to be estimated in the advanced methods than in the simple methods, while our data sets usually have much smaller number of samples than features/genes. we also tried to use more top genes  in the classification models and similar result patterns  were observed as shown in tables  <dig>   <dig>   <dig>   <dig>  although some previous studies showed that better results can be obtained when the number of top genes used in the prediction models are much larger than the number of samples, the improved performance may be due to over fitting effect. moreover, for clinical purpose, it is better to include fewer number of genes rather than larger number of genes in the prediction models due to cost issues.

previous studies have shown that the topological structure of a node  in a protein network is informative for functional module inference  <cit> . moreover, some useful approaches have been developed to measure the topology similarity of pairs of nodes in weighted networks  <cit> . it will be interesting to explore the network topology-sharing based method rather than the correlation-sharing approach to identify seed-based gene network modules and place them into our network-based classification framework. the mlda framework can be further extended in many ways. for example, it is possible to directly incorporate the modular-specific features in other advanced discriminant learning approaches . in the future we will explore these ideas in details.

list of abbreviations
dlda: diagonal linear discriminant analysis; dqda: diagonal quadratic linear discriminant analysis; knn: k nearest neighbor; lda: linear discriminant analysis; lr: logistic regression; mlda: modular-based linear discriminant analysis; mpclr: modular-principal component based logistic regression; pc: principal component; rma: robust multi-array average; sam: significance of microarrays; svd: singular value decomposition; svm: support vector machines.

competing interests
the authors declare that they have no competing interests.

authors' contributions
ph designed and performed the analysis and wrote the manuscript. ph, sb and hj designed the algorithms.

