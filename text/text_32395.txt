BACKGROUND
the notion of distance and similarity between two strings is a very important and widely studied one  <cit>  since it plays a fundamental role in biological sequence analysis, phylogeny and classification. classically, those notions hinge on sequence alignment methods. however, distance and similarity functions based on alignment methods do not scale well with data set size and they are no longer perceived as adequate now that entire genomes are available  <cit> . moreover, they are not flexible, since they can only be used with genomic and proteomic sequences. therefore, novel alignment-free functions are actively pursued, the ones based on textual compression being natural candidates because of the deep connection of compression with classification and modeling  <cit> .

in this scenario, li et al.  <cit>  have devised a universal similarity metric for strings-a remarkable achievement since universality means it is a lower bound for all the computable distance and similarity functions, including all the ones so far considered for biological applications, e.g.,  <cit> . unfortunately, being the measure based on kolmogorov complexity  <cit> , it is not computable. however, one can still get a practical tool from such a beautiful theoretical finding since the kolmogorov complexity of a string can be approximated via data compression  <cit> . this leaves open the problem of how to best approximate the universal measure, which can be regarded more as a methodology than a formula quantifying how similar two objects are. three distinct functions have been proposed as an approximation to usm: ucd, ncd and cd. we point out that two of them have been slightly changed in this work, with respect to their first appearance in the literature, to make our study consistent. moreover, usm is a distance function  implying that its three approximations are dissimilarity functions. in turn, the discriminative abilities of ucd, ncd and cd depend critically on the compression algorithm one uses for their computation. ncd has been the object of deep theoretical studies in  <cit> , where experimental evidence of its validity has also been initially assessed. cd has been used for classification and data mining in  <cit>  and it was obtained independently in  <cit>  in the realm of table compression. ucd has been used in  <cit>  to classify protein structures. those studies, although groundbreaking, seem to be only an initial assessment of the power of the new methodology and leave open fundamental experimental questions that need to be addressed in order to establish how appropriate the use of the methodology is for classification of biological data. the main ones are the following, which should be addressed accross data sets of biological relevance and with the aid of well known statistical indexes to quantify performance:  how well does the methodology classify and, in particular, which of ucd, ncd and cd is the best performer;  which classification algorithm performs best when using the methodology;  how does the classification ability of the formulas depend on the choice of compression programs, i.e., the experiments conducted so far exclude weak compression programs such as memoryless ones because they are likely to give bad results, yet they are very fast to be outright dismissed;  how does the methodology compare with existing methods, both based on alignments and not, i.e., whether it is worthy of consideration simply because it scales well with data set size and it can work on data other than genomic or proteomic sequences, or because it is also competitive even on data sets that could be analyzed with alternative methods, by now standard.

we provide two sets of experiments designed to shed light on the mentioned problems, contributing the first substantial experimental assessment of usm, of its possible uses in molecular biology and naturally complementing both the theory and the initial experimental work done so far to sustain usm.

RESULTS
experimental setup
several benchmark data sets of non-homologous protein structures have been developed in the last few years  <cit> . in this study, we have chosen the  <dig> protein domains of  <cit>  and the  <dig> prototype protein domains of  <cit> . the chew-kedem data set, which consists of  <dig> protein domains drawn from pdb entries of three classes , was introduced in  <cit>  and further studied in  <cit> . the sierk-pearson data set, which consists of a non-redundant subset of  <dig> protein families and  <dig> non-homologous protein families from the cath protein domain database  <cit> , was introduced in  <cit> .

for both the chew-kedem and the sierk-pearson data sets, we have considered several alternative representations. besides the standard representation of amino acid sequences in fasta format  <cit> , we have also used a text file consisting of the atom lines in the pdb entry for the protein domain, the topological description of the protein domain as a tops string of secondary structure elements  <cit> , and the complete tops string with the contact map. the tops model is based on secondary structure elements derived using dssp  <cit> , plus the chirality algorithm of  <cit> . for instance, the various representations of pdb protein domain 1hlm <dig>  a globin from the sea cucumber caudina arenicola  <cit> , are illustrated in figure  <dig> 

we have also included in this study a benchmark data set of  <dig> complete unaligned mitochondrial genomes, referred to as the apostolico data set  <cit> .

in summary, the six data sets with the acronyms used in this paper are as follows:

aa-15-dna: apostolico data set of  <dig> species, mitochondrial dna complete genomes.

ck-36-pdb: chew-kedem data set of  <dig> protein domains, amino acid sequences in fasta format.

ck-36-rel: chew-kedem data set of  <dig> protein domains, complete tops strings with contact map.

ck-36-seq: chew-kedem data set of  <dig> protein domains, tops strings of secondary structure elements.

sp-86-atom: sierk-pearson data set of  <dig> protein domains, atom lines from pdb entries.

sp-86-pdb sierk-pearson data set of  <dig> protein domains, amino acid sequences.

we considered twenty different compression algorithms and, for some of them, we tested up to three variants. the choice of the compression algorithms reflects quite well the spectrum of data compressors available today, as outlined in the methods section. finally, dissimilarity matrices, both corresponding to the usm methodology and to other well established techniques, were computed as described in the methods section.

intrinsic classification abilities: the roc analysis
this set of experiments aims at establishing the intrinsic classification ability of the dissimilarity matrices obtained via each data compressor and each of ucd, ncd and cd. in order to measure how well each dissimilarity matrix separates the objects in the data set at the level of cath class, architecture, and topology, we have taken the similarity of two protein domains as the score of a binary classifier putting them into the same class, architecture, or topology, as follows.

we first converted each of the  <dig> ×  <dig> dissimilarity matrices  and each of the  <dig> ×  <dig> dissimilarity matrices  to similarity vectors of length  <dig>  and  <dig> , respectively, and used each of these vectors, in turn, as predictions. also, for each classification task, we obtained a corresponding symmetric matrix with entries  <dig> if the two protein domains belong to the same cath class, architecture, or topology  and  <dig> otherwise, and we converted these matrices to vectors of length  <dig>  and  <dig> , respectively, and used them as class labels. we have performed the roc analysis using the rocr package  <cit> . the relevant features about roc analysis are provided in the methods section.

the result of these experiments is a total of  <dig> ×  <dig> ×  <dig> ×  <dig> =  <dig>   <dig> auc  values, one for each of the five alternative representations of the ck- <dig> and sp- <dig> data sets, three dissimilarity measures, three classification tasks, and  <dig> compression algorithms, together with  <dig> ×  <dig> ×  <dig> =  <dig> roc plots, which are summarized in figures  <dig>   <dig>   <dig>   <dig>   <dig> 

the following conclusions can be drawn from the experiments, with reference to the open questions posed in the background section:

1) question . ucd and ncd are virtually indistinguishable and have an auc value at least as good as that of cd in most cases. that is, once we have fixed the data set, classification task, and compressor, the value of the auc index is quite close for ucd and ncd and, in most cases, better than cd. in terms of the disciminative power of ucd, ncd and cd, the top performing compression algorithms, e.g., ppmd, achieve an auc value ranging from a minimum of  <dig>  to a maximum of  <dig>  for the various classification tasks on the various representations of the ck- <dig> proteins. these are excellent values, given that a perfect classification has an auc score of  <dig>  for the sk- <dig> data set, all measures performed poorly on all classification tasks. since neither the alignment methods nor the alignment-free methods did better, this is an indication that the data set is hard to classify.

2) question  the ppmd compressors are consistently at the top of the auc index. moreover, gzip provides in general a comparable performance: a maximum 8% difference in auc values, although in most cases the difference is much smaller and in one case gzip is better. on the other hand, bzip <dig> is lagging behind: a 17% maximum difference in auc values. the difference between "memoryless" and "with memory" compressors is rather subtle. as already said for the sk- <dig> data set, all measures did poorly, across compressors, classification tasks and data representation. for the ck- <dig> data set, "with memory" compressors had a noticeable gain in performance, across classification tasks, only on ck-36-pdb: a 15% maximum difference in auc values. as for rel and seq, the difference in performance is a maximum 7% difference in auc values, although most compressors are quite close to the maximum auc values.

3) question  here we consider the maximum auc value given by the existing methods  versus the maximum auc value given by the compression based measures. the difference is in favor of the former methods by 2% on architecture, 4% on class and 9% on topology, on the ck-36-pdb data set. on the sk-86-pdb data set, none of the methods performed very well, i.e., all auc values were below  <dig> . this seems to indicate that the main advantage of the usm methodology is its scalability with data set size.

classification via algorithms: upgma and nj
this set of experiments aims at establishing how well the dissimilarity matrices computed via ucd, ncd and cd, with different compressors, can be used for classification by two well known and widely available classification algorithms. we have chosen upgma  <cit>  and nj  <cit> , two classic tree construction algorithms, as implemented in bioperl  <cit> . in relation to the clustering literature  <cit> , both can be considered as hierarchical methods. in fact, upgma is also known as average link hierarchical clustering. nj does not seem to have a counterpart in the clustering literature, although it certainly belongs to the hierarchical clustering algorithmic paradigm.

in order to assess the performance of compression-based classification, via upgma and nj under various compression algorithms, we have computed two external measures  <cit> , the f-measure and the partition distance, against a gold standard. the relevant features of those two measures is presented in the methods section.

for the classification of protein domains, we have taken as the gold standard the cath classification  <cit> , although we might have adopted the scop classification  <cit>  instead and, as a matter of fact, there are ongoing efforts to combine both classifications of protein domains  <cit> . in order to obtain a partitional clustering solution from the tree computed by upgma, we place in the same cluster all proteins that descend from the same level one ancestor in the upgma tree. then, we can compute the f-measure by using this clustering solution and the gold standard division of proteins in groups according to cath class. the same procedure is used for nj.

on the other hand, the classification of species we have taken as the gold standard is the ncbi taxonomy  <cit> . in this case, we can simply compare the two trees computed by upgma and nj against the ncbi taxonomy, via the partition distance. again, we might have adopted any other classification of species instead such as, for instance, the global phylogeny of fully sequenced organisms of  <cit> .

we first computed for each compression algorithm the ucd, ncd and cd dissimilarity measures over all pairs of elements in the data set, then obtained classification trees using upgma and nj and finally, computed the f-measure between the clustering solution obtained from those classification trees and the corresponding gold standard, for the first five data sets. we used the partition distance for the last one, as mentioned already. we followed an analogous procedure in order to obtain results via the non-compressive methods, the only difference being the computation of the dissimilarity matrix. tables  <dig>   <dig>   <dig>   <dig>   <dig>   <dig> report the results on the six different data sets for the compression based measures, whereas table  <dig> summarizes the ones obtained using the standard methods.

experimental results for the ck-36-pdb data set, with the ucd , ncd , and cd  distance. for each compression algorithm, we report the f-measure for both upgma and nj methods. the f-measure is a value ranging from  <dig> for highest dissimilarity to  <dig> for identical classifications.

experimental results for the ck-36-rel data set, with the ucd , ncd , and cd  distance. for each compression algorithm, we report the f-measure for both upgma and nj methods.

experimental results for the ck-36-seq data set, with the ucd , ncd , and cd  distance. for each compression algorithm, we report the f-measure for both upgma and nj methods.

experimental results for the sp-86-pdb data set, with the ucd , ncd , and cd  distance. for each compression algorithm, we report the f-measure for both upgma and nj methods.

experimental results for the sp-86-atom data set, with the ucd , ncd , and cd  distance. for each compression algorithm, we report the f-measure for both upgma and nj methods.

experimental results for the aa-15-dna data set, with the ucd , ncd , and cd  distance. for each compression algorithm, we report the partition distance for both upgma and nj methods. since the data set contains  <dig> species, the partition distance ranges from  <dig> to  <dig> in this case.

experimental results for the ck-36-pdb and the sp-86-pdb data sets, with different alternative methods to build distance matrices, that is, global and local alignments as well as k-tuple statistics and correlations. for each distance matrix associated to a method, we report the f-measure for both upgma and nj methods.

the following conclusions can be drawn from the experiments, with reference to the open questions posed in the background section:

1) question . in agreement with the roc analysis, ucd and ncd are virtually indistinguishable. that is, once we have fixed the data set and compressor, the value of the f-measure or partition distance is quite close for ucd and ncd and in most cases better than cd.

2) question  upgma seems to take better advantage of the usm methodology with respect to nj. indeed, given a compressor, the values of the f-measure obtained via upgma is in most cases better than those obtained with nj, by as much as  <dig> %, on the different representations of the ck- <dig> data set. on the sk- <dig> data sets, all compressors and classification algorithms did poorly. analogous poor performance was obtained with the use of standard techniques, again indicating  that sk- <dig> proteins may be difficult to classify. on the aa-15-dna data set, once the compressor is fixed, the difference in value of the partition distance is very limited  in most cases and large  in a few others. it is also worth pointing out that the best performing compression algorithms, with upgma and nj, reach excellent values of the f-measure and partition distance  on the ck- <dig> data sets and on the aa-15-dna data set.

3) question . the results of the roc analysis are largely confirmed on the ck- <dig> and sk- <dig> data sets. in particular, the difference in performance, as measured by the f-measure, between "with memory" compressors and memoryless ones is of some significance only on the ck-36-pdb data set. as for the aa-15-dna data set, there are a few things worth noting. gencompress, the best performing compression algorithm for dna sequences, is the best performer on that data set together with ppmd. however, ppmd is  <dig> times faster in compression speed  and twice as fast in decompression speed. moreover, the difference between "with memory" and memoryless algorithms is substantial and there is, in fact, a clear separation in terms of the partition distance values for those two classes of algorithms. so, those results, together with the ones on the other data sets, indicate that "with memory" compressors are substantially better than memoryless compressors only on data sets where there is enough "structure", a property not shared by many data sets of biological relevance.

average compression ratio, in bits per symbol, of the tested algorithms for the six data sets.

4) question . the results of the auc analysis are largely confirmed here, again showing that the usm methodology has the same performance and limitations as more standard methods. in particular, ppmd together with ucd and upgma gives a value of the f-measure within 2% of the value obtained with standard methods. again, this seems to indicate that the main advantage of the usm methodology with respect to existing ones is its scalability with data set size.

compression performance
for completeness, we also report compression ratios on all data sets  as well as compression and decompression times on some relevant data sets .

average compression  and decompression  speed, in μ-seconds per input symbol, for the sp-86-atom and aa-15-dna data sets.

CONCLUSIONS
prior to this research, the usm methodology was perceived as adequate for analysis of biological data mainly because of its flexibility and scalability with data set size. in particular, it would be applicable to any biological data in textual format. that is, it would work well on data sets not necessarily consisting of genomic or proteomic sequences and even with large data sets. moreover, only the best compression algorithms were recommended for use with the methodology, based on the intuitively appealing explanation that the better the compression guaranteed by a program, the better classification it would guarantee when used with usm. as the results in table  <dig> show, memoryless compressors can guarantee compression results comparable to the ones of "with memory" compressors on biological data, even if they are, in general, bad compressors. so, they cannot be dismissed based only on that intuition, even more so since they are fast and use very little main memory. our study adds to the state-of-the-art the following methodological conclusions and a recipe to use usm on biological data sets.

the usm methodology is worth using, even on data sets of size small enough to be processed by standard methods, including the ones based on alignments. it has the same advantages and limitations of the standard methods, i.e., data sets that can be classified well and others that are difficult to classify. given that no similarity or dissimilarity measure is likely to be general enough to handle well all data sets, the usm methodology is a valid alternative to existing techniques. moreover, because of their speed  and low memory requirements, memoryless compressors should be dismissed as not worthy only when the data sets have enough structure-something that should be evaluated using domain knowledge before applying the methodology to a data set.

in general, one of ucd or ncd should be used, in conjunction with upgma. as for compression algorithms, when very little is known about whether the data set has structure or not, ppmd and gencompress are the algorithms likely to give the best results. when speed is important, gzip is a valid alternative to both of them.

