BACKGROUND
scholarly internet resources play an increasingly important role in modern research. we can see this by the increasing number of urls published in a paper's title or abstract  <cit> . until now, maintaining the availability of scientific contributions has been decentralized, mature and effective, utilizing methods developed over centuries to archive the books and journals in which they were communicated. as the internet is still a relatively new medium for communicating scientific thought, the community is still figuring out how best to use it in a way that preserves contributions for years to come. one problem is that continued availability of these online resources is at the mercy of the organizations or individuals that host them. many disappear after publication , leading to a well-documented phenomenon referred to as link rot or link decay.

the problem has been documented in several subject areas, with table  <dig> containing a large list of these subject-specific studies. in terms of wide, cross-disciplinary analyses, the closest thus far are those of the biological and medical medline and pubmed databases by ducut  <cit>  and wren  <cit> , in addition to yang's study of the social sciences within the chinese social sciences citation index   <cit> .

* denotes studies most similar to the current.

some solutions have been proposed which attack the problem from different angles. the internet archive   <cit>  and webcite   <cit>  address the issue by archiving web pages, though their mechanisms for acquiring those pages differ. the ia, beginning from a partnership with the alexa search engine, employs an algorithm that crawls the internet at large, storing snapshots of pages it encounters along the way. in contrast, webcite archives only those pages which are submitted to it, and it is geared toward the scientific community. these two methods, however, can only capture information that is visible from the client. logic and data housed on the server are not frequently available.

other tools, like the digital object identifier  system  <cit>  and persistent uniform resource locator   <cit> , provide solutions for when a web resource is moved to a different url but is still available. the doi system was created by an international consortium of organizations wishing to assign unique identifiers to items such as movies, television shows, books, journal articles, web sites and data sets. it encompasses several thousand "naming authorities" organized under a few "registration agencies" that have a lot of flexibility in their business models <cit> . perhaps 30-60% of link rot could be solved using dois and purls <cit> . however they are not without pitfalls. one is that a researcher or company could stop caring about a particular tool for various reasons and thus not be interested in updating its permanent identifier. another is that the one wanting the permanent url  is frequently not the same as the person administering the site itself over the long term, thus we have an imbalance of desire vs. responsibilities between the two parties. a third in the case of the doi system is that there may be a cost in terms of money and time associated with registering their organization that could be prohibitive to authors that don't already have access to a naming authority <cit> . one example of a doi system business model would be that of the california digital library's ezid service, which charges a flat rate  for up to  <dig> million dois per year <cit> .

in this study, we ask two questions: what are the problem's characteristics in scientific literature as a whole and how is it being addressed? to assess progress in combating the problem, we evaluate the effectiveness of the two most prevalent preservation engines: and examine the effectiveness of one prototyped solution. if a url is published in the abstract, it is assumed that the url plays a prominent role within that paper, similar to the rationale proposed by wren  <cit> .

RESULTS
our goals are to provide some metrics that are useful in understanding the problem of link decay in a cross-disciplinary fashion and to examine the effectiveness of the existing archival methods while proposing some incremental improvements. to accomplish these tasks, we downloaded  <dig>  web of science  abstracts containing "http" in the title or abstract from the years under study , out of which  <dig>  urls  were extracted and used. we developed python scripts to access these urls over a 30-day period. for the period studied, 69% of the published urls  were available on the live internet, the internet archive's wayback machine had archived 62%  of the total and webcite had 21% . overall, 65% of all urls  were available from one of the two surveyed archival engines. figure  <dig> contains a breakdown by year for availability on the live web as well as through the combined archives, and figure  <dig> illustrates each archival engine's coverage. the median lifetime for published urls was found to be  <dig>  years , with the median lifetime amongst unique urls also being  <dig>  years . subject-specific lifetimes may be found in table  <dig>  using a simple linear model, the chances that a url published in a particular year is still available goes down by  <dig> % for each year added to its age with an r <dig> of  <dig> . its chances of being archived go up after an initial period of flux . submitting our list of unarchived but living urls to the archival engines showed dramatic promise, increasing the internet archive's coverage of the dataset by  <dig> urls, an increase of 22%, and webcite's by  <dig>  an increase of 255%.

subjects are assigned to journals and not specific papers. note that in these models, a given url could contribute to multiple subjects due to appearing in multiple journals which could also have multiple subject areas. where possible, specific subjects were generalized . median survival estimated using r's survfit(). "na" indicates that an upper 95% limit was unable to be computed.

how common are published, scholarly online resources? for wos, both the percentage of published items which contained a url as well as their absolute number increased steadily since  <dig> as seen in figure  <dig>  simple linear fits showed the former's annual increase at a conservative  <dig>  % per year with an r <dig> of  <dig>  while the latter's increase was  <dig> papers with an r <dig> of  <dig> .

a total of  <dig>  doi urls were identified, consisting of 1% of the total, while  <dig> purls  were identified. due to cost <cit> , it is likely that dois will remain useful for tracking commercially published content though not the scholarly online items independent of those publishers.

url survival
in order to shed some light on the underlying phenomena of link rot, a survival regression model was fitted with data from the unique urls. this model, shown in table  <dig>  identified  <dig> top-level domains, the number of times a url has been published, a url's directory structure depth , the number of times the publishing article has been cited, whether articles contain funding text as well as  <dig> journals as having a significant impact on a url's lifetime at the p<  <dig>  level. this survival regression used the logistic distribution and is interpreted similarly to logistic models. to determine the predicted outcome for a particular url, one takes the intercept  and adds to it the coefficients for the individual predictors if those predictors are different from the base level; coefficients here are given in years. if numeric, one first multiplies before adding. the result is then interpreted as the location of the peak of a bell curve for the expected lifetime, instead of a log odds ratio as a regular logistic model would give. among the two categorical predictors , the three having the largest positive impact on lifetimes were the journal zoological studies  and the top-level domains org and dk . though smaller in magnitude than the positive ones, the  <dig> categorical predictors having the largest negative impact were the journals computer physics communications  and bioinformatics  as well as the domain kr , though the p values associated with the latter two are more marginal than some of the others .

positive numbers indicate longer median lifetimes. much like a logistic model, coefficients can be added to the intercept value  to obtain a median lifetime. for example, the median expected lifetime for a url published once, with depth  <dig>  whose publishing article had  <dig> citation, no funding text, domain au and published in a journal not listed  would be:   <dig>  + log2* <dig>  + 0*- <dig>  + log2* <dig>  + 0* <dig>  +  <dig>  =  <dig> years

predictors of availability
while examining url survival and archival, it is not only interesting to ask which factors significantly correlate with a url lasting but also which account for most of the differences. to that end, we fit logistic models for each of the measured outcomes  to help tease out that information. to enhance comparability, a similar list of predictors  without interaction terms was employed for all  <dig> methods and unique deviance calculated by dropping each term from the model and measuring the change in residual deviance. results were then expressed as a percentage of the total uniquely explained deviance and are graphically shown in figure  <dig> 

for live web availability, the most deviance was explained by the last year a url was published  followed by the domain . that these two predictors are very important agrees with much of the published literature thus far. for the internet archive, by far the most important predictor was the url depth at 45%. based on this, it stands to reason that the internet archive either prefers more popular urls which happen to be at lower depths or employs an algorithm that prioritizes breadth over depth. similar to the ia, wc had a single predictor that accounted for much of the explained deviance, with the publishing journal representing 49% of the explained deviance. this may reflect wc's efforts to work with publishers as the model shows one of the announced early adopters, biomed central  <cit> , as having the two measured journals  with the highest retention rates. therefore, wc is biased towards a publication's source .

archive site performance
another way to measure the effectiveness of the current solutions to link decay is to look at the number of "saved" urls, or those missing ones that are available through archival engines. out of the 31% of urls  which were not accessible on the live web, 49% of them  were available in one of the two engines, with ia having 47%  and wc having 7% . wc's comparatively lower performance can likely be attributed to a combination of its requirement for human interaction and its still-growing adoption.

in order to address the discrepancy, all sites that were still active but not archived were submitted to the engine from which they were missing. using the information gleaned from probing the sites as well as the archives, urls missing from one or both of the archives, yet still alive, were submitted programmatically. this included submitting  <dig>  to the wayback machine as well as  <dig>  to webcite, of which  <dig>  and  <dig>  were successful, respectively.

discussion
submission of missing urls to archives
archiving missing urls in each of the archival engines had their own special nuances. for the internet archive, the lack of a practical documented way of submitting urls  necessitated trusting a message shown by the wayback machine when one finds a url that isn't archived and clicks the "latest" button. in this instance, the user is sent to the url "http://liveweb.archive.org/<url>" which has a banner proclaiming that the page "will become part of the permanent archive in the next few months". interestingly, as witnessed by requests for a web page hosted on a server for which the authors could monitor the logs, only those items requested by the client were downloaded. this meant that if only a page's text were fetched, supporting items such as images and css files would not be archived. to archive the supporting items and avoid duplicating work, wget's "--page-requisites" option was used instead of a custom parser.

webcite has an easy-to-use api for submitting urls, though limitations during the submission of our dataset presented some issues. the biggest issue was webcite's abuse detection process, which would flag the robot after it had made a certain number of requests. to account for this and be generally nice users, we added logic to ensure a minimum delay between archival requests submitted to both the ia and wc. exponential delay logic was implemented for wc when encountering general timeouts, other failures  or the abuse logic. eventually, we learned that certain urls would cause wc's crawler to timeout indefinitely, requiring the implementation of a maximum retry count  if the error wasn't caused by the abuse logic.

to estimate what impact we had on the archives' coverage of the study urls, we compared a url survey done directly prior to our submission process to one done afterwards; a period of about  <dig>  months. it was assumed that the contribution due to unrelated processes would not be very large given that there was only a modest increase in coverage, 5% for ia and 1% for wc, over the previous period of just under a year and a half.

each of the two archival engines had interesting behaviors which required gauging successful submission of a url by whether it was archived as of a subsequent survey rather than using the statuses returned by the engines. for the internet archive, it was discovered that an error didn't always indicate failure, as there were  <dig> urls for which wget returned an error but which were successfully archived. conversely, webcite returned an asynchronous status, such that even in the case of a successful return the url might fail archival; the case in  <dig> out of a total of  <dig> .

submitting the  <dig> urls to ia took a little less than a day, whereas submitting  <dig> to wc took over  <dig> months. this likely reflects ia's large server capacity, funding and platform maturity due to its age.

generating the list of unique urls
converting some of the potential predictors from the list of published urls to the list of unique urls presented some unique issues. in particular, while converting those based on the url itself  were straightforward, those which depended upon a publishing article  were estimated by collating the data from each publishing. only a small amount, 8%, of the unique urls, appeared more than once, and among the measured variables that pertained to the publishing there was not a large amount of variety. amongst repeatedly-published urls, 43% appeared in only one journal and the presence of funding text was the same 76% of the time. for calculating the number of times a paper was published, multiple appearances of a url within a given title/abstract were counted as one. thus, while efforts were made to provide a representative collated value where appropriate, it's expected that different methods would not have produced significantly different results.

additional sources of error
even though wos's index appears to have better quality optical character recognition  than pubmed, it still has ocr artifacts. to compensate for this, the url extraction script tried to use some heuristics to detect the most common sources of error and correct them. some of the biggest sources of error were: randomly inserted spaces in urls, "similar to" being substituted for the tilde character, periods being replaced with commas and extra punctuation being appended to the url .

likely the largest contributors to false negatives are errors in ocr and the attempts to compensate for them. in assessing the effectiveness of our submissions to ia, it is possible that the estimate could be understated due to urls that had been submitted but not yet made available within the wayback machine.

dynamic websites with interactive content, if only present via an archiving engine, would be a source of false positives, as the person accessing the resource would presumably want to use it as opposed to viewing the design work of its landing page. if a published web site goes away and another installed in its place , then the program will not be able to tell the difference since it will see a valid  web site. in addition, though page contents can change and lose relevance from their original use <cit> , dates of archival were not compared to the publication date.

another source of false positive error would be uncaught ocr artifacts that insert spaces within urls if it truncated the path but left the correct host intact. the result would be a higher probability that the url would appear as a higher level index page, which are generally more likely to function than pages at lower levels  <cit> .

bibliographic database
web of science was chosen because, compared to pubmed, it was more cross-sectional and had better ocr quality based on a small sampling. many of the other evaluation criteria were similar between pubmed and wos, as both contain scholarly work and have an interface to download bibliographic data. interestingly, due to the continued presence of ocr issues in newer articles, it appears that bibliographic information for some journals is not yet passed electronically.

CONCLUSIONS
based on the data gathered in this and other studies, it is apparent that there is still a problem with irretrievable scholarly research on the internet. we found that roughly 50% of urls published  <dig> years prior to the survey  are still left standing. interesting is that the rate of decay for late-published urls  appears to be higher than that for the older ones, lending credence to what koehler suggested about eventual decay rate stabilization <cit> . survival rates for living urls published between  <dig> and  <dig>  inclusive, only vary by  <dig> %  and have poor linear fits , whereas years  <cit>  have linear slope  <dig>  and r <dig> . <dig>  indicating that the availability between years for older urls is much more stable whereas the availability for more recent online resources follow a linear trend with a predictable loss rate. overall, 84% of urls  were available in some manner: either via the web, ia or wc.

several remedies are available to address different aspects of the link decay problem. for data-based sites that can be archived properly with an engine such as the internet archive or webcite, one remedy is to submit the missing sites which are still alive to the archiving engines. based on the results of our prototype , this method was wildly successful, increasing ia's coverage of the study's urls by 22% and webcite's by 255%. journals could require authors to submit urls to both the internet archive and webcite, or alternatively programs similar to those employed in this study could be used to do it automatically. another way to increase archival would be for the owners of published sites to ease restrictions for archiving engines since  <dig>  of the published urls had archiving disabled via robots.txt according to the internet archive. amongst these, 16%  have already ceased being valid. while some sites may have good reason for blocking automated archivers , there may be others that could remove their restrictions entirely or provide an exception for preservation engines.

to address the control issue for redirection solutions  mentioned in the introduction, those who administer cited tools could begin to maintain and publish a permanent url on the web site itself. perhaps an even more radical step would be for either these existing tools or some new tool to take a wikipedia approach and allow end-users to update and search a database of permanent urls. considering the studies that have shown around at least 30% of dead urls to be locatable using web search engines  <cit> , such a peer-maintained system could be effective and efficient, though spam could be an issue if not properly addressed.

for dynamic websites, the current solutions are more technically involved, potentially expensive and less feasible. these include mirroring  and providing access to the source code, both of which require time and effort. once the source is acquired, it can sometimes take considerable expertise to make use of it as there may be complex libraries or framework configuration, local assumptions hard-coded into the software or it could be written for a different platform . the efforts to have reproducible research, where the underlying logic and data behind the results of a publication are made available to the greater community, have stated many of the same requirements as preserving dynamic websites  <cit> . innovation in this area could thus have multiple benefits beyond just the archival.

