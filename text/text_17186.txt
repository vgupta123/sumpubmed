BACKGROUND
protein complexes are basic cellular entities that carry out the functions of their components. those functions are performed in various biological processes in a cell, including transcription, signal transduction, and development. therefore, it is useful to identify protein complexes in order to elucidate various complicated mechanisms in a cell.

there exist a few databases of protein complexes of yeast. one of them is mips protein complex catalog
 <cit> , a comprehensive catalog of manually curated protein complexes of yeast. it contains  <dig> complexes, excluding complexes derived from high-throughput experimental data sets
 <cit> . an up-to-date database of curated protein complexes of yeast is cyc2008
 <cit> , which contains manually curated  <dig> heteromeric protein complexes.

the major type of protein complexes in these databases is heterodimeric complexes, that is, protein complexes composed of two different proteins. mips catalog and cyc <dig> have  <dig>  and  <dig>  heterodimeric complexes, respectively.

a number of unsupervised learning methods for predicting arbitrary types of protein complexes simultaneously have been proposed, for example, mcl
 <cit> , rrw
 <cit> , nwe
 <cit> , ppsampler
 <cit> , rnsc
 <cit> , mcode
 <cit> , dpclus
 <cit> , cmc
 <cit> , and coach
 <cit> . it is reported that they achieve good performance for protein complexes of size three or more . however, it can be found that those existing tools can not give high performance for heterodimeric complexes
 <cit> . the best known f-measure score is only  <dig> , which is achieved by ppsampler .

a fundamental reason of the drawback is that a score function for a predicted complex is often designed based on inter-connectivity between proteins within a predicted complex, like density measure of a subgraph of a protein-protein interaction  network. under such a scoring scheme, the number of proteins within a predicted cluster should be high to some degree, at least three or four, in order that such a score function works effectively. as a result, heterodimeric complexes are left out of consideration.

this weakness of existing tools toward heterodimeric protein complexes implies the need to develop another approach for predicting heterodimeric complexes. in this paper, we present a method for learning naïve bayes classifiers for heterodimeric complexes. those classifiers exploit the features for heterodimeric protein complexes which are designed with genome-wide data sets, including ppi data, gene ontology annotations, and protein localization data. those features are trained by positive examples, a part of known heterodimeric protein complexes, and negative examples, which will be pairs of two proteins that are not known to form heterodimeric protein complexes. such a feature can be considered to be essentially a pair of conditional probability distributions of possible values of features. one is a distribution of feature values specialized for positive examples, and the other is specialized for negative examples. the log-likelihood ratio by the naïve bayes classifier with those trained features can be used to score a given pair of two proteins and predict whether the pair forms a heterodimeric protein complexes or not.

it should be noted here that the problem of identification of two components of heterodimeric protein complexes is different from that of identification of ppis. in the latter problem, it does not matter whether an interaction forms a protein complex with interactions neighboring with that interaction. on the other hand, in the former problem, it is required to determine whether an interaction solely forms a protein complex or not.

we have carried out a five-fold cross-validation over the positive and negative examples derived from known heterodimeric complexes in cyc <dig> and known ppis in wi-phi, which is a database of ppis of yeast. this computational experiment shows acceptable performance and gives us interesting insights into heterodimeric protein complexes. furthermore, the trained classifiers are evaluated by predicting whether each of the ppis in wi-phi is heterodimeric or not. it then turns out that those classifiers show higher predictability than various existing algorithms on yeast data sets with the exact matching criterion. similar results are also obtained with an approximate matching criterion.

methods
in this section, we describe our method for predicting heterodimeric protein complexes by naïve bayes classifiers. for this task, we have designed several features for heterodimeric protein complexes based on a weighted ppi network, semantic similarities for molecular function and biological process aspects of the gene ontology, and protein localization information. here a feature means a  function mapping from a pair of proteins to a real number or an integer, which is called a score.

given a set of features, a five-fold cross-validation is carried out, in which classifiers are trained with training sets of positive and negative examples and those trained classifiers are evaluated with test sets of positive and negative examples . the trained classifiers are then used to predict whether each of known ppis form a heterodimeric protein complex or not , and the resulting performance is compared with those of other methods.

in the subsequent subsections, we introduce templates for features as well as individual features for a heterodimeric protein complex, and describe details of other parts of our methods.

design of features for heterodimeric protein complexes
we here design several features for heterodimeric protein complexes, which will be exploited in a naïve bayes classifier.

in general, measures of internal connectivity for a subgraph, like density measure, are often used as a feature characterizing heteromeric protein complexes. for example, mcode
 <cit>  is designed based on the observation that densely connected subgraphs may represent known complexes. however, such measures do not work well for heterodimeric protein complexes because the possible states of internal connectivity of a pair of proteins is binary, i.e., connected or not. in general, density-based measures works better for larger complexes. therefore, we have designed features specialized for heterodimeric protein complexes, which are derived from ppis, gene ontology annotations, and protein localization data.

here we introduce three templates for features for heterodimeric protein complexes. let e be a pair of proteins. the combination of a template and a score function for e leads to a concrete feature. in this work, four score functions for e are formulated based on the following four genome-wide data sets, respectively:  ppi weights of wi-phi
 <cit> ,  proximity from a protein to another obtained by random walks with restarts on the ppi network derived from wi-phi, and  semantic similarity for biological process aspect of go, and  semantic similarity for molecular function aspect of go, respectively.

before describing those data sets, a ppi network is introduced as an underlying graph for features. let g= be an undirected graph representing a ppi network where a node is a protein and an edge corresponds to an interaction between the corresponding proteins. this graph is used as the underlying graph for features to be defined here.

for an edge, e, let 

 ne={e′∈e||e′∩e|=1}, 

representing the edges adjacent to either end point of e. this graph, g, is made from the wi-phi database in this work.

wi-phi
 <cit>  is a ppi database with  <dig> yeast proteins and  <dig> interactions. among them,  <dig> with  <dig> proteins are non-self interactions. each interaction has a weight, which is determined from various heterogeneous data sources, including results of tandem affinity purification coupled to ms , large-scale yeast two-hybrid studies, and small-scale experiments stored in dedicated databases. the higher the weight is the more reliable it is. the lowest and highest values are  <dig>  and  <dig> , respectively. if e is not included in wi-phi, the weight of e is set to be zero. hereafter, the weight of e is denoted by ppiweight.

the next score function of e is a proximity of the two nodes between e derived from a random walk with restarts
 <cit> . the output of a random walk with restarts at a node, u, gives the stationary probability from u to the other nodes, v, denoted by π, satisfying the following equation: 

 xu=axu+αbu 

where xu=π⋯π)t, bu is the vector whose elements are all  <dig> except the u-th element being  <dig>  a is the column-normalized transition matrix derived from g, and α is the restart probability of a random walk with restarts. here, α is set to be  <dig> , the default value of the restart probability of nwe
 <cit> . the different values of  <dig>  and  <dig>  are also applied, and very similar results are obtained in the cross-validation . since the stationary probabilities between u to v are not symmetric, namely, π≠π, we here define the symmetric proximity of a random walk with restarts between u and v as 

 randomwalkproximity=+π)/ <dig>  

this score function is also given to templates to generate concrete features.

the gene ontology project, or go, provides a controlled vocabulary to describe gene and gene product attributes in any organism and the go database is a comprehensive knowledge structures relating functions of genes and their products
 <cit> . although it is still on-going project, it has already been proved to be effective in the evaluation of human ppis
 <cit> . let x be an ontology among the two ontologies, biological process  and molecular function . here, cellular component is excluded because this ontology contains many terms representing subunits or memberships of protein complexes. yang et al. proposed a method for improving existing go semantic similarity measures in
 <cit> . in this work, the proposed measure based on lin  is adopted. we denote by semanticsim.x the semantic similarity score of e over x, which is also used as a score function by the templates. an ontology file shows go terms and their relationships. the file we used is dated nov.  <dig>   <dig>  and downloaded from http://www.geneontology.org/go.downloads.shtml. an annotation file contains associations between gene products and go terms. the file we used is compiled by saccharomyces genome database 
 <cit>  and dated nov.  <dig>   <dig> 

suppose that for a pair of proteins, e∈v <dig>  a score function, m, gives a real number, m, which will be one of the followings: ppiweight, randomwalkproximity, semanticsim.bp, or semanticsim.mf.

score-type feature
the first template is used to generate a score-type feature. the score-type feature with m, denoted by m.score, is a function to just return the score of m for e, i.e.,m. thus this is the most simplest feature. for example, the returned value is  <dig> for e in the graph in figure
 <dig>  this type features will work well if m itself is a good characterization of heterodimeric protein complexes.

difftomax-type feature
the second template provides a difftomax-type feature, which calculates the differences between m and the maximum of scores of the adjacent edges to e, i.e.,

 m.difftomax=m-maxe′∈nem. 

for this feature, it is expected that the higher the gap is, the likelier e corresponds to a heterodimeric complex. for the edge, e, in the graph in figure
 <dig>  we have m.difftomax= <dig> 

rank-type feature
the last template gives a rank-type feature, denoted by m.rank, which returns the number of the adjacent edges to e whose scores of m are greater than m, i.e.,

 m.rank=|{e′∈ne|m>m}|. 

for the edge, e, in the graph in figure
 <dig>  we have m.rank= <dig>  it can be expected that the higher the returned value to e is, the lower the likelihood that e is a heterodimeric protein complex.

in addition to the three feature templates, we have designed three individual features as follows.

feature based on protein localization
the next feature is designed based on the observation that two proteins should express in the same location if they interact with each other. huh et al. <cit>  classified 75% of the yeast proteins into  <dig> distinct subcellular localization categories by their gfp-tagged library. by exploiting this localization information, the feature, localization, is defined as follows: 

 localization=1if both proteins ofeshare at least one category at which they express,0if either protein of ecompletely has no categories at which it expresses,-1otherwise 

feature based on neighboring common nodes
existence of nodes neighboring to both nodes, u and v of e, is often used as an index to determine whether u and v belong to the same protein complex of size three or more . thus, the number of such nodes can be used as an inverse indicator of e being a heterodimeric protein complex. the set of those nodes is equal to nu∩nv, where nu={z∈v|{u,z}∈e}. thus, the feature, neighboringcommonnode, is defined as

 neighboringcommonnode=|nu∩nv|. 

feature based on neighboring edges
the feature, neighboringedge calculates the number of neighboring edges to e, i.e., 

 neighboringedge=|ne|. 

if e corresponds to a heterodimeric protein complex, this feature can be expected to return a low value.

positive and negative examples of heterodimeric protein complexes
positive and negative examples are required in supervised-learning processes. in the problem of heterodimeric protein complex prediction, those examples can be modeled as a pair of different proteins. in this work, a positive example is a pair of proteins satisfying the following conditions:  it corresponds to a heterodimeric protein complex in cyc <dig>   it is not a proper subset of any other complex in cyc <dig>  and  it corresponds to a ppi in wi-phi. this means that positive examples used in the learning process are highly reliable. the total number of the resulting positive examples is  <dig>  on the other hand, negative examples are randomly chosen from ppis in wi-phi except all ppis corresponding to heterodimeric protein complexes in cyc <dig>  the number of them is set to  <dig>  ten times that of positive examples. note that these positive and negative examples are used only in a five-fold cross-validation. after that process, the resulting classifiers are evaluated with all ppis in wi-phi.

it should be noted here that some true positive examples would be missed due to the incompleteness of used databases of protein complexes. in addition, some of the current negative examples can be false negative ones  due to the same reason. this kind of issues will be resolved by further accumulation and annotation of data.

discretization
maximum likelihood estimation is applied to learn two conditional distributions of each of specified features from a training data set. if a feature returns a real number, the range is discretized into  <dig> equal-width bins over the interval from the minimum to the maximum among the values of all the positive and negative training examples. if a feature returns an integer,  <dig> bins are also prepared. for example, in a rank-type feature, the first  <dig> bins correspond to the feature values,  <dig> ,⋯, <dig>  respectively, and the last one covers  <dig> and more. in both cases, the resulting distributions are multinomial distributions. a dirichlet prior is applied to avoid the probability being zero of a particular bin. the pseudocount of one is given to each of the bins in this study.

naïve bayes classifier
in order to predict heterodimeric protein complexes, we exploit a naïve bayes classifier, which is a simple probabilistic model based on bayes’ theorem. figure
 <dig> presents an overview framework of a naïve bayes classifier. let x <dig> x <dig> …,xm be random variables for m features, and c a random variable representing a class whose value is either  <dig>  or  <dig> . in a naïve bayes classifier, it is assumed that each feature xj is conditionally independent of every other feature xk for k≠j.

for a pair of proteins in a given ppi network, we can compute the conditional probability of how likely it represents a heterodimeric protein complex using the following equation. 

 p=ppp=p∏j=1mpp 

bayes’ rule is used in the first row of the above equations. the second equation utilizes the conditional independence assumption of the naïve bayes model to decompose the conditional joint probability to the probabilities of different features.

let s be a pair of proteins to be classified whose feature values are x <dig> x <dig> …,xm for x <dig> x <dig> …,xm, respectively. the log likelihood ratio  for s can be computed using the two posteriors, p and p as follows: 

 llr=logpp=logp∏j=1mpp∏j=1mp=logpp+∑j=1mlogpp 

in order to make two llrs with the different numbers of features comparable, the above llr is normalized by dividing by m+ <dig>  hereafter llr means the normalized llr.

in the learning process of naïve bayes classifiers, the ratio of
pp is set to be proportional to the ratio of the number of positive examples to the number of negative examples, which is
 <dig>  as a result, the class llr,
logpp=- <dig> . in the evaluation process of trained classifiers with all ppis in wi-phi, the ratio of
pp is also set to be proportional to the ratio of the number of positive examples to the number of negative examples, which is
15249607-152= <dig> . in this case, the class llr is set to be
logpp=- <dig> .

if the llr of s is greater than a specified threshold, s is predicted to be positive, and negative otherwise. the default value of the threshold is set to be  <dig>  in order to relatively reduce the number of false positives. later, we will see how much the value of the threshold affects the predictability. by varying the value of the threshold, a receiver operating characteristic  curve is obtained.

here is a remark on functional dependencies between features. it is reported in an empirical study of the naïve bayes classifier in
 <cit>  that even if some features are functionally dependent, naïve bayes often works well. thus, in this work, various sets of features which can be functionally dependent are embedded into the same naïve bayes classifier. actually, the best performance feature set we have obtained contains features derived from the same source data sets, which will be shown in the result section.

performance measures
there have been many unsupervised learning algorithms proposed to predict heteromeric protein complexes, not specialized for heterodimeric complexes. some of them can predict clusters of size two. thus, it would be useful to be able to compare performance of supervised and unsupervised learning algorithms w.r.t. heterodimeric protein complexes. to realize it, firstly, we formulate the three major measures, precision, recall, and f-measure in a general way, which is the same as in
 <cit> . after that, we will explain how to use them.

to formulate those measures, a matching criterion for two sets of proteins is needed. let s and t be sets of proteins with arbitrary sizes. the overlap ratio between s and t, denoted by ov, is defined as follows: 

 ov=|s∩t||s|·|t|if|s∩t|≥20otherwise. 

 we say that s and t are matched if ov is no less than a predefined threshold, η. if s and t share at least two proteins, the overlap ratio is equal to the ratio of the number of common proteins in s and t to the geometric mean of the sizes of s and t. thus, it is one if s and t are identical to each other. on the other hand, if s and t share less than two proteins, the overlap ratio is defined as zero. the reason why the overlap ratio is zero even if s and t share one protein is to avoid unfavorable situations when the value of η is set to be the typical value of  <dig>   in the literature. without that criterion, by randomly generating clusters of size two, known complexes of size two can be matched with some of the clusters by chance.

let c be a set of predicted clusters of proteins by an algorithm, and k a set of known complexes. we denote by npc the number of predicted clusters matched with at least one known complex, i.e., 

 npc=|{c|c∈c,∃k∈k,ov≥η}|, 

 and by nkc the number of known complexes matched with at least one predicted cluster, i.e., 

 nkc=|{k|k∈k,∃c∈c,ov≥η}|. 

we then define the precision of c to k with η as 

 precision=npc|c|. 

in a similar way, the recall of c to k with η is defined as 

 recall=nkc|k|. 

the f-measure of c to k with η is defined as the harmonic mean of the corresponding precision and recall. namely, we have 

 f=2·precision·recallprecision+recall. 

in this work, two different matching criteria, which are exact and approximate ones, respectively, are used to evaluate predicted clusters of size two. for the set of all clusters predicted by an algorithm, c, we denote by c| <dig> the subset of all clusters of size two in c. notice that c is equal to c| <dig> if c is generated by our classifiers. let k be the set of all known complexes in cyc <dig>  and k| <dig> the set of all known heterodimeric complexes in cyc <dig>  the precision and recall with the exact matching criterion for size two are given as precision and recall, respectively. note that all the clusters and complexes used in the measures are of size two. the precision and recall with the approximate matching criterion with η for size two are given as precision and recall, respectively, where η is set to be
η= <dig> , the typical value in the literature.

k-l divergence
the kullback-leibler  divergence of two trained conditional distributions of a feature can be used as a measure for indicating how discriminative the feature is. the k-l divergence is a measure of the difference between two probability distributions, and defined as follows
 <cit> :

 kl=∑iplog2pq. 

 note that in general the kullback-leibler divergence is not symmetric, namely kl≠kl. the symmetric and non-negative kullback-leibler divergence is defined as follows: 

 klsym=12kl+kl. 

 for a feature xi with two trained conditional distributions, p and p, the symmetric k-l divergence of xi is defined as 

 klsym||p). 

 hereafter the symmetric k-l divergence is simply called the k-l divergence.

RESULTS
finding the best feature set
in the first stage of a five-fold cross-validation, the k-l divergence of the  <dig> designed features is calculated. the mean with standard deviation of those values of each feature over the five folds is shown in figure
 <dig>  the following observations are obtained from figure
 <dig>  at first, the k-l divergences of the three features, localization, neighboringedge, and neighboringcommonnode are relatively considerably lower than the others. their k-l divergences are  <dig>  or less. on the other hand, those of the other features are about  <dig>  or more. as a result, the three features seem to be relatively less effective to predict heterodimeric protein complexes. thus, in the next stage, these three features are excluded. secondly, the highest k-l divergence of  <dig>  is given by ppiweight.rank. thus this feature is expected to be the most effective feature to discriminate heterodimeric protein complexes from the other complexes. lastly, the k-l divergence of a feature derived from a feature template is largely dependent on the score function embedded in the feature. when considering the k-l divergence averaged over the different templates with the same score function, those score functions are sorted as follows: ppiweight, semanticsim.bp, randomwalkproximity, and semanticsim.mf. thus, according to this order, the corresponding features are expected to contribute the discrimination of heterodimeric protein complexes from the others.

in the second stage of finding discriminative naïve bayes classifiers, an exhaustive search was executed in the following search space for feature sets. at first, any of the three low k-l divergence features are not included in all the feature sets in the search space. secondly, for each of the four score function for a pair of proteins, ppiweight, semanticsim.bp, randomwalkproximity, and semanticsim.mf, three concrete features, rank, score, and difftomax, are created. lastly, any feature set in the search space should include one or two features from the four concrete features derived from the same score function. thus, the total number of feature sets in the search space is  <dig> 

for each of the feature sets in the search space, a five-fold cross-validation was carried out. note that the predictability of trained classifiers are evaluated with five test sets of the cross-validation. in the next section, the feature set with the highest mean of f-measures on the five test sets is analyzed deeply.

analyses
the best feature set found in the previous section consists of the followings: ppiweight.score, ppiweight. difftomax, semanticsim.bp.rank, semanticsim.bp.diff tomax, randomwalkproximity.rank, randomwalkproximity.score, and semanticsim.mf.difftomax. it is interesting that ppiweight.rank, whose k-l divergence is the highest among the  <dig> features, is not included in the above feature set. instead of that, ppiweight.score and ppiweight.difftomax are contained. probably, the combination of them would work better than ppiweight.rank alone. the results on test sets in the five-fold cross-validation are shown in table
 <dig>  recall that the numbers of positive and negative examples are  <dig> and  <dig>  respectively. each of them is predicted by one of the five trained classifiers to be either positive or negative in the cross-validation. totally,  <dig>  of the  <dig> positive examples are correctly predicted to be positive. namely, they are true positive, and the  <dig> remaining positive examples are false negatives. on the other hand,  <dig>  of  <dig> negative examples are true negative and the other  <dig> ones are false positives.

this table shows the numbers of true positives , false negatives , true negatives , and false positives  for the test set of each fold. note that the number of positive examples is  <dig> in the first and second folds, and  <dig> in the other folds. the number of negative examples is  <dig> in each fold.

for each test set, the performance measures of precision, recall, and f-measure are calculated. their means with standard deviations are shown in figure
 <dig>  the precision, recall, and f-measure are averagely  <dig> ,  <dig> , and  <dig> , respectively. these scores are acceptable result. this feature set is also applied to several different sets of training and test, and quite similar results are obtained .

it is interesting to see how many classes predicted by the five trained classifiers are consistent for each of the  <dig> positive and  <dig> negative examples. table
 <dig> shows the frequency of positive and negative examples according to the number of correct predictions by the classifiers. it can be found that  <dig>  of the positive examples are consistently predicted correctly by the five trained classifiers. in the next section, two instances are examined. interestingly, there are many  <dig>  positive examples that are consistently predicted to be negative by the classifiers. one of them is also picked up and the reason for the misclassification is considered. notice that, totally,  <dig>  of the positive examples are predicted to be the same class by the five trained classifiers. furthermore,  <dig>  of the negative examples are consistently predicted to be negative by the classifiers, and  <dig>  negative examples are consistently predicted to be positive. totally,  <dig>  of the negative examples are consistently predicted by the classifiers. as a result, the five trained classifiers are fairly consistent to each other.

this table shows the frequency of positive and negative examples according to the number of correct predictions by the five trained classifiers. the first row shows the number of classifiers whose predicted classes are correct. the next rows, p and n, give the results on positive and negative examples, respectively.

in the subsequent sections, some of true positives, false negatives, and false positives, which are consistently predicted by the five trained classifiers, are analyzed further.

true positive
the first instance of true positives is the pair of two proteins, ubp3/yer151c and bre5/ynr051c. these proteins are known to interact with each other to co-regulates anterograde and retrograde transport between the endoplasmic reticulum and golgi compartments
 <cit> . there is no other known complexes including at least one of the two proteins in cyc <dig>  the scores of the features to this instance are given in the column of tp <dig> of table
 <dig>  the ppi weight of the pair is shown to be  <dig> , which is relatively high. this means that feature ppiweight.score gives a high score to the pair. in addition, ubp3/yer151c and bre5/ynr051c have  <dig> and  <dig> interactions in wi-phi, and none of the neighboring interactions to the two proteins, except the pair, has a ppi weight higher than that of the pair,  <dig> . this means that ppiweight.difftomax, randomwalkproximity.rank, and randomwalkproximity.score, can return high scores to this pair. actually, randomwalkproximity.rank marks the optimal score of  <dig> . in addition, this pair has statistically significant biological process go term, “ribophagy” with p-value  <dig> e- <dig>  indicating semanticsim.bp features return high scores to this pair. notice that semanticsim.bp.rank marks the optimal score of  <dig>  figure
 <dig>  shows the mean of llrs of each feature, xj, by the five trained classifiers.

this table shows the returned values of features to particular examples. the columns of tp <dig>  tp <dig>  and fn correspond to the pairs of ubp3/yer151c and bre5/ynr051c, ecm17/yjr137c and met10/yfr030w, and cdc28/ybr160w and cln1/ymr199w, respectively.

all the features, except semanticsim.mf.difftomax, have scores greater than two. as a result, all the whole llrs obtained by the five trained classifiers are more than  <dig> , respectively, which is higher than the threshold,  <dig> .

this first instance can be trivial because the ppi weight of the pair is high. this means high llrs of features derived from the ppiweight score function. we then pick up all positive examples whose corresponding ppi weights are at most  <dig> and which are consistently predicted to be positive by the five trained classifiers. the following  <dig> instances satisfy this criteria: ypl147w ykl188c; ynl246w yll002w; yor363c yal051w; ycl009c ymr108w; yjr135w-a ygr181w; yjr137c yfr030w; ybr036c ybr161w; yfl041w ybr207w; ydl099w ydr517w; ylr067c yjl209w; yir021w q0115; yhr079c-a ypl121c; ycl017c yer048w-a; yjr035w ykl054c; yor321w ydl093w. among them, the pair of ecm17/yjr137c and met10/yfr030w is taken as the second instance to be analyzed.

the protein complex formed by those proteins is known as sulfite reductase complex . cyc <dig> does not contain any other complexes including one of the proteins. the scores and llrs of the features to this instance are given in the column of tp <dig> of table
 <dig> and figure
 <dig> , respectively. this pair’s ppi weight is  <dig> , which is considerably low. as a result, the llr by ppiweight.score is averagely below - <dig> ). ecm17/yjr137c has four ppis. the highest ppi is one with met10/yfr030w. on the other hand, met10/yfr030w has  <dig> ppis. the number of ppis whose weights are higher than the ppi weight with ecm17/yjr137c is only four. thus, the scores of randomwalkproximity features for this pair are relatively high because most of the neighboring ppis to the pair have lower weights than that of the pair, although the ppi weight of the pair is absolutely low. actually, the llr of randomwalkproximity.rank is the highest among all the seven features ). in addition, the most statistically significant biological process go terms are hydrogen sulfide metabolic process and hydrogen sulfide biosynthetic process with p-value  <dig> e- <dig>  furthermore, the most statistically significant molecular function go term is sulfite reductase  activity with p-value  <dig> e- <dig>  as a result, the llrs of semanticsim features are high. thus, in this case, the fault of ppiweight features is covered by the other features, so that this example is correctly predicted to be positive.

false negative
in this section, an instance is picked up from the  <dig>  positive examples that are consistently predicted to be negative by the five trained classifiers, and examined, too.

the pair of cdc28/ybr160w and cln1/ymr199w is known to form cdc28p/cln1p complex, which is a cyclin-dependent kinase complex to promote the g <dig> to s phase transition
 <cit> . especially, cdc28/ybr160w is well known as a master regulator of mitotic and meiotic cell cycles and to form nine heterodimeric cyclin-dependent kinase complexes with clb <dig>  clb <dig>  ⋯, clb <dig>  cln <dig>  cln <dig>  and cln <dig>  respectively. thus, this protein can be considered to be a hub protein. on the other hand, no other complexes including cln1/ymr199w are known. the scores and llrs of the features to this instance are given in the column of fn of table
 <dig> and figure
 <dig> , respectively. the ppi weight of the pair is  <dig> . thus, the llr of ppiweight.score is high. however, the others’ llrs are weak as follows. cdc28/ybr160w has  <dig> ppis, and three of them have higher weights than that of the pair. the highest is  <dig>  given with ybr135w. as a result, ppiweight.difftomax gives a negative llr. cln1/ymr199w has  <dig> ppis, and none of them have higher weights than that of the pair. although the ppi weight of the pair is relatively high, the pair has many neighboring ppis. this would make the llrs of the randomwalkproximity features weak. to make matters worse, the pair has no statistically significant go terms in both biological process and molecular function aspects. as a result, this pair is incorrectly predicted. as long as a component of a heterodimeric complex is a hub protein, it might be difficult to detect the complex correctly even if appropriate go terms were assigned to the two proteins of the complex.

false positive
lastly, we discuss false positives. among them, two interesting cases can be found. one is the case where either or both proteins of a given pair of proteins are components of a known heteromeric protein complex of size three or more. this result indicates that the features used here are still not enough to discriminate heterodimeric protein complexes from heteromeric ones. note that, among the  <dig> negative examples that are consistently predicted to be positive, nine negative examples are in this group, which can be identified in the section of the results on negative examples in additional file
 <dig>  another case is that a pair of proteins can be a true heterodimeric complex. actually, among the five remaining false positives, the pair of gir2/ydr152w and rbg2/ygr173w is known to be a heterodimeric protein complex
 <cit> .

in addition, msh4/yfl003c and msh5/ydl154w are known to form a dimer with each other. these pairs can be also positive examples. notice that the set of negative examples used in the cross-validation corresponds to only 3% of the ppis of wi-phi. among them, false positives with high llrs could be good candidates for positive examples.

roc curve
a roc curve is given in figure
 <dig>  this is obtained from the results whose llr thresholds are ranged from - <dig> to  <dig>  with  <dig>  increments in between. let tpi,fni,tni, and fpi be the numbers of true positives, false negatives, true negatives, and false positives, respectively, in the i-th fold of the five-fold cross-validation. next, tp is defined as the sum of tpi for i= <dig> ,…, <dig>  this is equivalent to the total number of true positives in the cross-validation. in the same way, fn,tn, and fp are given. the false positive rate and true positive rate are given as fp/ and tp/. the roc curve is created by plotting the false positive rate vs. the true positive rate at each llr threshold. we can see that as the false positive rate increases, the true positive rate quickly becomes large and saturated. actually, for the false positive rate of  <dig> , the corresponding true positive rate is  <dig>  the area under the roc curve  is  <dig> . thus, we can conclude that our method is successful in the cross-validation. note that the peak of f-measure is given by the llr threshold ranging from  <dig>  to  <dig> . the threshold of  <dig>  gives the best of averaged f-measure of  <dig>  with the standard deviation  <dig> . the corresponding precision and recall are  <dig>  with  <dig>  and  <dig>  with  <dig> , respectively.

raw output data
additional files
 <dig> and
 <dig> provide raw outputs of our method for the feature set analyzed in this section. additional file
 <dig> is the main output file of our tool. additional file
 <dig> gives the pair of trained multinomial distributions of a feature for positive and negative examples in each fold of the five-fold cross-validation.

performance comparison
qi et al. <cit>  have proposed a supervised approach with a bayesian classifier for protein complex prediction. however, their target is complexes composed of three or more proteins. most of the features embedded into their classifier are specialized to work well for relatively large complexes. thus it will be difficult to apply their method to heterodimeric protein complex prediction.

on the other hand, there have been many unsupervised learning methods proposed to predict heteromeric protein complexes. thus, to see how much performance our method achieves, performance comparison is carried out with the following nine unsupervised protein complex prediction tools, mcl
 <cit> , rrw
 <cit> , nwe
 <cit> , ppsampler
 <cit> , rnsc
 <cit> , mcode
 <cit> , dpclus
 <cit> , cmc
 <cit> , and coach
 <cit> . however, the last four tools are excluded from further analysis because they do not return any predicted clusters of size two. note that the above tools are all executed with their default settings, except the option of the minimum size of predicted complexes of rrw and nwe, which are set to be two. recall that features based on functional information, which are based on the biological process and molecular function ontologies, respectively, are exploited by our method. thus, our method should be compared with another algorithm incorporating functional information in protein complex prediction. rnsc 
 <cit>  is such a method and adopted in this comparison because it is publicly available.

usually, all ppis of a database are taken as input to unsupervised learning algorithms. here, all wi-phi ppis are given to the above unsupervised learning algorithms. in order to compare our classifiers with those algorithms as fair as possible, the five trained classifiers are imposed on the prediction of the class of the pair of the proteins of each of the ppis.

in the literature, heteromeric predicted clusters are approximately evaluated whether they are matched with some known complexes. a typical matching threshold is
η= <dig>  . thus, precision and recall are calculated with
η= <dig> . it should be noted here that, when c is given by our classifiers, we have c|2=c, i.e.,recall is equal to recall. on the other hand, in general, it does not hold for unsupervised learning methods. thus, this performance comparison with this approximate matching criterion is advantageous to unsupervised learning methods. however, as shown in figure
 <dig>  in f-measure, the classifiers are superior to the unsupervised learning methods .
η= <dig> .

the number of predicted clusters, npc, and nkc under
η= <dig>  are shown in table
 <dig>  those numbers of our method are averaged over the five trained classifiers. it can be seen that the number of predicted clusters varies with the individual tool, ranging from  <dig> to  <dig>  similarly, it holds for npc and nkc, respectively. from these numbers, the three performance measures of precision, recall, and f-measure are calculated and the resulting graph is shown in figure
 <dig>  the precision of our classifiers is slightly more than  <dig> , followed by  <dig>  of ppsampler. the best performer in recall is nwe, closely followed by mcl and our classifiers. these tools form the top group. as a result, the f-measure of our classifiers turns to be the best, which is  <dig> ± <dig> , followed by  <dig>  of ppsampler and  <dig>  of nwe. thus it is 54% better than the second.

n
p
c
n
k
c
the first row shows the name of a prediction algorithm. the second row gives the number of predicted clusters of size two. the subsequent columns show npc and nkc calculated with
η= <dig> . the column of nbcs  gives the result of our method.

next is the performance comparison with the exact matching criterion of η= <dig>  in this case, npc is equal to nkc, which is shown in table
 <dig>  it varies from  <dig> to  <dig>  the resulting precision, recall, and f-measure are shown in figure
 <dig>  in can be seen that the precision of the classifiers is still higher than those of the unsupervised learning methods, although the gap between the best and the second best precision score,  <dig> , given by ppsampler, is smaller than that with the approximate matching criterion. in recall, the classifiers and nwe, which are almost the same, are 43% better than the third best score of  <dig> , given by rrw. finally, the best f-measure,  <dig> ± <dig> , is also achieved by the classifiers, followed by nwe and ppsampler whose f-measures are  <dig>  and  <dig> , respectively. thus, the best one is 74% and 88% better than them, respectively.

the second row shows npc calculated with η= <dig> 

by comparing figures
 <dig> and
 <dig>  the following observations is obtained. at first, the precision of all tools are reduced. this fact indicates that some predicted clusters of size two are approximately matched with strictly larger known complexes, t. note that |t| is limited to ten in this work because of
ov≥ <dig>  with |s|= <dig>  notice that those predicted clusters are completely included in the matching known complexes from the definition of the overlap ratio. secondly, the recall of all the unsupervised learning methods, especially mcl, ppsampler, and rnsc, are also reduced. this fact indicates that some of the known complexes of size two, i.e., heterodimeric protein complexes, are approximately matched with predicted clusters of size ranging from three to ten. these two observations imply the difficulty of predicting heterodimeric protein complex exactly.

although the trained classifiers outperforms other methods, the performance measures are lower than those in the cross-validation. one of the reasons is that the unbalanced ratio of the number of negative examples to that of positive ones. the ratio is  <dig> to  <dig>  these numbers are obtained as follows. cyc <dig> contains  <dig> heterodimeric protein complexes. among them,  <dig> heterodimeric complexes do not have the corresponding ppis in wi-phi. thus,  <dig> positive examples are determined from wi-phi and cyc <dig>  recall that wi-phi has  <dig> non-self interactions. thus, the resulting negative examples in the wi-phi database is  <dig>  in general, to avoid making many false positives, the llr threshold and the class llr,
logpp should be relatively low. actually, the class llr is set to be lower than in the cross-validation. this causes that the number of true positives,  <dig>  in the cross-validation  is reduced to 78± <dig> in this performance comparison . another reason is due to not yet known ppis nor heterodimeric protein complexes. thus, some of the current negative examples, determined from the wi-phi and cyc <dig> databases, can be positive examples, as shown in section ‘analyses’. if these data sets are expanded quantitatively and qualitatively, prediction can be more accurate. lastly, information on ppis and heterodimeric protein complexes being static is also another reason, because they are intrinsically dynamic cellular entities. if time- and context-dependent ppis and protein complexes are available, more sophisticated features could discriminate heterodimeric protein complexes from the others more correctly.

potential protein complexes
we have conducted further analysis as follows. all pairs of proteins, x, satisfying the following conditions are extracted:  x is known to have an interaction between the proteins of x,  x does not correspond to any heterodimeric protein complexes, and  x is predicted to be positive by all the five classifiers. the total number of those ppis are  <dig>  among them,  <dig>  of them are completely included in known complexes of size three or more. thus, some of the remaining  <dig> ppis are candidates for true heterodimeric complexes. in addition, many of  <dig> ppis can be potential subunits of undiscovered protein complexes of size three or more because the fact that they are predicted to be positive by the five classifiers implies that they are functionally and topologically closely related. thus, these ppis are good candidates for unknown protein complexes. raw data of this analysis can be found in the last part of additional file
 <dig> 

future works
currently, there is no high-quality weighted ppi data in human, like wi-phi in yeast. it is a future work to create such data set and apply our method to human data sets. in addition, it is also an interesting future work to apply classifiers trained by yeast data sets to other organisms. in this case, the requirement is at least input data sets to features embedded into the classifiers.

it is also a future work to design more sophisticated features or templates for concrete features using some genome-wide data sets. especially, a feature based on 3d structure information can be promising.

very recently, an independent work for predicting heterodimeric protein complexes by a support vector machine  with new features based on protein domain information has been published
 <cit> . although the best f-measure of the proposed method in a ten-fold cross-validation is  <dig> , which is lower than  <dig>  of our best f-measure in the five-fold cross-validation, it would be worth considering to apply existing kernel functions to the problem and to design new kernel functions. furthermore, in addition to svms, other machine learning classification tools like decision trees and random forests should be considered.

CONCLUSIONS
in this paper, we have proposed a supervised learning method for heterodimeric protein complexes. for this purpose, we have designed templates for features and individual features, which is based on genome-wide data. the naïve bayes classifiers are evaluated in a five-fold cross-validation, and the trained classifiers are also tested with all known ppis. those classifiers are shown to attain much better performance than existing unsupervised learning methods.

competing interests
the article processing charge was funded by the institute of mathematics for industry at kyushu university. there is no other financial or non-financial competing interests.

supplementary material
additional file 1
main output file. this file is the main output file of our tool, which contains results of the five-fold cross-validation and the evaluation of the five trained classifiers with all ppis of wi-phi.

click here for file

 additional file 2
parameters of trained naïve bayes classifiers. this file shows for a feature, the pair of multinomial distributions trained by positive and negative training examples in each fold of the cross-validation.

click here for file

 acknowledgements
the authors would like to thank anonymous reviewers for their valuable comments and suggestions, which were helpful in improving the paper. this work was supported by a grant from the kyushu university global centers of excellence program, “center for math-for-industry,” from the ministry of education, culture, sports, science, and technology of japan.

a preliminary version of this work was presented at the 2nd acm conference on bioinformatics, computational biology and biomedicine, bcb’ <dig>  chicago, illinois, august  <dig> 
