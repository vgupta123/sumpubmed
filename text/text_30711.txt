BACKGROUND
much of the recent progress in our understanding of genomics has come from the study of model genetic organisms such as the fruit fly drosophila melanogaster, the nematode worm caenorhabditis elegans and the zebrafish danio rerio  <cit> . in these model species, a full genome sequence combined with a well annotated collection of gene models is currently available. to address fundamental questions in evolutionary biology, however, we need to expand the genomic and transcriptomic resources available for non-model species, notably insects  <cit> . non-model insects represent attractive models for the study of a range of important biological questions both of applied and fundamental importance, such as those relating to studying insecticide resistance  <cit> , wing pattern development  <cit>  or co-evolution  <cit> . from a functional biologist's point of view, crucial experimental tractability can be gained via a combination of rich sequence data , gene models, functional annotation and in-depth knowledge of an organism's genetics, preferably coupled with the ability to manipulate them  <cit> .

since the advent of est sequencing, the number of organisms represented in dbest   <cit>  has exploded. transcriptomics has grown to be at least as an important resource to non-model species communities as it has proven for the traditional models. it is now conceivable that many non-model species will have at least a workable outline of their genomes available. in turn, large collections of ests important in genome annotation will be generated as the community identifies -omics as a major resource for species with an evolutionary importance  <cit> . this scenario is already a reality as the related technologies become more cost-effective. such non-model species transcriptome projects, being focused on particular applications and biological questions, are especially useful to the wider community even if not targeting annotation of any specific genome  <cit> . one of the most important benefits of shallow est sequencing is the ability to acquire candidate sequence data for downstream applications such as phylogenetics, multi-locus population genetics and expression studies  <cit> . it is hoped that with a wide application of next generation sequencing  the bottleneck in obtaining sequence data will no longer exist  <cit> . the reality is, however, that this vast amount of sequence data has outstripped the ability of most researchers working on non-model species, who often have limited bioinformatic support, to analyze and mine their new datasets. further, there is currently no standardized platform for providing researchers with such analyses for transcriptomes. the generic model organism database  group, derived mainly from the model species communities, is a collection of software, platforms and standardized approaches in dealing with -omic data. they are best known for the gbrowse project, the sequence viewer used by wormbase, flybase and others  <cit> . one of the most important contributions of the gmod group, however, is the development and dissemination of standards capable of generic use: extensive use of bioperl  <cit> , database connectivity frameworks and chado, the generic database schema for almost any type of data produced by the community  <cit> . such standards allow tools to be capable of a unique level of interconnectivity and interoperability.

here we describe a software suite, est2assembly, which aims to address the above bioinformatic deficit while being embedded in the gmod framework. it accepts raw sequence data  and produces annotated assemblies in a gmod/chado-compatible format with minimum user input. further, using the common file format of gff , users can share their data with collaborators and visualize them with tools such as gbrowse. the platform is highly automated and standardized, and we show it allows for direct comparison between various datasets. the modular nature of est2assembly allows users to independently make use of different subroutines. extensive log files guide the user through the assembly process and the output. we demonstrate this platform using a range of  <dig> data from a phylogenetically diverse sample of insects. we benchmark the platform and compare these non-model species collections with  <dig>  public est data from d. melanogaster collected via conventional capillary sequencing which is still considered the gold standard in insect est data and bicyclus anynana, the butterfly with the highest number of sanger-sequenced ests in genbank.

implementation
software dependencies
all software on which the platform depends is free and installation requirements are straightforward. in brief, the platform makes extensive use of bioperl  and other open-source perl modules available from cpan. the gpl-licensed mira assembler is required  <cit> . the proprietary newbler <dig> - including the associated sff toolkit -  is optional. for  <dig> next generation sequencing files we use sff_extract  <cit>  as this is the only known  <dig> basecalling software which is not under a restrictive license. in this paper, we made use of the complete platform and utilized both newbler <dig> and mira version  <dig> . <dig>  further, some modules have dependencies such as installation of the chado database schema  <cit> , emboss  <cit> , ncbi-blast  <cit> , ssaha <dig>  <cit> , repeatmasker  <cit>  , prot4est  <cit> , fasty  <dig>   <cit>  and annot8r  <cit> . due to the modular nature of the platform, a researcher needs to install only the components which will be of use their application of the platform. we provide a comprehensive installation script to ease the procedure of installation of the above 3rd party software.

read pre-processing
the est_process module is driven by preprocessest.pl which accomplishes the following steps: i) project creation, including calling the bases or reading the ssf files; ii) masking short sequences  using ssaha2; iii) blast <dig> to detect unwanted sequences  which can cause problems in the assembly ; iv) removal or masking using these blast output files; v) repeat masking performed using repeatmasker and a user provided database ; vi) polya/t screening performed using a tiered approach of a custom algorithm. then a final step cleans the output files and prepares assembly specific input files including an xml ncbi traceinfo file. a user may interrupt the program and resume from any of the above steps.

the conversion of raw trace files to input files for an assembly is performed independently for each technology. users have the option to convert and quality trim sanger-derived sequences using the gold standard of phred or in the case of data derived from abi sequencers data, make use of any internal kb basecalling. in the latter case, we use a custom quality trim subroutine provided by steffi gebauer-jung  involving two sliding windows to avoid local optima: a larger one scans the trace for a sudden drop in quality values and a finer search pinpoints the exact location. for  <dig> sequencing, due to licensing prohibiting the use of roche's proprietary flowgram extracting software by ordinary researchers, sff files are extracted using sff_extract, an open-source alternative. in addition to detection of low quality regions, we identify adaptor sequence introduced either in the making of the cdna library or subsequent pre-sequencing steps. we use a two tiered search using ssaha <dig>  which combines the ssaha and the cross_match algorithms  <cit>  and blast <dig> from ncbi. we found that the ssaha <dig> search itself is best utilized by using three iterations: two searches for adaptor sequence  and one restriction site search with a parameterization for extremely short target sequences . the platform uses blast <dig> to screen for common contaminants found in molecular biology labs via a customizable fasta database. in any transcriptome project, it is also undesirable to clutter the assembler with sequences that are overrepresented due to transposon activity, mitochondrial or rdna origin. the platform allows users to screen them using repeatmasker via a user-defined database. to assist lepidopterists in particular, we have included a prediction of repetitive elements from bombyx mori  which users can concatenate with the insect repeat library from repbase. the intensive steps of running blast and ssaha <dig> can seamlessly utilize multiple threads to reduce run time.

trimming of polya/t tails is essential in est projects and we use a routine to iteratively scan for such homo-oligomers. the routine can also be used independently of the platform and is highly customizable; users can specify which base  they wish to search for, seed length, min/max length of homo-oligomer, depth of search of each sequence and other options. in order to minimize false positives in a/t rich genomes or errors produced by the pyrosequencing methodology of  <dig>  the platform utilizes  <dig> rounds with increasing minimum length and decreasing search seed length. except for the second round pair, the scan is performed only at the ends of the sequence for a length of one-third of the total sequence length. the second round scans deeper to  <dig> bp from each end. an additional feature of the routine is the use of any suspected polyadenylation signal site  upstream of a hypothesized polya/t site. currently, we use a simple pattern search but implementing a model-based approach  <cit>  could be of use. this assists in correcting the masking and avoiding over-trimming of the 3' utr or for allowing a short polya sequence which would normally be below acceptable length to be masked. in addition, it uses this information to detect false positives if there is a significant amount of sequence  between the end of the polya and the start of any vector masked sequence . if no pas site is found upstream of the polya tail  and the suspected polya is shorter than the specified cut-off, then the polya is not masked but still tagged in the log-files. we found that this option enhances polya masking in sanger-derived sequences but by default is switched off for short reads. an output for each polya/t found and which criteria were used is produced in a log file should users wish to exploit them in gene model construction. at this stage, an xml file  containing the low quality, adaptor sequence and polya/t trim points is generated. this file, when used with the original untrimmed and unmasked file, can guide assemblers such as mira on how to perform clipping of undesired regions. this approach can be used to tackle any potential false positives that may arise in the preprocessing steps.

it is worth noting here that we find that near-perfect signatures of the  <dig> adaptor sequences can persist even within regions of high quality assembly, which could be the result of the chimeric ligation of molecules. in such a scenario, we recommend that if manual inspection is not feasible that the sequence region is masked and the assembler allowed to determine if the region is truly an adaptor sequence or part of the sequenced species genome: false positives will have multiple reads in the assembly exhibiting high identity down- and up-stream of the suspected site and therefore still assemble. to facilitate assemblers who cannot make such judgements, preprocessest.pl allows the flagging of sequences which have more adaptor sequences than the user defines. if the user wishes then only the longest stretch of high quality sequence between two adaptors is used.

optimal assembly
the output files of est_process are provided to the second module, parameterize_assembly. it could be straightforward to plug-in various assemblers in future versions but we currently make use of newbler as it is the standard for  <dig> data and mira due to its ability to analyze sanger/next generation data concurrently and the provision of excellent support. in the current version, datasets from sanger and/or  <dig> can be provided and users will concatenate any datasets originating from identical technologies. a configuration file is responsible for defining which parameters are passed on to the assembler mira. this allows for multiple runs of the same datasets in order to explore the parameter space.

assembly quality is estimated using analyze_blast.pl via summary indexes based on the coverage of one or more reference organism databases used in a similarity search . coverage is calculated on a base pair basis by counting unique hits to a particular base pair. overlapping coverage  is calculated by counting the total number of hits a base pair  receives. we perform these calculations for both the assembly and the database. the ratio of redundancy over coverage is summed for the database and the assembly. if more than one reference database is used  then the total sum is used. one has to be aware that the absolute numbers of coverage are volatile as they are dependent on the blast cut-offs used. when the same cutoffs are used, however, comparison of assemblies is a meaningful index to evaluate how parameterization influences the assembly. another quality control index is the number of reads included in the assembly. this can act as a proxy for the downstream utility of an assembly, for example the number of snps which can be determined or the likelihood we can detect alternative splicing or frame-shift-causing sequencing errors. finally, we also consider the proportion of the reference database covered  as a proxy to eventual gene finding.

in est assemblies a large portion of the consensus can be non-coding sequence. such sequences often diverge rapidly due to the lack of any selective constraints. the unfortunate result in any assembly process is that sequences of this type, which are from identical genomic regions, fail to assemble together. the script trim_assembly.pl is one of the two methods to remove such redundant contigs. it first scans for polya/t tails which may have been built during the assembly. then it defines a set of 'high-quality' contigs using user-specified cut-off for length and number of reads included. the other contigs are only included in the final set if they a) don't have a high sequence similarity to a high-quality contig, b) have a high sequence similarity to a reference proteome, c) specifically requested by the user by proving a list file. the output of the contigs which have been excluded is cataloged in a log file.

protein identification, snp discovery and data mining
data mining of est projects is driven by searching the dataset for the signature of a favorite protein or sequence. the platform makes use of blast similarity searches using the contig consensus. the analyse_assembly.pl and analyze_blast.pl scripts, which are employed during parameterization, can be used standalone to estimate the quality indexes for any blast report. they allow users to identify the exact coverage of a fasta input file in relation to a reference database using blastx, blastn or tblastx. they provide the ability to run multiple blasts in a threaded fashion with one command. the script has the additional ability to output a fasta file with the part of the input file which matches the reference and a second file with the part which did not. this approach is very useful in extracting the segment of the assembly which is known to be coding. for example, a tblastx approach is useful when multiple species have been sequenced but no reference proteome exists or is under-annotated .

deeper annotation can be performed using predicted proteins. protein predictions are accomplished using prot4est  <cit>  . the current version of prot4est does not produce the open reading frame  which can differ substantially from the contig . we acquire the orf using fasty or failing that, emboss's transambig and attempt to correct for any ambiguous codons to match the consensus. these are then annotated using similarity to known proteins which may have annotated ontology terms: gene ontology   <cit> , enzyme commission   <cit>  and kyoto encyclopaedia of genes and genomes   <cit> . annotations based on electronic similarity are assigned using annot8r. if computational resources allow, interproscan annotations can be included to allow users to search for specific protein domains, motifs and sequence signals. the output of the above procedures is then converted to a common file format using ic_annot8r2gff.pl and can be databased.

further, we provide the script analyse_assembly.pl to help with blast reports on a single computer but if est projects are commonplace then access to a pc-farm or a high performance computing system is required. for this reason, we include a set of simple scripts to facilitate use and error-checking of lsf queue submissions allowing, therefore, for the automation of blast and interproscan annotations.

of particular interest to biologists is the identification of single nucleotide polymorphism markers . the assembler mira produces such a set and can be included in the assembly gff file. we also extract a 'high-quality' snp dataset by including those snps which have the minor allele frequency above a user-specified threshold and have at least  <dig> invariable bases up and downstream of the snp position. this padding can be customized by the user but we default to  <dig> in order to design primers and create a unique identification sequence for submission to dbsnp  <cit> . this snp identification is accomplished via ic_create_snps.pl which estimates the position of each snp in relation to the orf and, if determined as coding, provide the codon position, the alleles and whether the nucleotide change causes a synonymous or non-synonymous amino acid change. due to prot4est's approach to determine the orf, a simple translation of co-ordinates is not possible. we, therefore, perform a local alignment using fasty  <cit> . we also include a similar implementation for sean  <cit>  which would be of use to users with small amounts of data.

we can utilize gff as the middleman to populate chado and bio::db::seqfeature  database schemas. in this paper, we focus on the simple format and speed advantages of seqfeature as most users will be dealing with a limited number of datasets. we provide a set of scripts that produce both chado- and seqfeature-compatible gff files for each data type which the platform is capable of processing: caf assemblies in read-contig or contig-read sorting order, blast reports, orf predictions, snps predictions and kegg, go and ec annotations from annot8r. as the proper linking of the various reference sequences and their features is essential, we have a specific strategy for creating the gff files for use with gbrowse : a contig view is composed of a reference contig and associated annotation features such as the assembled est data, snp markers, blast annotation etc. is anchored to it. the orf feature is also anchored to the contig but also exists as a separate reference sequence in a second web page. this orf object has its own associated annotation, including a polypeptide features which serves as an anchor for the protein prediction reference. likewise, this protein prediction allows for anchoring protein-based annotations such as estimated molecular weight, assigned ontology terms and other protein-based data in a third web page. this approach enforces the one-to-one relationship between an orf and a protein but allows for one-to-many relationships between an assembled contig and protein predictions.

benchmark datasets
data for benchmarking the platform was provided by dbest  or by collaborators. in more detail, the gs <dig> sequencing of a manduca sexta  hemocyte cdna library was provided by haobo jiang and is published by zou et al  <cit> . the chrysomela tremulae  midgut gslfx and manduca sexta midgut are published by pauchet et al  <cit>  and  <cit>  respectively; the gsflx of cdna from whole larvae of euphydryas aurinia  was provided by yannick pauchet ; the gsflx-titanium sequencing of cdna from wing discs of papilio dardanus  by iva fuková ; the sanger capillary and gsflx data of heliconius melpomene were prepared from wing-discs of developmental stages from late larval through to mid-pupal stage by ronald lee and chris d. jiggins  and is published by ferguson et al  <cit> . the sanger capillary and gsflx data of heliconius erato was also generated from wing-discs and was provided by w. owen mcmillan . the gs <dig> melitaea cinxia dataset from whole larvae is published by vera et al  <cit>  and was downloaded from ncbi's short read archive after communication with howard fescemyer . the bicyclus anynana data are based on the capillary-sequencing technology of a variety of tissues, were obtained from dbest and is published by beldade et al  <cit> . the d. melanogaster data are also based on capillary-sequencing technology of a variety of tissues and were retrieved from dbest. we did not include any singletons in the resulting assemblies. for the saturation curves, we used the h. melpomene  <dig> preprocessed dataset  and created pseudo-datasets by randomly splitting it in datasets containing 1/ <dig>  2/ <dig>  3/ <dig> and 4/ <dig> of the initial data. we repeated the procedure  <dig> times for each pseudo-dataset thus generating and annotating  <dig> pseudo-assemblies using the same procedures as for the main assembly.

reference proteomes used in this study were from d. melanogaster, anopheles gambiae, apis mellifera, b. mori and tribolium castaneum. for each species we used the refseq  <cit>  and uniprot  <cit>  curated proteins, concatenated with the predictions provided by each organism's genome database  <cit>  and made non-redundant at the 100% level using cd-hit  <cit> . in the cross-dataset comparison, we identify orf coverage by calculating the proportion of the reference proteome aligned to the assembly  as an indication to gene-finding. we also estimate the proportion of the assembly aligning to the proteome  to determine the portion of the assembly likely to be coding and also include the improvement of including a tblastx search .

implementation overview for the biologist
the est2assembly platform allows for the processing of data either directly from dna sequencers  or as fasta files. the software also allows users to combine their own datasets with those available in public databases and a script is included to automatically download such sequences from the european bioinformatics institute . moreover, any large sequenced genomic regions  can contribute coding sequences  to the assembler. this form of dataset concatenation has the advantage that a pool of shorter ngs reads will assemble better if a longer sequence  is also included in the same pool. currently, two sequencing technologies can be processed: sanger capillary-based data and  <dig> pyrosequencing data. the input data is fed into the preprocessest.pl which removes low quality sequences, any adaptors and polya/ts that may be present. this script also removes any contaminants and repeats which can cause serious misalignments. two versions of the processed fasta files are produced: trimmed and 'masked', the latter accompanied by a quality file and an ncbi traceinfo xml file which defines trim points in relation to the original files. researchers can choose to use the untrimmed but masked files or the original sequences  or the trimmed files for assemblers such as newbler. further, at this point in the pipeline, graphs can also be generated  to examine the effect of the pre-processing on the data.

once a user is satisfied with the quality control of the reads, the assembly can begin. an optimal assembly needs to be chosen according to the needs of a project but often the assembler is used as a black box despite the fact that assemblers are mere computation machines and therefore the results may or may not be biologically meaningful. for this reason, the next step is submitting these files to a second script, parameterize_assembly, which launches the assemblers  with varying parameters, compares the results to one or more reference proteomes and computes a number of indexes suitable for transcriptome projects. which index is most useful  depends on the aim of the particular project. for example, in a gene-hunting project one may wish to optimize for the number of genes discovered and minimize redundant contigs, where as in snp project, one may wish to maximize for the number of reads in the assembly and tolerate redundancy which can later be addressed manually for contigs of interest. for the reference proteomes, we suggest that more than one is provided in order to remove species-specific bias and increase the power of detecting coding sequences but, for computational reasons, too divergent species will not be useful for parameterization. in our work and this paper, we used species with a genome sequence which are in the same phylum as our data datasets. as parameterization is focused on exploring how different parameters behave, the exact details of the reference proteome and blast cut-off values are not as important since the platform ensures they are used in a consistent fashion.

simple blast-driven indexes are reported for each proteome: the number of queries which have similarity with a reference protein; the number of reference proteins which have similarity to a contig in the assembly. it then calculates the indexes for each base pair/amino acid rather in order to detect the level of partial orf sequencing. further, an annotation redundancy index is based on the number of one-to-many hits  between the reference proteomes and the assembly. finally, est2assembly reports the number of reads included in the assembly. which criterion is chosen to identify the best assembly is left to the individual researchers. in this paper, since we are focused on maximizing the 'number of genes found' - like many non-model species projects - we used the reference proteins found as the main criterion.

ignoring the parameterization step is not recommended, as figure  <dig> shows: the mira.a <dig> parameter set is the default for mira's 'accurate quality' setting but produces a suboptimal assembly using the criteria of annotation redundancy. one should note that even though transcriptome sequencing  is producing transcripts from the whole mrna, it is unlikely that full length transcripts are sequenced. figure  <dig> shows that proportion of identified proteins in terms of cds coverage was significantly lower than the proportion in terms of number of identified proteins . in addition, for this dataset, mira outperformed newbler with our chosen criterion, showing it is important to attempt an assembly with more than one assembler.

once the desired assembly is chosen, users may opt for the removal of a subset of redundant contigs using trim_assembly.pl and evaluate if there is a loss of 'number of genes found'. by redundancy we mean here the fraction of contigs that are likely to originate from the same locus - as defined by the degree of similarity. often, an assembler fails to align them due to a high number of mismatches caused by a non-conserved region  accumulating mutations, an alternative exon or - in libraries constructed from outbred individuals- multiple snps. the aim of a reduction of redundancy is to reduce the strain of computing resources on the subsequent annotation steps. researchers will, therefore, annotate a considerably smaller dataset. we support gff conversion for a number of publicly available tools that we consider to be of most use in transcriptome project and we use routinely: the caf format ; prot4est ; blast ; annot8r  and interproscan .

as our interest is primarily in ensuring that data produced by multiple researchers can be integrated, we decided to utilize the community tools of bioperl and gmod. the bioinformatics community has been converging on a set of standard formats. one such flat-file format for sequence annotation is the general feature format  specification which is currently being standardized as version  <dig>  the gff format is a tab and semicolon delimited file making it both machine- and spreadsheet- readable and has become the format of choice for the gmod software group. from a database perspective, there are two additional important formats: bioperl's bio::db::seqfeature and chado. the former is a highly denormalized database schema which allows for rapid queries of sequence data by sacrificing control of data integrity. chado, on the other hand, is a normalized modular database schema created to serve as the main data warehouse of multiple types of data. it is logical therefore for researchers to utilize chado as a data warehouse and bio::db::seqfeature for driving user-visualization software such as gbrowse. data can be loaded into a database via bioperl and we provide a script to load multiple gffs in the correct order and allow for later additions. the chado schema requires a postgresql database and we find that the seqfeature database works well with postgresql as well. once the database is loaded, one can use to drive popular tools such as gbrowse, apollo and artemis  in order to curate the project. transcriptome project curation requires the ability to join contigs and we find that the user-friendliness of the proprietary program geneious  is efficient for the purpose and a free version is available. a future version of this platform may make use of geneious' interactivity interface . due to the popularity of gbrowse as a sequence viewer, we provide configuration files that can be readily customized. when the complete analysis is loaded, researchers and their collaborators can view any annotated contig in three inter-linked web pages: assembled contig, predicted orf and protein .

RESULTS
as est2assembly is unique in the field as it is not one pipeline but a complete framework to analyze transcriptomes from raw data to a format wet-lab biologists can analyze. the preprocessing step has been built to take advantage of ncbi's tracexml in order to annotate vector and clipping positions. this allows us to use the original  data to produce the assembly using mira which can make intelligent decisions if a clipped region is a false positive. the assembler parameterization allows bioinformaticians to seamlessly explore the parameter space as figures  <dig> and  <dig> mentioned above . had we used the default settings, we would have an assembly that had a lower number of identified reference proteins , a lower number of reads used but similar degree of the redundancy index . likewise, had we opted for the newbler assembler without investigating the mira assembly, we would have both a lower number of proteins identified and shorter cdss .

in addition, as we mention above and show below, current ngs assemblers produce non-redundant contig sets. this is a correct procedure in order to avoid assembling close paralogs  but results in a downstream computational problem with annotating a redundant set of objects. our trim_assembly approach is highly customizable using concepts intuitive to biologists and produces better clustering than the standard cd-hit-est we used to use: for one of the more redundant assemblies we started with  <dig>  contigs and reduced it to  <dig>  contigs with trim_assembly when compared to  <dig>  when using cd-hit-est with both strand search enabled.

subsequently, we extend the usefulness of prot4est by allowing users to build a orf model even for species where no orf is annotated in the public domain. further, our snp pipeline uses a similar approach as sean but predicts more markers  and is built to be fast and efficient with a large number of data  but also to decrease the number of snps which would be useful to wet-lab biologists: with the  <dig> technology we have more candidate snps that biologists can afford to screen or make use of . manual inspection is essential in order to identify markers which are most useful in downstream genotyping methods. for example, any base covered with less than  <dig> reads is of no use for a snp call as one cannot distinguish a snp from a sequencing error. in addition, the platform predicts high quality snps by demanding a certain region surrounding the snp to be invariable in order to assist with primer design. further, by comparing with the position in the predicted orf, a marker is classified as non-coding/coding and then determined if causing a synonymous  or non-synonymous  mutation. perhaps, however, the single most important innovation in the field of transcriptome processing is the utilization of the gff file format and integrating the assembly, protein predictions and annotations into a format the gmod framework.

utility
we used a diverse dataset to test and build this platform and due to the standardized approach which it follows, we have been able to evaluate data from different sequencing technologies and protocols in order to offer insights on non-model species transcriptomics. the cost-effectiveness of ngs has been a primary motivation for non-model species researchers to initiate project yet others are worried about the quality of data resulting from such short reads. examination of the data after pre-processing is essential in order to make a meaningful comparison and our graphical tools allows researchers to compare their raw data with other datasets .

in gene discovery projects, if one hopes to provide an accurate level of annotation, the length of the contigs is an important element which must be considered during project design. as figure  <dig> shows, the number of reads increases significantly the length of contigs  but technology has the largest effect. gs <dig> seems to be of limited use and the newest gsflx-titanium has comparable contig length to  <dig>  gsflx runs. further, with increased read number and length, we get an increased contig coverage. the coverage of each contig  is an important limiting factor in snp prediction, error correction and the overall quality of the assembly. for population genomicists, substantial contig coverage offers an additional advantage in being able to estimate the frequency of a particular snp marker. further, low frequency non-synonymous snps can guide a curator to regions of misassembly or erroneous orf prediction. once verified to be true non-synonymous snps, curators can look for genes showing an excess of non-synonymous polymorphisms and, therefore, possibly evolve under balancing selection. even though, current users need to perform this latter step manually, it would be of use to automate it for assemblies which are hand-curated. for such an investigation to be profitable, however, the design of the project, especially how many and which individuals, to be included in the cdna library must be carefully planned.

the number of contigs is, however, rarely an accurate prediction of the number of genes sequenced. non-coding dna  sequenced from multiple haplotypes is not easy to assemble due to high levels of heterozygosity caused by the fact that the majority of the utr is evolving without constraints. upon investigation, this seems to be the major cause of contig inflation and the platform can use two methods to alleviate the issue by filtering in favor of regions likely to be coding. one method is used in figure  <dig> but we may have the unfortunate effect of removing small contigs containing novel proteins which are small in size and low in expression. the other method - available only to users with a dataset from related species - is to use a tblastx approach  to complement the blastx approach of the reference proteome. novel proteins, even if evolving rapidly, are expected to show significant similarity on the amino acid level between the two species. the platform allows one to extract the contig regions matching this coding fraction and thus have a dataset known to be coding. the two approaches are not mutually exclusive but can be complementary: the trim_assembly.pl is highly customizable regarding similarity and abundance levels whereas the tblastx approach will not tackle the issue of redundancy .

we can, therefore, conclude that a better assembly benchmark is the identification of proteins from a reference proteome and the portion of the assembly identified as coding . the quality of a sequencing experiment can thus be evaluated by extracting the cds fraction and then calculating the proportion of reads contributing to this portion of the assembly . in our dataset, we find that the drosophila dataset covers only 70% of the drosophila proteome and only 56% of the assembly is defined as coding . from the biomart.org website we calculated that the proportion of non-utr in annotated d. melanogaster genes is only 80%. the trend of having a significantly lower cds proportion than expected is maintained across the data when a blastx versus a reference proteome is used. one reason can be purely technical: in a dataset originating from a library with large number of haplotypes, there is a contig inflation when the utr is included for assembly. this effect is more likely to be present in non-model species where isogenic lines or sufficient levels of inbreeding are unattainable. the second technical issue, especially in ngs datasets, is that due to the fragmentation step involved, there can be a preference for sequencing of the transcript ends and therefore utr  <cit> .

regardless whether a measure against redundancy is used, if multiple datasets are available, one can explore whether a reference proteome is useful and whether using a phylogenetic framework assists in gene-finding. due to the taxon focus and species richness in our data, we expect that mutual tblastx searches among the assembled ngs datasets  as reference databases, will identify additional proteins which may be absent from the reference proteome, or sufficiently diverged to be missed by comparison to it. because the b. anynana, m. cinxia, e. aurinia, p. dardanus, h. erato and h. melpomene datasets originate from butterflies with the latter two being from the same genus  and the m. sexta datasets originate from a moth in the same superfamily as the reference proteome b. mori  <cit>  we expect and find that the butterflies will show a significantly higher improvement with tblastx than m. sexta. oddly, we also find a large improvement in c. tremulae, a beetle which uses the tribolium castaneum as a reference but are in different superfamilies. this improvement is unlikely to be due to a poorer annotation in tribolium versus bombyx as the reference annotation proportions are relatively similar in all non-model species datasets. it is not unlikely that tribolium is not a good model for c. tremulae especially if one begins to consider the difference in their ecology. the other, not mutually exclusive, possibility is that there is a significant degree of rapidly evolving proteins in these non-model species. nevertheless, via the tblastx approach, each of the heliconius data has now a cds proportion comparable to the drosophila gold standard. indeed, by also counting the number of sequence reads belonging to the estimated cds  we are able to see that the heliconius datasets have also a higher proportion of reads belonging to the cds. this observation provides some evidence that contig inflation in this case is more likely to be driven by unassembled utr rather than due to fragmentation of the transcript ends.

one other point of note is relating to the procedure of choosing a cdna generation protocol. all ngs datasets compared here use the smart technology to produce cdna apart from the gs <dig> dataset of m. sexta which was produced using gc-rich random primers. we do not know why the number of reads was much lower than the gs <dig> dataset from m. cinxia but a significantly higher proportion of the reads matches a predicted cds which translates to a better return to projects aimed at gene-hunting. we cannot be sure why this occurs: it could be due to a high number of low quality reads in m. cinxia, it could be due to a slightly better protein identification based on the closely related b. mori but it is logical to expect that, in species known to have a gc enriched coding sequences, the primer protocol would enrich for regions with high gc content and therefore more likely target coding regions.

finally, researchers often wish to know how deep one should sequence in order to sequence the complete transcriptome in their sample. this is important in planning non-model species transcriptomes projects. as the h. melpomene  <dig> data originate from  <dig>  full-plates  and the cdna is harvested from a single tissue  we used  <dig> pseudo-assemblies to investigate the effect of deeper sequencing. as figure  <dig> shows, most transcripts for that particular tissue were identified after  <dig>  half plates were sequenced as the exponential curve is approaching a plateau . with  <dig> half-plate, however,  <dig> % of the plateau value for proteins identified was attained, showing that even shallow sequencing of a non-model species is highly worthwhile. for snp detection, the function is not exponential : with each subsequent run the number of high quality snp markers increased linearly.

discussion
there are several advantages of est2assembly over other platforms for processing est raw data . first, preprocessing of raw sequence is essential and our platform offers a standard method for consistently accomplishing this for hundreds of thousands of sequences with straightforward user customization. second, parameterizing an assembler is a tedious process and our platform is the only one which automates many of the routines. third, annotation of an assembly with est2assembly can be readily standardized and automated for processing large numbers of datasets with minimum investment in time. deciding on the optimal assembly is a subjective process and depends on the project but by providing the means to explore the parameter space allows for a standardization of an approach which is often ad-hoc. we calculate the blast-based index using two approaches and can determine the number of unique proteins found, actual proportion of amino acids found and obtain an estimate of the assembly proportion that is actually coding. in this case, as more datasets are published, we can benchmark laboratory protocols and sequencing technologies involved in acquiring full length genes. importantly, the rich log output guides the wet-lab biologist who generated the data to perform in-depth investigations and hold a better understanding of their project. with est2assembly, we have not aimed to produce a 'black box' but a program which gives feedback to the user as to the quality and characteristics of the different assemblies achieved with their data. we showed that analysis of an assembly can give important insights to the technologies and protocols employed to acquire a transcriptome. future work can focus on including more annotation modules and developing a java/jdbc-driven graphical user interface  and relational database to allow molecular biologists with no computing knowledge to supervise the data analysis.

shallow sequencing est projects are becoming a gold-mine for biologists working on non-model species and are often used for both gene or snp discovery but until now no software exists to link the snp to both an assembly and the codon it may be part of. the est2assembly platform allows for the classification and identification of snps which may be under selection or point to a misalignment. such data are important in manual curation of an assembly and lacking from any other software. we can also obtain coding synonymous snps for which a pcr primer is straightforward to design but are under low levels of selection. non-coding markers are also useful to researchers who wish to investigate selection in non-coding dna.

special considerations, however, have to apply to projects working on non-model species, especially when datasets are restricted for financial or biological reasons. often the design of the experiment is not conceived with full knowledge of a technology's capabilities and limitations. here we show that different technologies and lab protocols differ in their ability to produce an assembly and project design plays an important role. at times, but not always, such project design bottlenecks can be overcome. for example, assemblers treat the common issue problem of inflated contig number caused by non-optimal alignments by assuming that they are based solely on sequencing errors or repeats. the result is that fewer genes are discovered in non-model species. the issue is confounded because researchers undertake a transcriptome project for different reasons. even though in gene discovery project the norm is to sequence a cdna library from specific tissues and with a limited number of haplotypes, this is rarely the case in most est projects of non-model species. researchers often utilize est projects as both a gene-finding project and a snp discovery protocol and therefore are tempted to include a high number of out-bred individuals. it should be noted that both newbler and mira are not clustering algorithms but assemblers and therefore their main aim is not to identify alternative splicing events or cope with a high degree of heterozygosity in the sequenced sample. there are methods to alleviate the problem such as including a final clustering step  for each transcriptome project and make assignment of supercontigs  more robust. another issue which cannot be resolved using bioinformatics is the quality of material used for cdna preparation. even though we cannot be certain regarding the cause, the m. cinxtia and e. aurinia datasets argue against a whole animal approach in constructing the cdna library. such cases have been shown to be problematic in enzymatic reactions due to pcr-inhibiting pigments  <cit> . the inclusion of micro organisms in the digestive tract or the outer body can also result in acquiring contaminating sequence from another species. the later cases can be investigated bioinformatically  <cit>  so as to prevent generating an erroneous transcriptome survey.

CONCLUSIONS
in conclusion, the modern transcriptome sequencing approaches are very powerful and cost effective but they still yield partial transcriptomes. in the future, however, our single most important limitation will not be raw transcriptomic or genomic data. we have shown that the ability to accurately annotate an assembly depends on using a correct reference proteome or utilize phylogenetic framework. further, comparison to an appropriate reference proteome is invaluable in choosing among different assemblies, yet such proteomes are themselves incomplete. a concerted annotation effort based on transcriptome sequencing from a diverse phylogenetic collection is required, which will accelerate the filling of proteome space beyond the limited set of model organisms that currently occupy it. with the ngs capabilities, it is obvious that such an effort should make full use of transcriptomic data in order but we still lack the necessary infrastructure. the est2assembly software, however, has enabled the development of such infrastructure, an alpha stage preview of which is available at http://www.insectacentral.org.

availability and requirements
project name: est2assembly

project home page: http://code.google.com/p/est2assembly/

operating system: linux

programming language: perl

dependencies on proprietary software: none

other requirements: ncbi-blast, emboss toolkit, bioperl dev-branch, postgresql, mira , sff_extract 

optional programs : annot8r, prot4est

license: general public license version 3

abbreviations
bac: bacterial artificial chromosome; cds: coding sequence; ebi: european bioinformatics institute; est: expressed sequence tag; gff: general feature format; gmod: generic model organism database; gpl: general public license; gui: graphical user interface; ngs: next generation sequencing ; orf: open reading frame; pas: polyadenylation signal; snp: single nucleotide polymorphism; utr: untranslated region.

authors' contributions
ap conceived, designed and performed the study; analyzed and interpreted data; coded the software and drafted the manuscript. rs co-authored the gff writing software and the gbrowse schema. rhfc and dgh drafted the manuscript, financed and provided infrastructure for the study. all authors approved the final version of the manuscript.

