BACKGROUND
the effort to produce an index of all human proteins  began over twenty years ago. this project pre-dates the human genome project by more than a decade. however, the complexity of the task of creating this index was underestimated and the relative simplicity of the human genome with four known nucleic acids arranged in a linear coding order allowed the process of the sequencing of the human genome to progress exponentially  <cit> . the successful completion of the human genome project is now putting the focus back on proteins. the emergence of new and improved protein technologies from re-engineered two-dimensional  gel systems to mass spectrometry has made the mapping and identification of the entire proteome of a cell  a much more accessible goal. over the past few years a number of databases documenting the protein content of a single organism, organ or organelle have been created  <cit> , and a number of papers describing results of experiments using these new and improved techniques have been published.

the advantages of 2d gel technology
two-dimensional electrophoresis is an extremely powerful tool for the analysis of complex protein mixtures. proteins carry both positive and negative charges. the ph of the medium they are in determines their net charge. the ph that gives a zero net charge is the isoelectric point of the protein . in isoelectric focusing , protein mixtures are electrophoresed in a gel containing a ph gradient. the proteins in the mixture migrate according to charge density until they reach the part of the gel that corresponds to their pi. at this point, their net charge is zero, and migration stops. this is the first dimension of separation in a 2d gel experiment. the electrophoresed gel is then layered on top of a polyacrylamide gel and electrophoresed once again. the proteins now move from top to bottom depending on molecular weight. the distance covered by a protein is inversely proportional to its size. this is the second dimension in a 2d gel. 2d is an effective method for identifying qualitative and quantitative differences between proteins expressed in various tissues or between tissues exposed to different experimental treatments. although the number of proteins displayed by 2d is much lower than the estimated number of genes in a particular tissue, 2d is currently the only available technique that enables the isolation and separation of thousands of the individual proteins that constitute a tissue proteome  <cit> . anderson et al  <cit>  point out that although writing obituaries for 2d gels has become a popular past time, the supply of unrelated parameters applicable to protein separation is limited and nearly all other combinations have been explored in the past.

database for statistical analysis
images of 2d gels are acquired into a database using an image scanner. image analysis software converts the gel image into a digitised image in a computer, matches gels and spots on gels across the different groups and creates a database with information about spot intensity and spot location. as mentioned above, the two variables – the pi representing net charge of the protein and the molecular weight of the protein – are not correlated. in geometric terms this suggests that the two dimensions are orthogonal to each other. the two dimensions in a two-dimensional gel thus can be thought of as the two axes in a two dimensional graph. the coordinate on the x-axis is a measure of the isoelectric point  of the protein, and the coordinate on the y-axis is a measure of the molecular weight of the protein

the information in the database includes a gel identification variable, a spot identification variable, the x and y coordinates of a protein spot and its intensity measured by the amount of light transmitted by the spot. depending on the software package, one can obtain other parameters in the database, including a measure of the quality of the spot to various measures associated with spot intensity, such as volume, area, peak height, etc.

the rationale for this study
the intensity of a protein spot is assumed to be directly related to the amount of protein in the particular tissue under investigation at that given time point. changes in protein intensity are therefore approximated by changes in the intensities of protein spots in gel images. changes in protein structure associated with post-translational modifications such as phosphorylation, oxidative modification or glycosylations may result in changes in the pi or molecular weight of the protein and are manifested in the gel by a change in the vertical or horizontal position. the object of 2d gel experiments is to detect differences in protein intensity/complexity between two groups of gels.

a number of recent publications  <cit>  have used statistical models generally known as classifiers to detect differences in protein intensity/complexity between two groups of gels. classifiers are increasingly being used in the analysis of hi-dimensional data sets derived from gene and protein expression experiments. these models help one to determine if the changes in protein intensity /complexity are specific enough to enable a clear separation of the gels into the right groups. they can also be used to provide a good visual demonstration of the differences between groups.

classifiers
classification is the process of assigning objects to a category. an interest in classification permeates many scientific studies  <cit> . there are two broad categories of classification problems. in the first, e.g. discriminant analysis, one has data from known groups. information that distinguishes these groups  collected from an experiment is used to assign samples  to these known groups. in the second case, e.g., cluster analysis, one has the information but no preset classification. the data is mined to see if there are naturally occurring clusters. these clusters are then investigated to identify commonalities within and differences between clusters. stein and zvelebil  and patel et al  describe using 2d gel data sets to build supervised and unsupervised classifiers

both types of classification problems have three stages, input, algorithm and output. most published literature concentrates on the second of these. however, careful thought about what variables to use and how to characterize or summarize them as inputs into an algorithm are very important issues  <cit> . it is evident that the reliability and reproducibility of a classification is a function of the input, which in turn depends upon the process of data normalization, data reduction, and variable selection, i.e. the pre-processing of data. this paper focuses on the effects of preprocessing on the selection of variables that enters a classifier.

pre-processing
in order to conduct a systematic analysis of 2d gel data, one has to pre-process the data set. pre-processing in the case of 2d gel analysis includes: 1) normalizing intensities to remove effects of differential loading and staining; 2) transformation of outcome variables to normally distributed variables; and 3) imputing values for missing spot intensities. we are not aware of any other study that has looked systematically at the effect of pre-processing 2d gel data on the results of subsequent statistical analysis of the data. in this paper, we present a protocol for the analysis of 2d gel data and examine the effect of statistical pre-processing of 2d gel datasets.

the effect of using two different formulas for normalizing versus not normalizing, log-transformation versus no log-transformation and single value imputation versus multiple value imputation, and averaging spot intensities across replicates versus keeping the replicate information separate are compared using the results of two sample t-tests. we also compare the results of two-sample t-tests provided by the image analysis software pdquest to results obtained after following the protocol described in figure  <dig>  pdquest allows the user to normalize the data but has no facility for testing the distribution of the outcome variable and transforming it to fit a normal distribution. to the best of our knowledge, pdquest replaces missing spot intensities with a zero.

the experiment
the data used in this study is from an experiment that looked at the effect of a diet enhanced with grape seed extract on the proteome of whole brain homogenates of sprague-dawley rats  <cit> . there were five treated animals and five control animals. due to sample availability and other issues related to the creation of 2d gels, each biological replicate had different numbers of technical replicates. the maximum number of replicates was four, and the minimum was two. a number of changes in proteins that were attributed to treatment differences in this study have been identified with matrix assisted laser desorption ionization – time of flight  mass spectrometry. these changes have also been confirmed in later experiments with transgenic mice. thus the protein changes detected by the statistical protocol used in this study have been shown to be biologically valid and relevant to the systems being studied.

RESULTS
variability in the resolution of protein spots in 2d gels
the resolution of protein spots in a 2d gel is highly variable. it can differ considerably between technical replicates of the same biological sample. samples  <dig>   <dig>   <dig>   <dig>  and  <dig> were the five biological replicates in the treatment group. samples  <dig>   <dig>   <dig>   <dig>  and  <dig> were the five biological replicates in the control group. table  <dig> demonstrates the breakdown of the resolved protein spots in the different samples and its replicates. biological sample  <dig>  for instance, had  <dig> protein spots resolved in at least one of its four technical replicates.  <dig>  proteins occurred in all the four replicates. an additional  <dig>  were present in at least three replicates out of four. a further  <dig> were present only in two replicates out of four, and  <dig>  were present only in one of the four replicates.

low correlation between technical replicates
assessing the quality of a pre-processing technique
in tables  <dig> to  <dig> the spot identification numbers in bold represent proteins that were subsequently identified and found to be biologically relevant to the system being studied . in all, eleven proteins that had significantly different intensities at alpha =  <dig>  were identified. the measure of the quality of a particular pre-processing technique in this study was the proportion of these eleven proteins identified as statistically significant in a two-sample t-test after the particular technique was used.

the effect of log-transformation and minimum values substitution on the distribution of intensities
log – transformation
figures 3a and 3b are the qq plots for the raw spot intensity and normalized spot intensities for the  <dig> protein spots on a representative gel from the control group. these plots demonstrate that the normalization technique used does not alter the basic distribution of the raw data. they also demonstrate the highly non-gaussian distribution of the spot intensities. figure 3c demonstrates that the log transformation converts the distribution of the intensities from a very non-normal distribution to a normal distribution. the points lie very close to the straight line that represents a normal distribution. we have found that the log transformation reduces the skew in the distribution of the spot intensities if the image analysis is done in pdquest® and most data sets produced by the software progenesis®. only one out of seven 2d gel data sets analysed by us so far did not respond well to this transformation. a closer examination of this data showed it had a large number of saturated spots, and thus needed to be rerun. the transformation of the distribution of the  <dig> spots also worked reasonably well at the level of individual spot intensities. this was important to confirm since the two sample tests were done at individual spot level. in cases where there was a considerable skew in the distribution, e.g., ssp  <dig>  the log transformation made the distribution of spot intensities normal. the anderson-darling test for normality for ssp  <dig> in the control group, has a p-value =  <dig>  before the transformation, and p-value =  <dig>  after the transformation. in general, this was true of most spots we examined.

effect of normalization
effect of imputation of missing spots
in table  <dig>  columns  <dig>   <dig>  and  <dig> offer a comparison of the spots identified as significantly different  when the three different kinds of imputation of missing spots were used. the three different kinds of imputation did not make much difference to the spots identified as significantly different in intensities at alpha =  <dig> . in fact in this case we have identical lists in all three columns of table  <dig>  the purpose of using multiple imputation instead of single value imputation, however, has more to do with getting a better estimate of the variance of a quantity than with the correct estimation of the mean. since the t-statistic is a function of both the difference in means as well as the variance of a variable, given a constant mean, underestimating variance would lead to false positives, and overestimating variance would lead to false negatives. this data set shows a good example of the degree of variability on the intensities. table  <dig> demonstrates that single value imputations tend to either underestimate , or over-inflate the estimates of variance . the variance of ssp  <dig>  which has no missing spot intensities, gives us a rough idea of the degree of variability expected in spot intensities when there are no intensities missing. the estimates of variance for the proteins with missing intensities are much closer to the values seen in ssp  <dig> when one uses a multiple imputation technique.

averaging across replicates versus keeping replicates separate
given the lack of association between technical replicates, we used replicate information in two ways: 1) spot intensities were averaged across the replicates so that the t-tests compared average spot intensities in the five treatment samples versus five control samples, and 2) the replicate gels of each sample were treated as independent gels, and the t-tests compared spot intensities in the fifteen treatment gels to the intensities in the fifteen control gels. columns  <dig> and  <dig> in table  <dig> compare method  <dig> above to method  <dig>  six out of eleven proteins are picked up using method  <dig>  whereas all ten are identified as significantly different if the replicates are kept separate.

discussion
normalization
differential sample loading and stain absorption and other process variables can contribute to variability in measured protein intensity. in order to ensure that the detected differences in protein intensity are not due to a "technical" variability introduced by the process of gel creation, spot intensities are "normalized." dividing the intensity of each protein on a gel by the total protein intensity of that gel is a widely used technique to reduce the "individual gel effect" on protein intensities  <cit> . normalizing the data is an important step in many datasets, but it becomes especially important in proteomics experiments, which in general have many more variables than samples. in this case a systemic error in processing samples or gels that affects only one or two gels can have a huge impact on the results. this study has demonstrated that the results of statistical tests are not independent of the normalization technique.

testing for normality and transforming data
to the best of our knowledge, none of the image analysis software packages available to date provide the tools necessary test for the distribution of the data. all of them provide t-tests or the non-parametric wilcoxon rank sum test or anova to test for differences in individual spot intensities. the probability values  for differences between groups are based on the assumptions of the normality of the distribution of spot intensities and equal variances. in order to make an informed judgement about the validity of the p-values of the tests above, it is important to know if these assumptions are met  <cit> . if one uses the averaged spot intensities across gels, the argument could be made that the central limit theorem obviates the need for a log transformation. given the highly skewed nature of the raw spot intensities, the dependence of the mean and the variance of intensities, and the fact that even the non-parametric wilcoxon test assumes symmetry in the outcome variable  <cit> , log transformation of the data is still advisable.

missing spot intensities
despite the fact that 2d-gel technology offers many advantages, one of the pitfalls associated with this technology is the need for several replicates for proper validation of results. there are instances where one does not observe reproducible spot patterns or individual proteins even in replicate gels of the same sample. missing spot intensities are commonly observed in 2d gel datasets. multivariate techniques such as principal component analysis and discriminant analysis  are ideal tools to use on databases that have multiple outcome variables . however, sas or any other statistical software that is used to analyse the data using multivariate techniques such as pca and da requires data sets with non-missing values. gels with missing spot information will thus be dropped from the analysis. since all gels will have some spot information missing, this will result in no gels being available for data analysis. to the best of our knowledge, all image analysis software packages substitute zeroes for missing intensity values. missing intensities may be caused by the fact that a protein spot truly does not exist in one group compared to the other or because the spot intensity is so low that it is not detected by the image analysis software. in this study we treated all missing spots as undetectable spots. the question we were trying to answer was "if a spot exists but is 'undetectable', what is the best 'detectable' value to substitute as its intensity?" the most intuitive value was the smallest "detectable" intensity in the experiment hence we used the lowest intensity value in the experiment for the single value imputation. however, substituting a single value for all missing spot intensities would skew the distribution of spot intensities considerably. thus the second option was to use a random process to substitute missing spot intensities with a plausible set of "detectable" intensities. we created a set of lowest 'detectable' intensities from the lowest intensity on each of the thirty gels. these values were used to impute missing intensity values as described in the methods section. although we have used the terms "imputation" and "multiple imputation", these terms are not to be equated with multiple imputation advocated by rubin  <cit> . rubin's techniques assume the missing intensities to be missing at random . by our assumption, the spots are undetectable because of the intensity level, or the probability of missing is a function of the intensity. this by rubin's definition would make the missing spot intensities non-ignorable non-response. his multiple imputation techniques thus would not be valid in this context.

the issue of missing protein intensities is one that has not been addressed at all in the literature describing 2d gel studies. in this study we treated all missing intensities as the same. however, all missing intensities in 2d gels are not equal. some missing intensities are missing because they truly do not exist in one group versus the other, whereas others are missing because of the inherent variability in the process of creating 2d gels. this suggests that one needs to approach the filling of missing values differently based on the probability of a spot being a truly missing protein or one that is missing due to the process of gel creation. one way to do this would be to assume that spots that occur in two out of three replicates  of a sample are true spots. these missing intensities would then be replaced by the mean value of the remaining two  spots. on the other hand, the assumption that those proteins that are missing in all control gels or all treatment gels are proteins that are turned on or off is justifiable. this in turn suggests that for this set of proteins the random imputation of missing values with a set of plausible minimum intensity values, acting as placeholders so that the non-missing data can be used in analyses, can also be justified.

which method should one use?
in the last few years, published reports of 2d gel analysis have concluded that heart failure was associated with protein modifications in three cellular systems  <cit> , identified proteins expressed in six different regions of brains of alzheimer's disease patients  <cit> , had been used to establish genetic relationships in the brasscacae family  <cit> , to classify human ovarian tumours as malignant and benign  <cit> , in the detection of polypeptides associated with the histo-pathological differentiation of primary lung cancer  <cit> , and to identify eight protein feature changes that differentiated breast cancer cell lines that did or did not form tumours in nude mice  <cit> . these are all important studies that will be used as springboards to launch ever more expensive and sophisticated experiments. we have demonstrated that protein changes that are large  are independent of the statistical protocol used. the identification of more subtle changes can vary widely depending on the statistical algorithm used to pre-process and analyze the data. our experience with the gse data and a couple of subsequent experiments we have been involved in suggests that the algorithm we have developed is more sensitive with respect to identifying biologically relevant proteins that image analysis software might miss. however, it is a fact that pre-processing could also give rise to false positive results. it is important to establish the best statistical protocol for analysing the data from these studies. one way to get around the issue of the effect of pre-processing is to restrict a study to only those proteins that are picked up as significant by image analysis software. as we have mentioned above, in this study a large number of the spots selected by pdquest were poor quality spots either in terms of protein quality or consistency. another option is to consider only those spots that appear in two sets of analyses  as true changes. one is thus restricting oneself to gross changes. as the use of proteomic techniques moves forward, however, we think it will be important to identify more subtle changes in proteins. a number of studies have suggested that change in protein expression that starts the cascade of changes that leads to a diseased tissue need not always be gross or dramatic. subtle changes in expression early in a pathway can cause significant changes downstream. shapiro et al  <cit>  suggest that subtle changes in the spatial or temporal expression of the patterning molecule sonic hedgehog  is linked to the proliferation and patterning of developing limbs. similarly, a disease condition could be caused by small changes in expression in a number of proteins. reneiri et al  <cit>  suggest that the phenotypic expression of retts in some but not all girls with the mecp <dig> mutation suggests that mecp <dig> causes deregulation of a very small subset of genes that have not yet been detected or that very subtle changes in many genes  may cause the neuronal phenotype. the importance of picking up subtle changes in protein expression suggested by studies cited above point to a need to establish a way to identify the optimal statistical pre-processing techniques for 2d gel datasets.

an intuitively appealing way to do this is to create 2d gels with serially diluted quantities of commercially available proteins, establishing the relationship of protein quantity to spot intensity and then proceeding to compare different statistical pre-processing techniques to the datasets acquired from these gels. since it is a known fact that commercially bought proteins may not necessarily be pure and may be present in multiple modified forms due to the process of isolation, the experiment described above may be enhanced by using controlled biological samples from a cell culture with at least a hundred resolved spots. once again, the sample could be loaded in on gels in known concentrations and the process described above would be repeated. clearly, the set of pre-processing techniques that picks up differences that come closest to the true differences would be chosen as the optimal techniques. some recent publications have used similar techniques to establish the validity of emerging proteomic technology. alban et al  <cit>  used an escherichia coli lysate "spiked" with varying amounts of four different known proteins to test a novel experimental design that exploits the sample multiplexing capabilities of dige by including a standard sample in each gel. rabilloud et al  <cit>  compared the staining sensitivity of rubps and sypro ruby of serial dilutions of molecular weight markers. however, there are no designed experimental studies that have looked at the impact of statistical pre-processing or the effectiveness of various statistical techniques on the conclusions drawn from 2d gel experiments. given the proliferation and promise of 2d gel experiments, we suggest that the need to conduct these validation experiments is urgent.

in this study we have used the proteins that were subsequently identified by maldi-tof spectroscopy as a measure of how well particular statistical protocols perform. we concede that there is an inherent bias in that the spots that were identified by maldi-tof were selected on the basis of the protocol described in this paper. the described protocol therefore will seem to perform much better than others in this comparison. this however does not diminish the main thrust of this paper, which is that statistical protocols affect the conclusions drawn from a 2d gel experiment.

CONCLUSIONS
this study has demonstrated that the pre-processing of the data from 2d gel experiments can have a significant impact on the results of statistical tests. the purpose of the study was not to identify the particular statistical protocol used in this study as the optimal protocol, but rather to demonstrate that the results and conclusions from a biological experiment are not independent of the statistical protocol. the study has in effect looked at three different statistical protocols. the one described in figure  <dig>  the one described in figure  <dig>  in which the averaging of intensities across replicates allows one to proceed directly on to the t-tests, without the steps of testing distributions, or imputation of missing spots, and the protocol used by pdquest. allowing for the inherent bias we have described above, this study shows that the protocol described in figure  <dig>  with normalization technique  <dig>  and multiple imputations, is superior to the method used by pdquest, or the one in figure  <dig>  given the possible bias in this study, the larger conclusion from the study is that there is a great need for research into developing optimal statistical methodology to analyze data from 2d gel experiments.

