BACKGROUND
the three-dimensional  organization of a genome is closely linked to its biological functions; therefore, it is important to gain a full understanding of the genomic structure. in recent years, assays developed to identify long-range chromatin interactions genome-wide, coupled with next generation sequencing technology, revolutionize research in genomics and epigenetics. the most well-known assay for detecting chromatin interaction, hi-c  <cit> , produces data that identify pairs of fragments in close proximity to each other in the cell nucleus, and are commonly organized into a two-dimensional matrix  of contact counts. in addition to hi-c, other assays for detecting genome-wide long-range interactions have also been developed, such as chia-pet  <cit> , tcc  <cit> , and single-cell hi-c  <cit> . most recently, in-situ hi-c was debuted, achieving  <dig> kb resolution with  <dig>  billion contacts  <cit> . furthermore, it shortens the experimental time considerably from the original hi-c experiment.

a number of analytical approaches have been proposed to recapitulate the underlying 3d structure, with most of them developed for hi-c data. these approaches can generally be classified into optimization based and modeling based. most of the optimization-based approaches are two-step procedures. the idea is to first translate each pairwise contact count into a distance using a biophysical property, essentially stating an inverse relationship. one then obtains a consensus 3d structure by minimizing some objective function, such as the total “differences” between the translated distances and those inferred from the hypothesized 3d architecture . chromsde  <cit>  is an example in this category, but it also estimates the degree of the inverse relationship in addition to the 3d coordinates in the optimization step. further, the distances between pairs with  <dig> contact frequencies are penalized so that they are more separated in the 3d space. shrec3d  <cit>  also falls into this category, except that in the first step, the contact counts are converted to distances by not just applying the inverse relationship of the biophysical model, but by also finding the “shortest path” connecting two nodes on a weighted graph. many of the optimization methods are based on metric or non-metric multi-dimensional scaling to minimize the objective function  <cit> .

modeling-based approaches, on the other hand, are based on probability models that describe the relationship between the contact counts with the 3d physical distance. the contact counts are modeled either by a normal distribution to account for variability in the estimation  <cit>  or by a poisson distribution, as in bach  <cit>  and pastis  <cit> , with its intensity parameter assumed to be related to the physical distance by an inverse relationship. statistical inferences on the 3d structure  are made either by maximum likelihood  <cit>  or through casting the problem into a bayesian framework  <cit> . although independence assumption is not needed for optimization-based methods, this assumption is central for existing model-based methods. this leads to the concern that these models may provide a poor fit to the data as dependency among frequencies in the contact matrix are expected. furthermore, methods that rely on the poisson distribution fail to capture potential over-dispersion in sequencing data.

most of the methods discussed above were proposed and tested with dilute hi-c data at  <dig> mb resolution for human lymphoblastoid cell line  <cit>  or at  <dig> kb resolution for mouse embroyonic stem cell line  <cit> . as discussed above, with the advancement of the technology, higher and higher resolution data are being produced at a faster rate. for a given resolution, regardless of whether it is high or low, the data are organized into a 2d matrix of contact counts. however, as the resolution gets higher and higher, the 2d matrix gets sparser and sparser, that is, with a higher proportion of zeros in the entries of the matrix. for example, for the human lymphoblastoid cell line gm <dig>  <cit> , the proportions of zeros at resolution  <dig> mb,  <dig>   <dig>   <dig>  and  <dig> kb are approximately  <dig> % , <dig> , <dig>  and  <dig> %, respectively, for intra-chromosomal data. as such, questions arise as to whether existing methods tested for lower resolution data are still appropriate and effective for analyzing higher resolution data, and what maybe the likely impact of higher resolution on the methods. for example, as mentioned above, chromsde  <cit>  takes special care of zero contact counts, and therefore it would be of interest to evaluate whether it continues to perform well when challenged with a large proportion of zeros. on the other hand, for a model-based method such as those relying on the poisson distribution, the underlying distribution may no longer be adequate for modeling data with excess of zeros. specifically, if the proportion of zeros is much higher than the theoretical probability of getting a zero for a poisson distribution that otherwise fits the non-zero frequency counts, then the model will be a poor fit for the data that include the zeros. a recently proposed truncated poisson architecture model  is an attempt to address this issue, but its appropriateness for higher resolution data with a majority of contact counts being zeros  has not been evaluated. more seriously, as with the other model-based methods discussed above, tpam also requires the independence assumption and fails to accommodate overdispersion.

in this article, we propose a truncated random effect expression  model, which not only uses a truncated distribution to accommodate excess of zeros in higher resolution data, but also adds a random effect component into the model for counts. thus trex is expected to be robust to resolution specification, takes dependencies between contact counts into consideration, and addresses the issue of over-dispersion. by doing so, trex can achieve greater consistency with observed data. to thoroughly investigate the performance of trex and compare it with a number of current methods, we carried out an in-silico study. we also applied all methods to a human lymphoblastoid cell line hi-c dataset for which fish measurements are available to substantiate estimation accuracy.

RESULTS
we first describe the design and results from an in-silico study that generates data from a known 3d structure that serves as the “gold standard”. a total of  <dig> different  poisson contact frequency intensity models are considered. these results are based on synthetic datasets to allow for controlled settings to test methods and quantify relative merits. we emphasize the impact of resolution on the methods throughout the presentation of the results. results for the application to a human lymphoblastoid cell line hi-c dataset are presented following those for the in-silico study.

an in-silico study
using an existing estimated structure  <cit>  as the “gold standard”, we consider several scenarios that mimic various data resolutions. this 3d structure is selected as it is estimated from real data and as it depicts two topologically associated domains  ), an important feature for gauging the relative performance of the methods subjected to the “stress test”. two versions of the random effect model  and three sets of model parameters are used to generate the contact matrix. moreover, five different proportions of zeros among contact counts, roughly representing five levels of data resolution, are considered:  <dig> % ,  <dig> % ,  <dig> % ,  <dig> % ,  <dig> % . this setup leads to a total of  <dig> simulation settings. detailed data simulation process is described in the methods section. in addition to trex, the following methods are also subjected to the stress test: tpam  <cit> , bach  <cit> , pastis  <cit> , shrec3d  <cit> , and chromsde  <cit> , with the first three being model-based methods like trex, and the remaining two being optimization based. three aspects are evaluated:  estimation accuracy of the 3d coordinates of the estimated structure,  consideration of how well the two tads are preserved in the estimated structure, and  computational time.


estimation accuracy of 3d coordinates
we first discuss estimation accuracy of the 3d coordinates using two criteria: rmsd and correlation. briefly, rmsd first computes the squared deviations between the estimated 3d coordinates and the generating coordinates, averages them, and takes the square root of the average. a more detailed description is provided in the methods section. correlation, on the other hand, simply computes the pearson correlation coefficient between the estimated and the true coordinates. both criteria measure estimation accuracy, but with emphases from two different angles; thus both are good measures of performance of a method.

we present the results for data simulated under one set of model parameters. first we consider the nre model. for each method and each resolution, we summarize the results across all replications in a boxplot, one for rmsd and one for correlation. the results for all methods and all resolutions  are presented in fig.  <dig>  from these plots, one can see that bach  is affected the most by resolution, with both its rmsd increasing ), while its correlation decreasing exponentially ). this is not surprising as bach was not specifically proposed for data at higher resolution and thus could not handle an excess of zeros in the contact matrix. although the two truncated model-based methods, trex and tpam, appear to be slightly affected by resolution, they perform much better than bach, at all levels of resolution, with trex edging out tpam consistently under the rmsd criterion. this is expected since the truncated poisson model is specifically designed to be robust to the proportion of zeros. on the other hand, the optimization-based method, shrec3d, does not seem to be affected by resolution, with rmsd and correlation stay fairly constant through out the entire range, but its performance is inferior to trex and tpam uniformly across all levels of resolution. instead of being similar to its sibling optimization method, the behavior of chromsde is in fact quite similar to the modeling-based methods trex and tpam: as resolution increases, its performance deteriorates slightly. this feature may be due to its special handling of the zero counts as discussed above. the poisson-based likelihood inference method, pastis, on the other hand, has rather unpredictable behaviors. the rmsd generally decreases but then increases very slightly as the data resolution increases, contrary to the results of all the other methods. this behavior is also observed for the correlation measure. therefore, for higher resolution data, pastis may slightly outperform trex, even though trex is much better at lower resolution. nevertheless, the variability of the results from pastis increases dramatically  with the increase of resolution, casting a great deal of uncertainty about its results. overall, we see that all methods maintain over  <dig> % correlation across all resolution, except for bach, whose correlation drops down to about  <dig> %  with about  <dig> kb resolution.
fig.  <dig> boxplots for comparing 3d estimation accuracy of six methods under nre model and parameter setting  =. the comparison are for data simulated from the nre model based on two criteria:  rmsd, and  correlation. for each resolution/percent zeros, the six boxplots are for trex, tpam, bach, shrec3d, chromsde, and pastis, in that order



the results for the st models, given in fig.  <dig>  paints a clearer picture of the better performance of trex. for the rmsd criterion ), all methods appear to be only slightly impacted by resolution, with trex having smaller rmsd compared to the other methods, which all seem to perform similarly . with respect to correlation ), we can see that, like the data simulated under the nre model, bach is once again affected by resolution the most, with its correlation dropping down to only barely above  <dig> % with data at about  <dig> kb resolution. in contrast, the correlation for trex stays over  <dig> %, with shrec3d and pastis  being the best among the rest of the methods. these observations generally hold for the other two sets of model parameters .
fig.  <dig> boxplots for comparing 3d estimation accuracy of six methods under st model and parameter setting =. the comparison are for data simulated from the nre model based on two criteria:  rmsd, and  correlation. for each resolution/percent zeros, the six boxplots are for trex, tpam, bach, shrec3d, chromsde, and pastis, in that order

fig.  <dig> underlying 3d structure and its estimates. a the 3d structure depicted was used for simulating data in the in-silico study. the red and green balls denote two topologically associated domains that are well separated in the 3d space. b the estimated structures depict results from one representative replicate simulated under the st model. for each of the six methods considered, there are five estimated structures, each for the five levels of resolution/percent zeros



in summary, trex achieves consistently good performance in all settings. although some of the other methods may perform better at a particular resolution with a particular setting of model or parameters, trex is always among the best  in all  <dig> combinations of model, parameter setting, and resolution level under both evaluation criteria.

although figs.  <dig> and  <dig> provide clear visualizations of the results, we further quantify the performances by comparing trex’s results with those from each of the other methods by carrying out wilcoxon signed-rank tests for the same model and parameter settings. the results  are presented in table  <dig>  the top segment of the table ) provides the p-values for the nre model. these results provide formal statistical confirmation for our observations from fig. 1: trex has significantly smaller rmsd and significantly larger correlation than bach and shrec3d, but not necessarily uniformly significantly better than tpam, pastis, or chromsde under both criteria for all resolutions. for the data simulated from the st model, the bottom segment of the table ) shows, without a doubt, that trex has statistically significantly smaller rmsd and larger correlation than any of the other methods across all levels of resolution. the results for the other two sets of model parameters are similar .p-values of wilcoxon signed-rank tests comparing the performance of trex with each of the comparison methods for the model with parameters =

the results for the nre model are given in the top segment  and those for the st model are given in the bottom segment 



tad preservation
we next investigate the performance of the comparison methods for preserving the two topologically associated domains in the underlying structure. as we can see from the underlying structure depicted in fig.  <dig>  the two domains  are well separated, with loci within each domain cluster together tightly. as such, we use average silhouette width  <cit>  as a measure of how well the domains are separated. specifically, we compute the ratio of the average silhouette width of the estimated 3d structure to that of the underlying structure, and a larger value is indicative of better separation of the two tads. as we can see from the results of one parameter setting presented in table  <dig>  for data simulated under the nre model, trex, tpam, and chromsde are similar in their ability to identify the two topological domains, with their ability slightly decreased with higher resolution data. in contrast, shrec3d performed steadily across all levels of resolution, but its ability to separate the two tads is not as good as the other three methods just discussed. bach, on the other hand, sees its performance degraded quickly as the resolution gets higher. as seen earlier, the performance of pastis is rather unpredictable. it does not performed as well as the other methods for low resolution data, but its performance gets much better with the increase of resolution, especially for mid-level resolution data. for data simulated under the st model, trex has higher average silhouette ratios across all levels of resolution compared to the other five methods, among them, bach continues to perform the worst as resolution increases. results for the other two parameter settings are similar . as an example, we display in fig.  <dig> the structures estimated by all six comparison methods across the resolutions for a representative dataset simulated under the st model. as we can see from the figure, when the resolution is low , all methods but pastis recover the two tads in the underlying structure as shown in fig.  <dig>  however, as the resolution increases, one can see that the two tads are preserved only in the structure predicted by trex at the highest resolution considered . once again, bach has the worst performance, which started to have trouble separating the two tad at the  <dig> kb  resolution.


computational time
computational feasibility is a major concern for genomic data, but the concern is even greater for chromatin interaction data as the size of the data is o when there are n genomic loci, an order of magnitude increase compared to analysis of linear chromosomal data. for model-based methods , the computational time is typically longer as most methods are based on markov chain monte carlo  sampling, a computationally intensive technique. optimization-based methods are usually much faster. among modeling-based methods, those based on a truncated distribution  will have an advantage as its computational cost can be greatly reduced for data with a higher percentage of zeros through excluding the zero counts. to illustrate this, we present some computational time analysis in table  <dig> for one set of model parameters.

*for pastis and shrec3d, the typical computational time is less than  <dig> min. however, pastis took several min to run for a few of the replications



as we can see, shrec3d and pastis run much faster than the rest of the methods for each simulated data set. chromsde comes in next, although not an order of magnitude better as in shrec3d and pastis. bach’s running time is constant regardless of the percentage of zero contact counts. on the other hand, trex and tpam’s computational time reduces as the percentage of zeros increases since the number of data points needed to be analyzed decreases. chromsde appears to have a similar trend, perhaps due to its special way of handling zero count data. it is noted, though, that the computational time presented in table  <dig> is simply an illustration on how the percentage of zeros may affect different methods differently. indeed, an increase in resolution typically lead to an increase in the number of zeros in the contact matrix. however, the dimension of the contact matrix also increases for higher resolution data. as such, the decrease in computational time for trex and tpam with an increase in the percent of zeros does not mean that trex and tpam will take less time to analyze higher resolution data.

analysis of human lymphoblastoid hi-c data
we applied trex to the hi-c data produced by  <cit> . in fact, there are two hi-c experiments performed on the same karyotypical normal human lymphoblastoid cell line, which are combined into a single data set in our analysis given their high reproducibility  <cit> . for comparisons, we also run the other methods, tpam, bach, pastis, shrec3d, and chromsde. we focused on chromosome  <dig> and  <dig>  as experimental validation data based on fluorescence in situ hybridization  are available for several pairs of loci on these two chromosomes and are publicly available  <cit> . specifically,  <cit>  discussed interesting features of spatial interactions, based on the fish measures, among  <dig> loci on chromosome  <dig>  and  <dig> loci on chromosome  <dig>  using the fish experiment. in particular, the spatial 3d distance between l <dig> and l <dig> was observed by the fish experiments to be smaller than that between l <dig> and l <dig>  despite the fact that l <dig> is farther apart from l <dig> than from l <dig> in terms of their linear 1d distances. a similar observation was made for , in that the spatial 3d distance between l <dig> and l <dig> is smaller than that between l <dig> and l <dig>  the resolution used is  <dig> mb, which leads to  <dig> loci in chromosome  <dig> and  <dig> loci in chromosome  <dig>  that is, a total of  <dig> loci for a join analysis of both chromosomes.

to make it possible to compare results across different methods with the fish measurements due to different scaling factors, we first standardize all the distances . for greater ease of digesting the information, we calculated, for each of the  <dig> pairs of loci for which fish measurements of distances are available, the absolute difference between the median of the fish measurements and the median from each of the methods . from the table, we can see that trex has the smallest average absolute difference from fish . shrec3d’s estimated distances can sometimes be fairly different from those of fish; for example, all four distance estimates on chromosome  <dig> are outside of the middle  <dig> % of the fish measurements, with one being outside of the range of all  <dig> measurements. this is reflected in its average difference from fish being more than  <dig> times of that for trex . the other optimization-based method, chromsde, performed better, but still, all four distance estimates on chromosome  <dig> are also outside of the middle  <dig> % of the fish measurements resulting in its average difference from fish being more than  <dig> times of that for trex . finally, other than shrec3d, all methods predicted the spatial distance between l <dig> and l <dig> to be shorter than that between l <dig> and l <dig>  and that between l <dig> and l <dig>  consistent with the fish results. for shrec3d, its estimated distance between l <dig> and l <dig> is indeed shorter than l <dig> and l <dig> but longer than l <dig> and l <dig>  for the distance between l <dig> and l <dig> compared to l <dig> and l <dig>  all methods provided consistent predictions as fish.

aby design, we standardized all distances so that the median distance between loci l
 <dig> and l
 <dig> is  <dig> for all the methods . thus, the difference between each method and fish is zero


baverage is averaging over all pairs



the observations of the relative performance of the methods are also confirmed by the correlation plots of the estimates with the fish measurements ). we considered three measures of correlation, person’s, spearman, and kendall, with the latter two more suited for a non-linear relationship. all three correlations confirm that trex performed the best overall, with the highest correlation among all methods; even the smallest correlation, kendall’s correlation, is at  <dig> %. in comparison, the kendall’s correlation can be quite low for most of the other measures:  <dig>   <dig>   <dig>   <dig>  and  <dig> % for tpam, bach, pastis, shrec3d, and chromsde, respectively. on the other hand, pearson’s correlation is the highest for most of the method:  <dig>   <dig>   <dig>   <dig>   <dig>  and  <dig> % for trex, tpam, bach, pastis, shrec3d, and chromsde, respectively.
fig.  <dig> results from analysis of human lymphoblastoid cell line hi-c data. these plots depict correlation between the median distances from the estimates and the fish measurements. the nine points on each plot corresponds to the nine pairs of loci. a trex;  tpam;  bach;  pastis;  shrec3d; and  chromsde



discussion and 
CONCLUSIONS
three-dimensional organization of a genome has gained a great deal of attention in recent years. because the structure is intimately linked to the biological functions of the genome, especially in long-range gene regulation, it is important to gain a greater understanding of the structure so that its relationship with key genomic features, such as histone marks, can be ascertained. the experimental data, organized into a 2d matrix, has special features, the most important ones include correlation among contact counts in the 2d matrix and the high proportion of zero contact counts . to address these issues, in this paper, we propose trex that takes correlation between read counts into consideration. in addition, over-dispersion, known to exist in next generation sequencing read count data, is also accommodated in trex. the most important feature of trex is that it is robust to resolution specification.

an in-silico study demonstrates these properties by showing that trex is much more data consistent regardless of the resolution level and the underlying model from which the data were simulated. the study further shows that trex performs at least as well as any of the current state-of-the art methods, both optimization-based and modeling-based, and in fact outperforms in most of the scenarios considered. even when the data were simulated from the poisson model , trex was still among the top performers. when correlation among contact frequencies was introduced, trex’s advantage was clearly seen, as it was not only able to take dependencies into account, but also to accommodate overdispersion introduced by the correlation. in all, a total of  <dig> combinations of random effect, model parameters, and resolution level were considered, and trex achieved consistently good performance in all settings. the results indicate that trex is not only robust to resolution level, but it is also robust to model mis-specification and insensitive to parameter specifications. the comparisons are in terms of 3d structure estimation accuracy and preservation of topologically associated domains. however, trex is slower computationally compared to the two optimization algorithms and pastis.

application of trex to the human lymphoblastoid cell line data on chromosomes  <dig> and  <dig> led to the construction of a 3d structure that is consistent with the fish measurements of distances on several pairs of loci. it is shown that the corresponding distances predicted by trex have higher correlation with the fish measurements than any of the five comparison methods. using the fish measurements as the “gold standard”, this result indicates that trex is a viable alternative to other existing methods.

computational intensity is an important issue that deserves further discussion. as they are currently implemented, trex, tpam, bach, and chromsde are all too computationally intensive to recover the 3d structure genome-wide in a single run. nevertheless, compared among model-based approaches, trex and tpam are more apt for handling higher resolution data as well as inter-chromosomal data, as the sparse feature of the data helps with improving computational feasibility. pastis is a model-based approach that is computationally very efficient, but its performance appears to be unpredictable and unstable based on our observations from the simulation , and thus further evaluation is warranted. ultimately, faster algorithms and further methodological innovations are needed for genome-wise 3d reconstruction, especially for higher resolution data. one idea is to use a subset of loci from each chromosome, “the anchors”, to build an overall architecture of the genome-wide 3d structure. then a structure for each chromosome will be constructed and transformed to fit into the overall architecture, confirming to the positions of the anchors from the corresponding chromosome. whether this strategy will work warrants further investigation, but it is out of the scope of the current paper.

