BACKGROUND
it is a crucial step in drug discovery to identify targets  for novel drug candidates  <cit> . however, testing a large number of candidates in experiments would be still costly in both money and time. based on the well-accepted assumption that similar drugs tend to interact with similar targets, computational methods  are able to predict novel drug–target interactions  on a large scale by drug similarities and/or target similarities.

generally, there are four scenarios of predicting interactions  <cit>  between  known drugs and known targets;  new drugs and known targets;  known drugs and new targets; and  new drugs and new targets. here, a known  drug is the drug having one or more known interactions with a targeted protein; a known target is the target interacting with one or more approved drugs; a new drug, referred to as a “drug candidate”, has no any known interaction; and a new target is the potential target having no known interaction with any drugs. it is remarkable that the appropriate cross validations for different scenarios should be adopted when assessing computational approach. otherwise, over-optimistic results are perhaps obtained  <cit> .

since a set of known dtis can be represented as a bipartite graph  <cit> , network inference-based algorithms  are applied to predict new interactions by analyzing the topology of this graph. however, these algorithms cannot work well when the inference involves the drugs and/or the targets having no connection to the graph . matrix factorization algorithms have also been performed on the adjacent matrix of dti graph to predict potential dtis  <cit> . they suppose that a drug and a target may interact with each other if they share similar features in a common latent feature space . however, it is still hard to apply this approach in s <dig>  s <dig>  and s <dig>  which are corresponding to the well-known cold-start problem. in addition, matrix factorization usually bears larger computational complexity.

supervised classification models have been gained many concerns in dti prediction , because they are able to handle all predicting scenarios and have the advantage of elucidating explicitly why a drug interacts with a target. former supervised models can be approximately grouped into two categories: local classification model  and global classification model . lcm considers that the interactions between drugs and a focused target or between targets and a specific drug follow a common distribution . in contrast, gcm follows the assumption that dtis crossing all drugs and all targets follow a common distribution . usually, gcm needs much more memory space than lcm, because it always operates kronecker product on both drug similarity matrix and target similarity matrix.

in common, supervised classification models regard existing dtis as positive instances and unlabeled drug-target pairs  as negative instances respectively.

however, unlabelled dtps include many unapproved dtps as well as few potential dtis which are not approved yet. in fact, few out of unlabelled dtps are true dtis but not collected when people extract dti datasets. we call both potential dtis and uncollected dtis as missing dtis in the context of dti prediction because they have no technical difference.

from the point of view of supervised learning, a well-trained supervised classification model should have a decision boundary that separates positives and negatives significantly. in addition, an appropriate performance measure is crucial to fairly reflect the power of predicting models. thus, missing dtis would cause three important issues. firstly, directly regarded as negatives, they induce a confusing decision boundary in the trained model, which usually cannot separate positives and negatives clearly. secondly, they also cause a biased decision boundary in the classifier, by which positives tend to be determined as negatives. thirdly, they are always discriminated as highly-scored false positives by predicting model, so as that the performance under existing measures is sensitive to missing interactions. though some of the former approaches  provided a start to address the first issue, they have not yet addressed the remaining two issues.

this work focuses on the prediction in scenario s <dig>  to address the first two issues, we shall first introduce two strategies, spy and super-target, and further integrate them to form a two-layer local classification model. then, to cope with the third issue, we shall present a new performance measure, coverage, to assist the assessment of predicting model on the data containing missing dtis. in addition, we shall demonstrate the superiority of our approach by comparing with two former approaches, one semi-supervised approach and one supervised approach, respectively.

methods
dti prediction as locally supervised classification
the interactions between m known drugs  and n known targets  could be organized by an interaction matrix am × n, in which a =  <dig> if there is a known interaction between drug di and target tj, and a =  <dig> otherwise. in s <dig>  given a new drug dx, the task is to predict the interaction between dx and a known target tj or determine how likely it interacts with tj. it can be also treated as a problem of regular supervised classification as follows:  labelling the known drugs di as a positive instance if a =  <dig> , or as a negative instance ;  training the classifier with the labels of {di} and their pairwise similarities;  discriminating dx as a positive or a negative instance in binary, or assigning it with a confidence score of being a positive instance by the trained classifier. the confidence score will be directly used to evaluate the performance of dti prediction .

the classifier used in our approach is regularized least squared  classifier because its training only involves the solution of a linear system and its prediction at new testing samples is very elegant  <cit> . usually, for a testing sample x , its score of belonging to the j-th class is generated by the trained rls classifier as follows,  <dig> fjx=kxxtrnkxtrnxtrn+λi−1yj, where fj is the predicted score of x, k is the kernel matrix directly derived from drug similarity matrix, the 1 × m matrix k contains pairwise similarities between dx and m known drugs, the m × m matrix k contains pairwise similarities between m known drugs, λ is the regularization parameter , i is the m × m identity matrix and yj is the m ×  <dig> class label vector of training samples corresponding to the j-th column of adjacent matrix . in the context of predicting interactions, fj is just taken as dx’s confidence score s of interacting with tj.

spy strategy for identifying reliable non-dtis 
since unlabeled drug-target pairs contain missing dtis, simply regarding them as negative instances may cause a bad classifier  which has a confusing decision boundary between positives and negatives. in other words, it cannot separate newly-coming positives and negatives clearly. aiming to recover those hidden positive instances in the unlabeled instances, we first utilized a semi-supervised strategy, spy  <cit> , to identify the reliable negatives  from unlabeled instances. since rn is significantly different to positives, a better classifier of less confusing boundary can be trained by positives  and rn .

denote the set of labeled instances  as p and all the unlabeled instances as u. a small set of labeled instances, s, are randomly selected from p and injected into u. name the remaining labeled instances in p as p’ and the union set of u and s as u’ respectively. behaving similarly to the unknown positive instances in u, the instances in s act as “spy” instances in u’. therefore, they allow us to investigate the behavior of the unknown positive instances in u by the following steps:  the instances in p’ and u’ are labeled as positives and negatives respectively to build an ordinary classifier;  each instance in u’ is assigned with a predicting score of being a positive by this trained classifier;  the minimum of the scores of “spy” instances is taken as the threshold t to identify rn;  the instances in u having the scores less than t are determined as rn;  a new classifier is finally trained on p and rn to perform dti prediction. figure  <dig> illustrates this strategy.fig.  <dig> spy strategy to identify reliable non-dti. positives , unlabeled instances , spy instances taken from positives, and reliable negatives  identified among unlabeled instances, are rendered with red bars, gray bars, red frames and blue bars respectively. the traditional classifier is trained by positives and unlabeled instances . the spy-based classifier is built by injecting s into u and investigating the behavior of s to identify rn. the final classifier is trained with positives and rn



because the drug similarity matrix used in local classification model is unique, we assume that the thresholds of identifying rn in different local classifiers are identical. in practice, we estimated this unified threshold from the target which has the maximum number of drugs interacting with itself. the procedure is listed as follows:  randomly selecting 10 % “spy” instances out of p to determine the threshold,  repeating the selection ten times and obtaining ten thresholds, and  averaging the ten thresholds as the final threshold.

super-target strategy for grouping similar dtis  as many as possible
in dti data, most of the targets may interact with only one or very few drugs. for example,  <dig> out of  <dig> targets in enzyme dataset interact with less than  <dig> drugs , and  <dig> targets interact with only single drugs respectively  <cit> . in this case of instance imbalance, the training would build a classifier having a biased decision so as that the testing instances tend to be discriminated as negatives. missing interactions aggravate the imbalance between few positives and many negatives.

inspired by compound screening, super-target strategy is able to increase the number of positives by grouping similar targets and their interacting drugs  <cit> . a group of similar targets is named a super-target . super-target creates an additional layer of classifiers, which should further be incorporated into a regular local model to form a two-layer local model because our final goal is to determine how likely dx interacts with tj. in this layer, a classifier of less biased boundary is trained under the condition of as many dtis as possible. it is especially usefully in the case that no or only a few similar drugs interact with individual targets while more similar drugs interact with the super-target of those individual targets  <cit> .fig.  <dig> super-target strategy to gather more similar dtis .  dti graph and its adjacent matrix.  the groups of similar targets and their corresponding columns in the adjacent matrix.  drug-supertarget interaction graph and its adjacent matrix. the graph  and the adjacency matrices  are different ways to represent the interactions between four drugs and five targets. the drugs , targets  and super-targets  are labeled by the names starting with “d”, “t” and “st” respectively. similar targets are in the same color. the adjacency matrix between the drugs and super-targets  is obtained from the original dti matrix  by performing the union operation on the columns corresponding to targets belonging in the same super-target



to predict how likely dx interacts with stq, an ordinary classifier at the level of super-target is built by labeling all drugs linking to super-target stq as positives and those not linking to it as negatives. likewise, the confidence score s of dx interacting with stq can be calculated by formula .

however, the original super strategy in  <cit>  did not consider that drugs interacting with all the target in a super-target are significantly different. the interactions between these drugs and a super-target are called the fake interactions of super-target, which may cause a bad classification boundary. we identified fake interactions by the following rule. if a drug interacting with a super-target st, cannot find any of its top-k nearest neighbors  among other drugs interacting with st, its interaction with st is marked as a fake. after checking all the super-targets with this rule, we removed the fake interactions to obtain the cleaned super-targets which group similar positives as many as possible.

two-layer local classification model
we integrated two strategies for missing interactions to form a two-layer local classification model. for target tj, in the top layer, the model clusters similar targets into super-targets and predicts how likely dx interacts with a super-target stq which contains tj by a traditional supervised learning. then, in the bottom layer, it predicts how likely dx interacts with tj ∈ stq via the spy-based classifier built by positives and rn. the final confidence score of dx interacting with tj is calculated by  <dig> sdxtj=sdx,stqsdx,tj∈stq 

because some super-targets may contain fewer similar positives and more dissimilar positives, an adaptive rule was designed to determine when to apply super-target:  for each drug interacting with stq, counting the occurring number nuq of its k nearest neighbors  among other drugs interacting with stq;  for all the drugs interacting with stq, averaging the occurring numbers by ñq = ∑u = 1unuq/u, where u is the number of drugs interacting with stq;  rejecting the super-target strategy if ñq < k/ <dig>  otherwise accepting it.

such combination has the following advantages. super-target enables the training under the condition of as many similar dtis as possible, so as to relax the instance imbalance and generate the less biased decision boundary. meanwhile, training with positives and reliable non-dtis, spy strategy guarantees a less confusing decision boundary between dtis and non-dtis.

assessment
the validation for predicting interactions between known targets and new drugs should be appropriately designed  <cit> , otherwise the prediction is over-optimistic . we followed the five-fold cross validation  in  <cit>  to evaluate the proposed approach. the drugs in a given dataset are randomly split into  <dig> non-overlapping subsets of approximately equal sizes. out of the five subsets, one is used as the testing set and the remaining subsets are taken as the training set. this process is then repeated five times, to use each of the five subsets as the testing set in turn.

in each round of cv, the predicting performance is usually measured by the area under the receiver operating characteristic curve . the corresponding average of auc in five rounds is taken as the final indicator of assessment. in practices, auc is defined as  <dig> auc=n′+ <dig> n″/n′+n″, where n′ and n″ are the numbers of the cases that positives are larger than and equal to negatives in terms of predicting scores respectively. the larger, the better. deriving from the traditional supervised classification, the calculation of auc has the need to compare all positives with all negatives. auc reflects how often positives are greater than negatives on the average in terms of the scores assigned by the predicting model.

like compound or target screening, computational approaches predict potential interactions  from unlabeled drug-target pairs by selecting the instances of high scores as the candidates. thus, missing dtis in the testing negative instances would be recognized as potential interactions if they are assigned with high scores . however, they are usually counted as false positives in assessment since they are simply labeled as negatives. unfortunately, the calculation of auc cannot reflect the situation. in consequence, there is a need for another performance measure to assist the assessment.

a new measuring index, coverage, is presented to additionally assess the prediction on the data containing missing interactions. it is defined as follows  <dig> coverage=1p∑i=1pmaxt∈tirankdit− <dig>  where p is the number of the testing drugs and ti denotes the set of the targets interacting with di. the smaller, the better. when predicting interactions for a new drug, coverage is able to evaluate how far, on the average, we need to go down the target list in order to cover all the proper targets of the drug.

we believe that the number of unlabeled positives  is small, and most of them are assigned with the scores higher than the lowest scores of known dtis. in other words, coverage can embrace as many missing interactions as possible even though they are treated as negatives during the assessment. therefore, coverage is more robust than auc when missing interactions occur. a toy example of calculating the values of auc and coverage is shown in fig.  <dig>  totally, a good predicting approach is able to generate high auc as well as low coverage.fig.  <dig> a toy case showing the difference between auc, aupr and coverage. the first row denotes an interaction profile between a drug and  <dig> targets. the second row denotes an interaction profile with one missing interaction by removing the 9-th interaction from the first row. the third row contains the predicted scores generated by performing a predicting approach on the second row. the last row lists the ranks corresponding to the predicted scores. the values of auc, aupr and coverage accounting for the first row, are  <dig> ,  <dig>  and  <dig> respectively. in contrast, after labelling the missing interaction as a positive correctly, those value of auc and aupr  would change to  <dig> ,  <dig> , but the value of coverage doesn’t change. aupr is significantly sensitive to the missing interaction, auc is moderately sensitive and coverage is the most robust



RESULTS
data
the adopted datasets in our experiments were originally collected by  <cit>  and further used in subsequent works  as the benchmark. all the dtis in the original work were split into four subsets in terms of the type of protein targets, including enzyme, ion channel, gpcr and nuclear receptor. here, the four dti datasets are shortly denoted as en, ic, gpcr and nr respectively. former publications generally used chemical-structure-based drug similarities and sequence-based target similarities respectively when predicting dtis  <cit> . the pairwise drug similarity was measured by aligning the chemical structures of two drugs  <cit> . the pairwise target similarity was derived from by smith-waterman alignment  <cit> . more details can be found in the original work  <cit> . because the drugs having different structures may interact with common targets and the proteins having different sequences may be targeted by common drugs, we also adopted additional non-structure-based drug similarity and non-sequence-based target similarity which were proposed in  <cit> . the new pairwise drug similarity was calculated by comparing the class labels of two drugs according to anatomical therapeutic chemical  classification system. the new pairwise target similarity was calculated by comparing the functional categories of two targets according to the annotation of hugo gene nomenclature committee . the details of similarity calculation can be obtained in  <cit> . the final drug/target similarity matrix we used was just the average of the new similarity matrix and the previous similarity matrix. based on dti adjacent matrix, drug similarity matrix was used to train classifiers and target similarity matrix was used to form super-targets. the datasets used in the following experiments can be downloaded from the web address provided in  <cit> .

the effectiveness of individual strategies and their combination
to validate the effectiveness of two proposed strategies and their combination, we first run the ordinary local model ; then run the model incorporated with spy strategy alone ; after that, run the model extended by only super-target strategy ; last, combined two strategies and run the model again . the performance was measured by both auc and coverage . compared with rls, rlsm_spy is better because of the less confusing boundary generated by positives and reliable-negatives at the level of targets. rlsm_super outperforms rls as well because of the less biased boundary generated by grouping similar dtis as many as possible at the level of super-targets. as expected, rlsm_comb wins the best among above all approaches and improves the prediction significantly . these results demonstrate the effectiveness of our strategies for missing interactions.table  <dig> comparing local model with the models having spy and purified super-target strategies respectively

 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
the bold entries denote the best results on the benchmark datasets



comparison with other approaches
furthermore, our approach was compared with two recent approaches, netcbp  <cit>  and kronrls  <cit>  . both of them adopted the same 5-cv to validate the prediction. netcbp, a semi-supervised approach, only used auc to measure its performance, while kronrls, an ordinary supervised approach, used auc as well as aupr  to measure its performance. their results were generated by only using chemical-structure-based drug similarity and sequence-based target similarity  <cit> . to make a fair comparison with these approaches, using exactly same similarity matrices, we run the proposed two-layer local model and assessed it by both auc and aupr . in addition, we listed the results obtained by combined similarity matrices  together. the comparison shows that our approach is superior to these two approaches .table  <dig> comparison with other approaches

 <dig> | <dig> 
 <dig> | <dig> 
 <dig> | <dig> 
 <dig> | <dig> 
 <dig> | <dig> 
 <dig> | <dig> 
 <dig> | <dig> 
rlsm-comb-less is our approach running with chemical-structure-based drug similarity and sequence-based target similarity. the bold entries denote the best results in terms of both auc and aupr 



analysis on auc, aupr, and coverage
different performance measures should be applied in appropriate cases. in a supervised classification problem, when the number of negatives greatly exceeds the number of positives, auc is an optimistic measure  <cit> . such instance imbalance in dti prediction is significant. in this case, aupr is more appropriate than auc since it performs great penalty on highly-scored false positive predictions  <cit> . however, such penalty may cause pessimistic results because those highly- scored false positives are possibly unlabeled positives mixed in negatives.

a toy example in fig.  <dig> illustrates how missing interactions influence the values of auc, aupr, and coverage. suppose that the values of auc , aupr  and coverage  are calculated respectively when the interaction between the drug and the ninth target is missing. remarkably, when the missing interaction is found back, the corresponding values are  <dig> ,  <dig>  and  <dig>  accordingly. obviously, aupr is quite sensitive to missing interactions assigned with high predicting scores since aupr’s value changes sharply. in contrast, auc is moderately sensitive and coverage is the most robust . this is the reason why we didn’t use aupr but use both auc and coverage to assess the prediction when investigating the strategies for missing. when assessing the performance of a predicting approach, one should keep above points in mind. the further analysis about coverage is depicted in the next section.

message from coverage
to elucidate the message behind coverage, we compared our approach with two boundary-line approaches, random prediction, and oracle prediction. the former randomly assigns the confidence scores to all drug-target pairs, while the latter supposes both known interactions and unlabeled pairs are perfectly labeled with 1’s and 0’s respectively. since the values of auc generated by these two boundary-line approaches definitely ~ <dig>  and  <dig>  respectively, we only focused on their values of coverage, which denoted as crandom and coracle respectively. these two values were further used to normalize the coverage value of the proposed approach into . the normalized coverage is defined as nc = /. the smaller, the better. in addition, we calculated the ratio  between coverage and the number of targets as well. both of nc and this ratio facilitate the comparison across different datasets of varied sizes .table  <dig> message from coverage

#t is the number of targets in dataset, c/#t is the ratio between coverage and #t, crandom is the coverage derived from random prediction, and coracle.is the coverage corresponding to oracle prediction and nc is the normalized coverage



three messages can be observed from the results in table  <dig>   rlsm-comb is significantly better than random prediction in terms of coverage value.  the significant difference between rlsm-comb and oracle prediction highlights the needs to extract better drug similarities and to develop better models for dti prediction.  most importantly, related to the cost of screening, both nc and c/# t reflect that ~20 % out of all the targets along the candidate list, on the average, should be checked to cover all the proper targets interacting with the drug. therefore, coverage is able to indicate the predicting performance more informatively than auc or aupr.

CONCLUSIONS
in this paper, when predicting potential targets for new drugs under the framework of local classification model, we have addressed three important issues caused by missing dtis. first, simply treating directly unlabeled instances as negatives would cause the confusing decision boundary between positives and negatives. to cope with it, we have adopted a semi-supervised strategy, spy, which can identify reliable non-dtis  from unlabeled dtp  by investigating the behavior of dti  among unlabeled dtp. thus, spy enables the training to be under the condition of positives and reliable negatives, so as to generate a less biased decision boundary.

secondly, directly aggravating the toughness of few positives, missing dtis also cause a biased decision boundary which tends to predict newly-coming positives as negatives. to address it, we have adopted the strategy, super-target, to cluster similar targets as well as the drugs interacting with them. super-target creates an additional layer of the spy-based local classification model. in this layer, a classifier of less biased boundary is trained under the condition of as many dtis as possible. according to the number of similar drugs interacting with a super-target, we have also introduced the adaptive combination of two strategies to form a two-layer predicting model.

thirdly, existing measures  of predicting performance do not consider missing interactions, which are always assigned with high scores by predicting models and counted as false positives in assessment. as a complementary, we presented coverage which is robust to highly-scored missing interactions. besides, it enables us to evaluate how far we need to walk along the list of targets in order to visit all the proper targets of the queried drug.

in short, having less confusing and less biased decision boundaries at the levels of target and super-target respectively, the proposed two-layer model first predicts how possibly a new drug interacts with a super-target, then predicts how possibly it interacts with a member target contained in that super-target, and its performance is assessed by the new measure , in addition to the traditional measure .

finally, based on four real benchmark datasets, we have demonstrated that our approach is able to not only cope with missing interactions but also perform superiorly to two other approaches with respect to the problem of predicting interactions between known targets and new drugs. moreover, our approach can be applied to the symmetric predicting scenario s <dig> as well.

nevertheless, we are still walking on a long road. the performance of dti prediction, especially measured by coverage, reminds us that predicting potential targets for new drugs still remains a tough challenge. more efforts on drug similarity metric, as well as the predicting model and the appropriate assessment for missing interactions, should be done.

abbreviations
5-cv, five-fold cross validation; atc, anatomical therapeutic chemical; auc, the area under the receiver operating characteristic curve; aupr, the area under precision-recall curve; dti, drug-target interaction; dtp, drug-target pair; rls, regularized least squared; gcm, global classification model; lcm, local classification modelv; rn, reliable negative.

