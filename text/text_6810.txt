BACKGROUND
differential expression analyses of microarray data  <cit>  typically ignore any correlation between genes. in this paper we consider a model for microarray data which explicitly includes correlation structure between genes and we explore its implications for estimation and significance testing.

the model presented below involves the use of large sparse inverse covariance matrices  <cit>  and an associated graphical representation of the inverse covariance matrix  <cit>  which we use to encode the  relationships between genes. we discuss the estimation of mean and covariance structure, including the problems of determining the pattern of zeroes in the inverse covariance matrix and fitting the matrix to data once the pattern has been determined. for the purposes of hypothesis testing we will describe a permutation procedure  <cit>  to test the significance of a hypothesis overall as well as a breakdown into components involving differential expression and "differential connection".

RESULTS
the model
consider p expression measurements, measured on n individuals, arranged in an n × p data matrix x. in addition, each individual is subject to known "treatments". we assume that individual i is subject to a treatment  given by row i of an n × r design matrix d and that the p gene expression measurements for each individual have common covariance matrix Σ. if we denote the operation of making a vector from a matrix row by row by vec{...}, then we can write the joint model for this data set as

  vec{x} ∼n 

where b is an r × p matrix of treatment effects and ⊗ denotes the tensor product. from  it is easily seen that the ith row xi and jth column xj of x have distributions

  xi. ∼ nx.j ∼ n 

where σjj is the jth diagonal element of Σ, di. is the ith row of d and b.j is the jth column b. for those more familiar with stacking matrices column by column, see the methods section. from  we see that each row of x has a multivariate normal distribution with mean structure dependent on the treatment and covariance structure defined by Σ. similarly each column of x has a mean structure defined by the design matrix d and variance structure a multiple of the identity matrix, a typical structure in regression models.

in the above we consider Σ to be a function of some parameters θ. in particular, we will assume that Σ- <dig>  the inverse covariance matrix, is sparse and the parameters θ correspond to the nonzero elements of the inverse covariance matrix. such matrices define  gaussian graphical models , see for example  <cit>  and  <cit> . these models have conditional independence interpretations. writing xi for the ith variable , x-i for the vector of the remaining variables, μi for the mean of variable i and σij for the i,jth element of the inverse covariance matrix Σ- <dig> we have

  e{xi|x−i}=μi−∑j≠iv{xi|x−i}=− <dig> 

where e denotes conditional expectation and v conditional variance.

the interpretation of zero elements of the inverse being equivalent to regression coefficients ) being zero makes this class of models attractive for analysing microarray data as it provides useful information about the  interrelationships between genes. we define the set of neighbours of variable  i to be the set of variables with non zero regression coefficients in  above.

an undirected graph can be associated with any pattern of zeros in the inverse covariance matrix by the relation: there is an edge between vertices i and j if and only if σij ≠  <dig>  where vertex i denotes variable xi. an example of this is given in figure  <dig> below.

the cliques, maximal sets of vertices which are all connected, of the graph are { <dig>  2}, { <dig>  4}, { <dig>   <dig>  5}, and { <dig>   <dig>  5}. from this graph we can see for example that the regression of variable  <dig> on the rest has nonzero regression coefficients for variables  <dig> and  <dig> 

unlike traditional microarray analysis  <cit> , the above model will enable the analysis of microarray data in a way which makes use of correlations between genes and respects the idea of genes being connected in a network.

note that the model in this section is an example of a  mean linear hierarchical mixed graphical model as defined in  <cit> . see also the supplementary information .

parameter estimation
to implement the model described in section  <dig> above we require estimates for the matrix parameter b and for the non-zero entries of the inverse covariance matrix Σ- <dig>  we discuss these topics below.

estimating parameters in the mean structure
we use maximum likelihood to estimate the parameter matrix b. from , the log likelihood function is

  l=f− <dig> *vec{x−db}tvec{x−db}   =f− <dig> *trace{Σ−1t} 

where f denotes a function of Σ- <dig> independent of b. using matrix calculus  <cit> , and differentiating this expression with respect to b we obtain

  ∂l/∂b=−vec{dtΣ−1} 

it follows that the maximum likelihood estimate of b is

  b^=−1dtx 

from equation  we see that the estimates of the columns of b are simply obtained by individual regression of each column of x on the design matrix d.

estimating parameters in the variance structure
the discussion in this section involves computationally intensive methods aimed at discovering  relationships between genes. it is precisely this information which is ignored in traditional microarray analysis.

in principle, estimating the parameters in Σ- <dig> involves two computationally demanding problems. firstly, identifying the pattern of zeroes, and secondly, estimating the inverse covariance matrix for a given pattern of zeroes. the difficulties are caused by the high dimensionality of the data e.g. the number of genes can be of the order of p =  <dig> or more. for example, with p =  <dig> there are approximately  <dig> million unique elements in Σ- <dig> to work with. without care, attempting to do these tasks can result in very large memory requirements and very long cpu times. in this high dimensional setting standard methods for these tasks become very slow or even infeasible. however, some progress can be made and we begin by discussing methods for determining the pattern of zeroes in the next section below.

determining the pattern of zeroes
there are two common methods for determining the pattern of zeroes for a given a data set. the first method involves computing p individual regressions of each variable on the remaining variables. this is intuitively reasonable given our earlier discussion about the interpretation of elements of the inverse covariance matrix in terms of regression coefficients, see equation . the regression method used for the individual regressions could incorporate a sparsity penalty, as in the zero pattern finding method in  <cit> , or simply be some form of consistent stepwise variable selection, either using a forward stepwise variable selection method as in  <cit> , or a combination of forward and backward selection with a modified bic criterion  <cit> . a simple forward stepwise regression algorithm is described in the supplementary information . major advantages of these methods in the high dimensional setting are the ability to use existing software and to easily distribute the problem over multiple processors. however some care is required to avoid overfitting.

a second class of methods is maximum likelihood estimation with l <dig>  constraints on the elements of sigma inverse. these methods accomplish simultaneous model selection and fitting, see for example  <cit>  and  <cit> . a likelihood approximation with l <dig> constraint is considered by  <cit>  and  <cit> . note that if we use these methods as a pattern selector, we still may wish to compute maximum likelihood estimates of parameters for the selected pattern of zeroes.

of the above methods, the method of  <cit>  is a good benchmark for problems with a few thousand variables. however, this second class of methods is not well suited for data with tens of thousand of variables or more, both from the viewpoint of memory requirements and cpu time. the method of  <cit>  transforms the problem into a series of l <dig> regressions which are solved efficiently via a coordinate ascent procedure. unfortunately, experiments have shown that, in the case of very large numbers of variables, the overhead in creating these ll regression problems is too large and the cyclic updating procedure can converge very slowly for problems with realistic structure, see for example . the methods of  <cit> ,  <cit>  and  <cit>  are implemented in r ) as packages glasso, space and spice. they clearly are not designed for very high dimensional problems as they use dense matrix computations. in addition, convergence for a regularisation parameter value of  <dig> can be a problem, in particular when the sample covariance matrix is not full rank. as a consequence, high dimensional problems will not run on a desktop computer, and there are other problems as well. for example, the current implementation of the likelihood approximation method space doesn't allow specification of a pattern of zeroes in Σ- <dig> a priori. individual iterations for a fixed regularisation parameter must be done instead and can be very slow for models with large numbers of variables. the specification of a sparse model can also be clumsy,requiring vectors identifying all zero elements.

however, in the interest of computing speed, simplicity and easy accessibility, we propose using an efficient forward stepwise algorithm as implemented in the r package lars  <cit>  coupled with a modified bic criteria. one modified bic criterion  <cit>  is

  bicr=nlog)+klog+2γk) 

where s σ <dig>  is the maximum likelihood residual variance estimate for a linear regression model with k predictors,  <dig> ≤ γ ≤  <dig> and  = p!/!k!) denotes the number of subsets of size k when there are p variables to chose from. note that when γ=  <dig>   corresponds to the usual bic. in the case of very many more variables than observations the recommended value of γ is one. we also compared an alternative version of bic defined by

  bic2p=nlog)+k2log 

see  <cit> .

to determine the zero pattern in Σ- <dig> we adopt a simple strategy. for each gene we do forward variable selection up to a pre-specified model size kmax , considering all other genes as potential predictors i.e. for each column i in turn, we use forward selection to chose predictors for the ith column of the expression matrix x from amongst the remaining columns using a forward stepwise algorithm. we then choose the model size with minimum modified bic as in  or . each of these regressions contribute to a sparse neighbour matrix a defined by

 aij={ <dig> if gene j chosen as a predictor of gene i <dig> otherwise 

finally the zero pattern is determined by computing n = a + at and setting all diagonals and nonzero entries to  <dig> in the resulting matrix.

clearly we can use other regression strategies such as l <dig> constrained regression in a similar manner, however they will typically be at least  <dig> to  <dig> times slower in terms of computing time if cross validation is used to select the regularisation parameter.

simulation study
we conducted a simulation study to explore the properties of the forward stepwise procedure outlined above. choices are somewhat constrained by the difficult tasks of simultaneously controlling the median neighbourhood size, signal to noise ratios and positive definiteness for very large matrices. the simulations are also cpu intensive.

sparse inverse covariance matrices with known structure were generated as follows.

 <dig>  generate the p × p neighbour matrix a by randomly selecting q elements of each row to be non-zero. compute the matrix n = a + at, set the diagonals and all non-zero entries of the resulting matrix to one. note that as a result of the last step, neighbour sizes can and do vary from the selected q. the parameter q can be varied to control the median neighbour size. i.e. the median number of non-zero entries in a row of Σ- <dig> excluding the diagonal.

 <dig>  construct Σ- <dig> as follows. set the non-zero upper triangular elements of Σ- <dig> to be the same as those of n. generate each individual non-zero value from a uniform distribution over the interval ∪. . finally, symmetrise the matrix.

 <dig>  set the ith diagonal of Σ- <dig> to be the sum of the absolute values of the elements in the ith row excluding the diagonal, times a constant d,  <dig> < d ≤  <dig>  chosen so that the resulting matrix is still positive definite. this was done by stepping down from  <dig> in steps of . <dig> until the matrix was no longer positive definite. the smallest value for which the matrix was positive definite was chosen. the motivation for this step is to improve the typically poor signal to noise ratios in diagonally dominant matrices.

the simulation considered all combinations of the three factors

• median neighbour size m =  <dig>  10

• number of genes p =  <dig>   <dig> and 20000

• sample size n =  <dig>   <dig> and 500

for each combination of median neighbour size m and number of genes p, inverse covariance matrices were constructed as described above. multivariate normal samples of sizes of  <dig>   <dig> and  <dig> were then drawn from each of these. sparse matrix calculations are essential for this step. the zero pattern finding procedure was applied to each data set with different versions of bic and the resulting structure compared to the known structure. this entire process was repeated  <dig> times.

a typical covariance matrix for m =  <dig> had 90% of neighbour sizes in the range  <dig> to  <dig> with maximum size  <dig>  and for m =  <dig>  90% of the neighbour sizes were in the range  <dig> to  <dig> with maximum neighbour size  <dig> 

results for the case m =  <dig> are given in figure  <dig> below.

we note a dramatic improvement in the false negative rate as sample size increases. the results for the usual bic and bic <dig>  are very similar and consistently produce the best false negative rates. they also produce similar false positive rates, however they are now consistently worse compared to bic <dig>  and bic2p.

results for m =  <dig> are given in figure  <dig> below

the same qualitative patterns as in figure  <dig> are evident in figure  <dig>  note however that the false negative rates are much higher here for m =  <dig>  this can be partly explained by differences in the number of observations per parameter ≈ n/m in each regression relationship, this ratio being twice the size when m =  <dig>  the remaining difference may be explained by differences in the signal to noise ratios for the two cases, a factor which appears to be difficult to control precisely when generating these large covariance matrices. for an interesting discussion of the effect of the relationship between m and n and p and n in the ability to recover regression relationships in high dimensional data see  <cit> .

tables  <dig> and  <dig> below give the mean confusion matrices and standard deviations in the case m =  <dig>  p =  <dig> and n =  <dig> for bic <dig>  and bic <dig> . results for other combinations are qualitatively similar.

standard deviations in brackets.

standard deviations in brackets.

it is clear from these tables that the improved false negative rate in table  <dig> compared to table  <dig> comes at the expense of a very large number of false positives. on the basis of the above simulations we recommend the use of bic <dig> . this version of bic seems to have good control of the false positive error rate, a fact also noticed by  <cit> . the ratio m/n then appears to determine the false negative rate and our main source of error will be the inability to detect relationships as opposed to wrongly detecting non existent relationships.

estimating the non-zero elements of sigma inverse
given the estimate of b in  above, we can write the log likelihood function as

  l={log det−1)−trace−1s)} 

where s = rt r/n and r=x−db^. when the parameters θ correspond to the nonzero elements of Σ- <dig>  differentiating l with respect to theta and equating to zero we see that the likelihood equations can be expressed as

  σij=sij if σij≠ <dig> 

where, for example, σij denotes the ijth element of Σ and σij denotes the ijth element of Σ- <dig>  for details see  <cit>  and . from  we see that, at the maximum likelihood estimate, the elements of the estimated covariance matrix must be equal to those of the sample covariance matrix s whenever there is an edge between i and j in the graph and the elements of the estimated inverse covariance matrix must simultaneously be zero whenever there is no edge between i and j in the graph.

a necessary and sufficient condition for existence of the maximum likelihood estimate is that the sample covariance matrices restricted to the variables in the cliques of the graph associated with the model are all positive definite,  <cit> . this is almost certainly true provided the clique sizes are small relative to the sample size.

solving the likelihood equations requires special care when p is large. sparse matrix representations are required to minimise memory requirements. we use the methods of  to obtain maximum likelihood estimates for the high dimensional setting in this paper.

significance testing
testing hypotheses about b should really take into account the correlations between gene expression measurements. in this section we consider how to do this. our tests will be conditional on the fitted  covariance matrix.

beginning with , suppose we have an r × s matrix of orthogonal contrasts c, with  <dig> ≤ s < r and we wish to test the hypotheses

  ctb =  <dig> 

for example, in a study with n treatment subjects and m control subjects, we might have

 d= 

where 1k is an k ×  <dig> vector of ones. a contrast matrix of interest in this case with s =  <dig> might be ct=/ <dig> in which case we are interested in testing for treatment differences relative to controls.

we can re-parameterise the problem so that  corresponds to testing for zero values in a new parameter matrix as follows. expand c into an orthogonal matrix q such that

 q =  =  

where a is r × . from  we can write

  db = dqqtb      = d˜Γ 

where d˜=dq and Γ = qtb. if we partition Γ and d˜ conformably with q so that d˜= and Γ = , then  now corresponds to Γ <dig> =  <dig> in our new parameterisation.

from the result that

  Γ^=−1d⌢txvec{Γ^}=−1d˜t ⊗i)vec{x} 

it follows that vec has distribution

  n−1⊗Σ) 

under the null hypothesis, Γ^ <dig> has distribution

  n 

where  <dig> is an s × p matrix of zeroes and g−1=− <dig> is partitioned as

  . 

from  we define our test statistic for the null hypothesis Γ <dig> = ctb =  <dig> to be

  t=vect−1vec= vectvec−1Γ^2Σ−1)= trace{−1Γ^2Σ−1Γ^2t} 

now writing - <dig> = llt, we can write  as

  t=trace 

where Γ˜2=ltΓ^ <dig>  the diagonals in the trace can be decomposed as follows. let γ = t denote the kth row of Γ˜ <dig> then

  γτΣ−1γ=∑i=1pγii=∑i=1pγiσiiβijγj)=∑i=1pσiiγii 

where βij = -σij/σii, ndenotes the neighbours of variable  i and i=γi−∑j∈nβijγj are the neighbour corrected contrast values.

we could attempt to use the chi squared distribution to derive significance levels for t, however, due to the fact that Σ will be estimated and most likely contains specification errors, we will instead use permutation distributions.

motivated by  <cit>  we propose the following procedure for generating permutation distributions..

 <dig>  fit the mean model under the null hypothesis that Γ <dig> =  <dig>  this gives fitted values

  f^=d˜1−1d˜1tx=d˜1Γ^ 

 <dig>  compute the residuals r under the null hypothesis

 r=x−f^. 

 <dig>  for k =  <dig> ..,q, generate new data sets x according to

 x=f^+pkr 

where each pk is a randomly chosen permutation matrix

 <dig>  compute Γ^ using , namely

  Γ^=−1d^tx 

 <dig>  build up an empirical null distribution of t in  above using the Γ^ <dig> from step  <dig> above. at the same time we can also build up empirical null distributions for each of the components of Γ <dig> and Γ2nb = Σ-1Γt which make up the test statistic t in .

it is easy to see that for our model the residuals in step  <dig> above have distribution

  vec{r} ∼ n−1d˜1)⊗Σ) 

hence for any permutation matrix pk we have

  vec{pkr} ∼ n−1d˜1pkt)⊗Σ) 

when d˜1=c1n it can be seen that  and  are identical. such is the case, for example, when testing for the equality of the means of all groups in a study, see below. more generally, if the elements of d are bounded and /n→v for some positive definite matrix v as n tends to infinity then  tends to  as n tends to infinity since

  d˜1−1d˜1t=d1−1d1t=n−1/2d1−1d1tn−1/ <dig> 

to avoid complicated notation, in  above we have omitted a subscript n on d <dig> taking the dependence on n as understood.

in this paper the use of the normal distribution can be regarded as convenient way of keeping track of linear operations on means and covariances. however the results can all be interpreted as simply depending on means and covariances i.e. first and second order moments independent of specific distributional assumptions.

testing the components of Γ2nb = Σ-1Γt is a new element which for want of a better term might be called testing for differential connection. testing the individual components of Γ <dig> is analogous to testing for differential expression.

null distributions for the individual components of the first equation in  can also be derived by permutation to test for significantly large sub components. the method described above can also be used to generate null distributions for the two components involving γ in the last equation of . testing for the second component i is the new element due to the correlation between genes. to understand this second component, from equations  to  above, in the non null case, we can write

      γ ∼ nΣ−1γ ∼ n 

where γ0t is the kth row of ltΓ. some simple calculations show that

  i ∼ nβijγ0j,1/σii) 

where, for example, γ0i is the ith component of γ <dig>  under the null hypotheses γ <dig> =  <dig>  or equivalently, Σ-1γ <dig> =  <dig> the mean in  is zero. if this hypothesis is rejected then the expected contrast value at gene i is not "smooth" i.e. a specified weighted linear combination of neighbouring contrast values. we have termed such a case differential connection. intuitively, if nothing "unusual" is happening local to a specific gene, then we expect its contrast value to be roughly equal to a weighted linear combination of its neighbouring contrast values.

note that testing for no differential expression can be regarded as testing a hypothesis concerning the marginal distribution of a particular gene whilst testing for no differential connection is testing a hypothesis concerning the conditional distribution of a gene given all the other genes, see the section on mixed graphical models in the supplementary information  information for more details.

a simple example of using differential expression and differential connection to generate hypotheses for further investigation is given in the supplementary information .

a two group example
we illustrate some of the ideas above with an example involving n <dig> samples from a treatment group and n <dig> samples from a control group where the interest is in testing for equality of the treatment and control group means. here the design matrix d, after suitable reordering can be represented as

 d =  

and the contrast matrix c is defined as ct=/ <dig>  the orthogonal completion q is

 q= <dig> 

the matrix d˜ is

 d˜= <dig> 

writing n = n <dig> + n <dig>  we also have

 i−d˜1−1d˜1=i−1n1nt/nf^=1n1ntx/nr=xpkr=pkx−1n1ntx/n 

hence it can be seen that step  <dig> of the method for generating permutation distributions given above simply involves permuting the rows of x. the statistic t in  is a scalar multiple of tΣ− <dig> where for example β^ <dig> is the p ×  <dig> vector of sample means for the treatment group.

example
we use the smoking data of  <cit> , with n =  <dig> subjects and p =  <dig> gene expression measurements. there are two classes  <dig> smokers and  <dig> non smokers. we used the zero pattern finding strategy  defined earlier to determine the zero pattern in the inverse covariance matrix. for the regressions, the maximum neighbour size for each gene was restricted to  <dig> giving a ratio of observations per parameter  of approximately 1/ <dig>  the actual neighbour size distribution had minimum value  <dig>  maximum value  <dig> and 90% of the neighbour sizes were in the range  <dig> to  <dig>  the clique size distribution is given in table  <dig> below

using  <dig> permutations of the rows of the data matrix x we obtained the null distribution of the test statistic t∝ dtΣ^−1​d for testing the equality of the two class means, where d is the vector of differences of the means of the two classes. the value of the test statistic for our data was  <dig> . the quantiles of the null distribution were table 4

so we can see that test value is highly significant leading us to conclude that the mean expression values of the smokers and non smokers are different. a histogram of the null distribution is given in figure  <dig> below.

we also derived null distributions for the two components of the last equation in . testing the first of these is equivalent to testing for differential expression in this case.

we used the modified bonferroni procedure of  <cit>  to adjust for multiple testing. when testing m hypotheses the usual bonferroni procedure with parameter α rejects all hypotheses whose p values is less than α /m where  <dig> <α <  <dig>  the modification of  <cit>  allows, α >  <dig> in which case α is an upper bound on the expected number of false rejections i.e. false positives. this procedure exhibits strong control of the per family error rate under any dependence between p values. for details and a comparison with the benjamini-hochberg procedure see  <cit> . in setting the α parameter here, we note that we are performing approximately  <dig> tests, and if we are willing to accept an expected number of false positives of  <dig> i) the significance level to use in the tests is 8/ <dig> = . <dig> 

applying the above procedure,  <dig> differentially expressed genes were identified and  <dig> differentially connected genes were identified. of the  <dig> genes identified by  <cit>  as being differentially expressed using t-tests,  <dig> were also identified as differentially expressed using permutation tests and the procedure described above.

with this analysis, any gene of interest can be displayed in the context of a gene network. define a neighbourhood of size r for any particular gene to be the set of genes which can be reached from this gene by traversing r edges in the associated graph derived from Σ- <dig>  for example, a neighbourhood around each differentially expressed gene could be generated to identify interesting relationships. figure  <dig> below shows the neighbourhood of size  <dig> of the differentially expressed gene aldh3a <dig> 

a related method for constructing local networks near genes of interest is given by  <cit> . its focus is on creating local networks near a gene of interest, however, unlike the method described in this paper, it does not provide a joint model for the data or even a locally consistent model i.e. a positive definite covariance matrix. note that figure  <dig> is derived from a global consistent model.

the gene aldh3a <dig> is also differentially connected. figure  <dig> below displays the relationship of this gene to its neighbours rps6ka <dig>  ptpn <dig> and tkt. in the plot the null distributions for the contrasts for each of the genes is presented as a boxplot.

the red line joins the actual observed values of the contrast between smokers and non smokers for each of the genes. the genes to the right of the dotted line are the predictor genes for aldh3a <dig>  the associated regression coefficients are given at the bottom of the plot. the value  <dig>  is the variance of the error in the regression model. note that the actual observed weighted average of the contrasts of the predictor genes is much lower than expected

from this plot the role of tkt and possibly rps6ka <dig> in the expression of aldh3a <dig> in smokers and non smokers needs to be investigated. other explanations for this result such as post transcriptional processes may also be suggested.

restricting attention to the differentially expressed genes, the subgraph defined by these genes has  <dig> clusters of connected genes. table  <dig> below gives the distribution of these clusters by size of cluster.

a literature search revealed that all but  <dig> of the genes in figure  <dig> are known to be connected. some of the functions of genes in this sub network are xenobiotic metabolism,  and regulation of oxidant stress and glutathione metabolism, known to be important in cigarette smoking, see  <cit> .

graphical queries such as which is the closest differentially expressed gene to a specified gene and which is the shortest pathway between two given genes can also be answered.

mixed graphical models based on trees and forests have also been used by  <cit>  to analyse microarray data. software for their approach is described in  <cit> .

discussion
the parameter kmax in the strategy for determining the pattern of zeroes would not be necessary if we had large sample sizes. unfortunately, in practise, this is not usually the case. simulations in this paper and  support the general conclusion that more connections are detected and the bias and variability in the estimates of Σ- <dig> is reduced when the ratio kmax/n decreases. based on limited evidence to date, a tentative upper limit on this ratio would be about 1/ <dig> which corresponds to  <dig> observations per parameter in the largest regression models. it is easy to see that for any given example, a model derived from a smaller value of kmax will produce a sub-model of one derived from a larger kmax. determining the pattern of zeroes and fitting Σ- <dig> will typically be faster for smaller values of kmax. there are also other reasons one might wish to limit the size of kmax. for example, as a preliminary exploratory analysis, it would not be unreasonable to look for only a few of the strongest connections between genes to ovoid being overwhelmed by large numbers of network connections.

the inclusion of correlations into the analysis via sparse inverse covariance matrices comes at a substantially increased computational cost. the three main computational steps are

 identifying the pattern of zeroes in the inverse covariance matrix

 fitting the inverse covariance matrix and

 computing permutation distributions

steps  and  can be done in a day or so on a single desktop machine. a single lars stepwise fit stopped at kmax terms requires o operations and the computation of  requires o) operations, see  <cit>  and . however these steps are also very easy to parallelise and so can be speeded up with very little effort if more processors are available. depending on the structure of sigma inverse, step  can also be performed in a day or less. however, there are cases when this step is more difficult and can take longer. we are currently exploring ways of parallelising this calculation as well as a promising alternative optimisation method. another approach, given a pattern of non-zeroes in sigma inverse, could be to estimate the elements of Σ- <dig> simply by regression i.e. a regression of each gene on its neighbours. the equations βij=−, vi2=1/σii where βij is the regression coefficient of gene i on gene j, and vi <dig> is the residual variance for the regression then provide a means for "estimating" the elements of Σ- <dig>  however such an analysis would at best be an approximation since the estimated Σ- <dig> may not be exactly symmetric, nor positive definite. none the less it could be a method worth exploring as such a method would be asymptotically consistent if the neighbour structure was correct.

the advantage of the maximum likelihood estimate of sigma inverse is the possibility of doing simulations, for example of the likely effects of controlling the expression of specified genes.

the method can generate many interesting hypotheses involving the connections between genes, explanations for differential expression in terms of neighbouring genes and connected pathways, and places in the network where connected genes are acting unusually.

note that the extension to the case that the contrast matrix c in  is full rank rather than orthogonal is straightforward.

r code implementing the methods of this paper is freely available in the library mvama, see  <cit> .

CONCLUSIONS
there is a wealth of information about relationships between genes in a microarray experiment which is currently underutilised. in this paper we have present a practical strategy for accessing some of this information. we have presented a method for incorporating correlations between genes into the analysis of microarray data. a by-product is a method for the analysis of differential expression which does not require the empirical bayes model of the traditional approach of  <cit>  nor any need to estimate the number of differentially expressed genes a priori. the new approach produces a gene network for all the genes and allows differential expression to be placed within the context of the gene network.

future work
in future work we hope to consider transformations of expression data to make it have an approximate multivariate normal distribution, a comparison of different methods for identifying the pattern of zeroes in sigma inverse and faster algorithms for fitting the inverse covariance matrix.

