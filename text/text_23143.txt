BACKGROUND
dermoscopy is a prevalent method used by dermatologists in the diagnosis of melanoma and other pigmented skin lesions. dermatologists use a handy, high-resolution imaging tool called dermatoscope to take dermatoscopic images. dermoscopy is now a well-established diagnostic tool to improve the clinical recognition of a broad spectrum of various skin disorders. skin cancer detection is the most important indication of dermoscopy. there is evidence that the use of dermoscopy has increased the accuracy of diagnosis  <cit> . carli et al.   <cit>  showed that the examination of a pigmented skin lesion including melanoma using dermoscopy allows physicians to realize morphologic features which are otherwise not visible to the naked eye. this in turn, comparing to conventional non-dermoscopic examination, allows physicians to reach a more reliable diagnosis of skin lesions. thus, recent melanoma guidelines promote the use of dermoscopy in skin cancer screening and diagnosis  <cit> .

skin cancer is the most common form of cancer in the us and over  <dig>  million cases are diagnosed annually  <cit> . the deadliest form of skin cancer is melanoma. melanoma is a malignancy of melanocytes which are special cells in the skin located under the outer surface epidermis. 15% of melanoma cases are fatal  <cit> . women at 25- <dig> years of age are the most-commonly affected group  <cit> . although melanoma accounts for only 4% of all skin cancers  <cit> , it is the cause of 75% of skin-cancer-related deaths  <cit> . even with the help of dermoscopy, 70% of melanoma claims are still a false-negative diagnosis  <cit> . melanoma rates are rising amongst all demographics  <cit> . with early detection, melanoma can often be cured with a simple excision operation.

dermoscopy is a set of techniques for optical magnification of a region-of-interest on skin which makes subsurface structures more visible than traditionally photographic techniques  <cit> . the procedure measures many properties of a skin lesion, such as color, size, symmetry, border, and change over time. the odds of successful diagnosis between naked eye examination and dermoscopy are 15- <dig>  <cit> . even when dermoscopic images examined by an expert, diagnosis rates are not completely accurate.

an expert system capable of processing dermoscopic images could provide an additional diagnosis tool to aid dermatologist. in many common manual methods of examining photographs of lesions, a border is drawn by a dermatologist, and this border is 'subjectively' analyzed by dermatologist to diagnosis if the lesion is malignant melanoma or melanocytic. similarly, automated systems designed to processes dermoscopic images usually start with automatic border detection before examining the lesions for the features with diagnostic importance such as color, symmetry, etc.  <cit> . there are many methods for detecting the border of a lesion. the blue color channel is typically examined because empirical evidence suggests it provides the most accurate results  <cit> . reader is referred to  <cit>  for details on analysis of color models and color channels on biomedical image processing.

border detection is usually the first stage of analysis of dermoscopic images. our implementation presented here automatically delineates the lesion border by using density based clustering technique. in order to get a clear definition of the lesion, some preprocessing, such as color space transformations, contrast enhancement, and artifacts removal are typically applied to the image  <cit> . following this pre-processing, a partitioning of the image occurs in a process known as segmentation. these disjoint regions are then examined by computer algorithms and scanned for lesion data, and combined to detect the border of the entire lesion.

according to celebi et al.  <cit>  automated skin lesion border detection can be divided into four sections: pre-processing, segmentation, post-processing, and evaluation. the pre-processing step involves color space transformations  <cit> , contrast enhancement  <cit>  and artifacts removal  <cit> . the segmentation step involves partitioning of an image into disjoint regions  <cit> . the post-processing is used to obtain the lesion border  <cit> . the evaluation involves the assessment of the border detection results by a dermatologist. at the first stage of dermoscopy image analysis, border detection is usually applied  <cit>  to detect other features more accurately. an active contour model is also introduced to detect skin lesion borders in dermoscopy images  <cit> . many other approaches have been applied to dermoscopy images for accurately segmenting the melanocytic skin lesions. color histogram thresholding is one of them  <cit> . in peruch et al.  <cit>  in addition to thresholding, researchers incorporate cognitive process of dermatologists for accurate melanocytic skin lesion segmentation.

density based clustering algorithms identify regions of high data density, and require a definition of how dense the data should be  <cit> . density based spatial clustering of applications with noise, or dbscan  <cit> , as its name indicates, is a prominent density based clustering method. it is used for spatial data with noise and has the advantages of being able to find irregularly shaped clusters. dbscan also has a sense of border and noise data, and requires minimal knowledge of dataset  <cit> . it requires two inputs: minimum points, a measurement of how many points need to be grouped; and epsilon , a measurement of how close the points need to be grouped. in the context of this study, points refer to pixels. dbscan has many applications, including internet traffic classification; war-game frontline prediction; and facial recognition  <cit> .

in dbscan, a cluster is a group of points that the number of points is equal to or greater than the minimum number of points  in certain neighborhood of core points. different point  definitions of dbscan are illustrated in figure  <dig>  the core point is that the neighborhood of a given radius  has to contain at least a minimum number of points , i.e., the density in the neighborhood should exceed pre-defined threshold . the definition of a neighborhood is determined by the choice of a distance function for two points p and q, denoted by dist. for instance, when the manhattan distance is used in 2d space, the shape of the neighborhood would be rectangular . note that dbscan works with any distance function so that an appropriate function can be designed for some other specific applications. dbscan is significantly more effective in discovering clusters of arbitrary shapes. it was successfully used for synthetic dataset as well as earth science, and protein dataset  <cit> . once the two parameters ε and minpts are defined, dbscan starts to cluster data points  from an arbitrary point. if the neighborhood is sparsely populated, i.e. it has fewer than minpts points in the region query, then that point is labelled as a noise. points that are causing the cluster to grow called border points. in the context of this paper, terms "node" and "point" are used interchangeably with pixel. pseudocode of the dbscan is given in algorithm  <dig> 

algorithm  <dig> - dbscan

input:

d = set of all points

ε = max distance between nodes

minpts = density of nodes needed

procedure dbscan

foreach unvisited point p in dataset d

p is visited

neighborpoints = regionquery

if < minpts)

p is noise

else

p = next cluster

expandcluster

in our previous study we introduced computationally more efficient version of dbscan  <cit>  for the purpose of automatic border detection. in that version, we removed redundant computations that exist in dbscan by only expanding a cluster around a border region since cluster expands from borders. it was proven in  <cit>  that this approach is computationally more efficient than the traditional dbscan.

even computationally improved version of dbscan  <cit>  requires a lot of time for very large datasets. the need for speeding up dbscan is better understood when considering high resolution dermoscopic images, which consist of millions of nodes . even improved version of dbscan given in  <cit>  takes some time for generating results. reader is referred to table  <dig> for the timing of density based skin lesion border detection method's serial version for different size dermoscopy images. not only image size but also complexity and irregularity of the skin lesion also reduce the performance. thus, in order to benefiting available ubiquitous high performance computing hardware resources found in today's computers, we implement our density based skin lesion border detection method in a high performance parallel computing model called webcl. webcl provides a significant speedup and to provide a high level of portability for our case of dermoscopic image processing.

webcl parallel version: speedup factors of  <dig> dermoscopy images.

webcl
this section summarizes webcl and driving force behind webcl. computing capability of today's computers has been evolving with introduction of more computational cores in chips rather than faster computational cores. this is because of reaching the physical limits of silicon chip, excessive heat dissipation causing noise and high voltage requirement in increasing clock frequency of integrated circuits. as a result, we use multicore and multiprocessors that consists of many computing chips in one single integrated circuit . this trend in chip advancement can also be seen in mobile platforms such as smart phones and tablets. therefore, the performance improvement in any applications, especially applications with high demand in computation power, can be only realized with the use of parallel computation. although application parallelism for desktop applications is achievable at some degree for quite a while, up until introduction of webcl web-based client-side applications were not able to use available parallel hardware resources such as multicores and gpus at the client-side.

lack of parallelism creates the main limiting factors for many potential web applications. web applications work solely on the user device and although the device has enough computing ability, application cannot utilize the underlying multi-core and multiprocessor device . this is due to the limitations placed on the current web browsers' designs and lack of any middleware software layer. but with the recent technology called webcl, it is now possible to design and develop parallel algorithms that could effectively use the parallel hardware. specifically, webcl is a recently introduced  open web based standards for heterogonous parallel computing. as it is an open specification, it has been gaining wide acceptance nationally and internationally. webcl specification has been accepted and driven by khronos group . webcl mainly introduces a new software middleware layer aims at directly accessing to the parallel hardware within the web browsers.

with webcl, it is now possible to develop high performance web applications such as data visualization, video processing, 3d gaming, interactive simulations, image processing and segmentation that would not be possible before. the web applications that can now effectively use these computing capabilities of any mobile devices as tablets , smartphones  and also prospective devices that will be available in everyday use such as smart watches, smart glasses, and smart devices in smart homes.

the significance of using webcl is that it removes any device dependency and provides true platform independence. we can design and develop computation intensive web applications that are accessible by any device regardless of their hardware and software platform. this also enables webcl application portable, mobile, and future compatible; meaning that the application can adopt computing capability of upcoming devices in the future.

any algorithm developed with webcl needs to adopt a single instruction multiple sata  approach. in simd, there is a single execution task working on its own portion of data called kernel. therefore, kernel can operate in multiple cores at the same time simultaneously and independently which significantly increases the performance of the application.

webcl is fast and portable. it takes full advantage of parallel architecture in new computing devices. it currently runs on firefox, safari, and chrome web browsers  <cit> . it can provide instant high performance computing to a desktop, laptop, a tablet, or a smartphone. compared to java-script, the most popular language for web-based computation, webcl is 100x faster  <cit> . in the future, this technology will be implemented on more mobile phones and tablets, allowing for fast computing to be available anywhere, loaded straight from the internet.

webcl is a parallel programming environment that also takes advantage of general purpose processing for graphics processing units . a gpu has more transistors than a consumer level cpu, and can be considered a more powerful processor  <cit> . gpus also produce more flops/watt than cpus, making them energy efficient alternatives over cpus  <cit> . this makes gpus find application areas in medicine  <cit> . gpus use a simd programming paradigm. it computes a kernel, or an algorithm, on a stream, or ordered sequence of the same type of data, in parallel. a kernel operates on an entire stream.

webcl is a proposed standard, first announced in march  <dig>  designed to provide higher performance client side computation  <cit>  for web interfaces. webcl was noted by the nsf cross-layer power optimization and management workshop for increasing performance, productivity, and portability  <cit> . webcl is a set of javascript bindings to opencl. opencl is a high level abstraction that allows for high performance code to run on a large variety of devices  <cit> . it is an open specification  <cit> . webcl is currently in the api definition phase, and has three popular open source implementations; samsung, nokia, and motorola  <cit> . for our purpose of parallel skin lesion border detection, we use nokia's webcl implementation.

a goal of webcl is to provide a platform for a large variety of high performance web applications to run on multicore devices  <cit> . to run a webcl program, a computer needs a modified web browser, opencl hardware capable hardware, and opencl driver software  <cit> . webcl programs start on the javascript application level. webcl kernel is built from source to be optimized on the local device that it will run. jobs are created by matching a kernel with a datastream, and are executed by queuing the job in the command queue. jobs on the command queue executes directly on the device. when a job is complete, a javascript event may be created to signal its completion, which can be used to process output or as a barrier for tasks that need to be serialized. webcl has a high interoperability with webgl, similarly to how opencl has high interoperability with opengl  <cit> .

nokia's implementation of webcl is a firefox extension. webcl support can be added to firefox by installing opencl drivers for a local device, and installing nokia's webcl add-on. samsung's webcl implementation runs on safari, and is built on top of webkit. it must be installed from source code, and also relays on opencl drivers for a local device. the two implementations agree upon a single api, which is currently a working draft. both of these implementations are capable of supporting applications in their current state. for our purpose, we use nokia's firefox add-on for webcl due to ease of deployment, portability across platforms, and access to developer add-ons native to firefox. although webcl has many benefits such as accessibility, portability, and performance, it requires significant design and development time, and more importantly expertise in parallel computation and web application development. this is due to the fact that webcl can only benefit from the simultaneously execution of application tasks on multiple hardware computation cores. for any application a new novel parallel algorithm needs to be developed designated for webcl execution scheme and parallel hardware.

our implementation uses manhattan distance to calculate the distance between two pixels. it has two large advantages leading towards a small computational complexity: it maps easily to integer space which performs quickly on a gpu, and it maps naturally onto an image  <cit> . manhattan distance is the change in × plus the change in y. distance based calculations are very common in density based clustering. figure  <dig> illustrates a sample radius  <dig> manhattan region with the region's center being  <dig>  next section introduces webcl implementation details of the density based skin lesion border detection method.

methodology
to take full advantage of the acceleration available through webcl, our previous density based skin lesion border detection method was implemented in data-parallel. nodes are divided into geographic regions, with some redundancy in the area covered in order to limit the cross thread communication requirements. each node is assigned to a thread, creating up to millions of threads, and the distance is calculated between nodes in the same previously created geographic regions. using the transitive property, the output of each thread is combined using a tree reduction algorithm. a border is detected by noticing a drop in density amongst the nodes. after the image has been scanned, a line is drawn displaying the border of the lesion. after the image has been processed, it is drawn in an html <dig> canvas with border regions represented in a color that contrasts with the lesion image, thus delivering the border. this is illustrated in figure  <dig> 

c++ implementation
a serial version of density based skin lesion border detection method was implemented in c++. following that, a parallel webcl version was designed and implemented, taking full advantage of the speedups made available by the webcl framework. the serial c++ implementation reads a dermoscopy image, and generates every pixel's cluster id along with whether that pixel is border, core, or noise. the serial c++ version of the implementation uses the pseudocode mentioned in algorithm  <dig> 

webcl implementation
the webcl implementation accepts a dermoscopy image file as an input. the input file is divided into sizes capable of being stored inside the gpu. more specifically, a dermoscopy image partitioned on to a smaller pieces to be concurrently executed on the gpus. for effective image partitioning available memory spaces in the gpus are considered. nokia's firefox implementation currently has a tight memory limit. thus, we had to incorporate this limitation along with the available memory space in the gpus. each image partition is then scanned using the parallel implementation of density based skin lesion border detection . the output of each scan  is merged in to a global memory in the gpu such a way as to combine clusters span that across multiple partitions/borders. how image data is partitioned and how borders on different image partitions are merged are explained in the following section. more specifically, next section explains how merge operation is designed and implemented. due to the nature of independent concurrent threads running the image partitions, the same clusters falling in to different partitions may be labelled differently. in order to eliminate that problem after parallel threads executions on different image partitions, we need an additional operation called merging.

algorithm  <dig> - parallel version of dbscan for each image partition using density based clustering to find the data density around a certain location

input:

n = contains a location in an.image partition

partition.image = a 2d array, each location may be a node

eps = the distance from the center of n to be searched

output:

neighbors = a list of all nodes within eps distance of n

procedure regionquery()

for each location t inside an image partition

if t is a node

  if the distance between n and t <= eps

    neighbors.add

1) shingled partitioning
many devices have memory limitations. thus, partitioning an image is necessary if the image size is larger than the available memory space. this is better understood when virtual slides are considered. for instance, virtual slides with  <dig> gb size are quite common. for these very large images, partitioning the image and processing the partitioned image inside the gpu is necessary. not only that, image partitioning is also important for scalability and portability. a modern day gpu typically can support  <dig> gb of data. image partitioning increases the portability, because a device queue can be created to determine a maximum partition size. so that later data can be partitioned and processed in the gpu's device queue. this allows for an application to be optimized for high-end hardware, while still running on lower end or mobile hardware. so, when data is partitioned dynamically according to the available resources and data size, the application can also be dynamically scaled up or down. algorithm  <dig> summarizes image partitioning data structure. xoffset and yoffset in algorithm  <dig> are determined dynamically with an image size and the available resources in the gpu.

algorithm  <dig> - image partitioning data structure

datastrcture partition

image = 2d array, each position  is a node

xoffset = where this partition starts on the × axis of the whole image

yoffset = where this partition starts on the y axis of the whole image

because density based skin lesion border detection uses the density of a point's  ε-neighborhood, true partitioning, wherein any given pixel must reside in one and only one partition, would not be satisfied. this is due to the fact that for a point to be properly categorized as a core node, a border node, or a noise node along with a proper cluster id, the algorithm must be able to examine that point's neighborhood within a radius of ε. this is not a problem for the points falling inside the partition. however, there is a need for a special care for the points  lying on the edge of a partition . this special care is handled by examining the edge points with a large portion of their ε-neighborhood missing . more specifically, to deal with the edges of a partition, it becomes necessary to have some redundancy in the border regions of partitions. we call these redundant regions together with the partition that owns the redundant region as shingles. more specifically, in our implementation, each shingle is composed of a core partition plus a lap region which overlays a portion of the core regions of the shingles to its right and bottom. for instance, core partition of partition  <dig> with the length of ε is illustrated as a red region in figure  <dig>  the lap region is 2ε+ <dig> pixels, as needed to accurately measure the density of a point residing near a border.

for instance, figure  <dig> illustrates shingle  <dig> as partition  <dig> along with the redundant zone  which is illustrated as a shaded area. notice that, width of that shaded region is selected as ε for not to miss ε-neighbors of the edge points in the red area . so that, clusters in partition  <dig> including edge points/nodes ensured that they will have the same cluster ids with partitions  <dig>   <dig>  and  <dig>  this happens when partition 1's edge points' ε-neighbor region query determines that points in partitions  <dig>   <dig>  and  <dig> fall in to the same cluster.

serial image partitioning implementation is a simple o operation where p is the number of partitions. in our parallel implementation, partitioning is also done in concurrently. so that, each thread creates its own unique partition. uniqueness is guaranteed by unique thread offset which is calculated from thread ids. creating partitioning concurrently in different threads reduces computation complexity for partitioning to o. the × and y offsets along with the number of rows and columns of partitions are calculated from the core partition size, while the total partition height and width are 2ε+ <dig> pixels larger than their respective core sizes due to the inclusion of the lap region. pseudocode for image partitioning is given in algorithm  <dig>  once image is partitioned then next stage is processing these partitions concurrently with density based skin lesion border detection method.

algorithm  <dig> - method for creating list of partitions

input:

partitionwidth = width of a partition

partitionheight = height of a partition

height = height of image

width = width of image

output:

partitionlist = list of partitions

procedure createpartitions()

partsperrow = width / partitionwidth

partrows = height / partitionheight

for each row in partsrow

for each partition in partsperrow

  xoffset = indexof partition in row * partitionwidth

  yoffset = indexof row in partsrow * height

  partition = new part

  partitionlist.add

2) partition processing
partition processing also occurs in parallel inside a webcl device. partition processing finds clusters within a single partition. in this stage each pixel of a partition in the image is given a single thread. the thread checks whether the node  is a core/border/noise node. then every node in a cluster negotiates a cluster id, agreeing upon the smallest pixel id as a cluster id. the negotiation process is run iteratively until all elements agree. partition processing summarized in algorithm  <dig> and  <dig> accordingly.

algorithm  <dig> - partition processing. it is performed for every pixel in parallel. this scans for the minimum neighbor id.

input:

partition = a partition to process, a global variable

output:

nodelist = a list of all nodes in a partition

c=an integer value of the cluster id

procedure partitionprocessing() //create  <dig> thread per pixel in parallel

sum =  <dig> sum <dig> = 1

while

  sum <dig> = sum

  sum = 0

  nodelist = scan;//scans for minimum cluster id among ε-neighbors

  foreach node n in nodelist

  sum += n.c

algorithm  <dig> - scan operates on the data stream of an image, segmented and loaded as a partition. the partition is loaded into the constant memory.

input:

partition = a partition to search for clusters in, a global variable

minpts = a measure of minimum cluster density, a global variable

output:

nodelist = a list of all nodes and an associated integer value c or cluster id, a global variable.

procedure scan()// <dig> thread per pixel in parallel

pixel n = partition.image;

if//core or noise or border

neighbors = region query// ε-neighbors

if

  n.c = smallest c of all neighbor's cs

else if

  n.c = smallest c of all core nodes in n's neighbors

nodelist.add;

the longest path of the merged region determines how many iterations of scan must be completed before each node in a cluster is associated with the same cluster id. figure  <dig> illustrates the longest path as a grey area for the clustered group of pixels in figure  <dig>  figure  <dig> illustrates the worst case scenario.

3) partition merging
partition merging works by using the transitive property. areas of the image that are on the border of a partition are scanned twice. if this node is found in two clusters, than the transitive property demonstrates that the two clusters are actually the same cluster. nodes clustered in two partitions can be found on the border of a single partition, within  <dig> ε +  <dig> of an edge of a partition. this process is completed by merging each partition with a list of all partitions already merged. algorithms  <dig> and  <dig> summarize the merge operation.

algorithm  <dig> - shows the high level process of splitting an image into smaller pieces and combining the output of the smaller pieces.

output:

globalnodelist - a list of all nodes in the entire image

procedure mergeclusters()

partitionlist = createparititons()

for each partition in partitionlist

nodelist = paritionprocessing

partitionmerge

algorithm  <dig> - partition merge operation: shows how clusters spanning multiple partitions are detected.

input:

nodelist - a list of all nodes in a partition

globalnodelist - a list of all nodes in the entire image

procedure partitionmerge()

for each node in a cluster in partition

globalnodelist.add

for each node in globalnodelist within eps of a parition border

if two nodes are in the same location

  merge the clusters associated with each node

4) finding the border
density based clustering can classify nodes into three separate categories, core, border, and noise. a core node has high neighbor density that means that the node has more than minpts number of neighbors. a border node is inside the neighborhood of a core node, but it does not have a neighbor density greater than minpts. a noise node has sparse neighbor density, and is not neighbor with any core nodes. we use these definitions to determine the border of a skin lesion, and use it to handle noise values. after each node is classified, we can use the x, y data of each node to draw the core and border nodes onto an html <dig> canvas. we draw border nodes using a color with high contrast to the color we draw border nodes with . this allows for the border to be easily visible. algorithm  <dig> demonstrates the step by step procedure of classifying each node into three categories: core, border, and noise.

algorithm  <dig> - node classification

input:

globalnodelist = a list of all nodes in the image

minpts = minimum density to be determines a cluster

output:

corenodelist = a list of nodes that are at the core of a cluster

bordernodelist = a list of nodes that are at the border of a cluster

noiselist = a list of nodes that are not in any cluster

procudure classifynodes()

for each node n in globalnodelist

clustercount = number of nodes a where a.c = n.c

if clustercount < minpts

  noiselist.add

else if regionquery.size < minpts

  bordernoelist.add

else

  corenodelist.add

RESULTS
amdahl's law  <cit>  for a maximum theoretical speedup states that an algorithm can be accelerated by the portion of the algorithm that is parallelizable plus the portion of the algorithm that is serial. in the serial version of density based skin lesion border detection, each pixel needs to look up ε <dig> other pixels to determine density. drawing the image is done in linear time big o. the serial time is ε2*#pixels, or o. the speedup achievable according to amdahl's law is ε / #pixels, or big o. this turns the time complexity from quadratic based on ε and pixel count to a linear based on ε. this assumes that each pixel can be assigned a thread that is run concurrently. our implementation does not achieve amdahl's law because of communication overhead. however, the parallel webcl version of density based skin lesion border detection algorithm achieves an average of around ~ <dig>  speedup over the serial version on  <dig> dermoscopy images. see table  <dig> for a list of speedup factors for  <dig> dermoscopy images along with the resolution of each image. while parallel version has obtained considerable speedups, it had exactly the same accuracy ratios of the serial version given in our previous work  <cit>  in which mean of border error is  <dig> %, mean of recall is  <dig> %, and mean of precision is  <dig> % respectively. schaefer et al.  <cit>  is the first study that uses border error  measure for dermoscopic image analysis.

to determine that parallel version of the algorithm is generating the same accuracies with the serial version for the same images; we conduct controlled experiments on the same images. controlled experiments mean that whatever the order of randomly selected pixels in the serial version is, we used the same order for the pixels in the parallel version. with these controlled experiments, we obtained the same accuracy ratios as given in table  <dig> for both the serial and parallel versions. table  <dig> shows precision, recall, and border error for  <dig> dermoscopy images. however, both serial and parallel versions of the algorithm each time randomly select the pixels for processing, then results may not have exactly the same accuracies. by the way, discrepancy of accuracies will be unnoticeable . this unnoticeable discrepancy is even true for the serial version; when serial version runs on the same image at different times . thus, it cannot generate exactly the same results.

as seen from figure  <dig>  in some cases even though lesion size is smaller for some images, they have lower speedup factors or vice versa. in some cases; however, lesion size is large but speedup is large too. this is because: the lesion's shape is highly irregular so either causing many region queries or there are many merge operations in the parallel version. depends on the shape of a lesion, many merge operations may occur even for small size lesions in parallel version. see figure  <dig> for an artificially created example. for instance, in this case assume that there are  <dig> parallel sections which are illustrated by red lines in figure  <dig>  all of these parallel sections will run concurrently. thus, each of these concurrent tasks will find too many clusters in their local partitions. however, as can be seen from figure  <dig>  all of these clusters eventually fall in to the same global cluster. therefore, image resolution or lesion size may not always be a good indicator for a speedup.

CONCLUSIONS
using a parallel version of density based skin lesion border detection can provide quick skin lesion boarder detection for dermoscopic images while keeping the accuracy of the serial version. automated border detection can supplement expert dermatologist, and aid them in diagnosis of melanoma or other pigmented skin lesions. while webcl is currently an emerging technology, a full adoption of webcl into the html <dig> standard would allow for this implementation to run on a very large set of hardware and software systems through web browsers. webcl takes full advantage of parallel computational resources on a local machine, and allows for compiled code to run directly from the web browser. this makes it a good candidate for computationally expensive algorithms to be placed in a web browser.

competing interests
the authors declare that they have no competing interests.

authors' contributions
sk made the overall design of the study. mm provided insights and analysis of the density-based algorithms. jl implemented the webcl version as a graduate student under sk's supervision. sk developed the general comparison testbed, performed data analysis, algorithm testing, and statistical measurements. jl made benchmarking. th helped develop design parallel algorithm along with the help of implementing the algorithm in webcl. jl, sk, and th contributed to the writing of this manuscript. all of the authors read and approved the manuscript.

