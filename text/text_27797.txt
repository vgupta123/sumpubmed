BACKGROUND
there have been dimensional increases in ‘omics datasets, including introduction of new types of data, increases in the size of individual datasets, increases in the varieties of experimental platforms within a given data type , and very large increases in the total number of datasets being generated. this explosion of varieties and amount of data has led to a large increase in the number of bioinformatics tools; r/bioconductor has grown from  <dig> packages in  <dig> to  <dig> as of  <dig>  <cit> . the increasing number of types of datasets has led to a large increase in the demands placed on analyses, with new needs for cross dataset comparisons  and correlations between different platforms  <cit> . in total, this has led to a highly dynamic and rapidly changing analysis environment, with increasingly complicated and non-standard data analysis needs. under these conditions, systems that can be rapidly modified, extended, and yet easily understood are highly valued.

it has long been recognized that development of bioinformatics pipelines is a good way to organize complex analyses  <cit> . to ease creation of bioinformatics pipelines, the bioinformatics community has developed several large projects that have produced high-quality, often well-tested prewritten components for pipeline development . in addition, for highly used microarray analysis tasks , there has also been limited, but growing, production of fully developed analysis pipelines , sometimes hosted via websites  <cit> . however, it appears that most typical laboratory pipelines are developed in an ad-hoc manner, often using shell scripting or similar approaches  <cit> . for complex pipelines, these approaches often yield pipelines that are difficult to understand, modify or extend.

analysis of large, complex datasets involving a large number of individual analysis steps is a general problem in many areas of science. to address these general needs, there has been both theoretical work and systems development in the computer science discipline of scientific workflow automation  <cit> . a workflow corresponds to a bioinformatics pipeline. scientific workflow systems are developed with consideration of general pipeline needs, including a need for clarity of operations, appropriate level of abstraction of components, reusability, and easy extension and modification  <cit> . there are a number of scientific workflow systems that are applicable to bioinformatics pipelines, including taverna  <cit> , loni pipeline  <cit> , galaxy  <cit> , bioclipse  <cit> , jorca  <cit> , and the system used in this work, kepler  <cit> . comparisons of the properties of these systems are presented in the original reports on the systems and more generally in  <cit> . we chose kepler because it is a well-established, maintained system based on fundamental computer science principles and features a flexible, convenient graphical interface, an ability to clearly display parameters and comments on the workflow canvas, and the built-in presence of r support.

microarrays are widely used in biological research for both gene expression and chromatin immunoprecipitation experiments . microarray data analysis is complex and consists of several distinct steps with each step having a variety of options. there has been development of a large number of analysis approaches for microarray datasets and production of tools for individual steps in analysis  <cit> . furthermore, there has been full pipeline development both in an open-source environment and commercial systems . the complexity and probable need for future extensibility  of these pipelines indicate that workflow implementations are advantageous for microarray analysis systems.

the primary goal of this work is to provide fully functional workflows for critical microarray tasks in the kepler environment, a well-supported workflow system used in other areas of scientific research. although functional in themselves, these workflows are designed as a toolkit to be customized, extended, and repurposed for specific microarray analysis tasks. this toolkit is aimed at bioinformaticians interested in using scientific workflow systems for developing microarray analysis workflows for their specific applications. in particular, many of the workflow components  can be easily used for other purposes and kepler can have actors that embody complete workflows. effective use of this toolkit requires understanding of microarray analysis and, in many cases, r programming. however, these tools can be easily assembled into custom kepler workflows that are designed for usage by “naive” end-users with little understanding of microarray analysis issues. we develop a series of utilities that are appropriate for basic analysis of chip-chip data in gff format, which is the supplied format for nimblegen inc microarrays. for affymetrix gene expression microarrays, we developed two closely related workflows embodying modified versions of a previously developed text-based pipeline  <cit> . pcr experiments are employed subsequent to chip-chip or gene expression microarray experiments for data validation. we present a full pcr primer design workflow using standard tools that can design primers around a user chosen region of any genome in the ucsc browser and subsequently displays the primer locations graphically. all of these workflows can serve as templates to enable further bioinformatics workflow development in the kepler system. our secondary goal was to demonstrate a programming paradigm based primarily on use of local resources  as opposed to primary usage of remote web services. this paradigm is close to the oft-used scripting approach toward pipeline development. for example, our gene expression workflow demonstrates the translation of a fully local r-based pipeline into a workflow model. for gene expression microarray processing, the advantages of the workflow model over a traditional pipeline include clear abstraction of different conceptual stages and clear display of parameter values with graphical/textual categorization of different parameters. finally, we suggest that these workflows may be a first step toward larger studies basing comparison of workflow systems on comparing implementations of the same task  in different workflow systems, as opposed to consideration of different general properties of workflow systems, as in  <cit> .

our work differs significantly from previous work on developing microarray data processing workflows. to our knowledge, our pcr primer design workflow is unique in that it is the first workflow that only requires basic genomic coordinates and organism to produce predicted primers with a graphical display of primer location in reference to other genomic features, such as single nucleotide polymorphisms . for nimblegen chip-chip gff files, the galaxy system is a web-based platform  <cit>  offering a significant set of predesigned tools and workflows. in contrast to this web-based approach, our chip-chip workflows are entirely based on using local data sources and local programs. unlike galaxy, kepler allows workflow components to be directly written in r, java, python, or an internal kepler language, enabling rapid development and implementation of new functionality in microarray workflows. overall, kepler is designed to be a general purpose scientific workflow system supporting many models of computation  <cit> , in contrast to galaxy. in the taverna system, there are a number of related workflows for affymetrix microarray data processing  <cit> . these workflows are based on the rserver/client model and rely heavily on remote data services. in contrast, we establish an affymetrix microarray analysis workflow using local r scripting approaches , without use of remote web/data services, and that features clear  display of parameter values on the workflow canvas.

implementation
availability of kepler and the set of workflows
kepler is freely available and open-source  and under active development. it will operate under windows, mac osx, and also a large number of unix-type operating systems . the community of developers is open to skilled personnel. stable releases of the kepler system and current development versions are available. other programs included in the workflows are all open-source and freely available: primer <dig> , imagemagick .

all of the programming was performed in accordance with kepler user manual guidelines. all the workflows, required accessory programs, and detailed information on installation are available from http://chipanalysis.genomecenter.ucdavis.edu/kepler <dig> package.zip and all programming code is included and accessible from the workflow files. additional file 2: table s  <dig> lists the operating systems used for development of each of our workflows. many of these workflows will work under multiple operating systems, usually with no modification. workflows have been developed to use the kepler graphical interface for end user interaction.

kepler features relevant to microarray bioinformatics
kepler is currently used in many areas of science requiring manipulation of large and/or complex datasets . general features of the kepler system have been described  <cit>  and an extensive user manual is available . a kepler workflow for processing metagenomic data has previously been produced with some discussion of general kepler features for bioinformatics  <cit> . here, we highlight features of kepler relevant to microarray bioinformatics pipeline design.

in brief, general features of the system include:

· multiplatform. kepler is available for linux, windows, and mac osx.

· graphical. workflows are presented in a simple graphical format that promotes rapid comprehension of inputs and logic. elements of the workflow are added/removed, “grabbed”, moved, and connected in a graphical and intuitive manner.

· built-in support for programming using various methodologies popular in bioinformatics, including r, python , java. bioconductor can be easily used via r.

· presence of a simple internal language  for simple operations .

· ability to integrate external program components .

· ability to integrate internet and website data sources and services.

· workflows can consist of any combination of actor types .

· multiple levels of hierarchy. modules  can be composed of other full pipelines .

· extensive annotation/note-taking capabilities. multiple annotations  can be placed anywhere on the kepler canvas for clarity .

· distribution of complete workflows as single small files, even if composed of different kinds of actors . however, if external programs are invoked, these external programs need to be separately distributed.

kepler uses one fundamental metaphor for information processing. pipelines are viewed as a series of steps in which “work” is performed on the data . each actor accepts data via input port, manipulates the data and then outputs the results via output port. inputs and outputs can be single values, arrays of values, or complex objects. the “director” specifies the overall nature of information flow through the system. we used the sdf director in all our workflows, but other directors may be appropriate in some cases . for clarity, we list the following relationships between traditional procedural programming and kepler workflow programming:

· kepler “workflows” are bioinformatics pipelines

· kepler “parameters” correspond to global variables for traditional programming languages

· kepler “actors” correspond to functions for traditional procedural programming

· kepler “annotations” are similar to comments in a programming language

end user system interaction
all workflows are designed to be used from the graphical interface instead of the command line. to execute a workflow, the end user will begin by starting the kepler application and then loading the workflow into kepler via the file menu. workflow parameters are adjusted by clicking on the parameter as displayed on the workflow canvas, which will cause a box to be displayed in which the user may type the new parameter value . this new value will be immediately displayed on the workflow canvas. to guide the end user in system usage or appropriate parameter options, the bioinformaticist can place text boxes with instructions anywhere on the canvas and can control font, font size, and font color. this guidance is present in the majority of our workflows . the user initiates operation of the workflow by pressing the “play” button.

a model of bioinformatics usage in the laboratory
workflow implementation is fundamentally based on an implicit  model of use of workflows within the laboratory. different environments  may have different usage patterns and requirements. our development approach is based on the following model of workflow usage. the experienced bioinformatician uses a library of actors and workflows to create custom workflows for specific applications. some of these workflows may be reused often and rarely changed, but others will be frequently altered. this library of actors and workflows is provided by several sources, including myexperiment  <cit> , the tools in this study, and custom produced actors created by the bioinformatician. the experienced bioinformatician will provide basic instruction in usage of the system, as happens commonly in laboratories with many applications. the end-user only changes the input data and a small number of other parameters.

RESULTS
the set of  <dig> workflows
the full set of  <dig> workflows is presented in table  <dig>  additional information is supplied in additional file 2: . full display of each of the workflows is presented in additional file 1: figures s1-s <dig>  these workflows fit into four basic families. for affymetrix gene expression microarray processing, there are two closely related workflows, differing by differential gene expression determination methodology. for pcr primer design, there is one workflow. there are four basic workflows implementing unix utilities for convenience. for gff file processing aimed toward nimblegen chip-chip data files, there are  <dig> workflows that are designed to span a wide range of laboratory needs for chip-chip analysis. these nineteen workflows can be subdivided into four further categories: descriptive statistics and file information, file modification, file processing, and binding site analysis . “descriptive statistics and file information” consists of nine workflows that compute and display basic statistics and information on gff files. these workflows can be easily modified to pass this information as data instead of simply displaying it. the “file modification” set consists of three workflows that perform basic modifications of gff files for user convenience. notably, the gffmaketiny workflow greatly reduces the size of a gff microarray data file in a lossless manner while maintaining the gff format. the “file processing” set consists of six workflows that are primarily concerned with sorting, smoothing, and quantile normalization of gff files. finally, the “binding site detection” group consists of two workflows that implement the peak finding algorithm for chip-chip data  that is described in  <cit> . one of these workflows  also annotates the resulting list of peaks by using the r/bioconductor package chippeakanno  <cit> . the annotation consists of the identity of the nearest gene, distance to nearest gene, and peak relationship to gene transcription start site . taken in toto, these gff utilities provide a set of workflows/components that can be easily combined or altered. this is most clearly demonstrated by comparison of the gffqn_sm3_tiny workflow with the gffmaketiny, gffsmooth, and quantnorm workflows.

these workflows are further described in additional file 2: table s  <dig>  each workflow is displayed in additional file 1: figures s1-s <dig> 

the workflows associate data with workflow  using several strategies. for the gff workflows, generally the output files follow a naming scheme indicating origin. in addition, some workflows present only displayed data, making this issue moot. for the affymetrix gene expression microarray analysis workflows, both the r script file and rdata file are stored using user-specified names in the directory with the analysis output files. the primer design workflow stores parameters from every run  and the output of the primer design workflow will directly indicate if primers are in the correct region . finally, in all cases, it is possible to save the workflow itself under a run-specific name.

in the following sections, we present a detailed review of three workflows  across the range of our set  to illustrate design and functionality.

a simple kepler workflow for chip-chip gff file statistics
the gff file format  <cit>  is a widely used file format for genomics data. for nimblegen chip-chip data, the value field of a gff file stores the log <dig> enrichment value . in this workflow, we calculate basic statistics for this field. these basic statistics can indicate whether an experiment shows significant amounts of high enrichment.

we chose to use a python actor for gffread because python is much faster at reading large files than using read. other methods of reading gff files in r may allow large speed increases. an alternative version of this workflow  using  only  r  is  available   in our workflow set for comparison. because kepler allows the use of various programming approaches in the same workflow, this division of tasks across python and r is natural to implement.

a workflow for affymetrix gene expression microarray analysis
the workflow presented in figure  <dig> represents a relatively simple task. in contrast, analysis of affymetrix gene expression microarrays consists of several complex steps, with significant choices of options for each step. as a baseline for appropriate affymetrix microarray analysis tasks, we chose to implement the basic steps of the existing r/bioconductor pipeline amda  <cit>  within kepler . amda essentially develops a custom set of functions as a convenient wrapper for a series of well-known and well-established r/bioconductor modules. amda produces a variety of outputs, including both graphs and lists of differentially expressed genes. to allow amda to accommodate larger numbers of microarrays, as is often used in current data analyses , we slightly modified the amda pipeline to eliminate some memory intensive steps and to use alternative modules that are less memory intensive. we implemented analysis for the widely employed experimental design of comparing test conditions to a control condition ; this design is referred to as ‘common reference’ or ‘common baseline’ in the amda description; see  <cit> . we developed two versions of this workflow that differ in determination of differentially expressed genes. the figure  <dig> workflow displays a version that determines differentially expressed genes using t-tests as implemented in the r/bioconductor package simpleaffy  <cit> , while the second version  uses the package limma and allows determination based on adjusted p-value scores .

the data output of this workflow consists of the workspace image, the produced r script file, and a series of files containing graphs and data. graph and data files are stored in the location specified by the ‘working_dir’ parameter which is defined by the user in the ‘file and directory parameters’ section. figure  <dig> displays the heatmap output  of this workflow when the workflow was applied to a set of microarrays from ncbi geo gse <dig> . workflow output files are further described in the distribution package . the actual r script that is produced  is stored at the location and with the file name specified as the ‘file’ parameter on the canvas, and the workspace image  is stored in the ‘working_dir’ location starting with the name specified by the ‘workspace_image’ parameter; in figure  <dig> the output file will be gbm_tester <dig> rdata. because these r script and rdata files are created with each run of the workflow, the workflow is “autodocumenting” in that these files contain all the steps and parameters used to create the outputs.

the basic strategy is for each actor to add lines to an r script file that is then executed using the command line r program . the steps are as follows. first, it is necessary to assign each microarray to a group . the actor ‘create_covdesc_file’ is a composite actor: it is composed of a workflow that includes a java actor using the swing toolkit to allow a simple window interface for the user to enter information for each microarray, creating a small file often referred to as a ‘covariant description’ or ‘covdesc’ file in the r module literature. second, after initialization and loading of microarrays , the actor ‘normalize’ performs normalization of microarrays. normalization can be memory intensive  <cit> . the relatively low memory usage and fast justrma function is a standard choice for both small and large microarray datasets. for purposes of reproducing older published analyses, the r/bioconductor function expresso can be chosen  and subparameters can be set ’ group). third, the microarray set is “prefiltered” to eliminate probesets that are called as ‘absent’ on all microarrays, as these probes are considered uninformative . fourth, differentially expressed genes  are found using simple t-test and fold-change criteria, as are commonly used . these values are established in ‘important run parameters’ in actor ‘selecting deg’. use of fold-change, in particular, may be valuable when only small numbers of microarrays are available  and are intended to be analyzed to provide candidate target genes. given the range of deg selection approaches commonly employed, the deg selection approach should be carefully examined . the deg selection approach can be easily altered by the experienced microarray bioinformatician by altering the ‘selecting deg’ actor to use different r/bioconductor packages. for use of adjusted p-values , we have created a second gene expression workflow . this second workflow has parameters on the canvas for choice of adjustment method of p-values and, by comparison with the figure  <dig> workflow, illustrates directly how the deg selection method can be changed. fifth, independently of deg selection, the microarrays are clustered using hierarchical clustering and pearson correlation as a distance measure, an important step for determining potential outlier microarrays and overall relationships . sixth, degs are clustered to determine groups of genes with similar expression profiles across conditions . seventh, gene ontology  statistical significance is computed for the genes in each deg group . eighth, correspondence analysis, a data reduction approach for high-dimensional data, is performed . the workspace is saved by actor ‘save r workspace’. finally, the last actor  invokes command line r  to run the full script.

errors are indicated in several ways in this workflow. simple errors, such as having an incorrect name for the directory containing the cel files, produces a large red box around the offending actor. furthermore, the workflow at its conclusion displays a window that resembles the cmd program window in windows. rscript is run from this window, and just as in the case of using a cmd window, will produce the text output of the script, which can indicate where execution ceased. this raises the possibility of employing the classic error finding methodology of strategically placed print statements to determine the exact point of occurrence of a problem and to monitor variable values.

there are several notable features of this workflow. first, this workflow demonstrates some advantages of transforming an r/bioconductor pipeline into a kepler workflow. most importantly, the overall steps in the workflow are clearly illustrated without being buried in a long text file, as in a r/bioconductor script. unlike a script representation, this graphical representation clearly separates functions from parameters. the parameters are grouped into different sets and notes have been added to indicate functions. second, this workflow uses a composite actor - an actor that itself is a complete workflow. this allows simple clear packaging of complex tasks. third, the workflow is “autodocumenting” because the generated r script is automatically saved, as is the r workspace. hence, it is straightforward for experienced r users to investigate the actual exact steps used in the analysis and to retrieve all the parameters used for a given analysis outside of the kepler environment. also, optionally saving the kepler workflow file  separately for each analysis will allow simple, rapid inspection of analysis parameters; the parameters are presented in a natural set of categories, allowing quick and intuitive review. fourth, this workflow demonstrates the strength of kepler in integrating diverse programming methodologies: java + swing is used for the input of the covdesc file, then the workflow creates the r script using the kepler internal language, and finally the r application rscript is invoked as a local application. fifth, we found that an alternative implementation of these steps led to several tradeoffs. we initially implemented each step as a separate r actor. because the r actor starts and stops r each time the actor runs, we found this solution to be much slower than using the “create full script then execute script” approach shown in the figure. finally, the graphical display of the workflow highlights functional modules, allowing rapid discernment of the appropriate actor to alter for modified functionality. for example, if a different gene ontology statistics package were required, it is clear that the step labeled “gene ontology stats” should be changed. similarly, various collections of well-developed modules for bioinformatics  could be quickly implemented using this general workflow design paradigm. hence, existing, well-tested and understood collections of modules for bioinformatics can be easily moved into the kepler environment. furthermore, moving existing r/bioconductor pipelines into the kepler environment allows clear display of functional units and clear separation of parameters from functions.

automated design of pcr primers for microarray validation experiments
pcr primer design is a very common task in molecular biology and the usual required step for validation of microarray results . although there are valuable databases of pcr primers for some common tasks , these resources only address a portion of typical pcr primer design needs. for example, primer design for promoter and enhancer regions must often be performed de novo . a typical manual protocol for design of primers to a desired region in the human genome is:

 access ucsc browser and retrieve the dna sequence for the desired region.

 copy and paste this sequence into the primer <dig> website. adjust the primer <dig> primer-picking parameters to use required values.

 copy the set of resulting suggested primer pairs.

 for each primer pair, access the ucsc browser and perform in-silico pcr function to determine the exact coordinates of the resulting desired pcr product, that the primers do not overlap significant snps, that they do not overlap repeats, and that they do not map to other locations in the genome. repeat for each primer pair.

 repeat steps 1- <dig> for each different region requiring pcr primers.

for microarray experiments, there may be a relatively large number of regions to validate via pcr experiments. hence, automation of this procedure is advantageous.

 retrieve sequence from ucsc browser . notably, this actor can be used in any workflow to grab any sequence from the ucsc browser.

 produce predicted primer sets by running locally installed primer <dig> using sequence from ‘getdnasequence’ using appropriate primer <dig> parameters . a single appropriately formatted input file is created by actors ‘parsefilename’ and ‘makeinputfile’ and then passed to primer <dig> .

 submit each primer set to the ucsc genome browser to produce a graphical output showing locations and snps from the browser. ‘getprimers’ parses the primer <dig> output file to derive the primer sets. because the ucsc browser uses get calls for in-silico pcr, the workflow constructs http calls and executes them to get the resulting images . the ucsc images  are then passed to imagemagick for reformatting as png files, which are then displayed in a webpage format .

this workflow produces several output files. one graphical output of this workflow is displayed in figure  <dig>  there are several notable features of this workflow. first, the graphical design and clear naming of actor inputs and outputs allows conceptual clarity promoting the reuse of actors and sections of this workflow. for example, only casual inspection of the workflow is required to discern that ‘getdnasequence’ can be reused to grab dna sequences from the ucsc browser for other bioinformatics pipelines . similarly, it is clear that the workflow segment after ‘getprimers’ could be used in other workflows for performing in-silico pcr using the ucsc genome browser and that the ‘ucscimage’ actor can be used to grab “screenshots” from the ucsc browser. because the ucsc browser contains data from a wide variety of sources  <cit> , this actor would be useful for targeted, automatic grabbing of screenshots of regions of the genome for many uses.

discussion
the growth in size, types and subtypes, and overall number of large scale ‘omics datasets has led to a large increase in the diversity, complexity, and rate of change of bioinformatics analyses. in turn, these complex and evolving needs have led to an increasing emphasis on development of analysis pipelines. here, we developed a series of workflows that addressed microarray data set analysis. microarray analysis pipelines are complex, subject to alteration with the development of new analytical tools, and also growing in complexity due to the development of new types of resources for output analysis, such as gene pathway systems  <cit> , and new demands for integration with other sources of data . we found that the kepler system has functional and appropriate capabilities for microarray data analysis pipeline creation. the kepler workflow model, and in particular its graphical implementation, provides significant advantages for pipeline creation by promoting conceptual and visual clarity. finally, kepler workflow programming allows a variety of overall design approaches; we create microarray data analysis workflows using several types of programming approaches.

microarray data analysis will become more complex in the future. first, there are increasing current needs for microarray analysis involving integration of data from different gene expression platforms  and different designs . analyses involving different platforms will require much more extensive analysis pipelines. also, there are increasing needs for comparison of gene expression data with chip-chip or chip-seq  and comparative genomic hybridization  datasets, which again will require more extensive pipelines  <cit> . as analyses become increasingly complex, it will become critical to use workflow systems to allow modifiable, extensible, and easily comprehensible analysis pipelines. we suggest that the workflows produced in this report represent a critical set of components for these future, more extensive pipelines.

kepler features such as  actors with clear inputs and outputs and  the graphical nature of workflow development and representation offered significant advantages to us as compared to traditional shell scripting. for example, simple visual inspection quickly reveals that the gene expression workflow  includes a “correspondence analysis”, which is an optional step in gene expression workflows. the approach for eliminating this step is also clear: simple removal of the actor. similarly, if the method for determining differentially expressed genes  should be changed, the workflow visually indicates the correct actor to alter - the ‘deg selection’ actor must be modified. most importantly, we found that kepler offered special advantages for bioinformatics pipelines involving a large number of potential parameters: even a brief glance at the gene expression workflow  or the primer design workflow  shows which parameters are usually changed and which parameters are usually kept constant. this is enabled by the nature of the kepler canvas using positioning, text font, text size, and text color to organize parameters into clear visual groups.

kepler enables a large number of potential software design strategies. this seems to be an advantage in that there is not necessarily a single best design approach for bioinformatics pipelines. for example, an actor could be implemented as a large block of python code or, alternatively, as a composite actor embodying a workflow. both implementations of this actor could look identical from a usage standpoint. overall, the kepler model favors actors passing data to each other. however, in some cases , we may wish to have the actors simply compose a script to be run externally. kepler can easily support these different software design paradigms. this means that existing shell scripts and pipelines can often be easily moved into the kepler system. hence, a set of useful kepler workflows and reusable actors can be quickly produced.

runtime workflow errors are indicated in kepler with a red box around the actor that encounters the error. this has the advantage of the indicating clearly which stage of the workflow needs to be investigated. however, the actual error output that is passed through kepler is phrased in programming terms, which will be difficult to interpret for the end user. this problem is shared with many bioinformatics pipelines that rely on simple shell scripting and errors in r/bioconductor can also be difficult to understand. this problem can be addressed by having the experienced bioinformatician add error-catching tests to the pipelines to produce informative error outputs.

kepler offers advantages and disadvantages compared to other bioinformatics pipeline development approaches. in the much used “shell-scripting” approach, a series of external programs are invoked. shell scripting has the advantage of being a well-known and easily accessible approach that can produce very rapid development, in particular for small tasks  <cit> . however, shell scripts for complex tasks can require careful study to understand. furthermore, the mixture of programs and parameters at the point of invocation makes changing parameters confusing. finally, packaging of shell scripts for distribution involves finding each subprogram and dependency. in contrast, kepler workflows are often single files because the r, python, java etc. code is included in the workflow file . similarly, developing pipelines within a single programming framework  is attractive for the knowledgeable programmer, but can be very difficult for the outside user to follow, extend, or modify. a second major approach is the use of web-based pipelines. these pipelines often indicate the analysis steps and parameters clearly , feature user interfaces  that are already known to the user, and can feature attractive, informative outputs that document the parameters that were used. however, these pipelines are usually inaccessible for modification and extension. also, upload of large datasets to the servers can be slow and problematic.

a third option is the use of other workflow systems . workflow systems generally share many properties . hence, selection of a system may rely on specific features of the system. taverna has been used for many bioinformatics pipelines with a strong emphasis on use of web services  <cit> . however, microarray analysis currently features relatively large primary data files and the file sizes are increasing in many cases . moving large files from internet site to site, as is necessary for web services, is usually slow and can be error-prone, producing issues with speed of execution and effectiveness of using web-service oriented frameworks in this exact application area. hence, we strongly prefer microarray workflows featuring local resources . in addition, the published taverna workflow system for affymetrix microarrays  <cit>  uses a more complex rclient/server model rather than the simple local invocation of r in our workflows. our use of r is the basic use introduced in r courses and books . therefore, we believe that our approach is conceptually simpler, has a simpler implementation, and is more familiar to the majority of bioinformaticians, making this workflow easier to comprehend. also, kepler displays the parameters and parameter values simply and clearly on the actual canvas with the workflow, and changing these values is very simple . we strongly prefer this clear display of parameters, which is not present in taverna workflows. finally, kepler has a well-developed provenance module that has great promise for bioinformatics workflows.

galaxy  <cit>  is an increasingly popular platform for bioinformatics analyses. at this time, there are no gene expression microarray analysis capabilities in galaxy. to bring new tools into galaxy, a description of the tool is required to be produced. in contrast, external tools  can be simply and directly invoked within kepler actors without any need for descriptions of allowable inputs, outputs, or other formats. furthermore, the current galaxy canvas does not display parameters with values, unlike the kepler canvas. therefore, kepler offers significant advantages in that parameter values are directly displayed and easily changed and also it is much quicker to integrate existing tools into kepler. workflows in this report often invoke external programs and provide many examples of this approach. finally, galaxy is based on a web model instead of being a simple standalone program like kepler. using galaxy from a remote location can produce significant inconvenience of uploading data to a server, and a local installation of galaxy will still use a web interface, a more complicated arrangement than the simple, standalone program that is kepler.

kepler has some disadvantages. like other scientific workflow systems, using kepler requires a time investment in learning the kepler environment and the kepler approach to actor creation. we expect that our production of a series of working bioinformatics pipelines will enable this process. second, we found that the theoretically best approach for kepler development - passing data as structures  - led to some problems with execution time . we were able to develop a straightforward solution to this issue . finally, there are far fewer prepackaged bioinformatics resources in kepler as compared to taverna, but the biokepler project  <cit>  should change this situation.

making kepler workflows more widely accessible, e.g., through popular workflow repositories  <cit> , will allow pipeline developers to share workflows more easily, learn from each other, and thus make pipeline development increasingly straightforward

CONCLUSIONS
the set of  <dig> workflows provides a solid foundation for chip-chip and affymetrix gene expression microarray processing and subsequent validation using pcr experiments. we present the first workflow that designs pcr primers using any available genome/genome assembly in the ucsc genome browser and provides a graphical output indicating positions of known snps and repetitive elements in the pcr product. transforming r/bioconductor pipelines into kepler is relatively straightforward. kepler offers many advantages over traditional scripting approaches for microarray data processing pipelines, with increasing advantages as these pipelines grow more complex.

availability and requirements
· project name: kepler microarray workflows

· project home page:http://chipanalysis.genomecenter.ucdavis.edu/kepler <dig> package.zipthis zip package contains all workflows and instructions for installation.

· operating system: windows, linux, mac osx. some workflows will only function under windows; see zip package and supplementary table for details.

· other requirements: java  <dig>  r/bioconductor, kepler. see instructions in zip package for installation.

· license: mit open-source

· kepler is available at: http://www.kepler-project.org

competing interests
the authors declare that they have no competing interests.

authors’ contributions
tm, bl, and mb developed overall concept for project. mb developed list of workflows and programming approach, supervised and advised on all phases of project, tested and wrote some workflows, and wrote manuscript. tm and bl edited manuscript. ts created workflows and wrote underlying included programs. all authors have read and approved the final manuscript.

supplementary material
additional file 1
stropp et al. additional file 1: figures.pdf. this file contains additional file 1: figures s1-s <dig> showing screenshots of all workflows listed in tables  <dig> and s  <dig>  each figure includes some additional information on goals and usage.

click here for file

 additional file 2
stropp et al. additional file 2: table.pdf. this file contains additional file 2: table s <dig>  which is an expanded version of table  <dig> including details of implementation and operating systems among other information. full information on installation and running workflows is listed in the “availability” section above. 

click here for file

 acknowledgements
we thank all members of the bieda lab for feedback on use of the pipelines. this work was supported by university of calgary startup funds to mb and canadian institutes of health research training program in genetics, child development and health  to ts and national science foundation grants  to bl and tm.
