BACKGROUND
flow cytometry has evolved to an indispensable tool in biology and medicine, with a significant impact on hematology. to date, diagnosis and classification of acute lymphocytic leukemia , depend on the flow-cytometric description of the leukemic cell clone. recently, flow cytometry has also become an attractive approach for evaluation of therapy response and especially detection of minimal residual disease   <cit> . flow cytometry provides a quantitative cell description by a number of variables, including cell size, granularity and expression of cell-surface and intracellular proteins. due to the continuous development of flow cytometric techniques, their readouts have become increasingly complex and require adequate analysis methods.

current diagnostic evaluation of flow cytometry readouts relies on simplistic two-dimensional analysis techniques. the basis is a labor-intensive gating procedure. in a series of two-dimensional dot plots, leukemic cells are manually flagged by drawing polygons around regions, which are known to contain mostly leukemic cells. a large number of two-dimensional plots need to be inspected and several regions need to be defined manually. finally, candidates for leukemic cells are those inside a boolean combination of drawn regions, called gate  <cit> . from the data analysis perspective, gating lymphoblastic cells is a problem of supervised statistical learning. one starts with a training set of flow cytometry readouts, which are already gated by an expert. the challenge is to derive a multivariate classification model from this data, which is able to produce accurate gatings on different readouts that have not been pre-gated by an expert. the objects of classification are single cells that can either be leukemic or physiological lymphocytes. typically, each cell is described by a 4– <dig> dimensional vector of flow cytometry measurements. with only a small number of pre-gated readouts, one already has several thousand training points. problems are posed by the non-linear shape of the regions containing leukemic lymphocytes and the patient-to-patient variability of these regions.

to our knowledge, replacing the manual gating process by a computer-based automated multivariate analysis has not been described previously. some cytometer software suites contain tools for automated walk-away analyses once the gates have been defined. these tools, however, are also restricted to two-dimensional decision rules. in addition, some methods to make use of cytometry readouts in a multivariate setting have been proposed. here, we briefly review three of these methods and explain the differences to our concept.

valet et al.  <cit>  introduced a classification method for blood samples in flow cytometry, called algorithmic data sieving. for each class of samples, a discretized representative is derived from training data. new samples are then classified according to their similarity to these representatives. de zen et al.  <cit>  investigated the feasibility to classify acute-leukemia subtypes on flow-cytometry readouts. first, they determined the leukemic cells by a conventional, manual gating procedure and discarded all other cells from the data. for each sample, they summarized the measurements over all leukemic cells for each variable and used these summary values for classifying samples with linear discriminant analysis  <cit> . roederer and hardy  <cit>  proposed an algorithm for sample comparison based on cytometry readouts. their algorithm identifies multi-dimensional hyper-rectangular bins that significantly differ in the proportion of cells contained between a test sample and a control sample. the union of all these regions comprises a frequency difference gate. this gate may be used to assign new samples to test or control group, as well as to find differences between similar types of cells under different conditions. while their approach could be modified to search for regions in multivariate space, which differ between leukemic and non-leukemic samples, they restrict these regions to be rectangles, which is not the case in conventional gating  <cit> . moreover, in this case regions are not required to contain the same proportion of cells but rather to contain mostly cells of the same class. in contrast to these approaches, we are not interested in classifying blood samples based on their cytometry readouts, but rather in automated identification of cell populations within the samples. we report on the applicability of statistical learning methodology, for achieving automated, reliable in-silico gatings on flow cytometry readouts. to this aim, we employ supervised classification with support vector machines  <cit> .

RESULTS
algorithm
for supervised classification of the leukemic status of cells, we employ a support vector machine based algorithm that allows for non-linear decision boundaries in the input space spanned by the cells' measured characteristics and protein expression levels. our algorithm takes into account outstanding properties of flow-cytometry readout data, namely

• samples consisting of tens of thousands of individual observations

• large inter-sample variation due to non-standardized methods of obtaining measurements.

support vector machines
support vector machines   <cit>  are a class of regularized multivariate classification models that are widely used for predictive modelling of multidimensional data. we provide a quick review of svm here. let x be the data matrix holding n observations xi, with i ∈  <dig> ..., n, in columns and p variables in rows. the observations xi are said to reside in a p-dimensional input space. for each observation xi its class  yi ∈ {± 1} is known beforehand in case of the training set or to be predicted in case of the test set. svm fit a maximal  margin hyperplane between the two classes. with high-dimensional problems, there may be several perfectly separating hyperplanes . there is, however, only one separating hyperplane with maximal distance to the nearest training points of either class.

more formally, among all hyperplanes of the form

w·x + b =  <dig> | w ∈ ℝn, b ∈ ℝ

corresponding to linear decision functions

c = sgn

there exists one that maximizes the distance of each input vector to the hyperplane. it can be shown, that this optimal hyperplane can be empirically obtained from data x by solving

min⁡w,b{12‖w‖2}subject toyi⋅≥ <dig>  i= <dig> …,n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaafaqabeqadaaabawaacbeaeaacyggtbqbcqggpbqacqggubgbasqaagqabiab=dha3jabcycasiabdkgaibqabagcdagadaqaamaalaaabagaegymaedabagaegomaidaamaafmaabagae83dachacagljwuaaypcsdwaawbaasqabeaacqaiyagmaaaakiaawuhacagl9baaaeaacqqgzbwccqqg1bqdcqqgibgycqqgqbgacqqglbqzcqqgjbwycqqg0badcqqggaaicqqg0badcqqgvbwbaeaacqwg5bqedawgaawcbagaemyaakgabeaakiabgwsixlabcicaoiab=dha3jabgwsixjqadiab+hha4naabaaaleaacqwgpbqaaeqaaogaey4kasiaemoyaimaeiykakiaeyyzimraegymaejaeiilawiaeeiiaaiaemyaakmaeyypa0jaegymaejaeiilawiaesojgskaeiilawiaemoba4gaaaaa@67d6@

where

w=∑i=1nαiyixi|αi∈ℝ.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaieqacqwf3bwdcqgh9aqpdaaewbqaaggaciab+f7ahnaabaaaleaacqwgpbqaaeqaaogaemyeak3aasbaasqaaiabdmgapbqabaacbmgccqqf4baedawgaawcbagaemyaakgabeaaaeaacqwgpbqacqgh9aqpcqaixaqmaeaacqwgubgba0gaeyyeiuoakiabcyha8jab+f7ahnaabaaaleaacqwgpbqaaeqaaogaeyici48efv3yslgznfgdojdaryqr1ngbprginfgdobcv39gaiqaacqafdeiuieaacqweuaglaaa@5136@

in practice, αi will often be zero for many i and different from zero for only a limited number of m observations with m ≤ n. these m observations solely define the separating hyperplane and are called support vectors. the maximal-margin concept is typically combined with the kernel trick to allow for flexible non-linear classification boundaries. the kernel trick is applicable to classification algorithms that can be expressed in terms of inner products of the inputs, as it is the case for the maximum-margin hyperplane. the inner products are substituted by a kernel function k, which corresponds to a feature map Φ that maps the profiles from the input space into a feature space ℋ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwflecsaaa@3763@:

Φ: ℝp → ℋ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwflecsaaa@3763@

x ↦ Φ.

this results in the original algorithm being carried out in ℋ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwflecsaaa@3763@ now and leads to non-linear decision boundaries in the input space. after application of the kernel trick, the decision functions are of the more general form

c=sgn⁡+b).
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgjbwycqggoaakiewacqwf4baedawgaawcbagaemoaaogabeaakiabcmcapiabg2da9igbcohazjabceganjabc6gaunaabmaabawaaabcaeaacqwg5bqedawgaawcbagaemyaakgabeaaiigakiab+f7ahnaabaaaleaacqwgpbqaaeqaaogaem4aasmaeiikagiae8heag3aasbaasqaaiabdqgaqbqabagccqggsaalcqwf4baedawgaawcbagaemyaakgabeaakiabcmcapiabgucariabdkgaibwcbagaemyaakgabagaemoba4ganiabgghildaakiaawicacaglpaaacqgguaglaaa@51da@

soft margin classification
noise may cause large overlap of the classes even in the feature space, such that a perfectly separating hyperplane may not exist. in this case, one can allow for misclassifications  by relaxing the optimization constraints to

yi· ≥  <dig> - ξi, i =  <dig> ..., n.

ξi ≥  <dig> are commonly called slack variables. the optimal soft margin classifier is then found by minimizing the regularized risk function

r=12‖w‖2+c∑i=1nξi.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgybgccqggoaakieqacqwf3bwdcqggsaaliigacqgf+oaecqggpaqkcqgh9aqpdawcaaqaaiabigdaxaqaaiabikdayaaadaqbdaqaaiab=dha3bgaayzcslaawqa7amaacaaaleqabagaegomaidaaogaey4kasiaem4qam0aaabcaeaacqgf+oaedawgaawcbagaemyaakgabeaaaeaacqwgpbqacqgh9aqpcqaixaqmaeaacqwgubgba0gaeyyeiuoakiabc6cauaaa@49bc@

in this formulation, perfect separability of the two classes is not required and margin violations are allowed. the trade off between margin violations and margin size is reflected by the regularization  parameter c.

regularization is essential to counter the additional flexibility acquired by use of the kernel trick.

in recent years, svm have proven to be a powerful and robust classification method that can handle various kinds of input data  <cit> . for a more extensive introduction to svm see, e.g,  <cit> .

we chose a radial-basis kernel function, which can be defined as:

kγ = exp

where xi and xj are two input data points and γ is the inverse band width of the smoothing kernel.

building the classifier
with our data, we applied the svm to separate leukemic from non-leukemic cells. the optimal settings for the svm parameters c and γ were determined on set-aside calibration data, while the actual performance of the svm classifier was analyzed on another set-aside test data set. since our samples each contained up to  <dig>  cells, requiring highly demanding computations, we had to think about methods of data reduction. since with svm classification, the decision boundary is only determined by the support vectors , we can discard all observations that are not support vectors from the training data. we split the data into subsets, keeping only the support vectors from each subset and build the final svm classifier on the sets of support vectors, similar to boser et al.  <cit> . in detail, we used the following procedure to estimate the test error of the learned classifier:

 <dig>  split data into 50% training set, 25% calibration set, and 25% test set.

 <dig>  for a reasonable number of possible parameter settings Θk = :

 initialize an empty set of training vectors t
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwftepvaaa@3847@k = { }

 split training set into computationally feasible subsets

 for each of these subsets

• learn svm-classifier for leukemic versus non-leukemic cells

• identify support vectors s on this subset

• include these support vectors into the set of training vectors t
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwftepvaaa@3847@k = t
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwftepvaaa@3847@k ∪ s

 learn svm-classifier on t
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwftepvaaa@3847@k

 use learned classifier on calibration set to compute the calibration error ϕk

 <dig>  keep learned classifier with lowest calibration error.

 <dig>  use this classifier to predict test data.

 <dig>  return prediction error on test set as test error.

keeping only the support vectors from each data subset reduces the amounts of involved data and renders the computations feasible on common present-day computers.

the final test error provides an unbiased estimate of the classifier's prediction error  on new data .

artificial noise
we are interested in building classifiers that are insensitive to minor noise induced into experimental measurements by the experimental setup or measuring device. by generating artificial noise and applying it to training data, one tests the ability to learn a "correct" concept in the presence of noise  <cit> . here, we add artificial noise to our training data to simulate such minor experimental variations. as noise, we take random normally distributed numbers. classification rules that are built on noisy versions of the training data and apply to artificial-noise-free test data as well, have the potential to generalize better to new test samples  <cit> .

application on patient data
in each of  <dig> patient samples , the leukemic cells were identified by manual gating beforehand. we then randomly assigned  <dig> patient samples to a training set, while the remaining  <dig> patient samples made up the test set. we also included two other samples, for which the proportion of leukemic cells was predefined, into the training set. one of these samples originated from a non-leukemic blood sample  while the other one was enriched with leukemic cells by ficoll gradient-density centrifugation and subsequent manual filtering of the flow cytometry data upon visual inspection .

from each sample of the training set, we randomly drew  <dig>  cells and discarded all other cells. thus, our training data consisted of  <dig>  cells and the associated labels, either "leukemic" or "non-leukemic".

to simulate minor experiment-induced variations, we added random noise to the data. for each variable, we determined its standard deviation across all cells of the training data. we then drew random numbers from a normal distribution with mean  <dig> and standard deviation equal to 10% of the variable's observed standard deviation. these random numbers were added to the values of the training data.

on the noisy training data, we learned the optimal svm classifier and evaluated its classification performance on the test set as well as on the two samples with predefined leukemic proportion.

for building the svm classifier, we again split the training data into an actual training set, a calibration set and a test set to select the optimal parameter settings and to avoid overfitting.

the svm defines a region in six-dimensional space containing the leukemic cells. due to the kernel trick, the classification boundary between points in the six-dimensional space, at which cells would be classified as being leukemic, and the other points, at which cells would be classified as physiological blood cells, is a non-linear structure. to illustrate this, we presenl a projection of this region on the three-dimensional subspace spanned by the variables ssc, cd <dig> and cd <dig> .

the svm classifier employs a radial-basis kernel function with parameters σ =  <dig>  and c =  <dig>  however, changing each parameter selling by up to 25% had no effect on the classification performance, underlining the robustness of the method. the svm classifier is based  <dig> support vectors, of which  <dig> are leukemic cells. on the training data, the learned radial-basis svm classifier achieves a classification accuracy of  <dig> %.

for comparison, we also evaluated the classification performance of an svm classifier utilizing a linear kernel function instead of a radial basis one. we observed a slightly worse performance of this classifier on the training data, namely an error of  <dig> %.

we used the svm classifier to predict leukemic cells in the two samples, either without or enriched with leukemic cells, respectively. cells from both samples had been used for learning the svm as well, but we made sure that the same cells would not be used for testing the svm performance. from the non-leukemic sample, we drew a subsample of  <dig>  cells at random to avoid exceeding available ram during computations for the svm prediction. of these  <dig>  cells, only  <dig>  were misclassified as being leukemic . from the enriched leukemic sample , we also drew a random subsample of  <dig>  cells. of these, the svm classified  <dig>  as being non-leukemic .

we utilized the learned svm to predict the leukemic status of the cells in the test set, which consisted of  <dig> patient samples that had not been involved in training the classifier. we compared the predicted leukemia status with the one determined by gating beforehand. on this independent test set, the svm achieved a sensitivity of  <dig> % and a specificity of  <dig> % for predicting the leukemic status of cells. in summary, for  <dig> % of the cells of the test data, the leukemia status differed between svm prediction and gating assignment. the total number of true and wrong predictions can be seen in table  <dig> 

to evaluate the robustness of the observed classification performance, we repeated the random splitting of patient samples into training and test set  <dig> times, and reran the full analysis for each split. the mean sensitivity was  <dig> %  and the mean specificity was  <dig> % , which confirms the stability of our results. across the  <dig> random splittings on average, we observed  <dig> support vectors , which is more than for the original splitting.

to further assess the built classifier's precision, we took another independent peripheral blood sample, which was taken on the initial day of treatment, and separately measured aliquots from this sample with intervals of several minutes in between. cytometer settings were not changed in between measurements. on each of the six readouts, we applied the classifier to predict the percentage of leukemic cells included. the predicted values ranged from  <dig> % to  <dig> % percent, similar to the manual-gating assigned percentage range that extented from  <dig> % to  <dig> %. we compared the spatial distribution of cells being classified as leukemic by the svm to that of cells deemed leukemic due to manual gating and to the spatial distribution of all cells in the test data . the distributions of cells deemed leukemic by manual gating and by svm classification are nearly identical. however, the area in multivariate space, in which the svm would assign cells to the leukemic class, is slightly larger than that defined by manual gating.

the large majority  of the incongruently classified cells are deemed to be physiological blood cells by manual gating but predicted as being leukemic by the svm. most of these stem from samples taken before initial treatment . their scatter and fluorescence measurements, compared to those cells deemed leukemic by both methods, can be seen in figure  <dig> 

discussion
modern flow cytometers enable researchers to reliably measure six and more variables, such as size, shape and expression of cell-surface and intracellular proteins, for thousands of cells per second. in leukemia research, one is interested in the identification of leukemic cells, which are characterized by abnormal patterns of surface marker expression. the physiological co-expression patterns of these proteins during blood-cell development hints to a tight regulation of the expression of these markers. searching for abnormal expression patterns with analysis techniques employing at most two markers at the same time has been successfully established in clinical leukemia research  <cit> . however, these techniques, such as gating, are labor-intensive, subjective and not able to capture higher-order dependencies between measured variables.

some existing methods make use of the multivariate setting of cytometry readouts  <cit> , but these methods aim at sample classification based on the readouts rather than on cell population detection within samples.

here, we have shown the potential of well-established multivariate-analysis techniques, such as classification via svm, to automate detection of leukemic cells in flow cytometry readouts from patients' bone marrow and peripheral blood samples. the svm operates in the space spanned by all variables and even augments it by the use of the kernel trick  <cit> . classification in this complex space takes into account higher-order dependencies between the variables, which are disregarded when restricting oneself to one- or two-dimensional decision rules. with flow cytometry, there is no denying that dependencies between the measured variables do exist, due to properties of utilized materials and biological, superordinate regulatory mechanisms. developing blood cells are characterized by combinations of interacting surface markers  <cit> . also, measured fluorescence intensities cannot be considered independent from each other because of overlaps between the fluorochromes' emission spectra  <cit> . most cytometers can be set to compensate for these overlaps. while this compensation removes part of the influence of each measured variable on the others, one cannot expect it to remove every dependence between them.

we built a svm classifier on the training data, containing approx. 50% of the available data. we, again, split the training data into separate sets for building the classifier, selecting the optimal parameter settings, and assessing the training error. this procedure and the artificial noise added to the training data prevent overfitting of the learned svm classifier. the learned svm had a very low training error of  <dig> %. remarkably, this error did not increase, when modifying the svm's parameter sellings by up to 25%, indicating a very clear separation of leukemic and non-leukemic cells in the enhanced feature space.

we tested the learned radial-basis svm classifier on independent test data, generated from a non-leukemic blood sample, an enriched leukemic sample and  <dig> patient samples not used for training the classifier. on the non-leukemic sample, only  <dig> % of the physiological blood cells were misclassified as being leukemic. these few misclassified cells display physical properties and surface marker expression similar to physiological b-lymphocyte precursors . while such immature cells are usually restricted to the bone marrow, they have been described to appear in peripheral blood in small quantities  <cit> . on the enriched leukemic sample, only  <dig> % of the cells were predicted to be non-leukemic cells, a percentage of remaining physiological cells to be expected with the density-gradient centrifugation method for leukemic cell enrichment.

we applied the classifier on separately measured aliquots of one single sample to evaluate the classifier's precision. we observed a maximal difference of  <dig> % between the predicted leukemic-cell proportion in any two of these aliquots, underlining the precision of svm classification on cytometry readouts.

the built svm classifier was applied to identify leukemic cells in independent patient test samples. in these samples, leukemic cells had been pinpointed beforehand by conventional gating  <cit> . by svm-classification, we could recover these leukemic cells with a sensitivity of  <dig> % and a specificity of  <dig> % . a comparison of the spatial distribution of cells deemed leukemic by manual gating with that of cells classified as leukemic by the svm shows that both distributions are highly similar, but svm-predicted leukemic cells encompass a slightly larger area than gated ones .

nearly all  cells were classified congruently by both methods. importantly, in the day- <dig> and day- <dig> samples, taken after the first and second treatment phase, only a small number of incongruently classified cells were observed. the svm approach successfully recovered the small leukemic cell populations remaining at this stage, thus demonstrating its promising potential in the identification and monitoring of small leukemic subpopulations during leukemia therapy.

most of the cells that were incongruently classified by manual gating and svm prediction are deemed non-leukemic by gating but leukemic by the svm . these cells generally display a light scattering typical of leukemic lymphocytes, and the majority of them show a cd <dig> and cd <dig> expression similar to that of leukemic cells detected by manual gating . their main population is also characterized by a low cd20- and intermediate cd <dig> expression compatible with a leukemic immunophenotype. however, since their cd <dig> expression tends to be lower than that of the gating-identified leukemic cells, these cells were not included in the leukemic population by conventional gating.

as such staining variations can arise, e.g., from incomplete staining of cells in the experiment, they decrease the sensitivity of leukemic cell detection by low-dimensional gating. in contrast, the svm classification is based on all variables at once, and slight variations in only one variable do not hinder the detection of cell populations as long as the remaining variables are characteristic for the sought-after populations. this highlights the strength of the multivariate approach described here.

also in figure  <dig>  it can be seen that a small cell subpopulation with low to intermediate cd <dig> expression was classified as being leukemic by the svm. although the cd <dig> expression of these cells may have been considered below borderline in conventional gating for leukemic b-cells, the artificial noise added to the training data shifted the svm's decision boundary to include these cells.

the artificial noise seems to be advantageous for learning classification rules in the flow cytometry setting. various sources of variability arising in the experimental procedure, such as sample contamination, incomplete staining, and instrument instability, can induce shifts in fluorescence and scattering measurements  <cit> . classification rules that apply to noisy and noise-free cytometry readouts may be insensitive to such shifts.

compared to related approaches  <cit> , the svm approach has the advantage that it does not require any control sample group. thus, it obviates the need to take blood samples from healthy persons. instead, it is based on a given cell classification, gained from established diagnostic procedures. the svm approach also does not require a discretization of the numerical data, which would reduce the data's information content, but allows for stable event classification in the high-dimensional space spanned by all measured variables. it does not aim at assigning samples to classes, but rather at assigning single cells to predefined groups. therefore, no summarization of a variable's distribution over all cells is required.

CONCLUSIONS
the svm's high classification accuracy is promising, given the fact that the classifier has been build and tested on independent data sets and the training data had been artificially contaminated. automating the gating for leukemic cells in flow cytometry readouts from blood and bone marrow samples seems highly feasible, even with moderate variations in the experimental procedure.

furthermore, the svm automation is applicable to any gating-like procedure for identifying, even small, subgroups of cells in flow cytometry readouts. one of these applications could be the identification of mrd blast cells and monitoring of response to therapy in all.

multivariate classification allows for reliable automation of current diagnostic procedures, taking into account biological dependencies that provide obstacles to simplistic methods. it has the potential to greatly reduce the time, costs and arbitrariness associated with these procedures and allows for shifting efforts to potential research extensions.

in addition, our results show that classification techniques, whose use is already well established on common biological data types, such as gene expression data, can give rise to new algorithms for the analysis of various other existing and upcoming kinds of biological high-throughput data.

