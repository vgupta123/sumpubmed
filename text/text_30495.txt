BACKGROUND
text mining from the scientific literature has been considered promising for creating and updating structured databases of biomedical knowledge  <cit>  but it often falls short and, currently, manual curation by experts is still the standard practice for this task . some argue that text mining or natural language processing  becomes unnecessary when researchers report results following a standardized template  <cit> . others argue that crowdsourcing may yield better performance than state-of-the-art nlp solutions . however, given that scientific publications are still written in free-text and grow geometrically, automatic or semi-automatic approaches may still be necessary for scalable and sustainable data curation .

annotation vs. curation
machine learning has shown its potential in biomedical nlp and text mining . however, supervised statistical learning algorithms require a large number of annotated training examples. annotation and curation are fundamentally different processes. an annotation is a label applied to a span of text. hence, an annotation appears verbatim in the source text. annotated databases are rather labor-intensive albeit ideal for text mining applications. on the other hand, curated databases are prepared by domain experts who use a common terminology to describe entities in text. curated databases require in-depth domain knowledge to produce but the result often requires post-processing before being useful as training examples.

numerous biomedical databases are available in the public domain. many of them contain data derived directly from published literature either by curation by teams of experts or submitted by authors or other scientists. a survey estimated that in  <dig>  a total of  <dig> papers on biomedical databases that were published that also provided open url links to access the data. among these  <dig> databases,  <dig>  % of them collected data from the literature and contained citations as supportive information  <cit> . however, these data cannot be readily used as training examples because the curated data rarely provide any information of where and how the data were derived from the text.

catalog of gwas
consider the catalog of genome wide association studies   <cit> , an online database developed by the national human genome research institute  by a curation team of experts. on a weekly basis, epidemiologists from nhgri and more recently from the european bioinformatics institute  manually curate study-level fields of information from published gwas and add them to the catalog. as of may  <dig>   <dig>  the catalog of gwas has been inserted with approximately  <dig>  entries extracted from nearly  <dig> distinct articles.
fig.  <dig> an examplar entry in the catalog of gwas and its currated data. example of an entry in the catalog of gwas  and after matching to the curated data in the text of the source paper  <cit>  



learning from curated data
this paper describes a general approach to using curated data from existing biomedical databases as training examples for information extraction from full-text research articles. our approach is based on results of research in agnostic learning from data with noisy labels . among the results that fit our need here is importance reweighting  <cit> , where training examples are weighted according to the reliability of their labels, which, however, is unknown in practice and must be estimated. our solution is to employ a committee of weak classifiers that match candidate passages in the input text with curated data and then apply the em algorithm to estimate the reliability of the labels. then we can use a cost-sensitive learning algorithm that learns from training examples weighted by their misclassification costs derived according to the estimated reliability to develop accurate information extractors.

the applicability of this approach is not limited to the catalog of gwas. a large number of biomedical databases are available in the public domain, and many contain data derived directly from published literature either through manual curation by teams of experts or structured information submitted by authors or researchers. we intend for our approach to be generalizable across these databases as well.

the problem of learning from imperfect training examples is closely related to entity normalization, where the training examples available are normalized entity names rather than annotations in the text. for example, the gene normalization task in the biocreative ii and iii initiative  aiming at extracting standardized gene ids mentioned in an input article provided only the standardized gene ids for each article as the training examples. the problem is also related to learning from data with noisy labels and learning from crowds , where crowd inputs are considered noisy.

the test tasks
we implement the approach and apply it to two problems of information extraction from the full-text research reports of gwas. task  <dig> is to identify target phenotypes  examined in a gwas study. this task is different from well-studied disease mention tagging and normalization  in that not all mentions but only the study targets need to be identified and that gwas targets include not only diseases but a wide range of traits like eye color, response to ximelagatran treatment, sleeping habits, reading and spelling ability, education attainment, political ideology, etc. usually, a gwas targets a single phenotype, but a study may examine more than one phenotype. this is often the case when the study target is a complex disease, such as obesity, for which scientists may seek genetic associations with related traits such as body mass index, waist-height ratio, waist circumference, etc., in addition to direct association with obesity.

task  <dig> is to extract stage  and ethnic group of the study samples. a gwas involves study samples drawn from one or more ethnicity groups. task  <dig> is concerned with the problem of extracting these ethnicity groups that the experiment pertains to. there are conventionally two stages in the study: initial and replication, and each of these can be associated with several distinct sample populations. however, many articles may not specified all information clearly in the text. figure  <dig> illustrates some examples for this task. the first row gives an ideal case for information extraction, where both stages and their corresponding ethnicity information are specified in a single sentence, though it is still nontrivial to normalize terms in the sentence to a common terminology . figure  <dig> also contains examples of sentences where an ethnicity mention is not explicitly linked to either stage, or is specified for one stage and must be inferred for the other. this can be exacerbated when these mentions are far apart in a full text.
fig.  <dig> example sentences describing sample ethnicity groups and stages. example of sentences in free text from which the system extracts study targets



task  <dig> is an entity recognition problem while task  <dig> requires jointly extracting different attribute types and identifying linkage between them. for both tasks we develop information extractors by applying the same general approach to learning from the curated data in the catalog of gwas as the training examples. we are able to achieve  <dig> % of extraction in precision-at- <dig>  for the task  <dig> and  <dig>  in f1-score for the task  <dig>  both outperform their baseline, cost-insensitive counterparts. the remainder of this paper describes the method and the results in details.

methods
we start by describing our general approach and then follow by presenting the implementation of the general approach for the two test tasks.

the approachfig.  <dig> system architecture. our proposed system architecture summarizing the steps  in the machine learning training process



step  pairs each passage with a piece of matched curated data and creates a feature vector for the pair as the input to the committee classifiers. for example, we pair passage  <dig> in fig.  <dig>  to data item “ <dig> indonesian individuals” from the catalog of gwas, because passage  <dig> is likely from where the data item was derived. again, the matching should be inclusive to contain all potential pairs.

step  then sends the feature vectors to a committee of classifiers . each classifier classifies each pair into positive, if the passage is deemed to contain the information given in the curated data, or negative otherwise. the classifiers can be as “weak” as simple decision rules, like “whether the passage contains a substring that exactly matches the curated data”. therefore, each committee member classifier provides noisy positive-negative labels of the passages extracted from the text. combining the classification results of all committee members for all extracted passages creates a a large matrix of yes-or-no votes, where each element  contains the vote from classifier i for candidate passage j.

step  estimates from the matrix the probability that candidate passage j is truly positive by a label estimator that applies an expectation-maximization  algorithm to compute maximum likelihood estimation of the probabilities, which can then be treated as the weight, or the reliability of a candidate training example. a similar approach was used in the biocreative iii gene normalization task  <cit>  to create a silver standard. the em algorithm works as follows:
input: matrix m of committee -passages , where each element in the matrix is either positive  or negative ;

let pi be the probability that the i-th passage should be positive, ej be the error rate of the j-th committee classifier; let t=0;

initialize ej= <dig> for all j;

update for all i, pi=∑)mij+kj+k, where j is the number of weak classifiers in the committee, k/k is the laplace prior;

update for all j, ej=∑pimij+k′i+k′, where i is the number of the passages, k′/k′ is the laplace prior;

t=t+ <dig> and repeat update steps until convergent;

output: pi^=pi for all i and ej^=ej for all j as the final values.



with the estimated probability of each candidate passage, we can assign it a cost, and train a cost-sensitive learner  using the candidate passages as the cost-weighted training examples to learn to select correct passages that contain the desired information as step . the cost that we use here is derived according to lemma  <dig> in  <cit> , where the problem of classification with noisy labels is solved by importance reweighting. they show that an error bound can be achieved if the misclassification cost of a training example  is set to p/pρ, where ρ denotes sampling from a noise-perturbed distribution. though neither p nor pρ are known, we can approximate p by p^ from the em algorithm above and pρ by p> <dig> ) for a training example estimated as positive and analogously for a negative one. that is, let yi=round. if yi= <dig> then the cost of misclassifying the i-th passage as negative is ci=pi^∑iyi/i, else ci=pi^1−∑iyi/i.

we note that this cost-sensitive classifier may use a completely different set of features to characterize a passage.

after the cost-sensitive learning completes, to extract desired data from a new article, we apply the same step  to extract candidate passages and send them to the trained cost-sensitive classifier and extract data from the candidate passages classified as positive.

task 1: identifying target phenotype 
data
the curated gwas target phenotype data are in the form of a spreadsheet dated may  <dig> available for download at  <cit> . each row in the spreadsheet contains a column diseasetrait, reporting a phenotype term chosen by curators as the study target of the paper with its id given in column pubmedid. as we discuss earlier, these phenotype terms do not always exactly match what is mentioned in the text. column efotrait contains the phenotype term mapped to a concept defined in experimental factor ontology   <cit> , the standard terminology of diseases and traits for the catalog of gwas. efo classifies concepts into  <dig> high-level categories as given in column parent. for example, diseasetrait for the paper of pubmed id  <dig> is “male-pattern baldness”. the corresponding efotrait and parent are “androgenetic alopecia” and “other disease”, respectively. these columns constitute the curated data that we intend to use to train our information extractor.

the spreadsheet contains data for  <dig> unique pubmed ids of gwas articles. among them,  <dig> papers have full text available in the nxml format  <cit>  from pubmed central. nxml provides useful metadata for nlp and text mining such as section headings to distinguish main text from references and other elements but currently only about one third of all pubmed papers have their nxml version available from pubmed central. for other papers, we collected  <dig> full texts in pdf. these pdf files were all transcribed into xml using biopdfx  <cit> , a tool that we built on top of pdfx  <cit> , to prepare pdf papers for biomedical text mining.

for the purpose of evaluation, we manually augmented the data with the corresponding terms that actually appear in the text of the  <dig> nxml articles to serve as our hold-out gold standard. we kept the data of the  <dig> pdf papers intact and used the data for training.

the spreadsheet contains a total of  <dig> rows for  <dig> unique papers. that is, each paper has on average  <dig>  target phenotypes. three hundred thirty three papers  have more than one target phenotypes in the curated data. we therefore measure the performance based on the precision-at- <dig>  metric, that is, if either of the top  <dig> extracted phenotypes match the gold standard, the extraction is considered correct.

implementation
step : passage extractor. in this step, we identify all mentions of any disease or trait using an exact string matching approach, which is based on a dictionary of all diseases and traits from the search menu of the web query interface of the catalog of gwas at  <cit> .

after string matching, we extracted  <dig>  mentions in the training and  <dig>  in the test data. note that these numbers are the total mentions of any disease or trait in all articles in the data set, because a paper usually contains multiple mentions.

step : feature creator. the following features are generated for each training sample:
token-based features: character-level n-grams of the mention.

context-based features: word-level and character-level n-grams for up to  <dig> words before and after the mention.

position-based features: the location of each mention can be indicated using positional tags  in the converted xml papers; therefore, whether a mention is located within positional tags are extracted as binary features. these tags, however, are not always available from the pdf-transcribed xml versions.



for each mention, token-based and context-based features are represented as normalized tf-idf vectors. together with position-based features, each mention has approximately  <dig>  features.

step : committee of classifiers. to build the committee matrix, we design five rule-based binary classifiers as follows.
title or abstract: whether disease/trait mention occurs in the title or abstract of the paper; this is a simple yet strong indicator of a disease mentioned being the actual target of the paper.

exact match: whether disease/trait mention exactly matches the target given by human curator.

sub-string match: whether disease/trait mention partially matches the target given by the human curator .

synonym: whether disease/trait mention is an exact or partial match of a synonym of the target determined by the human curator. the synonyms are collected from umls  <cit> ; for a given disease or trait mention, all umls concepts that shared the same concept-id are considered to be synonymous. to reduce noise, we only keep the synonyms which are in english and are preferred terms .

compound token: whether the mention has multiple tokens separated by a space or hyphen .



it should be noted that although some rule-based classifiers are extremely weak , our idea is to show that multiple weak classifiers can actually contribute to a strong committee and make accurate predictions.

step : label estimator. we apply the em method described previously to label each pair of passage and curated disease or trait with an estimated confidence .

step : cost-sensitive learner. in this step, we utilize the estimated confidence generated by the label estimator to assign the cost to train a cost-sensitive variant of support vector machine .

post-processing. each paper may contains multiple mentions of various disease and traits. for example, a paper may contain  <dig> mentions of “diabetes”, and  <dig> mentions of “hypertension”. however, our cost-sensitive classifier may predict only part of them to be positive. this is reasonable because even though two sentences mention the same phenotype, it is not always the case that both are stating that the phenotype is the study target. we consider the following two scores for the post-processing, inspired by tf and idf, respectively:
ptf=vpivpi+vni, where vni is the number of negative votes assigned to the i-th candidate.

pidf=vpi∑vpi, where vpi is the number of positive votes assigned to the i-th candidate.



to combine ptf and pidf, we apply two mean computations, namely arithmetic and harmonic, to calculate the final scores and determine our predicted disease/traits. the harmonic mean better represents the mean value of these two metrics. that is because the ptf and pidf values are often quite small and include outlying values. harmonic mean is a more sensitive measure in such cases.

task 2: identifying stage and ethnicity of study samples
we represent this problem as that of extracting tuples of the form 〈stage, ethnicity 〉 from the free text of a gwas article, with the entities in the tuple corresponding to stage and ethnicity of the study sample.

data
again, our articles are selected from the catalog of gwas. the data in the form of a spreadsheet is available for download at  <cit> .

we selected articles that satisfy the following criteria:
curated data available:  <dig> pubmed articles were curated with the data available.

nxmls or pdfs available: we used nxml versions of the articles if they are available through pubmed central. these versions have high-quality text. otherwise, we transcribed pdf versions of the remaining articles to text. this leaves  <dig> articles.

no missing values or “nr”: the characteristics of the samples are available for whichever stage is mentioned in the article, and the curated data contain no blank entries. also excluded are those for which curators were unable to find a conclusive ethnicity group for the sample and the entries state “nr” . this leaves  <dig> articles.

ethnicity mentions in text: terms that correspond to ethnicity groups must be available in text .

do not contain errors: the curated data was found to contain errors in the entries for some articles. those were excluded.



the final dataset consists of  <dig> articles, comprising  <dig> 〈stage, ethnicity 〉 tuples.

the curated data is normalized to remove spelling errors and inconsistent wording primarily to ensure that there is only one top-level term for a given ethnicity entity. for example, ethnicity group entries in the curated data stating “north african/middle east” or “middle east/north african” are both considered to correspond to “middle east/north african”, with this choice of the eventual top-level entry being made arbitrarily.

implementation
we applied the same pipeline given in fig.  <dig> but we employed two committees to extract tuples with two elements: step passage extraction: mentions in the text corresponding to ethnicity entities are tagged and their surrounding passages extracted. these instances are  labeled according to curated data as positive or negative. step feature creator: the ethnicity instances are featurized and made suitable for classification. step committee of positive/negative classifiers: a committee of weak learners are exploited to generate noisy labels, for cost-sensitive learner to classify ethnicity instances as positive or negative instances. committee of initial/replication classifiers: ethnicity instances classified as positive are further classified into the initial and replication experimental stages of the gwas. for both committees, we perform step label estimator using em algorithm, followed by step cost-sensitive learner to predict the ethnicity and stage of the mentions. post-processing: instances of ethnicity classified into a particular stage are grouped as 〈stage, ethnicity 〉 and duplicates removed. the performance of this method is evaluated upon this final set of results.

these steps are described in detail below.

step : passage extractor. mentions in text are generally not exact string matches  of ethnicity groups, necessitating a dictionary mapping of mentions in text  to the top-level ethnicity entity . mentions in the text that correspond to a top-level ethnicity entity are mapped to their corresponding entities and tagged with the help of a constructed dictionary as described below, followed by passage extraction. mentions corresponding to a stage are not tagged.

we construct the dictionary of ethnicity mappings as follows. a multitude of terms can refer to the ethnicity of an individual, including the country of origin , the specific ethnicity group , an adjectival for the country , a demonym for the country , and similar sets of terms for cities and other regions. we handle these terms through the conventions:
country/region name: not every mention of a region, say, a country, maps to a specific ethnicity term. the nhgri curation guideline  <cit>  stating that a given set of individuals belong to an ethnicity group only if it is directly stated in the study, or if at least  <dig> % of the population of the region is known to belong to a single ethnicity group, with this knowledge being based on the cia world factbook  <cit> .

adjectivals and demonyms: an extensive list of the adjectivals and demonyms for countries are obtained from wikipedia and a dictionary is constructed to map the terms to their corresponding countries. these countries are then mapped to the corresponding ethnicity group .



the final dictionary comprises  <dig> terms that map to  <dig> top-level ethnicity groups. these terms cover a majority of the mentions in text, and we omit publications that do not contain language that can be matched to this dictionary. a more comprehensive dictionary may include lists of tribes and indigenous peoples of the world.

this dictionary is used to match mentions in text to ethnicity entities through string matching. the tagged instances are extracted along with their corresponding passages, which consist of the  <dig> words on either side of the entity in the sentence.

for training and testing, these instances are weakly labeled from curated data by checking if, for a given article, the ethnicity group is present in either experimental stage, initial or replication. if so, this is considered a positive instance, and negative otherwise.

step : feature creator. the following types of features are generated for each instance:
token-based features: a set of binary features each of which turn on for a specific ethnicity entity .

context-based features: these include normalized term frequency-inverse document frequency  representations of unigrams and bigrams of  <dig> words in either direction of the ethnicity mention, as long as the words are within the same sentence. the words are stemmed using the porter stemmer  <cit> .

position-based features: these include features like section title , the distance  of the ethnicity mention from the start of the article, or from the start of the section.

additional features: these include features that do not fit into the above categories, such as the number of times the ethnicity entity was observed  in the same article.



this results in sparse feature vectors of approximately  <dig>  dimensions. the features are normalized by removing the mean and scaling to unit variance across the values of each feature, or dimension of feature vector. as the feature set is mostly composed of various tf-idf vectors, truncated singular value decomposition , or latent semantic analysis, was explored as a feature selection technique to reduce the dimensionality of the feature vectors. however, this did not affect the performance significantly  and hence the complete feature vectors were retained. the complete list of the features are given in .

step : committee of positive/negative classifiers. we use the cost-sensitive learning approach described previously to classify instances as positive or negative. the committee members of weak labelers include:
binary classifier: the results of a logistic regression binary classifier trained on the weak labels from curated data.

rule-based classifier: this classifier predicts a positive example if the features meet any criteria, such as the presence of words that are commonly found in descriptions of a sample .  <dig> such terms are used in total.

weak labels from curated data: the labels obtained by exact-matching the ethnicity to the curated data.



the committee matrix obtained from concatenating the outputs of all the members is used to estimate the cost to be assigned to each training instance, as in previous sections. these costs are used to train a cost-sensitive, l2-regularized, linear support vector machine  classifier to classify instances as positive or negative instances.

committee of initial/replication classifiers. training a cost-sensitive classifier to classify positive instances of ethnicity entities into the corresponding stage of a study is performed in a similar fashion to that for the ethnicity instance classifier.

in this case, the positive training instances are now relabeled “initial” or “replication”. the committee members are:
binary classifier: as above, but trained to distinguish “initial” from “replication” instances.

rule-based classifier: the rule-based classifier is modified to use the presence of stage-specific words to make its prediction .  <dig> such terms are used.

weak labels from curated data: as above, but containing classes “initial” and “replication” instead.



the outputs of the members are used to construct the committee matrix and estimate the cost assigned to each training instance, which is then used to train a cost-sensitive, l2-regularized, linear svm to classify the test data into the initial or replication stages with the same set of features.

the output of this step is a classification of each positive ethnicity instance into a specific stage .

post-processing. either stage in a gwas may have multiple ethnicity groups. hence, the extraction can possibly result in multiple tuples of the form 〈stage, ethnicity 〉 for each study. we compile a list of such tuples for each article, with duplicates being discarded.

RESULTS
results of task 1: identifying target phenotypes 
we use precision at  <dig>  to measure the performance because a gwas may examine one or more phenotypes. if either of the top  <dig> extracted phenotypes match the gold standard, the extraction is considered correct.

we compare our cost-sensitive approach with a cost-insensitive baseline. also, we attempted the following additional alternatives to improve the cost-sensitive learner:
bioadi: since in many articles, the target phenotype only appears once in its full form and all following mentions are in its abbreviation, we identify and normalize the abbreviations in the input text using the bioadi system  <cit>  to pre-process the text in an attempt to improve the performance.

conditional random field : in order to deal with new diseases and traits that do not appear in our training dictionary, we also tried to apply crf in the passage extractor step. the design of the features for the crf is based on the method described in  <cit> ; we use a mixture of general linguistic, orthographic, contextual, syntactic dependency, and dictionary lookup features. by using this crf model, we discover  <dig>  mentions in test data, which is, however, less than the number of mentions using dictionary matching.



we train all systems with the data of  <dig> articles and test the trained systems with the hold-out data of  <dig> articles. table  <dig> shows the performance results of these systems. the cost-sensitive learner outperforms the cost insensitive learner and the harmonic averaging outperforms arithmetic averaging. however, the alternatives to improve passage extraction  fail to improve the result of the cost-sensitive learner.


we experiment with more than  <dig> extractions as well. if we consider top five extractions, the accuracy of at least one of them being correct is higher than  <dig> %. if we consider a single extraction, the accuracy drops by a few percentage points but stays above  <dig> %.

discussion of the results of task 1
the experimental results show that learning from curated data is feasible to accomplish a p@ <dig> up to  <dig> % for the task of extracting study target and that the cost-sensitive learning approach outperforms the cost-insensitive baselines.

we analyze the errors and summarize that many errors are from pharmacogenomic studies, which examine phenotypes such as “response to antipsychotics” as given in the curated data. antipsychotics are a class of psychiatric medication instead of a specific medication, while the disease targets of the medication, such as “schizophrenia”, “major depressive disorder”, etc., may present stronger signal as the study target than the response of a medication for an information extractor. studies of complex diseases also pose a main challenge because a complex disease may have many associated measurable traits.

results of task 2: identifying stage and ethnicity of study samples
we evaluate the performance by comparing them with the 〈stage, ethnicity 〉 tuples known to correspond to each gwas article. further, we also compare the results with the following alternative approaches. the evaluation methodology and metrics are described below.
baseline: all ethnicity instances tagged by the dictionary in an article are assigned to both experimental stages, and the results measured.

cost-insensitive classification: the framework described above is used in a cost-insensitive fashion by excluding the committees and directly training the classifiers on the curated labels. this provides a candidate for comparison to the cost-sensitive approach for evaluating the performance of cost-based learning.

cost-sensitive classification: the framework described above, including committee classification, is used.



in each of the methods , five-fold article-based cross validation  is performed. the articles in the dataset are randomly shuffled, and each fold of the 5-fold cv utilizes all 〈stage, ethnicity 〉 tuples belonging to  <dig> % of the articles in the dataset as training data, and the tuples in the remaining  <dig> % of articles as test data.

the results from each fold are then collected to obtain 〈stage, ethnicity 〉 tuples for all the articles in the dataset. these results are compared against the curated data and the f <dig> score calculated in the standard way:
if a 〈stage, ethnicity 〉 tuple in the result for a specific article is present in curated data for that article, it is considered a true positive ; otherwise, it is considered a false positive .

if a 〈stage, ethnicity 〉 tuple in the curated data for a specific article does not have a counterpart in the extracted results, it is considered a false negative .



using this, we calculate the precision, recall and the macro f <dig> score for each method on  <dig>  articles comprising  <dig>  〈stage, ethnicity 〉 tuples from approximately  <dig>  mentions of ethnicity-related terms. the resultant values are tabulated in table  <dig> 

table  <dig> performance of stage-ethnicity extraction 



the results in tables  <dig> and  <dig> indicate that the cost-sensitive approach is able to significantly outperform the similar but cost-insensitive approach, which performs only close to a brute-force baseline. not only is the cost-sensitive approach able to achieve a much higher degree of recall, but the improvement is accompanied by an increase in overall precision as well.

as the recall gets closer to the limit, the results also indicate that further improvements to the method will be gained by focusing not on extracting relevant ethnicity groups, but on eliminating the ones that are irrelevant to the article.

discussion of the results of task 2
the results show that a cost-sensitive committee learning approach reliably outperforms a similar, cost-insensitive approach. this holds true even when the additional committee members are weak classifiers that encode real-world domain knowledge and patterns as rules, which can compensate to some extent for the lack of data, as it is not presumable that all patterns are present in the data in significant quantity as to be learned by a model.

some of the challenges faced in the task of extracting ethnicity groups of sample populations from the catalog of gwas are described below.
entity normalization: there are various ways of representing the same entity, and it is necessary to normalize these representations to a single representative entity. however, there exist degrees of difficulty with respect to normalization; for example, it is relatively easy to equate “african american” to “african-american”, but much harder to equate the two represntations with “american citizen of african origin”.

studies with several target entities to extract: many gwas in the u.s. use a highly ethnically diversified study sample with, for example, “ <dig> % caucasian,  <dig> % latino,  <dig> % african,  <dig> % eastern asian and  <dig> % indigenous americans”, and studies may also divide into more than two stages. how to flexibly identify and deal with these situations is challenging.

varying concept granularity: mentions of ethnicity terms might not be correctly tagged in an article as the authors may report ethnicities in specific terms such as names of tribes and indigenous people, etc., which may not map perfectly to a top-level ethnicity group. this introduces ambiguity which can be an issue for ethnicity background identification.

inadequate reporting of ethnicity data: often, the importance of a study to a specific population or application only becomes apparent after the article is published. hence, the text in the article may never refer to the specific ethnicity group that their experimental sample was drawn from, but simply describe it in terms of the city, state or region, or even the hospital that the population was recruited at. this is doubly challenging as it requires an indefinite expansion of the dictionary of ethnicity-related terms, and also a standardized mapping from each such term to a top-level ethnicity group.



CONCLUSIONS
the large number of curated biomedical databases available in the public domain provides an unprecedented opportunity to train nlp systems to comprehend biomedical publications. in this paper, we present an approach to take advantage of this opportunity. the approach applied methods from learning from noisy-label and committee classifiers to assign costs to train cost-sensitive classifiers. we tested our approach for two challenging biomedical information extraction tasks. the results show that our approach is effective and outperforms alternative approaches. we will continue to investigate if it is possible to define standard passage extractors and weak learners applicable to extract biomedical entities, attributes and relations of common interest to enable rapid development and portability between domains for biomedical literature mining.

declarations
research reported in this manuscript and the publication costs for this manuscript were supported by the national human genome research institute  of the national institutes of health under award number u01hg <dig>  the content is solely the responsibility of the authors and does not necessarily represent the official views of the national institutes of health.

additional file
additional file  <dig> 
complete list of the features used for task  <dig>  



from the fourteenth asia pacific bioinformatics conference 
san francisco, ca, usa.  <dig> -  <dig> january 2016

competing interests

the authors declare that they have no competing interests.

authors’ contributions

cnh conceived, designed, and supervised the study and secured funding. sj developed the software system and designed and performed the experimental evaluation for task  <dig>  kt developed the software system and designed and performed the experimental evaluation for task  <dig>  sb developed the software system to transcribe part of test articles in pdf into xml. sj, kt, sb, ttk, and gl were involved in the debugging and optimization of the software systems. cnh, sj, kt, and ttk drafted the manuscript and all authors contributed substantially to its editing. cnh takes responsibility for the manuscript as a whole. all authors read and approved the final manuscript.

declarations

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2016: selected articles from the fourteenth asia pacific bioinformatics conference . the full contents of the supplement are available online at http://www.biomedcentral.com/bmcbioinformatics/supplements/17/s <dig> 

