BACKGROUND
an important task of bioinformatics research is to acquire and represent biomedical knowledge in computable form so that it can be efficiently stored, retrieved, and used for discovery of new knowledge. for example, the gene ontology  consortium  <cit>  and the gene ontology annotation  project  <cit>  are dedicated to the task of representing biological knowledge with the controlled vocabulary of go terms. knowledge of protein functions serves as a cornerstone of modern biomedical knowledge. much of such knowledge is contained in the form of free text in biomedical literature. a more compressed and accessible representation of this same knowledge is contained in bibliographic databases, e.g., medline. in addition to current manual annotation efforts, needs for automatic knowledge acquisition and representation exist, and a critical step of this process is to extract biological concepts from free text.

the task of automatic knowledge acquisition from free text is usually addressed within the frameworks of the natural language processing , information extraction , and information retrieval  techniques  <cit> , which has been wide applied in bioinformatics setting, as reviewed in  <cit> . recent trend in text mining is to acquire deeper semantic information from text, e.g., semantic information has be used to cluster genes  <cit>  and evaluate the functional coherence of a group of genes  <cit> . extracting semantic information from free text requires the capability of effectively dealing with the uncertainties commonly associated with human language. to this end, probabilistic semantic analyses serve as promising approaches for handling such uncertainties and performing semantically enriched text mining.

in this paper, we report extraction of semantic topics/concepts from a corpus of medline titles and abstracts using a probabilistic topic model, the lda model  <cit> . the goal was to identify the major and recurring concepts that represent the major knowledge domains of protein functions. furthermore, extraction of the semantic contents of a document provides a parsimonious and concise representation of that text. such information can be used for efficient indexing, information retrieval, and protein annotation.

RESULTS
representing semantic topics with a probabilistic topic model
in a scientific article, a scientist will refer to multiple real world objects and/or concepts, thus a paper usually consists of multiple topics/subjects, e.g., a paper may discuss a protein located in mitochondria and involved in the cellular process of apoptosis. when discussing objects or concepts, the author will choose certain words to convey the semantic meaning. for instance, when discussing the topic mitochondria, words like 'electron,' 'cytochrome,' and 'atp' are commonly used, while words like 'apoptosis,' 'programmed,' 'death,' and 'caspase' are commonly used to discuss the concept of apoptosis. thus a document can be treated as a mixture of words from multiple topics. the lda model represent such a notion by explicitly encoding multi-topicality of a document with a topic-composition variable and then simulating the "generation" of words by accordingly mixing words from topics, which are represented as multinomial distributions over a vocabulary, i.e., a word-usage pattern. figure  <dig> shows how a topic can be represented as word-usage pattern in a probabilistic topic model. given a corpus of text documents, the lda model is capable of extracting the topics by statistical inference as described in the methods section.

training of lda model
the lda model was applied to extract the semantic topics from a corpus of medline titles and abstracts downloaded from the goa project website as described in the methods section. the training of an lda model requires specification of the number of topics for the models, an issue of interest from both semantic analysis and statistical learning view points. from a semantic analysis point of view, this is equivalent to determining the granularity of abstraction of the concepts that can be used to summarize the semantic contents of the corpus. from the statistical learning point of view, this is equivalent to select among the models with different complexity. a bayesian model selection framework was employed to determine the "optimal" number of topics based on the posterior probability of a model, p. to perform the bayesian model selection, samples of the latent semantic topics, z, were collected for a model with a given number of topics, t, and the approximate the posterior probabilities were calculated according to equation  and plotted . the model with  <dig> topics had the highest approximated marginal likelihood and was thus used for the analyses reported in this paper.

evaluating semantic topics
a trained lda model returns estimated distributions of the following parameters and latent variables:  the word-usage distribution, φt, for each topic;  the latent topic labeling zi for each word wi; and  the topic-composition distribution θd for each document. the parameter vector φt is a distribution representing a word-usage pattern for the topic t. high probability words of each φt can be thought as the words frequently used to discuss the topics. in table  <dig>  the  <dig> most commonly observed topics and their high probability words of the trained lda model are listed. the topics are sorted in descending order according to the number of words assigned to them in the corpus. high probability words of these topics constitute clusters of words that coherently convey biological concepts. for example, topic #  <dig> reflects the concept of ligand-activated receptors, and the topic #  <dig> is related to serine/threonin kinase activity. because the lda model attempts to capture the major topics that can be used to "generate" the data, the concepts extracted by this model should reflect the recurring themes of the corpus. indeed, when multiple models with  <dig> topics were trained with different random-number seeds, similar major topics were extracted although the index of the topics differed among the models. thus, the topics listed in table  <dig> do reflect common biological themes in our corpus.

inferring the semantic content of a text
the instantiated latent variables zd indicates the semantic contents of the document. for the text in the training data set, the topic contents for each document were returned as the estimated latent variables zd of the trained model. for a newly observed text, the topic contents can be inferred by invoke the sampling algorithm with the estimated parameters as described in the methods section. figure  <dig> shows an example of a medline abstract, in which topic assignment for the words were inferred using a trained lda model. this abstract discusses a protein referred to as apoptosis inducing factor , a mitochondrial protein that induces apoptosis. in this figure, the inferred semantic topic for each word  is shown as the superscript numbers next to it. the abstract is associated with the following go terms:  go: <dig>  dna damage response, signal transduction resulting in induction of apoptosis;  go: <dig>  electron carrier activity;  go: <dig>  mitochondrion; and  go: <dig>  dna fragmentation during apoptosis. in figure  <dig>  two major topics, #  <dig> and #  <dig>  are the dominant topics of the abstract. topic #  <dig> is related to the mitochondrion and topic #  <dig> reflects the concept of apoptosis. interestingly, several words, which can belong to multiple topics depending on context, were found in the abstract, e.g., "space" and "outer." the lda model has captured their common occurrence in the context of mitochondrion and correctly assigned these common words to this topic based on the context. with the inferred topics, this abstract can be readily indexed with these two major topics which agree well with the human go annotations of this abstract. furthermore, a document can also be indexed as a vector containing the counts of the words in each topic or with the normalized estimated θ^d
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwf4oqcgaqcamaabaaaleaacqwgkbazaeqaaaaa@2ff6@, which be treated as a vector in the space spanned by the topics. such representation effectively projects the document from the high dimensional vocabulary space onto the reduced-dimensionality of topic space. such information could be used to automatically index the text.

assessing biological relevance of topics
the lda model simulates the "generation" of a corpus. by its generative nature, it will incorporate topics needed to capture the common characteristics in the corpus. however, some common features may not be necessarily relevant to biology but merely reflect the linguistic feature of the corpus. to determine the biological relevance of topics, we further inspected the high probability words and assigned a biological relevance score, ranging from  <dig>  to  <dig>  to each topic. a histogram of the assigned biological relevance scores  indicates that most topics/concepts extracted from this corpus were biologically relevant, with only a fraction with biological relevance scores equal to zero, indicating no biological relevance.

each medline abstract from the goa corpus was associated with one or more go terms, providing an opportunity to study the relationship between the semantic topics extracted by the lda model and the go annotations. the correlation between the semantic topic and the go annotation can be quantified by mutual information  between the latent topic and the annotated go terms. mi is a symmetric, non-negative quantity that measures the relevance  of one variable with respect to another variable, which equals zero if and only if the variables are independent. since go terms are designed to represent biological objects/concepts, the topics highly relevant to biological objects/concepts should have high mi with some go terms, while the topics irrelevant to biology should have low mi values for topic-go association. indeed, as shown in figure  <dig>  the topics rated low relevance have very low mi with any go terms, while topics with high relevance have the highest topic-go mi . however, there were some topics that were assigned high relevance scores but had low mi with go terms. this disparity was likely due to the way the mi for a topic-go association was calculated in this study, which specifies that, if a document was annotated with a go term g, every word in the document was considered as annotated with that go term. this method was adopted due to the lack of supervised training data specifying which words in a document were responsible for the go annotations. mi calculated under this assumption is skewed for the relatively uncommon topics in the corpus. nonetheless, the mi of topic-go association serves as a criterion of evaluating the biological relevance of a topic. when a topic had a high mi value for a topic-go association, it usually reflected a coherent biological concept. interestingly, a topic with low biological relevance did not mean that it was not a coherent semantic concept. for example, topics #  <dig> and #  <dig>  consisted of common english words that therefore had the lowest mi with any go term. however, the topics did contain the words that constitute coherent semantic concepts, e.g., topic #  <dig> contains words related to the concept of being unique.

associating topic with go terms
studying the correlation between the topics and the go terms also allowed the mapping of topics to the controlled vocabulary of go terms, laying a foundation for possible future automatic annotation/indexing of medline abstracts with the go terms. while annotating a gene product based on biomedical literature, a human curator needs to extract and summarize the semantic concepts of the literature, find a go term that is semantically close to the concepts, and assign that go term to the gene product. to identify the potential matching go terms for each topic, the mi values for all observed topic-go associations were calculated. then, for each topic t, a go term from each of the three go categories with the highest mi value was treated as the candidate go term matching the topic. table  <dig> shows examples of associating the extracted semantic topics with the go terms. the top  <dig> rows are the topic-go associations with high mi values, while the bottom  <dig> rows are examples of topic-go associations with low mi. when mi values for topic-go associations were high, the definitions of the go terms usually agreed well with the semantic concepts contained in the latent topics. interestingly, the inference of the topics by the lda model mimics the process of identifying the biologic concepts from the texts by a human curator; and determining the mi  of topic-go association mimics the process of mapping the biological concepts to the go terms. thus, mapping latent topics to go terms potentially provides a means to automatically annotate a protein with go terms based on the semantic concepts contained in the associated literatures.

clustering proteins according to their functional descriptions
in a topic that strongly related to a specific biological object or process, i.e., when mi of topic-go association was high, the names of the proteins involved in that process frequently appeared on the top of the word list for the topics. for example, topic #  <dig> in table  <dig> is related to threonine/serine phosphorylation process, and the protein names 'pkc,' 'akt,' and 'ste20' were among the most frequent words of the topic, indicating that the lda model was capable of clustering gene/protein names according to the concept of protein functions. interestingly, clustering of these protein names did not require them to co-occur within the same documents. the lda model was capable of clustering the gene/protein names simply based on their associations with some common key words of the biological concepts. this finding could be used as a tool to cluster genes with similar functions from different organisms based on their associated literatures. this finding also agrees with a previous study by homayouni et al  <cit> , in which proteins were represented as points in the vocabulary space based on their associated literature, and they were further projected onto a reduced-dimension semantic space constructed with the lsi techniques. the proteins with similar functions were form clusters within semantic space.

discussion
most biomedical knowledge is stored as free text in the biomedical literature, and the size of the biomedical literature is increasing rapidly. there is an urgent need for automatically acquiring and representing this body of knowledge in a computable form to facilitate the discovery of new knowledge, which requires the development of computational methods to extract knowledge from the text. the current state of the art of the text mining approaches have applied to biomedical literature and reported in several recent challenge evaluations, such as the kdd, the biocreative, and the trec  <cit> . however, most of these approaches are within the conventional nlp, ie, and ir framework, and the application of probabilistic or non-probabilistic semantic modeling of biomedical literature remains relatively sparse  <cit> .

in this paper, we report the extraction of a set of semantic topics from a corpus of protein-related medline titles and abstracts with the lda model. the key advantages of applying an lda model to perform statistical semantic analysis includes, but is not necessarily limited to the following:  it model is capable of extracting major recurring themes from a corpus of text in a unsupervised manner;  the assumption that a document is a mixture of topics naturally simulates real world text and allows modeling of text at finer granularity; and  it can effectively resolve many ambiguities commonly association with natural language.

recurring biological themes reflect knowledge domains
the lda model identifies topics from a text corpus by capturing the covariance of the words and organizes the words that tend to co-occur into a structure that mimics a topic. the inference algorithm for the model is unsupervised, precluding the need of expensive, manually-annotated data. the generative nature of the lda model ensures that the extracted topics/concepts reflect the recurring themes within the corpus. we used a well-annotated data set from the uniprot database  <cit> , thus the major topics identified from the corpus arguably reflect the major domains of our knowledge of proteins.

we applied a bayesian model selection approach to determine the "optimal" number of topics for the purpose of model fitting. the bayesian model selection favors the simplest model that explains data well  <cit> . with such a preference, many of the  <dig> topics in our results reflect the general themes of the corpus. however, the model is also capable of capturing strong co-occurrence patterns that are highly specific biological objects/concepts, as demonstrated in table  <dig>  as more training data become available, especially as full electronic texts of the biomedical literature become available, the bayesian model selection can accommodate more complex models thus simulating the data with finer granularity. one limitation of the lda model is that it requires a specified number of topics in order to model the data. however, it is a strong assumption to specify that a corpus is generated with a fixed number of topics, which may not be valid in the real world. to address this issue, recent development in the nonparametric approaches, such as the dirichlet process based methods may be more reasonable to model the data without a specified number of topics, such as in the dirichlet process related models  <cit> .

in the lda model, a topic is represented as a distribution reflecting the word-usage pattern. one key advantage of the lda model is that the extracted topics correspond to real world objects or concepts that are readily understandable by people with domain knowledge. in comparison, another extensively studied semantic analysis approach, the latent semantic indexing  model  <cit> , cannot recover understandable semantic topics from text. the lsi model also captures the covariance of the words from a collection of text and identifies the major directions of the covariance space. it applies the singular value decomposition  approach to identify the orthogonal directions of semantic space spanned by the word vectors of the documents and uses major directions to represent the semantic space with a reduced rank. thus, a document can be represented as a vector in a reduced-rank space spanned by few major directions – a process of indexing the document with respect to semantic directions. however, restricting the semantic directions to be orthogonal to each other, the lsi identifies the directions that may not correspond to any human-understandable topics, thus remaining "latent."

semantic analysis and automatic indexing
as shown in figure  <dig>  the lda model can be used to extract semantic contents of an abstract, indicating that the model should be useful for automatic document indexing and information retrieval. in comparison to conventional information retrieval by keyword indexing, semantic indexing by lsi has been demonstrated to be more accurate  <cit>  due to the fact that semantic indexing allows retrieval of documents whose semantic contents align well with the semantic meanings of the query terms, without requiring occurrence of the exact query terms in the documents. although not yet tested on as large a scale as the lsi, the lda model should have similar indexing power due to the fact that the semantic concepts extracted by the lda aligns well with human perception.

we have shown that many of the topics extracted by the lda model can be mapped to the controlled vocabulary of go terms, potentially serving as a means of automatically annotating a protein-related corpus. currently, most go annotations are manually performed by phd level biologists at different centers of go consortium. although accurate and specific, manual annotation is labor-intensive and cannot be expected to keep up with the pace of growth in the biomedical literature. automatic annotation of proteins based upon the biomedical literature is a growing and urgent task facing the bioinformatics community that motivated the specific tasks in the recent competitive evaluations  <cit> . our results indicate that it is possible to extract salient biological concepts from a large amount of biomedical literature and map the concepts to the controlled vocabulary. although the mapping between the latent topics from the lda model to the go terms may not provide annotations as specific as manual annotations, automatic annotation based on the lda should provide general and consistent descriptions of a protein

dealing with ambiguities of natural language
human natural language is full of ambiguities confounding the results of contemporary nlp, ie, and ir techniques  <cit> . most noticeably, the phenomena of polysemy and synonym need to be effectively addressed during nlp, ie, and ir. the lda effectively handles the uncertainties and ambiguities caused by the polysemes and synonyms due to its probabilistic representation of the topics. the distributional representation of concepts allows the synonyms to be group into a common topic, while a polyseme can participate in multiple concepts. such representation effectively captures the key relationship between the words and semantic concepts: the concept is conveyed by choice of words and sense of a word is dependent on context. the inference algorithm of the lda model explicitly utilizes such relationships to infer the topic for a word, so that the semantic topics of synonyms and polysemes can be assigned based on the context of text. this capability makes the lda model a powerful tool to enhance the performance of other nlp, ie and ir techniques for text mining. the result shown in figure  <dig> serves as a good example of the capability of the lda model to properly assign words to topics depending upon context. note that the words "space" and "induce" are general words that fit into different semantic context, and the lda algorithm correctly associated them with the concepts of mitochondria and apoptosis, respectively, based on the semantic context of the document.

CONCLUSIONS
in summary, we extracted a set of major semantic concepts from a protein-related corpus of text words from medline titles and abstracts by applying the lda model. the identified concepts are semantically coherent, and most of them are biologically relevant. the extracted biological topics reflect the major knowledge domains of current knowledge of protein function contained in the corpus. the semantic content of a document can be inferred from a text and used for automatically indexing the text. future directions will be explored to extend the current approach or to develop new techniques for extracting biological concepts of finer granularity and combining semantic analyses with conventional nlp, ie, and ir techniques to map the topics to the controlled vocabulary.

