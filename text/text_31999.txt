BACKGROUND
the field of protein structure prediction began even before the first protein structures were actually solved  <cit> . secondary structure prediction began  <cit>  shortly after just a few protein coordinates were deposited into the protein data bank  <cit> . in the 1980's, as the very first membrane proteins were being solved, membrane helix  and signal peptide prediction methods began to proliferate  <cit> . homology modeling, as a way of predicting 3d structures, followed in the mid 1980's  <cit> . later, in the 1990's the concept of threading  emerged, thereby allowing reasonably accurate fold prediction to be performed on very distantly related sequences  <cit> . over time, the accuracy and reliability of most protein structure prediction methods has grown considerably. this is, in part, due to the development of more sophisticated prediction methods using neural nets or hidden markov models  <cit> , the development of more rigorous testing methods  <cit>  and the explosive growth in both sequence and structure data on which scientists can "train" their software .

protein structure prediction continues to be an actively developing field with more than  <dig> papers being published on the subject each year. incremental improvements in prediction accuracy are still being reported and until "the protein folding problem" is formally solved, it is likely that protein structure prediction will continue to be an active area of research and development  <cit> . the continuing improvements in structure prediction accuracy are also having an effect on how proteins are analyzed and annotated. while once an anathema to most protein chemists, secondary structure prediction is now becoming a routine part of many protein analyses and proteome annotation efforts  <cit> . annotation systems such as pedant  <cit> , basys  <cit> , bacmap  <cit> , psortb  <cit>  and others all depend on large scale secondary structure predictions to assist in identifying possible functions, to determine subcellular locations, to assess global trends in secondary structure content among different organisms or certain organelles, to identify protein folds or to enumerate fold classes , to identify domains, and to estimate the proportion of "unfolded" or unstructured proteins in a given genome  <cit> . likewise protein secondary structure predictions can play a valuable role for molecular biologists in deciding where and how to subclone protein fragments for expression , where to join or insert gene fragments, or in choosing where to add affinity tags for protein purification  <cit> . secondary structure predictions can also be used to calibrate cd and ftir measurements when monitoring the folding or unfolding proteins with no known 3d structure  <cit> . secondary structure predictions may also be used to assist in the assignment of nmr spectra , to re-reference chemical shifts and to help determine protein flexibility  <cit> .

currently the performance  of the best secondary structure prediction methods, such as psipred  <cit> , jnet  <cit>  and phd  <cit>  is between 75–77%. these methods, which are specific to water-soluble proteins, utilize blast or psi-blast searches of the non-redundant protein sequence database to obtain evolutionary information. this information is then fed through a multi-layered feed-forward neural network that has previously been trained on known structures and known alignments to learn characteristic sequence/structure patterns. those patterns are then used to predict the secondary structure of the query protein  <cit> . similarly good scores can also be achieved using hidden markov models with programs such as sam-t <dig>  <cit> . more recently approaches that combine multiple high quality methods  have been described  <cit>  and these appear to do even better than the single-pass prediction approaches.

what is somewhat surprising about the methods described so far is that they do not fully exploit the information that is available in the protein structure databases. so far as we are aware, none of the above-mentioned methods attempt to find sequence homologues in the pdb and to use the known secondary structure of those homologues to assign, map or predict the secondary structure of the query protein. as a rule, this sequence/structure alignment approach to secondary structure assignment is normally reserved for homology modeling programs  <cit> . for pairwise sequence identities of >35%, these secondary structure mappings are typically more than 90% accurate. however, we believe this 3d-to-2d mapping approach to general secondary structure prediction is not being fully exploited. a recent survey has found that less than 3% of new protein structures deposited into the pdb have a totally novel fold  <cit> . even among structural genomics projects, where novel folds are explicitly being sought and solved, less than 10% of the targets exhibit completely novel folds  <cit> . furthermore, we have found that nearly 3/ <dig> of newly deposited pdb structures have sequence identities greater than 25% to a pre-existing structure. in other words, the vast majority of newly solved proteins could have at least a portion of their secondary structures predicted via this simple 3d-to-2d mapping approach. thus, by combining a pdb-based structure alignment with a high quality de novo structure prediction program it may be possible to achieve a much higher overall q <dig> score for protein secondary structure prediction.

here we wish to describe a program, called proteus, that exploits this concept of 3d-to-2d mapping and integrates it with multiple de novo methods to accurately predict protein secondary structure. specifically, proteus achieves an average q <dig> score of 88% when tested on newly solved protein structures. this level of accuracy is 12–15% above that previously reported  <cit> . if a query protein has at least some portion of its sequence that is homologous to an existing pdb structure, the average q <dig> score exceeds 90%. if absolutely no homology is found, or if the 3d-to-2d mapping option is turned off, the average accuracy of this method is still above 79%. in addition to greatly improving the average performance of secondary structure prediction, we have parallelized the prediction algorithm, developed a simple installation protocol and made the full source code and all associated databases freely available and as portable as possible. this was done in an effort to facilitate proteome annotation and to encourage large scale pipelined analyses or proteome-wide structure predictions to be done locally rather than remotely.

implementation
proteus consists of three components: 1) a large , non-redundant and continuously updated database of sequences with known secondary structures; 2) a multiple sequence alignment algorithm for secondary structure mapping and homology prediction and 3) a "jury-of-experts" secondary structure prediction tool consisting of three different, high-performing de novo secondary structure prediction programs . the prediction algorithm itself involves four steps including an initial search against the pdb sequence database to determine if all or part of the query sequence is similar to a known structure. if a hit is found, a secondary structure mapping is performed on whatever component that mapped to the query. in the second step, a de novo secondary structure prediction using our three different , high quality neural network  approaches is performed. in the third step these three nn predictions are then fed as inputs into a fourth neural network, which then combines these predictions to make a prediction of its own . finally, the jury-of-experts prediction and the results of the initial homology search are combined to produce the final secondary prediction for proteus . combining the two prediction methods allows proteus to fill in any prediction gaps derived from the initial 3d-to-2d mapping process and always yields a full sequence prediction, regardless of the extent of sequence overlap to a pdb hit.

key to the success of proteus is its effective use of secondary structure databases. proteus' secondary structure database  is assembled from a non-redundant version of the protein data bank  in which all sequences with >95% sequence identity to any other sequence were removed using the cd-hit utility  <cit> . each sequence was then assigned a secondary structure using vadar  <cit> . the secondary structures were then checked and filtered so as not to contain "impossible" structures, such as sheets or helices containing a single residue. vadar uses a consensus method of identifying secondary structures that closely matches "simplified" dssp  <cit>  structure assignments , stride  <cit>  and generally agrees well with manual secondary structure assignments made by x-ray crystallographers and nmr spectroscopists. in fact, using the proteus-2d database of secondary structures, the performance of psipred and jnet was actually found to improve slightly over the performance quoted for dssp-assigned secondary structures . the secondary structure content of the proteus-2d database, which currently contains over  <dig>  million residues from more than  <dig>  sequences, is 33% helix, 29% beta sheet and 38% coil. because of its critical importance to the prediction process, the entire proteus-2d database is automatically updated on a weekly basis. this database is also freely available for download at the proteus website.

the pdb homology search and 3d-to-2d mapping process in proteus both employ blast  to score and align high scoring hits found in the proteus-2d database. those database sequences having an expect score greater than 10- <dig> to the query sequence are retained for further analysis. this optimal expect value was determined by extensive testing with cut-offs ranging from 10- <dig> to 10- <dig>  depending on the length and domain structure of the query sequence up to 20+ homologues may be identified by this process. the pairwise blast alignments are then used to assemble a multiple sequence alignment over the length of the query sequence. the resulting multi-sequence alignment is then used to directly map the secondary structure of the proteus-2d database sequences  to the query sequence. the mapping process involves sliding a  <dig> residue window over each aligned sequence and assigning a similarity score  to the central residue. the sequence with the highest "identity score" for any given residue is then privileged to assign its secondary structure to the aligned residue in the query sequence. in this way the secondary structure of the query sequence is essentially predicted by homology. for those query sequences that are predicted in this manner , proteus also produces an image of the approximate 3d fold using the pdb coordinates to generate the picture.

in situations where no homologue is found, or only a portion of the query sequence could be predicted by 3d-to-2d mapping , proteus resorts to a jury-of-experts prediction to cover the unpredicted portion. this jury-of-experts approach uses three neural net predictors: psipred  <cit> , jnet  <cit>  and our own transsec  methods. the results from these predictors are then fed into a fourth neural network to produce a consensus prediction in a manner similar to that described previously  <cit> .

the methods and underlying theory to psipred and jnet have been published previously and the programs were used as received without further modification. the transsec program was developed in-house using a java-based neural network package known as joone  <cit> . transsec's underlying approach is relatively simple, consisting of a standard psi-blast search integrated into a two-tiered neural network architecture. the first neural network operates only on the sequence, while the second operates on a  <dig> × n position-specific scoring matrix consisting of the secondary structure determined via the first network. the first neural net uses a window size of  <dig>  and was trained on  <dig> sequences from the proteus-2d database . this neural net had a 399-160-20- <dig> architecture  and typically predicts the secondary structure of any given protein with a q <dig> = 64–65%. transsec's neural net secondary structure predictions are performed on all psi-blast homologues to the query sequence these homologues are then multiply aligned using xalign  <cit>  with the secondary structure serving as a guide to place gaps and insertions. the resulting secondary structure-based alignment  is then used as input for a second neural network. transsec differs from most other prediction programs  in that the predicted secondary structure, instead of the sequence, is used as input for the second neural network. what transsec attempts to do is to learn, via a neural net, how to "average" aligned secondary structures in a more intelligent way. a simple averaging of secondary structures typically reduces the prediction accuracy from 65%  to 63% , while using a neural net increases the performance by about 7% over naive averaging. the second neural net in transsec was trained on  <dig> sequences from the proteus-2d database, and achieved a q <dig> score of 70% and a sov score of 72%. it used a window size of  <dig>  and was based on a 36-44- <dig> architecture.

the jury-of-experts program,  which combined the results of the three stand-alone secondary structure predictions was also developed using joone. joe consisted of a standard feed-forward network containing a single hidden layer. using a window size of  <dig>  the structure annotations and confidence scores from each of the three methods  were used as input. the jsp neural net was trained and tested  on  <dig> sequences chosen randomly from the non-redundant database mentioned above. four output nodes were used, one for each of helix, strand or coil, as well as a fourth denoting the beginning and end of the sequence. a back-propagation training procedure was applied to optimize the network weights. a momentum term of  <dig>  and a learning rate of  <dig>  were used, and a second test set of  <dig> proteins was applied at the end of each epoch, to ensure that the network was trained for the most optimal number of iterations. the joe program outputs not only the secondary state call , but also a numeric confidence score . relative to simple averaging, the joe program is able to improve secondary structure predictions by an average of 3% . the improvement achieved using this jury of experts approach is likely due to the fact that jnet, psipred and transsec perform differently for different types of proteins, with one method typically outperforming the other two depending on the secondary structure content, protein length or amino acid content. it appears that joe's neural net was able to learn which method or which segmental prediction to trust more and therefore to place more weight on those predictions. it also appears that the joe method also learned to modify the jnet and psipred predictions  to conform better to the vadar-assigned secondary structures.

the final step in the proteus algorithm involves merging the homology prediction  with the jury-of-experts predictions. the proteus-merge program was designed to accommodate three situations: 1) the case where no pdb homologue could be found, 2) the case where complete 3d-to-2d mapping was achieved and 3) the case where the 3d-to-2d mapping provided only partial coverage of the full query sequence. in the simple situation where no 2d-to-3d prediction is available , the merge process simply takes the jury-of-experts or de novo result. similarly, if a complete pdb-based secondary structure prediction is available , the jury-of-experts prediction is generally ignored. in particular, if the homologue confidence score is equal to or greater than the consensus de novo score, then the homologue structure assignment is retained. otherwise the de novo structure assignment is kept. typically the de novo confidence scores range from 3– <dig>  while the homologue confidence scores range from 8– <dig>  the confidence of a homologue prediction is based on the running average  of the sequence identity between the query sequence and that of the top matching pdb homolog. if the sequence identity is less than 30% , the confidence score assigned to the middle residue in the window is  <dig>  if it is greater, the confidence score of the middle residue is  <dig>  confidence scores for the consensus de novo predictions are determined by the weightings of specific neural network nodes. if a homologous sequence or a group of homologous sequences is found  that did not cover the entire length of the query sequence , the unpredicted or unmapped portion is assigned the secondary structure determined by our jury-of-experts approach .

proteus also has a number of i/o utilities and interface tools that allow it to accept protein sequences  and to produce colorful and informative output including all sequence alignments, corresponding blast scores, sequence matches, confidence scores, colored secondary structure annotation as well as 3d images of any modeled structures. additional data handling and task handling tools were also written to manage the server side of the program, to update the proteus-2d database on a weekly basis, and to parcel out tasks to other processors in a parallel fashion. the programs used to create proteus and the proteus web server were written in both c and java  <dig> . specifically, xalign, vadar, jnet, blast and psipred were written in ansi standard c, while transsec, the jury-selector, most of the input/output handling routines, as well as the web server interface were written in java. the proteus-2d update script was written in perl.

RESULTS
proteus' performance was tested in four different ways, 1) through leave-one-out testing on a set of  <dig> training proteins from the proteus-2d database; 2) through a "blind" test and comparison on the latest eva training set ; 3) through analysis of  <dig> randomly chosen proteins that were recently solved by x-ray and nmr; and 4) through direct comparisons of  <dig> randomly chosen proteins to well-known secondary structure web servers. the intent of these different tests was to gain some understanding of the performance of proteus under different prediction situations and to assess its performance relative to other well known predictors. for the first test, the performance of the jury-of-experts system was assessed using a leave-one-out strategy on  <dig> randomly chosen proteins form the proteus-2d databases. as previously mentioned, this method achieved a q <dig> score of  <dig> % and a sov score of  <dig> %. when this method was combined with the 3d-to-2d mapping , the performance was q <dig> =  <dig> % and sov =  <dig> %. the performance for the "full" version of proteus  is about 10–15% higher than previously reported for other methods. because this first test was done on training data  it might be argued that the high performance may be due to overtraining or to the small sample size.

to more legitimately assess the performance of proteus a second "blind" test was done on data not part of proteus' training set and for which no pdb homologues would be expected. specifically the most recent release  of the eva  <cit>  sequence-unique subset of the pdb was downloaded and used to measure the performance of proteus. the eva collection represent a set of non-homologous proteins that do not match any 100+ residue segment of any other protein in the pdb with greater than 33% sequence identity. the eva test-set has been used for a number of years to benchmark protein secondary structure predictors, particular for casp competitions  <cit> . the use of a sequence-unique data set such as eva is intended to simulate the situation where one might be predicting secondary structures in a structural genomics project, where novel fold identification is key. in this particular situation one would expect that the proteus predictions would be dominated by its de novo methods and that the q <dig> and sov scores would be somewhat reduced over the first test. a total of  <dig> protein sequences and pdb id's were obtained from the eva website and the secondary structure for each of the test-set proteins was assigned by vadar  <cit> . proteus was then used to predict the secondary structures and the performance was evaluated against the vadar-assigned secondary structures. the program was tested in two modes, one with the pdb homologue search turned off  and other with the pdb search turned on. in both cases the q <dig> and sov scores were calculated for each protein in the  <dig> protein test set. note that the sov score is similar to q <dig> but more sensitive to the segment grouping or overlap of secondary structure elements  <cit> . at the same time the q <dig> and sov scores for jnet  and psipred  were also determined for all  <dig> eva proteins. additionally the secondary structure predictions posted on the eva server for porter  <cit> , prof-king  <cit> , profsec  <cit> , sam-t99-sec  <cit>  and vaspin  <cit>  were also downloaded and processed in a similar manner to the psipred and jnet predictions. note that the number of predictions for these predictors was much less than  <dig> as the eva server often only performs a small number  of predictions for any given predictor. as seen in figure  <dig>  proteus achieves a q <dig> of  <dig> %  when its homologue search is turned off and a q <dig> score of  <dig> %  when the homologue search is turned on , with a standard deviation of  <dig> % and  <dig> %, respectively. evidently, even in a sequence-unique data set, some fragmentary homology is still detectable by proteus. in particular for those proteins that exhibited some detectable homology to a portion of a pdb structure, the performance was actually quite good . comparisons to other predictors on the same set or proteins  or a subset of these proteins  indicate that these methods perform at levels from  <dig> %– <dig> %  or  <dig> %– <dig> % . the q <dig> and sov scores we obtained for these predictors on our eva test set are very close  to those reported by the authors or posted on the eva website. while the performance of proteus is not quite as impressive as seen in the first test, it still demonstrates that under strict "casp" testing conditions, proteus performs approximately 4–8% better than other high-performing secondary structure predictors.

the third test of proteus' performance was intended to simulate the situation where one is trying the predict the secondary structure of proteins that are being studied by x-ray and nmr, but not yet solved, not yet published or not yet released by the pdb. this kind of test is intended to answer the question: what is the secondary structure prediction performance of proteus for proteins that are of interest to genome annotators, structural biologists or protein chemists? a testing set of  <dig> randomly chosen, non-redundant, water soluble proteins was generated by downloading the pdb coordinates of a subset of proteins deposited from january  <dig> to june  <dig>  because the training set of proteins originally used to refine and optimize proteus consisted of proteins deposited into the pdb prior to december  <dig>  this precluded any possibility of testing on the training set. as with the previous tests, the secondary structure for each of the test-set proteins was assigned by vadar  <cit> . proteus was then used to predict the secondary structures  and the performance was evaluated against the vadar-assigned secondary structures. figure  <dig> summarizes the distribution of q <dig> scores for proteus as tested over the entire  <dig> protein test suite, with the homologue search turned off . the average score in this case was  <dig> %  and  <dig> %  with a standard deviation of  <dig> % and  <dig> %, respectively. figure  <dig> displays the distribution of proteus' q <dig> scores with the homologue search turned as applied to the  <dig> proteins in the test set for which a pdb homologue  was found. in other words,  <dig> % of the test proteins could have their secondary structure predicted via 3d-to-2d mapping. the average score for the  <dig> homologues was  <dig> %  and  <dig> %  with a standard deviation of  <dig> % and  <dig> % for q <dig> and sov scores respectively. therefore proteus' combined, consensus prediction  for all  <dig> test proteins yielded an average accuracy of  <dig> % and  <dig> % for q <dig> and sov scores respectively, with a standard deviation of  <dig> %  and  <dig> % . the low scoring outlier proteins  are typically very short peptides or proteins which have absolutely no homologue in the pdb. for further comparison the same test set  and testing procedures were used to evaluate the performance of several other high-performing secondary structure prediction methods including psipred  <cit> , jnet  <cit> , sam_t <dig>  <cit> , as well as a locally written version of gor  <cit>  and our own transec. to ensure complete consistency, the blast database searches, which were required for all programs , were performed on the same local copy of the non-redundant  ncbi protein database. figure  <dig> presents the results of these prediction programs in comparison to the predictions obtained with proteus. a quick visual comparison reveals that proteus' performance is significantly better  than all five tested programs. for instance, psipred, which is generally regarded as being one of the most accurate methods  <cit> , obtained scores of  <dig> %  and  <dig> %  respectively. in comparison, proteus' consensus method obtained scores of  <dig> %  and  <dig> % . therefore, in this test, proteus' scores were approximately 10% higher than those achieved by psipred. even when proteus is partially disabled  it still performs about 2% better than the best-performing routine . the statistical significance of this 2% improvement was verified using a standard paired two-sample t-test, which confirmed that the two means were statistically different .

to verify that the performance differences noted in figure  <dig> were not the result of improper program installation, limited tool selection or outdated software, we conducted a fourth test on a set of  <dig> recently  solved proteins using a number of popular secondary structure prediction web servers. note that these  <dig> proteins were not contained in the proteus-2d database. the proteins ranged in size from 76– <dig> residues. the results are summarized in table  <dig>  once again, the results largely confirm what was seen in figure  <dig>  with proteus averaging close to 90% in both q <dig> and sov and the others ranging between 55% and 75%. the performance of these servers in this test set is also consistent with what has been described in the literature  <cit> . overall, these four independent tests confirm that proteus is able to predict secondary structure of soluble proteins with very high accuracy. when restricted to the prediction of sequence-unique proteins  proteus has a q <dig> of  <dig> %, which as about 4–8% better than the best performing methods. when allowed to predict the structure of any generic protein  proteus has a q <dig> of 88%–90% which is about 12–15% better than the best performing methods described to date.

discussion
proteus was primarily developed to facilitate secondary structure prediction for genome annotation. in genome annotation one is primarily interested in getting the most correct annotations or the most accurate predictions in the quickest possible way. making use of prior information or fragmentary data to fill in knowledge gaps is perfectly reasonable and strongly encouraged  <cit> . likewise making this process as automated and fool-proof as possible is a basic requirement of genome annotation systems. if one is interested in getting the most complete and accurate secondary structure assignment of as many proteins as possible, then it is quite natural to want to combine an ab initio or de novo prediction method with a method that extracts known or partially known secondary structure assignments  and to have this done automatically.

perhaps the best way to appreciate the general utility of proteus is to imagine a scenario where one is given the sequence of a large  <dig> residue protein  and then asked to generate the most accurate or most correct secondary structure assignment for this protein. suppose a blast search or cdd search reveals that this protein has  <dig> different domains,  <dig> of which have pdb homologues  and  <dig> other domains which have no known structure. to generate the most accurate possible secondary structure assignment for this multidomain protein would require many manual steps and a good deal of bioinformatics skill including: 1) a blast search against the pdb; 2) manual selection of the highest scoring homologues; 3) homology modeling using swiss-model  <cit>  or another modeling server for the two homologous domains with >35% sequence identity; 4) assignment of the secondary structure for two of the domains using dssp, stride or vadar; 5) sequence-based threading on the 3d-pssm server  <cit>  to generate possible folds of the remaining two low-scoring homologues; 6) manual assessment and adjustment of the predicted folds and their alignments; 7) prediction of the secondary structure of the remaining  <dig> domains using a de novo predictor such as psipred or phd and 8) manually typing, cutting or pasting all the secondary structure assignments on to every residue in the  <dig> residue sequence. a skilled bioinformatician might be able to do this in a couple of hours, an unskilled individual might take several days. alternately, one may elect the easy route and simply predict the structure of the entire protein using a de novo structure predictor such as psipred or phd. however, choosing to do this would likely reduce the accuracy of the prediction by 10–15% .

now suppose that one was asked to do this kind of high-end structure prediction not for just one protein but for  <dig>  proteins  or that it has to be done on  <dig> proteins every  <dig> weeks . clearly such a manual intensive process would have to be replaced by an automated technique. this is the primary motivation behind proteus. proteus effectively replaces  <dig> manually tedious steps with a single automated process. in fact, this  <dig> step example of vav <dig> is not entirely hypothetical. the single step proteus result  for vav <dig> is shown in the sample output on the proteus homepage. inspection of the output clearly demonstrates how proteus can combine prior knowledge  with de novo predictions to generate optimally accurate secondary structure assignments for large and complex proteins.

proteus is able to achieve its very high level of accuracy because it brings together two high performing methods of secondary structure prediction – a novel de novo method based on a jury-of-experts approach and a novel 3d-to-2d homology mapping method. the 3d-to-2d mapping process is not completely unknown. in fact, it is frequently used as an intermediate step in several homology modeling programs to identify conserved structural scaffolds  <cit> . given the well known fact that secondary structure is more conserved than primary structure, it stands to reason that mapping the secondary structure onto a given query sequence – even for remotely related homologues – will yield a high quality secondary structure "prediction". this is borne out by the fact that our mapping method is able to predict secondary structure with greater than 90% accuracy. this mapping approach is obviously limited to query proteins that have a homologue or potential homologue already deposited in the pdb database. as might be expected, the accuracy of the mapping prediction is generally tied to the level of sequence identity or blast expect value. highly similar sequences  can have their secondary structure predicted with close to 90% accuracy. intermediate similarity  yields predictions that are 80–90% correct while low sequence identity  yields secondary structure predictions that are 75–80% correct. this partly explains the distribution of scores seen in figure  <dig> 

certainly, when the pdb was relatively small , this 3d-to-2d mapping method would prove to be relatively ineffective. however, with the rapid expansion of the pdb over the past  <dig> years we are now able to take advantage of the fact that an increasingly large fraction of protein structures that are being solved or for which people want to know the structure, have at least one homologue in the protein data bank. indeed, less than 3% of all newly deposited structures have novel folds  and it appears that less than 10% of structural genomics targets are yielding truly novel folds  <cit> . therefore, the odds that any given protein will have a novel arrangement or a unique order of secondary structures  is becoming relatively small. even with the modest approach employed here , we still find that 70% of "testable" proteins have at least one homologue or a portion of a homologue in the pdb. therefore, on average, the 3d-to-2d mapping process is going to be effective for about 70% of all query proteins which are solvable by today's x-ray and nmr methods. we would predict that this fraction  would continue to increase as the pdb continues to expand and the number of known folds grows.

note that this figure of 70% is not applicable if were to try to predict secondary structure for entire genomes. large scale homology modeling efforts suggest that only about 30–50% of a given genome is amenable to homology modeling or threading  <cit> . therefore if we applied the lower figure of 30%  to our protocol we would predict the performance of proteus in predicting the secondary structure of soluble proteins would drop to 83%. note that this figure is still 7–10% better than existing secondary structure prediction methods. obviously if one biased their selection of query proteins such that no portion of the sequence had any sequence homology whatsoever to something in the pdb, then proteus could do no better than its de novo approach , even with its pdb search turned on. similarly, we would predict that genomes from poorly sampled branches of the tree-of-life would probably be less well predicted than those belonging to the better studied branches .

given the potential variability in proteus' predictions, we believed it was important to provide a reliability or confidence score in proteus' prediction output. these reliability scores are determined on the basis of the neural network outputs  or the level of sequence identity to a given pdb match . reliability scores are generated not only for each residue for each prediction, but also for each residue in the consensus  prediction and for the entire protein. the maximum reliability score is  <dig>  and the maximum reliability score for a complete protein is 90%.

while proteus' 3d-to-2d mapping procedure offers a number of advantages in secondary structure prediction, it is also important to remember that another key strength in proteus lies in its de novo structure prediction routine. this jury-of-experts approach, which uses machine learning methods to combine three independent and high performing structure prediction algorithms into one, is able to consistently predict secondary structures with an accuracy approaching 79%. this is still 2% higher than any other single pass method with which we could directly compare. this consensus method uses psipred, which generates blast sequence profiles to extract evolutionary and sequence information using a neural network; jnet, which uses a combination of solubility information, evolutionary information, and a hidden-markov model/neural network combination; and transsec, a locally developed algorithm which uses a two-tiered prediction system to extract evolutionary similarities. these three methods are sufficiently "orthogonal" in their prediction methodology that the combination of the three is able to generate a consensus prediction that is 2–5% higher than any individual prediction. the ability to generate de novo secondary structure predictions which are consistently near 80% correct, especially in regions where the 3d-to-2d mapping approach fails, certainly helps to create consensus predictions that are consistently close to 88% correct.

while proteus clearly performs very well, there are still a number of improvements or additions that could be made to the program. one obvious improvement could be the integration of conventional membrane spanning prediction routines and signal recognition programs  <cit>  to make proteus capable of handling all protein types . this would be particularly useful in whole genome annotation applications. another improvement could be made in proteus' sensitivity in its 3d-to-2d mapping steps. by simply employing psi-blast  <cit>  instead of blast it should be possible to increase the fraction of pdb homologues  that could pass through the 3d-to-2d mapping steps. however, given the drop in predictive performance seen for homologues with <30% identity, it is not clear whether this would lead to a very substantial improvement in overall accuracy. yet another potential addition to proteus would be a 2d threading or fold prediction service. given the high accuracy of its secondary structure predictions, one might expect that proteus could yield somewhat more reliable results and somewhat improved fold classifications.

along with its high accuracy and its ready availability as a web server, we have also ensured that the downloadable version of proteus would be a well-documented, user-friendly system which is easy to install and does not require additional input or obscure pre-processing steps. during our testing processes we found that many other systems offered relatively limited documentation, required the user to provide additional inputs, such as an alignment and blast output files, or demanded that additional scripts or programs to be run to compile the input into a suitable format. often users will not know how to supply these extra inputs . given these difficulties, we have tried to make the installation and operation of proteus as simple as possible. the local version of proteus  requires nothing more than a sequence in either fasta or raw format. the output can be customized, and due to its open source nature, modular design and extensively commented java code, the algorithms can be incorporated easily into other applications for batch or online processing. proteus was also designed to take full advantage of multi-processor systems and should scale well as computational resources increase. this is a particularly important consideration in genome/proteome annotation efforts.

proteus' software does have a few drawbacks. because it is written in java, it requires substantial memory to run. furthermore, the neural networks used in the program were not optimized for minimal memory use; therefore proteus requires at least  <dig> mb of ram to be allocated to the java virtual machine. with increasing hardware availability and lower prices, this requirement should not be too much of a concern in the future. additionally, because of the requirement to run three independent de novo prediction methods, a 3d-to-2d mapping step and a consensus prediction generator, proteus is somewhat slower than other methods. while psipred can typically return a result within seconds of completing a lengthy psi-blast search, proteus requires almost a minute to complete its predictions . efforts are being made to reduce this time requirement with further code optimization and multi-processor utilization.

CONCLUSIONS
proteus is both an integrated web server and a stand-alone application that exploits recent advancements in data mining and machine learning to perform very accurate protein secondary structure predictions. proteus combines three high-performing de novo structure prediction methods , a jury-of-experts consensus tool and a robust pdb-based structure alignment process to generate all of its secondary structure predictions. for water-soluble protein proteus is able to achieve a very high level of accuracy  which is approximately 12–15% higher than that previously reported  <cit> . the program's performance was extensively tested and compared to both available programs and publicly accessible web servers using a variety of test proteins and test scenarios. in all cases proteus appears to perform better than existing tools. this performance improvement is statistically significant and robust. in the rare situations  where a query protein shows no similarity whatsoever to any known structure, or if the 3d-to-2d mapping option is turned off, proteus is still able to achieve a q <dig> score of ~79%. this is still statistically better than what has been reported elsewhere. however, it is still important to be somewhat circumspect in interpreting these results. the standard deviation for essentially all secondary structure prediction routines  still stands at ~10% and so some caution must be exercised in interpreting or relying upon these predictions. indeed, it is theoretically possible to get a proteus prediction that is only 50% correct. until a method is developed where the standard deviation in prediction accuracy is <5% or until the pdb expands to encompass all "fold space", there is still a strong need to develop better routines and more complete databases. to facilitate further algorithmic improvements, widespread adoption, and easy incorporation into genome annotation pipelines, proteus was designed to be completely open source. given its high accuracy and open-source nature, we believe proteus could make a very useful addition to the current arsenal of structure prediction tools available to protein chemists, genome annotators and bioinformaticians.

availability and requirements
the proteus website is accessible at . the entire proteus suite occupies approximately  <dig>  gbytes of data with the proteus-2d database occupying  <dig>  mbytes and the nr protein sequence database occupying  <dig>  gbytes. all programs were tested and compiled on a variety of unix platforms and should work on most systems operating linux and mac osx . all programs and databases are downloadable at  and are supported with an easy-to-use installation script. a typical proteus run for a  <dig> residue sequence takes approximately  <dig> minutes on a  <dig>  ghz machine equipped with  <dig> gb of ram.

authors' contributions
sm wrote, tested and installed most of the predictive software described here, designed and conducted all performance tests and prepared the first draft of the manuscript, ss wrote and tested the software used to generate the proteus-2d database, wjg provided direction, ideas and critical suggestions in the early phases of the project, dsw wrote the final manuscript, conceived of the central ideas in the paper and coordinated most of the project.

