BACKGROUND
many microarray studies involve human samples for which survival data are available. in the last two years there has been an increase in the number of new methods proposed for this kind of data  <cit> . many of these papers have emphasized not only gene selection and survival prediction, but also "signature finding": discovering sets of correlated genes that are relevant for survival prediction. for end-users , however, most of these methods are not easily accessible, which might explain why many papers in the primary biomedical literature implement from scratch varied ad-hoc approaches in the context of survival prediction.

unfortunately, in many cases, survival data are reduced to arbitrarily determined classes , with the consequent loss of information, simply because tools for class prediction are much more widely available. thus, tools for end users are badly needed that, while retaining user-friendliness, do not compromise statistical rigor.

statistically, and in addition to appropriate analysis of censored data, such a tool should ensure that selection biases  <cit>  are accounted for, to prevent overoptimistic assessments of the quality of the final model selected. moreover, such a tool should also present the user with assessments of the stability of the results obtained: variable selection with microarray data  can lead to many solutions that have similar prediction errors, but that share few common genes  <cit> . choosing one set of genes without awareness of the multiple solutions can create a false perception that the selected set is distinct from the rest of the genes. besides the statistical features, interpretation of results is enhanced if the tool provides additional information about "the interesting genes" such as pubmed references, gene ontology terms, and links to the ucsc and ensembl databases and kegg and reactome pathways.

such a tool should also try to incorporate the increasing availability of multicore processors and clusters made with off-the-shelf components. since cpu performance has improved less than 20% per year since  <dig>  <cit> , the major opportunities for significant speed gains and the ability to analyze ever larger data sets with more complex analysis methods do not lie in faster cpus. rather, it is widely acknowledged that scaling to larger data sets and reducing user waiting time depends crucially on our ability to efficiently use parallel, distributed, and concurrent programming because of the increase in the available number of cpus and cpu cores  <cit> . this trend affects even the laptop market  and, therefore, the gains from parallel computing can be realized not only on computing clusters, but also in workstations and laptops.

parallelization, such as provided by mpi  <cit> , allows us to distribute the computations over a computing cluster, thus decreasing execution time. for an end user, parallelization can result in dramatic decreases in the time she/he needs to wait for the analysis to complete . for developers, bioinformaticians, and biostatisticians, parallelization results in speed increases that ease method comparisons using extensive simulations and provides an example for the parallelization of further algorithms.

regarding the user interface, web-based applications have been gaining popularity in bioinformatics among other reasons because they allow the development of user-friendly applications that do not require software installation or upgrades from the user  <cit> . in addition, web-based applications, if run in a computing cluster and implemented appropriately, make it possible to exploit parallelization.

finally, source code availability under an open-source license allows researchers to improve upon the method, fix bugs, and verify claims by method developers, encourages reproducible research, and ensures that tool ownership resides in the international research community. these are all issues of particular concern in bioinformatics, where expedite progress builds upon previous research  <cit> . moreover, the value of code availability is further enhanced when standard best practices in software development  and the usual open source development mode  <cit> , are followed.

we have developed a web-based tool, signs , to fulfill the above needs. we know of no equivalent tool, and only brb-arraytools  <cit> , by r. simon and a. p. lam, provide somewhat similar functionality, but it is not web-based, does not ease accessing additional information, does not use parallel computing, and source code is not available. thus, signs is a unique tool, of immediate utility for biomedical researchers studying gene expression and its relation to survival , and of broad appeal also to computational biologists, biostatisticians, and bioinformaticians because of the methods it implements and the combination of parallelization with web-based computing in an open-source application.

implementation
signs is as a web-based application  that provides four methods for gene selection with survival data: the method by gui and li  <cit> , the approach of dave et al.  <cit> , a method that uses random forests with conditional inference trees  <cit> , and a boosting method with component-wise univariate cox models  <cit> . there are few methods that explicitly attempt to perform gene selection with survival data while preserving the identity of the individual genes and allowing the recovery of highly correlated genes. moreover, there are few comparisons among the available methods, except those from  <cit> . in this context, we chose to implement these four very different approaches. the available comparisons indicate that penalization methods, specially those based on the l <dig> penalty, such as lasso and lars, tend to perform well and return results with relatively few genes, thus enhancing interpretation  <cit> . the method of gui and li can approximate the lasso or lars estimates, while selecting more relevant genes  <cit> . on the other hand, relatively heuristic and simple approaches such as those based on clustering and the idea of signatures can sometimes perform remarkably well, compared to sophisticated penalization approaches  <cit> . the method of dave et al.  <cit>  is one such method that attempts to explicitly return signatures for survival data. finally, ensemble approaches are currently gaining popularity. the recent review by  <cit>  has found that random forest-based methods, as in  <cit> , can yield the best survival time predictions and, thus, we have also included this method. an alternative approach to using ensembles is via boosting, as in  <cit> ; this approach has the advantage of providing for explicit variable selection and being computationally efficient, and has been shown to be competitive for at least some microarray data sets  <cit> . we have parallelized all the algorithms, providing significant decreases in user wall time .

algorithms
briefly, the steps of the method by dave et al.  <cit>  are: a) genes are filtered by p-value using a gene-by-gene cox model. b) the retained genes are divided into two groups, those with a positive coefficient in the cox model and those with a negative coefficient, and a hierarchical cluster is built for each of these two groups separately. c) a potential signature is a group of genes  such that the minimal correlation between any two genes in the signature is above a  threshold, and such that this cluster has between a minimum and a maximum number of genes . d) the numeric value of a component, signature in the sense of  <cit> , is the average expression level of all the genes in a given component . e) finally, we carry out variable selection using as starting point the best two-signature model; the variables used are the signature values computed in step d) above. variable selection is carried out on the signatures, not on individual genes, and no gene can belong to multiple signatures. the main advantages of the approach of  <cit>  are that it is easy to understand, that it explicitly attempts to return sets of correlated genes , and that the user is both forced to be explicit about, and allowed to choose, parameters with relatively straightforward interpretation . thus, the method of  <cit>  is ideal for exploratory analysis, which is further enhanced by our additions, in particular the assessment of stability of solutions and functional annotation via idconverter and pals .

in contrast, the approach of gui and li  <cit>  has two parameters but they rarely need to be tuned, and can instead be chosen by cross-validation. the complete method, including the dimensional reduction and the ability to capture correlated genes, follows from the penalization used . the solutions of this method depend on two parameters: a threshold parameter τ and an infinitesimal increment Δυ. the threshold parameter τ, constrained between  <dig> and  <dig>  is related to the amount of penalization, and larger values lead to a larger numbers of coefficients in the cox model being set to  <dig> . the infinitesimal increment Δυ affects the rate of change of the coefficients at each iteration of the algorithm. note that, operationally, we can instead choose a sufficiently small Δυ, and modify the number of iterations.  <cit>  use cross-validation to choose the optimal parameters: a set of τ is decided in advance, and cross-validation is used to select the Δυ , that minimizes the cross-validated partial likelihood . the cvpl is a function of the difference in the log cox's partial likelihoods for the models with and without the ith group of subjects. once the optimal parameters are decided, it is immediate to obtain the fitted coefficients for the genes and the scores for subjects not in the sample. thus, if we choose a sufficiently small Δυ, no parameters need to be chosen by the user, as these are selected via cross-validation.

the approach of hothorn et al.  <cit>  for using random forests utilizes conditional inference trees as base learners. a conditional inference tree is built by recursively splitting the data into two groups according to the value of the covariables. first, we test the global null hypothesis of independence between any of the variables and the response. if this hypothesis is rejected, we select the variable with strongest association with the response, and implement a binary split in the selected input variable. we continue recursively  until the global null is no longer rejected. the tests are carried out in a conditional inference framework . a forest is built by fitting a conditional inference tree to each of the many  bootstrap samples of the data. the type of response returned by these forests are kaplan-meir curves. in contrast to the previous two methods, there is no variable  selection performed here although, implicitly, the construction of each tree involves choosing the best variable for a split. following  <cit> , by default we fit the forests to the best  <dig> genes, as determined from gene-wise cox models, but the actual number of genes used is a parameter that can be chosen by the user of the application.

the last method included in signs has been developed by hothorn and colleagues  <cit>  and uses boosting to analyze survival data when the number of variables is much larger than the number of samples. boosting is a general iterative optimization method that, at each iteration, fits the best predictor variable  and updates the estimates. the algorithm used, l <dig> boosting, is equivalent to refitting residuals multiple times  <cit> . for survival data,  <cit>  use component-wise univariate cox models. boosting requires choosing the number of boosting iterations, but as shown in  <cit> , we can use cross-validation to select this parameter. this method performs variable selection, similar to the lasso and other l1-penalty methods  <cit> . the genes selected are those with non-zero coefficients at the optimal number of boosting iterations.

implementation and parallelization
the method of  <cit>  is available in both the web-based application and the underlying r code. the method of  <cit> , as originally proposed, is only available in the r code because it is too slow for routine use in the web-based application; following a suggestion by j. gui, the web application provides an implementation where only one threshold is used . the core statistical functionality of both methods is written in r  <cit> . where possible, computations have been parallelized using mpi  via the r-packages rmpi  <cit> , by h. yu, and papply  <cit>  by d. currie. for the web-based application, the cgi , initial data validation, the setting-up and closing of the parallel infrastructure , and the fault-tolerance and crash recovery mechanisms, are written in python.

the implementation of the approach in  <cit>  follows closely the description of their method in the supplementary material to their paper. our main departures from their implementation are: a) we do not split the data into two halves, but instead use cross-validation to assess model selection; b) it is unclear how the initial two-signature model was selected by the authors, and we choose the one with the largest likelihood, which in this case would be identical to using aic, akaike information criterion, as a criterion, since all two-signature models have the same number of parameters; c) it seems, from the supplementary material, that the authors used p-values in their forward variable selection, whereas we use aic, generally a preferred criterion for model selection  <cit>  .

for this algorithm, we have parallelized computations over cross-validation runs, after experimenting with alternative parallelization schemes. parallelizing the initial computation of gene-by-gene cox models leads to decreases in speed because, in our setup, the communication costs are larger than the decrease in computing time; for instance, with a data set of  <dig> genes and  <dig> arrays, the non-parallelized algorithm takes about  <dig> seconds in contrast to the  <dig> seconds of a parallelized algorithm that distributes the computational load evenly over  <dig> cpus. parallelization of the next step, clustering genes with positive and negative coefficients independently, could be split into two within each run, but the final step  is inherently sequential. thus, we can minimize communication costs and simplify further modifications of the algorithm if the algorithm is parallelized at the cross-validation level. with this scheme, the total user wall time is the sum of the computing time of two runs of the algorithm –a first run with all the data, and another run corresponding to one of the cross-validations– plus the time invested in communication and data handling. in contrast, the user wall time of the purely sequential algorithm would be the sum of  <dig> runs of the algorithm –one for all data, and one for each of the cross-validation runs. for the method of  <cit>  our code is based on the original code by the authors, with many modifications for speed improvement and parallelization. first, several common operations in the code were implemented using faster  code . next, the code was parallelized. taking into account the number of nodes we had available and the number of nodes that can often be used in off-the-shelf computing clusters, we parallelized the computations that search for the best τ. following  <cit> , we explore the six possible τ values  <dig>   <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig>  and select the one that minimizes the cvpl, using 10-fold crossvalidation. thus, we can parallelize the finding of the best t into  <dig> independent computations . notice that parallelizing at this level yields increased speed even if no global/double/full cross-validation is performed. the speed-ups achieved with the code changes are discussed below. because of speed issues with this method, the web-based application does not explore a range of τ values, but instead uses a single one, chosen by the user. j. gui made us notice that, since their approach can only include genes, not delete them, it can result on small τ thresholds leading to the selection of many genes which can be false positives, and he suggested using only one or a few large t thresholds, and skipping cross-validation over the entire τ range, if time is at a premium . by default, therefore, the web-based application uses a t threshold of  <dig> .

adding other algorithms: random forests and boosting
the above two methods have been implemented either almost from scratch, or after extensive modifications of the original code, including careful tuning of how to conduct the parallelization. to show how signs can be extended with other algorithms, i have included the additional two methods in a much simpler way that also provide examples on how further methods can be incorporated. in both cases, there are r packages that implement each method, so essentially all we need to do is write several wrapper functions that call the necessary pre-existing functions, and that provide output in a way that can be used by signs.

first, i have written some convenience functions that make it easier to produce formatted html with the results from the fits, and to obtain figures . these convenience functions will be used with the two new methods, and could be used directly, or with very minor modifications, if other algorithms are implemented.

next, we will use the pre-existing code. the random forests method is available from the r package "party"  <cit> , and can be downloaded from the r repositories. likewise, the boosting method is available from the "mboost" r package  <cit> . for random forests, all we need to do is to wrap-up the existing function  together with the other needed elements for signs to operate. following  <cit>  we will perform a preliminary gene selection step, and after the model is fitted we will need to obtain predictions for each subject, to assess predictive performance . gene selection is carried out with a straightforward function, very similar to the one used in the method by dave et al. to select genes. subject prediction  is based on the expected survival time, as explained in p.  <dig> and ff. of  <cit> ; the mean survival time can be computed  even if survival at the largest observed time is much larger than  <dig> . these three parts of the algorithm  are wrapped in a single function .

next, we write another wrapper that will call this function repeatedly for cross-validation . it is in the cross-validation where the algorithm is parallelized, so the  <dig> calls to cforest, one for each cross-validation fold, are run concurrently. the parallelization is straightforward since we are using the "papply" function .

similar steps are followed for the boosting method. we define a function  that carries out all the needed steps: it calls the "glmboost" function in the mboost package for the initial boosting fit, and then the "cvrisk" function to select the best number of boosting iterations. the selected genes are the ones with non-zero coefficients at that best number of boosting iterations, and subject predictions  are obtained using the model with that best number of boosting iterations. as above, we also write another wrapper function  that will be used for cross-validating the predictive capacity of the approach and that is parallelized using "papply".

all the above code is available in the "signs <dig> r" file  and is called from the "f <dig> r" file. the "f <dig> r" file is the actual r script, whereas signs <dig> r is the underlying r package . finally, for the web-based application to operate, the python code needs to be modified so that the user can select the newly implemented method and pass the appropriate parameters .

crash recovery and fault tolerance
in distributed applications, partial failure  is unavoidable  <cit> . in our installation, we follow several complementary strategies to provide fault tolerance and crash recovery.

shared storage space that uses raid  <dig> provides protection against hard-disk failure, as well as access to results and data from nodes different from the one where computations started. redundancy and load-balancing of the web-service is achieved with linux virtual server with heartbeat and mon  <cit> . this setup ensures redundancy if one of the master nodes fails, and monitors the server nodes so that no http requests are sent to non-responding nodes.

in addition to problems in the web-service and hard-disks, three sources of partial failure that can affect an ongoing calculation are network problems , mpi  errors, and a crash in one of the nodes that are running a slave mpi job. all of these are recoverable errors and, therefore, there is no need to stop the complete calculation  or halt indefinitely. instead, signs provides mechanisms to continue an ongoing calculation in case of the above sources of failure. first, the web-based application periodically examines mpi and rmpi logs and existing lam/mpi daemons to determine if any of the above problems have occurred. if a problem is found, a new lam/mpi universe is booted. before booting the new lam/mpi universe, a script determines which nodes are currently alive and can run mpi processes and, if necessary, modifies the lam/mpi configuration files accordingly. once the new lam/mpi universe is successfully created, a new r process is launched. the r code includes checkpoints so that calculations are not started again from the beginning but only continued from the point they were stopped.

note that errors in our r code, since they are not recoverable, are functionally equivalent to completion of the calculations. the application monitorizes r logs and currently running r processes and, if any errors are detected, the calculation is aborted immediately, a message returned to the user, and the problem logged to allow for prompt fixing.

to ensure that the application is working properly a test is launched every hour from a machine external to the cluster. the test is one from the functional testing suite  and verifies both the user interface and the parallelization infrastructure. if there are any errors, an email is immediately sent to the author and other system administrators.

RESULTS
functionality
signs provides estimates of the performance of the final model using 10-fold cv . to assess predictive performance we use a simple and common  <cit>  strategy: splitting the test samples into several  groups based on their predicted scores , and comparing the survival functions of these groups. it must be emphasized that the predicted scores are obtained from a full  cross-validation, so the predicted scores for a sample correspond to the cv-fold for which that sample never participated in any of the steps leading to the final model.

if a validation data set is provided, the performance of the final model is also evaluated against this validation data set. the validation data set is only used to assess predictive performance, and is not used in any way to build the model.

to assess the stability of the results obtained, we report the number of signatures and the identity of the genes in each signature for the run with the original sample and the  <dig> cv runs, as well as tables with number  of common genes in different runs. the list of these signatures and genes can be sent to our application pals  <cit>  to examine pubmed references, gene ontology terms, kegg pathways or reactome pathways that are common to a user-selected percentage of genes and/or signatures. in other words, the shared features of each signature or set of selected genes can be examined with respect to gene ontology terms, kegg and reactome pathways, and pubmed references. tables with output from each run include clickable links to our application idclight  <cit>  which provides additional information, including mapping between gene and protein identifiers, pubmed references, gene ontology terms, and kegg and reactome pathways.

signs can run in platforms that range from a laptop to a cluster of workstations. our installation runs on a cluster of  <dig> computing nodes, each with two dual-core amd opteron cpus and  <dig> gb of ram. in our implementation, additional nodes provide load-balancing, high-availability, and shared storage. we also incorporate a careful scheme for fault tolerance and crash recovery . the input for the web-based application are either plain text files, or files that come from other tools of the asterias suite  <cit> . further documentation and examples for the web-based application are available from its on-line help  <cit> . signs has been running in production use for over a year and a half; monthly users in the last seven months, from  <dig> may to  <dig> november of  <dig>  are  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> , and  <dig> . bug-tracking is available from  <cit> . signs also includes a test suite that uses funkload  <cit> ; the tests allow to verify that the user interface and numerical output are working, thus ensuring appropriate quality control and regression testing.

the web-based application is accessible from  <cit> . all source code, including the web-based application, r code, and functional tests, are available from launchpad  <cit>  and  <cit>  under open source licenses, gnu gpl for the r package  and the affero public license for the rest of the code.

benchmarking
the speedups achieved in the method by  <cit>  with our code changes  and parallelization are shown in figure 1a), using realistic ranges of numbers of genes and samples . before any parallelization, rewriting the sequential code leads to speed improvements of factors between  <dig>  and  <dig> . these speed improvements are larger as we increase the number of arrays and, specially, the number of genes. parallelization leads to further, and large, increases in speed, which are almost linear with the number of slave processes . with  <dig> slave processes, there is a speed improvement of a factor of about 50: in parallel computing  <cit>  other factors in addition to number of cpus can become limiting, in our case most likely bandwith and latency of inter-node communication, and potential bottlenecks from memory and cache in nodes made of dual-core processors  <cit> . moreover, the rewritten code  shows good scalability: running time increases sublinearly with both number of arrays and number of genes .

for the other three methods  parallelization results in more modest gains . first, there is no gain in speed when we use more than  <dig> slave processes. this is what we would expect, since we parallelize over cross-validation runs  and adding further nodes can not result in increased speed since those are not used. in all three cases, however, the increases in speed with only  <dig> cpus are almost equivalent to doubling execution speed. for fcms, the scaling with number of genes is superlinear , a result of the superlinear scaling of clustering and model selection with number of genes; changes in computing time with number of arrays, however, do not show a consistent increase with number of arrays and, with small number of arrays, computing time can be much larger, because the model selection step takes much longer . for random forest, increases in number of genes only result in noticeable changes in computing time for large numbers of genes , which is to be expected since the random forest algorithm, itself, always uses at most  <dig> genes, so we will not notice the increase in computing with number of genes through random forest, but rather through the preliminary gene selection step . increases in computing time with number of arrays, when using random forests, are modest and sublinear. with boosting, increases in computing time with number of genes are more noticeable, but only become linear over  <dig> genes; increases in computing time with number of arrays are almost linear. figure  <dig> shows the time a user will wait for the web-based application to complete  as a function of the number of simultaneous users using the application in that very moment. . as can be seen from the figure, signs can handle a large number of simultaneous users and shows excellent scalability with number of users. this is the result of both the parallelization of the computations and the load balancing of the non-parallelized code. note that situations with  <dig> or more simultaneous users are completely unrealistic, since the average number of daily users of signs is less than  <dig>  the above benchmarks, though, show that signs can handle even those high numbers of users, which makes it suitable for classroom use.

scripts for all benchmarks are available from the repositories.

discussion
we have developed a web-based tool , signs, for gene selection and signature building from microarray data when we have censored and survival data. signs presents several unique features that make it very relevant for both applied and methodological work.

first, there are no alternative web-based applications for these types of analysis. source code and packages are available from some other approaches , but most of them are out of the direct reach of biomedical researchers, as they all require a minimal proficiency with r and bioconductor. there is only one alternative application with biomedical researchers as target users, brb-arraytools  <cit> . in contrast to brb-arraytools, signs is available as a web-based application  and, therefore, signs does not require any specific operating system or application, just a web browser. in addition, the complete code of signs is available as open source . moreover, signs implements four very different, complementary methods of analysis. finally, the availability of both the source code and the scripts for each run immediately provide for "reproducible research": the complete results can be reconstructed as the user has the code that implements the entire sequence of steps and the parameter settings used. "reproducible research" is a problem of great importance with complex analysis sequences that is gaining attention in the analysis of genomic data  <cit> . by themselves, these features make signs a unique and pioneering tool.

second, signs is one of the very few genomic analysis tools to use parallel computing. parallel computing is crucial to allow further improvements in user wall time and to analyze ever larger data sets: betting on single cpu performance improvements is no longer reasonable, given both the slow increase in cpu speed in the last five years, and the increased availability of multi-core and multi-cpu computers, from laptops and workstations to clusters. our results, using realistic scenarios regarding number of genes and samples, show that: a) our web-based implementation of signs can handle a large number of simultaneous users with good scalability ; b) the performance improvements of parallelization can be harvested even in dual-core laptops and personal computers: relative speed increases of the r code with  <dig> cpus are around 2× for tgd, and  <dig> × for the other three methods .

moreover, by its usage of parallel computing, signs sets a standard in terms of implementing tools that take advantage of recent advances in hardware and computer science. signs represents a rare case example of combining a user-friendly web-based interface with parallel computing, –including fault-tolerance and crash recovery– that, by making the full source code available, allows other researchers to build upon our work and, by the usage of open source licenses, ensures that the code remains owned by the research community. extending upon our work is further eased because we use no python-specific web-frameworks nor r extensions as a web-based application; therefore, the logic of the application  could be programmed in any other language and the computational engine could be different from r.

third, signs strives to ease the biological interpretation of results using functional annotation of results via links to additional data bases that allow mapping between gene and protein identifiers, pubmed references, gene ontology terms, and kegg and reactome pathways. moreover, signs further enhances the critical assessment of results by allowing the examination of possible multiple equivalent solutions .

CONCLUSIONS
signs fills an important need as a user-friendly, web-based application for gene selection and signature finding with survival data. it is also a unique tool , and thus it will be of immediate interest to biomedical researchers, biostatisticians and bioinformaticians. moreover, signs sets a high standard for future applications of this kind.

availability and requirements
project name: signs

project home page: 

operating system: platform independent 

programming language: r, python

other requirements: a web browser.

license: none for usage. web-based code: affero gpl . r code: gpl .

any restrictions to use by non-academics: none.

abbreviations
cgi, common gateway interface; go, gene ontology; kegg, kyoto encyclopedia of genes and genomes; lam, local area multicomputer; mpi, message passing interface.

authors' contributions
rd-u carried out all the work described and wrote the manuscript.

