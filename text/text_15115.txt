BACKGROUND
real-time rt-pcr, which allows to measure any chosen rna with great accuracy over a large dynamic range, has become the gold-standard for nucleic acid quantification. it has also opened new investigations fields, since very small amount of rna is needed, allowing transcripts from low-expressed genes or from very small samples to be quantified.

if constant developments in both reagents and data analysis make real-time pcr measurements more and more accurate and reliable, many considerations have to be taken to convert this technical precision into biologically relevant data. factually, real-time rt-pcr gives access to the number of copies of a chosen sequence in a cdna solution, which is obtained from rna extracted from a known quantity of tissue. the biologically relevant information that has to be ultimately obtained is the global expression level of the chosen gene in the tested sample, at least relatively to another sample.

the quantification of a target gene in a given sample needs three majors steps: rna/mrna extraction, reverse transcription of the extracted rna, and qpcr  processing of the synthesised cdna. a control normalization may be performed at each step to level out dissimilarities between samples  <cit> .

the first possible normalization is by equalizing samples size, such as cell number or tissue weight. this is the easiest and the most intuitive measure. however, its position upstream of the reactions sequence does not allow to correct for the distortions generated by downstream manipulations, especially by rna extraction, whose efficiency may broadly vary from one sample to another.

the second method consists in normalizing samples according to rna content after its extraction. this however does not take into account the reverse transcription efficiency, known to vary from one sample to the other  <cit> .

thus, a downstream normalization method appears to be the most effective. this may be performed by measuring the expression level of a gene transcript expressed in the sample, as an endogenous control for the different reaction steps. the housekeeping term, which is often applied to these genes, was initially given to genes that are necessary for the function of each cell. as a matter of fact, they have to be expressed in each cell type. the most typical case is β-actin, a cornerstone of the inner architecture of the cell. this makes housekeeping genes suitable for organism-wide positive controls for many cdna-based techniques, but does not ensure their expression levels to be equals. the expression levels of usual housekeeping genes has however been shown to vary in some conditions  <cit> .

if the belief in the existence of perfect reference genes, whose levels would remain unchanged in each cell whatever the tissue or the experimental conditions, is known to be more idealistic than real, reference genes have to be chosen specifically for a given experiment, on the basis of the stability of their expression in the subset of studied tissues and experimental conditions one is interested in. in consequence, the use of internal controls implies a proper validation for each experimental condition, as the use of unappropriate normalizing factors, with unconstant expression levels, would generate discrepancies in normalized expression results  <cit> . several methods have been described for that purpose  <cit> .

in addition to the use of reference genes, alternative normalization procedures have been proposed for rt-pcr: the addition of known quantity of artificial rna molecules to extracted rna prior to rt reaction  <cit> , or the use of an oligo-dt linked artificial target sequence quantifiable by quantitative pcr   <cit>  or the total cdna quantification by fluorescent dyes  <cit> .

in the specific context of brain trauma, sample size measurement is especially doubtful, since the oedema resulting from the trauma  <cit>  may enhance brain water content, hence lowering the cellular density of a given weight of tissue. this effect may also be strengthened, as necrosis and apoptosis are known to be major post-trauma events  <cit> .

we have reviewed the use of pcr to quantify mrna levels in mouse and rat traumatic mechanical brain injury models for the five last years. we have identified  <dig> qpcr and  <dig> semi quantitative pcr studies. about 85% of them used a reference gene for normalization, while the others only used rna quantification for this purpose.

gapdh has been up to now the most commonly used reference gene in qpcr  <cit>  and the second one  <cit>  for sqpcr . the use of β-actin has been predominant for sqpcr  <cit> , but less frequent for qpcr, to the benefit of 18s rrna  <cit> . we have found no correlation between the choice of a reference gene and the pathological model , closed head injury  or fluid percussion injury ) or species. strikingly, a single reference gene has been used in all the experiments, without reference to a prior validation, whereas the use of more than one reference gene is widely encouraged  <cit> , and the need for validation highlighted  <cit> . indeed, the choice of reference genes is especially crucial for brain trauma studies, where many biological pathways are implicated , resulting in numerous gene expression changes, which are thus likely to include or affect potential reference genes.

the present study aims at the validation of optimal nomalizing factors for rt-qpcr-based transcriptional studies of an early phase of a murine model of traumatic diffuse brain injury.

rt-qpcr was used to determine the expression levels of five potential reference genes. total cdna level was also measured using oligreen reagent and considered as an other potential nf. these six candidates were compared using different nf-selection methods.

RESULTS
reference genes and total cdna level
after induction of trauma,  <dig> mice were sacrificed at  <dig> min,  <dig> h,  <dig> h,  <dig> h,  <dig> h and  <dig> h, and rna was extracted from the lesionnal zone of injured brains or from the equivalent zone of uninjuried control mice brains, then reverse transcribed. the expression level of 18s rrna, β-microtubulin, s100β, β-actin and gapdh were measured by real-time pcr for each individual cdna.

oligreen reagent is a dye that emits fluorescence at 500/ <dig> nm when bound to single-stranded dna, but is insensitive to free nucleotides and to very short oligonucleotides. however, oligreen emits fluorescence in the presence of rna at room temperature; its use for cdna quantification therefore implies to work at 80°c to specifically quantify reverse-transcribed cdna  <cit> . levels of reference genes at different time post-trauma, as measured by qpcr and of total cdna as measured using oligreen reagent, are represented on fig.  <dig> 

anova
a statistical test was applied to look for significant differences between two experimental conditions for each nf level. as the variance equality hypothesis was verified for each of them , a one-factor anova test was used, with a fischer's test to detect significant differences  between two groups, for a given nf expression level.

the expression level fluctuations between time groups were found to be significant for β-actin, s100β and β-microtubulin. no significant differences were detected from one group to another for gapdh, 18s rrna and cdna .

significant differences between experimental groups clearly make the tested nf unsuitable to normalize sample of the tested experimental set. the expression level of β-actin, s100β and β-microtubulin should thus be avoided as a nf for the present experimental model.

to further study the suitability of the studied normalization factors, three published selection methods were applied: genorm analysis  <cit> , normfinder analysis  <cit>  and a confidence interval based method  <cit> .

genorm analysis
gene stability analysis was performed using the genorm vba applet as described in  <cit> . briefly, the basic assumption of this method is that the ratio of two perfect reference genes should be constant throughout the different experimental conditions. the inter-condition variability of this ratio is thus evaluated for each experimental condition and for each couple of reference gene, and a gene-stability-measure m is calculated for each candidate. this reflects the average pair-wise variability between one putative reference gene and all the others. the less stable candidate  is then excluded as the least suitable nf, and a new step of m-values calculation has then to be performed because the former inclusion of the less stable gene had influenced other m values. by sequential exclusions, the two most-stably expressed genes are selected. the ranking of the studied nf according to their m value, as calculated by the genorm software was, from the most stable to the least:

gapdh – cdna>18s rrna > s100β > β-actin > β-microtubulin .

as proposed by vandersompele et al.  <cit> , the use of a geometric mean of multiple nf among the best ranked should ensure a more accurate normalization. the choice of nf to be included starts from the most stable nf, by a step wise inclusion of the next-ranking nf and the influence of the inclusion on the overall stability is plotted by the genorm software . a strong instability elevation implies that the inclusion is detrimental, and that the last included nf should thus be discarded. in our case, genorm method leads to the use of the geometric mean of total cdna and gapdh levels for the best normalization.

- normfinder analysis
a model-based approach, was used as an another technique to rank the best potential reference genes, using the excel spreadsheet normfinder  <cit> . the criteria for sample sizes  and candidate rg  were met. briefly, for one given sample and one given candidate reference gene, the log-transformed measured level is formulated as the sum of three terms: the general expression level of the gene in the experimental group to which the sample belongs, the amount of mrna in the sample, and the random variation caused by biological and experimental factors. the latter term is used to calculate an intra-group variation, while the first term permits to evaluate an inter-group variability for the considered nf. both variations estimations are combined in a stability value used to rank the nf.

the ranking of each nf according to its stability value as calculated by the normfinder software was, from the most stable to the least:

gapdh>cdna> s100β >18s rrna > β-actin > β-microtubulin .
normfinder stability values output.

- confidence-interval method
a third approach has been used, based on the one presented in haller et al.  <cit> . equivalence testing is similar to classical statistical tests, but while most of them are based on the rejection of a relative closeness hypothesis to prove a significant difference between two means, equivalence testing relies on the rejection of an hypothese of a relative difference between two means to prove their significant closeness.

this can be tested by classical t-test, but, for a given level of confidence alpha, one can proceed more visually through a confidence interval calculation, which is the most-used method for statistical assessment of clinical trials.

briefly, two samples ti and tj, with expression levels μi and μj, are considered to be equivalent for a confidence level alpha if their confidence interval iij, centered on μi - μj and whose length is based on a student t-distribution depending of the distributions of the two samples  is such as iij ⊂ , epsilon being an arbitrarly chosen treshold. moreover, if  <dig> ∉ iij, the two groups are significantly different for the confidence level.

when dealing with expression levels, intrinsic meaning has to be found in ratio rather than in absolute difference. we thus worked with log-transformed expression levels for equivalence testing. in this way, the limit epsilon that has to be chosen is equivalent to a fold-change. we chose to set it to  <dig>  which means an expression ratio between the two compared groups inferior to two.

the confidence intervals for log-transformed normalized nf levels were calculated for each couple of experimental groups and for all nf, with 95% confidence levels . two groups were said to be equivalent with 95% confidence if their confidence level is included in .

measured levels were found to be equivalent between all groups for cdna.

for 18s rrna and gapdh, the expression levels were found not to be equivalent for two couples of groups out of twenty-one. it was the case for  <dig> combinations for s100β,  <dig> for β-microtubulin and  <dig> for β-actin  an equivalence-based ranking thus appears to be:

cdna> 18s rrna-gapdh> s100β > β-microtubulin > β-actin

discussion
we have used the oligreen reagent to quantify the cdna effectively synthesised during the rt step. whatever the procedure for nf choice, cdna as measured by oligreen ranked among the two best. as its use appears to be validated for the present model, it should also be very attractive for other cases, since small variability is unlikely to depend on experimental conditions, in contrast to other reference genes.

to ensure that the overall high stability of oligreen-assayed cdna is not an artefact, we compared for each sample the mean of all studied reference genes levels to the measured oligreen level. the aim is to ensure that oligreen reflects the level of cdna in each sample. the application of the previously used selection procedures for rna input – equal by definition to  <dig> μg for each sample – would indeed have led to the systemic selection of rna quantification as the best normalizing factor, with both intra- and inter- group variation equal to zero. rna level is however obviously not an appropriate normalizer, as it does not encompasses rt efficiency. as the mean of all the gene expression levels measured is likely to be proportionnal to the cdna content, studying the correlation between this average expression level and the oligreen level for each sample appears as a good mean to evaluate whether oligreen measurement is proportionnal to the cdna content or not. a significant correlation was found using a pearson test  and the correlation plot  shows that oligreen measurements effectively reflect cdna levels in the samples, as it correlates well with the the mean of different genes expression levels.

oligreen thus appears to be a good candidate for a generic nf in quantitative rt-pcr experiments. its use may be furthermore refined with alternative rt protocols, such as an oligo dt-primed cdna synthesis. in such a case, oligreen would reflect the level of effectively reverse-transcribed mrna, in contrast to total rna in the present study.

the use of oligreen combined to a standard curve of known dna concentration could moreover allows a precise cdna quantification. in parallel to an absolute quantification of target genes, one could thus formulate the expression level of a studied gene, as the number of copies per microgram of cdna, which would have a stronger intrinsic meaning and could open the gate for easier intra-experiments meta-analysis.

the question remains open whether the fact of combining oligreen measurements to the expression level of internal control  brings more accuracy.

following a confidence-interval based methodology, oligreen appeared as the only nf for which all groups were found to be equivalent to each other, but one has to keep in mind the arbitrary choice of fold-change cut-off level and confidence levels: 18s rrna and gapdh appear to be very close to equivalence and thus appear as acceptable normalizing factors.

when working with confidence levels, an arbitrary level of fold change has to be set. a possible criterion for assessing the pertinence of a chosen cut-off fold-change may come from the observation of confidence intervals iii calculated for a group with itself: as a group is trivially equivalent to itself, the calculated confidence interval is nothing but the reflection of the group's standard deviation. it would thus make little sense to choose a cut-off fold change that would not lead to the conclusion that a group is equivalent to itself. this consideration excluded the initially-considered choice of  <dig>  as a cut-off fold-change in our case, and led to the use of  <dig> as a cut-off fold-change for the study. a standardized confidence interval approach, which remains to be developped, should thus set the cut-off fold change in function of the intra-groups confidence intervals. one has however to keep in mind that alpha level is arbitrary too, and that its value has a direct effect  on the interval size: lowering cut-off fold change is in some extent equivalent to setting a higher confidence level alpha. these two values have thus to be chosen in a concerted manner to make sense.

among the different procedures of normalizing factors that were applied, two criteria appear to be determinant. on one hand, one criterion is whether the method takes into account the belonging of a sample to an experimental group . this should ultimately be a requirement, as the final goal of many, if not all, gene expression experiments consists in levels comparison between different groups. this is handled by model-based and confidence intervals based methodology, but not by pair-wise approach. however, if the normfinder software evaluates inter-group variability, its effect is combined to intra-groups variability to calculate a candidate's stability. this implies that a nf with a low variability in each group, but with an expression level clearly different in one single group, will be considered as the optimal nf, when compared to candidate with greater variability in each group but with almost no differences between groups . this is also the case for the genorm software, and is probably the worst type of error that may occur while selecting a nf, as it will generate systematic errors in normalized gene expression results.

on the other hand, a second determinant criterion for eliciting a nf is the absolute or relative character of its evaluation as a potential nf. this means whether its sole expression values, as measured for every sample, are sufficient or if it has to be compared to other potential normalizing factor. the relative comparison is clearly the basis of the pair-wise method, but is also implicitly present in the model-base approach for the calculation of both intra- and inter- group variations ). the main drawback of these circular methodologies is the risk of selecting co-regulated genes. one would thus need to examinate many carefully chosen potential nf  in order to ensure that a majority of them is not co-regulated, the quality of the nf selection being strongly dependant of the number of studied candidates.

the equivalence interval approach enables to evaluate each nf candidate separately. this permits to imagine a sequential approach to assess reference gene: rather than gathering an as large as possible pool of potential nf candidates, then measuring their expression level and sorting them to determine the most appropriate among them, one may choose a first-intention potential nf and test it ab nihilo. if the test concords with previously fixed criteria, then the candidate is validated as a nf, otherwise, an additional candidate has to be tested. as it seems best to normalize with more than one reference gene, one may stop the selection procedure when two or three candidates are positively evaluated.

an additional interesting feature of an absolute evaluation is that it enables researchers to proceed to a post-hoc validation of their previously chosen nf on the sole basis of already-existing data analysis, with no need to run additional experiments with additional nf. this appears to be especially interesting when the biological material was totally used for previous reactions.

despite its less restricted use, probably due to the lack of an easy-to-handle excel spreadsheet as it is the case with genorm and normfinder, the confidence interval based method appears to be the most powerful and most adequate for nf selection. a scoring procedure based on this approach, allowing a ranking of different nf, along with an automated easy-to-handle automated application, may be an interesting tool to develop.

CONCLUSIONS
in conclusion, we present here the first study aiming at the identification of optimal normalizing factors in a model of traumatic brain injury. if the use of β-actin and β-microtubulin appears to be avoided, the combination of different methods leads us to suggest the use of a geometric mean of 18s rrna, gapdh and total cdna as measured with oligreen. the use of cdna measurement with oligreen has been validated in the present case and is encouraged as first-intention generic normalization factor. the present study also highlights the interest of using confidence interval for normalizing factor accuracy assessing, and opens the way for an iterative, time and cost-effective normalizing factor selection procedure.

