BACKGROUND
methylation studies are a promising complement to genetic studies of variation in dna sequence and structure. most intensively studied is the methylation of dna cytosine residues at the carbon  <dig> position . methylation is typically associated with transcriptional repression  <cit> . this direct link to gene expression means that methylation studies can potentially capture more individual variation in disease susceptibility. methylation studies can also shed a unique light on disease mechanisms and clinical phenomena  <cit>  such as sex differences  <cit> , genotype environment interactions  <cit> , and age-related patterns associated with the disease course  <cit> . finally, methylation sites are appealing from a translational perspective because they are modifiable by pharmacological interventions  <cit>  and are easy to measure using cost-effective assays in readily available biosamples  <cit> .

for most common, complex diseases, detailed prior biological knowledge is typically lacking. therefore, genome-wide approaches that proved fruitful in the context of sequence variants  <cit>  will also be critical to detect disease relevant methylation sites  <cit> . next-generation sequencing  is an appealing technology for such methylome-wide association studies . compared to arrays, ngs provides better coverage of all possible methylation sites in the human genome  <cit> . furthermore, the relatively low amounts of starting material will reduce errors and bias caused by sample preparation and amplification. finally, the availability of fast semi-automated sample preparation, the increase in the amount of data generated per run, and the decrease in reagent costs have already made ngs a cost-effective option for a comprehensive interrogation of the methylome.

the most comprehensive method for ascertaining methylation  status at each nucleotide position is bisulfite sequencing  <cit> , where unmethylated cytosines in genomic dna are converted to uracil and then converted to thymine in post-bisulfite pcr  <cit> . the single base resolution is attractive because it allows precise mapping of disease relevant sites  <cit> . however, due to the combination of high costs of sequencing entire genomes and the large numbers of samples needed to provide adequate statistical power, whole-genome bisulfite sequencing is not currently economically feasible as a screening tool for disease association studies  <cit> . a commonly used cost-effective alternative aims to sequence only the methylated part of the genome. here, dna is first fragmented and the methylated fragments are bound to antibodies  <cit>  or other proteins  <cit>  with high affinity for methylated dna. the unmethylated genomic fraction is washed away, and the methylation-enriched portion of the sample is then collected and sequenced  <cit> .

knowledge of the fragment size distributions in enrichment-based mwas is important for several aspects of the data analysis. a clear example involves the calculation of enrichment measures. dna methylation is most often, although not exclusively, found in the sequence context cpg. certain enrichment protocols , can even only detect cpg methylation. given that we know exactly where the cpgs are located, there is no need to search for enrichment peaks using methods commonly used in chip-seq experiments  <cit> . although there are other ways to quantify enrichment  <cit> , a commonly used approach is to count the number of fragments covering each cpg. if the fragment sizes are unknown, the number of reads covering the cpg is typically counted instead  <cit> , where read length is sometimes extended to the expected fragment length. however, this is a rough approximation. first, due to the stochastic nature of dna fragmentation, one cannot assume an equal size for all fragments. second, the expected fragment size may be mis-specified if the sequenced fragment pool differs from the one obtained after fragmentation. this could arise, for example, if smaller fragments are more likely to be pulled down in the enrichment step. third, there will be variation in the fragment size distribution across samples despite standardized lab protocols. the possible implication of using the read count approximation is therefore that coverage estimates may become biased and imprecise.

a second example illustrating the importance of the fragment size distribution is that failure to account for differences in fragment size distributions between samples may create artificial inter-individual differences in coverage estimates. to illustrate this, assume that in sample a all dna fragments are exactly  <dig> bases in length and that we sequence at  <dig> bp read length. when using a read count to calculate coverage for the cpg that caused the enrichment, all reads will contribute to this count because the start positions of all aligned reads will be within  <dig> bp of that methylated cpg. now assume a second sample b that has identical methylation levels at the target cpg. however, for this sample all fragments are  <dig> bp long, but we still sequence only  <dig> bp of each fragment. as only a proportion of the reads would now start within  <dig> bases of the cpg, the read count will be less than for sample a and underestimate the number of fragments covering the target cpg.

rather than using an estimation procedure, by sequencing paired-end libraries we can obtain the fragment size distribution by subtracting the start positions of successfully aligned read pairs. however, paired-end libraries have only recently become available, so legacy data is typically single-end and not all sequencing platforms currently support paired-end libraries. second, the use of paired-end libraries is more expensive and almost doubles the sequencing run time. these disadvantages may be justified in studies that require single base resolution, such as calling dna sequence variants or estimating the percentage of methylation at specific bases after bisulfite conversion. in these scenarios, the number of reads covering each base is a critical determinant of data quality. however, for the enrichment-based methylation studies considered in this paper, it is the number of sequenced fragments that determine data quality. as the use of paired-end libraries does not increase the number of fragments, one could argue that it is better to spend the additional resources on sequencing more fragments using single-end libraries.

the goal of our investigation is to develop a method that uses single-end sequencing data to estimate fragment size distributions. due to the nature of the sequencing technology as well as specific lab procedures to optimize the assays , it is difficult to make strong parametric assumptions about this distribution. therefore, we propose a non-parametric method. to validate our method, we performed simulation studies and made comparisons with ngs studies of paired-end libraries, which provide a benchmark by allowing an empirical determination of the fragment size distribution.

methods
a detailed exposition of the proposed method to estimate cpg coverage can be found in the supplemental material and we confine ourselves here to a summary. in contrast to for example chip-seq data, in methylation studies there are often many sites that are located close to each and all can affect the enrichment. this complicates the estimation of the fragment size distribution. for example, using all reads in the neighborhood of a cpg that has other cpgs nearby will give imprecise estimates of the fragment size distribution because part of the enrichment at that locus will be the result of the nearby cpgs. to address this problem, our method uses only isolated cpgs. an isolated cpg is defined as a site c for which the interval  contains no other cpgs but c and where d is larger than the longest possible fragment size. for these isolated cpgs it is reasonable to assume that, given the fragment size x = x, the possible nucleotide positions r where the reads can start is independent and uniform distributed .

from this assumption it follows that the probability mass function of the possible read start position r equals:

  prr=r=∑x=r+1sprxx 

where pr denotes the probability that fragments have size x, and s is the size of the longest fragment. the summation over fragments starts at r +  <dig>  because fragments can only have reads with a start position r if x > r. we obtain the probability mass fragment size function by solving pr from :

  prx=r=prr=r−1−prr=r×r 

because the enrichment is imperfect, not all sequenced fragments will contain methylated cpgs. under the assumption that the start positions of such “noise” reads are uniformly distributed in the  interval, they will not bias estimates because pr in  is calculated from the difference between the numbers of reads starting at adjacent bases.

we need to know for each read the probability that the fragment it is tagging covers that cpg. this coverage function is equal to the complement of cumulative fragment size function 1-) or

  prx>r=∑x=r+1sprx 

for example, if the read length is  <dig>  pr =  <dig>  for reads starting within  <dig> bp of the cpg but pr <  <dig>  for reads starting further away as part of the tagged fragments will be too short to cover the cpg. once we determined for every read the probability that the fragments they are tagging cover the cpg, these probabilities can be summed over all reads to obtain a coverage estimate for the cpg.

an expected contribution of a randomly chosen read to the coverage, e, can be calculated by combining  that gives the distribution of read start positions with  that specifies how reads starting at these locations contribute to coverage:

  ecov=∑x=0s−1prr=rprx>r 

equation  shows that e depends on the fragment sizes. the implication is that coverage estimates will differ across samples if the fragment sizes differ across these samples. however, because the fragment size distribution is determined by the lab protocol and is not directly related to the amount of methylation, this difference represents an artifact. to avoid such differences it may be necessary to standardize the coverage estimates using this expected contribution. we calculated the required coverage standardization factor for each sample as the mean of the expected read contributions across all samples in the study divided by e in  for that specific sample.

estimation procedure
we apply the following stepwise procedure to estimate the coverage function in .

1) select the isolated cpg sites for the chosen interval  and count all the read start positions in the vicinity of these isolated sites. for reads aligning to the forward strand this involves all reads starting in the c-d interval that is upstream of the cpg, and for reads on the reverse strand all reads starting in the  + d interval that is downstream of the cpg. a value for d can be obtained from a visual inspection of an initial plot of the read start counts. for example, the dots in figure  <dig> show an example where these counts decrease until position  <dig> after which they start to fluctuate around the “noise” level. this pattern suggests that very few fragments are longer than  <dig> and for d we could therefore choose a value between 250– <dig> bp.

2) as the read start counts will show sampling fluctuations, we “smooth” the data prior to calculating pr with formula . our first method involved the nadaraya-watson estimator  <cit> . this smoother takes for each position the m nearest neighbors and estimates the number of read starts by averaging the values across this window using a kernel as a weighting function. as it essentially uses the mean, this kernel-based smoother assumes that the underlying function is locally constant. to provide an alternative we also used a cubic spline method that fits a more flexible local regression model instead. we illustrate results obtained with the two methods in figure  <dig> 

3) step  <dig> results in a set of estimates of pr that are used in step  <dig> to calculate the coverage function . the true underlying coverage function  is monotone descending but in practice this may not hold due to sampling fluctuations. to ensure monotonicity, as a final smoothing step we used the procedure proposed by dette et al.  <cit> .

to obtain the coverage functions and standardization factors we wrote an r function that also summarizes and plots the  data. for the kernel-based smoother we used the r function ksmooth. for the cubic spline we used smooth.spline, the design of which parallels the smooth.spline function of chambers and hastie  <cit> . the function monoproc was used in the final step to obtain monotone descending coverage functions. we also created a program to create the input data, which is a table with the counts of the read starts around isolated cpgs. prior to calculating this table, the program allows user specified quality control  of the reads. because of the size of the data files, this program was coded in c++. the source code, windows and linux executables, and documentation are freely available from http://www.people.vcu.edu/~ejvandenoord/.

empirical data used to test method
to validate our method we sequenced  <dig> bp +  <dig> bp paired-end libraries in  <dig> inbred adult c57bl/ <dig> male mice . the number of reads per sample was on average  <dig>  million. we could map 87% of the reads. using d =  <dig> bp, the total number of isolated cpgs was  <dig>  which corresponds to  <dig> % of all cpgs in the c57bl/ <dig> genome . in terms of uniquely mapped reads, an average of  <dig>  reads per sample mapped to isolated cpgs.

fifty-two percent of the mapped reads  satisfied our criteria for high quality read pairs meaning that they aligned uniquely with the right orientation and acceptable fragment size. we obtained the fragment size distributions from the paired-end data by subtracting the start positions of the successfully aligned read pairs. although these fragment size distributions may not be perfect , they should provide a good opportunity to validate findings as the distributions are “observed” and do not require estimation.

RESULTS
simulation studies
to test our estimation procedure through simulations, we generated  <dig>  random samples based on each of the three distributions in figure  <dig>  the number of reads with start positions close to isolated cpgs equaled  <dig> ,  <dig> ,  <dig> ,  <dig> , or  <dig> . the condition that assumes  <dig>  reads is comparable to what we observe in our empirical data. the other conditions enable us to get a sense of the robustness of our method in case fewer reads would be available. to assess the precision of our estimator, we first calculated the mean difference and absolute mean difference between the estimated coverage function and the real coverage function used to simulate the data, and then averaged these differences across all possible read start positions. when subsequently averaged across the  <dig>  simulated samples, the mean difference provides information about whether there are systematic differences  between estimated and true coverage functions. to obtain a measure of the variability of the estimated coverage functions, we also calculated the standard deviation of the mean difference in the  <dig>  simulations. finally, the mean of the absolute differences across the  <dig>  simulations provides an overall measure of precision that incorporates both systematic differences and the variability of the estimates.

all three statistics in table  <dig> show that precision increased with sample size. the mean was very close to zero, suggesting that the estimates were unbiased. the small standard deviation and mean absolute difference suggested that our method was precise. in addition to sample size, the fragment size distribution type affected the precision of the estimates . the least precise estimates were obtained for the kurtotic distribution and the most precise estimates for the distribution that was positively skewed. in table  <dig> results for the kernel-based method are shown but those obtained using cubic splines were almost identical.

estimating coverage functions with empirical data
additional file 1: figure s <dig> shows the plots with read start position distributions for each of the  <dig> samples. these distributions show systematic outliers at the very beginning of the read . however, after these initial positions, the frequencies of the read start positions do not show a systematic trend until the read length is reached. the decay after that point is expected and caused by parts of fragments becoming too short to cover the cpg , which essentially forms the basis of our estimator. in other data, we have sometimes observed a decay that starts before the read length is reached. however, this was the result of some fragments being shorter than the read length. such fragments can occur when the instrument initially sequences part of the adaptor. these reads are then “trimmed” during alignment. thus, when the methylated cpg is at the very beginning of the read, the assumption of uniform read start distribution does not hold. however, as our estimator only uses the data starting from approximately the minimum read length, these outliers do not affect the estimation. the absence of a systematic trend until the minimum fragment length is reached suggests that the assumption of a uniform distribution for position-level read counts is reasonable for the range from which the data are used.

before estimating the coverage function using the empirical sequencing data, we first eliminated one read from each pair as to create single-end read input data. empirical data will comprise multi- and duplicate-reads. many reads map to multiple locations of the genome. often a single alignment can be selected because it is clearly better than the others. in the case of multi-reads, multiple alignments are about equally good. selecting only the best alignment for each multi-reads read carries along the danger of alignment errors . on the other hand, excluding all multireads may affect accuracy in a negative way  <cit> . duplicate-reads are reads that start at the same nucleotide positions. when sequencing a whole genome duplicate-reads often arise from template preparation or amplification artifacts. in our context of sequencing an enriched genomic fraction, duplicate-reads are increasingly likely to occur by chance because reads are expected to align to a much smaller fraction of the genome.

we examined empirically whether it would be better to allow for a limited set of high quality multi- and duplicate reads or exclude all such reads. to select high quality multi-reads, any read that mapped to more than  <dig> loci was excluded from further consideration. from the remaining multi-reads, we selected those that aligned almost equally well to only a few loci. specifically, we selected the multi-reads that had fewer than five alignments with alignment scores  within five points of the best score. to avoid disproportionate representation, multi-reads were weighted in proportion to the number of alignments in the coverage calculations. in all instances where > <dig>  reads started at the same position, we reset the read count to  <dig> for the coverage calculations assuming that these reads all tagged a single fragment. if  <dig> or  <dig> reads started at the same position, we looked for other reads in neighborhood ± <dig> bp. if other reads mapped to this area, we retained the read count of  <dig> or  <dig> in the coverage calculations, assuming that the duplicate reads occurred by chance due to enrichment of fragments caused by methylated cpg in the region. if no other reads were found, we assumed that the duplicate reads were artifacts and reset the read count to  <dig> for the coverage calculations.

in table  <dig> we report the mean, standard deviation, and absolute mean difference between the estimated coverage function  and the coverage function as implied by the paired-end fragment size distributions of the  <dig> samples. the most precise results were obtained by including high-quality multi- and duplicate-reads. here, the mean was closest to zero indicating almost unbiased estimates, the standard deviation was smallest implying that the estimates were less variable, and the mean of the absolute difference that is a function of both a possible bias plus the variability in the estimates was also smallest. comparisons suggest multi-reads are more critical for precision than duplicate-reads. as shown by the first row of table  <dig>  the proportion of high quality multi-  and duplicate reads  can be substantial. this larger number of observations when multi- and duplicate- reads are used in for the estimation may explain the higher precision of the estimates.

note: # reads/sample is the number of reads around isolated cpgs used as input for our estimation method.

coverage estimation
in table  <dig> we report results from various coverage calculations performed on the mouse data. coverage was calculated for all  <dig>  million cpgs of the  <dig> autosomal mouse chromosomes. the first row shows the coverage calculations using the fragment size distributions as “observed” in paired-end read data. these coverage estimates were used as the benchmark. next, we present a “traditional” coverage calculation where we counted the number of sequence reads covering the cpgs. results show that this method severely underestimates the coverage. more precisely, the “ratio” column shows that the mean coverage is merely  <dig>  percent of that obtained after analyzing the paired-end data. furthermore, when we correlated these coverage estimates with those obtained from the paired-end data, we only obtained a very modest correlation of  <dig> . the dna samples were fragmented by ultrasonication  to a target median size of  <dig> bp. for the coverage calculations in the next row of table  <dig> we extended the read length from  <dig> bp to this  <dig> bp target. results improved but coverage was still underestimated by 13% and the correlation with paired-end coverage estimates was  <dig> . in the row labeled “kernel estimate”, we used our method to estimate individual coverage functions for all  <dig> mice thereby including high quality multi- and duplicate reads. results were now very similar to the results obtained with the paired-end data with only a slight overestimation of  <dig>  percent. in addition, these coverage estimates correlated  <dig>  with the estimates from the paired-end data suggesting almost identical results. we also explored whether using the mean coverage function produced even more precise results. this was not the case. the most likely explanation is the use of a mean function for estimating coverage when in reality considerable individual differences in fragment size distributions exist.

note: ratio is mean divided by mean of paired-end data and correlation is the correlation between coverage calculations with the method listed in the row versus those obtained from the paired-end data.

discussion
we developed a non-parametric method that uses isolated cpgs to estimate sample specific fragment size distributions from data obtained by sequencing single-end libraries. an important application of the proposed method is to quantify the amount of methylation by estimating the number of fragments covering a cpg. to optimize coverage estimation, we studied several variations. although it is possible that the optimal approach varies somewhat across settings, we found that the two smoothing methods had very similar overall performance. furthermore, the inclusion of particularly high quality multi-reads, rather than merely using uniquely mapped reads, improved the precision of the estimated coverage function. this finding is consistent with other reports showing that multi-reads contain information and should not automatically be discarded  <cit> . finally, the use of a mean coverage function across all samples may result in a loss of precision. this is because the reduced sampling fluctuations may not outweigh the biases that are introduced when a mean function is used to approximate fragment size distributions that are likely to vary across samples, even if stringent lab protocols are used to minimize these differences.

our data suggested that taking the fragment size distribution into account may be important to obtain unbiased coverage estimates even when the standard  read count method is used for coverage calculations. thus, using the mouse sequence data, we showed that even after careful size selection and the use of a standardized protocol to fragment dna, differences in fragment size distributions can occur that can create artificial inter-individual differences in coverage estimates. to avoid these biases we proposed a standardization factor that can be calculated from the estimated fragment size distributions.

further applications of the estimated fragment size distributions are conceivable as well. for example, enrichment-based methods are semi-quantitative in the sense that they do not yield direct estimates of methylation levels. for the purpose of assessing methylation levels of sites, methods have been developed to remedy this problem by normalizing the data based on local cpg density  <cit> . however, the optimal definition of cpg density depends on the fragment size distribution. for example, the local cpg density of a site will be higher if the fragments are larger. thus, the proposed method can yield a more refined measure of cpg density.

our estimator uses data from isolated cpg sites, which correspond to a modest proportion of all cpgs . it is possible that fragment length distribution differs for the remaining cpg sites. it is important to note that such a bias would affect our method but not the coverage function derived from the paired-end sequencing data that considers all fragments. the fact that we observed a correlation of . <dig> between coverage calculations based on our estimate versus those based on the paired-end coverage function suggests that a possible bias does not interfere with the precision of our method. a somewhat related point is that enrichment protocols may be less efficient for cpg poor versus cpg rich regions  <cit> , and that the enrichment will depend on the extent isolated cpgs are methylated. as the precision of our method depends on the successful enrichment of fragments with a single methylated cpg, it may not work as well with protocols that mainly enrich for cpg dense regions or in samples where isolated cpgs are not methylated.

CONCLUSIONS
methylation studies are a promising complement to genetic studies of dna sequence. however, detailed prior biological knowledge is typically lacking, so methylome-wide association studies will be critical to detect disease relevant sites. a cost-effective approach involves sequencing single-end libraries created from samples that are enriched for methylated dna fragments. a limitation of single-end libraries is that the fragment size distribution is not observed, which hampers several aspects of the data analysis. in this article we developed a non-parametric method that uses isolated cpgs to estimate sample specific fragment size distributions. we show that our method is highly accurate and can improve the analysis of cost-effective mwas studies that sequence single-end libraries created from samples that are enriched for methylated dna fragments.

abbreviations
ngs: next-generation sequencing; mwas: methylome-wide association studies; qc: quality control.

competing interest
the authors report no conflicts of interest.

authors’ contributions
gr, jb, and evdo and developed the method. lyx generated the data used to validate the method. ka and jlm contributed theoretical expertise about sequencing technology and methylation. evdo implemented the method. evdo and sn the performed data analyses. evdo, jlm and ka drafted the ms. all authors read and approved the final manuscript.

supplementary material
additional file 1
supplemental material for the paper: estimation of cpg coverage in whole methylome nextgeneration sequencing studies.

click here for file

 acknowledgements
funding
this work was supported by the national human genome research institute , the national institute on drug abuse , and the national institute of mental health .
