BACKGROUND
in  <dig>  around the time the human genome project was initiated, the cost of sequencing was around $ <dig> per base. by  <dig>  the cost had fallen to about  <dig> to  <dig> cents per nucleotide  <cit> . nowadays, sanger sequencing can be approached at a cost of around  <dig>  cents per nucleotide  but a recent technology breakthrough, pyrosequencing, is likely to drop the costs even further, while simultaneously increasing the throughput by an order of magnitude or more. pyrosequencing  <cit>  is a real-time, sequencing by synthesis method based on the detection of released pyrophosphate during dna synthesis. pyrosequencing's most impressive feature is the throughput, being up to  <dig> megabases/hour. on the other hand, the sequenced fragments have reduced lengths compared to sanger ones, being  <dig> bases on average in our experience.

the recent dramatic increase in sequencing throughput together with the reduction of costs calls for increased computation power, as well as increased storage space, in order to keep up. also, it is to be considered that most bioinformatics tasks such as genome assembly, inversion distance computation, genome rearrangement analysis and molecular dynamics have got a quadratic or higher complexity.

in addition, an analysis of the cpu speed trends reveals that the cpu speed increases are considerably lower nowdays than they used to be in the past. for example let's consider amd cpu release history:

- june 2000: athlon thunderbird  <dig> released  <cit>  

- february 2003: athlon xp barton 2500+ released  <cit>  

- may 2005: athlon  <dig> 3800+ released  <cit>  

between june  <dig> and february  <dig>  there was a speedup of  <dig> × making an average speedup of 71% per year, while between february  <dig> and may  <dig>  the speedup was a mere  <dig> × making an average speedup of 21% per year. some hope for new performance improvements is brought by recently marketed multi-core cpus, however at this point in time it is still not clear how quickly these can evolve . note that we quoted amd's cpu history and not intel's just because amd names its cpus against a performance rating , which is a better indicator of effective cpu speed than clock speed that intel uses, and hence makes it easier to compare cpu performances through time and across architectural improvements.

keeping up with bioinformatics data is hence becoming increasingly difficult in the localized environment. a computing cluster might seem the solution, however for small companies and small research groups producing uneven spikes of computationally intensive jobs, a privately owned cluster might not be an effective solution as it tends to be either very expensive or underpowered during the actual spikes of work, while remaining underused for the majority of the time.

in the aforementioned situation, the european data grid , a large community of computation clusters – load balanced as a whole, is likely to offer a better alternative. after formally requesting access to the grid to infn  <cit> , certificates will be issued and the new grid user can leverage the power of more than a thousand cpus spread all over europe. the grid power is not completely free: after a significant submission of jobs, infn might ask the new user to share some computing resources, however, the overall hardware cost would still be very low compared to that of a dedicated cluster able to handle a significant workload.

grid job submission itself is rather simple, however the strict limitation in the size of the input sandbox  and other subtleties described hereafter in this paper can discourage a regular use of the grid.

our first contribution to this paper is the development of a computation pipeline, integrated with a database system, for storing and analyzing human amplicons sequences coming from a high throughput  <dig> life sciences gs  <dig>  <cit>  pyrosequencer.

the pipeline started as localized and was then ported to the grid. to ease the porting to the grid we developed vnas, a grid job submission, virtual sandbox manager and job callback framework, which constitutes our second contribution to this paper. vnas is aimed at rendering the grid job submission significantly more powerful yet simpler, and allows to overcome the grid submission limitations without affecting the grid infrastructure negatively.

RESULTS
amplicons experiment
initially, we extensively leveraged the "repeatability with altered parameters" feature together with the "by hand searchability" of the results database mentioned in the methods section. this allowed us to easily compare results obtained with various parameter sets, fine-tune the parameters and thresholds for our pipeline and better understand the peculiar behaviour of our new  <dig> life sciences gs  <dig> pyrosequencer .

afterwards, we successfully analyzed  <dig> sequenced amplicons from chromosomes  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> of a cancerous human sample and correctly found punctual mutations confirmed by either ncbi dbsnp or sanger resequencing. more analyses are expected for the future.

the grid porting for this pipeline is discussed in the vnas sections.

vnas framework and pipeline grid porting
we report below the average times for the first computation step .

1- first computation step for a dataset using  <dig> slices: time gain factor = 5
the computation step  <dig> input was divided into  <dig> slices, each slice was submitted to the grid then results were fetched back. the whole process was repeated multiple times to calculate the following averages:

submission time
 <dig> seconds per slice . this involves fetching of sequence data, creation of sandbox implying some copying/linking of directory trees and files , gzip compressions and md <dig> hash computations , actual submission of the job to the grid .

queue time
 <dig> minute to 2+ hours,  <dig> minutes on average for one job,  <dig> minutes is the average for the maximum wait time on a set of  <dig> jobs. for a discussion on the great variance of the queue time in the edg, see the related paragraph in appendix b.

execution time
 <dig> minutes per slice .

total time 
 <dig> ×  <dig> sec +  <dig> min +  <dig> min ~=  <dig> minutes

the total time for the first computation step is  <dig> minutes on average instead of  <dig> minutes seen in local execution. similarly happens for the other steps for which we omit timing measurements. this is already a significant improvement, and more interestingly, our local computation resources remain free during most of the time.

2- first computation step for  <dig> datasets using  <dig> ×  <dig> slices: time gain factor = 15
the grid is more effective when more data can be processed in parallel. this is the case when we have to recompute a number of old datasets with altered pipeline parameters. the following are average time measurements for a recomputation of  <dig> datasets like the above one. in this case we used fewer, bigger slices  so as not to flood the grid with too many jobs and also to reduce the maximum queue wait time.

submission time
 <dig> seconds per slice 

queue time
the average for the maximum wait time on a set of  <dig> jobs is roughly  <dig> minutes

execution time
 <dig> minutes per slice 

total time 
 <dig> ×  <dig> sec +  <dig> min +  <dig> min =  <dig> minutes

the total time here is  <dig> minutes, instead of  <dig> hours seen in local execution. this is clearly a great improvement compared to local execution. one could submit even more computation data: the greater the computation you can submit which can execute in parallel, the higher the benefit you get from the grid.

in appendix b you can find some more details regarding grid queues times. in appendix c you can find some observations on the difficulties which still persist when porting applications to the grid.

CONCLUSIONS
we have shown how we realized a multistep computation pipeline integrated with a database system to analyze punctual mutations from human amplicons, taking into consideration the peculiarities of the new pyrosequencer, and how we ported such pipeline to the european data grid to leverage its enormous computing power obtaining a speedup factor up to  <dig>  we have also shown how the grid can be economically very advantageous for the small and medium research group producing uneven spikes of workload compared to a dedicated cluster.

we have shown how the porting of a parallelized application to the grid can be significantly eased with our newly developed vnas framework, providing a virtual sandbox of unlimited size with support for complex directory structures, multiuser transparent file sharing over the grid with timed expiration, and a callback mechanism for a higher automation of join points in computation pipelines. vnas is currently in beta stage but will soon be available for download. we expect the ease of use of vnas to help increase the usage of the european grid with profit also to the bioinformatics community.

