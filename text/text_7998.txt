BACKGROUND
rna molecules
ribonucleic acid  is made up of four types of nucleotide bases: adenine , cytosine , guanine , and uracil . a sequence of these bases is strung together to form a single-stranded rna molecule. rna plays important roles in many biological processes including gene expression and regulation. rna molecules vary greatly in size, ranging from nineteen nucleotide bases in micrornas  <cit>  to long polymers of over  <dig>  bases in complete viral genomes  <cit> . although an rna molecule is a linear polymer, it tends to fold back on itself to form a 3-dimensional  functional structure, mostly by pairing complementary bases. among the four nucleotide bases, c and g form complementary base pairs by hydrogen bonding, as do a and u; in rna , g can also base pair with u residues. the overall stability of an rna structure element is determined by the "minimal free energy" defined as the amount of energy it would take to completely unpair all of the base pairs that hold it together .

the 3d structure of an rna molecule is often the key to its function. because of the instability of rna molecules, experimental determination of their precise 3d structures is a time-consuming and rather costly process. however, useful information about the molecule can be gained from knowing its secondary structure, i.e., the collection of hydrogen-bonded base pairs in the molecule  <cit> . rna secondary elements can be classified into two basic categories: stem-loops and pseudoknots . both kinds of secondary structure elements, which have been implicated in important biological processes like gene expression and gene regulation  <cit> , must contain at least one inversion, i.e., a string of nucleotides followed closely by its inverse complementary sequence. figure  <dig> shows an example of an inversion, with the 6-nucleotide string "accgca" followed by its inverse complementary sequence "ugcggu" after a gap of three nucleotides.

rna secondary structure predictions
secondary structures are crucial for the rna functionality and therefore the prediction of the secondary structures is widely studied. development of mathematical models and computational prediction algorithms for stem-loop structures began in the early 1980's  <cit> . pseudoknots, because of the extra base-pairings involved, must be represented by more complex models and data structures that require large amounts of memory and computing time to obtain the optimal and suboptimal structures with minimal free energies. as a result, development of pseudoknot prediction algorithms began in the 1990's  <cit> .

most existing secondary structure prediction algorithms are based on the minimization of a free energy  function and the search for the most thermodynamically stable structure for the whole rna sequence. searching for a structure with global minimal free energy may be memory and time intensive, especially for long sequences with pseudoknots. to overcome the tremendous demand on computing resources, various alternative algorithms have been proposed that restrict the types of pseudoknots for possible prediction in order to keep computation time and storage size under control. yet, most programs available to date for pseudoknot structure prediction can only process sequences of limited lengths on the order of several hundred nucleotides. these programs, therefore, cannot be applied directly to larger rna molecules such as the genomic rna in viruses, which may be thousands of bases in length. at the same time, minimal energy configurations may not be the most favorable structures for carrying out the biological functions of rna, which often require the rna to react and bind with other molecules . our current work suggests that local structures formed by pairings among nucleotides in close proximity and based on local minimal free energies rather than the global minimal free energy, may better correlate with the real molecular structure of long rna sequences. this hypothesis has yet to be supported by more detailed experimental evidence. if proven correct, our approach will open the door to a new generation of programs based on segmenting long rna sequences into shorter chunks, predicting the secondary structures of each chunk individually, and then assembling the prediction results to give the structure of the original sequence.

in our previous work, we had proposed to predict secondary structures for long rna sequences using three steps:  cut the long sequence into shorter, fixed-size chunks;  predict the secondary structures of the chunks individually by distributing them to different processors on a condor grid; and  assemble the prediction results to give the structure of the original sequence  <cit> . we used this approach on the genome sequences of the virus family nodaviridae, leading to the discovery of secondary structures essential for rna replication of the nodamura virus  <cit> . however, the study also identified the necessity of having a more effective segmentation strategy for cutting the sequence so that the predicted results of the chunks can be assembled to generate a reasonably accurate structure for the original molecule. indeed, the selection of cutting points in the original rna sequence is a crucial component of the segmenting step. in this paper, we propose to approach the problem by identifying inversion excursions in the rna sequence and cutting around them. we consider two alternative inversion-based segmentation strategies: the centered and optimized chunking methods. both methods identify regions in the sequence with high concentrations of inversions and avoid cutting into these regions. in the centered method, the longest spanning inversion clusters are centered in the chunks, while in the optimized method, the number of bases covered by inversions is maximized. preliminary results have been presented in the authors' work  <cit> .

mapreduce and hadoop
the prediction of rna secondary structures for long rna sequences based on sequence segmentation can be performed in parallel, thus benefiting from parallel computing systems and paradigms. we use the well-known mapreduce framework hadoop for our parallel predictions. the mapreduce paradigm is a parallel programming model that facilitates the processing of large distributed datasets, and it was originally proposed by google to index and annotate data on the internet  <cit> . in this paradigm, the programmer specifies two functions: map and reduce. the map function takes as input a key k <dig> and value v <dig> pair, performs the map function, and outputs a list of intermediate key and value pairs which may be different from the input list 〈k <dig>  v2〈 - i.e., map 〈k <dig>  v1〉 → list 〈k <dig>  v2〉. the runtime system automatically groups all the values associated with the same key and forms the input to the reduce function. the reduce function takes as input a key and values pair 〈k <dig>  list〉, performs the reduce function, and outputs a list of values - i.e., reduce 〈k <dig>  list〉 → list 〈v3〉. note that the input values to reduce is the list of all the values associated with the same key.

mapreduce is appealing to scientific problems, including the one addressed in this paper, because of the simplicity of programming, the automatic load balancing and failure recovery, as well as the scalability. it has been widely adapted for many bioinformatics applications. for example, hong et al. designed an rna-seq analysis tool for the estimation of gene expression levels and genomic variant calling  <cit> , and langmead et al. designed a next-generation sequencing tool based on mapreduce hadoop  <cit> . to the best of our knowledge, our work is the first one to adapt mapreduce into secondary structure predictions of long

rna sequences. preliminary work on the reasoning behind adapting rna secondary structure predictions to the mapreduce paradigm can be found at  <cit> .

method
workflow for parallel chunk-based predictions
rather than predicting the rna sequence as a whole, we cut each sequence into chunks and predict each chunk independently before merging the predictions into the whole secondary structure. as the cutting process can be performed in different ways, the search for effective ways to cut sequences can require a large search space and generate a large number of independent prediction jobs that can potentially be performed in parallel. the workflow for a parallel chunk-based rna secondary structure prediction and accuracy assessment consists of the following four steps:  chunking: each rna sequence is cut into multiple chunks  according to various chunking algorithms and parameters;  prediction: the secondary structure for each chunk is predicted independently by using one or more prediction programs;  reconstruction: the whole secondary structure of a sequence is reconstructed from predicted structures, one for each chunk; and  analysis: reconstructed structures are compared against known structures to assess prediction accuracies.

chunking process based on inversions
given a long rna sequence, we identify regions with high concentrations of inversions by using an adapted version of the "palindrome" program in the emboss package  <cit> , which is a free open source software analysis package. two main reasons for adapting the emboss palindrome program are as follows: the original program works correctly on dna but not rna sequences and does not support g-u pairing that we plan to include in our adaptation. our adapted program, inversfinder, is written in java and is available for download at http://rnavlab.utep.edu. inversfinder requires a text file containing the rna sequence in fasta format as input. the minimum stem length l and maximum gap size g of the inversion are parameters specified by the user.

the chunking step relies on a general excursion approach first formulated in  <cit> , which has already been applied to a variety of sequence analysis problems but not to rna secondary structure predictions. in many bioinformatics applications, the problem calls for identifying high concentration regions of a certain property in the nucleotide bases of biomolecular sequences. for example, replication origins in viral genomes have been predicted by looking for regions that are unusually rich in the nucleotides a and t in dna sequences  <cit> . in this paper, we follow the same approach for rna sequences, but our focus is whether or not the nucleotide base is found inside an inversion. we refer to the excursions generated by this property as "inversion excursions." the excursion method requires assigning a positive score to each nucleotide if it is a part of an inversion , and a negative score if it does not. we go through the entire nucleotide sequence accumulating the scores to form inversion excursions.

to facilitate the analysis, we use a parsing program to convert an rna sequence into a binary sequence with the same length. if a nucleotide base is included in an inversion identified by the inversfinder program, it is given a value of "1"; if not, it is assigned a value of " <dig> " as illustrated in figure  <dig>  each "1" in the binary sequence is given a score of  <dig>  and each "0" a negative score of s which is determined as follows: we consider the binary sequence as a realization of a sequence of independent and identically distributed  random variables, x <dig>  x <dig> ..., xn, where n is the length of the rna sequence . these random variables take values of either  <dig> or s. let p = pr and q =  <dig> - p = pr. the parameter p is naturally estimated by the percentage of bases contained in one or more inversions in the rna sequence, i.e., the percentage of "1"s in the binary sequence. we require that the expected score per base μ= p + q * s to be negative. this requirement prevents the tendency of favoring long segments to be high scoring segments. as done in  <cit>  and other applications, the value of s can be conveniently selected by giving μa value of - <dig>  and then determining the value of s according to equation  <dig> 

  s=μ-pq 

the excursion score ei at position i of the sequence is defined recursively as in equations  <dig> and  <dig> 

  e0= <dig> 

  ei=maxfor1≤i≤n 

an excursion starts at a point i where ei is zero, continues with a number of rising and falling stretches of positive values, and ends at j > i where j is the next position with ej =  <dig>  the score then stays at zero until it becomes positive again when the next excursion begins. plotting the excursion scores along the nucleotide positions of the rna sequence offers an effective visualization of how inversion concentrations vary along the sequence. this plot can serve as a guide for choosing the cutting points for the segmentation process. figure  <dig> shows an example of an excursion plot. note that rising stretches in the plot indicate the presence of inversions.

after generating the excursion plot, we identify the positions, called peaks, where the excursion scores are local maxima. then, the bottom of each peak, which is the last position with a zero excursion score right before the peak, is located. after that, the length of the peak  is calculated. note that since we require chunk lengths to be smaller than a prescribed maximum c, peak lengths greater than c have to be flagged and analyzed separately. figure  <dig> also shows examples of peaks, peak bottoms, and peak lengths. peaks are sorted in decreasing order based on their excursion scores. the sorted peaks are then used to cut sequences in chunks by the centered and optimized chunking methods.

centered chunking method
the centered method cuts the sequence by identifying inversions and building the chunks around them. the objective is to segment the rna sequence in such a way as to avoid losing structural information as much as possible by centering the longest spanning inversion clusters in the chunks. after peaks are identified, they are sorted in decreasing order of their excursion values. the peak with the highest excursion value is considered first, then the second highest peak is considered, and so on. the algorithm stops either when all the peaks are exhausted or when all the inversion regions of the sequence  have been included in the chunks, whichever occurs first. overlapping chunks are adjusted so that any nucleotide base is captured by only one chunk, with priority given to the peak with a higher excursion score.

for each of the selected peaks, the positions of the inversions or peak length positions are centered within the maximum chunk-length of c bases where c is defined by the user. we start at the bottom of this peak and follow the excursion until it returns to  <dig> the very next time and locate the position of the very last peak before the excursion returns to  <dig>  we take the sequence segment between the peak bottom and the position of the very last peak and place the sequence segment in the center of the chunk as illustrated in figure  <dig>  suppose this centered segment contains x nucleotide bases. if  is even, then the resulting chunk will have / <dig> bases on each side of the centered segment. if  is odd, then we will adjust the lengths on each side to the integers below and above / <dig>  allowing one side  to have one more nucleotide base than the other.

as an example, we applied the aforementioned method to an rna sequence, that is, the 379-base rna sequence rf00209_a in the rfam database  <cit> . as shown in figure  <dig>  the sequence is segmented into six chunks using the centered chunking method. these six segments cover the entire sequence. labels  <dig> through  <dig> in figure  <dig> represent the six segments with decreasing order of peak excursion scores. after the peak scores are sorted, the peak with the highest excursion score is considered first. in this example, we use the maximum chunk-length c =  <dig>  the highest peak is found at position  <dig> with peak bottom at  <dig>  as there are other inversions after the highest scoring peak, we follow the entire excursion to the end at position  <dig>  after locating the last peak in this excursion at  <dig>  we center the sequence segment from  <dig> to  <dig> to produce the chunk covering the  <dig> positions from  <dig> to  <dig>  then the second highest scoring peak at position  <dig> is considered and the above procedure is repeated. this time, the peak bottom is at position  <dig> and the last peak before the end of this excursion is at position  <dig>  centering the segment consisting of positions  <dig> -  <dig> in a chunk of  <dig> would require  <dig> positions on each side, extending the chunk beyond the beginning of the sequence; we therefore adjust the chunk to start at position  <dig> instead. note that during the segmentation process, we might get a chunk that overlaps with previously established chunks. in those cases, we have to reconcile the situation by reducing one of the chunk lengths. for example, after establishing the first two chunks , the next highest peak to be processed is at position  <dig>  with peak bottom at position  <dig>  centering this peak produces a chunk from positions  <dig> to  <dig>  overlapping with chunk  <dig>  we resolve such conflicts by giving priority to the chunk with the higher number of bases within completely contained inversions. with this rule, we give priority to chunk  <dig>  and reduce chunk  <dig> to positions  <dig> -  <dig>  the process continues for the remaining chunks  <dig> -  <dig> 

optimized chunking method
in the optimized method, cutting points are decided by choosing a segment containing the peak in an optimal position that yields the highest inversion scores for the segment. the score is defined as the total number of nucleotide bases contained in the inversions that are entirely within the chunk. for example, consider a peak with peak length spanning the nucleotide bases between i and j and then all the chunks of size c covering this peak, that is, all segments with length c between positions j -  and i +  are considered . the chunk with the maximum inversion score is then selected. beginning with the highest peak, the above process is repeated until either all the peaks are utilized or all the inversions of the sequence are contained in established chunks, whichever occurs first. when chunks overlap, the cutting points are adjusted in a similar way as described for the centered method. the optimized method ensures that peak length positions are included within a chunk but not necessarily in the center of the chunk.

as an example, we applied the optimized method to the same rf00209_a rna sequence file from the rfam database, as shown in figure  <dig>  the optimized method produced only  <dig> chunks covering all except the first  <dig> positions of the sequence. it can be seen from figure  <dig> that this method avoids cutting into those sequence segments with rising excursion scores preceding the peaks. also, the chunks produced by the optimized method cover only  <dig> % of the sequence, leaving out those parts of the sequence where no inversions are found; therefore, the wasting of computing resources is minimal in the optimized method.

regular chunking method
the regular chunking method is the simplest method of segmentation and is used as a reference method in this paper. this method cuts the nucleotide sequence regularly into chunks of a specified maximum chunk-length c until the sequence is exhausted.

for example, with c =  <dig>  the sequence rf00209_a from the rfam database with  <dig> bases will be cut into four chunks made up of nucleotide positions  <dig> -  <dig>   <dig> -  <dig>   <dig> -  <dig>  and  <dig> -  <dig> . obviously, rising stretches in an excursion plot, which indicate the presence of inversions and are likely to be part of secondary structures, can often be cut by this method. as a result, it is relatively easy to lose important structural information. intuitively, one expects that both the centered and optimized methods, which take the inversion locations into account when placing the chunks, perform better in retaining the secondary structure information in the sequences.

prediction based on well-known algorithms
after the rna sequence is cut into chunks, the structure of each chunk is predicted independently using well-known algorithms and their programs. we use the same prediction algorithms to predict the entire sequence without chunking. we employ seven commonly used prediction programs to test the chunking methods. the programs that predict structures only for non-pseudoknotted sequences are unafold  and rnafold . the programs that predict both pseudoknotted and non-pseudoknotted sequences are ipknot , pknotsrg , hotknots , nupack , and pknots. these prediction programs, which typically involve some form of minimization of free energy, maximization of expected accuracy, or dynamic programming models in their algorithms, are all publicly available.

reconstruction based on concatenation
the results of the chunk predictions are assembled to build a whole secondary structure. currently, our framework simply concatenates all these predicted secondary structures to give the secondary structure for the whole sequence. this is possible because the cutting does not allow any overlap between two consecutive chunks. more sophisticated reconstruction methods that include partial chunk overlaps can be used with minor changes to our framework.

accuracy analysis based on comparisons with known structures
both the whole and the assembled predicted structures are compared to the known structure to obtain their respective prediction accuracies so that we can assess to what degree the chunking method can preserve the prediction accuracy of the program when applied without any segmentation. figure  <dig> shows the rf00209_a nucleotide sequence along with the bracket view of its experimentally known secondary structure. in the bracket view representation, bases that are hydrogen bonded with other bases are represented by a "", and a matching pair of "" indicates that the bases at those positions are paired to be part of a secondary structure. unpaired nucleotide bases are represented by a ":" .

various statistical tests are applied to the accuracy analysis for the different chunking methods including t-tests, pearson correlation analysis, and the non-parametric friedman tests. we use the statistical functions provided by matlab  <cit> . metrics of interests include:  accuracy chunking , which is the accuracy of the predicted structure assembled from the chunks when compared with the known secondary structure;  accuracy whole , which is the accuracy of the predicted structure obtained from the whole sequence when compared with the known secondary structure; and  accuracy retention , which is the ratio between ac and aw. while ac and aw reflect accuracies of the particular prediction in use with and without chunking, ar tells us how well a particular chunking method retains the accuracy of the original prediction program.

ac and aw are given by the percentage agreement of the predicted structure with the known real structure calculated as:

  100*n 

where a and b represent respectively the number of unpaired bases and the number of base pairs in common between the two structures, and n is the length of the rna sequence. large ac and aw values  for a predicted structure mean that it is highly similar to the real structure.

the accuracy retention  is defined as:

  ar=acaw 

ar provides a comparison of the prediction accuracies with chunking versus without chunking. intuitively, we expect that a good chunking method would cause only a minimal loss of prediction accuracy after cutting the sequence and would have ar values somewhat less than but close to  <dig>  however, we will see in the result section that in many cases the ar values turn out to be greater than  <dig>  meaning that secondary structure predicted using chunking is more similar to the real structure than it is the secondary structure predicted by using the whole sequence. several standard statistical tests, including t-tests, pearson correlation analysis, and the non-parametric friedman tests  <cit> , are applied to analyze the ar values for the different chunking methods.

adapting multiple searching paths to mapreduce
given an rna sequence, the search for the best set of chunking parameters  requires us to traverse or search a multi-level tree . in the chunking tree, each path from the root  to the leaves  represents a set of parameter values of the chunking method . the overall workflow  naturally adapts to fit into the mapreduce  paradigm and can be easily implemented with hadoop for which the chunking and predictions can be solved by multiple mappers while the reconstruction and the analysis are done by a single reducer. in our framework, each mr job is designed to partially traverse the multi-level tree. multiple mr jobs can be executed in parallel to explore the whole tree. the multiple searching paths combine attributes of both breadth-first search  and depth-first search . while traversing the tree with multiple mr jobs, we can explore the impact of different chunking methods as well as different c, l and g values for a given sequence. an example of an mr job is shown in the circled part of figure  <dig> b, for which we assume the centered chunking method, with fixed c =  <dig> bases, and we vary l and g between  <dig> and  <dig> and between  <dig> and  <dig> respectively. as previously outlined, for a sequence and a combination of parameters, the mappers perform the chunking and predictions. the input to each mapper is a 〈k <dig>  v1〉 value pair, in which k <dig> is the id of the sequence, and v <dig> is the chunking parameters' values . each mapper cuts the sequence according to the chunking parameters values in the chunking step by identifying a variable number of chunks meeting the parameter requirements. note that each combination of parameters  can result in a variable number of chunks. each mapper performs the prediction on one or more chunks using a certain prediction program. here we use five secondary structure prediction programs capable of predicting pseudoknots  and two programs that do not include this capability . other programs can be easily used in our framework as a plug-and-play software module. after the prediction, each mapper outputs the list of 〈k <dig>  v2〉 pairs as the intermediate output to reduce. the k <dig> is the id of the whole secondary structure to which the predicted chunk belongs and v <dig> is the predicted secondary structure of the chunk. after the hadoop runtime system groups all the values associated with the same key and passes the 〈k <dig>  list〉 to the reducer, the reducer reconstructs the whole secondary structure of the sequence using all the v <dig>  associated with the same k <dig>  if required, the reducer analyzes the results in terms of their accuracy. after the accuracy has been computed, the reducer outputs the final results as a list, in which v <dig> is the ar for reconstructed structures.

granularity of mappers
in general, a mapper is the process that runs on a processor which applies the map function to a specific key and value pair. in our framework, each mapper runs the chunking process on an rna sequence with a given set of parameter values and then predicts one or multiple chunks. the granularity of the mapping can vary based on the number of chunks each mapper is assigned to predict. our mr framework includes both a coarse-grained mapping and a fine-grained mapping as shown in figure  <dig>  in which each box represents a mapper. with the coarse-grained mapping, each mapper explores one branch of the chunking tree: it cuts the sequence into a set of segments based on a combination of l and g values and predicts all the segments it generates locally in order. with the fine-grained mapping, multiple mappers explore one branch of the chunking tree: each mapper cuts the same sequence into the same set of segments, but this time it predicts only one chunk that it generates. this means that if, for example, the sequence is cut into five segments, then there will be five mappers exploring the same branch of the chunking tree, replicating the chunking process but predicting only one distinguished segment of the five chunks available. the mappers determine which segment to predict based on a hash function; thus the mappers do not need to synchronize their work or directly agree on what chunk to predict. the hash function uses the ascii value of the chunk identifier as the key and the identifier of each mapper as the value. the function selects the segments to mappers in a round robin fashion.

RESULTS
datasets and hardware platform
for the study of both accuracy and performance, we plug seven rna secondary structure prediction programs into our framework for both the chunk-based predictions and the predictions of the same sequences without chunking . five of the programs, ipknot  <cit> , pknotsrg  <cit> , hotknots  <cit> , nupack  <cit> , pknots  <cit>  can predict both stem-loops and pseudoknots. the remaining two programs, unafold  <cit>  and rnafold  <cit> , can predict stem-loops only. we consider both the centered  and optimized  chunking methods and compare them against the naïve regular method  as a reference. we also consider a wide range of parameter settings with maximum chunk length c from  <dig> to  <dig> bases, minimum stem length l from  <dig> to  <dig>  and maximum gap length g from  <dig> to  <dig> 

to study the framework accuracy, we use two datasets of sequences which have previously established secondary structures. the first dataset, compiled from the rfam database, consists of  <dig> non-pseudoknotted sequences and the lengths of sequences range from  <dig> to  <dig> bases. the second dataset, compiled from the rfam and pseudobase++  <cit>  databases, consists of  <dig> pseudoknotted sequences, and the lengths of the sequences in this dataset range from  <dig> to  <dig> bases. note that there are no large datasets of experimentally determined rna secondary structures including pseudoknots, and to the best of our knowledge the one used in this paper is one of the few available to the public for free.

to study the framework performance, we use a smaller dataset of longer sequences  for which the secondary structures are not known. we assume pseudo-knots may be present and use the above-mentioned five prediction programs that are capable of capturing pseudoknots and we report only performance values but not accuracy. because these rna sequences are long  and contain possible pseudoknots, none of the available programs can predict the secondary structures for the entire sequences. the use of the mapreduce framework is vital for the exhaustive, efficient exploration of the tree branches.

we ran the mapreduce framework on a cluster composed of  <dig> dual quad-core compute nodes , each with two intel xeon  <dig>  ghz quad-core processors. a front-end node is connected to the compute nodes and is used for compilation and job submissions. a high-speed ddr infiniband interconnect for application and i/o traffic and a gigabit ethernet interconnect for management traffic connects the compute and front-end nodes. our implementation is based on hadoop  <dig> . <dig> 

accuracy
there are three main questions that we want to answer in regard to the effects of our chunk-based approaches on the accuracy of various established secondary structure prediction programs. first, we want to evaluate to what extent chunk-based predictions retain the prediction accuracy. second, we want to identify whether the capability of a chunking method to retain the prediction accuracy might decline with increasing sequence lengths. third, we want to assess the extent to which the inversion based chunking methods  outperform the naïve chunking method , and whether there is any difference in accuracy between the c and o chunking methods.

to assess how well the predictions based on chunking agree with known rna structures, we measure the maximum ac  values of the sequences in the two datasets. figures  <dig> and  <dig> present the box-and-whisker diagram for the two datasets and the three chunking methods - i.e., figures  <dig> a,b, and  <dig> c show the box-and-whisker diagram for the regular, centered, and optimized methods respectively for the dataset of  <dig> non-pseudoknotted sequences. figures  <dig> a,b, and  <dig> c show the box-and-whisker diagram for the regular, centered, and optimized methods respectively for the dataset of  <dig> pseudoknotted sequences. in the figures, the lower and upper quartiles are at the top and bottom boundaries of the box for the kernels; the median is the band inside the box; the mean is the black square; the whiskers extend to the most extreme data points or outliers; and outliers are plotted individually as "+" symbols.

as described in the method section, the ac value for a predicted rna structure is the percentage of agreement between the known structure and the structure obtained by concatenating the predicted structures of the chunks. likewise, the aw value is the percentage of agreement between the known structure and the predicted structure when the whole sequence is used. these values indicate how closely the predicted structure resembles the real structure. a larger ac value means that the chunk-based predicted structure is more similar to the real structure. for a given dataset, prediction program, and chunking method, our mr framework collects multiple predicted structures associated with different c, l, and g parameters. the mac value for a sequence is the maximum ac value, which gives the highest accuracy that can be attained for that sequence by the chunking method and the specific prediction program employed. in figures  <dig> d and  <dig> d, the aw of the sequences in the two datasets are presented respectively. from these figures, it appears that most of the prediction methods have similar accuracy ranges regardless of the chunking method used and whether the prediction was obtained with the whole sequence or with the chunks; however, the pknots program produces somewhat lower accuracies. this lower accuracy is quite expected because pknots is actually the earliest algorithm allowing for pseudoknot prediction. the other prediction programs with pseudoknot prediction capability that have developed afterwards have incorporated improvements over the original pknots.

from figures  <dig> and  <dig>  the prediction accuracies with chunking  - ) appear to be higher than those without ), suggesting that the prediction accuracy, on average, can be enhanced by sequence segmentation. to get a clearer characterization of the effect of sequence segmentation, we carry out statistical tests on the maximum accuracy retention  obtained for each rna sequence over the c, l, and g parameters. in the majority of the sequences in our dataset, the mar turns out to be greater than  <dig>  with a one-sample t-test, we test whether the mean mar is significantly greater than  <dig> with p-value >  <dig> . tables  <dig> and  <dig> display the means, standard deviations, and p-values for the non-pseudoknotted and pseudoknotted sequences respectively.

mean and standard deviations of mar for regular, centered, and optimized chunking methods over  <dig> non-pseudoknotted sequences, and the corresponding p-values of the t-test for mean mar > <dig> 

mean and standard deviations of mar for regular, centered, and optimized chunking methods over  <dig> pseudoknotted sequences, and the corresponding p-values of the t-test for mean mar > <dig> 

for non-pseudoknotted sequences, the mean mar is significantly greater than  <dig> for all three chunking methods, whereas the mean mar values for the pseudoknotted sequences are greater than  <dig> for the c and o chunking methods. with the r chunking method, one of the mean mar values  falls below  <dig> to  <dig> . looking at all the p-values, one can conclude that the average prediction accuracy attained with segmentation is not significantly less than that without. with the inversion based c and o chunking methods, we can conclude that the average prediction accuracies attained with segmentation are at least as good as, and often even better than, those without segmentation.

while the above results show that sequence segmentation will not reduce prediction accuracy on average, we still need to examine whether the mar values would decline as the whole sequence length grows, because a declining trend would imply that the accuracy retention will deteriorate when the segmentation approaches are applied to longer rna sequences. to this end, for each dataset, chunking method, and prediction program, we perform the pearson correlation analysis on the mar values of the sequences  <cit> . for each dataset, we report both the correlation coefficient r and corresponding p-value between mar and sequence length. if the r value is close to - <dig>  it means that mar and sequence lengths are negatively correlated, implying a decline in accuracy retention of the chunking method. if the associated p value is less than  <dig> , we consider the correlation statistically significant; otherwise the correlation is not significant.

correlation coefficients  between mar and sequence lengths and corresponding p-values  when testing for a negative correlation.

given the three chunking methods considered  we also want to determine which among them is better at retaining the accuracies of the various prediction programs. for this purpose, we examine each sequence in our two datasets and keep track of which chunking method produces the highest mar. table  <dig> gives the total counts of sequences attaining the highest mar for each of the chunking methods. if more than one chunking method gets the same highest mar for one sequence, we split the count of this sequence equally among the methods. we can see that the sequence counts in table  <dig> for the centered and optimized  methods are higher than those for the regular method .

count  and rank sum  of sequences attaining the highest mar with each chunking method for the various prediction programs.

to see if there are any differences among the accuracy retention capabilities among the three cutting methods, we perform the friedman test for each dataset and each prediction program. the friedman test is a non-parametric statistical test based on rank sums  <cit>  and requires ranking the mar attained by each chunking method for each prediction program and each sequence in our data sets. the method producing the lowest mar is given a rank of  <dig> and the method producing the highest mar is given a rank of  <dig>  again, the ranks are averaged for ties. table  <dig> shows the p-values of the friedman tests in the "r-c-o" columns. from these very low p-values, we can conclude that there are significant differences among the three methods.

p-values from the friedman test to compare the accuracy retention of the three chunking methods as well as the posthoc pairwise comparison tests.

because the friedman test does not reveal whether any one method is significantly better than another, we also perform the post-hoc pairwise comparison test on each pair of the three chunking methods in order to confirm that the inversion based centered and optimized chunking methods are indeed superior to the naïve regular method. the p-values, shown in the "r-c," "r-o," and "c-o" columns, indicate that both the centered and optimized methods are better than the regular method. furthermore, there are no significant differences between the centered and optimized chunking methods except when pknots is applied to the pseudoknotted sequences.

the results above demonstrate that, for a variety of secondary structure prediction programs, our segmentation approach for handling the long rna sequences can retain and even enhance the average prediction accuracy. furthermore, using the inversion based c and o methods to cut the sequence will produce better prediction accuracy than the naïve r method. more questions remain to be answered and are part of our current research.

our current investigations focus on the following two questions. first, we want to study how we should choose the parameters c, l, and g to maximize the accuracy retention. we have been conducting studies to identify how the prediction accuracy correlates with these parameters. some of the results have been reported in preliminary work of the group  <cit> . so far we have not found any definitive criteria that work for all sequences in general. rather, the nucleotide base composition and length of the individual sequence, as well as the sequence length limitations imposed by the particular prediction program, need to be taken into account. second, the fact that segmentation can in many cases improve the prediction accuracy for an rna sequence is somewhat counter-intuitive. one possible explanation is that secondary structure prediction algorithms are generally based on global minimal free energy, resulting in the most thermodynamically stable isoforms. however, these structures may not be most favorable for biological functions, which often require rnas to interact with other molecules or unfold during replication. our results suggest that local structures formed by pairings of bases in close proximity, rather than the global energies, may better correlate with the real structures of large rna molecules. this hypothesis is being tested in coauthor johnson's molecular virology lab using the virus family nodaviridae.

the above idea also prompted us to initiate a study on the correlation between accuracy and the free energy of our chunk-based predicted structures. since there are no straightforward mechanisms within the current prediction programs to compute the free energy of a given structure other than those outputted by the program, we try to obtain the overall free energy of our chunk-based predictions by simply summing the free energies associated with the chunks. among several examples that we have studied to date, most do not show any statistically significant relationship to support the idea that global structures with lower free energies are more similar to the known structure. one example scatter plot of the prediction accuracy versus free energy of different predicted structures of the sequence rf000_2a using the centered and optimized chunking methods with different l and g parameters with maximum chunk length of  <dig> is shown in figure  <dig>  the correlation coefficient is found to be positive  <dig> , which is against the expectation of a negative correlation. we anticipate that this line of investigation will require more coordinated efforts with the developers of the various prediction programs to establish appropriate ways of computing the free energies of any given predicted-or experimentally-determined structure.

performance
for the performance analysis, we use a smaller dataset of longer sequences from the virus family nodaviridae  <cit>  and we explore a wider range of parameter values. the virus family nodaviridae is divided into two genera: alphanodaviruses that primarily infect insects and betanodaviruses that infect only fish. these viruses share a common genome organization, namely a bipartite positive strand rna genome . the longer genome segment rna <dig>  encodes the rna-dependent rna polymerase that catalyzes replication of both genome segments, while the shorter rna  <dig>  encodes the precursor of the viral capsid protein that encapsidates the rna genome. the  <dig> sequences we analyze in this paper are identified as follows: boolarra virus  rna <dig> , pariacoto virus  rna <dig> , nodamura virus  rna <dig> , black beetle virus  rna <dig> , flock house virus  rna <dig> , striped jack nervous necrosis virus  rna <dig> , epinephelus tauvina nervous necrosis virus  rna <dig> , bov rna <dig> , pav rna <dig> , bbv rna <dig> , etnnv rna <dig> , fhv rna <dig> , sjnnv rna <dig> , nov rna <dig> . these sequences are sorted based on their increasing lengths, and this order is preserved in all the figures and tables presented below. there are three important questions that we want to answer when measuring performance. first, we want to quantify the time spent for exploring the several branches of the search trees for these  <dig> sequences using each of the two chunking methods  and for the granularity of the mapping . second, we want to identify how the time is spent for each search in terms of map, reduce, and data shuffling among processors. third, we want to measure the efficiency of the search and look for those aspects of the search that can impact performance.

we measure the total time needed to explore the chunking tree of each sequence using either the centered or optimized methods and with either coarse-grained or fine-grained mapping. the total time includes the time needed for chunking and prediction , reconstruction , exchange of predictions among nodes , and any overhead due to load imbalance and synchronizations. note that the total time does not include the time needed for analysis since the secondary structures of the sequences considered here are not known experimentally; thus an analysis in terms of accuracy is not feasible. we use ipknot for our predictions since it is the most recently implemented program and its accuracy values are very high in the previous section.

each of the four subfigures in figure  <dig> shows the total times in seconds for exploring the prediction trees  and the number of map tasks  for the  <dig> sequences when a maximum chunk length of  <dig>   <dig>  and  <dig> bases is used. in each subfigure there are three groups of times, one for each maximum chunk length. each group lists the  <dig> sequences sorted based on their length in nucleotide bases. more specifically, figure  <dig> a presents the times and number of map tasks when the coarse-grained mr implementation and the centered method are used; figure  <dig> b presents the times and number of map tasks when the coarse-grained mr implementation and the optimized method are used; figure  <dig> c presents the times and number of map tasks when the fine-grained mr implementation and the centered method are used; and figure  <dig> d presents the times and number of map tasks when the fine-grained mr implementation and the optimized method are used. as already presented above, when using coarse-grained mapping, each mapper performs the chunking for the assigned sequence using a set of parameter values for the max length of stems  and gap sizes . the mapper then predicts the secondary structures of all its local chunks. this results in the exploration of a whole branch of the tree by the mapper. the total number of branches  is given by the combinations of l and g values . when using fine-grained mapping, chunking of a sequence based on a set of l and g values is performed across mappers and mappers are assigned resulting chunks in a round-robin fashion. computationally this is performed by replicating the chunking processes across mappers and by using a hash function to assign different chunks to different mappers. the number of chunks equals the number of map tasks and depends on the number of inversions identified in the chunking process.

when comparing centered vs. optimized chunking methods for the coarse-grained mapping, we observe that the two methods result in similar execution times . table  <dig> quantifies the similarity for both subgroups  which is within 3%. this observation is different from the previous work in which the centered method resulted in shorter execution times due to the fact that a different implementation of the chunking methods and a different program were used.

average total times for the seven sequences in rna <dig> and in rna <dig> for coarse-grained mapping using centered and optimized methods.

when comparing centered vs. optimized chunking methods for the fine-grained mapping, the optimized method results in a slightly lower execution time. as shown in table  <dig>  the execution times of fine-grained mapping when using the optimized chunking method for both subgroups  is 11% to 18% slower than using the centered method. table  <dig> shows the average number of chunks  for both subgroups using centered and optimized methods. we can see that the optimized method results in 10% to 19% less chunks. the optimized method tends to cut sequences into fewer chunks, which leads to fewer map tasks and shorter mapreduce total times. this observation is different from the previous work in which the centered method results in shorter execution times due to the same reason we mentioned above  <cit> .

average total times for the seven sequences in rna <dig> and in rna <dig> for fine-grained mapping using centered and optimized methods.

average number of chunks  for the seven sequences in rna <dig> and in rna <dig> for fine-grained mapping using centered and optimized methods.

when comparing coarse-grained mapping vs. fine-grained mapping, we observe that coarse-grained mapping results in shorter execution time compared to fine-grained mapping, independent of the chunking method used. also we observe the trend that when the maximum chunk length grows from  <dig> to  <dig>  the time gain of coarse-grained mapping over fine-grained mapping decreases. the speedup of coarse-grained mapping over fine-grained mapping using the centered chunking method for rna <dig> subgroup of sequences decreases from  <dig>  to  <dig> , and for rna <dig> it decreases from  <dig>  to  <dig> . a similar behavior is observed for the optimized chunking method: speedup of coarse-grained mapping over fine-grained mapping for rna <dig> subgroup of sequences decreases from  <dig>  to  <dig> , and for rna <dig> it decreases from  <dig>  to  <dig> .

the same trend of the total times is summarized in the box-and-whisker diagram of minimum, median, mean, and maximum execution time for each subgroup of sequences  in figure  <dig>  more specifically, in figure  <dig> a, we show the box-and-whisker diagram of the total times for the rna <dig> subgroup of sequences  using coarse-grained mapping, both centered and optimized methods, and maximum chunk lengths of  <dig> , and  <dig>  in figure  <dig> b, we show a similar box-and-whisker diagram but for the rna <dig> subgroup of sequences . in figure  <dig> c, we show the box-and-whisker diagram of the minimum, mean, and maximum execution time for the rna <dig> subgroup of sequences using fine-grained mapping, centered and optimized methods, and max chunk lengths of  <dig>   <dig>  and  <dig>  in figure  <dig> d, we show a similar box-and-whisker diagram but for the rna <dig> subgroup of sequences. we observe that for coarse-grained mapping, when using the centered and optimized methods, the average total times increase with the maximum chunk length at the rate of  <dig>  for rna <dig> and  <dig>  for rna <dig>  on the contrary for the fine-grained mapping, when using centered and optimized methods, the average total times decrease with the maximum chunk length at the rate of  <dig>  for both subgroup of sequences. this suggests that potentially for larger maximum chunk length and sequence lengths the fine-grained mapping can outperform the coarse-grained mapping in terms of performance.

when decoupling the total time in its components, we observe that the time components for the reduce function and shuffling are very marginal compared to the times used for the mapping functions . we also observe that, as we explore a prediction tree, some mappers are performing more work than others, resulting in idle time and low efficiency. the load imbalance among mappers depends on the granularity and chunking methods used. to better understand the causes of load imbalance we cut down the mapping times into compute time  and idle time . figure  <dig> shows the percentage of compute and idle times in map function for coarse-grained mapping vs. fine-grained mapping as well as for centered vs. optimized methods. more specifically, figures  <dig> a and  <dig> b show the percentages for compute and idle times for the coarse-grained framework with the centered and optimized methods respectively; figures  <dig> c and  <dig> d show the same percentages but for the fine-grained framework and the two chunking methods.

independent of the maximum chunk length, figure  <dig> shows how fine-grained mapping reaches better efficiency compared to coarse-grained mapping. in other words, with fine-grained mapping, the mappers spend more time doing real chunking and predictions. we observe in figure  <dig>  how fine-grained mapping has a larger number of map tasks and each map task is shorter  making easier for the hadoop scheduler to allocate the several tasks efficiently by using a first-in-first-out  policy. on the other hand, coarse-grained mapping has a smaller number of map tasks and each map task is longer . in this case, once the scheduler assigns a longer task to a mapper, it has to wait for its completion, even if the other mappers have generated their chunk predictions, before proceeding to the reduce phase. we also observe that as the maximum chunk length increases from  <dig> to  <dig> bases, the map efficiency tends to drop. more specifically, the average map efficiency for coarse-grained mapping decreases from 36% to 25% on rna <dig> and from 18% to 15% on rna <dig> when using centered or optimized chunking methods. the average map efficiency for fine-grained mapping decreases from 91% to 79% on rna <dig> and from 97% to 93% on rna <dig>  this is due to the fact that the centered and optimized chunking methods tend to produce more chunks with shorter chunk lengths when using a maximum chunk length of  <dig>  on the other hand, when using a maximum chunk  <dig>  the same methods tend to produce fewer chunks each with longer lengths.

diverse chunk lengths within a prediction can also cause inefficiency. to study this phenomenon, we consider the nodamura virus  rna <dig> sequence which shows the largest drop in efficiency when moving from  <dig> to  <dig> max chunk lengths, as shown in figure  <dig>  figures  <dig> and  <dig> show the number of chunks and their lengths  for the different l and g parameter combinations with centered and optimized methods when the maximum chunk length is equal to  <dig>  and when the length is equal to  <dig> . when the maximum length grows from  <dig> to  <dig>  the number of resulting chunks for each combination of l and g parameters decreases. at the same time the length of each set of chunks increases as well as the length variability within the set of chunks for a defined combination of l and g values. note that for some combinations of l and g, the chunking process does not identify any set of chunks and we do not report any result for these cases. this confirms our observation that as the number of chunks decreases, the chunk lengths increase but not homogeneously within a prediction, causing load imbalance and loss in efficiency. selecting the shorter maximum length for the sake of efficiency is not always a wise decision: a maximum chunk length of  <dig> bases may be too short for the type of rna sequences we are considering. in figure  <dig>  the median is very close to the maximum length of  <dig> for the centered methods, indicating that we are cutting out valuable parts of the inversion and ultimately of the secondary structures we are predicting.

the overall results suggest that the best set of parameter values to achieve higher accuracy, performance, and efficiency depend on multiple aspects including the input sequence and the available resources. driven by these two aspects, in future work we will integrate an automatic selection of these values into our mr framework.

CONCLUSIONS
in this paper, we propose a mapreduce-based, modularized framework that allows scientists to systematically and efficiently explore the parametric space associated with chunk-based secondary structure predictions of long rna sequences. by using our framework we can observe how sequence segmentation strategies, directed by inversion distributions enable us to predict the secondary structures of large rna molecules. furthermore, the chunk-based predictions can, on average, attain accuracies even higher than those obtained from predictions using the whole sequence. the observations in this study have led to our hypothesis that local structures formed by pairings of bases in close proximity, rather than the global free energies, may better correlate with the real structures of large rna molecules. this hypothesis will be tested by further computational and experimental investigations.

competing interests
the authors declare that they have no competing interests.

authors' contributions
bz adapted and implemented the chunk-based rna prediction process into mapreduce hadoop, carried out the accuracy and performance experiments, and participated in drafting the manuscript. dy participated in the design, implementation and modification of the sequence segmentation strategies, conducted the statistical analyses of their accuracy retention, and helped to draft the manuscript. klj provided the nodavirus genome sequence data and their biological significance. myl supervised the accuracy analyses and summarized the statistical results. mt oversaw the mapreduce implementation and performance analysis, and coordinated the organization and writing of the manuscript. all authors reviewed and participated in finalizing the manuscript.

