BACKGROUND
the application of mathematical expressions for biochemical systems is useful for understanding complex biological phenomena. reactions are expressed as rate equations, and the numerical integration of variables is applied to simulate these reactions. the simulation of biochemical models requires the identification of all kinetic parameters of each rate equation. however, almost all biochemical models, and large-scale models in particular, involve parameters that are extremely difficult to measure in vivo or in vitro  <cit>  and the exclusive use of results from wet-bench experiments may be insufficient for the identification of all parameters. therefore, to establish precise models, it is essential to determine unknown kinetic parameters from the time-courses of concentrations.

various optimization algorithms such as the levenberg-marquardt method, genetic programming, simulated annealing, and evolutionary algorithms have been applied to infer parameters or equations in biochemical models  <cit> . the genetic algorithm   <cit>  is commonly applied to these problems because of its global optimization ability  <cit> .

however, the ga, including the real-coded ga  <cit> , relies on stochastic optimization algorithms and requires vast numerical integrations to evaluate estimated parameters. in addition, the computational cost increases further when the target model has the stiffness that often appears in equations for biochemical systems because each solution requires small integration steps  <cit> . the application of the stochastic method to large-scale or stiff models involves enormously expensive computational time and superior hardware performance. in addition, use of ga elicits the sampling bias phenomenon  <cit> . this becomes an inherent problem that may result in failure in cases where an optimal parameter exists around the boundary of the parameter space. in particular, parameters of stiff systems tend to be allocated around the boundary of the search space because they involve different-order parameters.

the radial basis function network   <cit> , a type of artificial neural network   <cit> , is one of the artificial learning methods that describe   complex non-linear relationships between inputs and outputs. the rbfn can solve parabolic, hyperbolic, and elliptic partial differential equations numerically  <cit>  and is able to approximate non-linear time-courses effectively  <cit> . due to the increased accumulation of biological information, rbfn are now applied in the biochemical field  <cit> .

in the work presented here, we adopted rbfn to parameter estimation for local and global searches. rbfn enable to reduce the computational cost by omitting numerical integrations, resulting in solution of the above problems. in applying rbfn to biochemical modeling, we implemented subsidiary  <dig> improvements:  since it is difficult to model highly nonlinear biochemical systems, we adopted a logarithmic transformation to both the input and output layer of the rbfn, thereby facilitating parameter-optimization.  rbfn require the selection of appropriate additional learning data to obtain a global minimum. here we propose additional data selection using a ga for selecting additional learning data sets. this method adopts elemental ga to search data-sparse areas in the parameter space. we applied the proposed method to parameter estimation in a stiff biosystem. our results demonstrate that compared to the ga, our method facilitates the acquisition of equivalent or more accurate parameters at half the calculation time and yields a 50% increase in the optimization success rate.

RESULTS
experimental conditions
to examine the performance of the proposed method we used the metabolic pathway model shown in figure  <dig>  it is one of the typically stiff models employed to show the difficult optimization often observed in biochemical modeling  <cit> . the model is composed of  <dig> reactions with  <dig> reactants and  <dig> feedback loop in terms of enzyme concentration. we adopted a stiff model whose parameters differed by about  <dig> orders of magnitude since biochemical models often have stiffness and inference of those parameters is difficult. the kinetic equations and parameters used in the model are presented in tables  <dig> and  <dig>  the concentration of x <dig> is fixed as an external metabolite. figure  <dig> shows calculated time-course data using tables  <dig>   <dig>  and 3; the concentrations of the enzymes are set to  <dig> . from each time-course,  <dig> points were sampled for optimization . table  <dig> shows the experimental conditions in this work. these parameters were selected empirically. the search ranges were ki âˆˆ . the convergence indicates that learning attains a 10% relative error within  <dig> minutes. the processing time is the average of the calculation time required until learning succeeds. the test error is the relative error between given and calculated time-courses using initial conditions shown in table  <dig>  the computer environment was as follows: pentium  <dig>  xeon  <dig> ghz, cpu with a memory size of  <dig> mb. our algorithm was implemented using python language. the 4th-order runge-kutta method was employed as the ordinary differential equation solver; the time step was  <dig> .

performance of adsga
we compared the performance of additional learning using adsga with knn. first, we inferred  <dig> parameters  in the figure  <dig> model to examine the data distribution in the early learning phase. the additional learning of knn and adsga was repeated  <dig> times. a density distribution of learned data is shown in figure  <dig>  cases where k =  <dig>   <dig>   <dig>  and  <dig> were examined. deepening colors represent the density of existing data. for instance, a white grid represents no learning data in the parameter space; a black grid indicates that there are too many learning data in the parameter space. we found that the density distribution varied greatly depending on the value of k. a few grids did not include additional data in cases where k =  <dig>  the grid that includes the answer exhibits the deepest color where k =  <dig>   <dig>  in cases where k was not suitable, a few grids were over-searched while others were not selected appropriately. therefore, knn was not effective for finding the area that included few learned data. in cases where the parameter space is large, the additional data deviation is important because the distribution of the subsequent data influences learning in the early stage. in contrast, as indicated by the small color differences among the grids, adsga succeeded. there were minor deviations in density, indicating that additional learning data were selected from each grid almost equally. we also found that the grids near the correct answer were densely searched. this shows that global and local searches can be performed simultaneously using adsga.

second, we inferred all parameters in the figure  <dig> model. table  <dig> shows that we succeeded in obtaining desirable results in terms of generalization ability using adsga and knn with k =  <dig>  knn with the other k values attained lower achievements compared with our algorithm. although adsga employed ga for determining additional data, the optimization speed was raised compared with knn.

performance of logarithmic transformation
rbfn application to stiff systems
the results obtained with the conventional ga method and our proposed rbfn method are presented side-by-side in the leftmost ga column and   the rightmost adsga+log column in table  <dig>  we employed simplex crossover as the recombination procedure   <cit>   as the recombination procedure and ranking selection as the selection procedure. parameter transformation applies to all cases. parameters for ga are shown in table  <dig>  these parameters were selected empirically.

the calculation time was reduced to less than half compared with ga. the  <dig> -fold and  <dig> -fold increases in the optimization speed are ascribable to rbfn and the combination of rbfn and the logarithmic transformation, respectively. rbfn learning omit the calculation of numerical integration because it learns the relationship between a parameter and its evaluation space. when problems are examined that require a long calculation time for each integration, the difference in the processing time between rbfn and ga increases further.

the convergence rate of rbfn was raised from 60% to 90% compared with that of ga. the rbfn application, not  <dig> subsidiary improvements, achieved to raise the convergence rate. ref  <cit>  reported a peculiar problem that pertains to the sampling bias phenomenon when the optimal solution exists at the edge of a parameter space. ga was unable to find the optimal solution because some parameters existed at the edge of a parameter space. as rbfn are only biased to the area near the estimated optimal solution, the optimal solution can be searched regardless of its location in a parameter space. the test error of rbfn did not decrease compared with that of ga. rbfn, which employ logarithmic transformation of fitness, are slightly inferior to ga in terms of local searches, resulting in a decline in the generalization ability compared with ga. if the logarithmic transformation is not applied in rbfn, the generalization ability of the  <dig> algorithms is equivalent and high convergence and fast optimization are retained.

discussion
ga involves the sampling bias phenomenon because the search region of unimodal normal-distribution crossover  <cit> , parent-centric crossover  <cit> , or spx is biased to the interior of a parameter space. the phenomenon is bound to occur more frequently as the parameters become larger. to solve this problem, extrapolation-directed crossover  <cit>  has been proposed; it is biased toward extrapolative crossover. in addition, boundary extension by mirroring  <cit>  and toroidal search space conversion  <cit>  have been proposed; they extend the search space arbitrarily. however, these techniques raise computational costs rather than conquering the sampling bias phenomenon.

the performance of rbfn was superior to that of ga in the parameter inference of stiff systems since it enables to reduce the number of numerical integrations.

for further improvement, the optimization algorithm is switched to the gradient method which is deterministic and can search local minima quickly at the end of the learning stage. a decrease in test error can be expected when a combination of algorithms is applied. it is considered that the additional improvement by the numerical integration methods such as gear algorithm  <cit>  is harmonious with our parameter estimation algorithm. the hybrid method is also one of tasks to be confirmed in the future. in this paper, we tested the case of stiff equations, and it is considered that non-stiff equations could also show the advantage of our method because of the simple dynamics.

we obtained solutions with low generalization ability when only a few time-course sets were given as input data. predicted solutions with high generalization ability are likely to be biologically meaningful parameters. careful selection of the number of sampling points  <cit>  and application of the data smoothing method  <cit>  are necessary when our method is applied to real problems because the most appropriate combination with our method is dependent on the data and the system in question.

rbfn are optimized by adding a new function o and then adding a new data set o, where p is the number of learned data and m is the number of the kernel function. the inference of numerous parameters causes a serious problem. biological restriction as well as the deletion of several learning data in overcrowded areas can lead to a reduction in computational costs. in addition, problem decomposition strategies  <cit>  are useful to reduce the number of parameters that should be inferred simultaneously.

CONCLUSIONS
we applied rbfn to the parameter estimation of biochemical models, especially stiff systems. rbfn method could reduce the number of numerical integrations. especially, it leaded to the fast optimization in stiff systems of biochmical models. it yielded equivalent or more accurate results compared to the parameters obtained with ga at half the calculation time and a 50% increase in the optimization success rate. also we adopted  <dig> secondary learning techniques to rbfn. one is the fitness transformation that changes a fitness function into a gently sloping function and reduces the calculation time  <dig> -fold. the other is the new selection method of learning data, adsga. adsga was able to quickly obtain optimal solutions with high generalization ability, almost equivalent to knn with optimal k. in this paper, it was shown that our rbfn technique attained the parameter optimization for stiff biochemical models from measured time-courses only.

