BACKGROUND
mathematical modelling is the central element in quantitative approaches to molecular and cell biology. the possible uses of quantitative modelling of cellular processes go far beyond explanatory and predictive studies  <cit> . they provide a way to understand complex bio-systems  <cit>  and have given rise to systems biology as a new way of thinking in biological research  <cit> . models in systems biology vary in their degree of network complexity and accuracy of representation  <cit> . dynamic  models offer the greatest degree of flexibility and accuracy to explain how physiological properties arise from the underlying complex biochemical phenomena. in fact, it has been argued that the central dogma of systems biology is that it is system dynamics that gives rise to the functioning and function of cells  <cit> .

the use of kinetic models to understand the function of biological systems has already been successfully illustrated in many biological systems, including signalling, metabolic and genetic regulatory networks . further, dynamic model-based approaches have also been used to identify possible ways of intervention or design, such as in optimal experimental design , metabolic engineering  <cit>  and synthetic biology  <cit> . other recent efforts have been focused on scaling-up, i.e. on the development and exploitation of large-scale  kinetic models  <cit> , and ultimately, whole-cell models  <cit> .

although nonlinear dynamical models have become the most common approach in systems biology, they have received relatively little attention in the statistical literature, especially when compared with other model types  <cit> . as a consequence, the area can be regarded as one of the most fertile fields for modern statistics  <cit> : it offers many opportunities, but also many important challenges  <cit> .

one of the main challenges is the calibration of these dynamic models, also known as the parameter estimation problem. parameter estimation aims to find the unknown parameters of the model which give the best fit to a set of experimental data. parameter estimation belongs to the class of so called inverse problems  <cit> , where it is important to include both a priori  and a posteriori  parameter identifiability studies. in this way, parameters which cannot be measured directly will be determined in order to ensure the best fit of the model with the experimental results. this will be done by globally minimizing an objective function which measures the quality of the fit. this problem has received considerable attention, as reviewed in . it is also frequently described as the inverse problem, i.e., the inverse of model simulation from known parameters, considered the direct problem.

this inverse problem usually considers a cost function to be optimized , which in the case of nonlinear dynamic models must be solved numerically. numerical data fitting in dynamical systems is a non-trivial endeavour, full of pitfalls . the inverse problem is certainly not exclusive of systems biology: it has been extensively studied in other areas, as reviewed in  <cit> , each one contributing with somewhat different perspectives regarding the difficulties encountered and how to surmount them.

here we would like to address two key pathological characteristics of the inverse problem which make it very hard: ill-conditioning and nonconvexity  <cit> . these concepts are intimately related with other similar notions developed independently in different communities  <cit> . for example, ill-conditioning can be related to the lack of identifiability arising from the model structure, and/or from information-poor data. nonconvexity and multi-modality usually cause convergence to local solutions , which are estimation artefacts. both are significant sources of concern that need to be properly addressed.

due to the nonconvexity of the parameter estimation problem, there is a need for suitable global optimization methods . relying on standard local optimization methods can lead to local solutions, producing wrong conclusions: for example, one can incorrectly conclude that a novel kinetic mechanism is wrong because we are not able to obtain a good fit to the data, but the real reason might be that the method used simply converged to a local minima  <cit> . indeed, a number of studies have described the landscape of the cost functions being minimized as rugged, with multiple minima  <cit> . it has been argued  <cit>  that local methods or multi-start local methods can be effective if properly used, but in our experience  this only holds for relatively well-behaved problems, i.e. those with good initial guesses and tight bounds on the parameters. therefore, in general, global optimization methods should be used in order to minimize the possibility of convergence to local solutions  <cit> .

the ill-conditioning of these problems typically arise from  models with large number of parameters ,  experimental data scarcity and  significant measurement errors  <cit> . as a consequence, we often obtain overfitting of such kinetic models, i.e. calibrated models with reasonable fits to the available data but poor capability for generalization . in this situation, we are over-training the model such as we fit the noise instead of the signal. therefore, overfitting damages the predictive value of the calibrated model since it will not be able to generalize well in situations different from those considered in the calibration data set. overfitting might be behind most failures in model-based prediction and forecasting methods in many fields of science and engineering, and it has probably not received as much attention as it deserves .

most mechanistic dynamic models in systems biology are, in principle, prone to overfitting: either they are severely over-parametrized, or calibrated with information-poor data, or both. however it is quite rare to find studies where a calibrated model is tested with a new data set for cross-validation . further, as we will show below, over-parametrization and lack of information are not the only factors to induce overfitting: model flexibility plays an equally important role.

the paper is structured as follows. first, we consider the statement of the inverse problem associated to kinetic models of biological systems, and we focus on its ill-conditioning and nonconvexity, reviewing the state of the art. we then present a strategy to analyse and surmount these difficulties. in the case of nonconvexity, we present a suitable global optimization method. in the case of ill-conditioning and overfitting, we consider the use of regularization techniques. our strategy is then illustrated with a set of seven case studies of increasing complexity, followed by a detailed discussion of the results. finally, we present practical guidelines for applying this strategy considering several scenarios of increasing difficulty.

methods
parameter estimation in dynamic models
mathematical model
here we will consider dynamic models of biological systems described by general nonlinear differential equations. a common case is that of kinetic models. for the case of biochemical reaction networks, and under the assumption of well-mixed compartments and large enough concentration of molecules , kinetic models describe the concentration dynamics using nonlinear deterministic ordinary differential equations. one of the most general form of these equations is given by the deterministic state-space model: 
  dxdt=f,u,θ), 

  y=g,θ), 

  x=x <dig> t∈, 

where x∈rnx is the state vector , the f:r1×nx×nu×nθ↦rnx vector function is constructed from the reaction rate functions and stimuli u. the nθ dimensional parameter vector θ contains the positive parameters of the reaction rate functions–for example the reaction rate coefficients, hill exponents, dissociation constants, etc.–, but can also include the initial conditions. the observation function g:rnx×nθ↦rny maps the state variables to the vector of observable quantities y∈rny, these are the signals that can be measured in the experiments. the observation functions may also directly depend on estimated parameters for example on scaling parameters. when multiple experiments in different experimental conditions are considered, typically the same model structure is assumed, but the initial conditions and stimuli are adapted to the new conditions.

calibration data, error models and cost functions
we assume, that the data is collected in multiple experiments at discrete time points ti∈, thus the model outputs must be discretized accordingly. let us denote the model prediction at time ti, of the j-th observed quantity in the k-th experiment by yijk. due to measurement errors the true signal value is unknown and a noise model is used to express the connection between the true value yijk and measured data y~ijk.

in general, the type and magnitude of the measurement error depend on both the experimental techniques and the post-processing of the data. for example, blotting techniques are generally used to obtain quantitative data for gene expression levels or protein abundance. these data is assumed to contaminated by either additive, normally distributed random error  or by multiplicative, log-normally distributed noise. rocke and durbin  <cit>  concluded that the gene expression data measured by dna micro-arrays or oligonucleotic arrays contains both additive and multiplicative error components. similar conclusions were reported by kreutz and co-authors  <cit>  for protein measurements using immunoblotting techniques. in this context, there are both experimentation techniques  and mathematical procedures  to ensure proper data pre-processing for model calibration.

maximum likelihood and cost function
assuming that the transformed measurements  are contaminated by additive normally distributed uncorrelated random measurement errors –i.e. y~ijk=yijk,θ)+εijk where εijk∼n <dig> σijk <dig> is the random error with standard deviation σijk and y~ijk is the measured value–, the estimation of the model parameters is formulated as the maximization of the likelihood of the data  <cit>  
  ℒ=∏k=1ne∏j=1ny,k∏i=1nt,k,j12πσijk2×exp− <dig> θ)−y~ijk)2σijk <dig>  

where ne is the number of experiments, ny,k is the number of observed compounds in the k-th experiment, and nt,k,j is the number of measurement time points of the j-th observed quantity in the k-th experiment. the total number of data points is nd=∑k=1ne∑j=1ny,k∑i=1nt,k,j <dig>  the maximization of the likelihood function  is equivalent to the minimization of the weighted least squares cost function  <cit>  
  qls=∑k=1ne∑j=1ny,k∑i=1nt,k,jyijk,θ)−y~ijkσijk2=rtr, 

where the residual vector r:rnθ→rnd is constructed from the squared terms by arranging them to a vector. with this, the model calibration problem can be stated as the well-known nonlinear least-squares  optimization problem: 
  minimizeθqls=rtrsubject toθmin≤θ≤θmax,dxdt=f,x,θ),y=g,θ),x=x <dig> t∈. 

a θ^ vector that solves this optimization problem is called the optimal parameter vector, or the maximum likelihood estimate of the model parameters. however, note that the uniqueness of the solution is not guaranteed, which results in the ill-posedness of the calibration problem, as discussed later.

post-analysis
post-analysis of calibrated models is an important step of the model calibration procedure. classical methods to diagnose the identifiability and validity of models, and the significance and determinability of their parameters are described in e.g.  <cit> . most of these methods, such as the χ <dig> goodness of fit test, or the distribution and correlation analysis of the residuals by, for example, the shapiro-wilk test of normality, assume that the errors follow a normal distribution, so they should be used carefully . similarly, the computation of the covariance and correlation of the parameters  and the computation of confidence regions of the model predictions  <cit>  are usually performed based on the fisher information matrix . but the fim has important limitations, especially for nonlinear models: it will only give a lower bound for the variance, and symmetric confidence intervals. nonparametric methods such as the bootstrap  <cit>  are much better alternatives. here, rather than focusing our post-analysis using these metrics, we will focus on examining the generalizability of the fitted model. in particular, below we will make extensive use of cross-validation methods, which are rather well-known in system identification to avoid overfitting  <cit> , but which have been very rarely used in the systems biology field.

global optimization method
it is well-known that the cost function  can be highly nonlinear and nonconvex in the model parameters  are especially efficient when provided with high quality first  and second order  information via parametric sensitivities  <cit> . however, in this type of problems they will likely converge to local solutions close to the initial guess of the parameters.

multi-start local methods  have been suggested as more robust alternatives. typically the set of initial guesses is generated inside the parameter bounds either randomly or by a more sophisticated sampling scheme, such as latin hypercube sampling  <cit> . multi-start methods have shown good performance in certain cases, especially when high-quality first order information are used and the parameter search space is restricted to a relatively small domain  <cit> . however, other studies  <cit>  have shown that multi-start methods become inefficient as the size of the search space increases, and/or when the problem is highly multimodal, since many of the local searches will explore the same local basins of attraction repeatedly.

therefore, a number of researches have supported the use of global optimization as a better alternative. however, the current state of the art in global optimization for this class of problems is still somewhat unsatisfactory. deterministic global optimization methods  can guarantee global optimality but their computationally cost increases exponentially with the number of estimated parameters. alternatively, stochastic and metaheuristic methods  <cit>  can be used as more practical alternatives, usually obtaining adequate solutions in reasonable computation times, although at the price of no guarantees. in the context of metaheuristics, hybrids  with efficient local search methods have been particularly successful .

here we have extended the enhanced scatter search  metaheuristic presented by egea et al.  <cit> . our extension of this method, which we will call ess <dig>  makes use of elements of the scatter search and path re-linking metaheuristics, incorporating several innovative mechanisms for initial sampling , an update method which improves the balance between intensification  and diversification , and new strategies to avoid suboptimal solutions. in ess <dig> we have also incorporated several methodological and numerical improvements with the double aim of  increasing its overall robustness and efficiency,  avoiding the need of tuning of search parameters by the user . these improvements can be summarized as follows: 
efficient local search exploiting the structure of the nonlinear least squares problem: after extensive comparisons of local solvers, we selected the adaptive algorithm nl2sol  <cit> . this is a variant of the gauss-newton method that utilizes the jacobian of the residual vector  to approximate and iteratively upgrade the parameter vector. in order to increase its efficiency, we also provide it with high quality gradient information , resulting in speed-ups of up to  <dig> times.

efficient integration of the initial value problem and its extension with parametric forward sensitivity equations using the cvodes solver  <cit> , providing it with the jacobian of the dynamics.

fast computation: although the global solver ess is implemented in matlab, the integration of the initial value problem is done in c in order to speed-up the computations up to  <dig> orders of magnitude.

robust default tuning: metaheuristics require the user to set a number of search parameter values which usually require a number of time-consuming initial trial runs. in the method proposed here, we have made sure that the default search parameters work well without the need of any tuning, which is an additional important advantage. these settings are given in table s. <dig>  in additional file  <dig> 



regularization
regularization methods have a rather long history in inverse problems  <cit>  as a way to surmount ill-posedness and ill-conditioning. the regularization process introduces additional information in the estimation, usually by penalizing model complexity and/or wild behaviour. regularization is related to the parsimony principle , i.e. models should be as simple as possible, but not simpler  <cit> . it also has links with bayesian estimation in the sense that it can be regarded as a way of introducing prior knowledge about the parameters  <cit> . regularization aims to make the problem less complex , i.e. to ensure the uniqueness of the solution  <cit> , to reduce the ill-conditioning and to avoid model overfitting. however, one crucial step is the proper balancing of prior knowledge and information in the data, also known as the tuning of the regularization  <cit> .

regularization has been mainly used in fields dealing with estimation in distributed parameter systems, such as tomography  and other image reconstruction techniques. recently, it has enjoyed wide success in machine learning  <cit> , gaining attention from the systems identification area  <cit> . however, the use of regularization in systems biology has been marginal  <cit> , especially regarding mechanistic  nonlinear models. bansal et al.  <cit>  compared tikhonov and truncated singular value decomposition regularization for the linear regression model of green fluorescent protein reporter system to recover transcription signals from noisy intensity measurements. kravaris et al.  <cit>  compared the theoretical aspects of parameter subset estimation, tikhonov and principal component analysis based regularization, also in a linear model framework. wang and wang  <cit>  presented a two stage bregman regularization method for parameter estimations in metabolic networks. a clear conclusion from these studies is that, for nonlinear inverse problems, there is no general recipe for the selection of regularization method and its tuning. further, it is known that even for linear systems, choosing a method from the plethora of existing techniques is non-trivial  <cit> .

here we want to investigate the role that regularization can play regarding the calibration of nonlinear kinetic models. first of all, we need to address to question of which type of regularization should we use, and how to tune its parameters. second, since kinetic models often have a fixed and rather stiff nature , it is a priori unclear if regularization can really help to avoid overfitting and enhance the predictive value of the calibrated model. third, since most dynamic models in systems biology are severely over-parametrized, we want to explore its capabilities for systematic balancing the effective number of fitted parameters based on the available calibration data. fourth, we want to evaluate the impact of regularization on the convergence properties of the global optimization solvers.

in order to answer these questions, here we present a critical comparison of a wide range of regularization methods applicable to nonlinear kinetic models. we then detail a procedure with guidelines for regularization method selection and tuning. finally, we use numerical experiments with challenging problems of increasing complexity to illustrate the usage and benefits of regularization, addressing the questions above.

statement of the regularized estimation
we consider penalty type regularization techniques  <cit> , which include a penalty Γ in the original objective function , which results in the following regularized optimization problem: 
  θ^α←minimizeθqr=qls+αΓsubject toθmin≤θ≤θmax,dxdt=f,x,θ),y=g,θ),x=x <dig> t∈. 

here α∈r+ is the non-negative regularization parameter and Γ:rnθ→r is the regularization penalty function. when the solution of the original problem  is ill-posed, one has to incorporate some a priori assumption, which makes the estimation well posed. it is assumed that the penalty function Γ is well conditioned and has a unique minimum in the parameters. thus, as the regularization parameter α→∞ the optimization  is well-posed but highly biased by the a priori assumption, and when α= <dig> one obtains the original, ill-posed estimation problem. therefore the role of the regularization parameter α is to properly balance the information of the data and the prior knowledge. however, this is a non-trivial task even for linear problems, as we will discuss below. besides, there are many approaches to formulate the penalty function, among which the tikhonov regularization  <cit> , least absolute shrinkage and selection operator  regularization  <cit> , the elastic net  <cit>  and the entropy based methods  <cit>  are the most frequently used.

determining the proper regularization parameter requires multiple solutions of the regularized optimization problem , therefore the computational efficiency is also crucial. here we chose the tikhonov regularization framework in order to match the form of the penalty to the least squares formalism of the objective function. in this case the least squares cost function can be simply augmented by the quadratic penalty function 
  Γ=θ−θreftwtwθ−θref, 

where w∈rnθ×nθ is a diagonal scaling matrix and θref∈rnθ is a reference parameter vector. in the special case, when w is the identity matrix, we call the scheme as the non-weighted tikhonov regularization scheme . if the θref is the null-vector, the corresponding regularization scheme is often referred as ridge regularization.

scenarios based on prior information
kinetic models can overfit the data leading to poor generalizability. here we propose using prior knowledge to select the most appropriate regularization method to avoid such overfit. based on the level of confidence in this prior knowledge, we can consider three possible scenarios: 
worst case scenario, where we have absolutely no prior information about the parameter values, typically resulting in very ample bounds and random initial guesses for the parameters.

medium case scenario, where there is some information about the parameters and their bounds.

best case scenario: the situation where a good guess of the parameters is at hand.



below we will provide, for each scenario, robust recommendations regarding the regularization method to use and its tuning.

prediction error
in order to evaluate the performance of the calibrated model, we will use cross-validation  <cit> , where the calibrated model is used to predict a yet unseen set of data, computing the prediction error. a good model should not only fit well the calibration data, but it also should predict well cross-validation data , i.e. it should be generalizable.

in this section we utilize the bias-variance decomposition of the prediction error and show when and how regularization can lead to smaller prediction error. first, let us introduce the subscript c for the calibration data and the subscript v for the validation data. for notational simplicity we consider only one experiment and only one observable for both of the calibration and validation scenario, but it is straightforward to generalize for multiple experiments and observables by the notion of normalized mean squared error . the expected prediction error  for the validation data can be written as 
  pe=ev,c) <dig> 

where y~v is the validation data, θ^c is the estimated parameter vector based on the calibration data and ŷv is the model predictions for the validation data. the prediction error depends on the calibration data –different calibration data would result in different estimated parameters θ^c– and also depends on the validation data. thus the expectation is taken over the distribution of the calibration and the validation data. the measurement error in the calibration data and in the validation data is often independent, leading to the well-known  bias-variance decomposition of expected prediction error as 
  pe=evyv−ecŷvθ^c2⏟bias2+evŷv−ecŷvθ^c2⏟variance+evε <dig>  

here, the first term corresponds to the squared bias of the calibrated model predictions from the true validation data yv, the second term is the variance of the model prediction, and the third term is the contribution of the measurement error evε2=σ <dig> 

the variance term the variance of the prediction is due to the uncertainty in the parameter estimates. this uncertainty can be especially large if the calibration data is scarce and the number of data points is close to the number of parameters. the variance term can be expressed for unbiased estimates  <cit>  as 
  variance=nθndσ <dig>  

where nθ is the number of estimated parameters and nd is the number of calibration data. each estimated parameter contributes by σ2nd to the prediction error, thus a model with fewer calibrated parameters would result smaller variance. for biased estimates the prediction variance becomes 
  variance=nθeffndσ <dig>  

where nθeff is the effective number of parameters, which depends on the regularization penalty and regularization parameter α. in general, the effective number of parameters can be expressed by the second order derivatives of the objective function  <cit>  with respect to the parameters as 
  nθeff=tracehls−1hls− <dig>  

where 
  hls=e∂r∂θt∂r∂θ|θ=θt 

is known as the gauss-newton approximate of the hessian and hΓ is the hessian of the regularization penalty function. note that  is also related to the fisher information matrix , which is often used in the practical identifiability and uncertainty analysis of the estimated parameters  <cit> . for example, the eigenvalue decomposition of the fim can identify correlated estimated parameters and parameters with high uncertainty  <cit> . small or zero eigenvalues  indicates ill-posedness, i.e. the parameter estimation problem does not have a unique solution. this eigenvalue decomposition has been widely used in the estimation literature .

in the special case of ridge regularization  <cit> , i.e. Γ=θtθ, the hessian of the penalty is the identity matrix and eq.  simplifies to 
  nθeff=∑i=1nθσi <dig>  

where σi  are the eigenvalues of hls. note that for α= <dig> –the non-regularized case– the effective number of parameters equals to the number of model parameters and for α> <dig> –the regularized case– the effective number of parameters is less than the number of model parameters nθ. thus, as the regularization parameter increases, the effective number of parameters decreases and therefore the variance term of the prediction error  decreases.

the bias term we saw above that regularization reduces the effective number of parameters, and therefore the variance of the prediction error. the cost to pay is the bias. sjöberg and ljung  <cit>  derived an upper bound on the prediction bias for the non-weighted tikhonov regularization, i.e. when the penalty Γ=t, where θref is a reference parameter vector. it was shown that in this particular case the bias is 
  bias2<α8||θt−θref|| <dig> 

where θt is the true model parameters. thus, the smaller the regularization parameter and the better our a priori knowledge is , the smaller the bias that will be introduced in the estimation.

the minimal prediction error there is a trade-off between bias and variance. from eqs. ,   and  one obtains that the reduced variance due to the regularization is larger than the introduced bias if the following inequality holds: 
  σ2nθ−nθeffnd>α8||θt−θref|| <dig>  

therefore, regularization generally increases the performance of the calibrated model whenthe calibration data is noisy  and the amount of data is limited ,

there are a large number of correlated parameters, and therefore the hessian of the original problem has very small eigenvalues. in this case even a small regularization parameter can largely reduce the effective number of parameters, i.e. nθ≫nθeff.

one has a good guess of the true parameters , for example from other independent experiments, previous studies or based on the biological or physico-chemical meaning of the parameters.



however, note that regularization may damage the prediction  if the original problem is not ill-posed, i.e. nθ≈nθeff, α is set to a large value and the provided reference parameters are far from the true parameters.

connection with bayesian parameter estimation
the considered parameter estimation problem  follows the so-called frequentist approach. in contrast to the bayesian approach, where the model parameters are considered random variables, in the frequentist approach the model parameters are assumed to be constants, i.e. we assume the existence of a true parameter vector θt which would predict the measurement error-free data. yet, the parameter estimates are uncertain quantities following well defined distributions , which can be calculated based on the available data.

despite of the above fundamental difference, the formulation of the bayesian approach can coincide with the regularized parameter estimation if some further assumptions hold. both the considered regularization method and the bayesian estimation approach use a priori knowledge in the parameter estimation. by noticing the similarities and differences of the two approaches we can gain further insight on how to choose the regularization parameter  <cit> .

from the bayesian perspective, when the estimated parameters have the prior distribution θprior∼n, i.e. the parameters are normally distributed with mean value θm∈ℝnθ and covariance matrix η∈ℝnθ×nθ, then the maximum likelihood posteriori estimate of the parameters is obtained by solving 
  θ^bayes←minimizeθ12rtr+12tη−1subject toθmin≤θ≤θmaxdxdt=f,x,θ),y=g,θ),x=x <dig> t∈. 

note the similarities between eqs.  and –. the regularized cost function is equivalent to the bayesian cost function if the regularization parameters are fixed as αwtw=η− <dig>  further, the reference parameter vector in the regularized estimation plays the role of the mean value of the prior distribution of the parameters in the bayesian formalism . thus, the bayesian maximum likelihood posteriori estimate can be seen as a special case of the regularization.

tuning the regularization
the regularization parameter balances the a priori knowledge and the information of the data, therefore plays a vital role in the regularization. when α= <dig>  the regularized optimization  becomes the original problem  and the variance of the estimated parameters dominates the prediction error . while as α→∞ the problem is well posed, but biased towards the reference parameter set. the goal of a tuning method is to find an optimal value for α, which minimizes the prediction  error .

the exact computation of the optimal regularization parameter is not possible, since the computation of the prediction bias-variance trade-off would require the knowledge of the true parameters. many tuning methods  have been developed based on different assumptions and approximations to compute an approximate regularization parameter value.

in general, in order to find the optimal regularization parameter, α is discretized as α1>α2>⋯>αi and then the search for optimal regularization parameter is reduced to choose the best regularization parameter in this set . the optimization problem  has to be solved for each candidate, which results in the regularization candidates: θ^α <dig>  θ^α <dig> …θ^αi. this is a computationally expensive task, although in an iterative framework the previously obtained solutions can be utilized to reduce the computational cost of the remaining candidates.

regularization tuning methods can be classified based on the type of information they require  and in the way the optimal regularization parameter is selected among the candidates. in table  <dig> we shortly summarize the regularization tuning methods that we have considered and compared. further details about each tuning method can be found in additional file  <dig>  the methods considered can be classified into the following five groups: 
discrepancy principle  is based on the idea that the regularization parameter should be chosen such that the sum of residuals should be equal to the error level of the data. for that, a good estimate of the measurement error is needed, which is often not known. other versions of the discrepancy principle, such as the modified discrepancy principle  and the transformed discrepancy principle  are known to be less sensitive to the accuracy of a priori error level.
α
max/α
min
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓


monotone error rule  and quasi optimality criteria : they use the observation that the differences between successive candidates, i.e. ||θ^αi−θ^αi+1||, are large due to either large regularization or large propagated error and the difference becomes small for the optimal regularization parameter.

balancing  and hardened balancing principle : they use all the candidates to estimate the regularization error, which is compared then to the so called approximated propagated error bound. the optimal regularization parameter is for which the two types of estimated error is minimal.

l-curve method: as proposed by hansen et al.  <cit>  to display information about the candidates θ^αi, i=1…i. by plotting the two parts of the objective function : the model fit qls and the regularization penalty Γ for {α <dig> …αi} one obtains a discrete pareto optimal front, which usually has an l-shape . the horizontal part is formed by the solutions corresponding to large regularization parameters, where the regularization bias is dominating. as the regularization parameter decreases the least squares error reaches a limit that is determined by the measurement noise and the model flexibility. on the vertical part of the l-curve a small reduction in the least squares model fit error usually cause a large increase in the penalty. intuitively, the optimal regularization parameter that balances the two types of error is located near the corner of the l-shaped curve. in  <cit>  the corner point is defined as being the point that has the largest curvature on the l-curve .


generalized cross validation : an approach by golub  <cit>  that aims to find the regularization parameter that minimizes the leave one out  prediction error  <cit> . it does not require any estimate of the measurement error, but it can be sensitive if a small number of measurement data is at hand. for this reason, other variants, such as the robust  and the strong robust generalized cross validation methods  <cit>   have been developed.



as we will see from the results in section “tuning the regularization and prior knowledge”, the generalized cross-validation method was found to be the best for the presented regularization procedures.

implementation details
in fig.  <dig> we outline the architecture of the resulting method and its implementation. the pre-processing step makes use of symbolic manipulation to derive the sensitivity equations and jacobians , and automatically generates c code to be compiled and linked with the cvodes solver. this procedure ensures the highest numerical efficiency and stability during the solution of the initial value problem. this is especially useful in stiff nonlinear dynamic systems, where wild dynamic behaviour can occur during the exploration of the parameter space.
fig.  <dig> architecture of the method: in the pre-processing phase, sensitivity equations and jacobians  are derived via symbolic manipulation, generating c code which is then linked to the initial value problem  solver, cvodes. the regularization scheme is selected according to the quality of the prior knowledge, and tuned following the procedure described in section “tuning the regularization and prior knowledge”. finally, global optimization with ess <dig> is used to find the regularized estimate of the parameters. the resulting calibrated model can then be further evaluated using cross-validation, followed by additional post-regression and goodness-of-fit analysis



the regularization scheme is selected according to the quality of the prior knowledge , and the cost function is formulated. the regularization is then tuned following the procedure described in section “tuning the regularization and prior knowledge”. finally, global optimization with ess <dig> is used to find the regularized estimate of the parameters. the resulting calibrated model can then be further evaluated using cross-validation, followed by additional post-regression and goodness-of-fit analysis.

RESULTS
numerical case studies
we have considered a set of seven parameter estimation problems, which are used as numerical case studies. these case studies have been chosen as representatives of the typical problems arising in computational systems biology, i.e. partially observed nonlinear dynamic models and sparse noisy measurements. these examples include signalling and metabolic pathway models of increasing complexity. table  <dig> contains a short summary of these case studies, with the original references and an overall view of number of estimated parameters, dynamic state variables, observables, and data. further details, including model equations and the data sets used for model calibration and cross-validation are reported in additional files  <dig> and  <dig>  respectively. it should be noted that in several of these examples the original references only describe the model dynamics, not the full parameter estimation problems.

*the dynamic model can be found in the biomodels database 



in the following sections, we use these examples to illustrate the issues and pitfalls arising from the nonconvexity and ill-conditioning of the estimation problems. next, we use them to illustrate the key ideas behind the methods presented above, including the bias-variance trade-off, the tuning of the regularization, the effect of the quality of the prior knowledge on the regularization, and their impact on cross-validation results. for the sake of brevity, we include summarized or selected results in the main text, but detailed results for all the case studies can be found in additional file  <dig> 

multi-modality of the optimization problem
since the estimation problem stated above is nonconvex, multi-modality  will be a key possible pitfall. as already discussed, local nonlinear least squares  algorithms will find the local minima of the objective function in the vicinity of the initial point. a characterization of the set of possible local optima can be obtained by the frequency distributions of the solutions found by a multi-start local procedure, i.e. starting local optimizations from different initial points, selected randomly in the parameter space. if the initial points cover the parameter space adequately well, the observed distribution of the local optima can be used to quantify the difficulty of the parameter estimation problem arising from multi-modality. for example, fig. 2a shows the distribution of these local minima for the goodwin’s oscillator  case study. the distribution was obtained by solving  <dig>  optimization problem  with the nl2sol nls algorithm started from randomly chosen initial guesses. these initial points were selected based on the logarithmic latin hypercube sampling  method . the distribution of the obtained local optima is spread along several magnitudes , with the best  objective function value of  <dig> , which is very close to the best known solution for this problem and therefore can be considered as global minimum of the objective function. although the local optimization was enhanced by high quality jacobian information based on the sensitivity calculations, only  <dig> % of the runs achieved the vicinity of the global optima.
fig.  <dig> local optima of the objective function corresponding to the goodwin’s oscillator case study . figure a shows the distribution of the final objective function values of  <dig>  runs of local solver nl2sol from randomly chosen initial points based on latin hypercube sampling. the distribution of the local optima shows that only  <dig> % of the runs finished in the close vicinity of the global optima . figure b shows the fit corresponding to the global optima . figure c depicts the fit corresponding to the most frequently achieved local minima . note the qualitatively wrong behaviour of this fit, i.e. the lack of oscillations in the predictions



the calibration data and the simulation results of the most frequently occurring local optima  is shown in fig. 2c. this is certainly a potential pitfall of using local optimization, which can lead to wrong conclusions about the model predictive capability. in contrast, the fit of the global solution  is depicted in fig. 2b, showing a good agreement between the model and the data.

we applied the same procedure to the other case studies, with the corresponding histograms shown in fig.  <dig>  these histograms show that all these case studies exhibit multi-modality, but in different degree. we can see that oscillators tend to exhibit more local minima than the other types. however, case study tsmp, which does not exhibit oscillations, presents a particularly challenging histogram: none of the local searches was able to locate the vicinity of the global solution. in summary, some of these problems could in principle be solved by a multi-start local method, especially if using high quality gradients. but this approach would fail in other cases, and we have no a priori way of distinguishing between these two groups. therefore, we conclude that an efficient global optimization approach should be used in all cases to avoid artifices  and ensure the best possible fit.
fig.  <dig> distributions of local optima for all case studies. each case study was solved by the ams method and the observed frequency of the local minima is reported here. note that the objective function values  are scaled by the global optimum qlsgo for each case study, and the resulting ratio is reported in logarithmic scale. the height of the first bin at  <dig> represents the frequency of finding the vicinity of the global solution



convergence of the optimization algorithms
once we have characterized the multi-modality of the case studies, we now illustrate the advantages of using the ess <dig> global optimization method presented previously. first we consider the solutions of the non-regularized calibration problems , and then in the following subsection we will discuss the regularized estimations . the metric to be used will be based on the convergence curves, i.e. cost function values versus computation time. in order to evaluate the improvements in efficiency and robustness, we will compare the following methods for all the case studies, using a fair stopping criteria based on when a predefined computational time budget is reached: 
simple multi-start  of nl2sol with finite difference jacobian computation.

advanced multi-start , similar to sms, but the bounds of the feasible range of the parameters are transformed by the logarithmic function and then the latin hypercube sampling method is utilized to sample initial points . this way the parameter space is better sampled, especially if the upper and lower bounds of some parameters have very different magnitudes . further, nl2sol is provided with high quality jacobian of the residual vector.

: the new enhanced scatter search described above, making use of nl2sol and high quality jacobian.

: like ess2a but initialized by the log latin hypercube sampling as in ams.



the above methods are compared based on their convergence curves  and the distribution of the final cost function values reached . the empirical convergence curve depicts the current best objective function value as the optimization algorithm proceeds. an optimization method performs better than another method if a lower objective function value is reached in the same amount of computation time. since both the multi-start and the ess <dig> approaches use random numbers, the convergence curves will be different for each run. thus we need to compare the convergence curves for several runs of each method.
fig.  <dig> comparison of convergence curves of selected optimization methods. the convergence curve shows the value of the objective function versus the computation time during the minimization . results are given for simple multi- start , advanced multi-start  and enhanced scatter search methods . results are shown for two case studies:  gosc and  tsmp



considering the results for all the case studies , we can conclude that the more refined version of multi-start can solve problems of small size  and with relatively tight bounds and good initial guesses for the parameters, but it is not reliable in more difficult situations. in contrast, the ess2b method performed consistently well, solving all the problems in reasonable computation time using its default options . in the remaining text we will refer to ess2b as ess <dig> 

the effect of regularization on the convergence

we now consider how the penalty regularization , which changes the topology of the objective function, affects the convergence of the optimizer. we used ess <dig> to solve the regularized problem for each case study, finding a narrower spread of the convergence curves. we also found improvements in the average time to reach the global solution. this benefit was especially clear in the tsmp case study, where the robustness was greatly improved . detailed results for all case studies are reported in additional file  <dig> 

this additional beneficial effect of regularization on the convergence can be explained as follows: while the original cost function is multi-modal, the penalty term in tikhonov regularization  is a quadratic  function. thus, in the limit α→∞ the regularized objective function becomes a convex function.

note that, the global minimum of the objective function is always larger for the regularized problem  in ) than the value for the non-regularized problem  in ). this is because the penalty term ) contributes only to the objective function in . further, the regularization avoids overfitting the data, thus the sum of squared residuals part of the objective function ), is also larger than the minimum of the non-regularized solution ).

tuning the regularization and prior knowledge
kinetic parameters of bio-models are generally unknown and vary for different cells. thus, even if we have some prior knowledge about the parameters, it should be tested against the data. as shown later in section “ill-conditioning, cross-validation and overfitting”, the predictions of the calibrated models using good prior knowledge in the regularization agree with the cross-validation data and thus generalize better.

in order to adjust the right level of the regularization, the regularization parameter  has to be tuned. the tuning includes three steps : 
ts1:a set of regularization parameter candidates are determined: α <dig>  α <dig>  …αi. to cover large range with few elements, typically the candidates are determined as the elements of a geometric series, i.e. αn=α0·qn for n=1…i, where α0> <dig> and 0<q< <dig> 

ts2:the regularized calibration problem - is solved for each regularization parameter. this results in a set of calibrated models , with estimated parameters denoted by θ^α <dig>  θ^α <dig> …, θ^αi.

ts3:the best candidate is selected based on a tuning method:

{θ^α <dig> θ^α <dig> …,θ^αi}→θ^αopt



in ts <dig>  the range  with i= <dig> candidates was found to be a good balance between accuracy and computational cost for all the case studies considered. in ts <dig>  the calibration problems with different candidates can be solved parallel, since they are essentially independent optimization problems. however, when solved sequentially, the previously obtained solutions can be used to start the next optimization problem from a good initial point, and thus reduce its computational cost. we report further practical considerations in additional file  <dig> 

the best way to select the optimal candidate in ts <dig> is cross-validation  <cit> , but it requires an independent set of data at the time of calibration. however, in general it is unclear how the total amount of data should be divided  <cit>  into a calibration and validation set for regularization. in case of scarce data, where the splitting is undesirable, tuning methods must be used.
fig.  <dig> tuning the regularization method for bbg case study. figure a shows the trade-off between the two terms of the regularized objective function, i.e. model fit and the regularization penalty, for a set of regularization parameters . a larger regularization parameter results in worse fit to the calibration data, small regularization parameter results in a larger penalty. figure b compares the candidates based on the generalized cross-validation scores. a larger score indicates worse model prediction for cross-validation data. the curve has the minimum at  <dig> . figure c shows the normalized root mean square prediction error of calibrated model for  <dig> sets of cross-validation data and regularizations considering different quality of the prior information . for a wide range of priors  the regularized estimation gives a good cross-validation error. small priors exhibit worse predictions



we have tested  <dig> tuning methods on the case studies by comparing the regularization parameter selected by each tuning method with the optimal regularization parameter which minimizes the prediction error . the optimal regularization parameter and the regularization parameters selected by the tuning methods are reported in additional file  <dig> for each case study. we found the  generalized cross validation method as the most reliable, since it identified the optimal regularization parameter reliably, outperforming the other methods.

the generalized cross-validation method does not use any further cross-validation data, but estimates the leave-one-out cross validation error of the candidate models based on the calibration data. the criteria is computed as 
  gcv=rssnd−nθeff,fori= <dig> …i 

where rss is the sum of squared normalized residuals for the candidate =rtr), nd is the number of calibration data and nθeff is the effective number of fitted parameters in the model calibration . the rss grows with α since larger regularization results in a worse fit to the data . the larger the α is, the more the fitted parameters are constrained by the reference parameter vector, thus the effective number of fitted parameters decreases with α ). the generalized cross validation error is small if the model fits the data well, while it also has a low number of effective parameters. figure 5b shows the computed gcv value for the candidates in the bbg case study. it shows a minimum for the regularization parameter  <dig> . note that in cases where the amount calibration data is small, the gcv method tends to under-regularize the calibration  <cit> , so the robust gcv  method was found to be a better alternative.

the quality of the regularized calibration depends not only on the regularization parameter, but also on the prior knowledge of the modeller encoded by the reference parameter vector θref and scaling matrix w. to test the robustness of the method with respect to these input information, we chose a range of reference parameter vectors and scaling matrices and solved the regularized optimization problem for each case study. in each case the generalized cross-validation score was used to select the regularization parameter. then, the calibrated models were tested by computing predictions for cross-validation data sets. figure 5c depicts the results for the bbg case study using box-plots. the first two columns show the distribution of the prediction error  for the nominal model  and for the model calibrated without regularization. the next  <dig> columns in the plot show the prediction error with different quality of prior knowledge. we can see that the regularization method gives better predictions than the non-regularized for a wide range of prior quality.

prediction and parameter bias-variance trade-off
here we consider the stability of the solution of the optimization problem with respect to small perturbation in the data. note that this numerical analysis is partially based on the bias-variance decomposition of the estimated model predictions and estimated parameters, thus it requires the knowledge of the nominal  parameter vector. obviously the true model is known only for synthetic problems, but it can be used as a way to analyse the reliability of computational methods.

the experimental data is always measured with some uncertainty, which also influences the model calibration. if we could repeat the experiments, for example  <dig> times, taking measurements in the same conditions, we could collect  <dig> different datasets with slightly varying measurements –due to the random measurement error. then each of the  <dig> datasets could be used to calibrate a model with and without regularization, which would result  <dig> slightly different calibrated models for both the non-regularized and regularized calibration procedure. analysing the consistency of these models can reveal the sensitivity of the calibration procedure to the measurement error.

the results of this procedure for the bbg numerical case study can be seen in fig. 6a and 6b, where the nominal model predictions are shown by dashed line together with the range of the measured data depicted by error bars. in fig. 6a the predictions of the models, calibrated in the traditional way –without the regularization– is also shown, in contrast, the models shown in fig. 6b were calibrated using regularization. we can observe that the model predictions are less sensitive to the error in the data when regularization is applied, i.e. the variance of the model predictions are smaller. however, we also observe larger bias from the nominal trajectory for the regularized models, since no prior knowledge was used in this case .
fig.  <dig> bias-variance trade-off for the bbg case study. figures a and b illustrate the nominal trajectory  and the range of perturbed measurements together with predictions of calibrated models  without and with regularization, respectively. the distribution of the regularized predictions  are narrower than in the non- regularized one , but are slightly biased from the nominal trajectory. figure c depicts the squared bias and the variance of these model predictions as a function of the regularization parameter. the mean square error  has a minimum at  <dig> . figure d, e and f shows the results for the estimated parameters: with regularization the estimated parameters are less sensitive to perturbations in the data



similar trends and results were obtained regarding the estimated parameters, shown in fig. 6d and 6e. here, the distribution of the parameter estimates in the  <dig> regularized and  <dig> non-regularized calibrations are depicted by box-plots and the grey boxes show the feasible range of the parameters. the regularized calibration results in much narrower distribution for the estimates . the bias-variance trade-off in the estimated parameters is shown in fig. 6f. the optimal regularization parameter for the minimum mean squared parameter estimation error  coincides with the previously obtained value for the minimum mean square prediction error in this case study. although for all case studies we found that αoptpred and αoptparam. are close to each other, they do not necessarily coincide.

ill-conditioning, cross-validation and overfitting
it is a common problem that due to the large measurement error  and due to data scarcity, a model with different numerical parameter values might fit the data almost equally well, which indicates identifiability problems.

a posteriori to the calibration, local analyses of the topology of the objective function can provide valuable information about the uncertainty in the estimated parameters. particularly, the ill-conditioning of the approximated hessian of the objective function ) evaluated at the global optima can indicate high uncertainty in the estimated parameters  <cit> . figure  <dig> shows the eigenvalues of this matrix for each case study. we can see that larger models with more parameters tend to have larger a spread in the eigenvalues, and thus larger condition number, indicating the lack of identifiability of its parameters. however, this is only a local measure of the ill-conditioning of the problem near the optima.
fig.  <dig> eigenvalues of the approximated hessian matrix for each case study. eigenvalues are related to the identifiability of the model parameters: a large spread indicates lack of identifiability of some parameters from the given dataset



a more sound way to measure the predictive value  of the calibrated model is cross-validation, where a different set of data is used to asses the calibrated model. over-fitted models will show a bad fit to cross-validation data since they fitted the noise, rather than the signal, and therefore are less generalizable. if the experimental conditions for collecting the cross-validation data are different from the calibration conditions –e.g. different stimuli levels, time-horizon etc.–, this effect will be more prominent.
fig.  <dig> calibration and cross-validation results for the bbg case study. left figure shows calibration data fitted with non-regularized and regularized estimations = <dig>  and regularized qls= <dig> ). right figure shows cross-validation data with the predictions from the non-regularized and regularized estimations. the regularized model shows a slightly worse fit to the calibration data but much better agreement with the cross-validation data. i.e. regularization results in a more generalizable model



in fig.  <dig> we present similar results for the goodwin’s oscillator case study . here, we already see larger differences between the model predictions in the calibration data, but note that the predictions are almost identical at the time of the measurements. thus, for example, based on the calibration data it would be impossible to decide whether the protein concentration decreases or increases right after the beginning of the experiment. when the two models are cross-validated on an independent set of data  we see that the regularized model is in good agreement with the new data, while the non-regularized model heavily overshoots the data in the first period of the oscillation.
fig.  <dig> calibration and cross-validation results for the gosc case study. left figure shows fits to calibration data, right figure shows agreement of predictions for cross-validation data. the non-regularized and the regularized fits show some differences in the first two oscillations, although at the measurement times the predictions are almost identical. in cross-validation, the non-regularized model shows a heavy overshoot at early times


fig.  <dig> prediction errors distribution for each case study. prediction errors  of the calibrated models with and without regularization are shown for each case study. these distributions were obtained by calibrating the models to multiple sets of calibration data  and cross-validating them on multiple cross-validation data sets. most cases show the trend that better prior knowledge results in smaller cross-validation errors, i.e. regularized models are more generalizable



the distributions can be compared by the observed medians, which are denoted by the black dots in the box-plot. in order to check if the observed differences in the medians are significant we utilized the wilcoxon non-parametric statistical test  <cit>  . the test results show that in the majority of the scenarios the differences in the medians are statistically significant at the  <dig>  level. the exception is the fhn case study where the differences turned out to be not significant for the three scenarios. further details of this statistical test are reported in table s <dig> . <dig> in additional file  <dig> 

by comparing the medians of the distributions we see that in almost all cases the non-regularized models overfit the calibration data, i.e. the non-regularized models fit well the calibration data, but do not predict cross-validation data as well as the regularized models. in each case, the medium and the best case regularization scenarios clearly outperformed the non-regularized estimation, leading to better generalizable calibrated models. however, in two cases we observe that the worst case regularization scenario performed worse than the non-regularized case. also note, that in case of the tgf- β pathway problem  all scenarios gave almost identical results, meaning that the original problem is a well-posed calibration problem. however, this is generally unknown before the calibration.

in this context, it is worth mentioning that the regularization of non-mechanistic  models –like those used in machine learning and system identification, such as e.g. neural networks– usually exhibits more dramatic benefits. the reason is that these data-driven models are by definition extremely flexible and therefore very prone to overfitting. in the case of the mechanistic kinetic models used in systems biology, in many cases they will have a rather rigid structure despite being over-parametrized. therefore, they might be less prone to overfitting. however, a clear exception are models exhibiting oscillatory behaviour, or models with many non-observable states.

regularization schemes based on available information
based on the above results, we recommend the following regularization procedures for the three scenarios defined previously : 
ibest case: a good guess of the parameter values  is available. in this case a first order weighted tikhonov regularization is recommended, i.e. θref:=θguess and the weighting matrix should be initialized by the parameters too, i.e. w=diag, where./ is the element-wise division. in this way, parameters with different magnitudes will contribute similarly to the penalty. in section “connection with bayesian parameter estimation” a similar concept about the weighting matrix was shown from the bayesian perspective.

iimedium case: a situation where a less reliable initial guess –but within one order of magnitude of the true values– is available. as in the best case scenario, the parameter guess should be used as the reference vector in the regularization penalty: θref:=θguess. however, we found, that including these values also in the weighting matrix amplified the error in the parameter estimate. therefore, the non-weighted tikhonov regularization is recommended.

iiiworst case: no prior knowledge and therefore only a random guess of parameters is available. here a two-step regularization procedure is proposed. in the first step ridge regularization is applied which results in a ridge estimate, denoted by θ^αr <dig>  in the second step this parameter vector is used as the reference parameter vector for tikhonov regularization, i.e. . this procedure could be repeated n-times –using the obtained regularized solution as reference parameter vector in the next step–, resulting in the n-th order tikhonov regularization  <cit> , but we found no practical difference after the second step.



the regularized optimization is solved for a set of regularization parameters in each scenario and depending on the amount of data at hand the generalized cross validation method  – for larger dataset– or the robust generalized cross-validation method  – for smaller dataset– is recommended to choose the optimal candidate. a summary of this regularization scheme is illustrated in fig. s. <dig>  in additional file  <dig> 

based on the results presented previously, we suggest that tuning of the regularization can be avoided in certain situations, saving considerable computation time. for scaled models where the number of data points and parameters are similar and the data has 5– <dig> % measurement error, our study indicates that the optimal regularization parameter will lie in the range . for the worst case scenario, rather common in systems biology, we found that the above procedure gave smaller mean square parameter estimation error than the traditional, non-regularized estimation. further, the optimization algorithm exhibited better convergence properties with regularization, although no significant improvements in the model predictions was observed. in the case of medium and best scenarios regularized estimation led to both better parameter estimates and smaller cross-validation prediction error in shorter computation times.

CONCLUSIONS
in this study we propose a new parameter estimation strategy for nonlinear dynamical models of biological systems. this strategy is especially designed to surmount the challenges arising from the non-convexity and ill-conditioning that most of these problems exhibit. the difficulties of parameter estimation problems in systems biology do not only depend on the number of parameters, but also on the structure  of the dynamic model, and the amount of information provided by the  available data.

our strategy combines an efficient global optimization method with three different schemes of tikhonov regularization, selected depending on the quality of the prior knowledge. we tested this strategy with a set of case studies of increasing complexity. the results clearly indicate that an efficient global optimization approach should always be used, even for small models, to avoid convergence to local minima. similarly, our study illustrates how ill-conditioning and overfitting issues can damage the generalizability of the calibrated models. overfitting was found to be especially important when models are flexible , even if the number of parameters is small. our results show how regularization can be used to avoid overfitting, leading to calibrated models with better generalizability. finally, the use of regularization significantly improved the performance of the optimization method, resulting in faster and more stable convergence.

additional files
additional file  <dig> 
detailed description of methods. this file contains detailed descriptions of the methods and associated computational details. 



additional file  <dig> 
regularization tuning methods. this file contains further details and results about the regularization tuning methods. 



additional file  <dig> 
numerical data. this file is a spreadsheet containing the numerical data used for the models calibration and cross-validation. 



additional file  <dig> 
numerical case studies. this file contains detailed descriptions of all the calibration case studies, including the dynamic mathematical models. 



additional file  <dig> 
detailed results. this file contains detailed results of the presented method for all the case studies considered. 



abbreviations
nlsnonlinear least squares

odeordinary differential equation

lassoleast absolute shrinkage and selection operator

fimfisher information matrix

essenhanced scatter search

dpdiscrepancy principle

dmpmodified discrepancy principle

tmptransformed discrepancy principle

mermonotone error rule

qoquasi optimality

bpbalancing principle

hbphardened balancing principle

lccl-curve method based on maximal curvature detection

looleave one out cross validation

gcvgeneralized cross validation

rgcvrobust generalized cross validation

srgcvstrong robust generalized cross validation

bbgbiomass batch growth model

fhnfitzhugh-nagumo model

goscgoodwin’s oscillator model

tsmpthree-step metabolic pathway model

smssimple multi-start

amsadvanced multi-start

competing interests

the authors declare that they have no competing interests.

authors’ contributions

jrb and ag conceived of the study. jrb coordinated the study. ag implemented the methods and carried our all the computations. ag and jrb analysed the results. ag and jrb drafted the manuscript. all authors read and approved the final manuscript.

