BACKGROUND
the elementary flux mode  framework is an approach to express reaction pathways contained in metabolic networks  <cit> . an efm is a minimal set of reactions carrying non-zero fluxes in the correct direction at steady-state. efms provide a systematic and rigorous platform to evaluate functional structures contained in metabolic networks and their link to metabolic phenotypes  <cit> . efms are also used in metabolic engineering to improve product yield  <cit> , <cit> . the definition and applications of efms have been extensively reviewed elsewhere  <cit> , <cit> . most computational approaches formally generate extreme currents due to the common practice of treating reversible reactions as two separate irreversible reactions  <cit> , <cit> . for convenience, “efm” will be used here to refer to both extreme currents and elementary flux modes.

current computation frameworks for enumerating efms are variants of the double description method  <cit> , <cit> – <cit> . generating efms is a hard task  <cit> , and for any large metabolic network requires very  large processor and/or memory capacities. while complete sets of efms have been generated for central carbon metabolism, this is not the case for larger metabolic models. high performance computing clusters become the only viable platform for generating efms. the deployment on computing clusters is made possible through several recent advances in efm computation, namely the combinatorial parallelization and divide-and-conquer approach  <cit> , and the bit pattern trees and born/die approach  <cit> , <cit> . these approaches are based on the classical nullspace approach  <cit> , <cit> . recent elegant strategies for problem sub-division  <cit> , <cit>  have lessened the physical memory load from an explosion of intermediate flux modes. load balancing across and the coordination of computing nodes remain to be significant issues, which limit scalability by problem sub-division.

it has been suggested that only a small subset of efms carry significant physiologically fluxes, and it would be more sensible to focus on these  <cit> . often we are only interested in enumerating a set of efms that satisfies a particular set of criteria, e.g., maximum yield of a product and/or biomass. using conventional algorithms, all efms must be enumerated before selecting those that satisfy the criteria. alternative efm generation algorithms based on linear programming  can incorporate flux criteria directly during efm generation  <cit> – <cit> . efms are generated by constraining reactions to zero flux; efmevolver utilizes a genetic algorithm  <cit> , while the k-shortest efm utilizes a binary solver  <cit>  for this purpose. current lp-based algorithms can produce a relatively small sample of efms satisfying certain flux criteria but would struggle to generate the complete set. identifying new efms becomes increasingly difficult as every solution is appended to an ever-growing constraint matrix to avoid repeated outcomes.

this paper describes the generation of efms using a depth-first search strategy, which is exhaustive, has a constant and low memory load, and can be massively parallelized into independent sub-problems to take full advantage of loosely coupled grid computing infrastructures. with the depth-first search strategy, the enumeration of efms becomes a cpu-limited problem. five sub-models of increasing size  extracted from the e. coli genome-scale model iaf <dig>  <cit>  were used for benchmarking against a double description method, efmtool   <cit> . the algorithm employs an lp-based termination criterion for branch searching and is the first algorithm that is capable of determining the complete subset of efms satisfying a set of criteria without first enumerating all efms.

methods
an alternative elementarity test
consider a reaction network represented by a stoichiometric matrix s with m metabolites and r irreversible reactions, and a flux vector v satisfying the pseudo-steady state condition s ⋅ v =  <dig>  v ≥  <dig>  the flux vector can be expressed using the null space matrix ns of s as v = ns ⋅ t, where the length of the coefficient vector t is equal to the rank of ns and to the nullity dof  of s <cit> . any set of dof independent fluxes will fix the flux vector and the individual members of such a set are called free fluxes  <cit> .

elementarity is normally tested using either the adjacency  <cit>  or the rank test  <cit> . both consider the set of active fluxes, z¯v=i:vi≠ <dig> ≤i≤r. for example, the rank test states that the flux vector v : sv =  <dig>  v ≥  <dig> is elementary, if  nullitys*,z¯v=z¯v−ranks*,z¯v= <dig> 

alternatively, we can consider the set of inactive fluxes z = {i : vi =  <dig>  1 ≤ i ≤ r}. since the nullity of s is dof, it follows from eq.  <dig> that an alternative test would be to establish that an elementary flux vector has exactly dof– <dig> inactive free fluxes. this property was demonstrated in the first nullspace approach paper  <cit> . while this test is not applied in existing efm algorithms, it does suggest an alternative algorithm to generate efms.

depth-first search strategy and implementation
by simple reaction indexing, every efm in a network can be made to have a unique set of dof- <dig> “inactive free fluxes” . this is illustrated with a toy network  in figure  <dig>  where the iff set for each efm is indicated by the shaded cells. a systematic search for efms by reaction knock-out only needs to test combinations involving exactly dof- <dig> reactions at a time. while there is a maximum of cdof−1r combinations, a depth-first search strategy can exploit the uniqueness property , where the next efm in the search has one or more “fixed to active flux”  from the current efm’s iff. this feature is marked by the arrows in figure 1b. furthermore, the depth-first search can reuse determined network constraints such as linear dependencies and v ≥  <dig> to further reduce the workload.

the depth-first search algorithm consists of an alternating pair of forward-tracking and backtracking subroutines . briefly, forward-tracking finds more downstream iff  by evaluating the feasibility of constraining potential free fluxes to zero. backtracking finds the terminal  iff that can be converted to faf  by testing the feasibility of the new faf constraint.

the pseudocode for the depth-first search algorithm is provided in figure  <dig>  during pre-processing, empty faf and iff vectors are created, with the iff vector preset to the leading dof- <dig> pivot rows in a reduced-row echelon form nsrref of the null space. an efm may exist for the initial faf/iff constraint configuration, which is checked by testing the feasibility of constraining fluxes downstream of the terminal iff to be a faf, one reaction at a time.

the main program begins with backtracking. backtracking consists of a series of feasibility tests that evaluates new constraint configurations where the terminal iff is sequentially removed from the iff vector and added to the faf vector, while all downstream faf are cleared. the algorithm is switched to forward-tracking if an faf/iff constraint configuration is found that produces a feasible solution, otherwise backtracking continues to test the preceding iff. in forward-tracking, a series of alternating rank and feasibility tests is performed on fluxes downstream of the previous terminal iff. a candidate flux is added to the iff vector if it passes both rank and feasibility tests. when the last flux is reached or an efm is found , the algorithm is switched to backtracking. the search is terminated when there are insufficient downstream fluxes to achieve dof- <dig> iff.

two examples of the search sequences are shown in figure 1c. between finding efm <dig> and efm <dig>  four sets of backtracking and forward-tracking were performed. the iff for efm <dig> are fluxes  <dig>   <dig>   <dig>   <dig> and  <dig>  while the conversion of fluxes  <dig> and  <dig> into faf during backtracking each produced a feasible solution, only the conversion of flux  <dig> into faf led to efm <dig> 

the feasibility test is achieved by solving an lp problem with a blank objective function  mint0¯ 

subjected to:  t≥0¯ 

nsrref⋅t≥0¯ for unconstrained fluxes

nsrref⋅t>0¯ for faf

nsrref⋅t=0¯ for iff

the values in vector t are by default zero or greater because v ≥  <dig> and the coefficients contained in the pivot rows of nsrref are either zero or one.

search speed-up
it is possible to reduce traversal down search trees that cannot yield any efms due to insufficient downstream fluxes to form dof- <dig> iff. every new iff introduced may force other fluxes to be active, and flux variability analysis  can be used to identify these fluxes for the current iff/faf constraint configuration. in the toy network , for example, if r <dig> is an faf, then r <dig> must be active when r <dig> becomes the new iff.

a recording matrix mrecord of r rows and dof- <dig> columns is created to register fluxes that are dependently constrained to be active for every new iff added. namely, the jth column of mrecord is a { <dig>  1} vector, with “1” to mark new reactions that are permanently active as the result of introducing jth iff during forward-tracking. marked fluxes are no longer candidates for iff, and are bypassed during forward-tracking. during backtracking, all entries in the jth column of mrecord are reset to  <dig> when the jth iff is converted to faf.

progress check-points are implemented to evaluate whether there are sufficient downstream fluxes to reach dof- <dig> iff. forward-tracking can only proceed if  the number of existing and candidate iff is equal to or greater than dof- <dig>   there are still dof- <dig> free fluxes among the set of reactions comprised of existing and candidate iff, and  a nullity of zero is obtained for the sub-stoichiometric matrix comprised of columns corresponding to the marked reactions in mrecord and the faf . for the third check-point, if the nullity is determined to be one, then forward-tracking has found an efm and can be terminated without finding dof- <dig> iff. the search algorithm is terminated when backtracking has removed all iff and forward-tracking has failed at the second check-point .

problem parallelization and yield constraints
the flux space can be divided into exclusive subspaces to be separately searched by controlling the participation of all or a subset of basis vectors in nsrref. each search thread generates efms that have a specific configuration of active and inactive fluxes among the pivot reactions. the lp formulation is similar to eq.  <dig>  except that the parameters contained in the vector t are segregated into three types  t=tfreetinactivetactive 

where tfree≥0¯,tinactive=0¯andtactive>0¯

it is also straightforward to introduce yield or flux constraints into eq.  <dig>  such as product flux and growth rate for a fixed substrate uptake. the depth-first search would only generate efms satisfying these constraints. for example, a flux constraint where the flux of reaction r is to be greater or equal to the value k can be added to the lp problem as  vr=nsrrefr,*⋅t≥k 

metabolic sub-models
five metabolic sub-models of increasing size derived from iaf <dig> were used to evaluate the computation time required by our approach and efmtool to enumerate the full efm set  <cit> , <cit> . the different model sizes were obtained by manipulating the number of biosynthetic outputs from glucose. first, a reduced version of iaf <dig> was obtained by extracting, using fva  <cit> , a subset of reactions  that can carry fluxes when biomass yield on glucose is constrained to the maximum. next, reversible reactions were decomposed into an opposing pair of irreversible reactions and the single biomass equation in the reduced iaf <dig> was replaced by  <dig> individual biomass component drains. the final reduced model has  <dig> reactions. five sub models were spawned from this reduced model by increasing the number of allowable biomass drains from  <dig> to  <dig> . all five models were compressed prior to efm computation .

test models were made incrementally larger by increasing the number of biomass precursors synthesized. a reduced iaf <dig> model generating all biomass components is provided as reference. see additional file  <dig> for model compression algorithm.

hardware and software
the depth-first search algorithm to generate efms was tested on the university of queensland’s barrine linux high performance computing cluster. the computer nodes used were equipped with intel xeon l <dig> dual socket quad core processors  and with 24gb memory. each job was allocated one cpu and 1gb memory.

all algorithms were scripted in matlab  , and could be run entirely on a local computer. for parallel computation, the matlab scripts were compiled using matlab compiler runtime  v <dig>   as a standalone application and subsequently deployed on the barrine cluster. pre- and post-processing were carried out in a local linux computer with an intel core i <dig>  <dig> processor  and 8gb memory. lp optimizations were performed using cplex class api provided in ibm ilog cplex optimization studio   for matlab, or gurobi optimizer . the depth-first search algorithm is provided as additional file  <dig> 

computation time was compared against efmtool <cit> . efmtool was implemented within cellnetanalyzer using matlab  <cit> . since efmtool is a multi-threaded program, an entire barrine node  was allocated to efmtool for each run.

the performance of efmtool and the depth-first search strategy was benchmarked using the cumulative cpu time  obtained using linux’s top command, which is equivalent to real-world time for a processor core working at 100% capacity. for the depth-first search algorithm, the deployed sub-jobs consistently ran at 100% processor capacity, and since only a single processor was allocated to each sub-job, the real-world time recorded in the matlab environment is the same as tcpu. this was manually verified. only the time spent searching the modes is recorded, i.e., pre- and post-processing time was excluded, and the tcpu reported for a given sub-model  is the sum of computation times for the sub-jobs.
efmtool


efmtool
efmtool
efmtool
both cplex or gurobi solvers were tested. absolute time and time ratios with respect to efmtool are shown. for model aantpe, efmtool failed at the 27th of the 39th iteration.

efmtool variably uses a few to all processor cores. in order to exclude pre- and post-processing computation time, the iteration “window” was identified using %cpu given by the top command . the efmtool tcpu observations may be inaccurate for model subaa and aa because their enumerations were completed within seconds, making the iteration window narrow and difficult to identify. ideally, tcpu would be the same regardless of the number of processor cores used, but a standard deviation of 10% of the average tcpu was observed when we separately processed the sub-model aarna using  <dig>   <dig>  <dig>   <dig>   <dig> and  <dig> cores . this variation, however, is small in comparison to the observed tcpu differences between the depth-first search strategy and efmtool.

RESULTS
heuristics to reduce computation time
the order in which reactions are processed greatly affects the performance of the double description method. a common strategy is to sort reaction rows according to the number of possible combination between the positive and negative elements presented within rows of the null space matrix  <cit> , <cit> . in  <dig> randomly ordered networks based on the  model subaa, computation time using the depth-first algorithm ranged from 310 seconds to almost 5000 seconds . row sorting improved computation time for the slowest networks , whereas several other heuristics failed .

we also investigated the possibility of using a sparse null space matrix, based on the hypothesis that faster computation can be achieved by minimizing the combinatorial possibilities between the positive and negative elements contained in ns. the sparse null space matrix, nsefm, was made to contain a full independent set of short efms . unlike nsrref, nsefm is not expressed in a reduced-row echelon form and does not contain dof pivot rows unless the network contains exactly dof efms. this means that some efm basis vectors in nsefm can have negative activity coefficients and these basis vectors can be used to parallelize the depth-first search.

the subaa model has eight efm basis vectors that can have negative activity coefficients. we ran all  <dig>  possible combinations of constraining none, some or all of these, and found that tcpu ranged from 270 s to 950 s . importantly, the slowest configuration with sparse matrix setup was significantly faster than more than half of the  <dig> random tests in basic setup. moreover, parallelization only moderately increased tcpu ; the unparallelized subaa model was solved in ~400 s , whereas the fully parallelized model was solved in ~600 s . the effectiveness of the strategy was confirmed in tests performed on the aa model, and was incorporated into the depth-first search algorithm .

performance comparison against efmtool using sub-models
the performance of the algorithm was tested using five different metabolic growth models with increasing size derived from a reduced iaf <dig> . the smallest model subaa had  <dig> reactions and a nullity of 57; the largest model aantpe had  <dig> reactions and a nullity of  <dig>  network compression removed up to 80% of the original reactions. the observed reduction in the models’ dof was between 43% and 60%. these reductions were attributed primarily to the removal of isolated metabolic pathways , and secondarily to the lumping of duplicated reactions.

the speed of the depth-first search algorithm using gurobi as the solver was compared against the speed of efmtool. for the smaller sub-models subaa, aa, aarna, efmtool was found to be faster by  <dig> to  <dig> fold, but the depth-first search algorithm was at least  <dig> fold faster for larger sub-models aant and aantpe . for the largest model aantpe, efmtool failed at the 27th row iteration  due to physical memory limitations, whereas the depth-first search algorithm was able to generate the full set of efms within the time it took efmtool to fail . gurobi was 10-fold faster than cplex, indicating that solver performance is a significant determinant of the computation speed for the depth-first search algorithm. the sets of efms generated by either approach were identical in terms of number and form.

the depth-first search algorithm, which was executed using matlab compiler runtime, showed resident memory usages between  <dig> to 97 mb. increase in memory usage as individual searches progressed was negligible because solutions were stored in a bit-matrix and written to hard-drive in batches. the computation time increased exponentially with model dof . by interpolation, the search algorithm  would have matched efmtool’s speed for a model with a dof of  <dig>  while it was unexpected to find that the test model aa showed the poorest time ratio despite it not being the smallest model , the trend suggested that the anomaly actually resides in efmtool. it is unclear why test model aa was more efficiently processed by efmtool than other test models, since all models were derived from the same source.

efms for producing valine from glucose
the largest test model aantpe was used to demonstrate the potential of generating a complete subset of efms satisfying a certain set of flux constraints. conversion of glucose to valine was used for illustration and, based on the maximum calculated theoretical yield of valine of  <dig>  mol per mol glucose, four yield intervals were applied as constraints . there is a total of  <dig>  efms describing the conversion of glucose to valine in the set of efms from the full model . the efms and the number of efms generated at a given yield interval was found to be the same as the subset extracted from the full efms set . the time required to generate these subsets of efms decreased with increasing yield stringency.

efms were generated for model aantpe using cplex solver. the theoretical yields range from  <dig>  to  <dig>  mol valine per mol glucose.

discussion
the classical double description method generates a massive number of intermediate modes before arriving at the final set of efms. existing elementarity tests, the adjacency test and the free-standing rank test  <cit> , <cit> , provide some attrition power, but neither is sufficient when applied on larger metabolic models. meeting physical memory requirements to accommodate the combinatorial explosion of intermediate modes is challenging even when using shared or distributed memory system in a computing cluster  <cit> , <cit> . recent elegant strategies for problem sub-division  <cit> , <cit>  has lessened the physical memory load, though the concomitant increase in computation load  <cit>  limits the potential of how many nodes can be used.

our depth-first search strategy developed based on an alternative elementarity test: a flux vector is elementary if it has dof- <dig> inactive free fluxes. in fact, all modes surviving the adjacency test in the nullspace approach have this feature  <cit>  . this efm feature is the basis for the depth-first search strategy, and crucially mitigates the need for large memory requirements. unlike existing lp-based approaches, the depth-first search strategy can be exhaustive, since it does not require an ever-expanding constraint matrix. moreover, the algorithm is readily parallelized using a divide-and-conquer strategy to split the problem into exclusive sub-problems using flux constraints and subsequently enumerate efms using as many computing nodes as available with minimal coordination effort. sub-division of the subaa problem into  <dig> sub-problems, did not significantly increase computation time . additionally, the algorithm is robust and running jobs can be interrupted, resumed and/or further sub-divided as desired.

we demonstrated that the lp-based depth-first search strategy is at least comparable in speed to the mainstream efmtool . using the gurobi solver, 86% of the computation time was spent on feasibility testing. the flux constraints v ≥  <dig>  v =  <dig> and v >  <dig>  combined with the speed-up strategies, were effective at preventing the search algorithm from traversing down tree branches that did not yield any efm. furthermore, finding the next efm involved only a few enumeration steps because efm solutions occurred very densely in the search tree. an average yield of  <dig> solutions per  <dig> optimizations was observed. it appears that the attrition power achieved outweigh the additional processor penalty of running an lp optimization compared to approaches based purely on arithmetic operations, particularly for the larger models.

the rank test contributes to a more efficient use of the lp solver. the rank test ensures that flux constraints are only applied to non-redundant reactions, therefore constraints are guaranteed to reduce the flux solution space. despite the frequency of rank testing, the time spent on calculating matrix rank was only 5% of the total computation time. matrix rank computation was quick because the matrices evaluated were small—the largest matrix would be a square matrix with the dimension of dof-1—and were largely invariant. in essence, the rank and the lp feasibility tests work synergistically to generate efms. in theory, a pure rank or pure feasibility approach could have been used to generate efms, but either approach would have required significantly larger number of enumerations than the concurrent use of both.

our depth-first search approach has not been optimized to reduce computation time. the main component that still requires improvement is the solver performance. the significant speed gained from switching the solver from cplex to gurobi drew our attention to the possibility of stripping down an lp solver to just performing a feasibility test by detecting the presence of an irreducible infeasible set. the optimality criterion is irrelevant here. additionally, our approach may benefit from more efficient methods to calculate matrix rank  <cit> , <cit> , rather than matlab’s convenient but inefficient rank function based singular value decomposition. however, considering the time spent on rank test was only 5% of the total computation time, the benefits would be minor. lastly, there may be an additional speed gain if our algorithms were to be re-scripted in a native programming language like c++, particularly for large-scale deployment onto computing clusters.

although this algorithm is suited for large-scale deployment, genome scale enumeration of efms in e. coli remains impossible with the irreversible version of iaf <dig> having a nullity of  <dig> even after network compression. the smallest, reduced iaf <dig> still capable of producing all biomass components has  <dig> dof  and is projected to require  <dig> million cpu hours to solve using our setup . the largest number of efms enumerated to date is for  <dig> million for e. coli and  <dig> billion efms for p. triconutum, respectively  <cit> , <cit> . the p. triconutum model  <cit> , which has  <dig> irreversible reactions and a dof of  <dig> after compression . the size of our largest test model  is significantly smaller in comparison to these achieved scales. the preliminary speed results nonetheless demonstrated that the depth-first search strategy is a viable alternative.

of greater practical importance, this lp approach can generate a complete subset of efms satisfying certain flux criteria without enumerating all efms first. the potential time saving is very significant as illustrated with the valine example; the time required to generate all efms with a yield greater than 75% of the theoretical yield was only 1% of the computation time required to generate a full set of efms . constraints derived from thermodynamics and regulation can be incorporated as well  <cit> – <cit> . instead of a holistic approach, metabolic pathway analysis may benefit from first establishing a narrower, targeted, well-defined metabolic hypothesis, and subsequently generating efms that are pertinent to the investigation.

CONCLUSIONS
a depth-first search algorithm to generate efms based on linear programming and rank test was developed. the speed is comparable to the conventional approaches based on the double description method and the algorithm is scalable, has a low and constant memory load, and can incorporate additional flux constraints to generate a full subset of efms of interest.

competing interests
the authors declare that there are no competing interests.

authors’ contributions
leq conceived and designed the method, generated the results and drafted the manuscript. lkn revised the manuscript. both authors have read the manuscript and approved the final version.

additional files
supplementary material
additional file 1:
a brief example to demonstrate that modes passing theadjacency testalways containdof-1independent zero fluxes. also contains demonstration using the toy network that the selection of inactive free fluxes can be made unique if the leading reactions are chosen.

click here for file

 additional file 2:
details on the method used for network compression.

click here for file

 additional file 3:
contains matlab scripts for the depth-first search algorithm.

click here for file

 additional file 4:
contains 
efmtool
’s tcpu s for the five sub-models and tcpu test results for running sub-model
aarna
separately using  <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> cores.

click here for file

 additional file 5:
results to show the effectiveness of different heuristics at reducing tcpu. details on the method used to generate efm basis vectors, which are then used to generate all others efms.

click here for file

 additional file 6:
graph of efms with non-zero valine yield coefficients.

click here for file

 acknowledgements
we are grateful for the resources provided the high performance computing support group at the university of queensland, and we thank david green and edan scriven for their continuous technical support. we thank the gurobi team for kindly providing a temporary virtual license to enable the use of gurobi on the barrine cluster.
