BACKGROUND
in order to study a biomolecular mechanism such as epigenetic gene control  and formulate a new hypothesis, we usually integrate various types of information to distil a comprehensible model. we can use this model to discuss with our peers before we test the model in the laboratory or by comparison to available data. a typical hypothesis is based on one's own knowledge, interpretations of experimental data, the opinions of peers, and the prior knowledge that is contained in literature. many web resources are available for molecular biologists to access available knowledge, of which entrez pubmed, hosted by the us national center for biotechnology information , is probably the most used by molecular biologists. the difficulty of information retrieval from literature reveals the scale of today's information overload: over  <dig> million biomedical documents are now available from pubmed. also considering the knowledge that did not make it to publication or that is stored in various types of databases and file systems, many scientists find it increasingly challenging to ensure that all potentially relevant facts are considered whilst forming a hypothesis. support for extracting and managing knowledge is therefore a general requirement. developments in the area of the semantic web and related areas such as information retrieval are making it possible to create applications that will support the task of hypothesis generation. first, rdf and owl provide us with a way to represent knowledge in a machine readable format that is amenable to machine inference  <cit> . ontologies have become an important source of knowledge in molecular biology. many ontologies have been created and many types of application have become possible  <cit> , with the life sciences providing a key motivation of addressing the information management problem that arises from high throughput data collection  <cit> . a downside to the popularity of bio-ontologies is that their number and size have become overwhelming when attempting to discover the best representation for one's personal hypothesis. moreover, building a biological ontology is usually associated with a community effort where consensus is sought for clear descriptions of biological phenomena  <cit> . the question arises how an experimental biologist/bioinformatician can apply semantic web languages when the primary aim is not to build a comprehensive ontology for a community, but to represent a personal hypothesis for a particular biomolecular mechanism. therefore, we explored an approach to semantic modeling that emphasizes the creation of a personal model within the scope of one hypothesis, but without precluding integration with other ontologies. secondly, information retrieval and information extraction techniques can be used to elucidate putative knowledge to consider for a hypothesis by selecting relevant data and recognizing biological entities  and relations in text  <cit> . for instance, tools and algorithms have been developed that match predefined sets of biological terms  <cit> , or that use machine learning algorithms to recognize entities and extract relations based on their context in a document  <cit> . these techniques can also be used to extend an ontology  <cit> . several tools exist for text mining , but for a methodology to be attractive to practitioners of experimental molecular biology we would like a method that is more directly analogous to wet laboratory experimentation. workflow management systems offer a platform for in silico experimentation  <cit>  where, for example, data integration  <cit> , and systematic large-scale analysis  <cit>  have been implemented. workflows can also be shared on the web such as accomplished in myexperiment  <cit> . in a workflow, the steps of a computational experiment are carried out by individual components for which web services provide a common communication protocol  <cit> . we adopted the workflow paradigm for the design and execution of a reusable knowledge extraction experiment. the main services in the workflow are from the 'adaptive information disclosure application' toolkit  that we are developing for knowledge management applications  <cit>  and this document). the output enriches a knowledge base with putative biological relations and corresponding evidence. the approach is not limited to text mining but can be applied to knowledge extracted during any computational experiment. the advantage of routinely storing extracted knowledge is that it enables us to perform posterior analysis across many experiments.

RESULTS
we present the methodology in the following order: 1) a description of representing prior knowledge through proto-ontologies; 2) extension of the proto-ontologies by a workflow that adds instances to a semantic repository preloaded with the proto-ontologies; 3) a description of how to query the knowledge base; 4) a description of the toolkit that we use for knowledge extraction and knowledge management. data and references are accessible from pack  <dig> on myexperiment.org  <cit> .

model representation in owl
different types of knowledge
step one of our methodology is to define machine readable 'proto-ontologies' to represent our biological hypothesis within the scope of an experiment. the experiment in this case is a procedure to extract protein relations from literature. our approach is based on the assumption that knowledge models can grow with each experiment that we or others perform. therefore, we created a minimal owl ontology of the relevant biological domain entities and their biological relations for our knowledge extraction experiment. the purpose of the experiment is to populate  the proto-ontologies with instances derived from literature. we also modeled the evidence that led to these instances. for instance, the process by which a protein name was found and in which document it was found. we find a clash between our intention of enriching a biological model, and the factual observations of a text mining procedure such as 'term', 'interaction assertion', or 'term collocation'. for example, it is obvious that collocation of the terms 'hdac1' and 'p53' in one abstract does not necessarily imply collocation of the referred proteins in a cell. in order to avoid conflation of knowledge from the different stages of our knowledge extraction process, we purposefully kept distinct owl models. this lead to the creation of the following models that will be treated in detail below:

❑ biological knowledge for our hypothesis 

❑ text 

❑ knowledge extraction process 

❑ extraction procedure implementation 

❑ mapping model to integrate the above through references.

❑ results 

biological model
for the biological model, we started with a minimal set of classes designed for hypotheses about proteins and protein-protein associations . this model contains classes such as 'protein', 'interaction' and 'biological model'. we regard instances in the biological model as interpretations of certain observations, in our case, of text mining results. we also do not consider instances of these classes as biological facts; they are restricted to a hypothetical model in line with common practice in experimental biology. the evidence for the interpretation is important, but it is not within the scope of this model. in the case of text mining, evidence is modeled by the text, text mining, and implementation models.

text model
a model of the structure of documents and statements therein is less ambiguous than the biological model, because we can directly inspect concrete instances such as  documents or pieces of text . we can be sure of the scope of the model and we can be clear about the distinction between classes and instances because we computationally process the documents. this model contains classes for documents, protein or gene names, and mentions of associations between proteins or genes.

text mining model
next, we created a model for the knowledge extraction process. this model serves to retrieve the evidence for the population of our biological model . it contains classes for information retrieval and information extraction such as 'collocation process' and properties such as 'discovered by'. we also created classes to contain text mining specific information such as the likelihood of terms being found in the literature. this allows us to inspect the uncertainty of certain findings. because any procedure could be implemented in various ways, we created a separate model for the implementation artifacts.

workflow model
for more complete knowledge provenance, we also created a model representing the implementation of the text mining process as a workflow of  web services. example instances are  the aida web services, and runs of these services. following the properties of these instances we can retrace a particular run of the workflow.

mapping model
at this point, we have created a clear framework for the description of our biological domain and the documents and the text mining results as instances in our text and text mining ontologies. the next step is to relate the instances in the various models to the biological domain model. our strategy is to initially keep the domain model simple at the class and object property level, and to map sets of instances from our results to the domain model. for this, we created an additional mapping model that defines reference properties between the models . this allows us to see that an interaction between the proteins labeled 'p68' and 'hdac1' in our hypothetical model is referred to by a mention of an association between the terms 'p68' and 'hdac1', with a likelihood score that indicates how remarkable it is to find this combination in literature.

in summary, we have created proto-ontologies that separate the different views on biomolecular knowledge derived from literature by a text mining experiment. we can create instances in each view and their interrelations . this allows us to trace the experimental evidence for knowledge contained in the biological model. in a case of text mining such as ours, evidence is modeled by the document, text mining, and workflow models. a different type of computational experiment would require other models and new mappings to represent evidence.

knowledge extraction experiment
the proto-ontologies form the basis of our knowledge base. they provide the initial templates for the knowledge that we wish to be able to interrogate in search of new hypotheses. the next step is to populate the knowledge base with instances. at the modeling stage we already anticipated that our first source of knowledge would be literature, and that we would obtain instances by text mining. an element of our approach is to regard knowledge extraction procedures as 'computational experiments' analogous to a wet laboratory experiments. we therefore used the workflow paradigm to design the protocol of our text mining experiment, here with the workflow design and enactment tool taverna  <cit> . a basic text mining workflow consists of the following steps:  retrieve relevant documents from medline, in particular their abstracts,  extract protein names from the retrieved abstracts, and  present the results for inspection. we implemented the text mining process as a workflow . we added an additional sub-workflow to process the input query in order to extract known protein names from the input query and expand the query with synonyms for known protein names. for this, we employed a web service that provides uniprot identifiers and synonyms for human, rat and mouse gene names. these were derived from a combination of several public databases  <cit> . the query is first split into its individual terms with a service from the aida toolkit that wraps the lucene tokenizer, and then all the terms  from the original query are checked for having a uniprot identifier by which they are identified as referring to a known protein. the sub-workflow makes the synonyms, uniprot identifiers, and the expanded query available for the rest of the workflow. the expanded query is the input for the next sub-workflow: document retrieval. we applied the document search service from the aida toolkit, parameterized to use the regularly updated medline index that is stored on our aida server and updated daily. the output of this retrieval service is an xml document that contains elements of the retrieved documents, such as the pubmed identifier, title, abstract, and the authors. we then extract titles and abstracts for the next sub-workflow: i.e. protein name recognition. sub-workflow  <dig> employs the aida web service 'applycrf' to recognize protein  names in text. this service wraps a machine learning method based on the 'conditional random fields' approach  <cit> . in this case it uses a recognition model trained on protein/gene names. we added the aforementioned uniprot service again to mark the extracted results as genuine human, rat, or mouse protein/gene names. in a number of cases the workflow produced more than one identifier for a single protein name. this is due to the ambiguity in gene and protein names. for instance, tuason et al. reported  <dig> % ambiguous occurrences of mouse gene names in text, and percentages ranging from  <dig> % to  <dig> % depending on the organism  <cit> . the final step of our text mining procedure was to calculate a likelihood score for the extracted proteins to be found in documents retrieved through the expanded input query. we used a statistical method where the likelihood of finding a document with input query  and discovered protein name  is calculated by: , in which q, d, and qd are the frequencies of documents containing q, d, and q and d, respectively; qdexp is the expected frequency of documents containing q and d assuming that their co-occurrence is a random event; n is the total number of documents in medline.

in parallel to the part of the workflow that performs the basic text mining procedure, we designed a set of 'semantic' sub-workflows to convert the text mining results to instances of the proto-ontologies and add these instances to the aida knowledge base, including their interrelations . the first step of this procedure is to initialize this knowledge base after which the proto-ontologies are loaded into the knowledge base, and references to the knowledge base are available for the rest of the workflow. the next step is to add instances for the following entities to the knowledge base: 1) the initial biological model/hypothesis, 2) the original input query, 3) the protein names it contains, and 4) the expanded query. we assumed that the input query and the proteins mentioned therein partially represent the biological model; each run of the workflow creates a new instance of a biological model unless the input query is exactly the same as in a previous experiment. figure  <dig> illustrates the creation of an instance of a biological model and its addition to the knowledge base, including the details for creating the rdf triples in java. all the semantic sub-workflows follow a similar pattern . the following sub-worfklow adds instances for retrieved documents to the knowledge base; it only uses the pubmed identifier. the sub-workflow that adds discovered proteins is critical to our methodology. it creates protein term instances from protein names in the text ontology to which it also adds the collocation relations with the original query a and a 'discovered_in' relation with the document it was discovered in. in addition, it creates protein instances in the biomodel ontology and a biological association relation to the proteins found in the input query. between term and protein instances in the different ontologies it creates reference relations. as a result, our knowledge base is populated with the discoveries of the text mining procedure and their biological interpretations still linked with the knowledge they are interpretations of. the final sub-workflow adds the calculated likelihood scores as a property of the protein terms in the knowledge base. finally, to be able to retrieve more complete evidence from the knowledge base, we extended our models and workflow to accommodate typical provenance data . we created an ontology with classes for workflow runs and web service runs. using the same semantic approach as above we were able to store instances of these runs, including the date and time of execution.

querying the knowledge base
the result of running the workflow is that our knowledge base is enriched with instances of biological concepts and relations between those instances that can also tell us why the instances were created. we can examine the results in search of unexpected findings or we can examine the evidence for certain findings, for instance by examining the documents in which some protein name was found. an interesting possibility is to explore relations between the results of computational experiments that added knowledge to the knowledge base. to prove this concept we ran the workflow twice, first with "hdac <dig> and chromatin" as input, and then with " and  and " as input. we were then able to retrieve three proteins that are apparently shared between the two biological models : nf-kappab , p <dig> , and bax . if we would like to investigate the evidence by which these proteins were discovered we designed a query that traces the chain of evidence . it retrieves the process by which the name of the protein was found, the service by which the process was implemented and its creator, the document from medline in which the protein name was discovered, and the time when this discovery service was run. for example, nf-kappab was found on the 18th of november  <dig> in a paper with pubmed identifier  <dig>  by a run of the 'aida crf named entity recognition service' based on 'conditional random fields trained on protein names', created by sophia katrenko.

the aida toolkit for knowledge extraction and knowledge management
the methodology that we propose enables a 'do-it-yourself' approach to extracting knowledge that can support hypothesis generation. to support this approach, we are developing an open source toolkit called adaptive information disclosure application . aida is a generic set of components that can perform a variety of tasks related to knowledge extraction and knowledge management, such as perform specialized search on resource collections, learn new pattern recognition models, and store knowledge in a repository. w3c standards are used to make data accessible and manageable with semantic web technologies such as owl, rdf, and skos. aida is also based on lucene and sesame. most components are available as web services and are open source under an apache license. aida is composed of three main modules: search, learning, and storage.

search – the information retrieval module
aida provides components which enable retrieval from a set of documents given a query, similar to popular search engines such as google, yahoo!, or pubmed. to make a set of documents  searchable, an 'index' needs to be created first  <cit> . for this the aida's configurable indexer can be used. the indexer and search components are built upon apache lucene, version  <dig> . <dig>  <cit> , and, hence, indexes or other systems based on lucene can easily be integrated with aida. the indexer component takes care of the preprocessing  of the text of each document as well as the subsequent index generation. different fields can be made retrievable such as title, document name, authors, or the entire contents. the currently supported document encodings are microsoft word, portable document format , medline, xml, and plain text. the so-called "documenthandlers" which handle the actual conversion of each source file are loaded at runtime, so a handler for any other proprietary document encoding can be created and used instantly. because lucene is used as a basis, a plethora of options and/or languages are available for stemming, tokenization, normalization, or stop word removal which may all be set on a per-field, per-document type, or per-index basis using the configuration. an index can currently be constructed using either the command-line, a soap webservice , or using a taverna plugin.

learning – the machine learning module
aida includes several components which enable information extraction from text data in the learning module. these components are referred to as learning tools. the large community working on the information extraction task has already produced numerous data sets and tools to work with. to be able to use existing solutions, we incorporated some of the models trained on the large corpora into the named entity recognition web service nerecognizerservice. these models are provided by lingpipe <cit>  and range from the very general named entity recognition  to the specific models in the biomedical field created to recognize protein names and other bio-entities. we specified several options for input/output, which gives us an opportunity to work with either text data or the output of the search engine lucene. we also offer the learnmodel web service whose aim is to produce a model from annotated text data. a model is based on the contextual information and uses learning methods provided by weka  <cit>  libraries. once such a model is created, it can be used by the testmodel web service to annotate texts in the same domain. in this paper we use an aida service that applies a service for an algorithm that uses sequential models, such as conditional random fields /crfs have an advantage over hiddem markov models because of their ability to relax the independence assumption by defining a conditional probability distribution over label sequences given an observation sequence. we used crfs to detect named entities in several domains like acids of various lengths in the food informatics field or protein names in the biomedical field  <cit> .

named entity recognition constitutes only one subtask in information extraction. relation extraction can be viewed as the logical next step after the named entity recognition is carried out  <cit> . this task can be decomposed into the detection of named entities, followed by the verification of a given relation among them. for example, given extracted protein names, it should possible to infer whether there is any interaction between two proteins. this task is accomplished by the relationlearner web service. it uses an annotated corpus of relations to induce a model, which consequently can be applied to the test data with already detected named entities. the relationlearner focuses on extraction of binary relations given the sentential context. its output is a list of the named entities pairs, where the given relation holds.

the other relevant area for information extraction is detection of the collocations . this functionality is provided by the collocationservice which, given a folder with text documents, outputs the n-grams of the desired frequency and length.

storage – the metadata storage module
aida includes components for the storage and processing of ontologies, vocabularies, and other structured metadata in the storage module. the main component, also for the work described in this paper, is repositoryws, a service wrapper for sesame – an open source framework for storage, inferencing and querying of rdf data on which most of this module's implementation is based  <cit> . thesaurusrepositoryws is an extension of repositoryws that provides convenient access methods for skos thesauri. the sesame rdf repository offers an http interface and a java api. in order to be able to integrate sesame into workflows we created a soap service that gives access to the sesame java api. we accommodate for extensions to other rdf repositories, such as the hp jena, virtuoso, allegrograph repositories or future versions of sesame, by implementing the factory design pattern.

complementary services from biosemantics applications
one of the advantages of a workflow approach is the ability to include services created elsewhere in the scientific community . for instance, in our bioaid workflows operations are used for query expansion and validation of protein names by uniprot identifiers. aida is therefore complemented by services derived from text mining applications such as anni  <dig>  from the biosemantics group  <cit> . the 'biosemantics' group is particularly strong in disambiguation of the names of biological entities such as genes/proteins, intelligent biological query expansion , and provision of several well known identifiers for biological entities through carefully compiled sets of names and identifiers around a biological concept.

user interfaces for aida
in addition to rdf manipulation within workflows as described in this document, several examples of user interactions have been made available in aida clients such as html web forms, ajax web applications, and a firefox toolbar. the clients access repositoryws for querying rdf through the provided java servlets. the web services in storage have recently been updated from the sesame  <dig>  java api to the sesame  <dig>  java api. some of the new features that sesame  <dig>  provides, such as sparql support and named graphs, are now being added to our web service api's and incorporated into our applications.

discussion
our methodology for supporting the generation of a hypothesis about a biomolecular mechanism is based on a combination of tools and expertise from the fields of semantic web, e-science, information retrieval, and information extraction. this novel combination has a number of benefits. first, the use of rdf and owl removes the technical obstacle for making models interoperable with other knowledge resources on the semantic web although semantic interoperability will often require an alignment process to take place for more far reaching compatibility. the modeling approach that we propose is complementary to the efforts of communities such as the open biomedical ontology  community. this community's stated purpose is to create an 'accurate representation of biological reality' by developing comprehensive domain ontologies and reconciling existing ontologies according to a number of governing principles  <cit> . our ambitions are more modest. we start with a minimal model to represent a hypothesis, i.e. a particular model of reality. we define our own classes and properties within the scope of a knowledge extraction experiment, but because of the modularity supported by owl this does not exclude integration with other ontologies. in fact, integration with existing knowledge resources enables a complementary approach for finding facts potentially relevant to a hypothesis. clearly, in order to scale up our methodology to represent knowledge beyond the experiments of a small group of researchers, alignment with standards would have to be considered. upper ontologies can facilitate integration , and we can benefit from the obo guidelines and the tools that have been developed to convert obo ontologies to owl  <cit> . another interesting possibility is the integration with thesauri based on the skos framework  <cit> . relations between skos concepts  are defined by simple 'narrower' and 'broader' relations that turn out to be effective for human computer interfaces, and may be the best option for labeling the elements in our semantic models. instead of providing a text string as a human readable label, we could associate an element with an entry in a skos thesaurus, which is a valuable knowledge resource in itself. the skos format is useful as an approach for 'light-weight' knowledge integration that avoids the problems of ontological over-commitment associated with more powerful logics like owl dl  <cit> .

a second benefit of our methodology comes from the implementation of the knowledge extraction procedure as a workflow. the procedure for populating an ontology is similar to the one previously described by witte et al.  <cit> , but our implementation allows the accumulation of knowledge by repeatedly running the same workflow or adaptations of it. this enables us to perform posterior analyses over the results from several experiments by querying the knowledge base, for instance in a new workflow that uses the aida semantic repository service. moreover, the approach is not limited to text mining. if one considers text documents as a particular form of data, we can generalize the principle to any computational experiment in which the output can be related to a qualitative biological model. as such, this work extends previous work on integration of genome data via semantic annotation  <cit> . in this case the annotation is carried out by a workflow. considering that there are thousands of web services and hundreds of workflows available for bioinformaticians  <cit> , numerous extensions to our workflow can be explored. in addition, the combination with a semantic model allows us to collect evidence information as a type of knowledge provenance during workflow execution. in this way, we were able to address the issue of keeping a proper log of what has happened to our data during computational experimentation, analogous to the lab journal typically required in wet labs  <cit> . ideally, the knowledge provenance captured in our approach would be more directly supported by existing workflow systems. however, this is not yet possible. there seems to be a knowledge gap between workflow investigators and the users from a particular application domain with regard to provenance. we propose that workflow systems take care of execution level provenance and provide an rdf interface on which users can build their own provenance model. in this context, it will be interesting to see if we will be able to replace our workflow model and link directly to the light weight provenance model that is being implemented for taverna  <dig>  <cit> . a third benefit is that the application of semantic web, web services, and workflows stored on myexperiment.org, allow all resources relevant to an experiment to be shared on the web, making our results more reproducible. we would like to increase the 'liquidity' of knowledge so that knowledge extracted from computational experiments can eventually fit into frameworks for scientific discourse  such as semantic web applications in neuromedicine   <cit> . if it is to be global, interoperability across modes of discourse would require large scale consensus on how to express knowledge provenance, not only about knowledge produced from computational experiments but also from manual or human assertions. some groups are attempting to address various aspects of this problem, such as the scientific discourse task force  <cit>  in the w3c semantic web health care and life sciences interest group  <cit> , the concept web alliance  <cit>  and the shared names initiative  <cit> .

CONCLUSIONS
in this paper we demonstrate a methodology for a 'do it yourself' approach for the extraction and management of knowledge in support of generating hypotheses about biomolecular mechanisms. our approach describes how one can create a personal model for a specific hypothesis and how a personal 'computational experiment' can be designed and executed to extract knowledge from literature and populate a knowledge base. a significant advantage of the methodology is the possibility it creates to perform analyses across the results of several of these knowledge extraction experiments. moreover, the principle of semantic disclosure of results from a computational experiment is not limited to text mining. in principle, it can be applied to any kind of experiment of which the  results can be converted to semantic models, almost as a 'side effect' of the experiment at hand. experimental data is automatically semantically annotated which makes it manageable within the context of its purpose: biological study. we consider this an intuitive and flexible way of enabling the reuse of data. with the use of web services from the aida toolkit and others, we also demonstrated the exploitation of the expertise of computational scientists with diverse backgrounds, i.e. where knowledge sharing takes place at the level of services and qualitative models. we consider the demonstration of e-science and semantic web tools for a personalized approach in the context of scientific communities to be one of the main contributions of our methodology. in summary, the methodology provides a basis for automated support for hypothesis formation in the context of experimental science. future extensions will be driven by biological studies on specific biomolecular mechanisms such as the role of histone modifications in transcription. we also plan to evaluate general strategies for extracting novel ideas from a growing repository of structured knowledge.

competing interests
the authors declare that they have no competing interests.

authors' contributions
marco roos, m. scott marshall, and pieter adriaans conceived the bioaid concept and scenario. marco roos, andrew gibson and m. scott marshall conceived the semantic modeling approach. marco roos created the ontological models and implemented the workflow. m. scott marshall coordinated the development of aida. martijn schuemie, edgar meij, sophia katrenko, and willem van hage and konstantinos krommydas, developed the synonym/uniprot service, the document retrieval service, the protein extraction service, and the semantic repository service respectively. all authors contributed to the overall development of our methodology.

