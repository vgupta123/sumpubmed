BACKGROUND
mass spectrometry  coupled with online separation methods, such as liquid chromatography , is commonly applied for differential and quantitative profiling of biological samples in metabolomic and proteomic research. such approaches are useful in the domains of systems biology, functional genomics, and biomarker discovery. one of the ongoing challenges of such molecular profiling approaches is the development of better data processing methods. several software packages have been developed for this purpose, and have been extensively reviewed by katajamaa and orešič  <cit> .

the recent introduction of mzml, an open and universal format for ms data  <cit> , represents an important milestone in the effort to address the issues of ms data exchange and standardization. it also underlines the need for a flexible and universal software framework to provide the necessary support for data import, export, and visualization, thus allowing the rapid development of specialized data-processing methods.

mzmine was first introduced in  <dig> as an open-source software toolbox for lc-ms data processing  <cit> . the first version of mzmine defined the data analysis workflow and implemented simple methods for data processing and visualization  <cit> . the software has been applied to numerous metabolomic analyses  <cit>  and comparative studies with other related software packages have been performed  <cit> . a weakness of mzmine was insufficient modularity in its initial design, thus limiting the possibility of expanding the software with new methods developed by the scientific community. for this reason, the new release, mzmine  <dig>  was completely redesigned to support modularity. here we describe the architecture of mzmine  <dig> as well as its basic features. we also introduce a new and efficient method for peak list alignment that was implemented in mzmine  <dig> 

implementation
mzmine  <dig> was developed using java technology, and is therefore platform independent. the software has been tested on the windows, mac os x, and linux platforms. we focused on three main aims during the software design and implementation.

first, the framework should be flexible and allow for easy and straightforward development of new data processing modules. we addressed this by keeping a strict separation between the application core and individual modules for data processing and visualization . a compact data model was designed and the code of each java class code was kept short and intuitive. to support the development of new modules, we provided an online tutorial available at the project web site.

second, the graphical interface of the application should be intuitive and easy to use. for this purpose, critical data processing methods such as peak picking were linked to embedded visualization modules, providing online previews during parameter setup. additionally, the use of any data-processing method in mzmine  <dig> does not remove the original  data, giving the user the option to return back to previous results or raw data at any stage of data processing.

the third goal was to provide good support for processing high-resolution ms data, e.g., as obtained from orbitrap or fourier transform ion cyclotron resonance ms instruments. we designed the data import and peak detection modules to maintain the precision of the imported data without any degradation due to inadequate resampling. because the use of high-resolution data suggests an increased data volume, mzmine  <dig> was tested and optimized with large datasets .

the flexibility of the java environment allows mzmine  <dig> to take advantage of several open-source libraries, including jfreechart  for the tic, spectra, 2d and other visualizers, visad  for the 3d visualizer, chemistry development kit   <cit>  for calculating isotopic distributions, jchempaint  for rendering 2d molecular structures, and jmol  for rendering 3d molecular structures. these libraries are included in the mzmine  <dig> distribution.

RESULTS
the typical ms data processing workflow comprises raw data file import, filtering/smoothing , peak picking, peak list deisotoping, alignment, gap filling, and normalization  <cit> . the mzmine  <dig> modules cover all these workflow stages and also include additional functionality for the visualization and interpretation of the results. only features new to mzmine  <dig> are described in this section.

project management
one of the new core features of mzmine  <dig> is project management, which allows the user to track and store intermediate results. each data-processing step can be performed multiple times with different parameters and the results can be observed and compared. the data processing pipeline settings  can be stored for future applications. direct export of the peak list data to comma-separated values  or xml files is also possible.

raw data file format support
mzmine  <dig> can read and process both unit mass resolution and accurate mass resolution ms data in both continuous and centroid modes, including fragmentation  scans. raw data import is modularized and the currently supported file formats are mzml , mzxml , mzdata , netcdf, and raw format used natively by thermo fisher scientific instruments . support for other file formats can be implemented as additional plug-ins.

data visualization
mzmine  <dig> includes several of visualization modules , all of which were newly implemented for this release. following the goal of providing the user with an intuitive interface, the visualizers automatically annotate raw data with the obtained peak picking and identification results, allowing for quick orientation when large amounts of data are being processed.

quantitative results in the form of peak lists may be observed using a table visualizer or chart-plotting modules . the scatter plot visualizer  has proven to be very useful for efficient comparison of multiple samples  <cit> .

peak detection
feature detection is a critical step in ms data processing. the peak detection methods and their implementations should be flexible enough to deal with great differences in data obtained from different instruments, such as variable mass resolution, chromatographic resolution and peak shape, or background noise. in mzmine  <dig>  peak detection is performed in several customizable steps . previews are provided to allow for optimal selection of parameter values.

in the first step , each ms spectrum is processed individually and converted to pairs of m/z and intensity values . several algorithms are provided as plug-ins, each suitable for a different type of mass spectra. the "local maxima" algorithm is a simple algorithm suitable for demonstrating the process: it detects each local maximum in the spectrum. the "recursive threshold" algorithm is based on an earlier method implemented in mzmine  <cit>  and adds two additional parameters of minimum and maximum peak m/z width. this method reduces the false positives by avoiding detection of noise peaks. the "wavelet transform" algorithm is particularly suitable for noisy data. it processes each spectrum using continuous wavelet transform, matching the m/z peaks to the "mexican hat" wavelet model. this algorithm is based on a previously reported method  <cit> . the "exact mass" algorithm assumes high quality spectra  and determines the center of each m/z peak using the "full width at half maximum" paradigm: m/z value is placed in the middle of the line, which crosses the peak at half of the maximum intensity . finally, the "centroid" algorithm is suitable for already centroided data. it detects all data points above the specified noise level as m/z peaks.

data obtained by fourier transform mass spectrometry instruments provide very high mass resolution, but suffer from the presence of noise signals known as "shoulder peaks" . these peaks are residues of the fourier transform function calculated by the instrument and their intensity is usually below 5% of the intensity of the main  m/z peak. to remove these noise peaks, we introduced an optional filtration plug-in that builds a theoretical model  with given mass resolution around each peak, and removes all noise peaks below this model. peaks are processed in the order of decreasing intensity. in the preview , the main m/z signal is indicated by the red color, while the shoulder peaks subject to removal are indicated in yellow. again, it is possible to implement other filtration algorithms as plug-ins.

the next step consists of an algorithm that connects consecutive m/z values spanning over multiple scans into chromatogram objects. the default algorithm provided by mzmine  <dig> connects m/z values in the order of their intensity, with the most intense peaks connected first. a chromatogram spanning a given minimal time range is constructed for each m/z value . each chromatogram is then deconvoluted into individual chromatographic peaks . several algorithms are provided as plug-ins. the "baseline cut-off" algorithm recognizes each chromatographic peak that has an intensity above a given minimum level and spans over a given minimum time range. the "noise amplitude" algorithm adds another parameter specifying the intensity range, which is considered noisy. the algorithm then finds the intensity level where most of the noise is concentrated and sets the baseline level to this intensity, individually for each chromatogram. following the setting of the baseline, the procedure is the same as the "baseline cut-off" algorithm. the savitzky-golay algorithm uses the smoothed second derivative of the chromatogram curve to detect the borders of individual peaks. the "local minimum search" algorithm attempts to identify local minima in the chromatogram as border points between individual peaks. several restrictions are placed on possible peak shapes, such as minimum absolute and relative intensities, or a minimum ratio between peak maximum and edge.

we also implemented an experimental module, which fits the  set of data points of each deconvoluted peak with an ideal peak model such as gaussian or exponentially modified gaussian . such an approach may reduce the chromatographic noise between samples, but the practical applicability of this method has not yet been thoroughly validated.

peak identification
assignment of intuitive metabolite or peptide names to detected m/z values greatly assists with the process of data interpretation. in mzmine  <dig>  identification of peaks can be performed either by searching a custom database of m/z values and retention times, or by connecting to an online resource such as pubchem  <cit> , kegg  <cit> , metlin  <cit> , or hmdb  <cit>  directly from the mzmine  <dig> interface . for each ion subjected to identification, its neutral molecular mass  is calculated from its m/z value. for that purpose, the charge of the ion  can be automatically determined from its isotope pattern. ionization mode  and ionization adduct  are selected by the user as parameters. neutral mass is then calculated as mneutral =  ± madduct, where the sign  is defined by the ionization mode and madduct is the mass of the selected ionization adduct. the neutral mass mneutral is the primary term for database search, within user-specified tolerance. isotopic pattern similarity can be used as a second filter to select optimal candidates, by comparing the ratios of the detected isotopes and matching isotopes from the predicted isotopic pattern of the database compound. because the online identification module is itself modularized, support for other molecular databases can be easily added. for proteomic applications, a module allowing identification of peptide peaks using the mascot  <cit>  search engine and ms/ms spectra is under development.

random sample consensus  aligner
the purpose of peak list alignment is to match relevant peaks across multiple samples. the original mzmine software introduced a simple alignment algorithm that first creates an empty master peak list and then aligns each peak from given peak lists  to the best candidate of the master list using a two-dimensional alignment window  represented by user-specified m/z and retention time tolerances. if no suitable candidate is found, a new row is created in the master list. in mzmine  <dig>  this algorithm is referred to as the "join aligner". one disadvantage of the join aligner is the inability to cope with a non-linear deviation of the retention times among samples. for this purpose, we introduced a new peak list alignment method based on the ransac algorithm.

the ransac algorithm  <cit>  is a non-deterministic iterative algorithm that estimates parameters of a mathematical model from a set of observed data, which may include outliers. the probability of obtaining a good result increases with the number of iterations. in each iteration, a random subset of observed data points is selected and a model is fit to this data. in our specific case, we used  <dig> points to find a non-linear model. the remaining data is tested against the fitted model and if a value fits well, it is considered a part of the model. finally, the model is evaluated and when the iteration is finished, the model with the most data points fitted to it is considered the best.

the ransac method of alignment makes use of two user-defined two-dimensional windows, the ransac window  and alignment window , respectively. the rw is defined by the m/z threshold rm <dig> and retention time threshold rr <dig>  and aw constitutes the same m/z threshold rm <dig> but a different retention time threshold ar <dig>  the retention time threshold in rw should be as big as the maximum observed deviation in the retention time among all peaks. the procedure for aligning a sample sj with the master list l is as follows:

step 1: for every row i in l, let

ri = the average retention time of all individual peaks in the row

mi = average m/z of all individual peaks in the row

rwi = , the ransac window for row i.

then, for row i in l, mark all peaks in sample sj in rwi as candidate alignments.

step 2: build a scatter plot representation of all candidate alignments, and apply the ransac algorithm to build a candidate model for alignment. this model represents a list of matching retention times.

step 3: apply the locally-weighted scatterplot smoothing  method for regression  <cit>  on all points in the model obtained with ransac.

step 4: using this regression model, for each row i in l, predict the correction for the retention time shift to locate the new center  of the alignment window awi. ransac alignment can correct the retention time deviation by centering the position of the aw to the correct position in the new sample.

thus, the alignment window awi = 

step 5: for each row i in l, apply the join algorithm for alignment using the alignment window awi.

ransac aligner performance
two types of errors can be introduced during the alignment process  <cit> . either two non-related peaks could be matched, or the matching of two related peaks could be omitted. a variable called "precision" represents the proportion of true alignments out of all alignments found by the algorithm. the proportion of peaks that are correctly aligned by the algorithm out of all true alignments inside the dataset is called "recall". these two variables together represent the quality of the alignment. to test whether the newly introduced ransac algorithm performs better than the join alignment, the results of two different approaches were compared.

first,  <dig> synthetic datasets were created using samples from  <dig> different lipidomic studies. a single sample from each study was used as a seed to create a synthetic set of  <dig> samples. these  <dig> samples contained identical information , but a random non-linear deviation in the retention time was introduced into each one. the mzmine  <dig> projects of all  <dig> datasets are available on-line . each dataset was aligned using the ransac aligner and join aligner with three different retention time tolerance thresholds . parameters used for alignment are specified in table  <dig>  run times of the ransac aligner were measured and are reported in table  <dig>  precision and recall values were calculated and the average results are shown in figure  <dig> . only the use of the ransac algorithm achieved 100% in both precision and recall performance on these synthetic data sets.

run times were obtained on an amd opteron  <dig>  ghz dual-core system with  <dig> gb ram, running linux.

our second approach for the comparison was to use the real proteomic  and metabolomic  datasets introduced by lange et al.  <cit> , together with their tables of "ground truth" alignments and an evaluation script for calculating the alignment precision and recall values. we applied the mzmine  <dig> join and ransac aligners to align all the datasets with the parameters specified in table  <dig>  run times of the ransac aligner are reported in table  <dig>  precision and recall values were calculated using the provided evaluation script and compared to already published results in table  <dig>  we used the latest available evaluation results published at http://msbi.ipb-halle.de/msbi/caap at the time of writing. compared to the join aligner, the ransac aligner provided better results in  <dig> of  <dig> alignments, with worse results obtained in only a single case . we assume that the high number of features in this fraction  made it somewhat difficult for the ransac algorithm to build a suitable model. notably, in all fractions of dataset p <dig>  the ransac aligner provided the best results among all the tested algorithms. complete datasets p <dig>  p <dig>  m <dig>  and m <dig>  as well as all alignment results, are available online .

CONCLUSIONS
the development of mzmine  <dig> was motivated by the need for a flexible and modular software platform that would allow the bioinformatic and analytical community to contribute new methods for specific stages of ms-based data processing. great emphasis was placed on achieving the three main goals of a flexible, extendable, and modular design; user-friendly graphic interface; and good support for high-resolution ms data. the authors of this manuscript work in the field of metabolomics utilizing an lc-ms analytical platform, and therefore the currently developed modules were tested mainly on lc-ms data. the flexibility of mzmine  <dig>  however, allows for easy expansion to other dataset types such as gas chromatography-ms, as well as interoperation with popular proteomics search engines such as mascot.

several other software packages have been introduced for lc-ms based data processing, such as xcms <dig>  <cit> , trans proteomic pipeline  <cit> , trequips  <cit> , openms-topp  <cit> , and proteowizard  <cit> . none of these tools, however, share the same goals with mzmine  <dig>  most of them being command-line oriented with fixed feature sets, aiming specifically for either proteomic or metabolomic research. rather then a single piece of software, the developmental aim of mzmine  <dig> is to create a universal platform through which researchers can contribute individual processing modules and implement and share novel ideas, spanning over multiple research fields and analytical methods.

mzmine  <dig> is available for download at the project www site, together with a printable manual, an animated tutorial, a module development tutorial, and further relevant project information such as a source code repository and developers' mailing list. the current version of the framework is already suitable for processing large batches of data, both for targeted and/or non-targeted analyses, and has been applied in metabolomic research  <cit> .

dataset download
the data associated with this manuscript may be downloaded from proteomecommons.org tranche using the following hash:  

the hash may be used to validate the files were published as part of this manuscript's dataset, and to check that the data have not changed since publication.

availability and requirements
• project name: mzmine 2

• project home page: http://mzmine.sourceforge.net

• operating system: platform independent

• programming language: java

• other requirements: java runtime environment   <dig> , java3d

• license: gnu gpl

abbreviations
aw: alignment window; lc-ms: liquid chromatography-mass spectrometry; ms: mass spectrometry; ransac: random sample consensus; rw: ransac window

authors' contributions
tp designed the data model and overall architecture of the mzmine  <dig> framework and implemented most of the raw data visualization and peak identification modules. sc implemented the project serialization and ransac aligner. avb implemented the peak detection module with previews, scatter plot and histogram visualizers, and isotope pattern support, and contributed to the online database search module development. mo participated in software testing and provided feedback on the framework design. all authors read and approved the final manuscript.

supplementary material
additional file 1
numerical values for figure  <dig>  precision and recall values of ransac and join aligner results for  <dig> synthetic data sets.

click here for file

 acknowledgements
we thank the present and past mzmine  <dig> contributors mikko katajamaa, yosuke kawasaki, jarkko miettinen, john rush, marco schaerfke, and sasha tkachev. we also thank the okinawa institute of science and technology promotion corporation for providing the funding and mitsuhiro yanagida for supporting the mzmine  <dig> development in his laboratory. we are very grateful to the developers of open-source libraries such as jfreechart, visad, jmol, and cdk. this work was in part supported by the eu-funded project etherpaths .
