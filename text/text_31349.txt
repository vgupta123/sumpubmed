BACKGROUND
industrial biotechnology exploits processes that use living cells, for instance yeast and various bacteria, to produce products like fine chemicals, active pharmaceutical ingredients, enzymes, and biofuels. the use of living material in manufacturing processes makes the processes challenging to develop and control. because of the complexity of these tasks, computational modeling and data analysis are used to improve the yield, reproducibility and robustness in bioprocesses. on the other hand, the regulatory demands on pharmaceutical manufacturing processes are increasing and, for example, the united states food and drug administration emphasize the importance of model-aided process development in its process analytical technology  initiative  <cit> . one of the important steps in process development is maximizing the product yield. in practice, the process optimization includes  identifying the process parameters that have most impact to the product yield and,  determining their optimal values. this data analysis task includes few features that are specific to the application area. for example, the number of process parameters  may be large with respect to the number of samples, the predictors may contain either numerical or categorical values, the datasets may contain missing values and, finally, the relationship among the predictors and product yield may be non-linear.

to build a model for data analysis requires selection of important features while leaving out the rest. several feature selection methods have been proposed but the results tend to vary, as generalization of the solution is problematic. typical issues are data redundancy, outliers and feature dependencies  <cit> .

methods
in this work, we have used three alternative approaches to model bioprocess data: multiple linear regression, regularized regression and random forests. the analyses were performed using matlab  <cit>  and rf-ace tool  <cit> .

multiple linear regression
in multiple linear regression, the response variable is modeled as a linear combination of multiple predictor variables. the general model can be expressed as

  y=β0+a1β1+a2β2+a3β3+…+apβp 

where y is the response variable, and ai and βi  are the predictor variables and their coefficients, respectively. the intercept is represented by β <dig>  alternatively, equation  can be represented in vector notation by y = hθ, where h is augmented predictor vector given as  and θ is the parameter vector.

in spite of being linear with respect to the predictor variables, multiple linear regression models fail to incorporate the underlying non-linear relationships, if it exists, between the predictors and the response variable. however, the model restricts only the coefficients to be linearly related, while the predictor variables can be non-linear. this gives a provision of including additional non-linearly transformed predictor variables in the linear regression modeling. the advantage of using such variables in regression analysis is that the non-linear behavior in data and interaction between different variables are incorporated while the model remains linear and easily interpretable. this is a typical procedure applied in traditional response surface modeling when constructing models with quadratic terms and interactions of terms. increasing the number of parameters in this way, however, causes high-dimensional predictor vector which results in over-fitting and the loss of generality. moreover, if the number of samples is small, increasing the parameter vector size by these transformations may cause rank deficiency or multicollinearity of the prediction vector. in such cases, standard regression modeling may either fail, rank deficiency may cause non-invertible matrix thus making parameter estimation difficult, or the estimates it gives for parameter vector are prone to give low prediction accuracy. hence, regularization is a key process in solving such cases. it produces a sparse parameter vector and also shrinks the coefficients towards zero as well as towards each other  <cit> .

regularized regression
the research on sparse and regularized solutions has gained increasing interest during the last ten years  <cit> . this is partly due to advances in measurement technologies, e.g., in molecular biology, where high-throughput technologies allow simultaneous measurement of tens of thousands of variables. however, the measurements are expensive, so typically the number of data points is small. in the field of bioprocess development, the number of variables is not that large but yet enough to hinder the use of many standard data analysis methods. conventional regression and classification methods are unable to process data with more predictor variables than samples . regularization methods help in defining a unique solution in this ill-posed problem. these methods shrink some of the coefficients to zero. this not only helps in feature selection but also decreases the variance at the cost of a small increase in bias. however, this has the effect of improving the generalization of the estimate.

in regularized regression, a penalty on the size of the coefficients is added to the error function. least absolute shrinkage and selection operator   <cit>  is one such technique which uses the l <dig> norm of the coefficients as the penalty term to produce sparse solutions, i.e., prediction models with several coefficients equal to zero. since variables with zero coefficients are not used, this procedure essentially acts as an embedded feature selection.

from the description of equation , the l <dig> penalized coefficient vector for our linear model is defined as

  θ^=||y-hθ||22+λ||θ|| <dig> 

where lambda  is the regularization parameter, ||θ || <dig> is the l1-norm of the parameter vector. there exist efficient algorithms for finding solutions for different values of regularization parameters  <cit> .

the result of the regularized regression is quite sensitive to the selection of the parameter λ. in order to appropriately assess the performance, the selection has to be done based on data. the usual approach is to estimate the performance with different λ using a cross-validation approach. since we also use cross-validation for estimating the performance of the overall method , this results in two nested cross-validation loops, one for model selection and one for error estimation. more specifically, the outer loop is used for estimating the performance for new data, while the inner loop is used for selection of λ.

random forests
decision trees have been studied for decades as a model for various prediction problems. the tree can be either a classification tree or a regression tree, and a common term including both is classification and regression tree . a decision tree is a hierarchical structure, which decides the class  or the predicted output  by hierarchically comparing feature values with a selected threshold, thus producing a hierarchy of if-then rules. such combination of rules is most conveniently expressed as a tree, where each input feature comparison corresponds to a node in the tree. eventually, the leaves of the tree describe the actual output value.

the decision trees can be learned from the data, and the usual approach is to add nodes using a top-down greedy algorithm. in essence, this means dividing the search space into rectangular regions according to the splitting points. the drawback of decision tree is that they are very prone to overlearning. this is one reason why regression trees have later been extended to random forests  <cit> , whose prediction is obtained by averaging the outputs of a large number of regression trees. due to averaging, random forests are tolerant to overlearning, a typical phenomenon in high-dimensional settings with small sample size, and have thus gained popularity in classification and regression tasks especially in the area of bioinformatics.

in our experiments, we use the rf-ace implementation in  <cit> . this implementation is very fast and it takes advantage of the random forest with artificial ensembles  algorithm, which enables both feature ranking and model construction. in our approach, a set of significant features was first selected from the experimental data using the rf-ace tool. then, a model was constructed using the given data.

experimental data
in order to test our modeling methodology we examined a dataset produced in a study related to culture media optimization . there, an enriched mixed microbial consortium was used in the bioconversion of crude glycerol to hydrogen, and the process was optimized in serum bottles by optimization of media components. the concentrations of five media components  were varied with the help of statistical design of experiments , and the resulting hydrogen production was measured . the data was modeled using first and second order polynomials in multiple linear regression. this data containing  <dig> samples is a typical data set produced during bioprocess modeling and optimization. multiple linear regression is a useful tool for modeling the data from individual designs of the study but other methods are needed in order to model the entire data set at once.

visualization and validation of models
in order to provide an overview to the models and the experimental data, visual representations were produced for the regularized regression model and the random forest model. since visualization of the high dimensional variable space  is not feasible, the variables are visualized pair-wise. the values of remaining variables  are set in their average values calculated from the data. in addition, each model is assessed with leave-one-out  cross validation technique which estimates the accuracy of the predictions in an independent dataset.

RESULTS
in our case study, we used multiple linear regression, regularized regression and random forests to predict the yield of hydrogen production. the performance of each method is evaluated by original dataset as well as transformed dataset with pairwise interactions and quadratic forms. therefore, the original dataset contains  <dig> variables while the transformed dataset contains  <dig> variables.

yield prediction using multiple linear regression
multiple linear regression is used with and without non-linearly transformed predictor variables to model the response variable. without the transformed predictors, i.e., the simple model, the estimated correlation value  was  <dig> . however, using the transformed polynomial model the estimate for correlation decreased to a very low value of  <dig>  and resulted in an insignificant model. this is mainly due to the aforementioned shortcomings of the multiple linear regression. it basically over-fits the model to the training samples and thus produces less accurate estimates for unseen data samples. table s <dig> lists the model coefficients for the transformed polynomial regression model . it can be noted that zero entries have been inserted to remove linearly dependent observations.

yield prediction using regularized regression
first, we evaluated the simple model without the transformed variables. in this case, the parameter λ for the regularized regression is chosen by both manual selection and proper cross validation. in other words, we wanted to see if the results improve by manually selecting the lambda value optimally for each loo cross validation fold. although this is not possible in practical applications, it may give insight on the efficiency of parameter selection using cross-validation with small sample size, and on the general applicability of a linear model for our problem.

as a result, the loo correlation estimate becomes  <dig>  with manual selection instead of  <dig>  using proper cross-validation. the large gap between optimal and estimated correlation is at least in part due to the inaccuracy of the cross-validation type error estimators with small sample size; see, e.g.,  <cit> .

in the case of transformed polynomial regression model, the estimated value for correlation was found to be  <dig>  which is higher than the case of the simple model. this clearly indicates the non-linear behavior of the original dataset. table s <dig> shows the resulting coefficients in the constructed model where regularization has forced  <dig> out of  <dig> coefficients to zero . although, the same number of non-zero coefficients were obtained from the multiple linear regression as well but the main difference is the regularized coefficients. that is, the non-zero coefficients from regularized regression were also shrunk towards zero. this results in generalized models with higher overall prediction accuracy  <cit> . the yield predictions are visualized in figure  <dig> as a response surface. in addition, the significant variables for the model and their corresponding coefficients are listed in table  <dig> 

yield prediction using random forests
the rf-ace tool  <cit>  is used to build the random forests model. in our experiment, the type of the forest, the number of trees in the forest, and the fraction of randomly drawn features per node split are set to "rf",  <dig>  and  <dig>  respectively. all other parameters were kept to their default values. the results indicated that all variables were significant in the model. the yield predictions of the constructed model are visualized in figure  <dig>  in the accuracy examination, the rf-ace model resulted in correlation of  <dig>  . the capability of modeling non-linear relationships is the primary reason for high prediction accuracy in the constructed model. on the other hand, the model provided correlation value of  <dig>  if the variable transformations were used as additional predictor variables. eventually, the increase is quite small, and may thus be a due to random fluctuation.

method comparison
both regularized regression with transformed variables and random forests produced results that are useful in bioprocess data mining. in particular, both methods determined all the variables significant and can be used to determine an advantageous control direction for them. the most notable difference in the results is the linearity that was in use in the regularized regression versus the nonlinearity that is inherent in random forests . simple linear models cannot fit to the nonlinearity of the data and, thus, the maximum response cannot be detected inside the examined space although it would be located in there. however, regularized linear regression with transformed variables was found successful in modeling the nonlinearity of the data to some extent. on the other hand, the random forest model is able to capture the nonlinearity. here, the maximum response was determined approximately at the same point as in the media optimization study performed using the methods of statistical design of experiments.

the loo estimates for correlation ascertain that the rf-ace provides a more accurate solution than the regularized regression. this, however, should not totally renounce the idea of using regularized regression as it mainly proves its worth in more complicated and high-dimensional data analysis. moreover, linear regression has a useful feature of producing easily interpretable models and, on the other hand, the models are capable in producing predictions beyond the already examined parameter space.

CONCLUSIONS
in this study, we applied two novel data analysis methods  in bioprocess data mining and compared them to multiple linear regression that is commonly applied in relation to statistical design of experiments. both of the studied methods were able to produce models that fit to the examined data. in particular, the non-linearity of the data was well modeled by random forests. this property is very valuable in data mining of multiple integrated data sets. as the results demonstrated, traditionally used multiple linear regression does not perform satisfactorily in nonlinear input-output relations. the traditional approach using the first and the second order polynomial models would face further problems if the data was multimodal. in the future, it would be of interest to further study regularized regression and random forests in bioprocess data mining. this could mean, for example, the inclusion of categorical variables in the data and studies with different types of bioprocesses.

competing interests
the authors declare that they have no competing interests.

authors' contributions
ssh and mf made substantial contribution in writing the manuscript, interpretation of the data, and design and analysis of the models. rm was responsible in acquisition of the data. ta and hh contributed to the design of the study, and in writing and revising the manuscript.

supplementary material
additional file 1
as pdf - table s1: significant coefficient values in different methods using transformed data. this file contains a table describing the coefficient values generated by lasso and multiple linear regression methods for the transformed dataset. here, the coefficient β <dig> represents the intercept, β <dig> corresponds to variable nh4cl, β <dig> to k2hpo <dig>  β <dig> to kh2po <dig>  β <dig> to mgcl <dig> h2o and β <dig> to kcl, respectively.

click here for file

 acknowledgements
the authors thank the academy of finland, project "butanol from sustainable sources" , for funding the study.

declarations
the publication cost for this work was supported by the academy of finland, project "butanol from sustainable sources" .

this article has been published as part of bmc systems biology volume  <dig> supplement  <dig>  2013: selected articles from the 10th international workshop on computational systems biology  2013: systems biology. the full contents of the supplement are available online at http://www.biomedcentral.com/bmcsystbiol/supplements/7/s <dig> 
