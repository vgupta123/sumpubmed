BACKGROUND
analysis of single nucleotide polymorphisms  in the dna of unrelated individuals revealed a block-like structure of haplotype variation along the human genome. using the first available genome-wide data of snps on chromosome  <dig>  patil et al.  <cit>  showed that in particular regions on the chromosome, the observed diversity of snp haplotypes is less than the expected. almost at the same time, a similar structure in haplotypes within a region of  <dig> snps on chromosome region 5q <dig> was reported by daly et al.  <cit> . in the latter study, a block structure of haplotypes was revealed using a hidden markov model for estimating recombination rates. this approach, unlike models based on haplotype diversity, incorporated a quantity measuring linkage disequilibrium  between pairs of snps.

it is well known that effects such as population bottlenecks, geographic isolation, and natural selection can increase the extent of linkage disequilibrium in genomes  <cit> . however, in established large populations under random mating, haplotype blocks reflect mutation events in the ancestors of that population and recombination events along the chromosomes. increased frequency of recombination events are likely to create genomic regions with haplotype blocks of small size. based on hapmap data, myers et al.  <cit>  identified short genomic regions within the human genome in which the recombination rates are orders of magnitude higher than background levels. such regions are called "recombination hotspots". identification of hotspots of recombination and estimation of rates of crossover therein are important issues  <cit> .

however, existing approaches that address these issues are generally complicated and computationally intensive. haplotype blocks can rapidly provide rough estimations about hotspots of recombination. knowledge of haplotype blocks has other applications as well. the block structure of chromosomes can be used in statistical approaches aimed at achieving better understanding of genetic features that differentiate ethnic groups  <cit> . the phasing problem in genotype analysis is typically solved by computational methods; one can reduce the computational cost by breaking the input genotype data into smaller units along boundaries of haplotype blocks. as a consequence, block partitioning is performed prior to phasing and other analysis in genomic sequence-based endeavors, including those incorporated in haploview  <cit> . similarly, zhao et al.  <cit>  proposed the use of block partition inside the pl-em algorithm for the haplotype inference problem  <cit> , as a substitute for the common pl technique.

one of the most important applications of snp haplotype data is in regard to identification of disease causing genes. state-of-the-art sequencing technologies that provide large volume snp data along with efficient statistical analyses; have made the use of haplotype data for gene identification a realistic goal. these statistical analyses usually incorporate time demanding reiterative tasks on large data sets. clearly, the reduction of data volume by making use of haplotype blocks allows for more rapid and efficient analyses. b. browning and s. browning  <cit>  presented a method for disease association studies based on haplotype blocks that relies on pairwise association between snps. other approaches in disease association studies require samples of limited haplotype diversity  <cit> .

in case-control association studies designed to identify disease causing genes, one of two strongly associated snps can be used as a "proxy" for testing association of the trait with the other. grouping such associated "proxy" snps together compresses information to be used for case-control studies. many strategies have been proposed to conduct case-control studies in an economic manner  <cit> . the key idea shared by all these approaches is that essentially all important information derived by analysis of association tests between each individual snps and phenotype of interest can be derived by analysis of a subset of snps, called tagsnps. the most widely used method for tagsnp selection has been introduced by carlson et al.  <cit> . applying a threshold on r <dig>  in each iteration of carlson's method, the snp included in the largest number of associated pairs is selected as a tagsnp.

another common approach is in identification of haplotype-tagging snps . here, the goal is to find the smallest subset of the snps  such that any single nucleotide difference between any two distinct haplotypes can be captured by an htsnp. ding et al.  <cit>  have shown that htsnp selection is notably effected by the manner in which haplotype blocks are defined. zhang et al.  <cit>  have incorporated several criteria for "candidate block" identification into their dynamic programming algorithm for finding the minimum number of htsnps in an entire chromosome. while there are different criteria to limit regions of the genome as "potential blocks", most of them can be classified as measures of haplotype frequency or measures of snp pair association . these two measures may seem equivalent at first sight, but in fact they quantify different features of the data. for example, "common haplotypes" feature haplotype frequencies, whereas |d'| quantifies allelic association of snp pairs. in theory, and even in practice, it is possible that values of these two measures derived from the same data will define different haplotype blocks  <cit> . however, the extent to which haplotype diversity is captured when blocks are defined on the basis of association of snp pairs remains to be established.

* the approach of the original method is recognized as a local partitioning. ** global partitioning for maximal association pairs

in this work, we discuss on a haplotype block partitioning that is based on pairwise association of snps. there are several statistics for determining allelic correlation between snps. two well known statistics are d and r <dig> which, respectively, represent the sample covariance and the sample correlation coefficient of two bi-allelic markers. theoretical distributions of these statistics have been well studied. for instance, it is known that nr <dig> is asymptotically chi-squared  <cit> , where n is the sample size. these two statistics depend on marginal frequencies of two snps. d', a third measure of linkage disequilibrium introduced by lewontin  <cit> , is preferred by some researchers. d' is a normalized statistic with respect to marginal frequencies and varies linearly between  <dig> and  <dig> as a function of the forth gamete frequency.

in fact, although the point estimate of d' is independent of sample size, its distribution under the hypothesis of independence is a function of sample size. to measure the significance of d', gabriel et al.  <cit>  suggested the use of an interval estimate. here, we suggest that p-values as derived from the fischer's exact test be used to assess the significance of d'. with respect to disease association studies, wherein this test is used to assess phenotype-snp association, it seems reasonable to use the same test for snp-snp association.

haplotype blocks can alternatively produce "local partitioning" and "global partitioning". in local partitioning, haplotype blocks are defined independently from the configuration of other haplotype blocks through the genome. usually non-contiguous blocks are produced that look like a series of "islands" within the genome. in contrast, in global partitioning, the aim is to split the genome into haplotype blocks; the entire genome is "tiled" meaning all regions of the genome are within a block. here, no single definition is applicable to all the blocks. to the best of our knowledge, little effort has been made to incorporate a pairwise measure of snps into a global block partitioning method.

in an earlier effort to discuss how well ld patterns are consistent with block boundaries, wall and pritchard have evaluated three measures, namely "coverage", "absence of holes", and "non-overlapping blocks"  <cit> . "coverage" refers to the portion of each chromosome which is covered by haplotype blocks. two haplotype blocks overlap if their boundaries cross each other. a "hole" in a haplotype block occurs where a snp is not in strong ld with any of other snps in the block. these features have been assessed only for the haplotype blocks produced by gabriel's method. obviously, all global partitioning approaches produce non-overlapping blocks which together completely cover the genome.

the block partitioning approach we present in this work consists of two steps. first, an association index is derived that characterizes each pair of snps as "associated", "independent", or "not-statistically-significant". then, haplotype blocks are determined such that the number of associated snp pairs within each block is maximized, while a limitation is set on fraction of acceptable independent snp pairs. an iterating search algorithm is used to find the solution of the constrained optimization. the method produces global partitioning of chromosomes. our method results in complete coverage, no overlapping, and "absence of holes" in blocks.

having applied the method, we comprehensively compare its performance with some previously reported methods for haplotype block designation. robustness of each method is assessed by evaluating the consistency of block boundaries on permuted samples. we also assess the potentials of block structures created by each method to serve as a reference block structure for genome-wide disease association studies. in this regard, they are assessed under conditions of use of different marker density. finally, we evaluate whether our haplotype block partitioning method can be used to detect recombination hotspots in the genome.

methods
association test for snp pairs
in this section, we discuss on the use of fisher's exact test on quantifying the concept of "strongly associated" snps. the fisher's exact test is a test of independence, mostly used on  <dig> ×  <dig> contingency tables  <cit> . despite tests based on r <dig> which are approximately assessed by chi-square distribution, fisher's exact test provides an exact probability of rejecting the null hypothesis , observing the samples. there are many concrete applications of fisher's exact test and its related statistics in molecular biology and genetics  <cit> .

assume that n samples of l bi-allelic sites are available. we assign a 0/ <dig> random variable to each snp.

let  be the joint random variables denoting snp pair . the statistic of fisher's exact test is defined by

  

where nab is the number of outcomes of ab for xixj, n1· = n <dig> + n <dig> and n· <dig> = n <dig> + n <dig>  in this context, it can be simply shown that fex depends only on n <dig>  n1· and n·1; i.e. fex = fex.

we apply one-tailed p-value with mid-p correction to measure the significance of independence tests  <cit> . to do so, we separate the probability space of the null hypothesis into two subspaces of positively and negatively co-varying samples which correspond to d >  <dig> and d <  <dig>  respectively. the one-tailed p-value subjected to this assumption can be defined by

  

where nmax= argmaxi fex and corresponds to the most balanced contingency table in which the sign of d changes.

we speed up the computation of p-value of association tests by establishing a table of precomputed p-values for every triple , for each n1· =  <dig> ..., ⌊n/2⌋, n· <dig> =  <dig> ..., n1·, and n <dig> =  <dig> ..., n· <dig>  by table look-up and taking the symmetry into account, p-values of association tests of all snp pairs in the genomic region of interest are obtained.

characterizing snp pairs based on association test
obviously, the significance increases when the size of sample is increased. in other words, when the sample size is increased then interval estimates become shorter as the p-value becomes smaller. taking these into consideration, we classify all snp pairs into three classes; "associated", "independent", and "not-statistically-significant". for given predetermined  and p <dig> we define those pairs with |d'| < as independent and other pairs as associated but if |d'| >  and p-valone-tailed > p <dig>  simultaneously, we count a not-statistically-significant snp pair.  is the least value of all |d'| for which snp pairs could be assumed as associated and p <dig> is the level of significance of the test of independence. choosing the proper value for  essentially depends on genetic features of population. however, choosing a stringent cut-off for p-valone-tailed makes the choice of  less strict.  =  <dig>  and p <dig> =  <dig>  are our default setting for these thresholds.

maximizing associated snp pairs subject to limited independent pairs
given haplotype samples, all pairs of snps are classified as associated, independent, and not-statistically-significant pairs in the whole region. it is usually convenient to avoid extra computation by setting a maximum physical distance above which no linkage is assumed between snps. for instance, markers  <dig> kb from each other are usually assumed independent by some researchers. toward an objective definition for haplotype blocks, we assume that in a population away from genetic drift, selection force, and migration, a haplotype block eventually determines two boundaries on the genome within which every snp pair is in "association". a similar idea has been recently considered by pattaro et al.  <cit> , though in their own approach, a likelihood model for the ld pattern in haplotype blocks is introduced in which two distinct distributions model independent and associated snps, separately.

many snp pairs may be identified as independent pairs in a block, basically because not all existing variations are available and the limited samples from the population haplotypes may not be adequate to estimate the real situation. therefore, we model the problem as finding a block partitioning such that the most possible number of associated snp pairs are included in blocks while independent snp pairs within blocks are kept limited.

like other multi-objective problems, there is a trade-off between achieving blocks including maximum associated snp pairs and blocks with minimum independent snp pairs. the former suggests haplotypes as large as whole chromosome while the latter results in single snp blocks. to address this issue, we model the problem using a constrained optimization in which both objectives are involved. formally, we define the problem as,

  

where a and b are the numbers of independent and associated snp pairs in the genome segment between snp a and snp b, respectively. the maximization is taken over all partitioning sets such that  <dig> = s <dig> <s <dig> < ⋯ <sk = l are  indices of snps at left edges of  k blocks. nind is the number of independent snp pairs in entire genomic region and α is an arbitrary constant between zero and one that denotes the largest tolerable fraction of independent pairs in blocks.

to solve the proposed problem, we convert the constrained optimization problem to an unconstrained problem by using a lagrange multiplier, as follows,

  

where λ is an unknown real positive parameter associated with α. given a fixed value for the lagrange multiplier, the reduced problem can be solved by a conventional dynamic programming approach as,

  

where s = b - λa is the score of the genomic interval ending at snp i and consisting of d snps and sopt is the score of optimum block partitioning for i leading snps. arbitrarily, the maximum number of snps within a block can be set to w. to obtain a proper value for lagrange multiplier, we apply a binary search procedure in which reduced problem with respect to different values for λ is iteratively solved until the desired constraint on the number of independent snp pairs within blocks is satisfied. in general, increasing λ decreases the sum of independent snp pairs included in blocks. in our experience and when α =  <dig> , the lagrange multiplier is obtained by about  <dig> iterations.

an alternative algorithm based on gabriel's index
in fact, our approach can be applied to improve any method that introduces haplotype blocks based on some pairwise index for snps. for instance, gabriel's method  <cit>  also introduces a three state index for snp pairs based on the confidence interval of d'. none of the previous methods in this category incorporates any global optimization on block partitioning.

we have developed other haplotype block partitioning using the above optimization scheme substituting the gabriel's index as snp pair characterization. both varieties of our method, one based on the association index derived from the fisher's exact test and the other based on gabriel's index have been incorporated in our extension to the open source widely accessed software, haploview ver.  <dig>  the software is running under java and is publicly available via .

to deal with unphased genotype data, our method is assisted by the "two loci genotypes" phasing approach as implemented by  <cit>  in haploview. this approach is a simplified em algorithm to infer frequencies of four possible alleles on two loci. detailed formulas for this preprocessing can be found in  <cit> .

method comparison
we compare our proposed algorithm with other methods of haplotype block partitioning based on some descriptive aspects of haplotype blocks, performance on a block-based case-control study, and detecting recombination hotspots. in addition to the new methods introduced in the present paper, we choose six other available haplotype block partitioning algorithms. table  <dig> summarizes the main features of these methods in our trial.

in table  <dig>  hot is an exception. it has been developed as a method for inferring recombination "hotspots" throughout the human genome. however, a region between two consecutive hotspots can also be considered a haplotype block because the recombination level in the region is relatively low. in addition, comparison between other methods and hot may suggest clues on extending application of block partitioning methods to recombination hotspot detection.

there is another method, mb, which has not been explicitly introduced in the literature. it is a special option in the hapblock software in which for each block, exactly one tagsnp is assumed. therefore, this algorithm finds the least possible number of haplotype blocks covering the genome while satisfying the haplotype diversity criterion.

general aspects of haplotype block partitioning
our first study concerns examining some general aspects of haplotype blocks in a real sample of haplotypes. we obtained the haplotype sample of the ceu population from hapmap database, release  <dig>  on ten encode regions. through the hapmap project  <cit> , dense genotype data for encode regions have been published. these ten regions have been selected by the encyclopedia of dna elements project  <cit>  as the pilot phase to identify the functional elements of human genome. table  <dig>  adapted from the hapmap website, summarizes the genomic information of ten encode regions and the number of assayed snps in ceu panel by hapmap.

adapted from  .

* genotyped snps in ceu panel.

there are about  <dig> snps assayed in each encode region in the ceu panel. however, we reduced snps to those which are commonly ascertained for all three hapmap panels, ceu, yri, and jpt+chb. moreover, for each region, we drew out the top  <dig> snps ordered by heterozygosity out of the whole region. to do so, we divided the region of interest into  <dig> equal-length subintervals and then for each one, we picked the  <dig> most heterozygous snps from the snps shared in all panels. therefore, a nearly uniform distribution of the most "informative" snps was obtained. this preliminary reduction was necessary for some of the block partitioning methods, as they can not achieve the result for a huge sample size in a reasonable time. we apply all methods of table  <dig> for block partitioning to these data. in our study, we first examine the resulting haplotype blocks for haplotype diversity and htsnp coverage.

to measure haplotype diversity, we apply a clustering approach that is a simple generalization of the commonly used definition of "common haplotypes" introduced by patil et al.  <cit> . for each block, we group haplotypes into the same cluster such that every two members differ in at most four percent of snps. this fine, yet nonzero tolerance, resolves the ill effects of random noises and/or wrongly-assayed snps in estimation of the haplotype diversity in long length blocks. the clusters with six or more haplotypes are considered as non-occasional clusters and indicate significant polymorphisms in the population. by common haplotype coverage, we mean the fraction of whole sample which belongs to any non-occasional cluster.

the consistency of haplotype blocks with the pattern of ld would be also appealing. intuitively, a hole in a haplotype block is where an snp has no significant association with other snps of the same block  <cit> . in a similar way, we call cases of snps that are in strong association with snps of other blocks islands. precisely, we count an snp as a hole if its intra-block average |d'| is less than  <dig>  and as an island if its inter-block average |d'| is greater than  <dig> .

it is a common question to what extent different methods would recognize similar haplotype blocks. the usual approach considers the sum of distances between "corresponding" boundaries in two different block partitionings. while it seems an intuitive measure to find block similarities but determining which boundaries are in correspondness does not have a straightforward rule, because two different partitioning configurations usually have different number of blocks and block boundaries might be often far apart from each other. in such cases, we propose another similarity measure by the following expression,

  

where l is the number of snps in the whole region, k is the number of blocks of p <dig> ∪ p <dig>  and l is the number of snps in each block of the union partitioning. this measure shows the fraction of snp pairs which are commonly included by both p <dig> and p <dig> 

robustness of block partitioning methods
a simplified explanation for existence of blocks is that recombination events in ancestral generations predominantly occurred at block boundaries, and not within blocks. as such, observed block boundaries may be taken as hotspots of recombination. based on this model, a robust block partitioning algorithm will define the same block boundaries whether applied to data of an ancestral generation or to data of a recent generation. the preservation of boundaries by various block partitioning methods can be checked by comparing the boundaries produced at generation one and boundaries produced some generations later . for this purpose,  <dig> hapmap 9q <dig>  haplotypes were followed by simulation through ten generations, assuming crossover probability of  <dig>  at the boundaries per generation and a fixed population size. this process was repeated  <dig> times for each method and the configuration of blocks obtained in each iteration was recorded to assess robustness of block partitioning algorithms.

the htsnp coverage
to obtain the tagging snps that are required to describe all haplotype variations within a block, we employed the htsnper software  <cit> . the htsnper software incorporates an efficient branch and bound algorithm to find the exact solution of the minimum htsnp selection problem. we used default settings of htsnper, i.e. htsnps were defined to cover 80% "common haplotypes" and the threshold for common haplotype frequency was set to  <dig> . however, we changed parameters of its block partitioning subroutine such that it does not function. this setting allows the minimum set of haplotype-tagging snps to be obtained for each haplotype block of each method. recall that resulting htsnps are mainly affected by the shape of underlying block partitioning.

given a set of htsnps, we find the largest vicinity on the chromosome within which only one htsnp is enough to capture all haplotype variations. then, we repeat the procedure on remaining regions by two, three, and more htsnps until the entire data is covered. we refer to the length of total genomic segments covered in the k-th step of this procedure as "k-htsnps coverage". we compare methods of haplotype block partitioning with regard to this measure, too.

application of haplotype blocks in disease association studies
the performance of block-like models of genomes in recognizing trait-associated loci can be assessed through a case-control design. a plan of experiment can include the following steps: 1) using available haplotype data in hapmap, the block structure of chromosomes in a certain population is determined. 2) considering the genotyping cost and practical limitations, an efficient number of snps is selected as markers to assay genotypes of case and control samples in these loci, as phase i genotyping. 3) an association test is performed on each block to obtain a scan of probably trait-associated blocks over the map. 4) more snps in those probable blocks of the previous step are genotyped until the desired fine map is achieved. compared to the frequently used method of sliding window, this approach has two advantages. first, it needs a lower cost for genotyping. secondly, there is not a common agreement on the selection of an optimum window.

to evaluate the performance of various models for block partitioning in case-control studies, we apply the first three steps of the block-based design as mentioned above on simulated data. we consider two additive single locus disease models with grr <dig> =  <dig> and grr <dig> =  <dig>  since we have obtained haplotype blocks on samples taken from hapmap ceu panel, it is necessary to make sure that simulated samples have the same genetic structure as the base data. the software gs  <cit>  enables us to generate genotype samples for case-control studies, using real genotype data under the desired disease models. applying the extension model of the software, we generate  <dig> sets of samples, each on consisting  <dig> case and  <dig> control genotypes. we repeat the simulation for low and moderate disease allele frequencies, independently. before the next experiments, we remove the causative snp from each sample.

clustering haplotypes in each block, we perform both association and significance tests by applying the pearson chi-square statistic. the clustering algorithm is the same as the one used to define "common haplotypes". in other words, after clustering each haplotype category consists of haplotypes no two of which differ from each other in more than four percent of their snp genotypes.

we follow two policies to select those markers needed for the phase i genotyping. in the first policy, we choose the first snp out of every k consecutive snps. following carlson's approach  <cit>  for tagsnp selection, in the second policy, we prioritize snps in each block based on their orders in carlson's algorithm. we select snps from each block based on their order until the required number of snps is taken from the whole region.

our objective is to compare the power of different algorithms under the condition that false discovery rates of all algorithms are the same. we set this common rate to 10%, as we have observed that lower levels result in unacceptably weak power in all methods . we use half the  <dig> sets of simulated case-control samples to find the proper p-value threshold corresponding to the 10% false discovery rate for each method. in details, chi-square values are obtained for blocks of each method. by the result, we can estimate the distribution of the chi-square statistic for each method. to obtain the desired p-value threshold, we find the p-value corresponding to the first decile of chi-square values of blocks which do not include the trait locus. once the p-value threshold is obtained for the respective algorithm, we perform the association test on the remaining  <dig> case-control sets and assess the statistical power. for a better comparison, we also examine the method of single site association test, besides the block-based association test.

performance on detecting recombination hotspots
our third assessment is on the application of haplotype block partitioning algorithms for detecting recombination hotspots. since there is no consensus on recombination hotspots within real haplotype data, we apply the mshot software  <cit> , to simulate haplotype samples. this software is an extension of hudson's algorithm  <cit>  and generates samples under the coalescent model with recombination. we generate  <dig> sample sets, each one includes  <dig> haplotypes of  <dig> snps. other samples with  <dig> haplotypes in each set are also generated. conditions set include setting hotspot region lengths at a maximum of  <dig> kb, a maximum of six hotspots per region of  <dig> kb, and a recombination rate of  <dig> to  <dig> times higher than the background rate at hotspot regions of recombination. the frequency of hotspots in the simulations was based on available knowledge of such features in the human genome  <cit> . the positions of the hotspots observed in the different simulations are recorded. all the block partitioning algorithms being considered are then applied to these sample sets.

to assess the performance and accuracy of haplotype block partitioning methods on detecting recombination hotspots, we counted the times that haplotype block boundaries and hotspot regions coincided with each other. block boundaries that occur outside hotspot regions are regarded as false positives, while hotspot regions not positioned at block boundaries are regarded as false negatives. in the latter case, we consider  <dig> kb flanking intervals around block boundaries as a standard extent of the hotspot region. we refer to the sum of the false positive and the false negative rates as total error rate in hotspot detection. we define the ratio of hotspot regions coinciding with block boundaries to the number of all hotspot regions as the power.

RESULTS
blocks of hapmap encode regions
results obtained by eight different haplotype block partitioning methods on hapmap data in ten encode regions have been summarized on table  <dig>  the common haplotype coverage as described in method comparison, denotes the fraction of haplotype sample that is covered by non-occasional haplotype clusters. recall that we allow a small tolerance such that the haplotypes having few different alleles have been clustered into the same group. therefore, the expected common haplotype coverage for methods like mb and hb are higher than the default 80% threshold that such methods are assuming for the haplotype block definition . even for other methods that are not subjected to such a diversity criterion, the level of common haplotype coverage is satisfactory. in particular, mdl, gam and gab usually produce short blocks that result in covering almost the whole region by common haplotypes. in contrast, our new methods, gpf and gpg, produce much wider blocks which still show reasonable common haplotype coverage.

result summary of haplotype blocks obtained by different methods on hapmap haplotypes of ceu in encode regions. common haplotype coverage denotes the fraction of all chromosomes that are covered by common haplotype variations in blocks. hole freq. and island freq. show the probability of a hole in blocks and the probability of an island out of any block. robustness shows the probability that any block boundary is placed at the same location during resampling. see table  <dig> for method abbreviations.

the consistency between block partitioning and ld pattern on encode regions can be shown by frequencies of "hole" and "island". in general, it is expected that the partitioning with wider blocks may include more holes in blocks and smaller blocks may miss more islands. considering these measures, methods gpg, gpf, and hb seem to be more reliable when most information of the ld pattern is to be maintained. in contrast, gab, gam, and mdl produce firmer blocks.

the similarity measure indicates the proportion of snp pairs commonly shared by different partitionings on ten encode regions. see table  <dig> for method abbreviations.

the last row of table  <dig> shows how much block partitioning methods produce consistant block boundaries when the resampling is performed. all methods except for hb are quite robust under permutation. the poor result of hb can be explained as an artifact of its underlying optimization approach. usually, the high sensitivity is a consequence of the optimality. as shown in figure  <dig>  some blocks obtained by hb have been merged into one block on resampled generations. it shows that the model of minimal set of tagsnps may ignore the essential structure of haplotypes data. in contrast, robustness of gabriel's method is surprising. both methods gab and gpg using the gabriel's association index are perfectly robust. using the real haplotype samples, the htsnp base coverage has also been computed for each method. figure  <dig> illustrates the average htsnp base coverage averaged over ten encode regions. in addition, for each encode region separately, the number of htsnps is shown in table  <dig>  as described in the previous section, with inclusion of blocks that need progressively larger number of htsnps in order to be identified, a complete coverage of the chromosome can potentially be achieved. it is observed that in all the methods the major coverage encompasses regions defined by 2– <dig> htsnps. it should be noted that the method used for block partitioning affects the haplotype diversity and consequently the number of tagging snps. generally, larger htsnp coverage produces a more economical genotyping. here, we observe that hb has the best htsnp base coverage in every level. this is not unexpected because its algorithm has been specially tailored for this purpose. as shown by the last segments of bars in figure  <dig>  methods producing smaller blocks do not reach the coverage achieved by other methods. by contrast, the difference at the start level – the genome coverage by a single htsnp – seems not to be considerable among different methods. by increasing the number of htsnps, a greater difference in the covered lengths can be observed. gam and mdl demand more htsnps for more complete coverage. gab results in the best coverage among the three local partitioning methods, even by few numbers of htsnps. it is an undeniable advantage for mb which produces the htsnp coverage very similar to the best method, hb, while its optimized objective is much less complicated than hb and attained by few computations.

for each block, the minimum number of htsnps has been obtained by htsnper. the number of htsnps in the whole region is the sum of the number of htsnps in all blocks. see table  <dig> for method abbreviations.

by comparing the result of htsnp selection for each encode region , we found that relaxing the constraint of haplotype diversity for block definition can potentially result in fewer htsnps. this proposition can be verified by comparing the result of gpf and gpg with hb in table  <dig>  however, it has been reported that carrying much genomic information by the least possible number of tagsnps, while appealing in reducing genotyping cost, can result in less accurate repeatable findings  <cit> . to address this issue, we examine the efficiency of the selected htsnps in haplotype reconstruction, to find out whether the number of htsnps in large blocks is underestimated. we performed a perfect cross-validation procedure for each method to assess accuracy of haplotype reconstruction based on hapmap haplotype data in all encode regions. as shown by figure  <dig>  htsnps in every method can describe all the necessary information to reconstruct at least 70% of crossed out haplotypes from given samples. it is close to the number that htsnper guarantees to cover for "common haplotypes". the reconstruction accuracy varies among different block partitioning methods, but in general, it slightly decays when tagged snps increase. the small blocks of mdl and gam have a greater effect on the accuracy as it seems hard to find a trend for accuracy decay in figure  <dig> for these methods. reconstruction accuracies of other methods are almost steady after six or more snps being tagged by a htsnp.

effect of different block structures on performance of disease association study
values of type i error on table  <dig> show the probability that the test recognizes a block as trait-associated when the block does not actually include the trait locus. these errors are results of chi-square tests at a  <dig>  level of significance, which have been performed on  <dig> sets of simulated case-control samples before taking adaptive thresholds for each method . as shown in table  <dig>  with higher risk ratio, more type i error is committed by all methods. however, disease allele frequency has a greater effect on type i error than grr <dig> 

chi-square tests have been performed at  <dig>  level of significance, on  <dig> sets of 50/ <dig> case-control samples. each entry depicts the proportion of blocks that have been falsely recognized as trait-associated to all the blocks that do not include the trait locus. * method of single site test. see table  <dig> for method abbreviations.

the power of the block-based association test and the single site method are depicted in figures  <dig> and  <dig>  the power of single site test decreases with sparser marker distribution. the same behavior can be partially found in block-based methods. however, the power of block-based methods is generally higher than the single site method even in the case of lower marker density. it shows that incorporating the ld information into the association study results in better performance. in fact, in the case of small disease allele frequency, the decrease of power is due to weaker ld between causative snp and other snps. as shown in figure  <dig>  our methods are slightly more successful than other methods in improving the power of association tests.

selecting marker positions by snps that have been ordered based on "informativeness" results in lesser decrease in the power of methods where marker distribution has lower density. in the case where snps are selected by prioritizing, the power of methods remains high, even when only one fifth of the original snps are used as markers . mdl is relatively efficient when markers are selected uniformly. when markers are prioritized, as one might expect, hb performs better. nevertheless, the two versions of our model – gpg and gpf – are more efficient even when marker density is low.

performance on recombination hotspot detection
given arbitrary hotspot regions, the sample generator program provides us with two datasets of simulated haplotype samples. using the hotspot locations as our references, we can estimate the hotspot detection error. this error is the sum of false positive and negative hits. table  <dig> illustrates the total error rate in recombination hotspots detection with respect to different sample sizes , for each block partitioning method. except for hb, the error rate of other methods is reasonable. the method structure in mb and hb is the same, but the former minimizes the number of haplotype blocks and the latter minimizes the number of haplotype-tagging snps. therefore, it can be deduced that the approach of minimal tagging does not fit well with patterns of recombination. at the opposite side, gpg and gpf get the least total error rate among other methods.

see table  <dig> for method abbreviations. n is the number of haplotypes. * all values are in percent.

it seems more probable that a block boundary occurs out of hotspot regions than a hotspot region is left undetected with no block boundary. figure  <dig> plots the power of the hotspot detection versus the error, i.e the false positive rate. in general, increasing haplotype samples reduces the error rate by all methods but mb . however, improvement in the performance of mb compensates for this effect. for gpg, mdl, gab and mb the power increases by increasing the sample size. by contrast, for gpf, hb and gam, we observe a decrease in power, but this is accompanied by a better prediction accuracy. our proposed method has the lowest type i error among the other methods and still, it correctly detects a hotspot with ~75% probability. gam also shows the same performance, around  <dig> percent. this is mainly due to its block definition that is relevant to the coalescent model of the simulated data.

CONCLUSIONS
here, we present a method for global haplotype partitioning based on pairwise analysis of snps. in this approach, haplotype blocks are defined such that the number of associated pairs in blocks is maximal, and blocks include only a small number of independent snp pairs. the normalized coefficient of linkage disequilibrium, d', is used as a scan statistic to determine independent snp pairs and fisher's exact test and its corresponding p-value determine the significance of dependency between snp pairs. furthermore, gabriel's index is applied in determination of association classes.

since the early observation of haplotype block structure in human genomes, several groups have developed block models assuming constraints on haplotype diversity. however, it has been suggested that such assumptions should be used carefully in applications  <cit> . our results from encode data show that methods based on pairwise analysis of snps, without initial assumptions on haplotype diversity, find blocks in which haplotype diversity is consistent with the standard thresholds used in classical methods. we assessed the similarity of haplotype block structures by counting snp pairs in overlapping regions in blocks of different partitionings. we did not find any general concordance among block boundaries in different methods. a previous study has also reached the same conclusion  <cit> . nevertheless, each method does produce blocks with 50% similarity with blocks of at least one other method.

the consistency of block boundaries within each single method was also investigated by a permutation resampling. to do so, we recorded the number of times in which a certain method would reproduce the same boundaries when applied to simulated recombinant samples. it was observed that the rule of gabriel to determine the association index within snp pairs was highly robust. our algorithm was also relatively robust.

in our method, the number of htsnps is not subjected to minimization. however, the number and also the coverage of htsnps within the resulting blocks compare well with the optimal values obtained by diversity-based approaches.

in a case-control study, a block-based approach for mapping a single locus trait was applied to blocks of various methods. the results show that any block-based association test is considerably more efficient than the conventional single site association test. in particular, our newly developed block partitioning method performed best accuracy for the case-control study, even when a low marker density is available.

biological considerations suggest that block boundaries produced by block partitioning methods should exhibit some concordance with recombination hotspots. in this regard, we assessed the performance of methods on simulated data. global block partitioning methods performed best both in terms of accuracy and power. in fact, our method may be considered an efficient and simple tool for gaining insight of recombination hotspots.

in conclusion, our assessments show that our proposed global partitioning method, the method of minimum description length, and gabriel's method are all promising for case-control association studies and for detection of recombination hotspots. furthermore, we have shown that allelic association of snp pairs can partially describe aspects of genomic variations in human populations.

authors' contributions
the problem and the use of pairwise measures to depict block-like structure of sample haplotypes have been proposed by ms. the mathematical model, computer implementation and method comparison have been developed by ak. hp revised the proposed model and redefined it in statistical terms. ee rewrote the article from a draft. all authors read and approved the final manuscript.

