BACKGROUND
gene regulation, signal transduction, metabolism and many other biological processes are nowadays analyzed using mathematical models  <cit> . mathematical models allow for the integration of available knowledge, providing mechanistic insights and an understanding of design principles  <cit> . the spectrum of employed modeling approaches ranges from qualitative boolean models  <cit>  to quantitative stochastic spatial models  <cit> . in particular ode models, such as reaction rate equations, are used on a range of spatial and temporal scales  <cit> . these models describe the temporal evolution of the concentration of biochemical species in cellular compartments as a function of initial conditions, parameters and stimuli.

ode models are flexible and allow for the mechanistic description of a range of processes . similar to other quantitative modeling approaches, ode models rely on accurate values for initial conditions and parameters, e.g., binding affinities, synthesis and degradation rates. initial conditions and parameters are often unknown and have to be inferred from experimental data  <cit> .

in most studies, experimental data from perturbation experiments are used to infer these unknown parameters  <cit> . in perturbation experiments, the response of the process to an external stimulus  is quantified, as illustrated in fig. 1a. as the initial condition of the process corresponds to a stable steady state of the unperturbed system, perturbation experiments provide information about the stimulus response. depending on the process and the input, the stimulus-induced changes might be transient or persistent. commonly used stimuli are ligands, which bind to receptors and induce downstream signaling, small molecules, which diffuse across the cell membrane and change the cell state, and physical stimuli .
fig.  <dig> schematic illustration of optimization problem with steady-state constraint. a measurement data and simulations of the system for three different pairs of parameters and initial conditions: optimum of the unconstrained optimization problem ,xs); suboptimal point on the manifold ,xs=xs)); and optimum of the constrained optimization problem ). the system is perturbed at time t= <dig> and should be in steady state for t< <dig>  b objective function landscape, steady-state manifold and pairs of parameters and initial conditions



the estimation of the parameters of ode models from data collected during perturbation experiments requires the solution of optimization problems. these optimization problems are in general nonlinear, non-convex and computationally demanding. this establishes the need for efficient and robust optimization methods  <cit> . in the literature, multi-start local optimization methods  <cit>  and global optimization methods  <cit>  have been employed. if the stationarity of the initial condition does not have to be enforced or if analytical expressions of the steady state are available, these methods mostly work well  <cit> . however, neglecting steady-state constraints often results in a loss of information and potentially implausible predictions. furthermore, an analytical expression of the parameter and input-dependent steady state is rarely available.

steady-state constraints are nonlinear equality constraints, which restrict the solution space to a manifold of feasible points. enforcing these equality constraints causes efficiency and convergence problems for standard optimization methods  <cit> . deterministic local optimization methods have to move along the manifold, resulting in small step-sizes or stagnation. stochastic local and global optimization methods are only allowed to propose update steps on the manifold, which is not possible in state-of-the-art toolboxes . the elimination of the equality constraints using analytical expressions for the parameter-dependent steady states resolves these problems  <cit> .

the first method to derive analytical expressions for steady states has been proposed by  <cit>  for networks of enzyme-catalyzed reactions. this method was later implemented  <cit>  and subsequently extended using results from graph theory  <cit> . furthermore, tailored methods for models with bilinear rate laws have been introduced  <cit> . for models with michaelis-menten and hill-type kinetics, py-substitution has been developed  <cit> . this method solves the steady-state constraint for a combination of states and parameters. an important recent extension ensured positivity of the solution  <cit> . py-substitution and its extension are however only applicable if the model possesses sufficiently many degrees of freedom  <cit> , which is difficult to assess a priori.

we propose two novel methods for solving optimization problems with a single or multiple steady-state constraints. these methods do not rely on an analytical expression for the steady state. instead, the geometry of the steady-state manifold and the stability of the steady state are exploited.

the first method we introduce borrows ideas from optimization algorithms on matrix manifolds  <cit> . these optimization algorithms employ retraction operators which map a point onto the manifold  <cit> . these retraction operators – usually analytical functions – facilitate an effective movement of optimizers on the manifold. retraction operators are however problem-specific and their construction is usually non-trivial  <cit> , which limits the application of established algorithms for optimization on manifolds. we exploit therefore a simulation-based retraction operator which exploits the stability of the steady state. this retraction operator can be used within state-of-the-art optimizers to eliminate the equality constraint and reduce the problem dimension. as the method uses both, a simulation-based retraction operator and a state-of-the-art optimizer, we will in the following refer to it as a hybrid optimization method.

the second method uses continuous analogues of local optimizers  <cit> . continuous analogues are dynamical systems whose trajectories converge to a locally optimal point of an optimization problem. these dynamical systems often possess a larger basin of attraction  <cit>  and can be solved using sophisticated numerical methods. this promises more robust convergence than simple step-size controls used in existing optimization methods. continuous analogues have been constructed for a series of linear and nonlinear problems . we introduce continuous gradient descent and newton-raphson methods for solving optimization problems with steady-state constraints. the manifold is stabilized using a continuous retraction derived from the model. this method is purely simulation-based and will therefore be referred to as a simulation-based optimization method.

the proposed optimization methods are illustrated using a simulation example. this is followed by a rigorous evaluation of the methods and comparison with alternative methods. to this end we consider two applications: ngf-induced activation of erk in primary sensory neurons; and raf/mek/erk signaling in hela cells. using novel data for the second application, new insights into raf/mek/erk signaling upon release from s-phase arrest are discovered.

methods
in this section we introduce the model class and the optimization problem. the differential geometry of the steady-state manifold is described and two optimization methods exploiting this geometry are introduced. the properties of these methods along with their implementation are discussed.

the optimization methods are used in the results section to study raf/mek/erk signaling in hela cells after release from s-phase arrest. the biological materials and the setups used to study this process experimentally are described below.

mathematical modeling of perturbation experiments
in this manuscript we consider ode models of biochemical reaction networks. ode models are quite general and allow for the description of many gene regulation, signal transduction and metabolic processes  <cit> . mathematically, ode models are commonly written as 
  <dig> dxdt=f,x=x0y=h, 

with states x∈rnx, observables y∈rny, parameters θ∈rnθ and inputs u∈rnu. the states x are the concentrations of biochemical species at time t. the observables y are the values of measurable quantities. the parameters θ are biochemical reaction rates, total abundances of biochemical species  and experimental parameters . the inputs u encode the experimental conditions applied to the system, e.g., extracellular concentration of ligands. to ensure existence and uniqueness of the solution of , the vector field f:rnx×rnθ×rnu→rnx is assumed to be lipschitz continuous. the mapping h:rnx×rnθ×rnu→rny describes the observation process and the mapping x0:rnθ→rnx provides the initial conditions.

the dynamics of model  are probed using perturbation experiments, e= <dig> …,e, with inputs ue. the initial condition x <dig> for an experiment condition is the steady state xse for a control condition . this steady state xse is parameter-, and input-dependent and fulfills the steady-state constraint, 
  <dig> 0=fxse,θ,uce. 

the stability of xse can be assessed using lyapunov theory  <cit> . we denote the collection of all parameter-state pairs  which fulfill the steady-state constraint  for a given input u as the manifold of steady states. for simplicity, we assume that  possesses a unique, exponentially stable steady state for every combination of parameters θ and inputs u. in this case, there exists a function xs:rnθ×rnu→rnx which maps the parameters to the corresponding steady state, i.e., x=xs . an analytical expression of the function xs is usually not available.

perturbation experiments provide measurement data, 
  <dig> d=tj,y¯ijei=1nyj=1ne=1e. 

the observable is indexed by i, the time point is indexed by j and the experimental condition is indexed by e. the measurements are noise-corrupted, 
  <dig> y¯ije=yietj,θ,xse+εije, 

with yet,θ,xse denoting the solution of  for input u=ue, parameters θ and initial condition xse at time t. the measurement noise εije is assumed to be normally distributed, εije∼n <dig> σije <dig>  the methods presented in the following are however not limited to this case.

parameter estimation
in this study we employ maximum likelihood  estimation to determine the unknown model parameters θ and steady states xs <dig> …,xse from the experimental data d. in accordance with the noise distribution, the likelihood function 
  <dig> pd|θ,xs <dig> …,xse:=∏e=1e∏j=1n∏i=1ny12πσijeexp−12y¯ije−yieσije <dig> 

is used. this likelihood function depends on θ and xs <dig> …,xse, variables which are coupled via the steady-state constraint .

the ml estimates for parameters and initial conditions, θ^ and x^see=1e, are obtained by maximizing the likelihood function  subject to the steady-state constraint . to improve the numerical evaluation and the optimizer convergence, this maximization problem is reformulated to the equivalent minimization problem 
  <dig> minθ,xs <dig> …,xsejθ,xs <dig> …,xse:=12∑e=1e∑j=1n∑i=1nyy¯ietj−yietj,θ,xseσije2s.t.0=fxse,θ,uce,e= <dig> …,e 

in which the objective function denotes the negative log-likelihood function, jθ,xs <dig> …,xse=−logpd|θ,xs <dig> …,xse. the solution of  provides parameter-state pairs θ^,x^se on the steady-states manifold which maximize the likelihood function . for these pairs it holds that x^se=xsθ^,uce.

in general, optimization problem  is nonlinear and possesses local minima. to compute the optimum of , we employ multi-start local optimization. this approach proved to be efficient in a variety of related problems . furthermore, sophisticated local optimizers allow for the consideration of nonlinear equality constraints . the consideration of nonlinear equality constraints is not possible for most evolutionary and genetic algorithms  <cit> , particle swarm optimizers  <cit> , simulated annealing  <cit>  and hybrid optimizers  <cit> . alternative methods are metaheuristics which combine ideas from local and global optimization methods  <cit> , facilitating the analysis of optimization problems with nonlinear constraints and multiple local minima.

the performance of multi-start local optimization depends on the local optimization method. in this study four alternative methods are considered, two established methods: 
unconstrained optimization method: an analytical expression of the steady state as a function of the parameter, xs, is used to eliminate the constraint and x <dig> from optimization problem . this yields the reduced optimization problem 
  <dig> minθjθ,xsθ,uc <dig> …,xsθ,uce 

which does not possess any equality constraints. while this method is rarely applicable – analytical expressions for the xs are difficult to compute – it provides a gold standard.

constrained optimization method: an interior point optimization method is used to solve the optimization problem . this is the state-of-the-art method and mostly used in practice.



and two newly developed methods 
hybrid optimization method: the optimization problem  is reduced to the manifold of steady states by computing xs numerically.

simulation-based optimization method: a dynamical system is formulated whose trajectories converge to local optima of the optimization problem . the dynamical system is solved using state-of-the-art numerical methods.



to facilitate efficiency and convergence, all methods are provided with the gradients of the objective function, 
  <dig> ∂j∂θ=−∑e=1e∑j=1n∑i=1nyy¯ie−yietj,θ,xse2∂yietj,θ,xse∂θ 

  <dig> ∂j∂xse=−∑j=1n∑i=1nyy¯ie−yietj,θ,xse2∂yietj,θ,xse∂xse 

and the gradients of the constraint. the sensitivities of the observable with respect to the parameters and initial conditions, ∂y∂θ and ∂y∂xse, are computed using the forward sensitivity equations  <cit> .

the optimization problem considered  is rather general and allows for the consideration of multiple steady-state constraints, as well as steady-state dose response curves. in the next section the geometry of the steady-state manifold is discussed. subsequently, the hybrid optimization method and the simulation-based optimization method are introduced.

manifold of steady states
the steady-state constraint defines the steady-state manifold which can be expressed in term of the mapping xs. in the considered setting, the existence of the mapping xs is ensured but an analytical expression is in general not available. for individual parameters θ, the steady state can however be computed by 
solving a feasibility problem ),

simulating the dynamical system until the steady state is reached, or

combining the simulation of the dynamical system with fine-tuning using the newton-raphson method  <cit> .



the last two methods are robust and computationally tractable. the computation of the steady state for individual parameters is however not sufficient, as the derivative is also required. to develop a tailored method for solving optimization problems , we will exploit the first-order geometry of the manifold of steady states. to this end we consider the sensitivities of the states x with respect to the parameters θ, 
  <dig> s=s <dig> s <dig> …,snθ∈rnx×nθwithsi:=∂x∂θi=∂x1∂θi,…,∂xnx∂θit. 

in the control conditions. the dynamics of s are governed by the forward sensitivity equation 
  <dig> s˙=∂f∂xs+∂f∂θ. 

in steady-state, s˙= <dig>  this equation simplifies to 
  <dig> s=−∂f∂x−1∂f∂θ, 

evaluated at ,u). the invertibility of the jacobian  follows from local exponential stability of the steady state. this result can also be obtained using the implicit function theorem.

the sensitivity of the steady state with respect to the parameters, s, provides a first-order approximation to xs, 
  <dig> xs=xs+s,u)rΔθ+or <dig>  

the perturbation direction and the step size are denoted by Δθ and r, respectively. by reformulating  and letting r→ <dig>  we obtain a dynamical system which evolves on the manifold of steady states, 
  <dig> dxsdr=sΔθ. 

given an update direction Δθ and a length r,  provides the steady state for parameter θ+rΔθ up to the accuracy of the chosen ode solver. hence,  enables moves on the steady-state manifold, similar to results in  <cit>  for other manifolds.

hybrid optimization method
the gold standard for solving optimization problem  is to determine an analytical expression for the parameter-dependent steady state. while such an expression is not always available, a straightforward approach is to compute the steady state numerically for given parameters θ. this is computationally more demanding than using an analytical expression, but it also yields the reduced optimization problem .

this straightforward approach is visualized in fig. 2a. as it can be employed in any state-of-the-art optimization method, we denote it as a hybrid optimization method. starting at a point , we employ a three-step procedure: 
step 1: the local optimizer proposes new parameters θl+ <dig>  this yields the point θl+ <dig> xs <dig> l,…,xse,l which is usually not on the manifold of steady states. 
fig.  <dig> schematic illustration of a the hybrid optimization method and b the simulation-based optimization methods. the path of the optimizers are illustrated along with the manifold of steady states and the objective function contour



step 2: the steady states xs <dig> l+ <dig> …,xse,l+ <dig> for the parameters θl+ <dig> are computed using one of the methods discussed in the manifold of steady states section . this yields the point θl+ <dig> xs <dig> l+ <dig> …,xse,l+ <dig> on the steady-state manifold. 

step 3: the objective function jl+1=jθl+ <dig> xs <dig> l+ <dig> …,xse,l+ <dig> is computed for parameters θl+ <dig> and numerically calculated steady state xs <dig> l+ <dig> …,xse,l+ <dig>  this objective function is provided to the local optimizer. 



the simulation-based retraction to the manifold of steady states  reduces the problem dimension and eliminates the constraint. the objective function gradient for this reduced problem is 
  <dig> djdθ=∂j∂θ+∑e=1e∂j∂xse∂xse∂θ 

with the sensitivity of the steady states with respect to the parameters, =s, as defined in .

the proposed hybrid optimization method possesses all properties and options of the employed local optimizer. in addition, the retraction accuracy εtol of the convergence criteria ||f||2<εtol has to be selected.

simulation-based optimization method
instead of using a discrete update as in local optimization, one could also think of choosing a continuous formulation of the update as illustrated in fig. 2b. the continuous analogue of a gradient descent method is dθ/dr=−t  <cit> . this ode system can be coupled with the dynamical system evolving on the steady-state manifold , using Δθ=−t. more generally we can consider any descent direction g in which j is decreasing. we obtain the ode system 
  <dig> dθdr=−gdxsedr=sdθdr,e= <dig> …,e, 

with the steady-state sensitivity sθ,xse,uce. initialization of  in a point θ <dig> xs, <dig> …,xs,0e fulfilling  yields a trajectory evolving on the steady-state manifold, along which the objective function decreases.

the formulation  avoids the repeated simulation-based retractions used by the hybrid optimization method presented in the previous section, however it also bears two disadvantages:  an appropriate initial point  has to be determined by solving ; and  numerical errors can result in a divergence from the steady-state manifold. to address these problems, we introduce the term λf which locally retracts the state of the system to the manifold by exploiting the stability properties of the steady state. this yields the system 
  <dig> dθdr=−gθ,xs <dig> …,xsedxsedr=Ŝθ,xse,ucedθdr+λfθ,xse,uce,e= <dig> …,e. 

for this modified system we do not require that the initial point  fulfill the steady-state constraint , hence, the jacobian ∇xf| might not be invertible. to address this, we define 
  <dig> Ŝ=−∂f∂x+∂f∂θ 

in which + denotes the moore–penrose pseudoinverse of  at . on the steady-state manifold, the jacobian is invertible and we recover the standard steady-state sensitivity. for a large retraction factor λ≫ <dig>  the state  is retracted quickly to the steady-state manifold.

in this manuscript we consider two possible choices for the descent direction: 
gradient descent: g=−djdθ and

newton-type descent: g=−f+μi−1djdθ.



the newton-type descent exploits the fisher information matrix f  <cit> . the fisher information matrix is an approximation to the hessian of the objective function. this approximation can be computed from first-order sensitivities and is positive semi-definite.

for the gradient descent we established local exponential stability of local optima for appropriate choice of λ for a broad class of problems . this implies that the trajectories of the system converge to the local optima of the optimization problem . we expect that a similar result can be derived for the newton-type descent. though this is not yet available, we included the newton type descent as we expect – similar to classical optimizers – faster convergence.

for local optimization of , the dynamical system  has to be simulated for r→∞. for this, implicit methods with adaptive step-size selection and error control should be employed as  might be stiff. appropriate numerical methods are implemented among others in matlab and the sundials package  <cit> . these simulations are stopped as soon as the convergence criterion max{∥dθ/dr∥,∥dxs/dr∥}<εtol is met.

implementation
the different methods are implemented in matlab and provided in an additional file  <dig>  the local optimization is performed using the matlab routine fmincon.m. fmincon is supplied with the objective function value and the values of the constraint, as well as the respective analytical derivatives. the continuous analogue used for simulation-based optimization is simulated using the matlab ode solver ode15s. the computationally intensive simulation of the perturbation experiments and the numerical calculation of the steady state is performed using the sundials package  <cit>  which was accessed using the matlab toolbox amici . default settings are used for fmincon.m and the simulation routines unless stated otherwise. the convergence tolerance for the hybrid optimization method is set to εtol=10− <dig> in the simulation example and the first application example and to εtol=10− <dig> in the second application example. for the simulation-based optimization methods it is set to εtol=10− <dig> 

experimental data
to evaluate the performance of the proposed methods, we use two application examples with experimental data. in the first application example, we consider a dataset for ngf-induced erk signaling in primary sensory neurons which was published by andres et al.  <cit> . in the second application example, we use novel data for raf/mek/erk signaling in hela cells after release from s-phase arrest. this novel experimental data for raf/mek/erk signaling in hela cells was acquired using the following methods.

cell culture. hela cells were obtained from the american type culture collection . cells were maintained in rpmi  <dig> supplemented with  <dig> % fetal bovine serum.

cell synchronization at the g1/s border. hela cells were synchronized at the g1/s border using an aphidicolin treatment. in brief, cells grown on petri dishes were incubated in medium supplemented with aphidicolin  for  <dig> h. afterward, cells were released from the s-phase arrest by washing with serum-free medium and were refed with growth medium.

protein extraction of cells. whole-cell extracts were obtained by solubilizing cells in hot protein sample buffer . after  <dig> min of incubation at  <dig> °c, extracts were placed on ice and centrifuged . samples were subjected to sds-page.

western blotting. equal amounts of proteins were separated by sds-page and blotted onto nitrocellulose membranes . after blocking with  <dig>  % blocking reagent , filters were probed with specific antibodies. proteins were visualized with irdye-coupled secondary antibodies using the li-cor odyssey system. protein bands were quantified using imagej.

antibodies commercially available antibodies used in this study were: anti-erk rabbit polyclonal, anti-phospho-p44/ <dig> mapk   rabbit polyclonal, and anti-phospho-mek1/ <dig>  rabbit polyclonal . the tubulin-specific mouse monoclonal antibody was from millipore. the irdye-coupled secondary antibodies were from li-cor biosciences.

RESULTS
in the following, we illustrate the behavior of the proposed optimization method. furthermore, the performance of the proposed methods will be compared to standard constrained and unconstrained optimization methods. for this purpose, we consider a simulation example for which the ground truth is known. furthermore, we test the methods on two application examples using real data.

simulation example: conversion process
in this section, we illustrate the proposed optimization methods by studying parameter estimation for a conversion process from steady-state data. conversion processes are among the most common motifs in biological systems, therefore particularly interesting, and provide a simple test case.

we consider the conversion process 
  <dig> a⇄θ1uθ2b, 

with parameters θ=θ <dig> θ2∈r+ <dig> and input u∈r+. assuming conservation of mass  and mass action kinetics, the temporal evolution of the concentration of biochemical species a, x= , is governed by 
  <dig> dxdt=θ2−xy=x 

with x0∈r+ denoting the initial concentration. the steady state of model  is 
  <dig> xs=θ2θ1u+θ <dig>  

to illustrate the properties of the hybrid and the simulation-based optimization methods, we consider the estimation of the parameters θ from artificial time-resolved data for y. the artificial data are obtained by simulation of  for θ= and u= <dig>  at the time points tj= , starting from the steady state of the control condition uc= <dig> at t= <dig>  assuming unit variance for observation errors, yields the optimization problem 
  <dig> minθ,xsj:=12∑j=1ny¯−y2s.t.0=θ2−xs, 

in which y¯ denotes the measured concentration of a at time point tj and y denotes the solution of  for initial conditions x=xs and input u= <dig>  at time point tj.

illustration of hybrid optimization method
the hybrid optimization method evaluates the steady state numerically but exploits the gradients of the objective functions . to this end the objective function gradient, dj/dθ, 
  <dig> djdθ=−∑j=1ny¯−y∂y∂θ+∂y∂xs∂xs∂θ 

and the local sensitivities of the steady state for u= <dig>  
  <dig> s=∂xs∂θ <dig> ∂xs∂θ2=−xsθ1+θ <dig> −xsθ1+θ <dig> 

are derived. the local sensitivities depend merely on derivatives of the vector field and can be computed without knowledge of an analytical expression of the steady state.

the trajectory of the hybrid optimization method is illustrated in fig.  <dig>  at the end of each iteration, the simulation-based retraction ensures that the parameter-state pair is on the steady-state manifold . on the steady-state manifold, the optimizer reaches a narrow valley for θ <dig> and θ <dig> and then moves along the valley to reach the optimum . the behavior is similar for other starting points.
fig.  <dig> illustration of the hybrid optimization method for the conversion process . a path of the hybrid optimization method , the true optimum , and the steady-state manifold ) are shown. the objective function values are indicated by the surface coloring. the optimizer path is partially covered by the steady state manifold. b the distance to the analytical steady state as well as c, d the path of the parameters , their endpoints  and optimal parameter value  are depicted



illustration of simulation-based optimization method
for simulation-based optimization the continuous analogue of the gradient descent method is derived. this yields the dynamical system 
  <dig> dθ1dr=−∑j=1ny¯−y∂y∂θ1+∂y∂xss1dθ2dr=−∑j=1ny¯−y∂y∂θ2+∂y∂xss2dxsdr=−xsθ1+θ <dig> −xsθ1+θ2dθ1drdθ2dr+λxs), 

with initial conditions θ1=θ <dig> , θ2=θ <dig>  and xs=xs, <dig>  it can be verified that the objective function j is locally strictly convex in θ – the parameters are locally identifiable – and that the model  is asymptotically stable. accordingly, system  converges to a local optimum of the constrained optimization problem  .

to illustrate the simulation-based optimization method we simulate the continuous analogue of the gradient descent method. exemplary trajectories are depicted in fig.  <dig>  we find that for retraction factors λ> <dig>  the states t converge to the optimal solution. as retraction renders the steady-state manifold  attractive, also for initial conditions t which do not fulfill the steady-state condition, fast convergence to the steady-state manifold can be achieved using λ≫ <dig> . for large retractions , the dynamic consists of two phases:  the state x converges quickly to the parameter-dependent steady state  ; and  the state t moves along the steady-state manifold to the global optimum .
fig.  <dig> illustration of the simulation-based optimization method for the conversion process . a trajectory of the continuous analogue for different retraction factors λ , the endpoints , the true optimum , and the steady-state manifold ) are shown. the objective function values are indicated by the surface coloring. b the distance to the analytical steady state as well as c, d the path of the parameters , their endpoints  and optimal parameter value  are depicted



application example 1: ngf-induced erk signaling in primary sensory neurons
to evaluate and compare existing and proposed local optimization methods for problems with steady-state constraints, we analyze ngf-induced erk phosphorylation in primary sensory neurons. primary sensory neurons are among others used to investigate pain sensitization in response to inflammation. during inflammation a cocktail of stimuli is present, including ngf. ngf binds to cellular receptors and induces the erk phosphorylation  <cit> . this modulates neuronal activity by triggering ion channel phosphorylation and protein expression  <cit> .

growth-factor induced erk signaling is a potential target for novel pain therapies  <cit>  and therefore of high practical relevance. in addition, this application is well-suited for the evaluation of the methods as ngf dose-response curves at late time points have been recorded. these data provide multiple steady-state constraints for the thorough assessment of the methods. in the following, we will compare the performance of unconstrained, constrained, hybrid and simulation-based optimization in the presence of multiple steady-state constraints.

experimental data for ngf-induced erk phosphorylation
erk phosphorylation in response to different concentrations of ngf was previously quantified using quantitative automated microscopy  <cit> . this technique provides single-cell data from which population average data can be derived. these population average data are highly reproducible and quantitative but provide merely the relative erk phosphorylation in comparison to the control as no calibration curve is employed. the unknown scaling constant is denoted by s.

mathematical model of ngf-induced erk phosphorylation
ngf induces erk phosphorylation by binding to the ngf receptor trka. the complex trka:ngf activates ras which in turn phosphorylates raf. praf phosphorylates mek and pmek phosphorylates erk . while all these steps are considered in complex models  <cit> , a previous analysis revealed that the intermediate steps do not have to be modeled to capture the measured data  <cit> . we therefore use the model introduced in  <cit> , 
  <dig> dx1dt=k1uk30−x1−k2x <dig> dx2dt=x1+k4s0−x2−k5x <dig> y=x <dig>  fig.  <dig> schematic of the model considered for ngf-induced erk phosphorylation in primary sensory neurons



to describe the activities of the ngf receptor trka  and erk phosphorylation  in response to ngf stimulation, u=  <dig>  this model possesses a minimal number of model parameters, θ=, and is structurally identifiable. for details on the model, we refer to the additional file 1: section  <dig> and the original publication  <cit> . the experimental noise ε is assumed to be normally distributed with the unknown variance σ <dig>  ε∼n.

the parameter-, and input-dependent steady state of  is given by 
  <dig> xs,1=k30k10k10+k <dig> xs,2=s0xs,1+k4xs,1+k4+k <dig>  

this steady state exists for all positive parameters and is exponentially stable.

parameter estimation problem with multiple steady-state constraints
in this study, the unknown parameters θ∈r+ <dig> and the states xs, <dig> and xs, <dig> for each considered input of ngf are inferred from published dose response data  <cit>  using ml estimation. the dataset contains  <dig> different ngf doses, yielding an optimization problem with 7+2·6= <dig> optimization variables and 2·6= <dig> nonlinear equality constraints. this nonlinear optimization problem is solved using multi-start local optimization. the local optimization is performed using unconstrained, constrained and hybrid optimization as well as simulation-based optimization using gradient and newton-type descent. bounds and scales for the parameters are provided in the additional file 1: table s <dig> 

to assess the convergence properties, the constraint satisfaction/violation and the computation time, the local optimization methods were initialized with the same  <dig> sampled starting points. the results are summarized in fig.  <dig>  additionally, we assessed the dependence of the convergence properties on λ. the results can be found in the additional file 1: section  <dig> 
fig.  <dig> comparison of optimization methods for ngf-induced erk activation model. a final objective function values , b comparison of convergence criteria with respect to steady-state constraint, c computation time for  <dig> runs and d average computation time per converged start of unconstrained optimization method , constrained optimization algorithm , the hybrid optimization method and the proposed simulation-based optimization methods with gradient descent and newton method based updates and e the best fit to the data are depicted



the convergence properties of unconstrained, hybrid and simulation-based optimization are comparable
to assess the convergence of the optimization method, we sort and visualize the objective function values achieved in the individual optimizer starts . in addition, we determine the percentage of converged starts. a start is considered to be converged if the final point cannot be rejected compared to the ml estimate using the likelihood ratio test with a significance level of  <dig> .

as expected, we find that the gold standard – the unconstrained optimization method – shows the best convergence properties. it converges in  <dig> % of the starts to the global optimum. a similar convergence is achieved by the proposed methods, hybrid optimization and simulation-based optimization using gradient descent. the third proposed method – simulation-based optimization using newton-type descent – displays intermediate convergence properties . the state-of-the-art method – constrained optimization – exhibits the poorest convergence. it converges in  <dig> % of the starts. hence, the proposed optimization methods are superior to constrained optimization regarding convergence to the global optimum.

beyond differences in the convergence to the global optimum, the convergence to local optima differs. the results of unconstrained, constrained and hybrid optimization reveal three local optima. the local optima with the worst objective function values are hardly found using simulation-based optimization, indicating altered regions of attraction.

hybrid and simulation-based optimization provide reliable estimates of the steady states
the individual optimization methods enforce the steady-state constraints differently. what all methods have in common is that the steady-state constraint f= <dig> is relaxed to a constraint on the norm of the vector field, i.e., ||f||2<εf. accordingly, parameter-state pairs returned by the optimization methods usually do not fulfill steady-state constraints exactly. different optimization methods might even achieve different accuracies. in addition, a bound for the difference of the estimated steady state xs for a parameter θ and the true steady state xs, Δxs=xs−xs, is usually not available.

we studied the relation of the solver indicating convergence based on the vector field  and the difference of the estimated to the analytical steady-state being small  for the different optimization methods. in our opinion a good optimizer should achieve equivalence of the two criteria. this would mean that enforcing the constraint of the vector field ensures a good approximation of the steady state. the result is depicted in fig. 6b for a tolerance of 10− <dig> 

the unconstrained optimization uses an analytical expression of the steady state and therefore the two criteria are identical. hybrid and simulation-based optimization also achieved a good agreement of both criteria, with ∼ <dig> %. in ∼ <dig> % of the cases, the solver indicates convergence based on the vector field constraint but the steady-state estimate is off . the precise percentage depends heavily on the retraction factor λ for the simulation-based optimization method. for the constrained optimization, all possible combinations are observed and the two criteria agree in merely  <dig> % of the runs. in summary, the results indicate that the proposed methods provide reliable estimates for the steady states while constrained optimization yields many inconsistent parameter-state pairs.

hybrid and simulation-based optimization are faster than constrained optimization
a key performance metric for local optimizers is the average computation time per converged start. this metric summarizes convergence properties  and computation times for individual starts . it is computed by dividing the overall computation time for the multi-start optimization by the number of converged starts. this measure of optimizer performance is depicted in fig. 6d.

unconstrained optimization using a  analytical expressions for steady states is most efficient. the individual runs are fast and the percentage of converged starts is high. hybrid and simulation-based methods are roughly  <dig> times slower but these methods can be applied if analytical expressions for steady states are not available. furthermore, these methods are  <dig>  times faster than constrained optimization due to the improved convergence rate. additionally, the fit to the data for the optimal parameters is convincing . accordingly, we conclude that hybrid and simulation-based optimization are promising approaches in the presence of multiple steady-state constraints.

application 2: raf/mek/erk signaling in hela cells after release from s-phase arrest
in this section, we study raf/mek/erk signaling in hela cells after release from s-phase arrest. experimental studies revealed that cell-cycle is, among others, controlled by raf/mek/erk signaling  <cit> . the signaling dynamics in different cell-cycle phases as well as the cell-cycle-dependent relevance of feedback mechanisms  <cit>  are however still not completely unraveled although a more thorough understanding could provide valuable insights into treatment resistance  <cit> . using the new data and model selection we study the relevance of negative feedback from phospho-erk to raf activation during g1/s phase transition.

in addition to its biological relevance, the raf/mek/ erk pathway is well-suited for the evaluation of the proposed optimization methods and the comparison to state-of-the-art methods. the pathway is nonlinear, yielding a nonlinear and non-convex optimization problem. furthermore, we will consider a synchronized cell population which reached a steady state before the start of the experiment. accordingly, a steady-state constraint has to be enforced and fitted along with time-resolved data for perturbation experiments.

experimental data for raf/mek/erk signaling after release from s-phase arrest
to study the raf/mek/erk pathway, hela cells were synchronized at the g1/s border using an aphidicolin treatment. after synchronization was achieved, aphidicolin was removed and the dynamics of phospho-mek and phospho-erk were quantified using western blotting. this was repeated after treatment with sorafenib and uo <dig> to explore the dynamic range of the pathway. sorafenib is an inhibitor of raf kinases  <cit>  and uo <dig> is a highly selective inhibitor of mek  <cit> .

as western blots are merely semi-quantitive, they provide the relative activity of phospho-mek and phospho-erk at different time points and under different conditions. the unknown scaling constants differ between blots and measured species. for a detailed discussion of characteristics of western blot data we refer to  <cit> .

mathematical model for raf/mek/erk signaling after release from s-phase arrest
raf/mek/erk signaling is induced by myriads of intra- and extracellular signals  <cit> . these signals converge on the level of raf kinase, which they phosphorylate. the phosphorylated raf kinase phosphorylates mek, which in turn phosphorylates erk. erk induces downstream signaling and can down-regulate the raf activity  <cit> . the latter establishes a negative feedback loop  <cit> . the activity of raf and mek can be inhibited by sorafenib and uo <dig>  respectively. the pathway is illustrated in fig.  <dig> 
fig.  <dig> schematic of the model considered for the raf/mek/erk signaling after release from s-phase arrest



in this section we develop a model for raf/mek/erk signaling which accounts for the core proteins as well as their inhibition with sorafenib and uo <dig>  the model considers the six reactions: 
  <dig> r1:raf→praf,rate=k <dig> maxξ,r2:praf→raf,rate=k <dig> r3:mek→pmek,rate=k3k2k2+,r4:pmek→mek,rate=k <dig> r5:erk→perk,rate=k5k3k3+,r6:perk→erk,rate=k <dig>  

the upstream signaling is summarized in the time-dependent rate constant k <dig> max with the flexible parameterization 
  <dig> k <dig> max=k <dig> +k <dig> −e−tτ1e−tτ <dig> 

. the effects of sorafenib and uo <dig> are captured by a reduction in the kinase activity of praf and pmek .

experimental studies proved an inhibition of raf phosphorylation by perk  <cit> . this feedback is however context-dependent  <cit> . to study the importance of this feedback during the g1/s phase transition, we considered two model hypotheses: 
inhibition of raf phosphorylation by perk: ξ=k1k1+

no inhibition: ξ=1



using mass conservation and reformulations explained in detail in additional file 1: section  <dig> we arrive at the ode model 
  <dig> dx1dt=k <dig> maxξ−k2x1dx2dt=k30k2x1k2+−k4x2dx3dt=k50k3x2k3+−k6x3y <dig> b=s <dig> b0x2y <dig> b=s <dig> b0x <dig> 

for the relative phosphorylation levels x1= / <dig>  x2= / <dig> and x3= / <dig> and the input u=. the model for the relative phosphorylation levels does not depend explicitly on the total abundances  <dig>   <dig> and  <dig> but only on products and ratios of these parameters with other parameters, e.g., k <dig>  defining these products and ratios as new parameters eliminates non-identifiabilities and reduces the number of parameters. each western blot, indexed by b= <dig> …, <dig>  provides time-resolved data for y <dig> b and y <dig> b for a combination of different experimental conditions. the measurement noise is assumed to be normally distributed and its variance is estimated from the experimental data. as all parameters are non-negative, a log-parameterization is used for parameter estimation  <cit> . the states of the reformulated model are between  <dig> and  <dig>  details regarding parameters and initial conditions are provided in the additional file 1: table s <dig> 

in addition to the kinetic, scaling and noise parameters, the initial conditions of the models for h <dig> and h <dig> are unknown. as the cells are however arrested in s-phase with k <dig> max=k <dig>  and u= <dig>  the initial conditions are the corresponding steady states. after significant manual preprocessing of the steady-state constraints, analytical expressions xs for the steady-states as a function of the other parameters could be calculated with symbolic math toolboxes. these analytical expressions are provided in the additional file 1: equation , .

parameter estimation problem with multiple perturbation datasets
we inferred the model parameters and initial conditions from the western blot data using ml estimation. the dataset provides time-resolved data for three conditions , all starting from the same steady-state. the optimization problem is solved using multi-start local optimization. the local optimization was performed using unconstrained, constrained and hybrid optimization as well as simulation-based optimization using gradient and newton-type descent each method starting at the same points. the starting points for local optimizations were obtained using latin hypercube sampling . the maximal number of iterations and function evaluations performed by fmincon were increased to  <dig> and 2000nθ for the unconstrained and constrained optimization. for the hybrid optimization, the maximal number of iterations was increased to  <dig>  the results for  <dig> starts of the local optimizations for the model of h <dig> are depicted in fig. 8a and b.
fig.  <dig> parameter estimation results for raf/mek/erk signaling in hela cells. a convergence and b computational efficiency of local optimization methods for the model with the negative feedback loop . c best fit of the model with the negative feedback loop  to data for three different treatment conditions. pmek and perk signals are rescaled with the respective maximum activity and the light gray area indicates 2- σ interval of the measurement noise



hybrid and simulation-based optimization outperforms constrained optimization
unconstrained optimization using the analytical expression for the steady state – the gold standard – converged in ∼ <dig> % of the starts . hybrid and simulation-based optimization methods achieved a percentage of converged starts comparable to the gold standard , but without requiring an analytical expression for the steady state. constrained optimization – the state-of-the-art – converged in less than  <dig> % of the starts, resulting in a relatively large computation time per converged start . even though hybrid and simulation-based optimization were slower than the gold standard, they were more than  <dig> times faster than constrained optimization. hence, the proposed optimization methods also outperform constrained optimization for this problem.

a detailed comparison of the proposed methods revealed that simulation-based optimization using gradient descent achieved the highest percentage of converged starts. hybrid optimization required however fewer simulations of the perturbation experiments – the time-consuming step – rendering this method computationally more efficient. simulation-based optimization using newton-type descent was the least efficient of the proposed methods. this might be related to the challenges in tuning the regularization parameters.

model selection reveals importance of negative feedback
the model with negative feedback  fits the experimental data . it captures the transient phosphorylation of mek and erk after release from s-phase arrest, the reduced erk phosphorylation in the presence of sorafenib and uo <dig>  furthermore, the increased mek phosphorylation after uo <dig> treatment is explained via a decrease in the strength of the negative feedback which is caused by the reduced abundance of perk. the model without the negative feedback loop  is not able to capture the difference between the control condition and the simulation with uo <dig>  the value of the bayesian information criterion   <cit>  is  <dig>  for the model with negative feedback  and  <dig>  for the model without negative feedback . the difference of  <dig>  indicates a strong preference for h <dig>  <cit> . the same conclusion is reached using the akaike information criterion   <cit> . we conclude that raf phosphorylation is inhibited by perk during g1/s phase transition.

to conclude, in this section we illustrated the proposed hybrid and simulation-based optimization methods. the applicability of the methods was demonstrated by studying relevant biological problems. the comparison with state-of-the-art methods revealed convergence and computational efficiency. the study of raf/mek/erk signaling using the methods underlined the feedback regulation of erk phosphorylation during cell cycle progression.

discussion
optimization problems with steady state constraints arise in many biology applications for a wide range of models. for some models an analytical expression for the steady state can be derived and used to eliminate the steady-state constraints  <cit> . while this is favorable, it is not always possible. in cases in which no analytical expressions are available, the vector of optimization variables contains the unknown parameters as well as the corresponding steady states. the optimizers have to evolve on the non-linear manifold, the set of steady states. in this manuscript, we propose a hybrid optimization method and a continuous analogue to solve optimization problems with steady-state constraints more efficiently. this simulation-based method exploits the local geometry of the steady-state manifold for optimization.

the proposed hybrid and simulation-based optimization methods are evaluated using three models for biological processes. following a simple illustration example, an application with multiple steady-state constraints and an application with time-resolved data for multiple perturbation conditions are considered. for this rich set of scenarios we find that the hybrid and simulation-based optimization methods possess improved convergence properties in comparison to standard constrained optimization methods implemented in the matlab routine fmincon. we expect that the proposed methods also outperform alternative optimization routines , this, however, remains to be analyzed. the proposed optimization methods yield convergence properties comparable to those of unconstrained optimization methods exploiting an analytical expression for the steady state. however, if analytical expressions for the steady state can be determined using available methods  <cit> , unconstrained optimization should be used as the computation time is lower. the proposed methods are also applicable to a broader class of problems for which no analytical expression for the steady state is available. furthermore, the method directly allows for multiple steady-state constraints. unlike methods based on sequential geometric programming  <cit> , steady-state and kinetic data can be handled.

beyond the evaluation of the proposed methods, our experimental and computational analysis of novel data for raf/mek/erk signaling after release for s-phase arrest provided new insights. parameter estimation and model comparison indicate that the negative feedback from erk to raf phosphorylation is also active at the g1/s border. this complements previous knowledge of stimulus-, context-dependence of this stimulus  <cit>  and its relevance for the robustness of mapk signaling in tumor cells  <cit> .

the implementation of the hybrid optimization method employed in this study is a simulation-based retraction operator. alternatively, efficient and accurate schemes combining simulation and local optimization could be employed to compute steady states and sensitivities  <cit> . this should improve the computational efficiency further. to relax the stability assumption for the steady state, conservation relations can be incorporated in the local optimization scheme.

for the simulation-based optimization method we established local asymptotic stability of optimal points using perturbation theory. this result is however restricted to the gradient-type descent and locally convex objective functions. the latter implies local practical identifiability. the theoretical properties of the newton-type descent and the properties in the presence of practical and structural non-identifiability remain to be analyzed. preliminary results and the applications suggest that in the presence of non-identifiabilities the simulation-based optimization method always yields a point on the non-identifiable subspace. furthermore, the available proof shows the retraction factor λ has to be chosen large enough to ensure convergence. however, as too large λ will result in a stiff system, an intelligent choice of λ is necessary.

the simulation example and the applications possess a unique exponentially stable steady state. however, preliminary results suggest that the methods also achieve good convergence for dynamical systems with multiple stable steady states and bifurcations  <cit>  . the theoretical analysis of the proposed methods and a detailed performance evaluation for dynamical systems with such properties remains to be addressed.

beyond parameter estimation, the proposed optimization methods can also accelerate practical indentifiability analysis and uncertainty quantification by speeding up optimization runs in bootstrapping uncertainty analysis  <cit>  and profile likelihood calculation  <cit> . in addition, bayesian uncertainty analysis using markov chain monte carlo sampling  <cit>  can profit from an efficient initial optimization prior to the sampling. this has been shown to reduce the warm-up and to improve convergence  <cit> . for a more detailed discussion of identifiabilty and uncertainty analysis methods, we refer to recent comparative studies  <cit> .

CONCLUSIONS
in summary, the proposed optimization methods are promising alternatives to constrained optimization for optimization problems with steady-state constraints. they are applicable to a wide range of ode-constrained optimization problems  <cit>  and can – unlike methods which rely on an analytical expression for the steady state – be extended to pde constrained optimization problems  <cit> . the availability of the matlab code will facilitate the application and extension of the methods, as well as the integration in toolboxes such as data2dynamics  <cit>  and copasi  <cit> . accordingly, our study has a strong potential influence on the analysis of optimization problems with steady-state constraints in practice.

abbreviations
not applicable.

additional files
additional file  <dig> supporting information s <dig>  this document provides a detailed description of the stability proof and the derivation of the different pathway models. furthermore, the parameter and bounds are listed, the influence of the retraction factor on convergence and run time as well as the behavior of the proposed methods in the presence of multiple steady states and bifurcations is illustrated. 



additional file  <dig> code s <dig>  this zip-file contains the matlab code used for the simulation example  and the application examples  presented in the paper. we provide implementations for the hybrid optimization and simulation-based optimization methods, the models and the optimization. in addition to the implementation, also all data and result files  are included. 



