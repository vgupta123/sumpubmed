BACKGROUND
nested data are common in neuroscience, where multiple observations are often collected in the same cell, tissue sample, litter, or treatment facility . for example, consider a study of differences between wild type  and knock-out  animals in the number of docked vesicles within presynaptic boutons. as each neuron has multiple presynaptic boutons, one can measure the number of docked vesicles in multiple boutons of every neuron, resulting in multiple measurements within each neuron . as the measurements are clustered within neurons, data resulting from this type of experimental design is referred to as clustered or nested data. <dig> such data have a hierarchical, or multilevel, structure. in the present example, the number of presynaptic boutons within a neuron is referred to as the level  <dig> variable, and neuron is the level  <dig>  clustering, variable. in this research design, which we refer to as design a, all observations from the same cluster belong to the same experimental condition . research design a has received considerable attention in neuroscience literature, emphasizing that such clustered data are common in neuroscience, and that statistical accommodation of the clustered nature of the data is crucial to avoid false positive results  .fig.  <dig> graphical illustration of nested data in research design a and b. in design a a, all observations in a cluster are subject to the same experimental condition. an example of this design is the comparison of wt and ko animals with respect to the number of docked vesicles within presynaptic boutons: bouton-measurements are typically clustered within neurons, and all measurements from the same neuron belong to the same experimental condition, i.e., have the same genotype. in this hypothetical example, we assume that a single neuron is sampled from each animal. if multiple neurons are sampled from the same animal, a third “mouse” level is added to the nested structure of the data. in research design b b, observations from the same cluster are subject to different experimental conditions. an example of this design is the comparison of neurite outgrowth in cells that are treated, or not , with growth factor . here, typically multiple observations from both treated and untreated neurons are obtained from, and so clustered within, the same animal



nested data, however, may arise in designs other than design a. in what we call research design b, observations from the same cluster are subjected to different experimental conditions. classical examples are studies in which mice from the same litter are randomized over different experimental treatments. research design b is common in the clinical and preclinical neurosciences  <cit> , but is also employed in the basic neurosciences. examples include studies on the effect of different pharmacological compounds, recombinant proteins, or sirna’s on cellular or subcellular features, where the experimental treatment is applied to different tissue samples of the same animal . other examples include the comparison of morphological features from animals or tissue samples, where each animal or tissue sample provides multiple measurements on different morphological features. examples of research design b data in biological neuroscience are given in table  <dig> table  <dig> examples of research design b nested data in biological neuroscience



in neuroscience literature, the discussion of research design b has been limited to the case in which the experimental effect is assumed to be the same for all clusters  <cit> . this is a strong assumption, and there is often no reason to believe that the experimental manipulation will indeed have exactly the same effect in each cluster. here we show that even a small amount of variation in the experimental effect across clusters inflates the false positive rate of the experimental effect, if that variation is not accommodated in the statistical model.

the aim of the present paper is to describe the intricacies of research design b, and explain how these can be accommodated in multilevel analysis . in neuroscience, the research question in nested designs is often formulated at the level of the individual observations. however, as a result of the clustering, the individual observations may show dependency, and this dependency needs to be accommodated in the statistical analysis. first, we briefly discuss research design a. second, we focus specifically on the defining features of research design b, and show how these can be accommodated in multilevel analysis. third, we demonstrate through simulations that misspecification of the statistical model for data obtained in design b results either in increased type i error rate , or in decreased statistical power to detect the experimental effects. finally, we discuss the use of cluster-related information to explain part of the variation in the experimental effect, with the aim of increasing statistical power to detect the experimental effect, and facilitating the biological understanding of variation in this effect.

research design a
in research design a, multiple observations are collected in the same cluster, and only one experimental condition is represented in each cluster . we recently emphasized that design a is common in neuroscience research: at least 53 % of research papers published in  <dig> high profile neuroscience journals concerned data collected in this design  <cit> . this design has received some attention in the neuroscience literature, focusing specifically on ways to correctly analyze such data . our central message was that multiple measurements per cluster  cannot be considered independent observations, since measurements from the same cluster tend to be more similar to each other than to measures from different clusters. this can result in systematic differences between clusters, i.e., the mean of the dependent variable varies across clusters. clustering implies that this variation exceeds that arising from random sampling fluctuation of individual observations within a cluster <dig> . standard statistical techniques, such as regression analysis, t test, and anova are unsuited to analyze clustered data, because these techniques rely on the assumption that all observations are independent. given dependency, they produce underestimated standard errors, and so underestimated p values. the result is  inflation of the type i error rate, i.e., false positive rate .

there are two ways to handle research design a data. one can average across all observations within each cluster and apply standard techniques using these means, which are independent observations. alternatively, avoiding the data reduction associated with such averaging, a multilevel model can be used to accommodate the nested structure. in multilevel analysis, the comparison of experimental conditions is conducted on the cluster level means, while retaining the distinction between the variance within clusters  and variance between clusters . see “box 1” for a description of the statistical multilevel model for design a. of these two approaches, multilevel analysis is preferable as it exploits all available information, and confers the greatest statistical power  <cit> . the multilevel model also allows one to obtain the intracluster correlation , which quantifies the degree to which measurements from the same cluster are more similar to each other than to measures from different clusters. the icc ranges between  <dig>  and  <dig> . the icc is the standardized version of the variance between clusters, denoted by σu <dig>  and also referred to as the intercept variance .

box 1: multilevel model for research design a
in the multilevel model for research design a, the nested structure of the data is accommodated by specifically incorporating the variation in cluster means, i.e., in the intercepts, in the statistical model. in case of a 2-level multilevel model, we have a level  <dig> model, the model of the individual observations, and a level  <dig> model, the cluster level model. the level  <dig> model takes on the following form:  <dig> yij=β0j+eijwitheij∼n, i.e., the dependent variable y for observation i from cluster j is predicted from the cluster j specific mean value of y in cluster j, denoted by the cluster-specific intercept β0j, and the zero mean residual eij. the variation in the intercept is specifically modeled in the cluster level model. without incorporating an experimental effect of a cluster level experimental manipulation, the cluster level model is:  <dig> β0j=γ00+u0jwithu0j∼n, where γ <dig> is the overall mean value of y calculated across all clusters, and u0j is the cluster j specific deviation from that overall mean value. hence, a distinction is made between ‘between cluster variation’, σu <dig> , and the remaining within clusters variation, σe <dig> . greater variation between clusters corresponds to a higher relative similarity of observations from the same cluster. therefore, a standardized measure for dependency is given by the intracluster correlation , which represents the degree of relative similarity of observations from the same cluster and is obtained by:  <dig> icc=σu02σu02+σe <dig>  i.e., the variance between clusters divided by the total variance in the data. this icc ranges between  <dig>  and  <dig> .

commonly, designs in neuroscience involve two conditions, e.g., a control and an experimental condition. to extend the model to include the effect of an experimental manipulation at the cluster level , and  explain the differences in mean between clusters , we expand the cluster level model in eq.  <dig> as follows:  <dig> β0j=γ00+γ01∗zj+u0jwithu0j∼n, where zj is a  dummy coded indicator variable that denotes the experimental condition of cluster j , γ <dig> is the overall intercept, that denotes the overall mean in the control condition given that the indicator variable zj equals  <dig> for the control condition, γ <dig> is the overall deviation of the experimental condition from the control condition given that the indicator variable zj equals  <dig> for the experimental condition , and u0j is the cluster specific deviation from the overall intercept that remains after taking the effect of the experimental condition into account. note that if σu02 =  <dig>  clustering is effectively absent, which renders the subscript j superfluous. in that specific case, the model at the level of the individual observations reduces to yi=γ00+γ01∗z+ei, i.e., the standard t test written in regression terms.

having discussed research design a, we now present the defining features of research design b nested data, and explain how these features can be accommodated in multilevel analysis.

research design b
design b data differ from design a data in that observations collected within the same cluster are allocated to different experimental settings . hence, both the mean value of the dependent variable and the effect of the experimental manipulation may vary over clusters .fig.  <dig> graphical representations of variants of research design b data. different possible combinations of cluster-related variation in the mean value of the control condition  and cluster-related variation in the experimental effect , illustrated for  <dig> clusters of data: no cluster-related variation , only cluster-related variation in the intercept , only cluster-related variation in the experimental effect , or cluster-related variation in both the intercept and the experimental effect 



again one can handle the dependency by calculating a summary statistic per experimental condition per cluster, and then using a standard statistical model  to analyze the summary data. for example, when investigating the effect of a growth factor on neurite outgrowth, one could obtain the mean neurite outgrowth for the treated and untreated cells per mouse, and use these in the statistical analysis. however, using such summary statistics is not recommended. summarizing implies a loss of information, and therefore may result in a loss of statistical power to detect the experimental effect of interest. in addition, this may result in incorrect parameter estimates if the cluster sample sizes vary   <cit> . another option in analyzing design b data is to take the cluster effect into account by including it as a factor in standard regression analysis . that is, if there are n clusters, the regression analysis would contain an indicator variable that denotes the experimental condition of the observation, plus n −  <dig> indicator variables that denote cluster membership . this solution is only practical if the number of clusters is small. besides, it restricts the statistical inference to the clusters in the study and no other clusters, rendering this approach unsuited for generalization to the population  <cit> . specifically, in this approach, the clusters are regarded as fixed, and not as a random sample from the general population, which limits the generalizability of the obtained research results. in addition, this approach does not easily allow quantification of the amount of cluster-related variation in the experimental effect, which can be informative on itself. in multilevel analysis of design b data, the clusters are regarded as random samples from the general population. the comparison between experimental conditions is conducted on the experimental condition specific means within clusters, while including information on the variance within clusters  and the variance in the experimental effect over clusters . see “box 2” for a description of the multilevel model for design b. multilevel analysis uses all available information, can be used with varying cluster sample sizes, allows generalization to the general population, and quantifies the amount of cluster-related variation and is therefore the preferred statistical approach. in multilevel analysis, cluster-related variation in the experimental effect is quantified by the variance of the experimental effect over clusters, which we denote by σu <dig> .

box 2: multilevel model for research design b
to accommodate possible variation in the effect of the experimental manipulation across clusters, the model at the level of the individual observations given in eq.  <dig> is extended as follows:  <dig> yij=β0j+β1j∗xij+eijwitheij∼n, i.e., the experimental effect, denoted by β1j, and the variable xij indicating the experimental condition of observation yij, are now defined at the individual observational level instead of on the cluster level . the experimental effect β1j now accounts for systematic differences between observations within a cluster, rather than systematic differences between observations in different clusters. how much observations within a cluster differ between experimental conditions can vary over clusters , resulting in a cluster level model for both the intercept β0j, and the cluster-dependent experimental effect β1j:  <dig> β0j=γ00+u0j,and   <dig> β1j=γ10+u1j,withu0ju1j∼nσu02σu <dig> u12σu <dig> u12σu <dig> i.e., the experimental effect β1j, just like the intercept β0j, is composed of an overall experimental effect across all clusters γ <dig>  and a cluster specific deviation from that overall experimental effect, u1j. the variance of the experimental effect over clusters is noted by σu <dig> . in this extended model, the parameter β0j represents the cluster-specific intercept, which is now interpreted as the mean of the observations belonging to the control condition of that particular cluster . β1j, in turn, represents the cluster-specific deviation from β0j of those observations in the cluster belonging to the experimental condition. the variance of the cluster-dependent experimental effect σu <dig> is often referred to as the slope variance, as β1j is often referred to as the cluster-specific slope parameter.

note that models that include both intercept variance and slope variance may include a covariance between the random intercept and random slope noted by σu <dig> u <dig> . when comparing a control and an experimental condition using a 0/ <dig> dummy coded indicator, the intercept represents the mean value of the dependent variable y of the control condition, and the slope represents the deviation from this mean value for the experimental condition. in this case a positive covariance between intercept and slope implies that higher values in the control condition coincide with larger experimental effects, while a negative covariance implies that higher values in the control condition coincide with smaller experimental effects. an example of negative covariance is when cells whose neurons show a relatively large value for neurite outgrowth in the control condition, tend to show a smaller effect of the growth factor.

in additional file  <dig>  a worked example of multilevel analysis of research design b data is presented. a detailed and accessible explanation of multilevel modeling, including details of the statistical analysis, can be found in hox, goldstein, and snijders and bosker . note that when performing multilevel analysis, a sufficient number of clusters, and observations per cluster, are required to obtain stable and unbiased estimates  <cit> . to obtain unbiased estimates of the overall experimental effect and its standard error, a minimum of  <dig> clusters and  <dig> observations per experimental condition per cluster is recommended. if one also requires accurate estimates of the cluster-related variation in the intercept  and, for research design b specifically, the experimental effect, a minimum of  <dig> clusters is recommended. hence, careful planning of the research design is required when dealing with nested data, as multilevel analysis requires sufficient observations on both levels. when the number of clusters is small , but the number of observations within each cluster is large , bayesian estimation methods are an alternative, as these have proven to yield less biased estimates than maximum likelihood approaches in this specific instance  <cit> . another option would be to use fixed effects regression, in which the obtained research results are only valid for the clusters in the study.

in the few methodological papers in the neuroscience literature that discussed the analysis of research design b data, the focus has been on the special case that the experimental effect is invariant over clusters  <cit> . this is a strong assumption, which may not hold. hence, the possibility that the experimental effect varies over clusters should be taken into account in the statistical model. intrinsic biological variation, and small differences in the experimental conditions or measurement procedures all may cause differences between clusters, and consequently differences in the experimental effect over clusters. for instance, in the growth factor experiment, not all experiments may be performed using the same batch of growth factor, which can result in variation in the experimental effect.

the failure to accommodate variation in the intercept , and/or experimental effect over clusters  in research design b data can result in incorrect inferences concerning the experimental effect. the exact consequence of ignoring the dependency depends on the effect of clustering. first, if in research design b data clustering is a source of variation in the intercept, but not a source of variation in the experimental effect , the failure to accommodate clustering , may result in a loss of statistical power to detect the experimental effect <dig>  <cit> . a correctly specified multilevel model does not incur this loss of power because it effectively accommodates the otherwise unexplained variation in the intercept over clusters, thus providing a better signal-to-noise ratio. below, we examine the loss in statistical power that may arise when conventional methods are used to analyze research design b data given an experimental effect that does not vary over clusters. we express this loss in power as a function of various characteristics of the data.

second, if clustering is a source of variation in the experimental effect , standard errors obtained in standard statistical models and multilevel analysis in which random effects are incorrectly specified, are likely to be underestimated  <cit> . this results in a downward bias in p values, and consequently an inflated type i error rate  that exceeds the nominal α-level . however, the degree of inflation produced in these models, and variation in the type i error rate as a function of variation in the experimental effect over clusters, has not been demonstrated before. moreover, little is known about the effect on the type i error rate associated with standard statistical models and misspecified multilevel analysis, given systematic variation in both experimental effect and the intercept .

the aim of this paper is to illustrate by means of simulation how misspecification of the statistical model for research design b data affects the false positive rate and statistical power.

methods
we use randomly generated  datasets to illustrate the effects of cluster-related variation in design b data on results of various statistical tests. we varied the magnitude of the experimental effect, the amount of cluster-related variation in the intercept and in the experimental effect, and the sample size. we determined how these variables influenced the obtained results. we considered a design with two experimental conditions, which we refer to as the control and the experimental condition. the generated datasets were analyzed using the following four statistical methods: a t test on the individual observations , a paired t test on the experimental condition specific cluster means, a multilevel model on the individual observations that only accommodates cluster-related variation in the intercept , and a multilevel model on the individual observations that accommodates cluster-related variation in both the intercept and the experimental effect . note that the standard statistical methods on summary statistics produces correct parameter estimates only if the sample sizes are equal over clusters and experimental conditions  <cit> . an overview of the parameter settings for each simulation study is provided in table  <dig> table  <dig> parameter settings used to generate the simulated datasets

study
aim of study
σu12
icc
d
n
nc

 absent
 present
the simulations with no cluster-related variation in the experimental effect  investigate the effect on the statistical power to detect the experimental effect in case that variation in the intercept is present but not accommodated. hence, in studies 1a and 1b, the magnitude of the overall effect of the experimental manipulation, expressed by effect size d, exceeds zero . the simulations including cluster-related variation in the experimental effect  investigate the effect on the false positive rate in case that this variation in the experimental effect is not accommodated in the statistical model. hence, in studies 2a and 2b, the magnitude of the overall effect of the experimental manipulation equals zero 


icc intracluster correlation, denoting the extent of dependency in the data, n number of clusters, nc number of observations per experimental condition per cluster



first, we illustrate the effect on the statistical power to detect the overall experimental effect in the specific case that variation in the intercept is absent, or present but not accommodated, and cluster-related variation in the experimental effect is absent . that is, we ask: if data is generated according to fig. 2a, b, how does the statistical power compare across the four statistical methods ?

second, we illustrate the effects of the presence of cluster-related variation in the experimental effect, in combination with either absent or present cluster-related variation in the intercept, on the false positive rate of the experimental effect . that is, we ask: if data is generated such that overall, i.e., taken over all clusters, the experimental manipulation has no effect, but the data includes cluster-related variation in the experimental effect, what is effect on the false positive rate? we illustrate this in case that the data includes no cluster-related variation in the intercept , or does include cluster-related variation in the intercept , and compare the false positive rate across the four statistical methods .

for all scenarios, we generated  <dig>  datasets. to establish statistical power in studies 1a and b, and the empirical false positive rate in studies 2a and 2b, we counted the number of times that the overall experimental effect was found to be statistically significant given α =  <dig> . the datasets were generated such that the experimental effect is expressed in terms of the effect size d , where we considered the effects  <dig> ,  <dig> , and  <dig>  to be small, medium, and large, respectively  <cit> . condition was dummy coded  <dig>  and  <dig> , such that the amount of cluster-related variation in the experimental effect σu <dig> could be interpreted according to the guidelines of raudenbush and liu  <cit> . accordingly, σu <dig> values of  <dig> ,  <dig> , and  <dig>  are considered small, medium, and large, respectively.

to understand the amount of variation in the experimental effect, consider a medium experimental effect of d =  <dig> . if the variation in the experimental effect is small, i.e., σu12 =  <dig> , this corresponds to a standard deviation of ~ <dig> . assuming normally distributed cluster specific deviations from the overall effect size, β1j, ~95 % of the cluster-specific experimental effects would lie between ~ <dig>  and ~ <dig>  . using the dummy coding  <dig> and  <dig> also ensures that the intercept variance equals the cluster-related variation in the intercept of the control condition in case that both the intercept and the experimental effect show cluster-related variation. the covariance between the intercept and the experimental effect was set to zero in all simulations.

all simulations were performed in r  <dig> . <dig>  <cit> , and multilevel models where fitted using the r package lme <dig>  <cit> . the r-code is available upon request from the corresponding author.

RESULTS
ignoring cluster-related variation can result in interpretational errors
our simulation results showed that the failure to accommodate the cluster-related variation in either intercept or slope  can result in interpretational errors. a general overview of all results is given in table  <dig>  below, we discuss the results of studies 1a and 1b  and studies 2a and 2b  in detail.table  <dig> consequences of not accommodating cluster-related variation in research design b

statistical power
a
study 1a
study 1b

variation in experimental effect

 absent
t test ind. obs.

t test summary st.
multilevel analysis i
decreased power
correct
decreased power
correct
false positive rate
study 2a
study 2b

 present
t test ind. obs

t test summary st.
multilevel analysis i
correct
increased false positive rate
correct
increased false positive rate
the results of four statistical tests to detect the experimental effect are compared with respect to  statistical power to detect the  experimental effect  and  false positive rate . fitted statistical models are a t test on individual observations , a paired t test on the experimental condition specific cluster means , a multilevel analysis that does not accommodate the variation in the experimental effect but does accommodate variation in the intercept , and a multilevel analysis that accommodates both variation in the intercept and in the experimental effect 


ain case that variation in the experimental effect is absent, all fitted statistical models result in a false positive rate that does not exceed the nominal α specified by the user 



ignoring variation in the intercept in design b data can decrease statistical power
the obtained results are equal for the multilevel model that only includes variation in the intercept, and the multilevel model that includes variation in both the intercept and the experimental effect. therefore, we do not differentiate between the two types of multilevel analysis in this section.

in case of design b data that includes no cluster-related variation in the intercept or experimental effect , conventional statistical analysis  on individual observations is equally powerful as multilevel analysis, but using multilevel analysis is more powerful compared to conventional statistical analysis  on summary statistics . the loss in statistical power when using conventional statistical analysis on summary statistics is only present when the number of clusters is small .fig.  <dig> use of conventional analysis methods on design b data can result in a loss of power. using conventional analysis methods to model design b data that includes cluster-related variation in the intercept and no cluster-related variation in the experimental effect  results in a loss of statistical power compared to using a multilevel model. the presented results are equal for the multilevel model that only includes variation in the intercept, and the multilevel model that includes variation in both the intercept and the experimental effect. fitted conventional analysis methods were a a t test on individual observations and b a paired t test on the experimental condition specific cluster means. the loss in statistical power is overall greatest when both the number of clusters and effect size d are small and the cluster-related variation in the intercept is considerable. in case that the cluster-related variation in the intercept and in the experimental effect both equal zero , using a t test on individual observations is equally powerful as multilevel analysis, but using multilevel analysis is more powerful compared to a paired t test on summary statistics. the actual statistical power of multilevel analysis given σu12 =  <dig>  =  <dig>  or  <dig> , n =  <dig>  and increasing numbers of observations per experimental effect per cluster is given in fig. 5b, solid line




analyzing design b data that only includes cluster-related variation in the intercept  using conventional statistical analysis  sometimes results in a loss of statistical power. compared to using a t test on individual observations, the difference in statistical power is greatest when both the number of clusters and the magnitude of the experimental effect are small, and the amount of cluster—related variation in the intercept is large . for example, in case of substantial cluster-related variation in the intercept giving rise to icc =  <dig> , using a t test on individual observations is ~25 % less powerful than multilevel analysis, given  <dig> clusters and an effect size of  <dig> . in case that the overall experimental effect is medium , multilevel analysis only results in more statistical power given substantial cluster-related variation in the intercept , and a small number of clusters and small number of observations per experimental condition. compared to using a paired t test on experimental condition specific cluster means, the loss in statistical power compared to multilevel analysis is only present when the number of clusters is small , and does not depend on the amount of cluster-related variation in the intercept.

the occasionally observed increase in loss of power as function of increasing number of observations per experimental condition per cluster is due to the fact that multilevel analysis gains in power with increasing number of observations per experimental condition per cluster. the observed decrease in loss of power when the number of observations per experimental condition per cluster increases, is due to the fact that multilevel analysis approximates the maximum power of 100 %, and thus the difference in statistical power between multilevel analysis and conventional analysis methods becomes smaller. the actual statistical power of multilevel analysis given no cluster-related variation in the experimental effect, an effect size d of  <dig>  or  <dig> ,  <dig> clusters, and increasing numbers of observations per experimental effect per cluster is provided in fig. 5b .

in summary, the failure to take into account the hierarchical nature of data or using summary statistics, results in a loss of power to detect the experimental effect, especially when both the number of clusters and the overall effect are small. neuroscience studies often report small effects, and may be underpowered due to small sample size  <cit> . multilevel analysis of research design b data can increase statistical power compared to conventional analyses, unless of course the statistical power of the conventional analysis approaches  <dig> 

ignoring variation in the experimental effect increases the false positive rate
given clustering with respect to the experimental effect, the use of a statistical model on individual observations that does not accommodate this variation results in an inflated false positive  rate. first, when variation in the intercept is absent , ignoring variation in the experimental effect results in an actual false positive rate as high as ~20–~50 % , depending on the number of observations per cluster and the amount of variation in the experimental effect. specifically, if the overall experimental effect is zero, both a conventional t test and misspecified multilevel analysis , yield similarly inflated type i error rates . even a very small amount of cluster-related variation in the experimental effect  can results in a type i error rate of ~20 % if it is not accommodated in the statistical model. in summary, the failure to accommodate cluster-related variation in the experimental effect results in a substantial inflation of the type i error rate, and this inflation is considerable even when variation in the experimental effect is small.fig.  <dig> ignoring variation in the experimental effect results in inflated false positive  rate. inflation of the type i error rate already occurs when a small amount of variation in the experimental effect  remains unaccounted for in the statistical model, and occurs both when the intercept  is invariant over clusters , and when the intercept varies substantially over clusters . in panel
a, the lines depicting conventional analysis  and misspecified multilevel analysis completely overlap. using a paired t test on the experimental condition specific cluster means results in a correct type i error rate. in panel 
b, the lines depicting the paired t test and the correctly specified multilevel analysis completely overlap



second, when variation is present in both the intercept and the experimental effect , accommodating only the cluster-related variation in the intercept , or not accommodating cluster-related variation at all  again results in an inflated type i error rate . if the variation in the intercept is large , the type i error rate increases up to approximately 35 % when using a conventional t test. when using a multilevel analysis that only accommodates cluster-related variation in the intercept, the inflation in the type i error increases up to approximately 50 %. in summary, the substantial inflation of the type i error rate that arises if cluster-related variation in the experimental effect is not accommodated arises irrespective of the presence of variation in the intercept.

accommodating cluster-related variation in the experimental effect by either using correctly specified multilevel analysis or using conventional models on summary statistics , does result in a correct type i error rate . see “box 3” for a detailed explanation of why ignoring cluster-related variation in the experimental effect results in an increased false positive rate.

box 3: inflation of the false positive rate in research design b
by considering the standard error of the overall experimental effect in multilevel analysis, seγ <dig>  we can clarify why increasing cluster-related variation in the experimental effect σu <dig> and/or increasing sample size per cluster nc  results in an inflated type i error in conventional analysis  on individual observations in nested data. in multilevel analysis,   <dig> seγ10=nc∗σu12+σe2nc∗n where n denotes the number of clusters, and σe <dig> denotes the residual error variance. in the t test, in contrast, the standard error of the experimental effect, denoted as seβ <dig>  is  <dig> seβ1=σe2nc∗n. comparing eqs.  <dig> and  <dig>  we see that ignoring non-zero cluster-related variation in the experimental effect, σu <dig>  results in an underestimation of seβ <dig>  and consequently in downward biased p values. the degree of underestimation depends on the number of observations per cluster nc, and on the amount of cluster-related variation in the experimental effect, σu <dig> important to note is that, first, eqs.  <dig> and  <dig> hold in the case of standardized data  and a balanced design . secondly, in eq.  <dig>  σe <dig> is actually a composite of all sources of variation that remain unexplained in the conventional analysis model, i.e., the actual residual error variance, but also the intercept variance and the variance in the experimental effect over clusters. thirdly, the intercept variance σu <dig> plays no role in eq.  <dig>  this explains why the type i error rate of the misspecified multilevel model in simulation studies 2a and 2b  is unaffected by the value of the icc. that is, the obtained type i error rate of the misspecified multilevel model is equal for the simulation studies that do and do not include cluster-related variation in the intercept .

explaining part of the variation in the experimental effect: increasing both theoretical insights and power
the simulation studies have shown that ignoring cluster-related variation in the intercept  and experimental effect  may result in incorrect inference concerning the experimental effect. however, it is important to emphasize that these variance terms are not merely “noise” i.e., nuisance parameters: they can advance our biological understanding, and can be of practical interest. here, we focus specifically on the information that can be obtained from cluster-related variation in the experimental effect.

variation in the experimental effect is informative about the generalizability of the experimental effect  <cit> : is the impact of the experimental manipulation similar across  different settings? in some instances, sources of cluster variation may be difficult to measure. for example, when investigating a feature at the cellular level in neurons taken from mouse embryos, it is conceivable that not all neurons were harvested at the exact same embryonic age, resulting in different levels of neuron maturation. the variation in neuronal maturation may in turn influence the magnitude of the effect of the experimental manipulation. in this case, multilevel modeling accommodates the variation in outcome due to different levels of neuron maturation, despite the fact that neuron maturation is not explicitly measured. an added advantage arises when possible sources of cluster-related variation can be recorded: they can be used to  explain the cluster-related differences in the experimental effect by including them in the model . as such, recorded sources of cluster variation can facilitate the understanding of the conditions in which the experimental manipulation does or does not have an effect, and thus of the generalizability of this effect. for instance, suppose pregnant mice are administered fluoxetine in their food, which induces life-long cortical abnormalities in the pups. the food intake, and therefore drug intake, which may vary between mice, can easily be measured. if we are interested in a certain drug b that is hypothesized to counteract these developmental changes upon treatment of the pups, we can administer drug b to half of the pups in each nest, and use the remaining pups as controls. besides the counteractive effect of drug b on drug a, we may investigate whether a measure of food  intake in the mothers explains some of the variation observed in the effect of drug b on the  cortical abnormalities. in this case, we might learn that the extent to which drug b can alleviate the detrimental effects of fluoxetine depends on the level of fluoxetine exposure.

explaining variation in the experimental effect by one or more variables is achieved by adding those variables as covariates to the model. the broad definition of a covariate is a variable that is used to adjust the predicted outcome y for differences associated with the covariate, which is measured before  the outcome variable y, and correlates with y  <cit> . note that in order to  explain the cluster-related differences in the experimental effect, one needs a cluster-level covariate, like food intake of the mother mouse in the current example.

besides the fact that explaining part of the variation in the experimental effect can advance our biological understanding, adding a relevant covariate to the statistical model can also increase the statistical power to detect the overall experimental effect. specifically, by  accounting for the cluster-related variation in the experimental effect, the remaining unexplained cluster-related variation in the experimental effect σu <dig> decreases. as shown in eq.  <dig> in “box 3”, a decreased σu <dig> results in a decreased standard error of the overall experimental effect, and hence an increased statistical power to detect this effect. as such, adding a cluster-level covariate to the model can be of practical interest, as it can boost statistical power without, or additional to, increasing sample size.

there are, however, some considerations regarding the inclusion of covariates. first, in case that the statistical model includes covariates, the estimated value, and hence the interpretation, of the experimental effect is conditional on the covariates. in our example, the research question would change from “does drug b counteract the cortical abnormalities caused by prenatal fluoxetine exposure?” to “when correcting for relative differences in prenatal fluoxetine exposure, does drug b counteract cortical abnormalities caused by prenatal fluoxetine exposure?”. note that when the covariate is a design variable, e.g., batch number, conditional interpretation of the experimental effect is biologically no different from the unconditional interpretation: we simply correct for measurement noise that we are not interested in.

second, if a relevant covariate is not measured routinely within the experimental setup, but needs to be measured specifically, this may involve an increase in research costs. if the sole reason to include the covariate is to increase statistical power , one needs to consider the return in power of these costs  <cit> . also, how much power is gained by decreasing the unexplained variation in the experimental effect depends on the allocation of sample sizes over clusters, and on number of observations per cluster . the program pint  <cit>  can be used to evaluate how much a particular covariate increases power given the amount of explained variation in the experimental effect and the allocation of sample sizes. in addition, one has to keep in mind that sample size can put a limit on how many parameters, hence covariates, can be added to the statistical model. so careful planning of the study, including the intended covariates, is advised.fig.  <dig> power of multilevel analysis to detect the overall experimental effect in research design b. power is depicted in nine conditions  and as function of the number of clusters  or the number of observations per cluster per condition . in both a and b, two experimental conditions are compared, using a balanced research design. as the cluster-related variation in the intercept in research design b does not influence the statistical power to detect the overall experimental effect , the icc does not feature in this figure. in a, the number of observations is held constant at  <dig> observations per condition in each cluster; in b, the number of clusters is held constant at  <dig>  evidently, the number of clusters, and not the number of observations per cluster, is essential to increase the statistical power to detect the experimental effect



third, one can also attempt to increase power by including a covariate at the level of the individual observations. a covariate at the level of the individual observations can increase statistical power if it  explains why observations within a condition within a cluster vary with respect to the dependent value. this decreases the residual error variance σe <dig> of the model, and hence increases the statistical power to detect the overall experimental effect. for example, when performing sirna-mediated knockdown in neurons, one could measure knockdown efficiency for each neuron besides the neuronal measurement of interest. one can observe in eq.  <dig> in “box 3”, however, that a reduced  variation in the experimental effect σu <dig> has a greater effect on decreasing the estimated standard error of the overall experimental effect than a reduced residual error variance σe <dig>  therefore, including a covariate at the individual observation level to increase statistical power is only advisable if the covariate is expected to explain a considerable amount of variation within condition within clusters.

maximizing power by optimally allocating sample sizes
in conventional analyses, ensuring sufficient statistical power to detect the experimental effect of interest is usually accomplished by calculating the required total number of observations to collect. when it comes to statistical power in multilevel analyses, however, one has to determine the sample size at two levels: the sample size at the individual level, i.e., the number of observations per cluster, and the sample size at the cluster level, i.e., the number of clusters. the number of observations per cluster, and the number of clusters, do not affect statistical power equally, and are often not equal in costs. hence, a key question is how to optimally allocate observations over clusters, balancing both power and costs. an excellent account of power in multilevel analysis in case of design b and a cost-benefit analysis of power is provided in  <cit> . for a balanced  2-level multilevel model without covariates, we provide a brief summary in additional file  <dig>  which includes an explanation of how to calculate the estimated power for a given number of observations per cluster n, number of clusters n, and choices of other key parameters in the model.

in design b, a larger number of observations per cluster provides precision on the estimate of the experimental effect within a cluster, while a larger number of clusters provides precision on the overall experimental effect. hence, the power to detect the overall experimental effect benefits most from increasing the number of clusters n. this is illustrated in fig. 5: the statistical power to detect the overall experimental effect steadily increases to 100 % as the number of clusters increases , while increasing the number of observations per cluster per condition sometimes results in a plateau  lower than 100 % . how much power increases as a result of extra observations per clusters depends on the amount of cluster-related variation in the experimental effect.

CONCLUSIONS
to draw valid conclusions in a nested experimental design, it is crucial to use the appropriate statistical method. we showed previously that design a data  are abundant in neuroscience literature, and that proper statistical analysis of such data is crucial to avoid false positives  <cit> .

here, we showed that in case of design b data , correct statistical modeling of such data is also critical to avoid incorrect inference. however, in case of design b data, the exact consequences of ignoring the dependency depend on the nature of clustering. if cluster-related variation in the experimental effect is present, not accommodating this cluster-related variation results in an inflated false positive rate. that is, in design a data, not accommodated variation in the intercept results in an inflated false positive rate, while in design b data variation in the experimental effect causes inflation in the false positive rate when not accommodated. importantly, inflation of the false positive rate already occurs with a small amount of cluster-related variation in the experimental effect. in addition, if cluster-related variation is limited to the intercept , failure to correctly accommodate this variation can result in a loss of statistical power to detect the experimental effect of interest. the loss in statistical power when using conventional analysis methods  on individual observations instead of correctly specified multilevel analysis is noteworthy when both the number of clusters and the overall effect are small. in addition, we showed that using standard statistical methods on summary statistics  does result in a correct false positive rate, but results in a loss of statistical power to detect the experimental effects of interest when the number of clusters is small. importantly, the use of standard statistical methods on summary statistics only results in correct parameter estimates if the sample sizes are equal over clusters and experimental conditions   <cit> .

finally, multilevel analysis can provide valuable insight into the generalizability of the experimental effect over  varying settings, and can be used to utilize cluster-related information to explain part of the variation in the experimental effect. therefore, multilevel analysis not only ensures correct statistical interpretation of the results, and thus correct conclusions, but can also provide unique information on the collected research data that cannot be obtained when standard statistical methods are used on either individual observations or summary statistics.


additional files

 <dig> /s12868-015-0228- <dig> effect of neurite location  on traveling speed of intracellular vesicles: a worked example. an example of multilevel analysis of research design b data, including syntax to perform multilevel analysis in the statistical packages spss and r.


 <dig> /s12868-015-0228- <dig> calculating the optimal allocation of sample sizes and estimating statistical power to detect the overall experimental effect. explanation on how to calculate the optimal allocation of sample sizes over clusters and within clusters given the available resources, and explanation on how to estimate power for a balanced  2-level multilevel model without covariates.



abbreviations
iccintracluster correlation

pintpower in two-level designs

 <dig> in the context of rcbd, the term “nested” is used to describe how experimental manipulations or treatments are combined. specifically, treatments are referred to as nested if the various experimental conditions of treatment b do not appear with every level of treatment a   <cit> . in multilevel literature, however, the term nesting is used to describe the data. specifically, nested data are characterized by a hierarchical, multi-level structure in which individual observations are clustered, or nested, within hierarchically higher organized groups or clusters  <cit> .

 <dig> let mi denote the true mean in the cluster i, and s denote the true within-cluster standard deviation, assumed to be equal for all clusters . let mi and si denote the estimated mean and standard deviation in cluster i based on ni observations. the standard error of the mean mi equals s.e. = si/√. if all mi in a given experimental condition  are equal across clusters, s.e. reflects the variation in cluster means that is solely due to random sampling fluctuation. in that case, the data can be viewed as independent, and can be analyzed using standard statistical models. however, due to systematic differences between the clusters, clustering often gives rise to variation in mi that exceeds this random sampling fluctuation. in the case that data display such  dependency, multilevel analysis is called for. the systematic differences between clusters may be due to known  or unknown factors.

 <dig> in a previous paper specifically on design a data, we showed that not accommodating variation in the intercept results in an increased false positive rate. to avoid confusion on the effect of not accommodating variation in the cluster-related intercept, let us note the following. in design a data, the experimental effect is at the cluster level and thus explains systematic differences between clusters. in this case, variation in the intercept thus represents variation in the outcome of one of the experimental conditions. when this variation in the outcome is not taken into account, this results in a too precise estimate of the experimental effect , and hence an increased false positive rate. in design b data however, the experimental effect is at the level of the individual observations, and is thus explains systematic differences within clusters and hence determined within each cluster separately. variation in the intercept  here represents fluctuations that do not influence the size of the experimental effect within each cluster. not accommodating variation in the intercept in research design b thus results in a lower signal to noise ratio, hence decreased statistical power, instead of a higher false positive rate.

authors’ contributions
ea planned and carried out the study, performed the simulation analysis and drafted the manuscript. mv, cvd and svds provided constructive input. all authors read and approved the final manuscript.

