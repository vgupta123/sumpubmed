BACKGROUND
the frequency of an allele in the population is a fundamental quantity in human statistical genetics. this quantity forms the basis of many population and medical genetic studies. many evolutionary forces change allele frequencies. consequently, allele frequencies can be used to infer past evolutionary events. for example, allele frequencies at single nucleotide polymorphisms  can be used to infer the demographic history of a population  <cit> . patterns of allele frequency are also informative about the possible effects of natural selection. after a completed selective sweep, an excess of low-frequency and high-frequency derived snps is expected around the selected site  <cit> . conversely, snps under the direct influence of negative selection are expected to be at lower frequency than predicted by demography alone  <cit> . many commonly used summary statistics in population genetics like tajima's d  <cit> , fu and li's d  <cit> , fay and wu's h  <cit>  and fst 
 <cit>  are direct functions of allele frequencies. allele frequencies also form the basis of association studies between snps and common disease. in their simplest form, case-control association studies seek to quantify the difference in allele frequency between cases  and controls   <cit> . in particular, there has been rapidly growing interest in performing association studies between rare variants and common disease using data obtained from next-generation sequencing approaches  <cit> .

given the importance of allele frequencies in genetic studies, it is critically important to be able to estimate them reliably. traditionally, allele frequencies were simply estimated by counting the number of times each allele had been seen in a sample from the population. this approach was often successfully used on snp genotype data and sanger sequencing data because the genotypes for each individual could often be unambiguously determined. however, this approach may fail when applied to data from next-generation sequencing technology. first, next-generation sequencing data has a higher error rate than traditional sanger sequencing or snp genotyping assays  <cit> . second, in order to sequence more samples, researchers often sequence each individual at shallow coverage . thus, each base will only be covered by a few reads, making it more difficult to accurately infer an individual's genotype at a particular site  <cit> . finally, because the reads from next-generation sequencing technologies are often quite short, additional errors can occur when trying to align the short reads back to the reference genome  <cit> . for these reasons, estimating allele frequencies remains challenging.

several different approaches have been proposed to attempt to make accurate inferences of allele frequency from next-generation sequencing technologies  <cit> . it is important to appreciate that no single approach has been consistently favored or endorsed by the community. instead, a variety of approaches have been proposed and used by different scientific groups. the first set of approaches uses the traditional paradigm of estimating allele frequencies by first inferring individual genotypes  <cit>  and then tabulating frequencies. here, strict filters are used to attempt to account for the increased error rate and uncertainty inherent in the data  <cit> . others have added linkage disequilibrium  information and data from reference haplotypes to make more accurate genotype calls  <cit> . the second set of approaches seeks to directly estimate allele frequencies from the next-generation sequencing data without first attempting to infer genotypes  <cit> . these approaches have the advantage that they directly estimate the quantity of interest without first inferring other uncertain information . the utility of this type of approach has yet to be fully explored for different types of population and medical genetic studies.

here we discuss the properties of a new likelihood approach designed to estimate the population minor allele frequency from next-generation sequencing data. we show that the new likelihood method can obtain accurate estimates of allele frequencies, even when the depth of coverage is quite shallow. further, we show that the new likelihood method either performs as well as, or better than, genotype calling methods. finally, we discuss the performance of the likelihood approach in testing for differences in allele frequency between cases and controls.

RESULTS
the minor allele is the less frequent allele in the population at a variable site. we first describe two main approaches to estimate the minor allele frequency  at a particular site in the genome. the first approach involves inferring individual genotypes and treating those inferred genotypes as being completely accurate when estimating the maf. we then examine the performance of a likelihood framework that directly takes the uncertainty in assigning genotypes into account. throughout our work, we assume that all segregating sites are biallelic.

estimation of maf from called genotypes
one way to estimate the maf from next-generation sequencing data is to first call a genotype for each individual using sequencing data, and then use those genotypes as if they are the true ones. this was the approach traditionally used for genotype data and sanger sequencing data. it is not clear how well it will perform when applied to next-generation sequencing data.

a maximum likelihood approach can be used to infer the genotype for each individual from the next-generation sequencing data. at each site j, for each individual i, the likelihood for each of the three possible genotypes  is given as:   

where di,j 
is the observed sequencing data in individual i at site j, gi
,
j 
âˆˆ { <dig>   <dig>  2} is the number of minor alleles contained in the genotype of each individual, and  and  control for sequencing errors and read base qualities, respectively. the observed sequencing data for each individual can be thought of as the alignment of reads at site j taking the read quality scores into account. this is represented as the genotype likelihood and is found in the genotype likelihood file  which is produced in many programs that analyze next-generation sequencing data, such as soapsnp and maq  <cit> .

to assign a genotype to a particular individual, the likelihood of each of the three possible genotypes can be calculated for the individual. the genotype with the highest likelihood can then be assigned. however, researchers often prefer a more stringent calling criterion and will not assign a genotype to an individual unless the most likely genotype is substantially more likely than the second most likely one. here the three possible genotypes are sorted by their likelihoods: , where g corresponds to the genotype with the kth largest likelihood. with a given threshold f, one can call the genotype g if . otherwise, a genotype is not called and the individual's genotype is considered missing. a common threshold value of f is  <dig>  indicating that the most likely genotype is at least  <dig> times more likely than the second most likely one. note that this type of filtering may result in higher confidence for the "called" genotype, but it also results in more missing data.

maximum likelihood estimator of allele frequency
instead of estimating the maf from the called genotypes, a maximum likelihood  method introduced by kim et al.  <cit>   directly estimates mafs and takes genotype uncertainty into account. specifically, given a minor allele, the probability of observing the sequence data at each individual i is obtained by summing over the probabilities corresponding to all three possible genotypes.

suppose that the three genotype likelihoods defined in equation  <dig> are available. using the same notation as above, let dj 
and pj 
be the observed sequencing data at site j and the corresponding maf, respectively. the genotype probability given that minor allele frequency can be computed by assuming hardy-weinberg equilibrium . then, assuming independence among individuals, the likelihood of the maf at this locus is a product of all the likelihoods computed across all n individuals:   

the ml estimate of pj 
can be computed either by directly maximizing the likelihood for a restricted parameter space using the broyden-fletcher-goldfarb-shanno  method  <cit>  or by using the expectation-maximization  algorithm  <cit> . when using the em algorithm, the posterior expectation of a genotype is computed for each individual, and the mean of those posteriors is repeatedly updated. our implementation of bfgs was faster than the em algorithm. for example, to obtain estimates from  <dig>  sites, bfgs took ~ <dig> seconds but em took ~ <dig> seconds. however, the difference in speed may be implementation specific. in our case, for both methods, we stopped updating parameters when the increase in the likelihood was less than  <dig> .

maximum likelihood estimator with uncertain minor allele
in practice, often the second most common nucleotide across individuals can be used as the minor allele. however, for rare snps , it is hard to determine which allele is the minor allele, since all four nucleotides may appear in some reads due to sequencing errors. to deal with this situation, we now describe a likelihood framework that takes the uncertainty in the determination of the minor allele into account.

suppose that for site j we know the major allele m. note that deciding which of two common alleles is likely to be the major one is not important since we are mostly concerned with estimating the frequencies at rare snps. further, for alleles with intermediate frequencies , the distinction between major and minor allele is less important. assign the other three non-major nucleotides m <dig>  m <dig>  and m <dig>  the likelihood introduced in equation  <dig> assumes a fixed major allele m and fixed minor allele m. therefore, to allow for uncertainty in the designation of the minor allele, the likelihood function can be modified as:   

further, assuming that any of the three possible minor alleles is equally likely, we obtain:   

where . since  can be very small with big data sets , it is useful to compute the likelihood in the log-scale. order the three conditional log-likelihoods as to , l, l), where l is the largest one. then,  

g-test using called genotypes for association mapping
in association studies, snps showing significant differences in allele frequency between cases and controls are said to be associated with the phenotype of interest. association mapping can be performed using data from next-generation sequencing studies. we first discuss approaches that require calling individual genotypes and then perform a test for association using the called genotypes. in this approach, a genotype is first called for each individual. the genotypes can be filtered or unfiltered. assuming independence across individuals and hwe, a  <dig> Ã—  <dig> contingency table can be built by counting the number of major and minor alleles in both the cases and controls. this leads to the well-known likelihood ratio test for independence, the g-test:   

where ok,h 
is the frequency observed in a cell, and ek,h 
is the frequency expected under the null hypothesis in which the allele frequency is the same between cases and controls. the well-known pearson's chi-square test is asymptotically equivalent to the g-test. if the table is generated from true genotypes, then the g-statistic asymptotically follows a chi-square distribution with  <dig> degree of freedom ). however, in our studies, we construct the g-statistic using "called" genotypes, thus hwe may not hold due to over- and under-calling of heterozygotes. furthermore, constructing the test statistic by counting "called" genotypes instead of "observed" genotypes likely introduces extra variability. therefore, the statistical theory may not be valid any more. note that when a genotype is not called for a certain individual, the data is considered missing and is not included in the  <dig> Ã—  <dig> table.

likelihood ratio test accounting for uncertainty in the observed genotypes for association mapping
instead of calling genotypes, the likelihood framework allows for uncertainty in the genotypes and tests at each site j whether the allele frequency is the same between cases and controls. formally, we compute the likelihood of the hypotheses ho 
: pj
, <dig> = pj
, <dig> and ha 
: pj
, <dig> â‰  pj
, <dig>  where pj
, <dig> and pj
, <dig> are the mafs in cases and controls, respectively.

assuming that minor  and major  alleles are known, the likelihood of the minor allele frequency can be computed as described in equation  <dig>  and the likelihood ratio test statistic is computed as:   

where  and  are the observed data for cases and controls, respectively, and  and  are the mles of the mafs in cases and controls, respectively.

if the minor allele is unknown, the likelihood under the null hypothesis is computed as in equation  <dig>  and the lrt statistic is modified as:   

where dj 
is the observed data for both cases and controls, and  is the allele frequency under the null hypothesis. other notations are the same as in equation  <dig> 

estimating maf in simulated data
we compare the estimates of allele frequency on simulated data using true genotypes , called genotypes without any filtering , called genotypes with filtering , and the maximum likelihood method . for rare snps, the minor allele type is often not apparent. when calling genotypes, the second most common nucleotide is assumed to be the minor allele. the ml method directly incorporates uncertainty in determining the minor allele and unless otherwise stated, results using the unknown minor allele method  are shown. note that the unknown minor allele ml method performs similarly to the known minor allele ml method but the former better for very rare snps .

we first evaluated how well the different approaches were able to estimate the maf in  <dig> individuals across a range of sequencing depths for  <dig>  snps with a true maf of 5%. figure  <dig> shows boxplots of the distributions of estimated mafs using the four different approaches. as expected, for higher coverage data, such as an individual depth of 12Ã—, all the methods perform as well as when the genotypes are known with certainty . however, when the depth decreases, the estimates of the maf obtained by first calling genotypes become biased. for example, the median maf estimated using the call f method is  <dig> % at 6Ã— coverage and is  <dig> % at 2Ã—. the reason for the upward bias is that it becomes harder to call heterozygotes since true heterozygotes often look like sequencing errors. therefore, more heterozygotes than minor homozygotes tend to have missing genotypes. however, the overall bias in maf estimates from called genotypes is not always in one direction . interestingly, the bias appears to be worse for the call f method than the call nf method. this pattern may seem counter-intuitive since filtering the genotype calls would seem to decrease the probability of calling a sequencing error a heterozygote. however, the call f method also results in a larger amount of missing data since many homozygotes for the major allele will not be called due to sequencing errors. thus, in this instance, calling genotypes without filtering seems to be the better strategy than filtering genotypes when trying to estimate the maf.

the results are dramatically different for the new ml method. this method provides unbiased estimates of the maf  across a range of depths. even at 2Ã—, the estimates show only a slightly larger variance than those based on the true genotypes.

we also compared the estimated mean squared error  of the different estimates of the maf across a range of sequencing depths . the ml method has a lower mse than the calling methods with  <dig> or  <dig> individuals. in particular, the mse computed based on the call f method is much higher than those from the other methods especially when the depth decreases. the mse of the estimates of the maf based on the true genotypes reflects the lower limit of the mse and is not constant across depths due to sampling variance and a finite sample size. using  <dig> individuals, the mse approaches  <dig>  with increasing depth and when using a sample size of  <dig> individuals, it approaches  <dig>  with increasing depth.

overall, the new ml method out-performs genotype calling methods.

estimating a distribution of mafs from simulated data
we next examine how the different estimation approaches performed in estimating the proportion of snps at different frequencies in the population . here we simulated  <dig>  snps where the distribution of the true mafs followed the standard stationary distribution for an effective population size of  <dig>  . note that in practice, however, it is very difficult to distinguish a very rare snp from a sequencing error. therefore, for comparison purpose with real data, we discarded snps with estimated maf less than 2%. figure  <dig> shows the proportion of snps falling into each different frequency bin after excluding those snps with estimated maf<2%.

as expected, with a high depth of coverage, such as 10Ã— per individual, all methods provide estimated maf distributions that are similar to the expected distribution based on the true genotypes . with a shallower depth of coverage, such as less than 4Ã— per individual, the distributions of mafs obtained by genotype calling methods significantly depart from the expected maf distribution based on true genotypes . in particular, these methods over-estimate the proportion of low-frequency snps. for example, the expected proportion of snps in the second bin  is 18%. the corresponding proportion based on the call nf method at a depth of 4Ã— is 26%, which is  <dig> -fold higher than expected. the over-estimation of the proportion of low-frequency snps occurs due to confusion of sequencing errors with true heterozygotes, which results in overcalling heterozygous genotypes. the magnitude of this inflation differs across different filtering cutoffs, but a larger cutoff does not necessarily increase or decrease the inflation.

the picture is entirely different for the ml method. the estimated maf distribution obtained from the new ml method closely follows the true distribution even with shallow depths of coverage. here there is almost no excess of low-frequency snps. at a depth of 4Ã—, the proportion of snps in the second bin of the histogram is  <dig> %, which is very close to the expected proportion . thus, more reliable estimates of the frequency spectrum can be made from low-coverage data by using our likelihood approach than by using the genotype calling approaches.

association mapping in simulated data
we compare the performance of methods that treat inferred genotypes as true genotypes in tests of association  to our likelihood ratio test  that accounts for uncertainty in the genotypes. we examine the distribution of the test-statistic under the null hypothesis of no allele frequency difference between cases and controls. we also compare the power of the different approaches.

with reasonably large sample sizes, standard asymptotic theory suggests that under the null hypothesis both the g-statistic and lrt statistic follow a chi-square distribution with one degree of freedom ). therefore, we have compared the null distribution of the g-statistic computed based on calling methods as well as the lrt statistic to the Ï‡ <dig> distribution using qq-plots . we simulated  <dig>  snps across a variety of sequencing depths in  <dig> cases and controls where the maf used to simulate genotypes was 5% in both cases and controls. the distribution of the g-statistic computed using the true genotypes shows a very good correspondence with a Ï‡ <dig> distribution. however, the distribution of the g-statistic computed based on the called genotypes substantially departs from a Ï‡ <dig> distribution. calling genotypes and then treating those genotypes as being accurate produces a vast excess of false-positive signals if the p-values are computed using a Ï‡ <dig> distribution. for example, at a depth of 2Ã—, 11% of the snps had a p-value less than 5%, compared to the expected 5%. the effect is caused by an increase in the variance, due to overcalling homozygotes as heterozygotes, in the allelic test used here for detecting association. genotypic tests such as armitage trend test, which are robust to deviations from hardy-weinberg equilibrium, do not show a similar increase in the false positive rate . consistent with this observation, filtering the called genotypes results in a decrease in the fraction of significant tests when using the g-test, although filtering does not completely solve the problem. on the other hand, the lrt statistic shows only a very slight departure from a Ï‡ <dig> distribution for either 2Ã— or 5Ã— depths of coverage.

we also generated receiver operating characteristic  curves for each of the different association tests. these curves show the power of the test at different false-positive rates. since the distributions of some of the test statistics do not follow the Ï‡ <dig> distribution under the null hypothesis, to make a fair comparison, we obtained the critical value for each false positive rate based on the empirical null distribution. the power is computed as the fraction of simulated disease loci that have a statistic exceeding the critical value. overall, we find that the lrt
performs better than the g-test based on either genotype calling method . for example, at a 5% false positive rate and with a sequencing depth of 5Ã—, the power to detect a disease locus with a maf of 1% and a relative risk  of  <dig> is 51% with the lrt, but power drops to 33% using the calling method without filtering and to 34% using the calling method with filtering. in particular, at low depth, the g-test applied to called genotypes with filtering performs very poorly . if we compare the power of the lrt to the armitage trend test using called genotypes, we find that the lrt also has higher power than the armitage trend test . this suggests that if one wishes to use called genotypes, filtering them based on call confidence can result in a loss of power.

application to real data
we analyzed  <dig> exomes from controls for a disease association study that have been sequenced using illumina technology at a per-individual depth of 8Ã—  <cit> . we used the genotype likelihoods generated by the "soapsnp" program  <cit>  for our inference. for more details, see methods.

first, we explored the accuracy of the estimates of the maf from next-generation sequencing data for  <dig> snps by comparing them to the estimated mafs from sequenom genotype data. both the estimates using the ml method and the genotype calling method without filtering are highly correlated with the estimates made from the sequenom genotype data . however, estimates based on genotype calling with filtering show poor correspondence to the frequencies estimated from the sequenom genotype data, especially when sequencing depth is low. interestingly, there is one snp where the estimated maf from the resequencing data is very different from the estimate obtained from the sequenom genotype data, even though the sequencing depth is very high . specifically, the estimated maf from the sequenom genotype data is  <dig> %, but is  <dig> % when estimated using the ml approach. individual examination shows that in many individuals, the highly supported genotype based on the sequencing data differs from the sequenom genotypes. given that this snp is covered by many reads in these individuals and that the observed read bases have high quality scores , it is likely that the difference is due to sequenom genotyping errors. note that there are a couple of snps in which the estimated mafs from the genotype calling approach without filtering seem to better correspond to the mafs estimated from the sequenom genotyping than the estimates from the ml approach do. for example, at one snp the estimated maf is  <dig> % from the sequenom genotype data,  <dig> % from the genotype calling method without filtering, and  <dig> % from the ml method. however, individual inspection reveals there are a few individuals for which the called genotype from the sequencing data differs from the sequenom genotype. in these cases, the errors in the called genotypes canceled, giving the appearance of better correspondence with the sequenom genotype data. therefore, for these snps, it is hard to tell which method performs best.

we next examined the distribution of mafs computed using several approaches across a range of sequencing depths from our next-generation exome sequencing data . we discarded snps with estimated maf <2% since it is difficult to distinguish these very low-frequency snps from sequencing errors in this dataset. we further removed sites in which there was a significant difference  in the quality score of read bases between the minor and major alleles. these sites are likely to be artificial snps that may occur due to incorrect mapping or unknown biases introduced during the experimental procedure. then we classified each site into bins based on the depth of coverage. the number of snps in each bin is shown in table  <dig>  when the average depth is less than 9Ã—, the distributions of estimated mafs based on genotype calling methods are very different from the one based on the ml method. specifically, the genotype calling approaches give rise to a large excess of low-frequency snps . this pattern mirrors what was seen in our simulation studies . also, for the genotype calling methods, the allele frequency distribution changes dramatically as sequencing depth changes. therefore, as discussed previously, when depth is not very high, the genotyping calling methods are likely to include a lot of false snps that are sequencing errors. these errors appear as an excess of low-frequency snps in the frequency distribution. the distribution based on the ml method is more stable across depths, but there still is an excess of snps with low allele frequency with depth less than 9Ã—as compared to the proportion of low-frequency snps at greater depths.

finally, we used this exome-resequencing data to simulate a case-control association study. to examine the distribution of the association test statistics under the null hypothesis, we randomly assigned  <dig> individuals to a case group and the other  <dig> to the control group. for all snps on chromosome  <dig> with maf estimates > 2% , we tested for allele frequency differences between cases and controls by computing the g-statistic using called genotypes both with and without filtering as well as the lrt statistic. figure  <dig> shows the qq plots comparing the distributions of the test statistics to the standard Ï‡ <dig> distribution. as seen in simulation studies, the null distribution of the g-statistic calculated when calling genotypes without filtering substantially departs from the Ï‡ <dig> distribution. however, the null distribution of the lrt statistic closely follows the Ï‡ <dig> distribution. the inflation factor  <cit>  is  <dig> , implying that lrt statistic performs well when applied to real data.

discussion
the likelihood method discussed here is an extension of our previous approach  <cit>  which was similar to that of lynch  <cit> . we have improved this approach by allowing for uncertainty in determining which allele is the minor allele. additionally, the present formulation includes base-specific error rates . these additions may have a practical benefit particularly when estimating the frequencies of rarer alleles, where it may not be obvious which allele is the minor allele and where sequencing errors may have the greatest effect on frequency estimation.

though not surprising, it is important to note that with higher sequencing coverage, the particular approach used to estimate allele frequencies does not matter as much. for depths of coverage > 10Ã—, the genotype calling methods both with and without filtering behave appropriately and similarly to the ml approach. thus, with high depths of coverage, the traditional and simple method of calling genotypes and then treating those genotypes as being known with certainty is still effective. the reason for this is that with such high depth, the called genotypes are likely to be accurate. with lower depths of coverage, however, there is considerable uncertainty regarding the true genotype. often the most-likely genotype will not be the true genotype, leading to biases in estimates of allele frequency and spurious signals of association in case-control studies. in this situation, the ml method is a superior approach.

in our simulations, we compared the performance of our ml approach to a relatively simple genotype calling approach . it is possible that more sophisticated genotype calling approaches such as soapsnp  <cit> , maq  <cit> , and gatk  <cit>  may show improved performance relative to the simple genotype calling approach used here. however, many of the same trends found in our simulations, where the simple genotype calling approach was used, were also seen in the exome sequencing data where genotypes were called using soapsnp. for example in both the simulations and the exome data, genotype calling approaches result in the appearance of an excess of low-frequency snps  especially when sequencing depth is less than 8Ã—per individual.

we have explored whether it is better to call genotypes with filtering or without filtering when analyzing low-coverage data. intuitively, one would expect that if there was uncertainty in the genotypes, it would be better to call genotypes only if one was very confident in that genotype and treat the other less confident genotypes as missing data. however, as discussed by johnson et al.  <cit> , calling genotypes only when one is very confident can adversely affect downstream analyses that use the inferred genotypes. our simulations and analyses of real data show that for estimating allele frequencies, genotype calling methods perform better without any filtering because filtering creates a strong upward bias in the frequency estimates. for association studies, it is not always clear whether it is better to filter the genotypes. not filtering can result in an excess of false-positive results for allelic-based tests, but filtering can result in a decrease in power.

studies have suggested that genotype calling approaches that use ld information to call genotypes  <cit>  may result in more accurate inferences from low-coverage data. however, it is unclear whether using population genetic characteristics of the data, like ld patterns, to call genotypes biases downstream population genetic and evolutionary analyses. such an evaluation is beyond the scope of the present work. however, this is not a concern for our method to estimate allele frequencies because our approach does not use any ld information.

as currently implemented our method does not tackle the problem of snp calling itself. in principle, our approach could be extended to use a lrt to call snps. specifically, the test could compare the probability of the data under the hypothesis that there is no snp at a given site  and under the hypothesis that there is a snp at a given site . such an approach is the subject of ongoing research. a feature of our approach to estimating frequencies is that it is not necessary to first call snps prior to estimating frequencies, which may be beneficial in certain circumstances.

finally, our likelihood method has some limitations. it cannot estimate the frequencies of very rare alleles from low-coverage data. this is not so much a deficiency with the likelihood approach, but instead, speaks to the difficulty in detecting very rare variants using little data in a background of sequencing errors. to reliably detect and estimate the frequencies of rare variants with < 1% frequency, higher-coverage sequencing will be required. however, approaches that take genotype uncertainty into account may still be important. as shown by garner  <cit> , not taking uncertainty into account can lead to an increased false positive rate in association studies when sequencing depth varies between cases and controls. our approach also assumes that the loci analyzed are in hardy-weinberg equilibrium. while this is a reasonable assumption for most human populations, our approach can be extended to consider more complex relationships between allele frequencies and genotype frequencies. we want to emphasize that all methods to estimate allele frequency from next-generation sequencing data require that accurate genotype likelihoods can be calculated. if these calculations are not accurate, even after recalibration, none of the methods are likely to perform well.

CONCLUSIONS
we have evaluated the performance of a likelihood method and genotype calling methods to estimate the minor allele frequency from next-generation sequencing data. the likelihood method accurately estimates allele frequencies even when applied to low-coverage data  since it models the uncertainty in assigning individual genotypes. however, genotype calling approaches can lead to biased inferences when applied to low-coverage data. we have also extended the likelihood approach to test for differences in estimated minor allele frequency between cases and controls. through simulations and the analysis of exomes from  <dig> individuals, our lrt has appropriate false-positive rates and higher power than genotype calling approaches when analyzing low-coverage data. finally, we have shown that under certain circumstances, if one uses genotype calling approaches, it is better to not filter genotypes based on the call confidence score.

