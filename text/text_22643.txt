BACKGROUND
nonparametric tests in multiple hypothesis testing scenarios
large-scale statistical analyses have become hallmarks of post-genomic era biological research due to advances in high-throughput assays and the integration of large biological databases. as the analysis becomes larger and more complex, various kinds of computational issues arise. the context of our investigation is multiple testing, the simultaneous estimation of p-values for a large number of hypothesis tests. for example, in a typical control-treatment microarray experiment, the goal of the analysis may be to identify target genes by applying the same testing procedure on each of the genes and selecting those that show the most extreme differential expression.

most multiple testing scenarios involve the assumption of a parametric null distribution  for each observed test statistic. however, in many applications, this parametric assumption may be unreasonable, resampling-based p-values are the preferred procedure for establishing statistical significance. for example, in the usual permutation test framework, resamples are generated by randomly permuting the treatment and control labels among the available data samples. we then calculate the test statistic for each of these resamples and calculate the p-value for each gene as the fraction of the resamples that have more extreme test statistics than the observed test statistic for that gene. ideally, we would be able to evaluate the test statistic for every possible resample, and thus calculate the resample-based p-value exactly. however, this is usually not feasible for datasets involving many replicates, so the usual procedure is to use monte carlo simulation to estimate each p-value based on a large set of resamples. as an example, an option in the popular sam microarray analysis software  <cit>  allows the user to use permutation tests to assess the p-value without the normality assumption. a similar re-sampling scheme for estimating p-values can be based on the bootstrap. in  <cit> , a nonparametric test procedure is applied to every gene to examine how well the expression profile of a gene  fits some preset order-restrictions; the p-value of the test is obtained using  <dig> bootstrap resamples per gene. we refer the reader to  <cit>  for the rationale and more details on bootstrapping, permutation tests, and other nonparametric tests. in this paper we collectively refer these methods as resampling procedures and a randomly generated sample  is called a resample.

this paper focusses on the following setting: we have n units , and we want to conduct a hypothesis test for each gene i based on observed test statistic ti. we do not want to make any parametric assumptions about this test statistic, so the p-value pi for each test needs to be estimated by a resampling procedure. the additional element that is implicit in our framework is that the number of tests n is large , so we need to control for the large number of tests being performed. many multiple testing procedures focussed on control of the family-wise error rate , with a popular choice being the bonferroni correction  <cit> . more recently, the focus in multiple testing procedures has shifted to control of the false discovery rate   <cit> , which is much less conservative than fwer-control procedures. since this current work was motivated by biological applications, we will use the terms gene and unit interchangebly, with the understanding that our methods are applicable to any multiple testing situation.

typical uniform resampling strategies
typical resampling procedures for p-value estimation use an equal number of resamples, say b, assigned to each of n genes, for a total of n × b resamples. even in the simple framework where each gene will be assigned the same number of resamples, there are several alternative strategies for resampling-based inference. the first issue is whether each resample should be performed by randomly permuting the treatment and control labels of an entire column  of data values, with the alternative being that treatment and control labels are permuted within each gene independently. we refer to the first strategy as a column-wise procedure and the second strategy as an gene-independent procedure. many recent investigations  argue for column-wise resampling procedures in order to retain potential dependencies between genes. other recent microarray investigations  have employed gene-independent resampling procedures. clearly, a column-wise resampling procedure allocates resamples to all genes simultaneously, which implies a uniform allocation of resamples across genes. although this column-wise strategy is preferred in certain situations, it suffers from the same inefficiencies as any uniform allocation procedure: genes that are clearly distant from the decision threshold will receive the same number of resamples as genes that are quite near the threshold. we focus on a gene-independent resampling procedure since it provides a more flexible framework for differential allocation of resamples among genes, which is the primary motivation for our current work.

another issue is whether or not to combine resampled test statistics across genes when estimating the p-value for each gene. many researchers  prefer a concatenation procedure that uses all available resampled test statistics  to achieve a higher resolution on the resampling-based null distribution when estimating each p-value. since all resampled test statistics  are used for each p-value calculation, there is little distinction between resampling strategies based on uniform allocation of resamples across genes versus differential allocation of resamples across genes. however, a concatenation procedure is only reasonable when the resampling-based null distribution is similar across genes, which is an uncomfortable assumption in many applications, such as genome-wide association studies when the allelic frequencies vary across loci. in these applications, a non-concatenation or gene-separate procedure would be preferred. recent work  proposes concatenation of statistics across only subsets of genes to correct for the fact that the null distribution is likely to differ between significant and non-significant genes. in this paper, we will focus on situations where gene-separate  procedures are preferred, which is the area where differential allocation of resamples provides a substantial efficiency gain over a uniform allocation strategy.

in most multiple testing situations the vast majority of units are truly non-significant, which means that a uniform allocation strategy is devoting a large proportion of resamples to test statistics that are not even close to significant. for most estimated p-values that are quite large or extremely small , then we are reasonably confident about our decision based on those p-values without need for a high degree of p-value accuracy . instead, we should focus a larger number of resamples on the estimation of p-values that are near to our decision-making threshold. for example, one may limit the number of resamples for a gene when the number of resamples with test statistic exceeding the actual statistic is larger than p <dig> × b, since the p-value of this gene will definitely be higher than the threshold p <dig> when all b resamples are computed. the gene is clearly nonsignificant, so we can stop evaluating more resamples for this gene and save computational time. this simple heuristic, which we call the shortcut approach, has been discussed previously  <cit>  and implemented more recently in the popular software plink  <cit> .

in this paper, we develop a principled iterative procedure for allocating different numbers of resamples to each unit. the overall intuition behind our approach is similar to the shortcut method in that we want to preferentially allocate more resamples to genes which have "borderline" p-values, i.e., p-values near to our classification threshold. the main difference is how the resample allocations are determined: we use a bayesian-inspired approach that assigns resamples to each unit based on its individual "risk", the chance that the current p-value estimate leads to a misclassification of the unit. the goal is to lower the numbers of classification errors, since we are giving a higher resolution to the null distribution of genes that are more likely to be misclassified in a uniform allocation setting. this higher resolution comes at the sacrifice of resamples to non-borderline genes that should not need a very resolute null distribution for correct inference.

a detailed description of our differential allocation procedure is provided in the methods section. the results section includes an experimental comparison that demonstrates the gains of our procedure over uniform procedures using two publicly available datasets: one microarray dataset on breast cancer  <cit> , and one genome-wide association study  <cit>  where computational efficiency in p-value estimation is a necessary concern due to its size. our procedure maintains a low error-rate  while using substantially fewer resamples in total. we then provide an additional experimental comparison to demonstrate that our method outperforms the shortcut method.

methods
differential allocation of resamples using risks
we separate the description of our procedure into several subsections for clarity of presentation.

algorithm initialization
the input data for our algorithm is an n × j matrix of data values, where n is the number of genes and j is the number of observations per gene. our algorithm is initialized by a uniform allocation burn-in round, in which we assign b <dig> resamples to each gene, where b <dig> is a proportion of the b resamples that would be assigned to each gene by the typical uniform resampling procedure. each of these resamples gives us a test statistic under the resampling-based null distribution, which we can use to get an initial p-value estimate for each gene.

based on the given threshold p <dig> and our current estimated p-values , we have the current classification for each gene i: gene i is significant if  ≤ p <dig> or gene i is non-significant if  > p <dig>  in case when p <dig> is determined using other criterion such as fdr, we use these p-value estimates to calculate our decision threshold p <dig> using the original fdr-control procedure proposed by  <cit> .

differential allocation
our algorithm now proceeds sequentially through multiple rounds and in each round a total of k new resamples are assigned. we want to allocate new resamples differentially to each gene i with the goal of minimizing the expected number of mis-classified genes ie. either non-significant genes that are inferred to be significant  or significant genes that are inferred to be non-significant . our framework treats either type of error  as equally bad, though our approach could be easily generalized to differentially weight the two types of errors. our proposed strategy is to assign new resamples with probability proportional to the risk ri of each gene i: the current probability of that gene i being misclassified.

  

where pi represents the true p-value for gene i. only one of these two terms is non-zero, since any gene i can only be considered as either a false positive or false negative  based on its current estimated p-value . we estimate the probabilities p and p based on the posterior distribution of p-value pi for gene i. let ni be the number of resamples already performed on gene i and let ai be the number of resample test statistics that are more extreme than the observed test statistic for gene i. this pair of numbers  contains all the information we currently have for gene i. assuming a uniform prior on each pi, pi ~ beta, and with a binomial likelihood for our extreme resample counts ai ~ bin, then we have:

   

so each probability ri becomes

   

where b is the cdf of the beta distribution evaluated at x. b is often also referred to as the incomplete beta function. in figure  <dig>  we see the risk for different locations of the posterior distribution p. this illustration shows that the risk ri is the amount by which the posterior distribution  for gene i overlaps the significance threshold p <dig> 

after the end of each round, k new resamples have been assigned proportional to the risks given in , and for each affected gene, the new p-value pi and the risk ri must be calculated. the algorithm stops when the total number of resamples assigned reaches a preset cap btot. we should note that the above scheme considers the decision threshold p <dig> to be fixed and known, when it is actually itself an estimated quantity. a more general procedure that acknowledges the uncertainty in both the p-values pi and decision threshold p <dig> for fdr is the focus of continuing research. we provide a more detailed description of our proposed differential allocation algorithm below.

input: microarray measurements gi =  for each gene i,  <dig> ≤ i ≤ n.

output: set of significant genes as defined by threshold p <dig> 

parameters
 <dig>  b0: number of reseamples per gene for burn-in.

 <dig>  b: average number of resamples to allocate per gene, so that n × b is total number of resamples to be used.

 <dig>  k: number of resamples allocated in each round.

algorithm
 <dig>  for each gene i, compute observed test statistic fi = f .

 <dig>  burn-in allocation: ni ← b0

 <dig>  iterative allocation: repeat:

 for each gene i, calculate ai = number of ni resamples with test statistic ≥ fi and set  = ai/ni

 for each gene i, compute ri ← b. if pi ≥ p <dig> then set ri ←  <dig> - ri.

 for each gene i, compute wi = ri/Σiri.

 while j <k:

i. select a gene b from the set  with probability 

ii. assign a resample to selected gene b: nb ← nb + 1

iii. j ← j + 1

 <dig>  output the set of significant genes by applying threshold p <dig> on final set of {}.

the shortcut method
an alternative differential allocation idea that we call the the shortcut method is to stop allocating resamples to any genes which have already accumulated enough non-extreme test statistics to guarantee that the null hypothesis for those genes will not be rejected.  <cit>  discuss a sequential shortcut method for monte carlo estimation of p-values and more recently a shortcut method has been implemented in  <cit> . the popular software plink  <cit>  for genome-wide association studies allows for more sophisticated approaches, such as using a confidence interval of the estimated p-value of a unit to decide if more resamples are needed.

again let n be the number of genes and let b be the number of resamples that we would allocate to each gene in a uniform allocation scheme, so that we have a total of n × b resamples available to us. we again consider an iterative scheme where ni is the number of resamples already performed for gene i and let ai is the number of resample test statistics that are more extreme than the observed test statistic for gene i. if a particular gene i has accumulated enough non-extreme resample test statistics, i.e. if  > b·p <dig>  then the resampling-based p-value  is guaranteed to exceed the threshold p <dig> and so allocating any more resamples to gene i is pointless. all remaining  resamples that we would have devoted to gene i can now be allocated to other genes that still have a chance of rejecting the null hypothesis. this shortcut approach clearly differs from our proposed method in terms of how resamples are differentially allocated, but both should still be more efficient than a uniform allocation scheme. another major difference is that our differential allocation method will also assign fewer resamples to genes when the p-value is much lower than the cutoff, whereas the shortcut method always tends to allocate more resamples to genes with a lower p-value.

experimental comparision
application to a breast cancer microarray dataset
the hedenfalk et al. breast cancer dataset  <cit>  consisted of  <dig> sporadic cases,  <dig> cases with brca <dig> mutations, and  <dig> cases with brca <dig> mutations. following the guidelines in  <cit> , we only examine samples associated with either brca <dig> and brca <dig> mutations, which results in  <dig> samples for brca <dig> and  <dig> samples for brca <dig>  following the preprocessing procedure in  <cit> , we log2-transformed all measurements and removed outlier genes ; this left us with  <dig> genes for further analysis. the subjects were divided into two groups. for each gene, we tested whether the mean expression levels of the two groups are significantly different. we used the absolute value of the student's t-statistic and used permutation tests to compute the significance: the p-value of the gene is the fraction of random permutation resamples with larger statistic scores than the correct grouping of subjects. we varied the number of resamples per gene to see how the p-value estimation of our algorithm and the uniform allocation improved as the number of resamples increased. we assessed the accuracy by computing, as a reference, the exact p-values calculated by enumerating all  =  <dig> possible resamples for each gene. the error of any p-value estimation is the number of genes mislabeled as significant or nonsignificant when compared with the significance calls using these reference exact p-values and a significance threshold of  <dig> . all computations were done using the r statistical software  <cit> .

application to a parkinson disease genome-wide association study dataset
the parkinson's dataset  <cit>  consisted of the genotype information of  <dig>  snps on  <dig> cases and  <dig> controls. we randomly partitioned the dataset into  <dig> subsets of  <dig>  snps each on average, and applied our algorithm to each subset separately. for each snp, we used the chi-square statistic for the  <dig> ×  <dig> contingency table, and computed the exact chi-square test p-value with  <dig> degrees of freedom as the "reference" p-value. we also applied our differential allocation algorithm by setting b =  <dig>  b <dig> =  <dig>   <dig>   <dig>   <dig> , and p <dig> = 10- <dig> in the differential allocation algorithm. we then computed the accuracy and false discovery rate of the output from the four allocation algorithms using different p-value cutoffs; the "reference" set of significant snps were determined using the "reference" p-value using the same p-value cutoff.

simulation study to compare our algorithm and the shortcut method
we compared our method and the shortcut method using the following parameter settings:

 <dig>  we use n =  <dig>   <dig>  typical for genome-wide association studies. the actual p-values of all markers are generated as follows. first, for each marker we randomly sample an integer between  <dig> and n; the p-value of the marker is this number divided by n. thus each marker will have a p-value between 1/n and  <dig> at this moment. we then replace the p-values of five of the markers by 10- <dig> to represent real significant markers.

 <dig>  for both methods, we use the same p-value cutoff settings:

10- <dig>   <dig> × 10- <dig>   <dig> × 10- <dig>  10- <dig>   <dig> × 10- <dig>   <dig> × 10- <dig>  10- <dig> 

 <dig>  for the shortcut method, each iteration allocates b =  <dig> resamples. the algorithm stops when the average number of resamples per marker exceed  <dig> 

 <dig>  we use a simplified version of the adaptive permutation algorithm in plink, a program widely used in the analysis of genome-wide association studies  <cit> . at each iteration, the p-value estimate of marker i is  = /, where fi is the total resamples allocated to i so far, and di is the number of such resamples that yield higher statistics than the actual statistic . this  estimate is equivalent to the posterior mean when assuming a uniform prior distribution, and improves upon the poor performance  <cit>  of the usual estimate  = di/fi when di =  <dig>  if the actual p-value cutoff p <dig> is outside the c-level confidence interval for pi then marker i will not be included for resample allocation in the next round. the confidence interval is approximated by a normal distribution with mean  and standard deviation . we use c =  <dig> , <dig> ,  <dig> ,  <dig> ,  <dig>  in our simulation.

 <dig>  for our algorithm, b <dig> = k =  <dig>  b =  <dig> 

RESULTS
experimental validation
we applied our algorithm to two different datasets to check how efficient it is compared with the conventional uniformly-allocated re-sampling. the first dataset is a publicly available microarray dataset to detect genes differentially expressed across two conditions. the second, much larger dataset, is a publicly-available genome-wide assocation study on parkinson's disease  <cit> .

application to a breast cancer microarray dataset
our algorithm was first applied to the microarray dataset presented in  <cit> . the details of preprocessing and application of the algorithm to this data are presented in the methods section. the results are in figure  <dig>  we observe that in the microarray dataset, the differential allocation algorithm  outperforms the uniform allocation algorithm  substantially, though the gap becomes smaller when b increases. we also measured the areas under roc curve to eliminate the effect of selecting a particular threshold of significance, and observed the same trends . in both datasets, the choice of burn-in proportion b0/b for the differential allocation algorithm did not seem to affect the performance of the algorithm.

application to a parkinson disease genome-wide association study dataset
the results from the previous section suggest our algorithm has the best improvement over the uniform allocation when the number of possible resamples is relatively small. in this section, we test our algorithm on a publicly-available genome-wide association study where the number of possible resamples is relatively large. typical datasets in genome-wide association  studies may consist of several thousand case and control subjects each, using single nucleotide polymorphism  genotyping arrays that can genotype up to  <dig> snps. the most common goal of a genome-wide association study is to find snp that are highly correlated with the case/control status. one simple way to test the association is to run chi-square tests on the two-way  <dig> ×  <dig> contingency table between the genotype of each snp  and the case-control status  <cit> . existence of such snps suggests nearby genomic regions may carry significant genes, regulatory motifs, or other dna sequences that may affect the disease risk.

this setting is an important test of computationally efficient resampling-based procedures for several important reasons. first, the high number of snps being tested implies a very stringent p-value threshold if we take the issue of multiple testing into consideration: setting p-value cutoff at 10- <dig> or lower is typical, so any resampling-based p-value computation for each snp requires at least  <dig> resamples if uniform allocation is used. second, the high number of subjects means evaluating the test statistic for each resample is more costly. finally, although we focus on simple chi-square tests as a proof of concept for our procedure, even more complex and computational demanding tests that may involve interactions between multiple snps and pedigree information relating subjects are being actively developed and applied to improve the sensitivity of gwa studies. as a example, it is common to consider the maximum p-value between multiple tests, such as an allelic test and a genotypic test, in a gwa analysis. these tests may employ statistics that are computationally expensive, and p-values have to be evaluated using resampling if exact p-value formulas are not available.

as an illustration of our procedure in this difficult setting, we applied our algorithm to a public parkinson genome-wide association study dataset  <cit> . refer to the methods section on details of the dataset and the application of our algorithm. the results are summarized in figure  <dig>  notice that our procedure has excellent accuracy and false discovery rate: at most 20% except when the p-value cutoff is  <dig> × 10- <dig>  moreover, the proportion of "burn-in" permutation resamples has little effect on the accuracy of the differential allocation algorithm, probably because the enormous number of total snps implies there are always enough resamples from nonsignificant snps to be reallocated when needed. nonuniform allocation always outperforms uniform allocation by a substantial amount. since b =  <dig>  for p-value thresholds lower than 10- <dig> only snps with  <dig> as their estimated p-values can pass the threshold under the uniform allocation. the uniform allocation algorithm has much higher error and false discovery rate because p-value estimation for significant and borderline snps is less accurate, and the situation is not improved even when the p-value threshold increases to 10- <dig> 

simulation comparison to shortcut method
in addition to demonstrating increased efficiency over a uniform allocation scheme, we also evaluate our method against the shortcut method, which is also described in our methods section. we use n =  <dig>   <dig> markers, typical for genome-wide association studies. we generate the "actual"  <dig>  p-values following a uniform distribution since we know that the p-values of all  markers should be uniformly distributed in a well-designed genome-wide association study where no confounding factors such as population stratification exist. we evaluate our performance relative to the shortcut method using simulated p-values directly. see the methods section on details of the simulation.

please see figure  <dig> for the results of the simulation. as can be seen, our method consistently outperforms the shortcut method. note that as we increase the p-value cutoff, the fn rate increases and the fp rate decreases for the shortcut method. moreover, when the value of confidence level c increases in the shortcut method, the fp rate decreases but the fn rate varies in a more complex pattern affected by both the confidence level c and p <dig>  small values of c in the shortcut method has good fn rate in general  and an asymmetric approach such as our algorithm is preferred. another contributing factor is that our approach is more global in the sense of allocating resamples proportional to the risks across all markers as opposed to the shortcut approach that treats each marker independently of the progression of other markers.

running time
we explored the overhead associated with our differential allocation approach and found it to be negligible on a modern computer. we computed the running time of the shortcut method and our differential allocation algorithm for  <dig> repetitions of our simulation involving  <dig>  snps on a dual-quad-core xeon linux server using r . since the permutation tests in this simulation are generated by random p-values, the running time is almost entirely a function of the overhead of the allocation algorithms, not the individual statistical tests. the average running time of shortcut method in this situation was about  <dig>  minutes, and the average running time of our algorithm was  <dig> minute, suggesting that neither method has substantial overhead. certainly in a situation with more complex individual statistical tests, the running time of either approach will be dominated by the unavoidable calculations of each test statistic.

CONCLUSIONS
in this paper we presented a new approach to more efficiently assign resamples  within a nonparametric multiple testing framework. we formulated a bayesian-inspired approach to this problem, and devised an algorithm that adapts the assignment of resamples iteratively with negligible space and running time overhead. in two experimental studies, a breast cancer microarray dataset and a genome wide association study dataset for parkinson's disease, we demonstrated that our differential allocation procedure is substantially more accurate compared to the traditional uniform resample allocation. in a simulation study we showed our algorithm outperforms the simpler shortcut method under various settings. it is worth emphasizing that our methodology is not ideally suited for the accurate estimation of all p-values, especially p-values far from the significance threshold . rather, our methodology focusses on the accuracy of significance decisions by ensuring that p-values near the decision threshold are most accurately estimated.

the idea of using a non-uniform search among a large number of tests is quite common in other multiple testing situations. an example is efficient variable selection in regression models where the number of covariates is very large. similar applications can also be found elsewhere: in finance,  <cit>  used a stepwise regression procedure to predict bankruptcy, where significant predictors are added  sequentially using a procedure where there is differential allocation for the threshold of significance. techniques such as this are different from our situation since we are taking a non-parametric approach to a simpler testing situation, but we still share the similar idea that one can gain power by differentially allocating resources towards the tests that are most likely to be significant. when individual tests are simple to compute, e.g., fisher's exact test on small contingency tables when the p-value can be computed exactly, the gain by our algorithm or other differential allocation methods is limited. however, a differential allocation approach is much more important when more computationally intensive tests are used, such as in gene set enrichment analysis  <cit> , or family-based association tests in genome-wide association studies  <cit> .

authors' contributions
sj and lw designed this study and developed the new algorithm. lw coded the new algorithm and the shortcut method. ss and lw ran the experiments. all three authors wrote and approved the manuscript.

