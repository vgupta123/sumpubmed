BACKGROUND
in the past few years, the hereditary component of complex multifactorial diseases has started to be explored through the novel paradigm of genome-wide association studies . a gwas searches for patterns of genetic variation, in the form of single nucleotide polymorphisms , between a population of affected individuals  and a healthy population . the objective of a gwas is twofold: on the one hand, one searches for the set of snps that best explains the hereditary component of the disease ; on the other hand, one tries to learn a rule for classifying unknown subjects as cases or controls, given their genetic profile and possibly other environmental covariates  <cit> .

further applications of gwass include searching for the genetic predisposition to complex traits, such as height  <cit>  or body mass index  <cit> , or to the responsiveness to a treatment in a randomized trial  <cit> . the application of gwas is not limited to human: successful results have been obtained by applying the gwas framework to animal  <cit>  and plant research  <cit> .

the extremely large numbers involved in a gwas  have led the vast majority of studies to rely upon single, univariate snp association tests  <cit> . multifactorial diseases, however, have an heterogeneous nature, arising from complex patterns of interaction between a set of genetic traits and the environment: to fully capture the optimal set of genetic biomarkers, thus, all snps in a gwas should be analyzed simultaneously in a multivariate framework  <cit> .

in the literature, the few approaches to multivariate snp analysis on a genome-wide scale mainly rely on two methodological frameworks: penalized logistic regression  <cit>  and bayesian analysis  <cit> . in the first case, snps are modelled as discrete variables from the finite domain { <dig> ,2}  and a log-additive model of genetic effect on the disease is assumed. in the second case, snps are modelled as ternary categorical variables and no assumptions are usually made on pre-specified genetic models.

all methods for the simultaneous analysis of the whole snp set have to cope with genetic linkage, i.e. the non random association between portions of the genome close to each other, which acts as a confounding factor: in the proximity of a true causal genetic biomarker, several snps highly correlated with the biomarker but mildly associated to the disease are often observed  <cit> .

in this work, we present bag of naïve bayes , an algorithm for classification and genetic biomarker selection from the simultaneous analysis of genome-wide snp data. our algorithm is based on naïve bayes  classification  <cit> , thus it relies on contingency table analysis and it does not assume a pre-specified model of genetic effect.

three strategies are exploited in bonb to tailor the naïve bayes framework to genome-wide snp data analysis:  a bagging of naïve bayes classifiers, to improve the robustness of the predictions,  a novel strategy for ranking and selecting the attributes used by each bagged classifier, to enforce attribute independence, and  a permutation-based procedure for selecting significant biomarkers, based on their marginal utility in the classification process.

bonb is tested on the wtccc case-control study on type  <dig> diabetes  <cit> . in terms of classification performance, assessed through repeated random sub-sampling cross validation and measured with the matthews correlation coefficient  <cit> , bonb outperforms both a standard naïve bayes classifier, trained on the snps that reached genome-wide significance in a univariate test, and hyperlasso, a state-of-the-art penalized logistic regression technique specifically designed for the simultaneous analysis of genome-wide data  <cit> .

RESULTS
algorithm
given a dataset x, consisting of n observations  of p attributes , and a set y of class labels, one for each observation , a naïve bayes classifier  estimates from the dataset a classification rule in the form:

  pr=pr ∏ipr∑jpr ∏ipr, 

where y is the random variable representing the class label and x <dig> . . . xp are the random variables representing the p attributes.

the classification rule of equation  states that the probability of a subject being in class yk, given a combination of values for the attributes x <dig> . . . xp, is equal to the a priori probability of class yk, pr, times the probability of each attribute given class yk, pr: the implicit assumption below this classification rule is that attributes x <dig> . . . xp are all conditionally independent given y.

for categorical attributes, such as snps, probability distributions pr and pr are represented with conditional probability tables, which are estimated from the data by counting the occurrences of each combination of genotypes and class labels .

our algorithm, bag of naïve bayes , consists in an ensemble of naïve bayes classifiers, trained on gwas data with the procedure known as bootstrap aggregating or bagging  <cit> .

given a training dataset x, the bagging procedure starts by computing a set of b bootstrap replicates of x, i.e. a set {x . . . x} of datasets, each one obtained by sampling n observations with replacement from the training set x  <cit> . a naïve bayes classifier nbc is then trained on each bootstrap sample x. class probabilities of unseen subjects, drawn from an independent test set, are then obtained by averaging the output class probabilities computed by each nbc . such an approach is known to increase the robustness of the predictions  <cit> .

given the binary nature of the case/control classification problem and the frequent unbalance between the number of cases and controls in a gwas, we decided to rely on the matthews correlation coefficient  for assessing classification performance. the mcc is defined as:

  mcc=tp⋅tn-fp⋅fn⋅⋅⋅, 

where tp, tn, fp and fn stand for true positives, true negatives, false positives and false negatives, respectively.

the mcc is often preferred to standard classification accuracy, i.e. to the proportion of correctly classified examples, because it is not sensitive to class unbalance: the mcc, in fact, ranges from - <dig>  to  <dig>  and equals  <dig> in case of majority classification, i.e. when all labels are assigned to the most represented class.

the conditional independence assumption below the naïve bayes classification rule ) is unlikely to hold if all the snps of a gwas are exploited as attributes, because of genetic linkage. moreover, computing equation  for the whole snp set can be computationally cumbersome and can lead to numerical and overfitting problems.

we thus developed a procedure for selecting a good set of independent snps for each nbc: the procedure consists of a ranking step followed by an attribute selection step. in the ranking step, each snp is given a score, according to its ability in discriminating the subjects in the bootstrap sample x. the score is thus defined as the mcc of a naïve bayes classifier, trained and tested on the same set x, with the snp as a single attribute . snps are then ranked in decreasing order of score.

in the attribute selection step, snps are iteratively extracted from the top of the ranked list and added as attributes of nbc. each time a snp is included as an attribute, the procedure removes from the ranked list all the snps that are both close to the snp on the genome  and correlated with it . such an approach enforces attribute independence, thus coping with the problems arising from genetic linkage.

rather than including one snp at a time, uncorrelated snps are added in groups of exponentially increasing size, starting from one snp and doubling the size at each new addition. new snps are added as long as the generalization ability of nbc increases: to estimate the generalization ability, we test each nbc on the corresponding out-of-bag sample oob, consisting of all the observations left out from x when sampling x, and measure the mcc of the prediction. the exponential increase in the number of added attributes allows bonb to reach the adequate size for the attribute set of each nbc in a logarithmic number of steps.

the attribute selection procedure, iterated for the b bootstrap samples, results in an ensemble of b naïve bayes classifiers, each with a possibly different set of attributes. classification of unseen subjects, the first objective of gwass, is then obtained by averaging the output class probabilities across all nbcs. classification performance of the ensemble of nbcs can then be assessed on an independent gwas test set, by measuring the mcc of the predictions.

for the second objective of gwass, biomarker selection, we adapted for bonb a procedure originally designed for the random forests bagged classifier  <cit> : for each of the snps included as attributes by at least 5% of the nbcs, we randomly permute the genotype of the snp in the oob sets, test each nbc on its corresponding oob and record the relative decrease in mcc due to the permutation. such a measure, which we define marginal utility , can be used as an indicator of the importance of each selected attribute, given all other selected attributes.

for each snp, the permutation procedure returns a list of values of mu, one value for each nbc that included the snp: we test for mus significantly greater than zero with a one-tailed wilcoxon signed rank test, selecting as biomarkers the snps for which the p-value of the test is lower than  <dig> .

the following pseudocode summarizes the training phase and the biomarker selection phase of the bonb algorithm:

bonb

// training

 <dig> for b =  <dig> to b

 <dig>     = bootstrap replicate from x

 <dig>    for s =  <dig> to p

 <dig>       compute the contingency table for snp s from x

 <dig>       compute the naïve bayes attribute score of s

 <dig>    l = list of snps in decreasing order of score

 <dig>    initialize nbc as a naïve bayes classifier with no attributes

 <dig>    extract m =  <dig> new attributes for nbc from the top of l, excluding from future additions all snps at distance >  <dig> mb and with r <dig> <θ

 <dig>    while mcc of nbc, tested on oob with the new attributes, increases

 <dig>       add the new attributes to nbc

 <dig>       update m =  <dig> * m

 <dig>       extract m new attributes from the top of l, excluding each time from future additions all snps at distance >  <dig> mb and with r <dig> <θ

// biomarker selection

 <dig> for s in all snps selected by at least 5% of the nbcs

 <dig>    for b in all nbcs that selected s

 <dig>       permute the genotype of s in oob

 <dig>       record the marginal utility  of s

 <dig> select as biomarkers the snps with mu significantly larger than zero.

for analyzing the computational complexity of bonb, one can start by noting that, for each b in b, the attribute ranking step  has complexity o for computing the contingency tables and the scores  plus o for sorting the score list, thus has a total complexity of o.

the attribute selection step , executed for each b in b, has a computational complexity dominated by two operations: computation of the squared correlation coefficient r <dig> between snps and test of nbc on oob. if we define m¯ the average number of attributes included by each nbc  and p ¯1mbthe average number of snps in a  <dig> mb section of the dna , we can note that the first operation costs o for each snp pair and is executed m¯⋅p ¯1mb times, having thus a total computational complexity of o. the second operation, on the other hand, is executed log m ¯+ <dig> times, each time with a doubling number of features for nbc, and its computational complexity is thus expressed by the following summation:

 ∑i=0log m ¯+1n ¯oob⋅2i=n ¯oob≅o, 

where n ¯oob is the average number of subjects in an oob set, tending to  · n for large n  <cit> ; the total complexity of the second operation is thus o, asymptotically negligible with respect to the cost of computing the squared correlation coefficients. the total computational complexity of the training phase of the bonb algorithm is thus o.

for the complexity of the biomarker selection phase of bonb, we define p ¯5% the number of snps selected by at least 5% of nbcs  and note that the inner loop of lines 15- <dig> is executed at most o times; since the cost of the two operations in the loop is linear in n, the biomarker selection phase has a total computational complexity of o.

testing
bonb was tested on the wtccc case-control study on type  <dig> diabetes  <cit> : the study examined approximately  <dig> t1d cases and  <dig> healthy controls. each subject was genotyped on the affymetrix genechip 500k mapping array set.

we excluded a small number of subjects according to the sample exclusion lists provided by the wtccc. in addition, we excluded a snp if  it is on the snp exclusion list provided by the wtccc;  it has a poor cluster plot as defined by the wtccc. the resulting dataset consists of  <dig> snps, measured for  <dig> cases and  <dig> controls.

the number b of bootstrap replicates used by bonb was set to  <dig> and the threshold θ on r <dig> for uncorrelated snps was set to  <dig> . please see methods for an analysis of how performance is affected by variations of the parameters b and θ.

independent train-test set pairs for assessing the classification performance of bonb were obtained by repeatedly sub-sampling at random 90% of the dataset for training and 10% for testing. the procedure was iterated  <dig> times and classification performance was assessed with the mcc of the predictions on the test sets. the list of selected biomarkers, on the other hand, was computed on the whole dataset.

classification performance was compared with the ones obtained by a standard naïve bayes classifier, trained on all the snps that reached the significance threshold of  <dig> × 10- <dig>  in a single 2df χ <dig> test of association with a general genetic model, and by hyperlasso, a logistic regression method for the simultaneous analysis of all snps in a genome-wide association study  <cit> . the former algorithm was chosen to assess the improvement of bonb both in terms of biomarker selection, with respect to a standard univariate test, and in terms of classification performance, with respect to the algorithm on which bonb is based. the latter algorithm was chosen because of its best performance among classification and biomarker selection methods for genome-wide data, as reported in  <cit>  and  <cit> , and because of the complete availability of the source code .

on the experimental dataset, bonb reached an mcc of  <dig>  ±  <dig>  , significantly higher than the ones reached by both the standard naïve bayes classifier  and by hyperlasso . figure  <dig>  shows the boxplots of the mcc obtained by the three algorithms on the ten iterations of the sub-sampling procedure. for the sake of completeness, figure  <dig>  shows also the boxplots of classification accuracy. the dashed lines in the two plots represent the classification performance of a majority classifier.

to further analyze the behaviour of the three methods at different levels of the output function  we report in figure  <dig> the precision vs recall curve and the receiver operating characteristic, or true positive rate vs true negative rate curve, of the three algorithms on one of the ten random subsamplings . as it is clear from the figure, the performance of the standard naïve bayes classifier is completely dominated by the performance of both bonb and hyperlasso. concerning the two latter algorithms, one can observe that hyperlasso has a better performance at the two extremities of the curves, i.e. for subjects whose logistic regression value is closer to the maximum or the minimum; moving from the extremities to the middle scores, bonb outperforms hyperlasso, being indeed able to reach overall higher mcc and classification accuracy.

for biomarker selection, we run bonb on the whole dataset and compared its results with the biomarkers identified by hyperlasso and by the general 2df test. the average number of attributes included by bonb in each nbc was  <dig> ,  <dig> snps were included by at least one nbc and  <dig> snps by at least 5% of the nbcs . among the  <dig> snps, only  <dig> snps reached the significance level on the permutation test and were chosen as genetic biomarkers . all the  <dig> selected snps fall into regions of interest for type  <dig> diabetes according to the on-line database t1dbase  <cit>   and their association with the disease was confirmed in a larger meta-analysis, subsequent to the wtccc study  <cit> . the squared correlation coefficients between all pairs of selected snps are all lower than  <dig> , indicating low redundancy in the information coded by the set of  <dig> snps.

first column: dbsnp rs id. second column: snp chromosome. third and fourth column: annotated gene and relation with the snp. fifth column: percentage of naïve bayes classifiers that included the snp as attribute. sixth column: median of the marginal utility of the snp. snps selected as genetic biomarkers by the permutation procedure are marked in bold.

compared to the  <dig> snps that reached the significance level on the 2df general test, both the list of  <dig> snps used for classification and the list of  <dig> biomarkers selected by bonb are more compact, but this does not prevent bonb to reach significantly higher classification performance.

hyperlasso selected  <dig> snps, all in the mhc region of chromosome 6:  <dig> of the snps are among the biomarkers selected by bonb, thus suggesting a certain coherence between the two algorithms and providing further confidence on the identified biomarkers.

implementation
bonb is implemented in c++ and relies only on standard libraries, thus being fully portable across operating systems. on the wtccc case-control study on type  <dig> diabetes, bonb takes approximately  <dig> minutes for training  <dig> nbcs and selecting the biomarkers on a  <dig>  ghz intel xeon processor e <dig>  a careful allocation strategy makes bonb occupy around  <dig> mb of ram for the wtccc dataset, allowing it to be easily run on a desktop computer.

discussion
in this paper, we presented a novel algorithm for classification and biomarker selection from genome-wide snp data. the algorithm, bag of naïve bayes , is based on the naïve bayes classification framework, enriched by three main features: bootstrap aggregating of an ensemble of naïve bayes classifiers, a novel strategy for ranking and selecting the attributes used by each classifier and a permutation-based procedure for selecting significant biomarkers, based on their marginal utility in the classification process.

the effectiveness of bonb was demonstrated by applying it to the wtccc case-control study on type  <dig> diabetes: bonb indeed outperforms two algorithms from the state of the art, namely a naïve bayes classifier and hyperlasso, in terms of classification performance and all the genetic biomarkers identified by bonb are meaningful for type  <dig> diabetes.

learning an ensemble of classifiers from a bootstrap sample of the original dataset provides bonb with two main advantages: on the one hand, it guarantees a higher generalization ability by increasing the stability of the learning process  <cit> ; on the other hand, it allows to define a measure of the marginal utility of each snp, given all the other snps exploited for classification, and to select significant biomarkers among these snps in a sound and statistically principled way.

two features of the naïve bayes classifier, chosen as building block of the bonb algorithm, make it rather appealing for genome-wide data analysis: on the one hand, conditional probability table analysis does not assume a pre-specified model of genetic effect, on the other hand, missing values are seamlessly handled by both the learning and the classification procedure.

the idea of bagging naïve bayes classifiers has already been proposed in the random naïve bayes algorithm of prinzie and van der poel  <cit> . the authors suggest, as a means for enforcing independence between the attributes of each nbc, to sample the attributes at random from the whole attribute set. such an approach, however, is unfit to genome-wide data analysis: the number of informative attributes is largely lower than the total number of attributes and the probability of capturing them by random sampling is thus extremely low.

our approach to attribute selection, consisting in a univariate ranking step followed by a multivariate selection step, has the advantage of favouring informative attributes, but without the need of pre-selecting fixed sets of attributes or of defining cut-offs on the strength of the association with the disease: attributes, in fact, are added to the classifiers as long as their combined effect on the generalization ability increases.

to provide the reader with further insight on the naïve bayes attribute score, exploited in bonb for univariate attribute ranking, we studied it against the 2df χ <dig> statistic of association for all the snps in the wellcome trust case-control study on type  <dig> diabetes .

as it can be seen from the figure, the two measures are in a strong monotonic relation for the majority of snps; when used as ranking criteria, thus, they are deemed to return similar ranked lists.

major exceptions are the points plotted along the two axes of figure  <dig>  along the vertical axis lie the snps for which the χ <dig> test can not be run, because at least one of the entries in the snp contingency table has less than  <dig> elements. along the horizontal axis, on the other hand, lie the snps that, when used to train a naïve bayes classifier, lead to a majority classifier, i.e. a classifier that always returns the most frequent of the two classes as output: this happens when one of the classes is the most represented for all the three genotypes of a snp.

analyzing the extreme behaviours of the two scoring measures provides the key for understanding the main difference between them: while χ <dig> is designed to capture a difference in snp frequencies from the frequencies expected under no association between the snp and the disease, the naïve bayes attribute score is meant to select good predictors of the disease, under the naïve bayes classification model.

for this reason, the naïve bayes attribute score is not much sensitive to small variations of contingency table entries with few or zero elements and thus it does not require a minimum number of elements per entry. on the other hand, it does not reward snps with even large differences in frequencies from the case of no association, if one of the two classes is consistently over represented, since such snps can not be effective as univariate predictors in the dataset under analysis.

CONCLUSIONS
the analysis of genome-wide snp data for multifactorial diseases mainly suffers from two, intertwined problems: on the one hand, multifactorial diseases are caused by complex patterns of interaction between multiple genetic traits and the environment, on the other hand, genetic linkage confounds the search for genetic biomarkers, because of the non-random association between the true genetic causes and the snps in genomic regions close to them. the algorithm we proposed, bag of naïve bayes, proved effective in tackling both of these problems: the simultaneous analysis of all snps on a genome-wide scale can capture the sets of snps with the strongest joint effect on the disease; the novel procedure for attribute ranking and selection enforces attributes independence, thus discriminating causal snps from nearby weaker signals.

apart from genome-wide association studies, bonb can also be applied, with minor modifications, to the analysis of snp data from case/control exome sequencing experiments  <cit> : in this scenario, given the lower average distance between snps on the same genes and the consequently stronger effect of genetic linkage, the ability of bonb to enforce attribute independence would prove even more useful.

