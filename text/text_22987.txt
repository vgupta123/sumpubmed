BACKGROUND
one of the difficulties in medical and biological data analysis is the highly skewed class distribution of different sample types. this could happen when special cases or "positive" samples are of limited size, while control or "negative" samples are more abundant  <cit> . sometimes, disease samples are divided into subtypes, with some of which are common while others are very rare. samples from those rare subtypes are represented as minority classes which also cause the imbalance of the class distribution  <cit> . here the challenge is how to precisely and correctly classify the minority samples  because they often carry important biological implications but tend to be ignored by the classification model which is overwhelmed by the majority samples. in data mining community this problem is known as imbalanced data classification  <cit>  and recently received an increasing attention for its practical importance.

there are mainly two strategies in dealing with imbalanced data learning: via sampling and via cost-sensitive learning. although cost-sensitive learning does not modify the data distribution or introduce duplicated samples, it requires the right cost-metric to assign different penalties for misclassification of different sample types. however, the correct cost-metric is often unknown a priori for a given dataset, and an improper cost-metric can significantly degenerate the classification accuracy  <cit> . recently, much effort has been made for developing new sampling strategies  <cit> .

data sampling strategies can often be categorized into two groups: oversampling and undersampling. in oversampling, the samples in the minority class are increased to match the samples of the majority class, while in undersampling the samples in the majority class are decreased to match the samples of the minority class. the classical or "naive" method is to randomly select samples from minority class and use the selected samples to increase the size of the minority class for oversampling  or to randomly select samples from majority class and remove them so as to decrease the sample size for undersampling   <cit> . more advanced methods attempt to employ certain intelligent strategies such as clustering  <cit> , working on the decision boundary  <cit>  or synthesizing new examples based on the data characteristics  <cit> . there are also many distance-based methods which try to select the samples with the nearest distance or farthest distance between the majority class and the minority class  <cit> . however, currently there is no clear way to determine which rule should be followed, and simply applying random sampling often beats those "smart" methods  <cit> . the unsuccessful experiences imply that those methods are largely data-depended. therefore, designing more flexible and better generalized algorithms which are self-adaptable to different data patterns in imbalanced data sampling and accurate model construction is clearly a desirable goal. this is particularly true in medical data classification and diagnosis because a false positive prediction will cause unwarranted worries while a false negative prediction will increase the risk of missing medical attention.

in previous work, zhang and yang successfully applied a genetic ensemble hybrid system to the feature selection of high-dimensional data  <cit> . if we convert the question by treating samples as features and re-adopt such kind of feature selection methods to select a subset of samples in majority class for building a balanced classification model, will such formulation lead to a better balanced classification result? this study is set out to investigate this quest. here we formulate the problem as an optimization process and employ the particle swarm optimization  algorithm as the sample selection strategy  <cit> . multiple classification algorithms with several most indicative metrics for imbalance classification measurement are used as multiple objectives to guide the sample selection process. although there are continuing debates on which technique is better  <cit> , undersampling is often preferred because no duplicated samples are introduced  <cit> . therefore, our study will concentrate on selecting an optimal subset of majority samples and combine them with the minority samples for building a balanced classification model. nevertheless, the proposed algorithm can be easily applied to oversampling by changing the target as minority samples.

methods
system overview
the problem of using highly imbalanced dataset for pattern recognition is that the classification model built on the training data tends to be biased on preferring the majority class while ignoring the samples from the minority class. data sampling method tries to remedy the skewed class distribution by either increasing the sample size of minority class or decreasing the sample size of majority class. however, algorithms that modify the sample distribution with greedy measures can introduce undesired bias. in this study we re-apply the techniques in feature selection to data sampling using a pso based hybrid system. the schematic flow of sampling and evaluation processes in our hybrid system is illustrated in figure  <dig> 

as can be seen, the work flow can be divided into two steps, namely, sampling and evaluation. for a given dataset, an external 3-fold stratified cross validation is applied to partition the dataset into external training sets  and external test sets . then, the external training sets are further partitioned with an internal 3-fold stratified cross validation, which gives the internal training sets and internal test sets. the internal training sets are used for sampling, while the internal test sets are used for guiding the optimization process. the external test sets are reserved for evaluation of the balanced dataset and is excluded from the sampling procedure.

in the sampling procedure, the pso hybrid system is used to evaluate the merit of each sample from the majority class in compensating the class imbalance. this is accomplished by generating different sample subsets of majority class and combining them with samples from the minority class for classification model construction and then for internal test fold classification. those subsets that can create more accurate classification models are favored and optimized in each pso iteration. when the termination criterion is met, selected samples from the last iteration are ranked by their selection frequency. after the sample selection frequency list is obtained, a balanced dataset can be created by combining the highly ranked samples of majority class with samples of minority class. in the evaluation step, different classification models are created using the balanced dataset generated by pso hybrid system, and the external evaluation dataset is applied to evaluate the classification accuracy with different evaluation metrics. such a training and evaluation process keeps the evaluation dataset for independent validation, which provides an unbiased evaluation.

particle swarm based optimization
particle swarm optimization  is a new group of population-based algorithms which uses the idea of social communication and historical behaviors to adjust the optimization process  <cit> . it possesses the advantages such as high-performance and global optimization, which make it very popular in many biological related applications. specifically, lee combined pso with genetic algorithm  and support vector machine  for gene selection of microarray data  <cit> , xu et al. used pso to optimize the structure of recurrent neural network  in gene network modeling  <cit> , while rasmussen and krink applied pso for hidden markov model optimization in multiple sequence alignment  <cit> . in our system, a binary version of pso   <cit>  is employed for a new application, in which bpso is hybridized with multiple classifiers and metrics for data sample selection and ranking.

each sample of majority class in the training dataset is assigned an index in the particle space. the locus equals "1" if the sample is selected for building classification model or equals "0" if the sample is excluded from building the classification model. suppose we have a population of n particles, with i be the index of a particle in the swarm , j be the index of dimension in the particle , and t be the counter of iterations. the velocity of the ith particle vi, j and the position of this particle xi, j is updated by bpso with following equations:

where pbesti, j and gbesti, j are the previous best position and the best position found by informants, respectively. random() is the pseudo-random number generator that creates uniform distribution between  <cit> .

fitness and evaluation metrics
fitness function is the optimization guide of the bpso. it governs the update of pbesti, j and gbesti, j. it has been pointed out that in the imbalanced data evaluation a simple classification accuracy is not an indicative measure because the accuracy value is profoundly influenced by the large class  <cit> .

alternatively, metrics including area under the roc curve , f-measure , and geometric mean  are often chosen as more appropriate measures  <cit> . here we combine multiple evaluation metrics in bpso fitness function, which is defined as follows:

where l is the number of classifiers integrated in the hybrid system and fitnessi is formulated as follows:

where s is the sample subset to be evaluated. this fitness function is essentially a weighted combination of the above three evaluation metrics, auc is calculated using mann whitney statistic  <cit> , while fmeasure and gmean are calculated as follows:

where each component in fmeasure and gmean is further defined as follows:

precision:

sensitivity or recall:

specificity:

where ntp is the number of true positive, ntn is the number of true negative, nfn is the number of false negative, and nfp is the number of false positive.

classifiers
one limitation of previous efforts on imbalanced data analysis is that most studies only focused on decision tree as evaluation criterion  <cit> . instead of choosing certain type of classification algorithm for evaluation, multiple classifiers have been incorporated in our particle swarm based hybrid system. the reason of utilizing multiple classifiers is to balance multiple classification hypotheses so as to reveal true improvement of the sampling dataset.

specifically, the classification algorithms employed in the hybrid system composition includes decision tree , k-nearest neighbor , naive bayes , random forest  and logistic regression . j <dig> is a widely used decision tree classifier. it approximates discrete-valued functions and a group of favorite features selected by the algorithm are used as the test points at the tree nodes. each path of the node is then created for partitioning the value of the feature. knn classifier calculates the similarity, which is called distance, of a given instance with the others and assign the given instance into the majority class which the k most similar instances belong to. such similarity can be defined as euclidean distance, manhattan distance or pearson correlation. naive bayes classifier bases its learning strategy on probability theory. it tries to estimate the distribution of the data and classify a sample by assigning the sample into a class with the highest probability. random forest, as its name indicates, is a collection of decision trees  <cit> . instead of using a single tree to make the classification, random forest algorithm combines the decisions of several trees each trained on a feature subset of the original dataset. lastly, the logistic regression classifier uses a logistic function to compute the coefficients of input features with respect to the class label. it has been used extensively in modeling binomially distributed data.

main loop
putting above components together, the bpso based hybrid system can be summarized by pseudo-code in figure  <dig> 

experimental settings
datasets
four typical medical datasets are obtained from uci machine learning repository  <cit>  and a genome wide association study  dataset is obtained from the genotyping of single nucleotide polymorphisms  of age-related macular degeneration   <cit> .

for the medical data, the first dataset named "blood" was generated by blood transfusion service center in taiwan. it has  <dig> samples denoted as not donating blood and  <dig> as donating blood in march  <dig>  and the prevalence of the dataset is  <dig> %. the task is to classify these samples based on the information of blood donation frequency, recency etc. the second dataset, "survival", was generated from the survey conducted on the survival of patients who had undergone surgery for breast cancer. it contains  <dig> patients who survived  <dig> years or longer and  <dig> patients died within  <dig> year. the prevalence of this dataset is  <dig> %. the third dataset with the name of "diabetes" is obtained from the study of diabetes in pima indian population.  <dig> samples were identified as negative while the other  <dig> samples were identified as positive, which gives the prevalence of  <dig> %. the last dataset called "breast" was created for breast tumor diagnosis. within this dataset,  <dig> are benign samples and  <dig> are malignant samples, and the prevalence is  <dig> %.

the gwas dataset contains  <dig> samples with each sample been described by more than  <dig>  snps. within the  <dig> samples  <dig> are labeled as geographic atrophy either central or non-central to the macula ,  <dig> are labeled as uniocular choroidal neovascularization , and the rest  <dig> are the control samples. therefore, the task is divided to classify cga samples from the rest  and to classify neov samples from the rest . in snps selection, we applied the selection procedure utilized by chen et al.  <cit> , and obtained  <dig> snps from two linkage disequilibrium  blocks. they are rs <dig>  rs <dig>  rs <dig>  rs <dig>  rs <dig>  and rs <dig> from the first block, and rs <dig>  rs <dig>  rs <dig>  rs <dig>  rs <dig>  rs <dig>  rs <dig>  rs <dig>   <dig>  rs <dig>  and rs <dig> from the second block. based on previous investigation of amd  <cit> , we added another six snps to avoid analysis bias. they are rs <dig>  rs <dig>  rs <dig>  rs <dig>  rs <dig>  and rs <dig>  moreover, environment factors of smoking status and sex are also encoded into each dataset due to their high association to the amd development. together, we formed the two subtype datasets with each sample represented as  <dig> factors.

the summary of each dataset is given in table  <dig> 

implementation
we compare our particle swarm based sampling strategy with random undersampling, random oversampling, and clustering based sampling. random undersampling and random oversampling are implemented by decreasing samples of majority class or increasing samples of minority class to match the counterpart with a uniformed possibility, respectively. clustering based sampling is implemented as the base version of those described in  <cit> , that is, to cluster the data samples with k-mean algorithm and randomly select samples of majority class according to the majority/minority ratio of each cluster and the cluster size. we used the k size of  <dig> for k-mean clustering and the euclidean distance for similarity calculation.

as per the particle swarm based hybrid system, we code the particle space as an m dimension space with m equals to the size of the majority samples in the training set. different parameter settings of the particle swarm component are investigated empirically, and we fix the best combination  for evaluation and comparison. different classification algorithms are implemented by using apis of the weka machine learning suite  <cit>  through the main code.

RESULTS
tables  <dig>   <dig>   <dig>   <dig>   <dig>   <dig> provide the evaluation details of each sampling method on each dataset, respectively. all results are obtained by averaging three independent trials on each dataset. we named particle swarm based hybrid system as "pso", random undersampling as "ru", random oversampling as "ro", and clustering based sampling as "cluster" for convenience. for each sampling method, the evaluation results are presented with respect to  <dig> evaluation metrics and  <dig> different classification algorithms including decision tree , 3-nearest neighbor , naive bayes , random forest with  <dig> trees , logistic regression , 1-nearest neighbor , 7-nearest neighbor , sequential minimal optimization of support vector machine , random forest with  <dig> trees , and radial basis function network . with a careful observation it is clear that in most cases pso achieved better classification accuracy using all three evaluation metrics in comparison with the other three sampling methods. this can be further confirmed by averaging across different classification results with respect to each evaluation metric . also observed is that the improvement is essentially consistent across  <dig> different types of classifiers. this can be seen from the row "c. avg." of tables  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  it should be noted that only the first five classifiers are used in pso optimization and data sampling, while the last five classifiers are only used for evaluating the generation property of the hybrid system. also, the evaluation is done on the independent test set through external cross validation. therefore, it is safe to draw a conclusion that re-sampling dataset using pso can lead to a higher data sampling quality and better generalization property. for random undersampling and random oversampling we found that random undersampling is more effective, albeit in a few cases random oversampling appear to be quite competitive. as to clustering based sampling, it performs competitively to random under- and oversampling in "diabetes", "breast", "amd-cga", and "amd-neov" datasets but relatively poor on "blood" and "survival" datasets.

by plotting the evaluation results with respect to different evaluation metrics , we can see that the pso hybrid achieved the highest accuracy within all six datasets. however, it is also clear that each evaluation metric gives a different evaluation indication. that is, a sampling method "a" performing worse than another method "b" according to certain evaluation metric may be superior to the method "b" using a different evaluation metric. by plotting the evaluation results with respect to different classification algorithms , it is readily noticed that different classifiers also perform differently among these datasets. but within a given dataset, there seems to have certain data-classifier correlation regardless which type of the sampling method is used. interestingly, logistic classifier seems to be quite effective, while 1nn appears to be the most unsuccessful one.

with above observation, it is clear that the evaluation of different data sampling strategies is compounded by different classification algorithms and evaluation metrics. therefore, relying on a sole classifier or evaluation measure for imbalanced data sampling could potentially lead to the loss of generalization property. caution should be drawn when a claim is made on the basis of a single type of classifier or evaluation metric.

CONCLUSIONS
in this work, several popular sampling methods are investigated on imbalanced medical and biological data classification. a particle swarm based hybrid method is proposed to improve the overall classification accuracy. the experimental results on four medical datasets and a gwas dataset illustrated the effectiveness of the proposed method. this is quantified in our experiments by using three evaluation metrics across  <dig> different classification algorithms.

the study demonstrates that with a proper modification feature selection algorithms can be tailored for imbalanced data sampling. in addition to being self-adaptable to different datasets, the proposed hybrid system is quite flexible, allowing different classifiers and evaluation components to be easily integrated for any specific problem at hand. the imbalanced data sampling problem is ubiquitous in clinical and medical diagnoses as well as gene function predication and protein classification  <cit> . the proposed hybrid system can not only recover the power of classifiers on imbalance data classification but also indicate the relative importance of samples from majority class in contrast to samples from minority class. this information could be used for further biological and medical investigations which may result in the discovery of new conditions or disease subtypes. we anticipate that such a hybrid formulation can provide a new means for tackling imbalanced data problems introduced in these applications.

availability
the pso sampling system is implemented in java and is available from: http://www.cs.usyd.edu.au/~yangpy/software/sampling

competing interests
the authors declare that they have no competing interests.

authors' contributions
py conceived the study and drafted the manuscript. py and lx designed and implemented the algorithms, performed the experiments. bbz, zz, and ayz revised the manuscript critically and introduced the problem initially.

note
other papers from the meeting have been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2009: eighth international conference on bioinformatics : bioinformatics, available online at http://www.biomedcentral.com/1471-2105/10?issue=s <dig> 

