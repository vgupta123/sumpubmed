BACKGROUND
the rapid growth of the number of protein sequences has far outpaced the experimental determination of their structures, but knowledge of the three dimensional structure of a protein can help to determine its function. thus, computational methods are often used to predict the structures of sequences for which no experimental information is available. such approaches are based on the premise that all the information needed to determine the three dimensional structure is encoded in the amino acid sequence  <cit> . a critical first step is the accurate prediction of the protein secondary structure, the local, regular structure defined by hydrogen bonds. over the past  <dig> years, researchers have been predicting secondary structure with various approaches. notably, the predictive accuracy has improved substantially over the past  <dig> years through the use of evolutionary information and machine learning algorithms  <cit> . in  <dig>  qian and sejnowski pioneered the application of artificial neural networks  to predict secondary structure  <cit> . different ann architectures have been used to predict the secondary structure, such as feed-forward back-propagation ann  <cit> , bidirectional recurrent ann  <cit> , cascade-correlation ann  <cit>  and cascaded ann with linear discriminant analysis  <cit> . the most successful methods in the 1990s, such as phd  <cit>  and psipred  <cit> , used multi-layer feed-forward anns and achieved predictive accuracies of around 77%-78%. moreover, other approaches have been used over the past  <dig> years, such as analysis with hidden markov models  <cit> , multiple linear regression  <cit>  and, more recently, non-linear dynamic systems  <cit> . other predictors, such as jpred  <cit> , make consensus secondary structure predictions. since  <dig>  <cit> , the support vector machine method  has been applied to predict secondary structure  <cit> . pmsvm  <cit>  enhanced the prediction of the single svm scheme with a dual-layer svm approach. more recently, yasspp  <cit>  improved the svm-based predictions by combining position-specific and nonposition-specific information with better kernel functions. despite relatively accurate predictions, there is still an opportunity for additional information and novel methods to boost the predictions.

the backbone dihedral angles, ϕ and ψ, can provide important information about the three dimensional structure of the protein. they vary from -180° to +180°, but they cannot adopt all possible values, because of steric restrictions. the famous ramachandran plot  <cit>  illustrates the sterically allowed regions of the dihedral angles. the experimental determination of dihedral angles is usually time-consuming and expensive, but can be accelerated by algorithms that use sequence information and chemical shifts  <cit> . accurate prediction of dihedral angles can facilitate tertiary structure prediction. it has been suggested that if none of the dihedral angles of an eight-residue fragment differs from another eight-residue fragment by more than 120°, the rmsd between the two segments is less than  <dig> Å  <cit> . the rosetta server  <cit> , the most successful method for three dimensional structure prediction, uses predictions from hmmstr  <cit>  of the secondary structure and the dihedral angles, which are described with an alphabet of eleven states. apart from protein structure modelling, predicted dihedral angles have been used successfully to improve sequence alignment  <cit> , fold recognition  <cit>  and secondary structure prediction  <cit> . early studies used simple models to explore protein conformational space and facilitate 3d structure prediction  <cit> . over the past few years, several methods have been developed to predict dihedral angles based on different numbers of structural states. de brevern and colleagues  <cit>  used self-organising maps and hidden markov models to identify a structural alphabet of  <dig> "protein blocks". this alphabet was used in locustra  <cit>  and by dong and colleagues  <cit>  to predict structural states using svms and anns, respectively. kuang and colleagues  <cit>  developed an svm-based method that makes three-state and four-state predictions with an accuracy of  <dig> % and  <dig> %, respectively, based on the dihedral regions proposed by a previous study  <cit> . dhpred  <cit> , another svm-based method, achieved 80% three-state accuracy based the dihedral regions defined by lovell and colleagues  <cit> . the definition of the dihedral angle regions is important in this kind of approach. other methods predict the real value of the dihedral angles. anglor  <cit>  uses anns and svms to predict the backbone ϕ and ψ angle, respectively. furthermore, zhou and co-workers developed real-spine  <cit> , a method that predicts the real-valued dihedral angles, using consensus predictions from five anns. real-spine has achieved the best mean absolute error  <cit>  and correlation coefficient  <cit>  reported to date.

the backbone dihedral angles and the secondary structure elements are highly correlated and, therefore, can be used together to boost the predictions. destruct  <cit>  implemented this idea using an iterative set of cascade-correlation neural networks to predict independently both the real value ψ angle and the secondary structure and it used the results to enhance the predictions. the predictive secondary structure accuracy on a non-redundant set of  <dig> proteins  <cit>  is, until now, the highest reported score on that particular dataset. even though the dihedral prediction was limited, it provided additional information, which improved the secondary structure prediction significantly. furthermore, the inclusion of secondary structure prediction improved the ψ angle prediction.

here, we take the approach one step forward. using various definitions of dihedral states created by two unsupervised machine learning algorithms, our method improves the predictions of backbone dihedral angles and secondary structure. multi-state dihedral prediction offers some advantages over real-value prediction, such as easy sampling and detailed analysis of the dihedral space. moreover, clustering techniques, often called class discovery techniques, can provide important insight into specific regions of the dihedral space which cannot be easily addressed with real-value prediction. we use the svm method, which is superior in many practical applications, because it finds the optimal hyperplane to separate two classes. the results we present in this paper show that our method predicts the three-state secondary structure significantly more accurately than other contemporary methods, due to the dihedral information used. additionally, the multi-state predictive accuracy of dihedral clusters enhanced with predicted secondary structures is comparable to, and in some cases more accurate than, other methods.

methods
support vector machines
the svm  <cit>  is an algorithm for learning classification and regression rules from data. the svm method has become an area of intense research, because it performs well with real-world problems, it is simple to understand and implement and, most importantly, it finds the global solution, while other methods, like anns, have several local solutions  <cit> . the svm can find non-linear boundaries between two classes by using a kernel function, which maps the data from the input space into a richer feature space, where linear boundaries can be implemented. furthermore, the svm effectively handles large feature spaces, since it does not suffer from the "curse of dimensionality", and, therefore, avoids overfitting, a common drawback of supervised learning techniques.

since an svm is a binary classifier, it cannot be used to separate data with more than two classes. however, several techniques allow combinations of svm models to create a multi-class svm method. the most popular methods are called one-against-one and one-against-all. the former constructs  binary models for n classes and each one trains on data from two different classes. a voting scheme is applied at the end to decide the final prediction. the one-against-all method constructs n binary models for n classes and each one decides whether an instance belongs to a class or not. at the end, winner-takes-all decides the final prediction. in this work, we use the libsvm package  <cit> , which offers multi-class svm using the one-against-one approach.

the ultimate goal is to classify previously unseen examples correctly. therefore, it is not useful to achieve high training accuracy if the prediction is not accurate enough. in order to estimate the generalisation error, we use n-fold cross-validation. the training data are split into n subsets and, sequentially, n -  <dig> of them are used for training and the remaining one for testing. this approach is repeated n times, until all subsets are used once for testing. in our case, 10-fold cross-validation was used.

the selection of the kernel function, which maps the input data into a high-dimensional feature space, is one of the main challenges in svm methods. the radial basis function , shown in equation  <dig>  is the most suitable kernel function for complex problems. secondary structure prediction appears to be such a problem and rbf has been used by the majority of svm-based secondary structure prediction methods  <cit> . hence, we use the rbf kernel.   

where xi and xj are the input vectors for instances i and j, respectively. to optimise the learning process, one can adjust parameters c and γ. the regularisation parameter, c, controls the trade-off between training error and the margin that separates the two classes, while γ controls the width of the rbf kernel. the parameter optimisation was performed using a grid search approach, where pairs of  were tried on the training set and the one with the best cross-validated accuracy was selected. a practical method  <cit>  to identify good parameters is to try exponentially growing sequences of c and γ. we tried the following values: c = 2- <dig>  2- <dig>  ...,  <dig> and γ = 2- <dig>  2- <dig>  ...,  <dig>  after the best pair of values was found, a finer search on that specific region was conducted to identify the optimal values. here, the optimised parameters for cb <dig> dataset were found to be c =  <dig>  and γ =  <dig> . however, the predictive accuracy was similar for c and γ in the ranges  <cit>  and , respectively.

clustering of dihedral angles
there is no clear optimal way to separate the dihedral space into regions in order to provide structural information. other dihedral prediction methods  <cit>  have used various definitions of the dihedral angle regions, taken from previous studies  <cit> . here, we attempt to discover the best clusters using two unsupervised machine learning techniques, k-means and expectation maximisation , that group a given collection of patterns into clusters based on a similarity measure  <cit> . this approach is often called data clustering and has been successfully used in bioinformatics, especially to identify new classes for gene expression data  <cit> . both the clustering algorithms we use are partitional methods, which divide the data into k clusters without overlap and each cluster can be represented by a centroid.

k-means  <cit>  is one of the simplest and fastest clustering algorithms. the main goal is to divide a dataset into k clusters, where k must be defined a priori. it starts with an initial selection of k centroids, which is usually random, and keeps reassigning the data points into clusters based on the similarity between the data point and the cluster centres, until a convergence criterion is met. euclidean distance is used as a similarity measure in our method. the k-means algorithm is popular, because it is easy to implement, and its time complexity is o, where n is the number of instances in the dataset. a drawback is that the algorithm is sensitive to the selection of the initial partition and may converge to a local minimum  <cit> .

another way to tackle clustering problems is using gaussian mixture models, in which the underlying assumption is that the data points are drawn from one of k gaussian distributions with mean μi and standard deviation σi. the goal is to identify the parameters of each gaussian distribution. the most popular algorithm in this case is the expectation maximisation  algorithm  <cit> , whose steps are similar to those of the k-means algorithm. em starts with a random assignment of the k gaussian distribution parameters, μi and σi, and computes the cluster probability for each data point based on the probability density function. the probability parameters are re-estimated and the procedure is repeated until a termination criterion is met. em is useful when one wants to identify and separate several probability distributions in the data. on the other hand, like k-means, em can get stuck in local minima  <cit> .

we used the weka implementations  <cit>  of the above algorithms to cluster the dihedral space into regions. in order to study many different partitions, we used different numbers of clusters, from two to  <dig>  to prevent the algorithms from getting stuck in local minima, the clustering process was carried out several times with different initial partitions. because of periodicity, the dihedral angles +180° and -180° are identical. however, this cannot be captured by distance-based clustering algorithms, like k-means and em. in order to reduce the effect of the angle periodicity, we perform a single transformation of the ψ angle, originally proposed  <cit>  in real-spine  <dig> , by shifting the ψ angles between -180° and -100° by 360°. hence, the ψ angles were in the range -100° and +260°. there are few dihedral angles with values at either end of this range, which improves the clustering.

datasets and svm design
disspred was trained and tested on three different datasets. the first was cb <dig>  <cit> , a non-redundant non-homologous set of  <dig> protein sequences. cb <dig> was used to study the impact of various input coding schemes and to tune the kernel parameters. all  <dig> proteins have less than 25% sequence similarity to ensure that there is very little homology in the training set. since cb <dig> was used to train many secondary structure prediction methods, we can compare the cross-validated accuracy of our method directly with other methods. the second dataset was pdb-select <dig>   <cit> , a set of  <dig> chains from the pdb with less than 25% sequence similarity and x-ray resolution less than  <dig>  Å. after removing chains with regions of unknown structure, the final dataset contained  <dig> chains from  <dig> proteins with a total number of  <dig>  <dig> residues. in order to make the training process faster and validate the performance on an independent dataset, pdb-select <dig> was divided into two subsets, one of which was used for training and the other one for testing. the subsets have approximately the same composition of three-state secondary structure elements: 35% for helix, 23% for strand and 42% for coil. moreover, we ensured that both datasets have a similar distribution of small/large protein chains. thus, subset one contains  <dig>  <dig> residues from  <dig> chains, whereas subset two contains  <dig>  <dig> residues from  <dig> chains. finally, we also report disspred's predictive accuracy on four subsets of the dataset provided by the eva secondary structure prediction server  <cit> . the pdb codes and chain identifiers as well as the scop class  <cit>  of each chain in the above datasets are listed at disspred's website http://comp.chem.nottingham.ac.uk/disspred.

the secondary structure can be assigned using dssp  <cit> , stride  <cit>  or define  <cit> . here, we use dssp, the most established method, which assigns the secondary structure using eight states: h , g , i , e , b , t , s  and "_" . most of the existing methods predict the secondary structure using a three-state assignment. therefore, we reduce the above representation to three states, by assigning h, g and i to the helix state , e and b to the extended state  and the rest  to the coil state .

since their first use by psipred  <cit> , psi-blast  <cit>  position specific scoring matrices  are employed by the majority of secondary structure prediction methods. pssms are constructed using multiple sequence alignments and they provide crucial evolutionary information about the structure of the protein. pssms have n ×  <dig> elements, where the n rows correspond to the length of the amino acid sequence and the columns correspond to the  <dig> standard amino acids. pssms represent the log-likelihood of a particular residue substitution, usually based on a weighted average of blosum <dig>  <cit> , and are created using the psi-blast algorithm. we generated the pssms using the blosum <dig> substitution matrix with an e-value of  <dig>  and three iterations against the nr database, which was downloaded in february  <dig>  the data were filtered by pfilt  <cit>  to remove low complexity regions, transmembrane spans and coiled coil regions. the pssm values were linearly scaled simply by dividing them by ten. typically, pssm values are in the range  but some values outside this range may appear. linear scaling maintains the same distribution in the input data and helps avoid numerical difficulties during training.

we used different coding schemes for the secondary structure prediction and the dihedral angle prediction. after testing different local window sizes , we selected w =  <dig> for secondary structure prediction and w =  <dig> for dihedral prediction, which give the highest predictive accuracy for each case. hence, using the pssm values for each residue, the input vector has length  <dig> ×  <dig> for secondary structure prediction and  <dig> ×  <dig> for prediction of dihedral angles. a local window, rather than just the individual residue, allows the classifier to capture useful additional information  <cit> .

our method consists of two different models, m <dig> and m <dig>  that predict secondary structure and backbone dihedral angles, respectively . m <dig> uses a local window of  <dig> residues. the input vector of m <dig> contains  <dig> ×  <dig> scaled pssm values,  <dig> for each residue in the fragment, and the output is one of the three states of secondary structure: h, e or c. m <dig> uses a shorter window of  <dig> residues and the input vector consists of  <dig> ×  <dig> scaled pssm values. the output of the model is an integer in the range , where n is the number of clusters used to identify the dihedral angle regions. we systematically partitioned the dihedral space into different numbers of clusters, from two to  <dig>  after the first run of the models using only the pssm values, the input vector of m <dig> was augmented with n binary values, which were equal to unity if the residue was predicted to be in that particular cluster and zero otherwise. only one of the n values can be equal to unity, since the residue is predicted into a single cluster. similarly, the input vector of m <dig> was augmented with three binary values, one for each secondary structure. this second stage is iterated several times to improve the predictions further. in other words, the predicted secondary structures from model m <dig> and the predicted dihedral clusters from model m <dig> at step m are used to augment the input vector of models m <dig> and m <dig> respectively at step m +  <dig> 

prediction accuracy assessment
we used several measures to assess the performance of disspred, most of them defined in the eva server  <cit> . q <dig> is the three-state overall percentage of correctly predicted residues:   

where nres is the total number of residues and mij is the number of residues observed in state i and predicted in state j, with i and j ∈ {h, e, c} . in the case of dihedral prediction, i and j can be any number from  <dig> to nc -  <dig>  where nc is the number of clusters. moreover, we calculate the per-state accuracy, the percentage of correctly predicted residues in a particular state:   

where obsi is the number of residues observed in state i. additionally, the matthew's correlation coefficient  <cit> , ci, provides a measure for the performance at each state:   

finally, errsig is the significant deviation for three-state accuracy, a measure used to distinguish between two methods. it is defined as the standard deviation divided by the square root of the number of proteins .

we use two additional measures to assess the accuracy of dihedral prediction. firstly, the mean absolute error  is the average of the absolute distance between the predicted and the real  value, p and x, respectively. in order to take in account the periodicity of the dihedral angles, the mae is calculated by:   

the predicted value corresponds to the centre of the predicted cluster. finally, it is interesting to see the fraction of residues whose dihedral angles are predicted close to the real value. q <dig> score is the percentage of residues whose predicted value is within 30° of the real value.

RESULTS
in the additional file  <dig>  the cluster centroids and the standard deviation of each cluster are shown, while additional file  <dig> shows all the different partitions of the ϕ - ψ space as well as the distribution of secondary structure element in each cluster. the helical residues belong mainly to one compact, highly-populated cluster, while there are clusters that consist mostly of strand residues, the most difficult secondary structure element to predict. for the above reason, the predictive accuracy of both helical and extended residues is improved significantly after the predicted dihedral information is used. on the other hand, the coil residues are distributed in different clusters, which makes their prediction more difficult.

the accuracy from the initial pssm-only prediction is shown in the first row. in bold are the most accurate predictions based on q <dig>  nc = number of clusters used to predict dihedral angles, dhr = input vector augmented by predicted dihedral cluster,

the results for psipred, phd and destruct were obtained from reference  <cit> .

l: length of the secondary structure element

q>65: the percentage of elements that have more than 65% of their residues predicted correctly

short: q> <dig> of elements with length up to eight residues

med: q> <dig> of elements with length between nine and  <dig> residues

long: q> <dig> of elements with length more than  <dig> residues

n-term res: the percentage of elements whose first residue  is predicted correctly

c-term res: the percentage of elements whose last residue  is predicted correctly.

it is interesting to analyse how the predictive ability changes in every cluster when the predicted dihedral angles are used, shown in additional file  <dig>  unsurprisingly, the prediction accuracy improves the most in clusters that contain mainly helical residues. in particular, the clusters with centroids around , which mainly consist of residues in right-handed helices, and the clusters with centroids around , which mainly consist of residues in left-handed helices, show significant improvement. moreover, clusters that contain mainly strand residues are also predicted more accurately. on the other hand, clusters that contain mainly coil residues or mixed strand/coil or helix/coil residues do not show any significant improvement. in fact, in some cases the additional dihedral information can decrease the predictive accuracy. however, these clusters are not highly populated and, therefore, do not affect the overall accuracy significantly.

nc: the number of clusters,

pssm-only: input vector with only pssm values,

sse: input vector augmented with predicted secondary structure elements.

on the other hand, although the predictive accuracy is low for large number of clusters, the predictions can provide important information about the local structure. we explore this by calculating the mae and q <dig> score. figure  <dig> shows that the mae decreases and q <dig> increases as we increase the number of clusters after each iteration using em clustering. the best results are obtained after the second iteration, which is in agreement with the predictive accuracy shown in table  <dig>  additional file  <dig> shows the results for the mae and q <dig> score using all different numbers of clusters with em clustering after the second iteration. six and seven clusters give the lowest mae and the highest q <dig> score and are presented in table  <dig>  therefore, the structural information contained in a dihedral prediction does not necessarily depend on the predictive accuracy. in fact, the improvement of secondary structure prediction was higher when we used predicted dihedral data from six and seven clusters. notably, the mae of our method is comparable to the mae reported by real-spine  <dig>  and  <dig>   <cit> , even though we only predict dihedral states instead of real value dihedral angles. real-spine  <dig>   <cit>  has maes of 36° for the ψ angle  and 22° for the ϕ angle . moreover, locustra  <cit>  reports maes of  <dig> ° and  <dig> ° for ϕ and ψ, respectively, while anglor  <cit>  achieves maes of 28° and 46° for ϕ and ψ, respectively. since, the above methods are trained on different datasets, their maes should not be compared directly. we present them here just to give a rough comparison between the methods.

the mean absolute error  and the percentage of predicted dihedral angles within 30° of the real value  for both backbone dihedral angles ϕ and ψ after two iterations, using six and seven clusters with em clustering.

tables  <dig> and  <dig> show the mae for ϕ and ψ, respectively, for each amino acid. we use the number of clusters that gives the lowest overall mae , which are seven clusters for ϕ and six clusters for ψ. glycine has the largest error for both angles, because is the smallest and the most flexible amino acid and can take many different conformations without steric restrictions. on the other hand, proline has the smallest mae for ϕ, because its ring structure restricts the ϕ angle to around -60°. amino acids that have strong helical preferences  <cit> , such as alanine, methionine and glutamic acid, have lower maes than the others. on the other hand, amino acids with a high hydropathy index  <cit> , such as leucine, isoleucine and valine, also have low maes. these residues are usually densely packed in the hydrophobic protein core and, hence, they have limited flexibility compared to residues on the hydrophilic surface. finally, apart from glycine and proline, residues that have coil preferences, such as asparagine and serine, have the highest maes.

the mean absolute error  of each amino acid for ϕ angle after two iterations, using seven clusters with em clustering.

the mean absolute error  of each amino acid for ψ angle after two iterations, using six clusters with em clustering.

the per-residue predictive accuracy of both secondary structure and dihedral clusters based on the scop classification of the protein chains is analysed in table  <dig>  unsurprisingly, residues in all-α proteins are predicted particularly well, while the prediction of residues in all-β proteins is less accurate. however, the secondary structure prediction of all-β proteins is more accurate than the prediction of strand residues shown in table  <dig>  notably, the predictive accuracy of residues in mixed α - β proteins is similar to the overall predictive accuracy for secondary structure and dihedral angles, shown in table  <dig> and table  <dig>  respectively. residues in α/β proteins are predicted slightly more accurately than residues in α + β proteins.

the second column shows the secondary structure predictions while columns three and four show the dihedral prediction using three and seven clusters, respectively.

from table  <dig>  it is clear that the secondary structure prediction improves significantly after the first iteration when the predicted dihedral angles from the initial run  are used. the subsequent iterations have no impact on the prediction results. similarly, there is significant improvement in the dihedral prediction after the second iteration when we use the predicted secondary structures from first iteration. therefore, we use only the iterations that improve the predictions significantly to train disspred using pdb-select <dig> dataset, i.e. the first iteration for dihedral prediction and the third iteration for both secondary structure and dihedral prediction are omitted, because their results do not improve the predictions of the subsequent iterations. the new design makes the training process faster and, most importantly, it saves time predicting new structures. table  <dig> shows the results for secondary structure prediction using pdb-select <dig> dataset. the models are trained on one subset and tested using the other. since no chain in pdb-select <dig> has a sequence similarity over 25% with another chain in the dataset, the predictions are independent. the overall accuracy is identical for both subsets. models trained on subset one predict helical and coil residues slightly better the models trained on subset two, whereas they predict the strand residues slightly worse. finally, table  <dig> shows the results for dihedral predictions on pdb-select <dig> dataset. the predictive accuracy for small number of clusters is similar to the achieved accuracy using cross validation , whereas when the number of clusters increases, the accuracy decreases significantly. this suggests that the partition may depend strongly on the dataset used to create the dihedral clusters. nevertheless, despite limited accuracy, the dihedral prediction can be used to enhance secondary structure prediction .

the second column shows the predictions for subset <dig> when svms are trained using subset <dig> and the converse is shown at the third column. info is a measure of the per-residue information content  <cit> .

columns two and three show the predictions for subset <dig> when svms are trained using subset <dig> and the converse is shown in columns four and five.

disspred server
our method is available online at http://comp.chem.nottingham.ac.uk/disspred/. disspred is written in perl using a cgi interface. only fasta files are accepted as input or compressed archives, containing fasta files. the user can choose the preferred clustering algorithm and the number of clusters. for each input file, one output file is created that contains the amino acid type, the amino acid number in the sequence, the predicted secondary structure, the predicted dihedral cluster and the ϕ and ψ values of the predicted cluster centre. the output files, together with the log files, are sent to the user by e-mail after the calculations are completed. table  <dig> shows disspred's prediction accuracy, for different subsets of the dataset provided by the eva secondary structure prediction server  <cit> , compared with other secondary structure prediction servers: psipred  <cit> , phdpsi  <cit> , profsec  <cit> , sam-t <dig> sec  <cit> , profking  <cit>  and prospect  <cit> . note that the results are not independent predictions, since some of the sequences in eva dataset are homologous with some sequences in pdb-select <dig> dataset, which was used to train disspred.

the results for the other methods were obtained from eva secondary structure prediction server.

CONCLUSIONS
using predicted secondary structure and dihedral angles, our method improves the predictive accuracy of both secondary structure and dihedral angle prediction in an iterative process using svms. the achieved secondary structure q <dig> accuracy of 80% on a set of  <dig> non-redundant proteins shows that our method is more accurate than other secondary structure prediction methods. the dihedrally-enhanced secondary structure prediction method significantly improved the predictive accuracy of helical and extended residues. moreover, the prediction of dihedral angles is more accurate than other multi-state dihedral prediction methods and achieves a mae comparable to the reported mae of real-spine  <dig>  and  <dig>   <cit> , a real-value dihedral predictor. the online version of disspred was trained using the larger pdb-select <dig> dataset. we are currently investigating the use of predicted dihedral angles as constraints for molecular dynamics simulations and together with the secondary structure predictions to facilitate predictions of protein tertiary structure. finally, we are working on enhancing the prediction of tight turns in proteins using predicted dihedral angles.

authors' contributions
pk carried out the experiments and wrote the manuscript. jdh conceived the study and assisted in writing the manuscript. both authors read and approved the final manuscript for publication.

supplementary material
additional file 1
cluster centroids and standard deviation for each cluster. the cluster centres with the standard deviation of each cluster are shown for all the different partitions of the ϕ - ψ space are shown using em and k-means clustering.

click here for file

 additional file 2
clustering and secondary structure distribution in every cluster. all the different partitions of the ϕ - ψ space are shown using em and k-means clustering as well as the distribution of secondary structure element in each cluster.

click here for file

 additional file 3
secondary structure prediction in every cluster before and after using additional dihedral information. the impact of additional dihedral information on the secondary structure prediction in every cluster is presented.

click here for file

 additional file 4
the mae and q <dig> after the second iteration of disspred using em clustering. the mean absolute errors  and the percentage of predicted dihedral angles within 30° of the real value  for both backbone dihedral angles ϕ and ψ after two iterations of our method using em clustering. in bold are the best results in every case.

click here for file

 acknowledgements
we acknowledge the hpc facility at the university of nottingham. we thank b. bulheller for his help in designing the disspred web server and c. bruce for technical assistance. we also thank the university of nottingham for a phd studentship.
