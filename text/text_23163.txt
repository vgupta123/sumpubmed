BACKGROUND
this paper introduces a flexible and loosely coupled data management system for high throughput sequencing experiments. the system is designed to face the challenges of research, and is required as the versatility and applicability of high throughput sequencing experiments is growing rapidly. the system can be overlaid on top of existing software, and can be used to integrate different specialized algorithms.

there already exist a number of commercial solutions , and non-commercial solutions  for the management and analysis of high throughput sequencing information. the main drawback to these solutions is that they focus on providing static "one stop shop" solutions, which are designed to fit known markets, using well-established methods. while these static systems are useful for non-technical researchers in a production science environment, they lack flexibility for the research scientist who wishes to use cutting edge methods and tools.

the existing systems tend to focus on well-established applications for high throughput sequencing: experiments where the technology is seen as a more accurate "digital" equivalent to microarrays , experiments to determine protein binding , or large scale genome assembly projects. however, high throughput sequencing has the potential of becoming ubiquitous across many avenues of investigation. this potential is due to both an increase in our understanding of systems biology and the capabilities of the new generation of instruments. as the field is constantly evolving new discoveries are continually being made, including new medically related functionality of small rnas  <cit> , new families of rna  <cit> , and signaling through extra-cellular rnas  <cit> . new techniques and instruments are also being developed that provide insight into these new facets, due to an increase in throughput  and sophistication . for these reasons, any sequencing software infrastructure used in the research environment must be easily adaptable. by this we mean it must have the ability to be readily changed for new usage. for example, we can expect each research area to require different mechanisms for normalization and replication strategies, sample and experiment vocabularies, and analysis algorithms. generally within research each project requires a large amount of de novo analysis development and customization to support: new technology strategies such as allowing for multiplexing or integrating with new instrumentation; informatics strategies, to allow for data and system integration; and new computational strategies, to support analysis and data-mining tasks. additionally, each laboratory will have their own demands in terms of experiment qa, annotations and integration with processes  and integration with other data types. therefore, it is important that the research community have access to a system that is:

• open. the system must be distributed as an open software project as many users will need to modify the system to meet their specific needs.

• standardized. the system should follow widely used standards for both software development and data exchange. this will ensure that the code base will be easier to maintain and have greater connectivity with external systems and tools.

• adaptable. the system must be easily adaptable without requiring a detailed understanding of the aspects of the internal software architecture. in this way, significant modifications can be implemented efficiently and quickly.

• deployable. the system must be easy to rapidly deploy and modify. a system that is cumbersome or overly complex wastes the end user's development time with unnecessary setup and technical details.

seqadapt follows these principles, and provides a standardized and modular architecture which is easy to use, adapt and maintain. the underlying enterprise architecture, addama  <cit>  has been designed to provide the adaptability required to enable the rapid development needed within research driven science.

implementation
to meet the demands of researchers we have developed seqadapt, a solution that is able to: scale to meet the requirements of the research environment, use best practices for mainstay applications , and be readily adapted to new usage.

the system is built using a general software infrastructure to support adaptable data management . seqadapt integrates external sample tracking software , workflows for executing analyses  and robust data management  to provide a modular and adaptable system for high throughput sequencing experiments.

due to the data volumes involved with high throughput sequencing a software infrastructure is often required to facilitate storage, management and analysis. we have used the addama system to provide the necessary support for the creation of a workflow encompassing the entire process  that is complete, lightweight and easily adapted to changing requirements. this solution allows for changes in the underlying sequencing technology while still providing the ability to plug in new processing methods. a pluggable architecture is important as the technology, data formats, and processing methods are changing rapidly in the field of sequencing. performance and the ability to scale up as datasets grow in size is also a requirement with the management of sequencing data. to allow for flexible deployment, we use a service oriented architecture , where distributed services provide the required functions. as these services grow, they can be deployed on the appropriate computational infrastructure .

RESULTS
the seqadapt default download consists of a preconfigured bundle of services. each of these services can be interchanged with alternative implementations. additionally the underlying system makes it possible to develop custom applications, and to rapidly integrate new analysis tools .

the software infrastructure is comprised of three main services: a sample tracking system, a data management system and a process management system . the sample tracking system's functionality includes sample submission, annotation with controlled vocabularies and file management. the data management system uses addama to organize the data and to trigger new analyses. the process management system  is a lightweight and generic system for executing processing pipelines and persisting inputs and outputs. the addama robot allows for analyses to be run directly on a dedicated machine which has been configured for a specific analysis . typically the analyses are run on either a server machine or, if the analysis is still under in-house development, on the specific developers computer. the robot is responsible for monitoring jobs and the transparent transportation of required data between the repository and the analysis environment.

the data management system uses standard open technologies including: content repositories which are used to generically store all experimental information; information indexing services, which provide for search capabilities across all data and metadata stored; and service registries, which allow for run-time discovery of different content repositories and associated services. addama also provides an abstraction so that a set of interlinked content repositories can be accessed through a single web application layer. this layer is exposed with a json based restful web service. the process management system coordinates jobs using a java message service . with this configuration, the system can scale from a single computer to a distributed set of execution agents on multiple servers to listening to a jms message queue.

all components of the seqadapt system are loosely coupled to allow for easy replacement with alternative systems. these alternative systems could include different sample tracking systems  different persistence stores  and different analysis tools .

this system allows for rapid integration of scientific algorithms using the standardized addama framework. the integration system is designed to be flexible, and allows for any command line analysis tools to be "plugged in". the framework is suited for developing small-scale analyses as well as for large scale processing that requires scaled-up distribution. further, all components needed for this system are provided in an easy-to-install package.

the default download for seqadapt has been set up for chip-seq analysis, and will process data from the illumina genome analyzer ii using the macs algorithm . the default install allows the user to submit an analysis, monitor its progress, and then view the result files.

integrating new analyses
as discussed above, the enterprise system can be extended in a number of ways. as the system is based upon distributed loosely coupled services, these services can be replaced with ones offering the same interface. each of the major components are built against technology standards .

new analysis pipelines can be added by writing a mapping module and then registering the analysis with addama. a benefit of the addama systems is that a script can execute in a preexisting development environment, eliminating the time consuming task of replicating software installations on a processing server. when algorithms have reached a mature state and are used widely, the system scales up to have the execution agent installed on many servers. a key benefit of this system is that it manages all of the non-scientific functionality needed in this type of processing. this freedom from writing boilerplate infrastructure code allows the computational biologist to focus on developing the needed scientific software.

the addama robot  allows for the rapid integration of these tools in a relatively short period of time. integration of these tools requires that the developer has a rudimentary understanding of addama, and also understands how the specific analysis tool works .

by way of example we have integrated the erange  <cit>  rna-seq analysis tool. once erange was installed the integration work was completed in less than a day. any analysis script that can be run from the command line can be integrated in the same manner. the steps involved in such integration are:

 <dig>  define input location. this is done by providing a command-line executable wrapper script. this script will define all of the inputs to the erange analysis and execute it. it will read the inputs from a local json file downloaded from the addama service by the robot.

 <dig>  control outputs by configuring the wrapper script to write all results to an "/outputs" directory. this directly will be in the same location as the script, the creation of the directory will be handled by the robot as well. similarly any log information  should be written to a "/log" directory.

 <dig>  register the script by configuring the addama robot. the robot uses a properties file that defines the wrapper script that is to be executed, and a local path where each run will be output. update these properties to reflect the locations of the erange script and the directory where it write inputs, outputs and log messages.

 <dig>  enable user submissions. to make submitting simple for the user, an optional web application may be developed. this application will take the expected inputs and send them to the addama system via the rest interface. this same page can also be used to query addama for the results of the robot analysis and display those for the user as well.

the robot automates the tasks that are required to integrate the analysis with the enterprise system. when the analysis is triggered the robot is responsible for the delivery of the inputs to the analysis, starting the analysis and monitoring the outputs. when completed the outputs, and any associated logs are loaded back, into addama.

walkthrough
a walk-through showing the default workflow for seqadapt is given in figures  <dig> , <dig> and  <dig>  this walk-through shows how the system can be used to capture information about a chip-seq experiment, store the results and then analyze the reads using macs.

CONCLUSIONS
the default system can be used "as-is" to support chip-seq analysis, however it can also be rapidly customized to suit new usage. the requirement for such flexibility is due to the fact that it is rarely possible to foresee all the usages with which a new instrument technology, such as high throughput sequencing, will be applied.

to be able to meet the requirements of a rapidly evolving research environment, as is found in most scientific institutions, an adaptable data management system is required. the modular framework described in this article is designed to provide such adaptability. the major advantage of the seqadapt/addama system is that "ad hoc" tools can be rapidly integrated. in this instance "ad hoc" means tools that do not adhere to predefined standards and have been built without integration in mind .

the system is available from the institute for systems biology's informatics web site, as: a demonstration system running as a web application, a preconfigured install for quick deployment and as a full download containing all the separate services . if access control is a requirement then the full download should be used.

availability and requirements
project name: seqadapt.

project home page: http://informatics.systemsbiology.net/informatics/seqadapt

operating system: platform independent.

programming language: java; other requirements: java  <dig>  or higher.

licence: apache  <dig> .

any restrictions to use by non-academics: none.

full installation instructions and software downloads are available at the project web site.

abbreviations
json : a simple object serialization format originally designed to work with javascript. json is growing in popularity over other text based formats  due to its lightweight and simplistic nature. json encoders and parsers are available in most languages; soa : a software architecture method where modules of software are separated into services that expose an interface of functionality. these modules can be combined in several ways to create larger distributed applications; web service: a software system that supports interaction over a network. web services are typically exposed using http; jms : a java api standard that allows java applications to exchange messages. it allows for distributed communication that is loosely coupled and asynchronous; rest : an architecture for distributed content, such as the world wide web. a restful web service uses standard http constructs  to provide services, and communicates back to the client using http response codes  along with content; jcr : a type of object store that is suited towards the storage, searching and retrieval of hierarchical data. a jcr can store both metadata and as well as files;

authors' contributions
sr and db developed the initial chipseq analysis, and translated that work into a pipeline. bm developed the slimseq suite and consulted on pipeline development, hr, db, cc, sk, jh and jl developed the addama software. rb and jl provided the installation package, sk, jh and jl tested and wrote the installation and use instructions. is and jb conceived of the study, and participated in its design and coordination. all authors read and approved the final manuscript.

