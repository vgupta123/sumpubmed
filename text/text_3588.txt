BACKGROUND
it remains an important and relevant problem to accurately predict the secondary structure of proteins based on their amino acid sequence. the identification of basic secondary structure elements – alpha helices, beta strands, and coils – is a critical prerequisite for many tertiary structure predictors, which consider the complete three-dimensional protein structure. to date, there has been a broad array of approaches to secondary structure prediction, including statistical techniques, neural networks, hidden markov models, support vector machines, nearest neighbor methods and energy minimization. in terms of prediction accuracy, neural networks are among the most popular methods in use today  <cit> , delivering a pointwise prediction accuracy  of about 77% and a segment overlap measure   <cit>  of about 74%  <cit> .

however, to improve the long-term performance of secondary structure prediction, it likely will be necessary to develop a cost model that mirrors the underlying biological constraints. while neural networks offer good performance today, their operation is largely opaque. often containing up to  <dig>  parameters and relying on complex layers of non-linear perceptrons, neural networks offer little insight into the patterns learned. moreover, they mask the shortcomings of the underlying models, rendering it a tedious and ad-hoc process to improve them. in fact, in the past  <dig> years, the largest improvements in neural network prediction accuracy have been due to the integration of homologous sequence alignments  <cit>  rather than specific changes to the underlying cost model.

in our approach we focus on simpler, more natural cost models that are based on the underlying biophysics. due to the lack of experimentally determined free energy values, we begin with parameterizable cost functions, and treat parameter value estimation as an optimization problem. our goal is then to determine the values of these "pseudo-energies" such that they correctly predict known protein structures. an iterative constraint-based optimization method is used to do this machine learning, incorporating the power of support vector machines .

using a cost function based on hidden markov models , we develop a secondary structure predictor for all-alpha proteins. with only  <dig> parameters, representing the energetic benefit for each residue being in a helix or being a certain distance from the n- or c-cap, our predictor achieves a qα value of  <dig> % and a sovα score of  <dig> % when applied to a database of all-alpha proteins. our technique does not depend on any homologous sequence alignments. when compared to other methods that do not utilize alignment information, it appears that our qα represents a  <dig> % improvement of the previous best  <cit> , while our sovα is comparable . however, due to differences in the data set, we emphasize the novelty of the approach rather than the exact magnitude of the improvements. we are extending our technique to beta strands  as ongoing work.

related work
king and sternberg share our goal of identifying a small and intuitive set of parameters in the design of the dsc predictor  <cit> . dsc is largely based on the classic gor technique  <cit> , which tabulates  the frequency with which each residue appears at a given offset  from a given structure element . during prediction, each residue is assigned the structure that is most likely given the recorded frequencies for the surrounding residues. king and sternberg augment the gor algorithm with several parameters, including the distance to the end of the chain and local patterns of hydrophobicity. they use linear discrimination to derive a statistically favorable weighting of the parameters, resulting in a simple linear cost function; they also perform homologous sequence alignment and minor smoothing and filtering. using about  <dig>  parameters, they estimate an accuracy of qα =  <dig> % for dsc. the primary difference between our predictor and dsc is that we achieve comparable accuracy  without providing alignment information. incorporating an alignment profile is often responsible for 5–7% improvement in accuracy  <cit> . in addition, we learn the position-specific residue affinities rather than using the gor frequency count. we also consider multiple predictions simultaneously and maintain a global context rather than predicting each residue independently.

many researchers have developed hidden markov models  for secondary structure prediction. once it has been trained, our predictor could be converted to an hmm without losing any predictive power, as our dynamic programming procedure parallels the viterbi algorithm for reconstructing the most likely hidden states. however, for the training phase, our system represents a soft-margin hidden markov svm  <cit>  rather than a traditional hmm. unlike an hmm, a hidden markov svm has a discriminative learning procedure based on a maximum margin criterion and can incorporate "overlapping features", driving the learning based on the overall predicted structure rather than via local propagation.

tsochantaridis, altun and hofmann apply an integrated hmm and svm framework for secondary structure prediction  <cit> . the technique may be similar to ours, as we are using their svm implementation; unfortunately, there are few details published. nguyen and rajapakse also present a hybrid scheme in which the output of a bayesian predictor is further refined by an svm classifier  <cit> . the qα score is  <dig> % for the bayesian predictor alone and  <dig> % for the bayesian/svm hybrid; the sovα score is  <dig> % for the bayesian predictor and a comparable  <dig> % for the bayesian/svm hybrid. to the best of our knowledge, these are the highest qα and sovα scores to date  for a method that does not utilize alignment information.

bystroff, thorsson, and baker design an hmm to recognize specific structural motifs and assemble them into protein secondary structure predictions  <cit> . using alignment profiles, they report an overall q <dig> value of  <dig> %. our approach may use fewer parameters, as they manually encode each target motif into a separate set of states. martin, gibrat, and rodolphe develop a 21-state hmm model with  <dig> parameters that achieves an overall q <dig> value of  <dig> %  and 72%   <cit> . alpha helices are identified based on an amphiphilic motif: a succession of two polar residues and two non-polar residues. won, hamelryck, prügel-bennet and krogh give a genetic algorithm that automatically evolves an hmm for secondary structure prediction  <cit> . using alignment profiles, they report an overall q <dig> value of 75% . they claim that the resulting 41-state hmm is better than any previous hand-designed hmm. while they restrict their hmm building blocks to "biologically meaningful primitives", it is unclear if there is a natural energetic interpretation of the final hmm.

schmidler, liu, and brutlag develop a segmental semi-markov model , allowing each hidden state to produce a variable-length sequence of the observations  <cit> . they report a q <dig> value of  <dig> % without using alignment profiles. chu and ghahramani push further in the same direction, merging with the structure of a neural network and demonstrating modest  improvements over schmidler et al.  <cit> .

while our technique is currently limited to an alpha helix predictor, for this task it performs better  than any of the hmm-based methods described above; furthermore, it does so without any alignment information. our technique is fundamentally different in its use of hidden markov svms for the learning stage. lastly, some groups have applied hmm-based predictors to the specific case of transmembrane proteins, where much higher accuracy can be obtained at the expense of generality  <cit> .

there has been a rich and highly successful body of work applying neural networks to secondary structure prediction. the efforts date back to quian and sejnowski, who design a simple feed-forward network for the problem  <cit> . rost and sander pioneered the automatic use of multiple sequence alignments to improve the accuracy as part of their phd predictor  <cit> , which was the top performer at casp <dig>  more recently, jones employed the psi-blast tool to efficiently perform the alignments, boosting his psipred predictor  <cit>  to the top of casp <dig>  baldi and colleagues employ bidirectional recurrent networks in sspro  <cit> , a system that provided the foundation for pollastri and mclysaght's porter server  <cit> .

petersen describes a ballotting system containing as many as  <dig> neural networks; while an ensemble of predictors is commonly used to gather more information, this effort is distinguished by its size  <cit> . a neural network has been followed by an hmm, resulting in a simple and fast system  <cit> ; neural networks have also been used as a post-processing step for gor predictors  <cit> .

the psipred predictor  <cit>  is among the highest scoring neural network techniques. while it achieves an overall q <dig> of about 77% and an sov of 74%, its performance for alpha helices is even higher: for recent targets on eva, an open and automatic testing platform  <cit> , psipred offers an sovα of  <dig> % .

though state-of-the-art neural network predictors such as psipred currently out-perform our method by about 5%, they incorporate multiple sequence alignments and are often impervious to analysis and understanding. in particular, the number of parameters in a neural network can be an order of magnitude higher than that of an hmm-based approach . a notable exception is the network of riis and krogh, which is structured by hand to reduce the parameter count to as low as  <dig> .

recently, support vector machines  have also been used as a standalone tool for secondary structure prediction  <cit> . in contrast to our technique, which uses an svm only for learning the parameters of an hmm, these methods apply an svm directly to a window of residues and classify the central residue into a given secondary structure class. the number of parameters in these techniques depends on the number of support vectors; in one instance, the support vectors occupy  <dig> mb of memory  <cit> . regardless of the number of parameters, it can be difficult to obtain a biological intuition for an svm, given the non-linear kernel functions and numerous support vectors. nonetheless, these techniques appear to have significant promise, as nguyen and rajapakse report an overall q <dig> of  <dig> % and an sov of  <dig> % on the psipred database  <cit> .

RESULTS
we have applied our method to the problem of all-alpha protein secondary structure prediction. we worked with a set of  <dig> non-homologous all-alpha proteins taken from eva's largest sequence-unique subset  <cit>  of the pdb at the end of july  <dig>  the sequences and structures have been extracted from pdb data processed by dssp  <cit> . only alpha helices have been considered ; everything else has been lumped as coil regions.

in our experiments, we split our  <dig> proteins into two  <dig> protein subsets. the first set is used to train our parameterizable cost function; the second set is used to evaluate the cost function once its parameters have been learned. since the results vary a bit depending on how the proteins are split in two sets, we train the cost function on  <dig> random partitions into training and test sets, and report the average performance. our predictor minimizes the free-energy function g using the viterbi algorithm on a simple 7-state finite state machine . the finite state machine recognizes alpha helices of length greater than  <dig> amino acids using  <dig> elementary free-energies as learned weights. these weigh each amino acid's propensity to be within a helix , or within three residues of an n- or c-cap of a helix . two weights also penalize  <dig> and  <dig> length coils. the motivation for and implementation of the finite state machine is described in more detail later.

on average, our method predicts helices in all-alpha proteins with an accuracy of  <dig> %  or  <dig> % . unfortunately, these results are difficult to compare with existing prediction methods which usually do predictions on both alpha helices and beta strands. rost and sanders caution that restricting the test set to all-alpha proteins can result in up to a 3% gain in accuracy  <cit> . nonetheless, if one does compare our technique with the previous best amongst methods that do not utilize alignment information  <cit> , our results represent a  <dig> % improvement in qα and a  <dig> % improvement in sovα.

additional care should be taken in comparing these numbers to recent techniques such as psipred  <cit> , which consider  <dig> helices  to be part of a helix rather than a loop; they report gains of about 2% in overall q <dig> if helices are restricted to 4-helices . apart from prediction accuracy, our technique is distinguished from others by its emphasis on an intuitive and biophysically-motivated cost function. while some of techniques require upwards of  <dig>  parameters , our predictor achieves competitive accuracy using only  <dig> parameters.

the real power of the machine learning method we use is its applicability beyond hmm models. as will become evident in the description of the method, we could describe protein structures as a parse tree of a context-free grammar  rather than as a sequence of hmm states. with these enriched descriptions, we should be able to include in the cost function interactions between adjacent strands of a beta sheet. this should allow us to incorporate beta sheet prediction into our algorithm.

unlike most secondary structure methods, we would then be able to predict not only which residues participate in a beta sheet, but also which residues are forming hydrogen bonds between adjacent sheets.

CONCLUSIONS
this work is a promising first pass at using svm techniques to find the elementary free-energies needed to predict protein secondary structure. the method we use is general and can be extended beyond the all-alpha case described here. in future work, we plan to extend this method to super-secondary structure prediction, generating contact maps of individual hydrogen bonds in beta sheets.

