BACKGROUND
protein-protein interactions  are critical for almost all biological processes  <cit> . many efforts have been made to investigate the residues at protein-protein interfaces. the checking of a large number of protein-protein interaction interfaces has shown that there are no general rules, which can describe the interfaces precisely  <cit> . it is also well known that the binding free energy is not uniformly distributed over the protein interfaces, and a small portion of interface residues contribute the most of binding free energy instead  <cit> . these residues are termed as hot spots. identifying hot spots and revealing their mechanisms may provide promising prospect for medicinal chemistry.

alanine-scanning mutagenesis  <cit>  is a popular method to identify hot spots by evaluating the change in binding free energy when substituting interface residues with alanine. hot spots are defined as those sites where alanine mutations cause a significant change in binding free energy . owing to the high cost and low efficiency of this traditional experimental method, public databases of experimental results such as the alanine scanning energetics database   <cit>  and the binding interface database   <cit>  contain only a limited number of complexes.

some works focused on the characteristics of hot spot due to its critical role. studies on the composition of hot spots and non-hot spots have revealed that trp, arg and tyr rank the top  <dig>  with the rates of 21%,  <dig> % and  <dig> % respectively. while leu, ser, thr and val are often disfavored  <cit> . furthermore, hot spots are found to be more conserved than non-hot spots, and they are usually surrounded by a group of residues not important for binding, whose role is to shelter hot spots from the solvent  <cit> .

based on the existing studies on the characteristics of hot spots, some computational methods have been proposed to predict hot spots. these methods roughly fall into three categories: molecular dynamics  simulations, energy-based methods and feature-based methods.

molecular dynamics   <cit>  simulations simulate alanine substitutions and estimate the corresponding changes in binding free energy. although these molecular simulation methods have good performance on identifying hot spots from protein interfaces, they suffer from enormous computational cost.

energy-based methods use knowledge-based simplified models to evaluate binding free energy for predicting hot spots. kortemme and baker  <cit>  proposed a simple physical model using a free energy function to calculate the binding free energy of alanine mutation in a protein-protein complex. guerois et al.,  <cit>  provided foldef whose predictive power has been tested on a large set of  <dig> mutants spanning most of the structural environments found in proteins. tuncbag et al.,  <cit>  established a web server hotpoint combining conservation, solvent accessibility and statistical pairwise residue potentials to computationally predict hot spots effectively.

in recent years, some machine learning based methods with focus on feature selection were developed to identify hot-spots. ofran and rost  <cit>  proposed a neural network based on sequence to predict hot spots. darnell et al.,  <cit>  provided a web server kfc by using decision trees to predict hot spots. some works use different features as input of a support vector machine  classifier to predict hot spots. cho et al.,  <cit>  developed two feature-based predictive svm models for predicting interaction hot spots. xia et al.,  <cit>  introduced both a svm model and an ensemble classifier based on protrusion index and solvent accessibility to boost hot spots prediction accuracy. zhu and mitchell  <cit>  developed a new web server, named kfc <dig>  by employing svm with some newly derived features.

although machine learning based methods have obtained relatively good performance on the prediction of hot spots. there are still some problems remaining in this area. though many features have been generated and used in the previous studies, effective feature selection methods and useful feature subsets have not been found yet. moreover, most of the existing methods use very limited data from experiment-derived deposits, therefore the training set is insufficient, which leads to unsatisfactory prediction performance.

to deal with the problems mentioned above, in this paper we first extract features of both sequence and structure, and employ random forests  <cit>  to generate an effective feature subset. then we propose a boosting svm based approach, sbsvm, to improve the prediction of hot spots by using unlabeled data. our method integrates unlabeled data into the training set to overcome the problem of labeled data inadequacy. finally, we evaluate the proposed method by 10-fold cross-validation and independent test, which demonstrate the performance advantage of our approach over the existing methods.

methods
datasets
the first training data set in this study, denoted as dataset <dig>  was extracted from asedb  <cit>  and the published data by kortemme and baker  <cit> . to eliminate redundancy, we used the cath , architecture , topology  and homologous superfamily ) query system with the sequence identity less than 35% and the ssap score less than or equal to  <dig>  details are listed in table  <dig>  we define interface residues with ΔΔg ≥  <dig>  kcal/mol as hot spots and those with ΔΔg ≤  <dig>  kcal/mol as non-hot spots  <cit> .

h stands for hot spot and nh stands for non-hot spot. dataset <dig> was derived from dataset <dig> 

as a result, dataset <dig> consists of  <dig> interface residues derived from  <dig> protein-protein complexes, where  <dig> residues are hot spots and  <dig> residues are energetically unimportant residues. in order to train better predictors, we balanced the positive and negative samples as in  <cit> . the negative samples  were divided into  <dig> groups and each was combined with the positive samples . the third group  combines with  <dig> hot spots, which is denoted as dataset <dig> and can obtain better results than the other two combinations when being used to train our predictor.

an independent test dataset, denoted as ind-dataset, was obtained from the bid database  <cit>  to further evaluate our method. in the bid database, the alanine mutations were listed as either "strong", "intermediate", "weak" or "insignificant". in this study, only residues with "strong" mutations are considered as hot spot and the others are regarded as non hot spot. as a result, ind-dataset consists of  <dig> interface residues derived from  <dig> protein-protein complexes, where  <dig> residues are hot spots and  <dig> residues are energetically unimportant residues.

as a summary, the statistics of dataset <dig>  dataset <dig> and ind-dataset are presented in table  <dig> 

features
based on previous studies on hot spots prediction, we generate  <dig> sequence features and  <dig> structure features.

sequence features
the sequence features used in this paper include the number of atoms, electron-ion interaction potential, hydrophobicity, hydrophilicity, propensity and isoelectric point. these physicochemical features can be obtained from the aaindex database  <cit> .

structure features
firstly, we used the implementation psaia proposed by mihel et al.,  <cit>  to generate features about solvent accessible surface area , relative solvent accessible surface area , depth index  and protrusion index , which are defined as follows:

• accessible surface area  is the atomic surface area of a molecule, protein and dna etc., which is accessible to a solvent.

• relative asa  is the ratio of the calculated asa over the referenced asa. the reference asa of a residue x is obtained by gly-x-gly peptide in extended conformations  <cit> .

• depth index : the depth of an atom i  can be defined as the distance between atom i and the closest solvent accessible atom j. that is, dpxi = min where d <dig>  d <dig>  d <dig>  ..., dn are the distances between the atom i and all solvent accessible atoms.

• protrusion index  is defined as vext/vint. here, vint is given by the number of atoms within the sphere  multiplied by the mean atomic volume found in proteins; vext is the difference between the volume of the sphere and vint, which denotes the remaining volume of the sphere.

from asa and rasa, five attributes can be derived:

• total ;

• backbone ;

• side-chain ;

• polar ;

• non-polar .

and based on di and pi, four residue attributes can be obtained:

• total mean ;

• side-chain mean ;

• maximum ;

• minimum .

therefore,  <dig> features were generated by psaia from unbound and bound states.

in addition, the relative changes of asa, di and pi between the unbound and bound states of the residues were calculated as in xia et al's work  <cit> , and  <dig> more features were generated by the equations below:

 rcasa=/asaunbound,rcdi=/dibound,rcpi=/piunbound. 

furthermore, we generated some useful features following the strategy of kfc <dig>  <cit> . residues' solvent accessible surface is used in the following features and is calculated by naccess  <cit> .

delta_tot describes the difference between the solvent accessible surfaces in bound and unbound states:

 delta_tot=asaunb-asabnd. 

sa_ratio <dig> is the ratio of solvent accessible surface area over maxasa, which stands for the residue's maximum solvent accessible surface area as a tripeptide  <cit> :

 sa_ratio5=delta_tot×maxasaasaunb. 

another form of ratio of solvent accessible surface area, core_rim, is given by:

 core_rim=delta_totasaunb. 

and this feature is quite like the relative change in total asa described before. the main difference lies in that psaia treats each chain separately during the calculation  <cit> . in our work we will use at most one of these two features in order to avoid a bias.

pos_per is defined as below, where i is the sequence number of the residue and n is the total number of the interface residues:

 pos_per=core_rim×in. 

rot <dig> and rot <dig> stand for the total numbers of the side chain rotatable single bonds to target residues for the residues within  <dig> Å and  <dig>  Å, respectively.

hp <dig> is the sum of hydrophobic values of all neighbors of a residue within 5Å.

fp9n, fp9e, fp10n and fp10e were directly calculated by fade  <cit>  that is an efficient method to calculate atomic density.

plast  <dig> and plast  <dig> were calculated as:

 plast4=wt_rot4atmn4×maxasa,plast5=wt_rot5atmn5×maxasa, 

where wt_rot <dig>  wt_rot <dig> count weighted rotatable single bond numbers of a residue's side chain within 4Å and 5Å respectively, and atmn <dig>  atmn <dig> indicate the total numbers of surrounding atoms of a residue within 4Å and 5Å respectively.

feature selection
feature selection is an important step in training classifiers and is often utilized to improve the performance of a classifier by removing redundant and irrelevant features.

in this work,  <dig> features were generated initially. such a feature set may cause over-fitting of the model. therefore, we employed random forests proposed by breiman  <cit>  to find important features, with which to get better discrimination of hot spot residues and non-hot spot residues.

random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forests. random forests return several measures of variable importance. the most reliable measure is based on the decrease in classification accuracy when the values of a variable in a node of a tree are permuted randomly  <cit> .

the feature importance of the balanced training data set, dataset <dig>  is illustrated in figure  <dig>  here, we still tried various combinations from the top- <dig> features. the features we used in the prediction model for dataset <dig> are: sa_ratio <dig>  relative change in side-chain mean pi upon complexation, relative change in minimal pi upon complexation, relative change in total asa upon complexation, s-chain rasa, relative change in polar asa upon complexation.

semiboost framework
mallapagada et al.,  <cit>  presented a boosting framework for semi-supervised learning to improve supervised learning, termed as semiboost, by using both labeled data and unlabeled data in the learning process. the framework is given as follows.

given a data set d = {x <dig>  x <dig>  x <dig>  . . ., nn}, the labels for the entire dataset can be denoted as y =  where the labeled subset is denoted by yl= and the unlabeled subset is denoted by yu= with n = nl + nu. it can be assumed that an unlabeled data xu and a labeled data with the highest similarity to xu may share the same label. the symmetric matrix slu represents the similarity between labeled and unlabeled data. the term fl stands for the inconsistency between labeled and unlabeled data. it can also be assumed that two unlabel data points with the highest similarity may share the same label. the symmetric matrix suu represents a similarity matrix based on the unlabeled data. the term fu stands for the inconsistency among unlabeled data. thus an objective function f can be obtained from the above two terms. our goal is to find the label yu that minimizes f.

concretely, the objective function is given as

  f=fl+cfu 

where c weights the importance between the labeled and unlabeled data. the two terms in  are given as follows:

  fl= ∑i=1nl∑j=1nusi,jluexp, 

  fu= ∑i,j=1nusi,juuexp. 

let ht denote the classifier trained at the t-th iteration by the underlying learning algorithm a and h denote the combined classifier, we have

  h= ∑t=1tαtht 

where αt is the combination weight. then, the learning problem is transformed to the following optimization problem:

  argminh,α ∑i=1nl∑j=1nusi,jexp)+c ∑i,j=1nusi,jexpexp)s.t.h=yil,i= <dig> …,nl. 

by variable substitution and regrouping,  can be transformed into

  f1¯= ∑i=1nuexppi+expqi 

where

  pi= ∑j=1nlsi,jule-2hjδ+c <dig> ∑j=1nusi,juuehj-hi, 

  qi= ∑j=1nlsi,jule-2hjδ+c <dig> ∑j=1nusi,juuehi-hj. 

above, pi and qi are considered as the confidences in classifying the unlabeled data into the positive and negative classes respectively.

the semiboost algorithm starts with an empty ensemble. at each iteration, it computes the confidence for unlabeled data and then assigns the pseudo-labels according to both the existing ensemble and the similarity matrix. the most confident pseudo-labeled data are combined with the labeled data to train a classifier using the supervised learning algorithm. the ensemble classifier is updated by the former classifiers with appropriate weights, and the iteration is stopped when α < <dig>  here

 α=14ln∑i=1nupiδ+ ∑i=1nuqiδ∑i=1nupiδ+ ∑i=1nuqiδ. 

mallapagada et al. proved the performance improvement on the supervised algorithms by using semiboost on different datasets, and semiboost outperforms the benchmark semi-supervised algorithms  <cit> .

svm
in this paper, we employed the support vector machine  as the underlying supervised learning algorithm in the semiboost framework.

svm was first developed by vapnik  <cit>  and was originally employed to find a linear separating hyperplane that maximizes the distance between two classes. svm can deal with the problems that can not be linearly separated in the original input space by adding a penalty function of violation of the constraints to the optimization criterion or by transforming the input space into a higher dimension space. it was widely used for developing methods in bioinformatics and has been proved to be effective in predicting hot spots  <cit> .

sbsvm: an svm with semi-supervised boosting to predict hot spots
in this study, we propose a new method that combines the semi-supervised boosting framework with the underlying supervised learning algorithm svm to predict hot spots.

in the original semiboost framework proposed by mallapagada et al., both confidence values of pi and qi might be large and there no any persuasive criterion to choose the most confident unlabeled data. directly choosing the top 10% of the unlabeled data will include too many ambiguous samples with pseudolabel at the early iterations.

in order to overcome the above problem, we modified the terms in equation  and equation  by assigning weights according to the similarity matrix sul and suu as follows:

  argminh,αϕ∑i=1nu∑j=1nlsi,julexp)∑jsi,jul+ψ∑i=1nu∑j=1nusi,juuexpexp)∑jsi,juus.t.h=yjl,j= <dig> …,nl 

where ϕ=1/ and ψ=c/. c is the tuning parameter for the importance of the labeled and unlabeled data, and we set its default value to nl/nu. given the above function, we can obtain the values of pi and qi as follows:

  pi=11+c <dig> ∑j=1nlsi,jule-2hjδ+c21+c <dig> ∑j=1nusi,juuehj-hi, 

  qi=11+c <dig> ∑j=1nlsi,jule-2hjδ+c21+c <dig> ∑j=1nusijuuehi-hj, 

which will have the maximum of  <dig>  then we sample the unlabeled data according to the following two criteria:  |pi − qi| ≥  <dig> ,  top 10% |pi − qi|. with that, we can assign pseudolabels to unlabeled data according to sign, and choose the most credible ones for training the classifier.

at each iteration, like the original semiboost framework, we update the ensemble classifier h with h + αtht. the algorithm stops when the number of iterations reaches t  or α < <dig>  figure  <dig> illustrates the basic workflow of the sbsvm approach. the similarity matrices are calculated initially and play an important role in selecting unlabeled samples. the unlabeled data with highest confidence will be added to the training set for the next iteration of training.

performance evaluation
to evaluate the classification performance of the method sbsvm proposed in this study, we adopted some widely used measures, including precision, recall , specificity, accuracy and f <dig> score. these measures are defined as follows:

 precision=tp,recall=tp,specificity=tn,accuracy=,f1=2×precision×recallprecision+recall. 

here, tp, fp, tn and fn denote the numbers of true positives , false positives , true negatives  and false negatives , respectively. f <dig> score is a composite measure, which is widely used to evaluate prediction accuracy considering both precision and recall.

RESULTS
parameter selection
the similarity matrices sul and suu are computed by the radial basis function. for example, let xi and xj be two samples from the dateset, the similarity between them is calculated by si,j = exp2/2σ2), where σ is the scale parameter that has a great impact on the performance of the learning algorithm. we tested  <dig> values of σ from  <dig> to  <dig> in a 10-fold cross-validation on dataset <dig> to get the best performance of our method. the performance of our method varies according to the value of σ, which is listed in table  <dig>  we chose the value of  <dig> for σ that produces the best performance. and for dataset <dig>  our method has the best performance when σ is set to  <dig> 

the optimization process will stop when α < <dig> during the iterations. however, in order to avoid a slow convergence, we set the maximum number of iterations t =  <dig> 

performance comparison and cross-validation
in this section, the performance of sbsvm is examined and compared with three existing machine learning methods, including svm  <cit> , bayes network  <cit>  and decision tree c <dig>   <cit> . we first conducted several cross-validation  tests and an additional test called random- <dig> test  on dataset <dig> to show that the boosting with unlabeled data method, sbsvm, outperforms the other three methods. the experimental results  are shown in figure  <dig>  from figure  <dig>  we can see that even when the training data is small, sbsvm still outperforms the others. as all the results of decision tree are less than  <dig> , we do not show them in figure  <dig> 

our approach was further compared with other five existing hot-spot prediction methods by 10-fold cross-validation on dataset <dig>  the compared methods include kfc  <cit> , robetta  <cit> , foldef  <cit> , min-erva  <cit>  and kfc <dig>  <cit> .

the results of the methods compared were collected from the original papers where these methods were published. all results are listed in table  <dig>  we can see that sbsvm has the best recall of  <dig>  among all these methods, and its f1-score is only outperformed by minerva. besides, the specificity and accuracy of our method are also competitive. table  <dig> shows the results of 10-fold cross-validation on dataset <dig>  we can see that our method has outstanding performance, with the highest recall  and f <dig> score . figure  <dig> illustrates the roc curves of our method on both datasets. the area under the curves are  <dig>   and  <dig>  .

independent test
here we evaluate sbsvm and compare it with other methods by independent test on ind-dataset described in the method section. the results are presented in table  <dig> and table  <dig>  performance results of the compared methods were obtained from their corresponding web servers.

remarks on the selected features
in this paper, we extracted a large set of features from previous studies, but only several were used in hot-spot prediction. the selected features for dataset <dig> and dataset <dig> are listed in table  <dig>  note that none of the sequence features were chosen in the two final feature combinations for dataset <dig> and dataset <dig>  this may imply that general sequence information is not so important in hot spot prediction.

the relative change in side-chain asa upon complexation, the relative change in total asa upon complexation, sa_ratio <dig> and core_rim measure from different aspects the changes in accessible surface of a residue between unbound and bound states. these structural features were all chosen in our prediction, which suggests that residues surrounded by others and sheltered from solvents are more likely to be hot spots  <cit> . meanwhile, the two different relative changes in protrusion index  used in our method are also strong evidence of hot spots. it was found that hot spots tend to protrude into complementary pockets  <cit> . therefore, these selected structural features also suggest that the high local packing density of a residue is helpful in predicting hot spots  <cit> .

as the structural information used in this paper indicate the nature of hot spots, our approach obtained the highest recall in hot spot prediction.

case study
epo  is produced by interstitial fibroblasts in the kidney, which is in close association with peritubular capillary and tubular epithelial cells. it is the hormone that regulates red blood cell production.

there exists a competition between emp <dig>  and epo to bind the erythropoietic receptor    <cit> . experimentally found hot spots at the 1ebpac interface are f93a, m150a, f205a and w13c, and t151a, l11c and t12c were found experimentally to be non-hot spots . our method predicts correctly two out of the four hot spots - m150a and f205a, and all of the three non-hot spots.

CONCLUSIONS
in this study we proposed a new effective computational method, named sbsvm, to identify hot spots at the protein interfaces. we combined sequence and structure features, and selected the most important features by random forests. our method is based on a semi-supervised boosting framework that samples some useful unlabeled data at each iteration to improve the performance of the underlying classifier . the performance of sbsvm was evaluated by 10-fold cross-validation and independent test. results show that our approach, with the best sensitivity and f <dig> score, can provide better or at least comparable performance than or to the major existing methods, including kfc, robetta, foldef, minerva and kfc <dig> 

our study has achieved substantial improvement on performance of hot spots prediction by using the unlabeled data. in our future work, on the one hand we will explore more useful features of both hot spots and non-hot spots, and on the other hand, we will try to develop more sophisticated hot spot prediction methods based on advanced machine learning techniques .

authors' contributions
bx and ld designed the method, bx implemented the method, conducted the experiments and data analysis, and finished the draft. xw prepared the data. sz and jg conceived the work, supervised the research and revised the manuscript.

competing interests
the authors declare that they have no competing interests.

