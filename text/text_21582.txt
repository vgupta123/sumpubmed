BACKGROUND
the objective of class prediction  is to develop a rule based on variables measured on a group of samples with known class membership , which can be used to assign the class membership to new samples . many different classifiers exist, and they differ in the definition of the classification rule  <cit> . nowadays classification rules are increasingly often developed using data that are high-dimensional  and also class-imbalanced . high-dimensional classification has become a popular task in the biomedical and bioinformatics community with the advent of high-throughput technologies in biomedicine a decade ago. for example, many researchers attempted to develop gene-expression classifiers based on microarray experiments for prognostic and predictive purposes in breast cancer  <cit> . the number of subjects included in the microarray based classification studies is usually in the range of hundreds, while the number of measured genes is in the tens of thousands. nowadays, the newly available next-generation sequencing methods provide billions of short reads for each subject, further increasing the high-dimensionality of data.

for high-dimensional data tibshirani et al. <cit>  proposed the nearest shrunken centroid method , which can be seen as a modification of the diagonal linear discriminant analysis . the classification rule of dlda is based on the scaled distance between the expression profiles of new samples and class centroids ; pam uses a very similar rule, but it shrinks the class centroids towards the overall means and it embeds a variable selection mechanism, which is generally useful in high-dimensional class-prediction  <cit> . the amount of shrinkage is usually determined minimizing the cross-validated error rate on the training set  <cit> . since its proposal, pam has been widely used in practice. the paper that first described the pam methodology  <cit>  has been cited about thousand times, mostly in journals from the biomedical field: just papers from the fields of oncology, biochemistry and biotechnology account for about half of the citations .

the classifiers trained on class-imbalanced data tend to classify most of the new samples in the majority class  <cit>  and this bias is further increased if data are high-dimensional  <cit> . it is somehow surprising that while dlda can perform fairly well with imbalanced data   <cit> , pam is very sensitive to the class-imbalance problem: it assigns most new samples to the majority class and achieves very poor accuracy for the minority class even when the level of class-imbalance is only moderate  <cit> . even when the differences between classes are large the predictive accuracy tends to be smaller in the minority class. for example, reeve and colleagues  <cit>  used pam to build a classifier to distinguish rejection from non-rejection kidney transplant using gene expression microarray data, achieving a better predictive accuracy for the majority class of non-rejection transplants: the cross-validated predictive accuracies were 80%  in the non-rejection group and 69% in the rejection group . similarly, korkola et al.  <cit>  used pam to predict the prognosis of  <dig> breast cancer patients and obtained a cross-validated predictive accuracy of 76%  for good prognosis patients and of 62%  for poor prognosis patients.

the class-imbalance bias of dlda can be attributed to the larger variability of the estimate of the minority class centroid  <cit> ; variable selection reduces the bias, but does not completely remove it if data are high-dimensional. the bias increases when the class-imbalance is larger and when more variables are measured because in these settings large discrepancies between sample values and true values are common in the minority class. intuitively pam should have an edge over dlda in the class-imbalanced scenario, as shrinking the class centroids towards the overall centroid should reduce the extreme mean values that arise by chance and consequently diminish the class-imbalance bias.

wang and zhu  <cit>  reinterpreted pam in the framework of the lasso regression  <cit>  and proposed two classifiers that enable different amount of shrinkage for each variable . they used simulated and real data to show that their methods outperform pam in most circumstances, but did not address specifically the class-imbalance problem.

in this article we identify the features of the nsc classifiers that contribute to the class-imbalance bias and propose modified methods, gm-pam, gm-alp and gm-ahp, to reduce the class-imbalance bias. gm classifiers estimate the optimal shrinkage maximizing the cross-validated geometric mean of the class-specific predictive accuracies  and do not use the class prior correction that is embedded in the original classifiers.

the rest of the article is organized as follows. in the “methods” section we present pam, ahp and alp classifiers. in section “results” we first explain the pitfalls of the existing approach for the determination of the optimal threshold and present the novel algorithm; we apply the algorithm to three nsc classifiers and compare them with the existing approaches using simulated and real high-dimensional data. we end with a discussion and conclusions in sections “discussion” and “conclusion”.

methods
let xij be the value of variable j  for sample i . each sample belongs to one of k classes  and yi is the class of the ith sample. let zik be a class membership indicator variable , and nk=∑i=1nzik be the number of samples in class k. the jth component of the centroid in class k is x¯kj=∑i=1nzikxij/nk and the jth component of the overall centroid is x¯j=∑i=1nxij/n. the shrunken centroid is defined as 

  x¯kj,=x¯j+d^kj·mk·, 

where sj is the pooled within-class standard deviation for the jth variable, s <dig> is a constant , mk=1/nk−1/n and in pam d^kj is defined as 

  d^kj=sgn+, 

where dkj=x¯kj−x¯jmk, λ≥ <dig> is a threshold parameter that needs to be tuned, and + is the positive part of .

the classification rule of pam for a new sample x∗ is 

  c=argminkδk, 

where δk is the discriminant score for class k, defined as 

  δk=∑j=1p22−2log=∑j=1plkj−2log; 

πk is the proportion of class k samples in the population , −2log is a class prior correction and lk=∑j=1plkj.

variable j is effectively not considered in the classification rule  when all x¯kj, are shrunken to x¯j as l1j=⋯=lkj; we call the other variables active.

wang and zhu  <cit>  showed that if the observation xi= from class k follows a multivariate normal distribution ) and the covariance matrices are the same across different classes and are diagonal ) then  is a solution to 

  d^kj=argmind~kj12∑i=1n∑j=1p∑k=1kziknk2+λ∑j=1p∑k=1k|d~kj| 

where x~ij=xij−x¯jmk, d~kj=μkj−x¯jmk and ∑j=1p∑k=1k|d~kj| is a penalty function. based on the observation that  is a lasso type estimator for d^kj, wang and zhu  <cit>  proposed two different penalty functions, 

  λ∑j=1pwj·maxk, and 

  λγ∑j=1pwjγγj+λθ∑j=1p∑k=1kwkjθ|θkj|, 

where wj, wjγ and wkjθ are pre-specified weights and λ, λγ and λθ are threshold parameters .

the shrunken centroids, discriminant scores and classification rules are the same as in pam; the classification rules that use  and  are denoted with alp and ahp, respectively.

pam, alp and ahp require the estimation of the threshold parameter λ, λγ and λθ. a normal procedure is to use the training data to estimate a cross-validated  error rate for different values of the threshold and use the threshold that produces the lowest overall error  <cit> . note that when the threshold is zero, then the classification rules of pam, alp and ahp are essentially the same as the classification rule of dlda , which is defined as 

  δk=∑j=1p2sj2−2log=lk−2log, 

where lk is the discriminant score omitting the class prior correction.

in practice for high dimensional data the class prior correction contributes little to the discriminant scores  and δk≈lk for large p), while it can bias the nsc classification towards the majority class if all or most of the variables are inactive ). for these reasons we used equal class priors for all the classes ), similarly as huang et al. <cit> . moreover, in case of ties the class membership was assigned at random to one of the classes with the smallest discriminant scores.

RESULTS
in this section we discuss the implications of estimating the optimal threshold for nsc classifiers by minimizing the cross-validated overall error, when data are class-imbalanced and high-dimensional. we then present a modified approach for threshold estimation aimed at reducing the class-imbalance problem for nsc classifiers, and show its effectiveness on simulated and real high dimensional class-imbalanced data.

threshold selection
in practice the threshold parameters of the nsc classifiers are estimated minimizing the cross-validated error rate for different values of the threshold; the threshold value that produces the lowest error is used to shrink the centroids.

the overall error is the probability of misclassifying new samples: 

  error=1−∑k=1kp=k|y∗=k)πk, 

and it depends on the class specific predictive accuracies =k|y∗=k)) and on the level of class imbalance. overall error and predictive accuracy  are misleading measures of the classifiers performance when data are class-imbalanced  <cit> : the predictive accuracies of the minority classes are given little weight and classifying all new samples in the majority class produces small overall error when the class-imbalance is extreme.

the high-dimensionality of data can additionally contribute to making the overall error an inappropriate measure to minimize. for the sake of simplicity let us focus on dlda; we consider a two-class classification problem and assume that there is no real difference between the classes  and that class  <dig> is the minority class . data are simulated from xi∼iidmvn) for both classes for n= <dig> samples, with π1= <dig>  or π1= <dig> ; results are shown in figure  <dig> 

the probability of classifying a new sample in the minority class is smaller when the class imbalance is more extreme and/or when more variables are measured. as a consequence, the error rate is a decreasing function of the number of variables and, when the number of variables is large, it approaches the proportion of the minority class samples in the population. for this particular setting the classification probabilities were derived also analytically, additionally assuming that the variances are known, see additional file  <dig> 

if we consider the shrunken centroid classifiers as a special form of dlda this result would suggest that in the class-imbalanced scenario the threshold selection based on the overall error will favor small threshold values , which in turn will lead to small probability of classification in the minority class and large bias in favor of the majority class.

the proposed approach
we propose to select the optimal threshold as the value that maximizes the cross-validated geometric mean of the class specific predictive accuracies , 

  gm=∏k=1kpakk. 

g-means is an accuracy metric often used for class-imbalanced data that captures the performance of the classifiers in all classes  <cit> . it gives the same weight to all the classes, it is independent of the class distribution of the test set and it penalizes the classifiers whose performance is heterogeneous across classes. furthermore, for a fixed total , it has the maximum when the class specific predictive accuracies are equal  <cit> .

in practice the class specific pa are estimated with pak=1/nk∑i=1nzik·ziy^, where ziy^ is the indicator for a correctly classified sample i =yi and zero otherwise) and they depend on the selected threshold value. it is not feasible to evaluate the cross-validated gm for all possible threshold values, therefore we limit our attention to a fixed number of thresholds. we consider t equally spaced threshold values, ranging from  <dig>  to λmax, the minimum threshold value that shrinks all the class centroids to the overall centroid, for all the variables . in the additional file  <dig> we show how to derive λmax for pam, alp and ahp.

the proposed approach for the estimation of the threshold can be used for each of the three nsc classifiers considered in this paper; the modified classifiers are denoted with gm-pam, gm-alp and gm-ahp, respectively. the proposed algorithm is presented below.

algorithm  <dig> gm shrunken centroid classifier  

results on the simulated data
in this section we present a series of selected results based on simulated data to assess the performance of the gm method and compare it with the original nsc classifiers.

in a two class classification scenario, we simulated  <dig>  variables from a multivariate gaussian distribution. we used a block exchangeable correlation structure, in which the variables in the same block were correlated  while the variables from different blocks were independent ; each block contained  <dig> variables and all variances were equal to  <dig>  the mean values were equal to  <dig> for all variables in class  <dig> . in the null case all the variables were non informative , while in the alternative case  <dig> variables were informative about class distinction .

the training sets contained  <dig> samples and the proportion of class  <dig> samples varied from  <dig>   to  <dig>  . 10-fold cv was used to estimate the optimal threshold parameter, using  <dig> different threshold values. the classifier trained on the complete data set with the estimated optimal threshold was used to make predictions on a large independent and balanced test set , simulated from the same distribution used for the training set. the performance on the test set was evaluated in terms of class-specific predictive accuracies, g-means, the area under the roc curve : these measures do not depend on the data distribution in the test set and can be estimated with equal precision when the test set classes are balanced. furthermore, it was previously shown that matching the prevalence in the training and test set does not attenuate the class-imbalance problem  <cit> . in the simulations we evaluated also the false discovery rate  and the false negative rate . each simulation was repeated  <dig> times.

the nsc classifiers assigned most new samples to the majority class when there was no difference between the classes  or the difference between classes was small or moderate . the bias towards the majority class was smaller when the classes were more balanced, when the number of variables was smaller  or the difference between classes was larger. the nsc classifiers were not effective in removing the non-informative variables: the number of active variables markedly increased with class-imbalance and most of the variables were not shrunken towards the overall centroid in the most imbalanced settings; as a consequence the fdr was close to  <dig>  in general, ahp had less active variables and a slightly smaller fdr, but the overall performance of pam, alp and ahp was similar.

the table reports the estimated optimal threshold , the number  of active non-informative variables  and the number  of active informative variables ), false discovery rate ), class specific predictive accuracies, g-means and auc, averaged over  <dig> repetitions; standard deviations are reported in brackets. the simulation settings are the same as in figure  <dig>  a for ahp and gm-ahp only λθ was optimized while λγ was set to zero.

the gm-nsc classifiers performed very similarly to nsc classifiers in the settings where the nsc classifiers were not biased towards the majority class, i.e. when the classes were balanced or were very different . in the other situations the gm-nsc classifiers reduced the gap between the class specific pa, obtaining larger minority class pa, g-means and auc, and greatly reducing the number of active variables; the removal of most of the non informative variables reduced the fdr and the bias towards the classification into the majority class , while the removal of a part of the informative variables increased the false negative rate . the best performance was obtained when the gm threshold optimization was used with pam, while the smallest improvement was seen for ahp. this can probably be attributed to the fact that the variables with larger dkj values are weighted and therefore shrunken less, which is not desirable for the non informative variables as large values of dkj arose by chance and should therefore be actually shrunken more; note that the smallest bias was observed for pam , where all variables are shrunken for the same amount. similar results were obtained simulating independent variables .

when some variables were differentially expressed and the class-imbalance was moderate, methods using the gm approach achieved slightly higher pa for the minority class, while this bias was only marginal if the original approach was used. note that the overall centroid can be expressed as x¯j=∑k=1kx¯kjnk/n; it is a weighted average of class specific mean values with more weight given to the majority class, so the overall centroid is closer to the sample mean of the majority class. when the threshold is large this has a consequence of shifting the minority class centroid towards the sample mean of the majority class and hence some of the new samples from the majority class are closer to the minority class  centroid than to the majority class  centroid. classifiers using the original approach do not suffer from this problem as the amount of shrinkage is small when the training set is class-imbalanced. one way of diminishing this bias would be to define the overall centroid as x¯j=∑k=1kx¯kj/k, and mk=1kk−2nk+1k∑k=1knk− <dig> as to assure that the denominator in calculation of dkj will be the appropriate standard error. we performed a limited set of simulations with this overall centroid definition for pam and observed that the bias in favor of the minority class was removed. however, when there was large class-imbalance, the results were slightly more biased in favor of the majority class  than with the original overall centroid definition. one reason for poor performance in the case of large class-imbalance is that relatively more weight was given to a less accurate estimate.

one of the possible strategies when dealing with class-imbalanced data is to use case weighting in order to adjust for the class-imbalance bias  <cit> . since case weighting is not implemented in the nsc classifiers we performed a limited set of experiments with random over-sampling to give the same weight to both classes. class balanced training sets were obtained by replicating a subset of randomly selected samples from the minority class − min samples from the minority class and obtaining the training set of size  <dig> max). pam and gm-pam were trained on the over-sampled training sets and evaluated on independent test sets. the simulation settings and the settings of pam and gm-pam were the same as presented above .

over-sampling did not significantly change the performance of pam, while it increased the class imbalance bias of gm-pam. after over-sampling pam and gm-pam performed exactly the same, achieving poor predictive accuracy for the minority class when the original training set was class-imbalanced. the number of active variables in gm-pam was much larger than in the simulations where over-sampling was not used. after over-sampling the training set contains exact copies of the minority class samples. when the level of class-imbalance is severe there are many copies of the same minority class sample in the training set and the predictive accuracy for the minority class obtained by using cross-validation is  a re-substitution  estimate, as the same minority class samples can be used in the training and testing phase. the fact that the gm-pam approach favored the use of large number of variables can be explained by realizing that classifiers that use many variables minimize the re-substitution pa   <cit> ; because of the class-imbalance bias the majority class pa is also increasing when using more variables. consequently, the g-means is an increasing function of the the number of variables, and is maximized when the amount of shrinkage is small. the use of large number of variables  translates into a large class-imbalance bias on the independent test set.

we considered also a three class scenario, simulating  <dig>  variables from a multivariate gaussian distribution; the correlation structure was the same as in the two-class scenario. we considered the null case and the alternative case where class  <dig> was the minority class nested between class  <dig> and class  <dig> . the test sets were balanced  and the classifiers were trained and evaluated as described for the two-class scenario. the null case results are in the additional file  <dig>  where additional simulation results with  <dig> variables and balanced data  are also presented.

in the null case the nsc classifiers assigned most new samples to the majority classes and the proportion of samples classified in the minority class decreased when more variables were considered. the gm-nsc classifiers assigned approximately the same number of samples to each class. all the classifiers performed similarly on balanced data, classifying approximately the same number of samples to each class. in the alternative case the nsc classifiers obtained very low pa for the minority class and pam performed worse than alp and ahp . the gm-nsc classifiers performed better, substantially increasing the minority class pa and g-means, in spite of using a larger number of non-informative variables. note that class  <dig> is the hardest class to predict also with balanced data  and that the gm-nsc classifiers achieved approximately the same classification results on balanced and imbalanced data, showing to be insensitive to class imbalance in this setting.

the table reports the same information as table 1; # non-info  was selected out of  <dig>  non-informative variables and #, % info was selected out of  <dig> informative variables; see text for details.

afor ahp and gm-ahp only λθ was optimized while λγ was set to zero.

application to real high-dimensional data sets
we used four breast cancer microarray gene expression data to assess the performance of the gm-nsc and nsc classifiers. we predicted the estrogen receptor  positivity, the histological grade , the disease relapse and the prognosis of the breast cancer patients . grade, prognosis and relapse are harder to predict than er status using breast cancer gene expression data  <cit> . table  <dig> summarizes the main characteristics of the data sets originally published by ivshina et al. <cit> , wang et al. <cit> , sotiriou et al. <cit>  and korkola et al. <cit> , and the classification tasks addressed. we used 5-fold cv to estimate the optimal threshold parameter and, if not noted otherwise, the accuracy measures were estimated using leave-one-out cv .

akmin is a proportion of the minority class samples in the training set.

all the classifiers performed well in predicting the er status on the ivshina data set . ahp used few active genes and had the best performance among the nsc classifiers, while pam and alp did not remove any of the genes nor did they shrink any of the components of the centroids. the gap between the class specific pa was small despite the large class imbalance, because the difference between the classes was large. nevertheless, the gm classifiers used fewer active genes and improved the minority class pa, g-means and auc of pam and alp, and performed similarly to ahp. the absence of shrinkage for pam and alp in the prediction of er status on the ivshina’s data set can be attributed to the large class imbalance  and to the fact that, unless there was very little or no shrinkage, the minority  class had better class specific pa in this data set. the bias towards classification into the majority class, caused by the use of many non informative variables, appeared only when most or all the variables were included in the classifier . the majority class pa and consequently the overall pa were maximized by the classifier with no shrinkage.

the table reports the same information as table 1; # genes is the number of active genes. optimal thresholds were estimated with 5-fold cv and the accuracy measures with loocv; see text for details.

afor ahp and gm-ahp only λθ was optimized while λγ was set to zero.

relapse was more difficult to predict on wang’s data set . pam used the smallest number of active genes and performed slightly better than alp and ahp in terms of class specific pa and g-means. exactly the same results were obtained using nsc and gm-nsc classifiers. this result is in line with the simulations, where we showed that for moderate class-imbalance the performance of nsc and gm-nsc classifiers was very similar.

pam used the largest number of variables on korkola’s data set  but still performed better than ahp and alp that used less variables. the performance of gm-pam and gm-ahp was similar to the original methods, while gm-alp outperformed alp and achieved the best overall performance on this data set.

in order to explore the effect of class-imbalance on real data, we obtained multiple training sets from the sotiriou’s data set, varying the level of class-imbalance. we used a fixed number of samples from the minority class  and varied the number of samples from the majority class ; the samples not included in the training set were used to estimate the accuracy measures. to account for the variability arising from random inclusion of samples in the training or test set, we repeated the procedure  <dig> times and averaged the results.

in the balanced situation the class specific pa were approximately equal for all classifiers, indicating that the classes were roughly equally difficult to predict . for the nsc classifiers the pa of the minority class decreased when majority class samples were added to the training set, while the majority class pa increased, similarly as observed in our simulations. moreover, the number of active genes increased substantially, especially for pam and alp. gm-pam and gm-alp were effective in maintaining the minority class pa above the values achieved on class-balanced data and the gap between the class specific pa was very small even when the class-imbalance was large. the number of active genes increased with class-imbalance for gm-pam and gm-alp but not as dramatically as for the original methods. gm-ahp performed very similarly to ahp. comparable results were obtained for the two-class prediction of grade .

we addressed also a three-class classification problem on ivshina’s data, predicting the grade of tumors . on the complete data set all the classifiers performed very similarly . only gm-pam removed a part of the variables, while the other classifiers did not shrink the centroids at all. grade  <dig> was the majority class but it had the worst pa, indicating that the potentially heterogeneous grade  <dig> class is the most difficult to predict. similarly as on the sotiriou’s data, we varied the number of samples in grade  <dig> class to try to isolate the class-imbalance effect . when grade  <dig> was the majority class the results were very similar to those obtained on the complete data set, and gm-nsc and nsc classifiers performed similarly . in the balanced setting grade  <dig> had the lowest pa, confirming that grade  <dig> was the most difficult class to predict; gm improved the performance of pam and alp.

the table reports the same information as table  <dig>  there were  <dig> grade  <dig> and grade  <dig> samples and the number of grade  <dig> samples varied; see text for more details.

afor ahp and gm-ahp only λθ was optimized while λγ was set to zero.

decreasing the number of grade  <dig> samples had the effect of further decreasing the pa of grade  <dig> and the g-means; the pa of the other two classes, which were high when data were balanced, increased only moderately for most classifiers. the drop in the pa of grade  <dig> was less pronounced for the gm classifiers. similarly as for the other classification tasks the gm method was the most useful in improving the performance of pam and alp.

discussion
in this paper we proposed a modified approach  to the estimation of the amount of centroid shrinkage for the nsc classifiers. the approach estimates the optimal shrinkage by maximizing the geometric mean of the class-specific predictive accuracies, rather than the overall accuracy. we used our approach with pam and with two recently proposed nsc classifiers, alp and ahp.

the motivation for the new approach is to alleviate the class-imbalance problem of the nsc classifiers. we showed with a limited set of simulations that alp and ahp, similarly to pam  <cit> , are biased towards the classification in the majority class when data are class-imbalanced: they assign most new samples to the majority class and achieve poor predictive accuracy for the minority class, unless the differences between the classes are very large. increasing the number of measured variables has the effect of further increasing the bias.

we identified the main reason for the biased nsc classification in the method used in practice for estimating the threshold parameter, which is based on the minimization of the cross-validated overall error rate. the threshold parameter plays a fundamental role for nsc classifiers, as it determines how many variables are effectively used in the classification rule and by which amount the centroids are shrunken.

simulation results and the analysis of three large data sets of breast cancer showed that the greatest gains were obtained by gm-nsc when the nsc classifiers had a large bias towards the majority class, while gm-nsc performed similarly to nsc in the absence of bias. gm-nsc classifiers used less active variables when data were class-imbalanced.

in the biomedical applications the improvements obtained using the gm-nsc classifiers are relevant from the practical point of view. the reduction of the class-imbalance bias results in more accurate prediction for the minority class samples, which are often the samples for which an accurate prediction is more important. moreover, the inclusion of a smaller number of variables in the classifiers seems a desirable property in the biomedical applications where the aim is to develop prognostic or predictive models. many researchers argued that it is advantageous to use microarray-based classifiers that include a small number of genes . the reason is that classifiers that include numerous genes can be more difficult to transfer to the clinical practice because their interpretation and practical implementation is more difficult. at the same it was shown that classifiers that include few genes can perform well in practice  <cit> .

the current implementation of the nsc classifiers does not allow for case weighting so we performed random over-sampling in the attempt to give equal weight to the classes. we observed that random over-sampling had no effect on pam, while it increased the class-imbalance bias of gm-pam, substantially increasing the number of active variables. the reason for poor performance of gm-pam is that the g-means used to determine the optimal threshold is, because of over-sampling, not fully cross-validated estimate as the same minority class samples are used when training and evaluating the classifier. the not properly cross-validated g-means is maximized by classifiers that use large number of variables. although we did not perform the experiments with over-sampling for alp and ahp, we expect that the same conclusion would still apply, as the determination of the optimal threshold would likely suffer from the same problem. in general special care is needed when the tuning parameters are determined after the training set is over-sampled; for example, random forests and support vector machines require the optimization of the tuning parameters, which is normally done with cross-validation.

we chose to select the optimal shrinkage by maximizing the g-means of the classifiers, which is more appropriate than overall error for the assessment of the effectiveness of the classifiers trained on imbalanced data  <cit> . other assessment measures were proposed for class-imbalanced data: two popular alternatives in the two-class problems are the f-measure and the area under the roc curve , however their generalization to multi-class problems is not as straightforward as for g-means. the f-measure is a function of predictive accuracy and predictive value of the positive class, the weight given to each measure depends on a parameter that is chosen by the user. being a function of the predictive values it is sensitive to data distributions, which is not a desirable property when data are class-imbalanced. auc depends on the class-specific predictive accuracies, similarly to g-means. a possible advantage of g-means over auc is its behavior when evaluating uninformative classifiers =1|y=1)=p=1|y=2) for two classes). in this case g-means favors the classifiers that assign the same number of samples to each class, while auc is approximately the same for all uninformative classifiers; as a consequence the estimation of the threshold parameter using auc is very unstable when the differences between the classes are small. we considered also the maximization of the sum of the class-specific predictive accuracies , however this measure has a similar drawback as auc as it can not distinguish between the uninformative classifiers. experimental results for pam showed that under the null hypothesis  this approach performed slightly worse than using g-means to determine the optimal threshold, while the results were very similar when the difference between the classes was large .

tibshirani et al. <cit>  proposed the procedure for adaptive choice of threshold for pam that enables different shrinkage for each class and showed that this approach can lead to smaller number of active variables. however, wang and zhu  <cit>  observed that using the adaptive threshold procedure does not change the predictive accuracy of pam. we obtained similar results and observed that while in the multi-class classification problems sometimes the number of active variables decreases, the predictive accuracy of pam and gm-pam is not affected . therefore, the adaptive choice of threshold does not seem beneficial in decreasing the class-imbalance problem of the nsc classifiers.

in this paper we focused on pam, alp and ahp; others proposed further modifications to nsc methods  <cit> , which were not evaluated in this study. however, we believe that all the classifiers that base their tuning on the minimization of overall error should present the same type of problems, and would benefit from using a tuning strategy based on g-means or other cost functions that are less sensitive to the class-imbalance problem.

huang et al. <cit>  observed that the estimators of the discriminant scores for discriminant analysis are biased and derived a bias-corrected discriminant score for dlda and dqda. their findings are interesting in the context of class-imbalanced high-dimensional classification, as they show that the bias of the discriminant scores depends on the class-imbalance in the training set and on the number of variables; the bias is larger in the minority class and when more variables are considered. the bias-correction outperforms the original approach, especially when the class-imbalance is large; unfortunately this approach can not be extended straightforwardly to nsc classifiers as the distribution of the estimator of the shrunken class centroid is not known.

a computational issue is the estimation of the optimal threshold. we used an approach similar to what was used for pam  <cit> , evaluating a fixed number of threshold values and using cross-validation. in most situations we evaluated  <dig> threshold values, equally spaced between  <dig>  and the minimum threshold value that shrunk all the class centroids to the overall centroid . this choice was a compromise between accuracy of estimation and computational burden, which was particularly high for ahp. however, we observed that this strategy might not be optimal in all situations since, especially for ahp, the relationship between the threshold and the number of active variables was highly nonlinear. often the smallest positive threshold produced few active variables: the estimated number of active variables could be either very small or equal to the total number of variables, while intermediate solutions were not evaluated. this could explain why in some applications gm was not successful in improving the performance of ahp. our observations would suggest that equally spacing the threshold values could be an effective choice when the number of variables distinguishing the classes is relatively small, as it is often the case for microarray data. in problems where many variables distinguish the classes a solution would be to equally space threshold values on the logarithmic scale, which would include more thresholds associated with a large number of active genes.

we presented the gm-ncs approach limited to the case where each class is given equal importance, but the method can be extended and incorporate different misclassification costs for each class by weighting the class-specific predictive accuracies. this approach would be useful for problems where the cost of misclassification is not equal for each class.

CONCLUSIONS
we showed that three nearest shrunken centroid classifiers  achieve poor accuracy for the minority class when data are class-imbalanced and high-dimensional, unless the difference between classes is large. we proposed gm-nsc, a straightforward yet effective approach to diminish the class-imbalance problem of nsc classifiers, which consists in estimating the optimal amount of shrinkage by maximizing the g-means of the classifiers, rather than its overall accuracy. we used simulated and real data to show that when the ncs classifiers are biased towards the majority class the gm-nsc approach outperforms nsc, and it performs similarly to nsc otherwise. gm-nsc classifiers generally select less variables which seems a desirable property in the biomedical applications where the aim is to develop prognostic or predictive models.

our experiments with random over-sampling showed no improvement for pam while the class-imbalance bias of gm-pam was increased. we therefore recommend that this strategy is not used with the nsc or gm-nsc classifiers.

abbreviations
nsc: nearest shrunken centroid; pam: prediction for microarrays; dlda: diagonal linear discriminant analysis; alp-nsc: adaptive l∞ norm penalized nsc; ahp-nsc: adaptive hierarchically penalized nsc; pa: predictive accuracy; auc: area under the roc curve.

competing interests
both authors declare that they have no competing interests.

authors’ contributions
rb derived the gm approach, performed the computations and wrote the manuscript; ll designed research and wrote the manuscript. both authors read and approved the final manuscript.

supplementary material
additional file 1
derivation of the expressions for λmax. in the additional information we derive the expressions for λmax for the classifiers considered in the paper. see text for more details.

click here for file

 additional file 2
classification error and the probability of classification in class  <dig>  in the additional file we derive the classification error as a function of a probability of classification in class k and class-prior probabilities. we also derive the probability of classification in class  <dig> for the example presented in the main text additionally assuming that the pooled variances are known. see text for more details.

click here for file

 additional file 3
classification results under the null hypothesis for a large number of variables  and the correlated scenario . in the additional file we show predictive accuracy for class  <dig>  and pa for class  <dig>  for different levels of class-imbalance  in the training set containing  <dig> samples. there was no difference between the classes .

click here for file

 additional file 4
classification results for a large number of variables. in the additional file we show optimal threshold parameter , number of active irrelevant variables , pa for class  <dig>  and pa for class  <dig> , g-means and auc for different levels of class-imbalance  in the training set containing  <dig> samples for a situation where there was no difference between the classes , where the difference between the classes was small  and where the difference between the classes was moderate .

click here for file

 additional file 5
classification results under the null hypothesis for a small number of variables. the additional file shows pa for class  <dig>  and pa for class  <dig>  for different levels of class-imbalance  in the training set containing  <dig> samples. there was no difference between the classes .

click here for file

 additional file 6
classification results under the alternative hypothesis for a small number of variables. the additional file shows pa for class  <dig>  and pa for class  <dig>  for different levels of class-imbalance  in the training set containing  <dig> samples. the difference between the classes was moderate  and  <dig> variables were differentially expressed .

click here for file

 additional file 7
classification results for a small number of variables. in the additional file we show the optimal threshold parameter , number of active irrelevant variables , pa for class  <dig>  and pa for class  <dig> , g-means and auc for different levels of class-imbalance  in the training set containing  <dig> samples. there was no difference between the classes  and the differences between the classes were moderate ; table 2).

click here for file

 additional file 8
classification results for the over-sampled training set for pam and gm-pam. in the additional file we show the simulation results obtained by training pam and gm-pam on over-sampled training sets obtained by replicating randomly selected samples from the minority class in order to obtain the class-balanced training set. simulation settings were the same as presented in the additional file  <dig>  see text for more details.

click here for file

 additional file 9
classification results for the three class scenario. in the additional file we show the same information as in additional file  <dig> for the three class scenario. in tables  <dig> and  <dig> we report results for the the class-balanced scenario  and a large number of variables  under the null and the alternative hypothesis, respectively. table  <dig> reports the results for the class-imbalanced scenario  under the null hypothesis and table  <dig> reports the result under the alternative hypothesis. results for smaller number of variables are in table  <dig> 

click here for file

 additional file 10
error rate, class specific predictive accuracies and g-means as a function of the threshold parameter for the ivshina’s data set and prediction of er. in the additional file we report the error rate , accuracy for er- class , accuracy for er+ class  and g-means  for different values of the threshold parameter  obtained on the ivshina’s data set. see text for more details.

click here for file

 additional file 11
classification results for the gene expression data sets. results on the sotiriou  data set for classification of er and grade of the tumor  and on the ivshina data set for the multi class classification task .

click here for file

 acknowledgements
the high-performance computation facilities were kindly provided by bioinformatics and genomics unit at department of molecular biotechnology and heath sciences, university of torino, italy.
