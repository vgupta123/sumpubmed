BACKGROUND
dna sequence comparison is a common problem in biology. in this problem, we wish to measure the similarity of two sequences of dna. hamming distance  <cit>  can be used to quantify similarity but forces the two sequences to be of the same length. more generally, the idea of a weighted edit distance can be applied, which allows for base changes, insertions and deletions  <cit> , with weights chosen to reflect their likelihood of occurrence. given some set of operators that can modify a sequence, we wish to find the set of edit operators that transforms one sequence into a sequence of the other by maximizing a similarity score. this problem can be solved by a dynamic programming algorithm, which was first described in  <dig>  <cit> . this led to the smith-waterman algorithm  <cit>  that has been a critical component of local sequence alignment. affine gap penalties were subsequently introduced, whereby in practice the per-base average penalty decreases, but the overall penalty increases with longer length <cit> . this algorithm has a known o running time and o) space requirements, for both finding a maximum similarity score and finding a transformation that achieves the maximum similarity score, where n and m are the lengths of the two sequences to be compared  <cit> . the resulting algorithm has become the standard for dna sequence comparison  <cit> .

sequence comparison has an important application to re-sequencing, whereby a dna sequence that is observed may differ from a reference due to biological events or measurement errors. we wish to find the maximum similarity score between the observed sequence and a substring of the reference sequence. this is referred to as local sequence alignment and is typically a final finishing step in a two-stage search process found in many current sequence alignment tools  <cit>   that support alignment of a short sequence to an entire genome. among the 'next-generation' dna sequencing technologies that produce millions to billions of short sequence reads, there is one  that does not observe each dna base  individually, but measures successive sequential pairs, with the  <dig> possibilities encoded degenerately in groups of four, using four "color" codes . the resulting two-base encoded form of data is referred to as color space sequence data, to distinguish this from the decoded base space sequence data <cit> . for example, a 50-base dna sequence would be encoded as  <dig> sequential two-base measurements, each of which is one of four states . given the first base of the sequence as a boundary condition , the chosen encoding allows for the bases to be sequentially decoded, moving from first to last, in a fully deterministic manner. while the actual two-base encoding used has a number of interesting and useful algebraic properties  <cit> , the most important properties are that a single base change to the dna base sequence results in two adjacent color changes in the color space sequence, and that an isolated error in color space will cause all subsequent bases to be altered in the decoding. the result is that isolated measurement errors and real variants have distinguishable signatures that in principle provide some ability to perform error detection and correction. in particular, two specific adjacent measurement errors are required to produce a single base change error in the decoded sequence, so that the base calling error rate could be reduced to the square of the intrinsic measurement error rate , if the encoding properties can be fully exploited when comparing the color space reads to a reference dna sequence.

in a typical re-sequencing experiment using next-generation sequencing technology, millions of short sequence "reads", 20– <dig> bases in length, must be aligned to a large reference genome, such as the human genome. this demands an initial search space reduction step  <cit>   prior to performing the more expensive optimal local alignment. this first step typically involves some form of indexed look-up or hashing of the full genome or reads, so that a small number of candidate alignment locations are quickly obtained for each read, in a way that is tolerant of the read containing errors or real variants relative to the reference. the optimal local alignments are then used to select which of these candidates is the true location, as well as to identify the differences from the reference sequence at that location. in the case of color space data, the look-up phase can be performed entirely in color space, using the color-space encoded form of the reference genome to find candidate locations for each color space read. the optimal alignment algorithm described here would then be used as the finishing step, which simultaneously decodes, identifies color  errors, and optimally aligns resulting dna sequence to a short candidate segment of the reference sequence, typically 100– <dig> bases in length .

RESULTS
power of two-base encoding
we performed simulations to evaluate the power of our proposed algorithm to align sequences with two-base encoding compared to the local alignment without two-base encoding . we model errors as base substitutions when the sequence is not encoded and model errors as color substitutions  when the sequence is encoded in color space. in figure  <dig>  we demonstrate that for sequences with increasing error rates, aligning with two-base encoding is nearly equal to  or more powerful than  aligning without two-base encoding. nevertheless, if we examine base substitutions in the presence of error , the current algorithm is unable to properly align sequences with an increasing number of base substitutions in the presence of a small number of random errors. the scenario where there are many base substitutions that are not errors  is rare, especially in the human genome <cit> , and therefore this behavior is tolerable. in figures  <dig> and  <dig> we see the power to detect deletions and insertions with an increasing number of errors. for a contiguous deletion the power to align such sequences is equal or greater with two-base encoding, except in the case of a one base deletion with no errors where the power is slightly reduced. for a contiguous insertion, the case is more ambiguous. as expected with greater error , the two-base encoding becomes more powerful. nevertheless, for a small amount of error, the two-base encoding has lower power to align longer contiguous insertions. in this case, over-correction can occur, whereby we align with too many color substitutions rather than the contiguous insertion. this may be mediated by decreasing the penalty for extending an insertion or deletion, although this may reduce the accuracy for high-error sequences without insertions or deletions.

performance of two-base encoding
we performed simulations to evaluate the performance of the current algorithm compared to the local alignment without two-base encoding . we found that for length  <dig> and  <dig> color space sequences our algorithm was  <dig> and  <dig> times slower, respectively, than the standard dynamic programming algorithm applied to base space sequence. although the algorithmic complexity as a function of read length and reference length is not increased, the absolute number of operations does increase , and thus we observe a decrease in the speed performance compared to sequences without the two-base encoding. this performance decrease is particularly relevant given that an experimentalist may be required to choose between competing sequencing technologies that do not utilize the two-base encoding scheme and sequencing technologies that do use the two-base encoding scheme. two base encoding has potentially powerful error correction modes and at the time of this publication is able to generate substantially more data than direct sequencing approaches. thus, the two base encoding strategy while preferable in some scenarios for base error correction and better performance of alignment does impose a need for increased computational capacity largely due to the local sequence alignment complexity.

discussion
although the power of this algorithm enables accurate alignment of color space sequences with greater error, it is also computationally an order of magnitude more expensive than the standard dynamic programming algorithm applied in sequence space. to partially mitigate this, the performance can be optimized without changing the results by employing some simple search space reduction and greedy search techniques, as follows: first, decode the encoded sequence by the standard deterministic rules and perform an exact string matching search. if an exact match is found, then the algorithm stops. upon unsuccessful return, we find a lower bound for the optimal similarity for the proposed algorithm by first performing our two-base encoded alignment but without allowing insertion or deletion edits, which substantially reduces the computational cost. using this lower bound, we then reduce the search space of our full algorithm by omitting the paths where the search parameters that permit detection of insertions or deletions would result in a score below the established lower bound. in this manner, the empirical running time of the algorithm can be improved by approximately 20%  while still obtaining the true optimal alignment.

we note that the general strategy of two-base encoding in color space is possible to apply in more complex formats for error correction. for instance, three or more bases may be encoded by four or more colors. this would further increase the power of discriminating between encoding errors and base substitutions, albeit at a substantial added cost in local alignment performance. in practice these alternate encodings could further reduce false-positives detections when the goal is to find biological variants with next-generation sequencing technology with relatively high measurement error rates. this may be an advantageous strategy, for example, to increase read lengths by accepting noisier color space reads that are correctable after alignment. the current algorithm can be extended to accommodate these generalizations, and in future work we will investigate the detailed performance properties of such hypothetical encodings.

the present algorithm can be readily extended to include support for the case where sequence data is missing or unavailable, in either the given color-encoded sequence or in the target base space sequence. we introduce a fifth color code to represent an unknown color in encoded sequence, and a fifth base code  to represent an unknown base in the decoded or target sequence. to incorporate an unknown encoding color we modify the color substitution function Π to include a score for this fifth unknown color and any other color. to incorporate an unknown base in the target, we modify the base substitution function Δ to include a score for the unknown base and any other base. also a simple modification to the initialization step in the algorithm is required if the start base p is not known. while we do not rely on quality values for each color read, however it is possible to incorporate into the current alignment algorithm quality values that represent the certainty of color calling similar to sequence calling with phred scores  <cit>  by weighting the color substitution function Π.

finally, figures  <dig>   <dig>   <dig>  and  <dig> demonstrate the power to correctly align two-base encoded sequences in the presence of a large number of color errors. depending on the distribution of sequences with a given number of errors, two-base encoding and this algorithm may make it feasible to accept higher error sequences generated by next-generation sequencing technology, improving both throughput and cost-effectiveness. additionally, we place a constraint on our scoring functions, making a conscious choice to prefer a base substitution to two adjacent color substitutions that would cause that base to match the reference. this is by no means the only constraint available, but serves to help define the trade-off in power to detect errors over biological variants. in these practically important but ambiguous cases, a decision must be made over which scenario to prefer, and in practice this ambiguity can be overcome by using coverage where multiple sequences observe the same event.

CONCLUSIONS
dna sequence alignment algorithms have been thoroughly studied in molecular biology, resulting in well-developed dynamic programming algorithms that optimize an edit distance to find optimal alignments between two sequences. however, there is a resurgence of interest in sequence alignment due to large scale re-sequencing efforts made possible by massively parallel sequencing technology. the classical algorithm remains an ideal approach for local alignment of such short-read sequence data, but some sequencing technologies produce reads in encoded form, which must be decoded to obtain standard dna sequence. we extend the previous class of dynamic programming algorithms to allow for errors in the encoding, as well as the usual base substitutions, insertions and deletions. our algorithm remains o time, where n and m are the length of the encoded and target sequence respectively. we show in practice that performance is decreased due to the added complexity of considering encoding errors, although this can be somewhat mitigated by standard search optimization. this performance decrease must be kept in mind when comparing the overall computational cost of analyzing various next-generation sequencing technologies. using this new algorithm, local sequence alignment as well as error detection and correction are performed in a reliable and systematic manner, enabling the direct comparison of encoded dna sequence reads to a candidate reference dna sequence. this new algorithm should facilitate the use of two-base encoded data for large-scale re-sequencing projects.

