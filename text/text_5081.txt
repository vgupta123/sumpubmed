BACKGROUND
a crucial issue in microarray studies is the elucidation of how genes change expression and interact as a consequence of external/internal stimuli such as drug assumption, hormone stimulation, illness, etc. given a system whose elements regulate each other, inference of the regulatory network from the observed dynamics of the system is denoted as reverse engineering. several approaches are available in the literature; among them boolean models  <cit> , models based on differential equations  <cit> , bayesian networks  <cit>  and methods based on measurement of pair-wise gene expression correlation  <cit> . application of reverse engineering to real data suffers from some drawbacks. first, although the regulatory network controlling gene expression involves rnas, regulatory regions on dna, proteins and metabolites, usually only gene expression data from microarray experiments are available and used as a proxy of protein activity. therefore, gene-gene interactions identified using reverse engineering methods from microarray data are not, in general, direct regulatory actions or physical interactions, but functional relations  <cit> . second, experiments are often characterized by a very limited number of samples with respect to number of analyzed genes . this e.g. renders difficult to estimate with sufficient accuracy model parameters describing differential equations or, in case of bayesian networks, conditional probability distributions. application of reverse engineering thus results in an exceedingly large number of false positive interactions. in this situation, a more realistic objective is to focus on groups of genes whose expression profiles are linked to each other by a set of cause-effect relationships, rather than reconstructing the entire regulatory network underlying gene behaviour. to this purpose, the use of discrete approaches offers some advantages, since it may reduce the probability of finding random associations between genes and limits the dimensionality of the problem and thus the computational time needed to search the space of possible relationships between genes. however, discrete data offer a simplified representation of reality even if, as pointed out by shmulevich et al.  <cit> , there is evidence that meaningful biological information can be extracted from discrete gene expression data.

here we explicitly address short time series microarray experiments and explore the use of a discrete approach to identify gene relationships. a quantization method is presented, using a threshold which is optimized based on a model of the experimental error and on a compromise between false positive and false negative classifications. two standard quantization methods are also considered based, respectively, on a model of the experimental error but with a threshold corresponding to an arbitrary 5% significance level, and on rank sorting. the three methods are evaluated based on their ability to identify relations among genes when used as preliminary step to two discrete reverse engineering methods: reveal  <cit>  and dynamic bayesian networks   <cit> . the analysis is performed on synthetic data generated as continuous profiles from simulated regulatory networks consisting of different sub-networks with random scale-free topology. the ability to identify relations among genes belonging to the simulated sub-networks is used to quantify the performance of the methods.

methods
quantization
expression of gene x at time t is quantized in three levels , representing "underexpressed", "not differentially expressed" or "overexpressed" values with respect to baseline, according to the following rules:

x is quantized as + <dig> if it exceeds its basal value xb by at least θ

 - xb) > θ ⇒ x = + <dig>     

as - <dig> if xb exceeds x by at least θ:

 - xb) < -θ ⇒ x = - <dig>     

as  <dig> if x differs from xb for less than θ:

|x - xb| ≤ θ ⇒ x =  <dig>     

the novelty of the method is how the threshold θ is fixed from the distribution of the error, on the basis of a compromise between false positives  and false negative  classifications. more in details, to derive θ a model is required for  - xb) distribution, under the null hypothesis that x and xb are two realizations of the same variable. when dealing with real data, experimental replicates can be used to derive the null hypothesis distribution . θ is then evaluated according to a significance level α, but, rather than fixing it a priori, α is optimized so as to compromise between false positive and false negative classifications. the expected number of false positives  is approximated as the product of α by the number n <dig> of not differentially expressed genes:

fp = n <dig> ·α     

the expected number of true negatives  is then:

tn = n <dig> - fp = n <dig> ·     

hence, the expected number of false negatives  is derived by subtracting tn  and the sum of true positives and false positives  from the total number of genes n:

fn = n - tn -  = n - n <dig> · - sα     

a compromise between fp  and fn  is achieved if the following condition holds:

fn = fp ⇔ n - n <dig> · - sα = n <dig> ·α ⇔ n - n <dig> = sα     

n <dig> is unknown and is estimated using the bootstrap based procedure described in  <cit> . sα is evaluated using equations  for different values of α, and α that guarantees sα = n - n <dig> is then selected.

dynamic bayesian networks and reveal
the performance of the quantization method is assessed on synthetic data, with two discrete reverse engineering algorithms: dynamic bayesian networks   <cit>  and reveal  <cit> . the two algorithms are implemented using a three steps procedure: 1) clustering of identical discrete profiles; 2) search for causal relationships using reverse engineering algorithms; 3) sub-network identification.

step1: clustering
genes with identical discrete profile are grouped together since identical profiles give the same information content. this step is useful to reduce the computational time since a smaller number of expression patterns are considered, thus facilitating the search through the whole space of potential gene interactions. flat profiles are excluded from the analysis, since they are not involved in the observed process in terms of changes in the transcription level.

step2: reverse engineering
dynamic bayesian networks
bayesian networks are directed acyclic graphs that encode a series of relations of conditional dependence among interacting variables. in the case of gene networks nodes represent the genes and edges represent the relations of conditional dependence among genes. the aim of the learning procedure is to find the network structure g that is most supported by the data d, i.e. that maximize the posterior probability p. bayesian networks do not allow cycles in their topology; therefore, it is not possible to represent feedback control which is actually a critical aspect of gene regulation in real biological systems. to include cycles and feedback control in the regulatory network, dynamic bayesian networks are used, as implemented in the software developed by kevin murphy .

reveal
in its original formulation, reveal uses a boolean model of the regulation and searches for minimal set of input-genes that can univocally explain the behaviour of the output-gene x from 0– <dig> discretized data. to explicit possible causative relationships, the algorithm uses the entropy and mutual information score  <cit>  and searches, for each gene x, all the possible interactions of connectivity k =  <dig>  if no genes univocally determines x profile, it searches for all the possible interactions of connectivity k =  <dig>  and if even this search is unsuccessful, it searches in the space of interactions of connectivity k =  <dig>  reveal stops at connectivity k =  <dig> for two reasons: first, because search in higher connectivity space is computationally unaffordable; second, when k increases, the disproportion between the number of analyzed genes and the number of available samples causes a more elevated number of false positive discovered relationships  <cit> . we extended reveal to data quantized in three levels, with either an instantaneous model of regulation  regulates x at time t) or a synchronous one  regulates x at time t + Δt).

step 3: sub-networks identification
dbn gives as output a network structure g codified in a connectivity matrix with a non null entry at ith row and jth column representing the relationship found by the algorithm between gene profile j and gene profile i. from the connectivity matrix a network is drawn with genes represented by nodes and regulation by edges, and searched for sub-networks non connected to the remaining of the network. reveal gives as output a list of input-genes  for each "regulated" output-gene. a connectivity matrix is derived from the lists of regulators and sub-networks are identified as for dbn.

simulated data
we have developed a simulation tool, able to model concurrent regulation, i.e. a gene affects regulation depending on its interactions with other genes. synthetic data are generated by simulation of regulatory networks of random scale-free topology, using differential equations in which the rate of change of gene expression is a function of a combination of different regulatory rules.

network topology
each simulated network consists of h sub-networks. sub-networks are generated by randomly assigning regulators to each gene, according to a scale-free structure: the probability for each node of having a number of connections with other nodes equal to h is h-γ . the nodes with the highest number of connections are called hubs  <cit> . sub-networks are connected to each other through nodes randomly selected among the non hub genes. this strategy gives a scale-invariant characteristic to the simulated network  <cit> .

regulation rules
for a generic gene x, its r regulator genes  <dig> ...,r with expression level yit,  at time t, act in concurrency by activating or inhibiting transcription as results of a combination of different rules. the rate of change of gene x expression at time t, depends on the value of the regulatory function rx:



where mlx is a positive constant, representing the maximum achievable expression value of gene x. the regulatory function rx is a combination of three basic regulatory actions:

 min/τ processes a regulatory effect achieved only if all the regulators are simultaneously active

 sum/τ processes a regulatory action which can be alternatively performed by different regulators

 the minus sign processes a negative regulation 

where τ is a time constant and weights wxi represent the strength of regulatory action performed by each regulator on the regulated gene x. complex regulatory actions are obtained by combining the functions described above, e.g. a regulatory function rx with five regulators could be rx = min - sum). the combination of rules for the regulators and weights wxi are randomly chosen, with the only constraint that each gene has at least one activator and one inhibitor.

in order to test the performance of the quantization method,  <dig> different networks were simulated, each consisting of h =  <dig> sub-networks and  <dig> genes. different topologies and regulatory rules were randomly chosen for each simulation; mlx was set equal to  <dig> for each gene x. to reproduce the data-poor conditions, ten samples were collected from each data set; gaussian noise with constant standard deviation sd =  <dig>  was added to samples ranging from  <dig> to  <dig> 

simulated data were quantized in three levels using three different methods: a) the new method based on a model of the experimental error with significance level tuned to reach a compromise between fp and fn classification; b) same as a) but with an arbitrary 5% significance level; c) quantiles based quantization as in  <cit> , i.e. the lowest  <dig> % of the values  were quantized as - <dig>  the next highest  <dig> %  as  <dig>  and the highest  <dig> % as + <dig> 

both dbn and reveal  were applied to discrete data.

scoring
to assess the performance of the quantization method used in conjunction with the two reverse engineering methods, genes in the identified sub-networks are compared to those in the simulated sub-networks. precision  and recall  are used at this purpose. more precisely, for each simulated sub-network simh  and identified sub-network idd , precision is defined as:



and recall as:



in order to quantify the ability to identify sub-networks with all or most genes belonging to a single  sub-network, the maximum precision across simulated sub-networks simh  is considered for each identified sub-networks idd. the corresponding recall is also considered, thus obtaining d pairs of scores  for each simulated data set.

RESULTS
quantization methods a) and b) require a model of the distribution of the differences between two expression values under the null hypothesis. since the error is fixed in all simulations as gaussian with zero mean and a constant sd =  <dig> , this distribution is gaussian with zero mean and a constant sd =  <dig> * . based on this model, the average threshold θ obtained for method a) in  <dig> simulations equals  <dig>  with sd =  <dig> . θ in fact varies among  <dig> simulations, since the significance level α, fixed on the basis of a compromise between fp and fn classifications, depends on n <dig>  i.e. the estimated number of samples in the data set that do not change expression with respect to their baseline value . the high coefficient of variation of θ indicates that n <dig> strongly depends on the observed dynamics and, thus, on simulated network topology, regulatory rules and initial conditions. conversely, using quantization method b) the threshold θ is equal to  <dig>  for all simulations, according to 5% significance level. for method c) the two thresholds θ <dig> and θ <dig> vary among simulations and equal respectively - <dig>  with sd =  <dig>  and  <dig>  with sd =  <dig> . performance of the three methods used with reveal and dbn are shown in figure  <dig> as average precision at different ranges of recall intensities , using reveal  and dbn . for method a) the area under the curve is  <dig>  using reveal and  <dig>  using dbn; for method b) it is  <dig>  using both reveal and dbn; for method c) it is  <dig>  using reveal and  <dig>  using dbn. these results show that the trade-off between precision and recall improves using method a). in particular, for recalls higher than 40%, precision obtained using the proposed quantization method a) is consistently higher than that obtained using other methods. also of interest, the overall performance of reveal is similar to dbn, in the considered data-poor condition.

discussion
a data quantization approach usable with discrete reverse engineering methods has been proposed, which is based on a model of experimental error  and on a compromise between fp and fn classification. modelling experimental error is a fundamental step since it allows to quantify the error and to assess its distribution. this is particularly important e.g. with affymetrix chips, since the measurement error is dependent on the expression intensity  <cit> . in this case, the threshold θ in equations , has to be intensity dependent so as to penalize genes expressed at low intensity levels  with respect to genes expressed at high intensity levels   <cit> . the quantization method here presented, besides exploiting information on the experimental error, derives θ on the basis of the variability of the data-set to be discretized. in fact θ corresponds to a significance level α chosen so as to compromise between fp and fn classifications, where fp and fn are estimated on data, based on the number n <dig> of samples that do not change expression with respect to their baseline value . the other two quantization methods we have considered are based respectively on a model of the experimental error, but with a threshold corresponding to a 5% significance level, and on rank sorting. the first takes into account the experimental error, but with an arbitrary threshold level independent from the data-set variability, the second does not exploit information on experimental error, but, by using ranking, takes somehow into account the data-set dispersion.

to quantify the performance of the quantization methods when used with reverse engineering, synthetic gene expression profiles were generated from completely connected scale-free networks of  <dig> genes;  <dig> time samples were collected from each gene profile to reproduce the data-poor situation. application on  <dig> synthetic data sets indicated that: 1) quantization based on compromise between fp and fn classifications improves the algorithm performance; 2) reveal and dbn perform similarly. figure  <dig> shows the trade off between precision and recall for the identified sub-networks in the  <dig> simulations. it is of interest to concentrate on precision, which is related to the false positive rate in predicted sub-networks. precision can be improved by focusing on genes that could be central in the regulation . to this purpose, each identified sub-network was searched in order to rank nodes on the basis of the degree of connectivity . the gene with highest degree was ranked first; other nodes were ordered depending on their degree of connectivity, with the constraint of having at least a direct connection with the genes previously ranked. figure  <dig> shows the average per cent improvement in precision  obtained by applying the ranking step to reveal results .

improvement in precision depends on the percentage of genes considered from the ranking. it is above 50% when less than 40% of the ranked genes are considered and still reasonable, above 25%, with less than 60% of the genes. precision thus substantially improves with ranking, but recall obviously deteriorates. back to results of figure  <dig>  if recall is higher than 40%, precision ranges between 40% and 60%. this limited range of precision may have several explanations. first, the model implemented in reveal and dbn to find cause-effect relationships is much simpler than the model used to generate simulated data and may not distinguish behaviours of different complexity. the simulation model is in fact based on differential equations in which the rate of change of gene expression is a function of a combination of different positive/negative regulatory actions, e.g. achieved only if all the regulator are active simultaneously or alternatively performed by different regulators. in this sense the simulation model used in this paper combines characteristics of models based on differential equations  <cit>  as well as on boolean networks  <cit> . boolean networks describe important aspects of gene regulation such as complex concurrent regulatory mechanisms, but do not describe continuous changes in gene expression. in contrast, differential equation based models generate continuous data and allow to include the processes of transcription and mrna degradation, but, in general, do not address regulatory logic more complex than additive or multiplicative effects. the strategy we adopted combines the major advantages of the two approaches.

a second source of fp may arise from the use of discrete data, which is a simplified representation of gene expression. to assess to which extent the use of discrete rather than continuous data is critical, we compared our results on simulated data with those obtained by applying a continuous reverse engineering method such as aracne  <cit> . aracne uses an extension of mutual information to continuous data  <cit>  and, as other continuous methods, explicitly requires hundreds of data points to perform the analysis to a sufficient degree of accuracy and was not proposed to address sparse datasets. however, at variance with other continuous methods, it does not require model parameter identification and is computationally affordable. therefore, we explored its use on simulated data sets for sake of comparing discrete vs continuous approaches in data-poor conditions. when aracne was used to identify subnetworks on simulated data , it always identified a single sub-network with recall ranging from  <dig>  to  <dig> and precision always equal to  <dig> , thus indicating random results; when aracne was used to reconstruct the entire regulatory network, recall and precision ranged between  <dig> – <dig>  and 0– <dig> , respectively. these results confirm that the use of discrete rather than continuous data is advantageous when few samples are available. continuous approaches are likely to become advantageous with increasing number of samples.

CONCLUSIONS
a new method was presented to quantize data in a statistically robust way, which can be used as a preliminary step to discrete reverse engineering algorithms. the performance of the method was tested with two basic discrete reverse engineering methods: reveal and dynamic bayesian networks, using continuous synthetic data generated by a simulation of regulatory networks of random scale-free topology. the simulation model generates continuous data using differential equations and uses an extension of boolean logic to continuous data to mimic regulatory programs. the new quantization method improves precision and recall trade-off, both with reveal and dbn. reveal and dbn perform similarly on simulated data.

authors' contributions
bdc and fsc conceived the study and performed data analysis under the guidance and supervision of gt. bdc designed and implemented the current version of the algorithm and of the simulation model here presented. skn, zt and cc were responsible for the overall conception and project coordination. all authors read and approved the final manuscript.

appendix
by assuming a log-additive error model as in  <cit> , the log-expression of a generic gene x in replicates a and b, can be expressed as:



where μ represents the actual  gene expression and εa, εb two realizations of the error. to quantify the difference between two expression values under the null assumption, a variable δ is defined as:

δ = log - log = εa - εb     

different distribution models  can be used to fit the entire set of δ values obtained by applying equation  to all genes and available replicates. once the best model is selected based on a number of criteria, θ is evaluated from the distribution of δ according to a significance level α.

