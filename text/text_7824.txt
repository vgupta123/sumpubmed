BACKGROUND
the integrated analysis of multiple data sets obtained for the same biological entities under study, has become one of the major challenges for data analysis in bioinformatics and computational biology. two main causes for this trend are the availability of complementary measurement platforms and the systemic approach to biology; in both cases, multiple data sets are obtained on the same set of samples . first, examples where several measurement platforms are included are the study of the metabolome composition of escherichia coli  using several analytical chemical methods to screen for metabolites  <cit>  and the combination of cdna and affymetrix chips applied to sixty cancer cell lines  <cit> . in both examples, there is overlap in the metabolites or genes screened but also complementarity. second, the modern systemic approach to biology leads to a probing of the biological system on different levels in the cellular organization, such as for example the transcript, protein, and metabolite level  <cit> . these approaches lead to situations where several data blocks are obtained that are coupled in the sense that they were obtained for the same set of samples. a key issue in integrative data analysis is to analyze such data simultaneously instead of separately or sequentially as this yields an aggregated view. in this respect, simultaneous component methods, that are an extension of principal component analysis  to the case of multiple coupled data blocks, were proposed and successfully used  <cit> .

however, a drawback of component based methods like pca is their lack of sparseness: processes underlying the data are revealed by a weighted combination of all variables . from an interpretational point of view, this is not very attractive and it also does not reflect that biological processes are expected to be governed by a limited number of genes  <cit> . the problem holds even more for simultaneous component methods as these involve multiple large sets of variables. to deal with this issue, sparse approaches have been proposed mainly within the context of regression analysis  but also for principal component analysis  <cit> : these select a limited number of variables by shrinking many of the weights to zero which is accomplished by proper penalization of these  weights. a favorable characteristic of such penalty based methods is that the selection is built-in . here, we extent sparse pca to sparse simultaneous component methods, accounting for the fact that the data are structured in several data blocks holding both shared and complementary information. the estimation procedure used is efficient and the associated matlab code can be found in the additional file.

first, we present the sparse simultaneous component model, starting from ordinary principal component analysis and sparse pca. a generic modeling framework is introduced that incorporates several types of penalties. then we present some results for metabolomics data obtained with two measurement platforms for the same set of e. coli samples and we validate the method by means of simulated data.

RESULTS
algorithm
notation
we will make use of the following formal notation: matrices are denoted by bold uppercases, vectors by bold lower case, the transpose by the superscript t, and the cardinality by the capital of the letter used to run the index , see  <cit> .

throughout the paper, we suppose that all variables are mean-centered and scaled to norm one.

model
simultaneous component analysis is an extension of principal component analysis  to the case of multiple coupled data matrices. consider the pca of a single data block xk containing the scores of ik objects  on jk variables . in a first model formulation  <cit>  based on component scores, pca decomposes the data as follows,

  xk=tkpkt+ek 

with tk the component scores of the ik objects on the r components, pk  the loadings, and ek  the matrix of residuals. to identify the model, usually the constraints are imposed that the axes have a principal axes orientation and that the component scores are orthogonal: tkttk=i. another formulation of the pca model is based on component weights as follows  <cit> ,

  xk=xkwkpkt+ek 

with wk  the component weights. note that we can write tk = xkwk resulting in the equivalence of models  and . however, usually  is constrained to have orthogonal weights: wktwk=i. note that under a least squares approach to pca, pk = wk and thus also pktpk=i. the principal components are interpreted by considering the contribution of the variables to the components. for the score-based model  this is based on the fact that the loadings are equal to the correlation of the variables with the components . let xjk be the jth variable in data block k and trk the rth component for block k, then

  rxjk,trk=pjrk, 

with r used as a notation for correlation and pjrk the loading of the jth variable on the rth component of block k. in the weight-based model , interpretation of the components is based on the weights as these express each component as a weighted linear combination of the variables,

  trk=xkwrk. 

for both model formulations this implies that for each component a total of jk correlations or weights have to be taken into account in the interpretation. especially in the case of omics data, that usually consist of thousands of variables, there is a need for methods that facilitate interpretation. to that end,  <cit>  proposed a sparse pca method for the weight based model , that shrinks a  number of component weights to zero. their method is based on a least-squares approach to pca model  in which the objective function is augmented with an l <dig> penalty  and an l <dig>  penalty: minimize with respect to wk and pk

  lwk,pk=xk-xkwkpkt2+λlwk1+λrwk <dig>  

such that pktpk=i and with λl ≥  <dig> and λr ≥  <dig> tuning parameters for the lasso and ridge penalties respectively, wk1= ∑jk,rwjkr and wk2= ∑jk,rwjkr <dig>  the lasso, tuned by the parameter λl, has the property to simultaneously shrink coefficients and select variables, keeping only those variables with the highest coefficients. the higher λl, the stronger the shrinkage and selection. note that the selection is done in an unstructured way meaning that correlations between variables are not taken into account. the ridge penalty, tuned by λr, only shrinks the coefficients and does not perform variable selection . it is often introduced when it is of interest to group correlated variables  <cit>  or in case of ill-conditioned optimization problems  to solve the non-uniqueness of the parameter estimates. a particular case is regression analysis with more variables than objects, jk >ik, which yields an under determined estimation problem. in the context of pca, this is of relevance for model  because the estimation of the component weights boils down to a regression analysis. adding the ridge penalty with λr >  <dig> solves the non-uniqueness; in addition, with the appropriate normalization, the ridge ensures that the solution of  yields the principal components in case λl =  <dig> .

the simultaneous component decomposition of k coupled data blocks xk having a common set of samples  is given by imposing the constraint that all tk are equal. applied to the score based model this gives:

  xk=tpkt+ek, 

for all k and under the constraints of a principal axes orientation and orthogonality of the component scores: ttt = i. applying the idea of a common matrix of component scores to the weight based model as used by  <cit> , can be realized as follows,

  x1…xk=x1…xkw1t…wkttp1t…pkt+e1…ek 

  =tp1t…pkt+e1…ek, 

under the constraint of a principal axes orientation and orthogonal loadings: p1t…pktp1t…pktt=i. simultaneous component model  shows that the common component scores t lie in the space spanned by all variables, this is from all data blocks. for ease of notation, we will use the shorthand notation xc =   and pc=p1t…pktt and wc=w1t…wktt . note that several simultaneous component models were proposed in the literature:  <cit>  gives an overview that emphasizes the different ways of weighting the data blocks in connection to different principles to realize a fair integration of the data.

the problem that a lot of variables have to be taken into account when interpreting the components is exacerbated in the case of simultaneous component analysis as this involves several blocks of variables. to solve for this problem, we propose to go for a sparse simultaneous component method by penalizing either the loadings  or the component weights  within a least-squares approach. one possibility, in line with sparse pca, is to use the lasso penalty if necessary in conjunction with a ridge penalty . however, other types of penalties can be used that, when selecting variables, explicitly take into account that variables belong to  groups/blocks by selecting variables within blocks only, between blocks only , or both within and between blocks. a penalty that introduces selection only within each group is elitist lasso , defined for the rth component as

  λe ∑kwrk <dig> =λe∑k∑jkwjkrk <dig>  

elitist lasso was introduced by  <cit>  in the context of regression analysis. the behavior of this penalty can be understood by observing that it behaves as the lasso within blocks and as the ridge between blocks, resulting in shrinkage and a selection of the variables with the highest coefficients within each block  and a shrinkage but with no selection between blocks .

to select entire  groups of variables, the group lasso  <cit>  was introduced. it uses the euclidean norm  of the group coefficients as a penalty,

  λg ∑kjkwrk2=λg ∑kjk ∑jkwjkrk <dig>  

this penalty behaves as the lasso at the block level and as the ridge within blocks: within blocks shrinkage and grouping of correlated variables occurs however with no selection ; between blocks selection of those blocks with the highest sum of squared coefficients occurs while other blocks are dropped . the group lasso applied to groups consisting of one variable only is the same as the lasso.  to obtain also sparsity within the groups that are not dropped by the group lasso,  <cit>  proposed the sparse group lasso that blends the lasso with the group lasso and implies shrinkage and selection both within and between groups. the behavior of each of the four penalties and associated norms is summarized in table  <dig> 

different norms used in the context of sparse approaches, their properties, and specific sparse approaches based on particular combinations of the penalties. a 'yes' indicates that the norm is active in the approach.

we propose the following generic functions that combine all penalties: first, for the approach based on sparse component weights,

  lwk,pk=∑kxk-xkwkpkt2+λlwk1+ ∑kλrwk22+λgjkwk2+ ∑kλewk <dig> =xc-xcwcpct2+λlwc1+λrwc22+ ∑kλgjkwk2+ ∑kλewk <dig> , 

which has to be minimized with respect to wk and pc under the constraint that pctpc=i. second, for the approach based on sparse component loadings,

  lt,pk=xc-tpct2+λlpc1+λrpc22+ ∑kλgjkpk2+λepk <dig> , 

which has to be minimized with respect to t and pk under the constraint that ttt = i. note that estimation of the loadings is not a regression problem. therefore, unlike the model based on sparse weights, unique solutions are obtained when jk >i. this is the case even when λr =  <dig> 

the generic loss functions  and  allow for a flexible use of all these approaches to sparseness. all combinations of the four penalties are made possible. however, often some prior idea about the structure  exists such that it is not necessary to consider all possible combinations. furthermore, some combinations are not advisable. for example the combination of the group lasso and elitist lasso does not seem useful because the behavior of the one interferes with the behavior of the other. by setting the appropriate tuning parameters in the objective functions to zero, particular known sparse approaches can be obtained. for example, with λg = λe =  <dig> the extension of sparse pca to simultaneous component analysis is obtained and with λr = λe =  <dig> a sparse simultaneous component version of the sparse group lasso in linear regression is obtained. with all four tuning parameters set equal to zero, the ordinary simultaneous component analysis model results. k =  <dig> leads to principal component analysis and setting λg = λe =  <dig> yields sparse pca as proposed by  <cit> . in table  <dig> a summary is given of these different existing sparse approaches in terms of which penalties are active.

algorithm
given fixed values for the different tuning parameters  and a fixed number of components r, we make use of an alternating scheme to minimize  or  with respect to wc  and pc : wc  and pc are alternatingly updated, conditional on fixed values for the other parameters. for example, focusing on :

• step 1: initialize wc

• step 2: conditional on the current estimate of wc, obtain the optimal least-squares estimate of pc under the orthogonality constraint as follows : pc = uvt with usvt the singular value decomposition of wctxctxc

• step 3: check the stop criteria: 1) is the difference in loss with the previous iteration smaller than 1e -  <dig> or, 2) is a maximum of  <dig> iterations reached? if yes, terminate, and else continue.

• step 4: conditional on the current estimate of pc, obtain the update of wc using a majorization minimization procedure ; see the methods section for a derivation of the estimate. return to step  <dig> 

this particular scheme guarantees that the loss is a non-increasing function of the iterations. due to the convexity  and the fact that the loss function is bounded from below by zero, the procedure will converge to a fixed point for suitable starting values. the majorization minimization  procedure has a linear rate of convergence; this slow convergence rate may, however, be compensated for by the efficiency of the calculations . to account for the problem that the fixed point may represent a local minimum instead of the global optimum, a multistart procedure can be used. see the methods section for details on the algorithm used to minimize . matlab code implementing the algorithms can be found in the supplementary material.

testing and implementation
in this section we apply the proposed approach both to empirical and simulated data. the application to empirical data  is mainly for illustrative purposes. the simulated data are used to check how the different penalties  behave under various conditions, and to compare the sparse component weights and sparse component loadings modeling approaches.

metabolomics data
as an illustrative case, we use empirical data on the metabolome composition of  <dig> samples of e. coli. the different samples refer to different environmental conditions and different elapsed fermentation times. mass spectrometry  in combination with on the one hand gas chromatography  and on the other hand liquid chromatography  as a separation method was used, resulting in two coupled data blocks: a gc-ms block with the peak areas of  <dig> metabolites in the  <dig> conditions and a lc-ms block with the peak areas of  <dig> metabolites in these same conditions. simultaneous component analysis was previously successfully applied describing the data well by five components . however, a better understanding of the processes underlying the data may be obtained by a sparse simultaneous component analysis  approach as this characterizes the components by a few instead of all metabolites and thus facilitates interpretation.

our proposed method allows to model the data in several ways, depending on the one hand on the choice of penalizing either the weights or the loadings and on the other hand on the particular values of the different tuning parameters. therefore, we will analyze the data under different options, namely either under model  or under model  and, for both models, with several combinations of values for the different tuning parameters. here we explain how we chose a suitable range of values for the tuning parameters using the notation for the model with penalized weights. the different values of λl, λg, λe, and λr were chosen in a way that reflects the balance between lack-of-fit and strength of the penalty by setting them as a fraction of ||xc|| <dig>  and |wc|p,q with wc obtained from the ordinary sca solution . let λp,q denote the tuning parameter of the penalty corresponding to the  lp,q norm, then this yields λp,q = f||xc||2/|wc|p,q with f taking values  <dig> - <dig> - <dig> - <dig> - <dig> . <dig>   <dig> , and  <dig>  we only consider those combinations of non-zero values for the tuning parameters that were considered in the regression literature, namely the lasso, elastic net, group lasso, sparse group lasso, and elitist lasso . note that the case with all tuning parameters equal to zero corresponds to regular simultaneous component analysis.

first we discuss the results for the approach based on penalized weights, then the approach based on penalized loadings, followed by a brief comparison of the two approaches. we end the empirical application section with a discussion on the choice and interpretation of a particular sparse simultaneous component analysis.

penalized weights
different panels correspond to different approaches: the lasso in the left panel, the group lasso in the middle panel, and elitist lasso in the right panel. within each panel, both the fit of the model to the data and the percentage of zero weights are reported. the different rows correspond to different values of the tuning parameter.

penalized loadings
a summary of the results obtained for the approach with sparse loadings is given in table  <dig>  the general result that increasing the tuning parameters yields a decrease in fit and an increase in sparsity also holds here. a comparison between tables  <dig> and  <dig> shows that for an equal proportion of zeros, the fit of models with sparse loadings is  lower. this can be understood from the fact that the loadings contribute more directly to the reconstruction of the data than the component weights  and ): for example, in a model with one component, a zero loading results in a zero vector for the reconstructed variable. this also explains why different from the approach based on penalizing the weights with an l <dig> penalty, the number of non-zeros is not bounded by i. table  <dig> also shows that to obtain zero loadings with the group lasso, high values of the tuning parameter are needed.

different panels correspond to different approaches: the lasso in the first panel, the group lasso in the second panel, and elitist lasso in the last panel. within each panel, both the fit of the model to the data and the number of zero loadings are reported. the different rows correspond to different values of the tuning parameter.

reflections on penalizing the weights versus the loadings
as illustrated, the results obtained under the model with penalized loadings are different from the results obtained under the model with penalized weights. in our view, the most important differences are at the level of data reconstruction and at the level of interpretation. with respect to data reconstruction, the model based on weights yields a better fit while the model with sparse loadings may yield many zero vectors for the reconstructed data. also, in this respect, the components based on sparse weights have a higher correlation with the components of the ordinary sca solution than the components resulting from a model with sparse loadings. with respect to interpretation of the underlying components, for the model based on sparse weights this is done in a regression-like way, while for the model based on sparse loadings it is based on considering loadings as correlations of the variables with the component. in ordinary sca, the loadings are the correlations and in the sparse model we observed a close connection in that zero loadings represent close to zero correlations and higher loadings represent higher correlations. the weights do not have such a relation with the correlation between the variable and the component.

selection and interpretation of the sparse sca solution
as has been illustrated in the previous results section, the data can be analyzed in many ways depending on choices made with respect to the generic model  or ) and with respect to the values of the different tuning parameters. selection of the appropriate model is of key importance and substantive issues may form a good point of departure. first, concerning the choice of the generic model, the model with penalized weights seems more appropriate for the data at hand because all metabolites can be considered to be involved in the biological processes underlying the data. for applications of component models with sparse loadings to microarray gene expression data, see  <cit>  and  <cit> . second, to choose appropriate values for the tuning parameters we consider the properties of the associated penalties. having components for which the interpretation is tied exclusively to one type of analytical platform  is convenient. also, because for each platform many metabolites result, sparseness within each platform/block is needed. this means that we are interested in selection both across and within groups. recently, there has been a growing interest for methods that perform such a selection  <cit> , with particular interest for the group lasso that has been extended and applied in several ways  <cit> . therefore, we will restrict ourselves to a group lasso type of simultaneous component model, however, including a ridge penalty to account for the fact that grouping is useful : λl >  <dig>  λr >  <dig>  λg >  <dig>  and λl =  <dig>  then, we eliminate solutions that 1) yield components with all weights equal to zero, 2) yield components having non-zero weights for both data blocks, and 3) solutions that do not fit well  or that are not sparse . the remaining solutions are summarized in table  <dig> in terms of the fit and the number of zeros per component. the solutions in bold, with high values for the lasso tuning parameter  and low values for both the ridge and group lasso parameters , show the best tradeoff between fit and sparsity. we select these solutions for an interpretation.

solutions with five components that have non-zero component weights in only one data block, a fit > . <dig>  and more than  <dig> percent of zero weights in the remaining block. the strength of the different tuning parameters is indicated in the first three columns, the fit is displayed in the fourth column, and the  <dig> remaining columns show for each component  how many of the  <dig> metabolites received a zero-weight.

the metabolites with non-zero component weights are displayed for both selected solutions in table 5; table  <dig> contains the component scores corresponding to the weights of the solution with fl =  <dig> . observe that the solution with fl =  <dig> is a further selection of the metabolites in the solution with fl =  <dig> . the first component shows an effect of phenyllactate,  <dig> -dihydroxypentanoate, and two aromatic amino acids , together with two branched-chain amino acids ; the corresponding component scores  show a clear increasing linear effect of fermentation time. the second component is made up by metabolites like fumarate, malate, aspartate and are associated to succinate catabolism  making biological sense as these metabolites are close to succinate in central metabolism. for c <dig>  we find non-zero weights for a large number of  disaccharides and pyruvate and lactate and high scores in the oxygen related conditions. the identification of pyruvate and lactate could be indicative of a changing, i.e. reduced, dissolved oxygen concentration in the course of the fermentation as pyruvate can be converted into lactate during anaerobic growth. the fourth component is made up by nucleotides important for the energy metabolism in a cell  and is associated to the growth condition with an elevated ph at the early  phase. finally, c <dig> seems specific for the wild type strain, although the relation to the metabolites guanine and thymine  and the other metabolites is not very clear.

metabolites with non-zero component weighs for each of the five components . the component weights of two selected models are shown that differ in the degree of sparsity .

component scores for each of the five components . the samples where obtained in a specific environmental condition  and at a particular fermentation time .

simulated data
to validate the proposed sparse simultaneous component method, we make use of simulated data. the general setup is that data are generated under some specific conditions and with known structure; after addition of noise, the performance of the method in terms of recovering the underlying structure is assessed. here, we are particularly interested in two aspects: a first one is whether the penalties reflect the structure in the selection of the variables ; a second one is the behavior of the method in function of the model . we also manipulated the amount of error in the data  and the degree of sparseness . all factors were fully crossed and for each of the resulting  <dig> ×  <dig> ×  <dig> ×  <dig> =  <dig> conditions,  <dig> data sets were generated, resulting in a total of  <dig> data sets. to obtain a realistic simulation, we generated the data using the metabolomics data described in the previous section.  <dig> samples were sampled with replacement from the original data; then a singular value decomposition was performed to obtain three components: the three loading and weight vectors were obtained as the three right singular vectors corresponding to the three largest singular values and multiplied by these, the three component score vectors were set equal to the corresponding left singular vectors. sparseness was imposed by setting either weights or loadings equal to zero as follows: in case of sparseness between blocks, all weights/loadings of the first component that correspond to the first data block  were set equal to zero and for the second and third component the weights/loadings corresponding to the second data block  were set equal to zero; in case of sparseness within blocks,  <dig> or  <dig> percent of variable indices were randomly sampled and their corresponding weights/loadings were set equal to zero; in case of sparseness within and between data blocks, the two previous strategies were combined. the resulting component loadings and weights were used to generate the true data part using the model part of expressions  and  . noise was then added to this true part of the data with the noise being generated from a normal distribution with mean zero and variance such that these residual matrices account for  <dig> or  <dig> percent of the total variation  <cit> . each of the data sets was analyzed under both models  and with varying values for the tuning parameters . the elitist lasso penalty was only combined with the ridge penalty because it interferes with the lasso and group lasso .

in the discussion of the results of the simulation study, we first focus on the conditions where the data are generated and analyzed under the same model , the error amounting to  <dig> percent of the total variation in the data, and the ridge penalty set equal to the smallest non-zero value. figures  <dig> and  <dig> display boxplots of the proportion of variables correctly classified  in function of the value of the tuning parameter. figure  <dig> refers to the case with  <dig> percent zero weights/loadings, figure  <dig> to the case with  <dig> percent zero weights/loadings. in each figure, the different panels refer to the different combinations of structure in the variable selection  and of sparseness approach . the panels referring to the sparse group lasso are with varying values for the lasso tuning parameter and with the group lasso tuning parameter fixed at fg =  <dig>  in general, the results confirm the expected relation between the structure of the variable selection and the different approaches to sparseness: the best recovery for selection within blocks is by elitist lasso with a value of  <dig>  for the tuning parameter fe, for selection between blocks is by the group lasso with fg =  <dig>  and for selection between and within blocks the sparse group lasso . deviations from the expected behavior occur for the sparse group lasso when selection is both within and between blocks in case of many zeros : the lasso and elitist lasso then outperform the group lasso. this can be attributed to the fact that the group lasso is less aggressive than the lasso and elitist lasso  <cit> . on the other hand, the lasso and elitist lasso perform less well when selection is within blocks and the true structure is not so sparse  because of their aggressive behavior. note that a penalty that selects between groups in a more aggressive way was proposed by  <cit> . the same pattern of results is obtained when the error amounts to  <dig> percent  or when the tuning parameter of the ridge penalty takes higher values. in case the ridge equals zero, the box plots show worse results for the lasso and elitist tuning parameters equal to zero .

a second point of interest, is the influence of the model used to generate and analyze the data. figure  <dig> displays four panels of boxplots for the proportion of correctly classified variables. within panels, the boxplots are displayed in function of the block structure present in the variable selection. the upper panels refer to data generated under a model with sparse loadings, the lower panels to data generated under a model with sparse weights. the panels at the left were obtained when analyzing the data with a model based on sparse weights and at the right with sparse loadings. in general, analyzing the data with the sparse weights model yields less misclassifications than using the sparse loadings model. however, generating the  data under a model with sparse weights, in general, results in more misclassifications than generating under a sparse loadings model. these results can be explained by the more direct relation between the loadings and generated or modeled data: generating the data with sparse loadings imposes a clearer structure than generating them with sparse weights; analyzing/modeling the data with sparse loadings imposes a stronger structure on the modeled data than modeling them with sparse weights. this is because 1) unlike a zero loading, a zero weight for a variable does not necessarily imply a modeled score of zero, because a zero weight for one variable can be compensated by non-zero weights for other variables, and 2) unlike shrinking the weights, shrinking the loadings results more directly in shrunken modeled scores. the latter can be explained by the dependence of the scale of the data, as modeled by pca model , on the scale of the loadings .

discussion
we proposed an extension of sparse pca to the context of several data blocks, relying on a generic modeling framework that allows either for sparse component weights or for sparse component loadings and that incorporates several approaches that were taken to sparsity in the regression literature . a very flexible algorithm was developed that allows to analyze the data under a variety of approaches that take the structure of the data in different ways into account. it also allows for combinations of penalties that were not yet considered in the regression literature.

the flexibility of the approach is important as often a particular kind of structure is needed from data integration methods. the group lasso is a popular tool to find structures that only involve one data block. this is for example relevant in comparative genomics when the focus is on divergence  <cit>  or on tissue-specificity  <cit> . elitist lasso, on the other hand, finds sparse structures that involve each of the data blocks. not only is this of relevance in the aforementioned case of comparative genomics to find conserved processes, but also in a top-down systems biology approach. for example, to integrate microarray gene expression data and interaction data with the aim of finding transcription factors and their target genes  <cit> .

although the model and algorithm were proposed in the context of simultaneous component analysis, it can be easily translated to the context of principal component analysis and also of regression analysis. in fact the algorithm can be used as it is for pca and the adaptation to regression analysis is a minor one. in the context of simultaneous component analysis, adaptations of the model  to a context that allows for different values of the tuning parameter for each component and/or each block would be valuable. however, such an extension is not trivial. moreover, the problem of selecting an optimal model becomes more difficult in that more parameters need to be tuned and this would make the choice of selecting appropriate values for the tuning parameter even more difficult than it already is. a major theoretic challenge for many sparse methods is to find a good method to select the value of the tuning parameters.

CONCLUSIONS
we offered a flexible and sparse framework for data integration based on simultaneous component methods. the method is flexible both with respect to the component model and with respect to the sparse structure imposed: sparsity can be imposed either on the component weights or on the loadings, and can be imposed either within data blocks, across data blocks, or both within and across data blocks. as such, it allows to find structures exclusively tied to one data platform as well as structures that involve all data platforms. a penalty based approach is used that includes the lasso, the ridge penalty, the group lasso, and elitist lasso. the method includes principal component analysis, sparse principal component analysis, and ordinary simultaneous component analysis as special cases. real and simulated data were used to validate the method. we believe the method offers a very flexible and versatile tool for many data integration problems.

