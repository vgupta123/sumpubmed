BACKGROUND
high-throughput technologies measure simultaneously tens of thousands of variables for each of the observations included in the study; data produced by these technologies are often called high-dimensional, because the number of variables greatly exceeds the number of observations. microarrays are high-dimensional tools commonly used in the biomedical field; they measure the expression of genes  <cit>  or mirnas  <cit> , the presence of dna copy number alterations  <cit>  or of variation at a single site in dna  <cit> , across the entire genome of a subject.

microarrays are frequently used for class prediction . in these studies the goal is to develop a rule based on the measurements  obtained from the microarrays from samples  that belong to distinct and well-defined groups ; these rules can be used to predict the class membership of new samples for which the values of the variables are known but the class-membership is unknown. for example, many studies tried to predict the clinical outcome of breast cancer using gene-expression  <cit> ; in this case the classes are the clinical outcome of breast cancer while the variables are the expression of the genes. some of the classification methods most frequently used for microarray data are discriminant analysis methods, nearest neighbor  and nearest centroid classifiers  <cit> , classification trees  <cit> , random forests  and support vector machines  .

an important aspect that specifically characterizes classification for high-dimensional data is the need to perform some type of variable selection. variable selection consists in the identification of a subset of variables that are used to define the classification rule, and it can be performed either before developing the classifier or it can be embedded in the classification method  <cit> . the importance of variable selection for high-dimensional data rests on two facts: some classification rules cannot be derived if the number of variables is larger than the number of observations, and removing the variables that have little variability across observations improves the predictive accuracy  <cit> .

in this paper we focus on classification problems for class-imbalanced data, i.e., on data sets where the number of observations belonging to each class is not the same. class-imbalanced data are common in the biomedical field and they also arise when data are high-dimensional. for example, using gene-expression microarray data, ramaswamy et al.  <cit>  classified primary versus metastatic adenocarcinomas: metastatic specimen comprised about 16% of the training set ; shipp et al.  <cit>  developed a classifier to distinguish diffuse large b-cell lymphoma from follicular lymphoma using a data set with a 25% class imbalance ; iizuka et al.  <cit>  predicted early intrahepatic recurrence or non-recurrence for patients with hepatocellular carcinoma, with a training set with a 36% class imbalance . the classification methods used by these studies were some variants of the diagonal linear discriminant analysis ; the third study used also support vector machines.

standard classification methods applied to class-imbalanced data often produce classification rules that do not accurately predict the minority class  <cit> ; for this reason the between-class imbalance problem has been receiving increasing attention in recent years and many different strategies were proposed for deriving classification rules for imbalanced data . however, their use is not widespread in practice and very often standard classification methods are used when the classes are imbalanced  <cit> . for example, ramaswamy et al.  <cit>  and shipp et al.  <cit>  did not modify the classification rules to take class imbalance into account, while iizuka et al.  <cit>  tried to adjust for by making training and test set equally imbalanced.

the aim of our study was to investigate how class imbalance affects classification for high-dimensional data, and to evaluate if the high-dimensionality poses additional challenges when dealing with class-imbalanced data. we devoted special attention to the isolation of the possible effect of variable selection and to the investigation of the effectiveness of some strategies that were proposed to deal with class imbalance. to our knowledge the joint effect of high-dimensionality and class imbalance on classification has not been thoroughly investigated.

the few works that dealt with the class imbalance problem for high-dimensional data mostly focused on developing methods for variable selection  <cit> , on the comparison of the performance of classifiers using different variable selection methods and/or classifiers  <cit> , or on proposing and evaluating different strategies for adjusting classifiers trained on class-imbalanced data  <cit> .

to investigate the effect of class imbalance on high-dimensional data, we evaluated the performance of six types of classifiers on imbalanced data. the classification methods were chosen among those most commonly used for high-dimensional data and for the sake of simplicity we considered only classification problems where the number of classes was two . the classifiers were evaluated both on simulated data and on a publicly available data set from a breast cancer gene expression microarray study  <cit> ; we assessed both the overall and the class specific predictive accuracy of the classifiers. we simulated situations where there was no difference between classes  and where the two classes were different , varying the number of different variables and the magnitude of their difference. we used over-sampling, down-sizing and a variant of asymmetric bagging to correct the class imbalance problem.

in results we present a series of selected simulation studies showing the consequences of using class-imbalanced high-dimensional data sets for classification, we show the performance of the corrections for class imbalance, and the results obtained on the breast cancer data. in discussion we outline the problems related to classification for high-dimensional data. in the methods section we briefly describe the classification methods that we used and the strategies to deal with the class imbalance problem; we also describe the simulations that were performed and the breast cancer gene expression microarray data.

RESULTS
the classifiers were developed on the training sets, while the predictive accuracy , predictive values  and area under the roc curve  were evaluated on the test sets. if not otherwise stated, the samples were normalized , while the variables were not , and the test sets were balanced . the classification with rf and penalized logistic regression  were based on the  <dig>  threshold, if not differently specified . each simulation was repeated  <dig> times. most of the figures show the results only for dlda, plr and one of the nearest neighbor classifier; the results for the other classifiers are shown in the additional files.

simulations results: null case
under the null case there was no difference between the two classes, as all the variables were simulated from the same distribution . in the first set of simulations only p =  <dig> variables were generated, and all were used to derive the classification rule . the imbalance was the same in the training and in the test set .

the class specific pa were not equal when the classes were imbalanced: most of the samples from the test set were classified in the majority class, which had a larger pa compared to the minority class . however, in all the situations the classifier showed no relation with the outcome  and had no information about the true class status .

the overall pa reached its minimum value when the data were balanced , and increased when the class imbalance of the test set became larger . the average class specific pa depended on the class imbalance of the training set but not on the class imbalance of the test set ; moreover, the overall pa was equal to  <dig>  for all the classifiers when the test set was balanced, regardless of the imbalance of the training set .

for most classifiers, performing variable selection further increased the probability of classifying a new sample in the training set majority class . for example, for 1-nn with k1train =  <dig> , when we increased the number of variables  and performed variable selection  we obtained pa <dig> =  <dig>  and  <dig> , and pa <dig> =  <dig>  and  <dig> , respectively, instead of the values expected under the null case ; also in this case the classifiers were not informative . the departures of the pa from the expected values depended on the procedure of variable selection, as we did not observe a similar effect when we increased the number of variables  but did not perform variable selection .

the effect of variable selection can be explained recognizing that the sampling variability is larger in the minority class. sample mean values far from the true population values arise more frequently in the minority class, and the variables that show large differences between the classes are more likely to be selected. the new samples from the test set are therefore more similar to the samples of the majority class, and as a consequence they have a larger probability of being classified in that class. we observed this behavior not only for t-test with equal variances but also for other commonly used parametric and non-parametric variable selection methods .

among the classifiers that we considered, rf, svm and pam  were the most sensitive to class imbalance when we did not perform variable selection , while apparently variable selection had little or no effect on their class-specific pa . the reason is that these classifiers perform some type of variable selection automatically, therefore for these classifiers the results of figure  <dig> embed variable selection. when the classification rules of rf and plr were adjusted to take the class imbalance into account , the dependency of the class specific pa on class-imblance diminished but it did not disappear . variable normalization  did not change the null case results: regardless of the class imbalance, its impact on data was very limited since the true means of all variables were all equal .

simulation results: alternative case
for the alternative case we considered situations in which some of the variables had different means in the two classes, varying the number of different variables  and the mean difference ) .

similarly to the null case, most samples were classified in the majority class: the class specific pa of the minority class rapidly decreased as the class imbalance increased; this effect was more marked when the differences between classes were smaller  or when we increased the number of variables  and performed variable selection  =  <dig> and g =  <dig> variables were selected). the auc and pv of the majority class decreased as the class imbalance increased, even though not substantially : the limited decrease of the pv of the majority class can be explained recalling that under the null hypothesis its value is large, being equal to the proportion of samples from that class in the population . the pv of the minority class increased : when the pa of one class approaches the value of  <dig>  so does the pv of the other class . the dependency of pv and auc on class imbalance was more marked if smaller differences between classes were considered .

the noise introduced in the classifier by selecting null-variables was only partially responsible for the decrease in the pa of the minority class . in an attenuated form this effect was still present even when all the variables were different between the two classes . similarly to the null case, we were more likely to select variables for which the discrepancy between the true and the sample values was larger in the minority class; as a consequence we were less likely to classify new samples in this class. this behavior was not a peculiarity of the t-test with equal variances, but was observed also for the other variable selection methods that we considered .

the classifier that showed the smallest decrease in the pa for the minority class was dlda, which was practically insensitive to class imbalance when the number of variables was small ; pam, svm and rf were the most sensitive to class imbalance also under the alternative case .

similarly to previous findings  <cit>  we also observed that variable selection improved the performance of the classifiers under the alternative case: the class specific pa were consistently better when variable selection was performed, also for situations where there was a large class imbalance .

we evaluated how the performance of the classifiers was affected by the magnitude of the difference between classes : we considered the same simulation settings of figure  <dig> but varied the mean of the pde =  <dig> non-null variables  =  <dig>   <dig> ,  <dig> , ...,  <dig>   <dig> , 3). for the balanced training set  the overall pa  reached the value of  <dig> when μ was between  <dig> and  <dig>  for all the classifiers; much larger differences between the classes were needed  ≥ 2) to obtain the same pa for the highly imbalanced data . the differences between classifiers trained with different class imbalance were not entirely due to their ability of selecting the non-null variables  ≥ <dig> ). rf, svm and pam required the greatest difference between classes in order to predict correctly all the samples in the imbalanced cases . normalizing the variables worsened the pa on class-imbalanced data ; this can be attributed to the different class imbalance in training and test set , as variable normalization did not have this negative effect when the imbalance was the same .

solutions
all the solutions were evaluated using the same simulation settings described for figure  <dig>  left panels, with p =  <dig> 

over-sampling
we ran a set of simulations in which we obtained a balanced training set by increasing the sample size, replicating the samples from the minority class . over-sampling  produced almost no change in pa compared to full-data analysis  for most classifiers. most of the classification rules were just slightly modified by the presence of replicated samples, as they depended on the within-class mean and variability of the variables, which are hardly modified by over-sampling. for the same reasons the variable selection process was also not substantially affected by over-sampling. only 3-nn and 5-nn benefitted from over-sampling, while 1-nn was not modified at all .

the performance of 1-nn, dlda and plr improved when over-sampling was used together with variable normalization, but only when the test set was balanced , therefore this result seems of limited practical utility . over-sampling with variable normalization partly removed the dependence of class specific pa on class imbalance for rf and pam when there was the same imbalance in training and test set , however only if the class imbalance was moderate .

down-sizing
in a second set of simulations we obtained a balanced training set by removing a subset of samples from the majority class . the pa of the minority class was greatly improved by down-sizing and the class-specific pa became the same for both classes, regardless of the class imbalance in the original training set . for example, using only  <dig> samples per class , all the classifiers achieved a pa of about  <dig>  for both classes, while the full-data analysis assigned all the samples from the test set to the majority class . the pv of the majority class further increased, while the pv of the minority class decreased substantially as the pa of the majority class moved away from  <dig> . the classifiers that were the most sensitive to class imbalance were those that benefitted the most from down-sizing. for example, pa <dig> increased from  <dig>   to  <dig>  when we down-sized the training set using pam with k1train =  <dig>  .

importantly, the variability of the estimated pa obtained by down-sizing increased with class imbalance; the 95% prediction intervals obtained for k1train =  <dig>  were between  <dig>  and  <dig>  while they were between  <dig>  and  <dig>  when k1train =  <dig>  .

the pa  obtained by down-sizing decreased as the class imbalance increased ; this effect was not due to class imbalance but to the decrease in sample size of the training set.

multiple down-sizing 
neglecting information from the majority class as in simple down-sizing is intuitively unappealing, therefore we considered multiple down-sizing , i.e., for each training set we repeatedly down-sized the training set, randomly selecting the samples from the majority class and including all the samples from the minority class, developed a classifiers on each training set, and assigned new samples to the class to which they were classified more frequently . the performance of multds  was similar but consistently better than down-sizing in terms of average pa. the decrease of pa for imbalanced data due to smaller sample size was still present but less pronounced. in our simulation settings the pa of all classifiers did not vary for a wide range of imbalance levels . multds had a smaller variability of pa compared to simple down-sizing ; for example, when k1train =  <dig>  the average pa was around  <dig>  for most classifiers and 95% prediction intervals were between  <dig>  and  <dig> .

to evaluate if the results obtained by multds were influenced by the number of samples left out to obtain a balanced training set we run a set of additional simulations where the sample size was smaller : at the same level of class imbalance multds worked better when the number of samples was larger; the observed differences increased with class imbalance and the performance of multds became similar to simple down-sizing when the sample size was small .

use of different threshold for rf and plr
we considered two variants of rf and plr, where the threshold value used for classification was based on the class imbalance of the training set rather than on the fixed value of  <dig>  . using these variants the dependency of the class specific pa on the class imbalance was less pronouced but still present .

an application to breast cancer microarray data
we used the published gene-expression microarray data set of sotiriou et al.  <cit>  to evaluate the effect of class imbalance on classification for real high-dimensional data . we considered two classification problems: the prediction of estrogen receptor  status  and of grade of breast cancer .

we obtained different levels of class imbalance by repeatedly randomly selecting subsets of the samples from the complete data set:  <dig> different training and test sets were obtained for each situation.

in a first set of analyses we trained the classifiers using class balanced and class-imbalanced training sets ; the test sets were balanced . the class specific pa  were strongly dependent on class imbalance of the training sets for all the classifiers ; most test set samples were classified in the er+ class  and paer+ was larger than paer-. for example, when the training sets included  <dig> er+ and  <dig> er- samples, the paer+ was above  <dig>  for all the classifiers, while paer- ranged from about  <dig>  to  <dig> ; the pv for the er+ class  was about  <dig> , while it ranged between  <dig>  and  <dig>  for the er- class , and the auc was between  <dig>  and  <dig> .

using the smaller but balanced training set  the class specific pa were approximately the same . for most classifiers the auc and pver+ were smaller than those obtained using the larger imbalaced data, while the pver- were larger. similarly to the simulation studies, dlda was the least sensitive to class imbalance. over-sampling did not remove the dependency of class-specific pa on class imbalance for most of the classifiers, with differences between class-specific pa as large as  <dig>  . over-sampling worked reasonably well only for 3-nn and 5-nn when the imbalance was not too large ; these results were in line with the simulation studies results.

we used multds for several different levels of class imbalance in the training set . pam seemed to benefit the most from multds; however, the gains in pa achieved by using multds rather than simple down-sizing were not as considerable as those observed in the simulation studies for the same level of class imbalance. this could be the consequence of using a smaller sample size in the breast cancer application.

the major advantage of multds over simple down-sizing was the reduction of variability of the estimated predictive accuracy; for example, when the minority class included only  <dig> samples  the prediction intervals obtained by simple down-sizing included the value of  <dig>  for all the classifiers, while the lower limit of the prediction intervals obtained by multds were above  <dig>  for most classifiers even for the largest degree of class imbalance. compared to simple down-sizing, the pver+ slightly increased, while the pver- decreased, ranging between  <dig>  and  <dig> ; the auc increased .

grade of breast cancer was more difficult to predict than er status . the smaller differences in between-class gene-expression translated in larger sensitivity of the classifiers to class imbalance , therefore the effect of class imbalance was stronger. overall, the results obtained for grade prediction were in line with those obtained from the simulations.

variable normalization further increased the effect of class imbalance, causing even more cases to be classified in the training set majority class . using normalized variables down-sizing worked well, and so did over-sampling for k-nn, dlda and plr, but only if the test set was balanced; therefore, the practical importance of this result seems very limited.

discussion
our results showed that some of the classifiers that are more frequently used for class prediction with high-dimensional data are highly sensitive to class imbalance. all the classifiers that we considered assign most of the new samples to the majority class from the training set, unless the difference between classes is large. this problem arises for two reasons: the probability of assigning a new sample to a given class depends on the prevalence of that class in the training set, and with variable selection this probability is further biased towards the majority class. as a consequence, when classifiers are trained on class-imbalanced data there are usually large differences between class specific predictive accuracies; moreover, the overall predictive accuracy is not informative, especially when both the training and the test set are imbalanced . in most circumstances, the unequal predictive accuracies produced by class-imbalanced classifiers have the effect of slightly decreasing the difference in the class-specific predictive values, which is present when the predictive accuracies are equal and the classes are imbalanced; generally, the predictive values of the minority class increase, while there is a slight reduction for those from the majority class, which are large even when the classifer is uninformative. similarly to our previous findings for another classifier  <cit> , we observed that also in this setting all these properties are mantained even if the prevalence in the training and test set is matched. normalizing  the variables generally additionally biases the classification results, and only if the training and test set have the same imbalance it does not produce an additional negative effect. using the embedded class imbalance corrections available for rf, svm or pam does not remove the dependency of the classification probabilities on class imbalance. our results indicate that variable selection further increases the probability of assigning a new sample to the majority class; the reason is that the sampling variability is larger in the minority class and therefore the biggest deviations between the true and the observed values arise in this class. as a consequence, the selected variables are those that have the biggest departures from the true values in the minority class, either indicating differences between classes that do not exist , or amplifying some differences that exist . however, at the same time variable selection plays also a positive role in high-dimensional classification; similarly to previous findings  <cit>  our results also indicate that the predictive accuracy of all the classifiers is improved if the classification rule is derived using only a selected subset of the measured variables.

the next question is if there are satisfactory remedies for these problems. a first set of solutions consisted in creating a balanced training set, either by replicating  or by removing  some of the samples. over-sampling does not remove or attenuate the class imbalance problem  <cit>  also in our settings because we considered classification rules and a variable selection method that are hardly modified by the presence of replicated samples; the k-nn classifiers  are the only exeption. on the other hand, simple down-sizing works well in removing the discrepancy between the class-specific predictive accuracies, but as expected it has a large variability and the predictive accuracy of the classifiers worsens when the effective sample size is reduced considerably because the class imbalance is large.

our attempt in overcoming these problems was the combination of classifiers trained on balanced down-sized training set. multiple down-sizing can be seen as a special case of asymmetric bagging  <cit> , except that the variables are selected in each down-sized training set. we based the combination of the classifiers on majority voting even though more complex methods are available; for example, easyensemble  <cit>  combines the outputs from the classifiers using adaboost  <cit> .

in practice multiple down-sizing improves on simple down-sizing: the major advantage is the reduction of variability of the estimated predictive accuracy  <cit> ; in some situations we observed also a limited improvement of predictive accuracy. the relative benefit of multiple down-sizing over simple down-sizing depends on the amount of information discarded by simple down-sizing, i.e., on the level of class imbalance but also on the number of left-out samples. the real data had smaller sample size compared to the simulated data and for that reason multiple down-sizing was not as beneficial on real data as in the simulations.

we used penalized logistic regression  as a classification method and evaluated its predictive accuracy as the fraction of correctly classified samples . the classification based on the  <dig>  threshold on the predicted probabilities assigns most samples to the majority class from the training set, similarly to simple logistic regression. using the threshold based on the imbalance from the training set, which works well for logistic regression, reduces but does not remove the classification bias towards the majority class, also when no variable selection is performed before fitting the plr model; variable selection further increases this bias. similar results hold for random forest.

we did not attempt to perform a comprehensive study on class imbalance for high-dimensional data, but we focused solely on some types of classifiers and of classification strategies. we selected which classifiers to evaluate among those that are most commonly used for high-dimensional data. it is possible that other methods that we did not consider might be less sensitive to class imbalance. most of our results were based on one single method for variable selection, i.e., t-test with equal variances, which bases the selection of the genes on the difference between their means. some of the effects of variable selection on class-imbalanced classification might depend on this choice. however, our results show that also other parametric and non-parametric variable selection methods have the same type of problem on imbalanced data. this is because they all attempt to select, among a very large number of candidate variables, those that differ the most between the classes, using different metrics to define the difference. we decided to focus on some of the solutions for the class imbalance problem: over-sampling, down-sizing and multiple down-sizing. we observed that these approaches performed well in removing the bias towards the classification into the majority class, with the exception of over-sampling. more complex methods might be more effective in reducing the variability of the predictive accuracy. the development of guidelines for the design of class prediction studies with class-imbalanced data is also a very important issue, which we considered only marginally in this paper.

CONCLUSIONS
our results show that the naive use of classifiers on class-imbalanced high-dimensional data can produce classification results that are highly biased towards the classification in the majority class. the extent of this bias depends on the classification method, on the magnitude of the difference between classes, and on the level of class imbalance, and it is further increased when variable selection methods are used; variable normalization generally increases the bias and it should be avoided, unless the class imbalance is equal in training and test set. when class imbalance is moderate, and no correction for class imbalance is applied, our results indicate that dlda performs well. in addition to its relative robustness to class imbalance, another advantage of dlda is its simplicity and interpretability.

our results suggest that using a balanced training set is a good choice for the design of high-dimensional class prediction studies, also in situations where the proportion of samples from each class is not equal in the population. if class imbalance cannot be avoided, researchers should take the class imbalance problem into account, and appropriately adjust their classification rules. we showed that multiple down-sizing can be effectively used if class imbalance is not too severe. this method is useful in removing the bias towards the classification in the majority class, and in reducing the variability of the predictive accuracy compared to simple down-sizing. further work is needed in order to asses if more complex approaches to the correction of class imbalance problem can further increase the class-specific predictive accuracies and predictive values, and reduce their variability.

