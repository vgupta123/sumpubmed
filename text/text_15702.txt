BACKGROUND
cellular response to extracellular stimuli is governed by biochemical reactions that allow the transfer of information from the cell membrane to the nucleus and back  <cit> . based upon well-crafted bench experiments, many of the molecular players in the various signaling pathways are known. however, the roles that individual proteins play at specific points in time and in particular systems are largely unknown  <cit> . it is precisely in this situation that mathematical models are most helpful  <cit> .

mathematical models are tools used to rationalize about the intracellular signaling mechanisms that underpin biological response  <cit> . mathematical models that describe biochemical kinetics are explicit statements about molecular-molecular interactions that are presumed to be important in a system and the corresponding dynamics of these interactions. these interactions give rise to a flow of molecular information in the form of reaction pathways  <cit> . the primary goal of the analysis of these reaction pathways is to make predictions: what do we expect to happen in a particular reacting mixture under particular reaction conditions, given our current understanding of molecular interactions? similarities confirm our explicit statements while differences between the expected behaviors and new data highlight areas of uncertainty in our understanding and provide the engine for scientific progress  <cit> . analogous to experimental studies, the ability of a particular mathematical model to describe a system of interest must include a statement of belief.

belief derived from a mathematical model is expressed commonly in terms of a single point estimate for the predictions, obtained from the set of parameters that minimizes the variance between model and data  <cit> . given that a model constrains the set of possible states of the system, it would be particularly valuable to provide an estimate of the uncertainty associated with the model predictions given the available data. a bayesian view of statistics is a mathematical expression of our beliefs  <cit> . beliefs are established based upon the observation of data and the interpretation of that data within the context of our prior knowledge  <cit> . mathematical models provide a quantitative framework for representing prior knowledge of the detailed biochemical interactions that comprise a signaling network. the unknown parameters of the model can be calibrated against the observed network dynamics. with recent advances in computational power, computationally-intensive bayesian techniques, such as markov chain monte carlo  algorithms  <cit> , are an attractive option for assessing the uncertainty in the model parameters given the calibration data and are increasingly applied to biological systems  <cit> .

mcmc techniques provide random walks in parameter space whereby successive steps are weighted by the likelihood of observing data given the corresponding parameter values. a prior distribution is used to determine the size of proposed steps  within parameter space. the historical challenge with implementing a bayesian approach is to specify a prior distribution for the values of the unknown parameters  <cit> . conventional application of bayesian techniques to cellular signaling networks focus on using proposal distributions that dynamically scale the prior distributions during the simulation , whereby the structure of the proposal distribution is unchanged during the simulation, or that is based upon the prior information encoded within the model and the parameter values at local optima . moreover, a common approach in the field is to use small models as a vehicle to demonstrate new bayesian algorithms . while these small models, sometimes called toy models, are computationally attractive, larger models foreshadow potential problems that may arise when a bayesian technique is used in practice, such as computational efficiency. the computational efficiency of a mcmc algorithm depends highly on the structure of the proposal distribution. one of the recent advances in the field has been has been a technique that allows the mcmc algorithm to "learn" a better structure for the proposal distribution "on-the-fly." these algorithms are "empirical" bayesian techniques in the sense that the structure of the proposal distribution is conditioned on the observed data. adaptive mcmc  algorithms  dynamically adjust the proposal distribution from a non-informative prior distribution at the start of the simulation to a proposal distribution that reflects the structure in the cumulative markov chain. the objective of this study was to develop and apply an adaptive mcmc technique to a typical study of cellular signaling pathways. as an illustrative example, a kinetic model for the early signaling events associated with one receptor of the epidermal growth factor  signaling network was calibrated against dynamic measurements observed in primary rat hepatocytes.

methods
a model for early erbb <dig> signaling
to illustrate the empirical bayesian approach, a mathematical model was developed for describing the early signaling events via the erbb <dig> receptor. developing this model was divided into three steps: specifying the model topology, calibrating the model, and estimating the uncertainty in the model predictions. in the following sections, each of these aspects is described in greater detail.

model topology
the application of quantitative methods to the egf signaling pathway was pioneered by kholodenko et al.  <cit> , and extended by many others . the model used here is similar to the original kholodenko model but incorporates three key changes. first, the reactions were grouped into reaction classes that are defined based upon peptide motif-motif interactions  <cit> . organizing cell signaling reactions into a small set of reaction classes provides a direct link to the automatic reaction network generation literature . parameters for specific reactions were assigned based upon membership in a particular reaction class. second, binding between proteins, which include multiple peptide motifs, was assumed competitive. in practice, this additional constraint for erbb <dig> converts a differential equation for unoccupied phosphotyrosine erbb <dig>  into an algebraic conservation equation. finally, the molecular mechanism associated with ras activation was modified to reflect recent biochemical observations that the activity of membrane-bound sos is modulated via an allosteric mechanism  <cit> . for brevity, the resulting reaction network is shown schematically in figure  <dig> and the corresponding equations can be found in an additional file . with the addition of the ras mechanism and other modification, the resulting model, comprised of  <dig> non-linear ordinary differential equations and one algebraic equation, was slightly more complicated than the original kholodenko model that contained  <dig> non-linear ordinary differential equations.

model calibration
the resulting biochemical kinetic reaction network specifies a particular causal connectivity diagram between proteins . this connectivity diagram must be coupled with parameters for each reaction to simulate the concentration changes with time. the values for the parameters were determined to be consistent with observed experimental data. in particular, dissociation constants for particular protein-protein interactions were constrained by the recent observations by macbeath and coworkers  <cit> . in addition, measurable interaction strengths between peptide motifs were used to estimate reaction path degeneracy  . these data helped reduce the number of parameters determined from  <dig>  in  <cit>  to  <dig>  in this study.

as cellular context may influence the strength of a particular pathway  <cit> , experimental data was extracted from the literature to calibrate the initial signaling events in a specific system: the response of primary rat hepatocytes to  <dig> nm of egf. parameter values were chosen based on appropriate measurements in rat hepatocytes if available. the changes in phosphorylated erbb <dig>  <cit> , grb <dig> binding to erbb <dig>  <cit> , grb <dig> binding to shc  <cit> , phosphorylated shc  <cit> , phosphorylated plcγ- <dig>  <cit> , sos binding to erbb <dig>  <cit> , and the active fraction of ras   <cit>  were calibrated to observations in isolated hepatocytes from harlan sprague dawley rats upon exposure to  <dig> nm egf. initial expression of erbb <dig> prior to egf stimulation was observed to be  <dig> nm. however, in the absence of specific kinetic measurements in rat hepatocytes, data obtained from human cell lines were used. total erbb <dig> receptor expression was calibrated to changes in erbb <dig> expression in hmec 184a <dig> cells observed following exposure to  <dig> nm egf  <cit> . in addition, the dynamics of shc phosphorylation was also compared against measurements obtained using  <dig> nm egf in the mcf- <dig> cell line  <cit> . the model equations were encoded and evaluated in matlab v <dig>  . summed squared error between experimental and simulated measurements was used to determine goodness-of-fit. using the set of coupled differential algebraic equations, the maximum likelihood estimates for unknown parameter values were determined from these experimental data using a simulated annealing optimization algorithm  <cit> . the optimum values were used as a starting point for the markov chain.

a bayesian perspective on model-based inference
the expected value for some property of a model can be calculated from the following integral:   

where f is a generic function of the model parameters. the expected value is dependent on the particular formulation of the model, m, and the data used in calibrating the model, y. as not all combinations of parameters provide realistic simulations, values for f are weighted by distribution of parameters given m and y ). using bayes theorem, the posterior distribution of the parameters can be re-expressed in terms of more computationally tractable quantities:   

in equation  <dig>  p is the probability of sampling a point in parameter space Θ prior to any knowledge about data y , p is called conventionally the evidence for a model, and p is the conditional probability of simulating data y given Θ and m .

as the evidence for a model can be considered as a constant, the posterior distribution of the parameters is proportional to the product of the likelihood of the data and the priors. a form of the likelihood can be obtained by assuming that the errors associated with each experiment are independent samples from multivariate normal distribution with a mean of zero:   

where v is the the dispersion matrix, k is the number of experimentally observed variables acquired nobs times, and Σ is the variance-covariance matrix of the error associated with the experimental observations. the dispersion matrix is a k × k positive-definite matrix where each element,   

corresponds to the normalized sum of the product of the deviation between specific observations, yiu, and their respective model prediction, mi. a common approach for evaluating the likelihood function in bayesian inference problems is to assume that the variance-covariance matrix of the error associated with the experimental observations  is a scaled identity matrix . however, the error models associated with an observation may depend highly on the experimental assay selected  <cit> . in addition, the data used in calibrating the model may correspond to different experimental studies and different techniques. given that Σ is unknown a priori in most practical problems, box and draper noted that the marginal conditional probability, p, can be obtained by assuming a prior for p equal to |Σ|-/ <dig>  <cit> . the corresponding likelihood relationship, of the form of a wishart distribution, can be reduced analytically to a simple form expressed in terms of the determinant of the dispersion matrix:   

similarly, an approximate bayesian approach was recently described that uses a likelihood function equal to the determinant of the dispersion matrix alone  <cit> . in practice, the dispersion matrix can be normalized to a reference quantify to minimize the contribution of round-off error in calculating p. given the rugged landscape of the dispersion matrix with respect to the parameter space, a closed form solution of the expectation integral  is intractable. as an alternative, monte carlo sampling is an efficient method for integrating such complex integrals  <cit> . in the following section, a monte carlo algorithm is proposed that provides a bayesian estimate of p.

a bayesian estimate for p can be obtained using computer-intensive methods. the metropolis-hasting  algorithm  <cit>  is a monte carlo technique that creates a sequence  of θs that, in the limit of a long chain are drawn from the target distribution, p. when the proposed steps are drawn from a stationary distribution, the resulting chain is a markov chain. a proposed new step in the markov chain, θn, is accepted based upon the relative conditional probability of the new step in capturing the observations ) relative to the current step. this acceptance criteria enables the collective markov chain to represent samples drawn from the target distribution p. new proposed steps within the markov chain are drawn from a proposal distribution ∏. as mentioned above, the challenge in implementing a markov chain monte carlo  approach is specifying the proposal distribution, ∏. in this study, the initial estimate of scale of proposal distribution for Θ was proportional to 1/p, where p is the row dimension of the vector Θ  <cit> . parameter sampling was performed in log space to ensure that all kinetic parameters and species concentrations remain positive. in high dimensional problems with expensive calculations of the likelihood function, efficient sampling of p is essential and can be obtained using a proposal distribution that reflects the structure of the target distribution ).

a prior distribution, p, is used to provide an initial estimate of the proposal distribution, ∏. in conventional mcmc algorithms, the proposal distribution can be scaled to achieve a target acceptance fraction but the structure of the proposal distribution remains fixed to that specified by the prior distribution. a recent development in the mcmc field has been the development of adaptive mcmc  algorithms  that dynamically adjust the structure of the proposal distribution based upon the prior steps of an evolving markov chain. the prior distribution used in this study was non-informative , proper , normally distributed ), and used to specify the initial proposal distribution ). non-informative in this context means that no information was used to specify that there was greater uncertainty in some of the parameters relative to others. following a specified "learning" period, the proposal distribution was adjusted, as described in the next section, to reflect the structure in the cumulative markov chain.

adaptive mcmc algorithm
the following adaptive mh algorithm was used to generate a monte carlo markov chain from multi-response data:

 <dig>  start the chain using initial values for θ <dig> obtained via simulated annealing and an initial proposal distribution with the corresponding elements:   

where p is the row dimension of Θ, δjk is the kronecker's delta, and s is an adjustable proposal scaling factor used to give an initial acceptance fraction of  <dig> . select a value for s. note that the kronecker's delta, δjk, is equal to  <dig> when the indices j and k are equal and δjk is equal to  <dig> when the indices are not equal.

 <dig>  using the current θi of the markov chain, propose a candidate step, θn, that is drawn from ∏ according to   

which cumulatively represent a random walk through parameter space with the step size in a particular parameter direction determined by ∏.

 <dig>  evaluate p using the form of the likelihood:   

where yj is a vector of observed responses for experiment j, mj is the corresponding vector of predicted responses using a model and the corresponding parameter values θn, and nobs, j is the number of responses observed in experiment j.

 <dig>  calculate the metropolis acceptance probability:   

where q is the jumping probability in the direction given as the argument. given that, in this case, the proposal distribution is symmetric, the q terms cancel.

 <dig>  accept move θi+ <dig> = θn with probability h, else retain current location θi+ <dig> = θi.

 <dig>  update cov and s every x amcmc steps.

 <dig>  using the current values for cov and s, update the proposal distribution whereby elements of ∏ are calculated using the following adaptive relationship:   

where choljk is the jk element of the upper triangular matrix obtained from cholesky factorization of the covariance matrix, cov, and nl is the number of steps contained in an initial non-adapting "learning" period. as proposal density is symmetric, the condition of a detailed balance among random steps within parameter space is satisfied using h as an acceptance probability.

 <dig>  go to  <dig> 

the algorithm was implemented in matlab , numerical integration of the set of differential algebraic equations was obtained using the sundials ida solver , and r/bioconductor was used for the analysis of the markov chain. in addition, the markov chains generated by this amcmc algorithm converge to a stationary target distribution for p  as the adaptation of the proposal distribution exhibits diminishing adaptation   <cit> .

an inherent characteristic of a markov chain generated using a metropolis-hasting algorithm is the autocorrelative structure of the chain. the presence of autocorrelation between subsequent steps biases estimating the covariance of the model parameters. mixing is a term used to characterize the autocorrelative structure of the chain. in well-mixed chains, steps separated by a specified distance within the chain exhibit minimal autocorrelation. to minimize the effect of autocorrelation, independent samples from the posterior distribution can be obtained by selecting values from the markov chain at every nth iteration, a technique called "thinning." thinning was used to improve recursive calculation of the proposal covariance of the markov chain during the mcmc run. a thinning value of  <dig> was used to estimate the covariance recursively from the evolving markov chain, while a thinning value of  <dig> was used to obtain p from the final markov chains.

convergence of markov chain
one of the challenges with implementing a mcmc approach for bayesian inference is deciding when the cumulative markov chain is a representative sample drawn from the underlying stationary distribution. a markov chain represents a random walk within parameter space weighted by the relative conditional probability of each step. convergence is a criteria used to evaluate how long of a chain is necessary to traverse a representative sample of parameter space. numerous algorithms have been developed to diagnose the convergence of a markov chain  <cit> . most conventional applications of these convergence criteria have focused on the model parameters. however, models of cellular signal transduction pose significant challenges for model-based inference and require a non-conventional approach.

as the objective of this study is to make statements of belief about the predictions of the model rather than particular values of the parameters, a convergence criterion was developed based upon the predictions of the model. the focus on predictions rather than the parameters was motivated by timescale considerations. when a complex dynamical system experiences an abrupt change in environmental conditions, the response of the system can be simplified into different kinetic manifolds . this phenomenon has been termed the slaving principle  <cit> . the evolution in the system is constrained by the slow variables  while the fast variables  exist at a pseudo-equilibrium. an implication of the slaving principle on bayesian inference is that the posterior distributions of parameters may exhibit one-sided distributions. the parameters associated with a variable contained within the fast kinetic manifold may be increased above a threshold value with minimal impact on the dynamic response of the system. below the threshold value, the dynamics of the corresponding variables impinge upon the slow kinetic manifold altering the dynamic response of the system and fitness of the model. in practice, the upper bounds of the parameters are defined by the tolerance of the ode solver to solve systems with divergent timescales. to surmount this computational challenge, the gelman-rubin method was used for assessing convergence of model predictions derived from a series of three parallel markov chains  <cit> . the method is based upon the concept that convergence has been achieved when the variance among chains is less than within single chains.

to estimate convergence, we use the prediction, pyij, obtained from a single draw from j parallel mcmc samples of length n, where j ∈ j and i ∈ n. to illustrate the calculations, a simplified notation is used where pyij represents the state of species k at time m ). for each prediction, the between-sequence  and within-sequence  variances for pyij are computed as follows:   

where  is the mean of the n predictions obtained from the j -th sequence:   

 is the mean of the predictions across sequences:   

and the sample variance of the predictions for the j - th sequence, sj is given by:   

the overall variance of the predictions derived from the target distribution is estimated from w and b by:   

the gelman-rubin method diagnoses convergence from the potential scale reduction factor:   

whereby parallel chains of length n should be increased until  is less than  <dig>   <cit> . these converged parallel chains represent samples drawn from a stationary distribution. in conventional use, the gelman-rubin method is applied to the second n iterations drawn from parallel chains, each of length 2n. the first n steps are discarded as they are assumed to be drawn from tails of the stationary distributions. due to the cost of these calculations, only the "learning" period was discarded. the remainder of the parallel chains were used to estimate the convergence of the predictions to a stationary distribution.

RESULTS
model calibration
the mathematical model, shown schematically in figure  <dig>  was calibrated against values obtained from the literature. the values reported in the literature were obtained largely in primary rat hepatocytes in response to  <dig> nm egf. kinetic parameters associated with egf binding to erbb <dig> and affinities for protein-protein interactions were used from the literature to reduce the number of free parameters. dissociation constants  for particular protein-protein interactions, where reported, were also obtained from the literature  <cit>  . in addition, dissociation constants for reactions that create a cyclic pathway  were constrained by thermodynamic considerations. in addition, the initial values for grb <dig>  plc-γ, shc, sos, rasgtp, and rasgdp were determined using simulated annealing. a list of all of the parameters can be found in . the initial concentration of ptp was specified to be  <dig> nm, as the initial concentration of ptp was confounded with other parameters . the initial values for protein expression are dependent on the assumed dissociation constants for protein-protein interaction. similar dynamics of the system are observed if systematic reductions in the kd's of protein-protein interaction are coupled with corresponding decreases in the concentration of the signaling proteins . given the best fit values determined using simulated annealing, an empirical bayesian approach was used to estimate the uncertainty in the model parameters given the available calibration data.

an empirical bayesian approach to sensitivity analysis
a series of markov chains generated using a metropolis-hasting algorithm were used to estimate the conditional uncertainty in the model parameters. three parallel markov chains were calculated each containing  <dig>  steps. the simulation of each chain took approximately  <dig> hours on a single core of a  <dig>  ghz dual-core intel xeon 64-bit processor with  <dig> gb ram. the parameter values obtained using simulated annealing provided the starting point for the chain. a "learning"  period of  <dig>  steps was specified a priori to provide an initial estimate for the proposal covariance. in addition, this "learning" period was used to correct for starting values of the chain that may correspond to samples from the tail of p. following the "learning" period, ∏ was adjusted to correspond to the covariance of the cumulative target distribution p. the trace of the acceptance fraction  demonstrate that the scaling factor was adjusted at regular intervals  to maintain the acceptance fraction around  <dig> . the trace of the conditional probability for each of the three chains, shown in figure  <dig>  suggests that markov chains were no longer sampling from the tails of the posterior distributions, as may occur during the "learning" period. one of the chains  entered a stiff region of parameter space whereby numerical difficulties in selecting an appropriate integration step size retained the markov chain within this region. the gelman-rubin potential scale reduction factor  was applied to the model predictions to assess the convergence of the cumulative markov chains obtained following the "learning" period. the gelman-rubin psrf statistics were calculated for the species observed experimentally as a function of time and amcmc step and shown graphically as a contour plot in figure  <dig>  the colored contours correspond to values of the psrf. many of the model predictions exhibited a potential scale reduction factor of below  <dig>  immediately after the "learning" period  while other predictions were unable to converge for the entire prediction window . given the variability in convergence, the entire chains, following the initial learning period, were used in the final analysis of the markov chains. the maximum expectation values determined using the amcmc algorithm are also reported . in addition, the psrf applied to the parameters are also shown for comparison .

a common feature seen in the different subplots of figure  <dig>  was that the variability among chains, as represented by an increase in psfr, increased as a function of simulation time . this behavior was due to undersampling at longer times relative to earlier times. oversampling of a region implicitly applies a higher relative weight in estimating p. for comparison, the 95th percentile, 5th percentile, and median responses for each of the three chains are overlaid upon the experimental data in figure  <dig>  traces for each of the parameters are shown in figure  <dig> 

discussion
in this study, an empirical bayesian approach was developed to quantify the uncertainty in the estimated parameters, given the available data, and to estimate the uncertainty in the model predictions, given the range of plausible parameter values. in contrast to recent efforts that use proposal distributions that have fixed structures , the empirical bayesian approach used in this study was based upon an adaptive markov chain monte carlo algorithm that adjusts the structure of the proposal distribution based upon the covariance of the cumulative markov chain. in another recent publication, we used this algorithm to infer the relative strength of control mechanisms in the interleukin- <dig> signaling pathway observed in naïve cd4+ t cells by our own hands . in this study, an amcmc algorithm was used to help interpret measurements of epidermal growth factor signaling observed in rat hepatocytes by others using signaling mechanisms postulated from the literature.

following an assessment of the convergence of the parallel markov chains, pairwise scatter plots were used to illustrate the structure in the posterior distributions of the parameter values . pairwise comparisons of the expected parameter values showed that a subset of the parameter values were not independent but exhibited interdependence, as illustrated by diagonal structure in the scatter plots . pairwise correlation coefficients are shown above the diagonal. analysis of the covariance of the thinned markov chains, shown in figure  <dig>  suggests that parameters rasgdp and kf <dig> are not identifiable as they exhibit a negative correlation coefficient of  <dig> . this was expected as increase in the rate of rasgtp hydrolysis  can compensate for low levels of rasgdp. the density plots, shown to the right of the diagonal in figure  <dig>  suggest that the initial concentrations of grb <dig>  shc, and sos are tightly constrained given the available data. in addition, kdeg <dig> and kf <dig> were two of the kinetic parameters that were tightly constrained. in contrast, many of the parameters  exhibited a much larger uncertainty range ). it is also interesting to note that - even though shc, sos, and grb <dig> were constrained by the data - the initial concentrations of plcγ and rasgtp/rasgdp were not tightly constrained. this difference can be attributed to the calibration data for plcγ and rasgtp/rasgdp were reported in terms of relative amounts instead of absolute quantities and these species were specified as terminal species in the model. the broad range in posterior distribution of plcγ expression reflects the inability of the markov chain to converge in predicting the phosphorylation state of plcγ at longer times. future simulations that include an estimation of initial concentration of plcγ within this cell type may aid in convergence.

estimated values of the psrf applied to the parameters  are consistent with the pairwise scatter plots where parameters with low variance converge first  while parameters with high variance require a long chain to converge . in general, the model results were sensitive to the initial concentrations of grb <dig>  shc, and sos as the posterior distributions were narrower than many of the other parameters shown in figure  <dig>  a similar observation was reported by kholodenko et al.  <cit>  and highlights how variations in protein expression influence cellular response. the rate of degradation of phosphorylated erbb <dig>  and the rate of ptp binding to phosphorylated erbb <dig>  exhibited the narrowest distributions of all of the parameters. despite the uncertainty in the parameter values, the model predictions are confined within a relatively narrow region . by marginalizing the predicted responses over p, this range in model predictions provided an estimate of p.

efficiency of sampling parameter space
high dimensional problems are particularly challenging for mcmc algorithms due to the large volume of parameter space that must be searched. the existence of correlation among the parameters and different timescales inherent in the problem compound this challenge. the contours of the proposal density distribution of Θ enclose an p-dimensional hyperellipsoid. the volume of this ellipsoid  is proportional to the determinant of the proposal density by:   

a plot of the right hand side of equation  <dig> shown as a function of amcmc step is shown in figure 8a. following the "learning" period, the proposal distribution rapidly expanded to reflect the structure of the cumulative markov chain and established a stationary proposal distribution after an additional  <dig>  steps. the structure of the proposal distribution was slightly different for each of the three parallel chains, as shown in figure 8a. in the amcmc algorithm, ∏ is obtained from the covariance of p. conceptually, an eigenvector/eigenvalue decomposition of the covariance of p provides a convenient framework to rationalize about the structure of the parameter space. in a well-posed problem, the unit orthogonal vectors  of the parameter space exhibit finite length . high correlation between model parameters effectively reduce the dimension of the parameter space and reduce the length of one or more of the orthogonal vectors to zero. this dimensional reduction implies that the hyperellipsoid lies wholly in a -dimensional subspace and the volume in p-space is zero. the efficiency of the amcmc algorithm is that it samples from the -dimensional subspace rather than p-space. the expansion and contraction of parameter space in the different orthogonal parameter directions, as shown by the change in eigenvalues as a function of amcmc step, is shown in figure 8b. as specified in the amcmc algorithm, the unit dimensions are constant during the "learning" period and adapt following this initial period.

model-based inference of cell signaling pathways
rationalizing about the biological mechanisms that underpin biological response can be aided using mathematical models. a mathematical model allows for the prediction of a series of response variables, , given a model topology, m, and a set of parameter values, Θ, determined from some calibration data, y. biological responses can be explained using mathematical models with finite accuracy. this accuracy is in part limited by uncertainty associated with measured data. the uncertainty in experimental measurement can be attributed to technical variation  or biological variation . the variance may vary between experimental assays of the same biological system and limit observing significant trends in the response variables . mathematically, the relationship between experimental observations, mathematical predictions, and uncertainty can be related by:   

where δ are systematic differences between the model and data and ϵ is a random error component that reflects the inherent uncertainty of a particular assay to probe biology. in analyzing measured data using mathematical models, it is typically assumed that ϵ is independently and identically distributed with a mean of zero and δ is negligible. these assumptions provide the basis for likelihood functions, such as equation  <dig>  and some cost functions, such as root mean squared error, used in optimization algorithms. optimization focuses on finding the best solution to a problem given a set of constraints, such as finding p such that ϵ is a minimum. optimization and other aspect related to fitting a model to data was recently reviewed with respect to systems biology  <cit> . however, there are two aspects of analyzing data using mathematical models that deserve further clarification with respect to the empirical bayesian approach described herein.

the first aspect is related to the model predictions, p. a single point estimate for p ) is the most commonly used approach to generate model predictions  <cit> . however, the model predictions are also dependent on the particular parameter values used in the prediction. a better estimate of model adequacy can be obtained by reporting the uncertainty associated with the predictions of the model given a range in plausible parameter values . the uncertainty in the predictions can be obtained by integrating over the posterior distribution in the parameter values:   

in practice, marginalizing the predicted response variables over the converged markov chain provided a numerical integration of equation  <dig> 

the second aspect is the uncertainty associated with what the model is unable to predict:   

the absence of systematic trends instills confidence in both the model predictions and the experimental data. variance that exhibits systematic trends ) implies that features of the biological system are not represented in the mathematical model or that inconsistencies exist within the experimental data. when systematic trends are observed, it may be tempting to discard the model as inadequate. however, a closer examination of δ may provide insight into aspects of the underlying biology as trends identify gaps in knowledge. these gaps in knowledge are also opportunities for discovery. for the egf model analyzed using the empirical bayesian approach, the normalized error, shown in figure  <dig>  provided an estimate of δ for each measurement used in calibrating the model. the distributions in δ for total erbb <dig>  total grb <dig> on erbb <dig>  total grb <dig> on shc, and total sos on erbb <dig> were distributed around zero, suggesting the absence of systematic trends. however, a common approach in the field is to collect calibration data from a variety of sources and systems due to the lack of data on a specific system. in that case, systematic trends may reflect inherent variation in response among systems. a more detailed analysis of the calibration data identified inconsistencies within the experimental data. the comparison among the different reported measurements of total py-erbb <dig> was interesting. the model predictions largely capture the values reported by markevich et al.  is centered around zero), consistently underpredict the values reported by moehren et al.  is greater than zero), and consistently overpredict the values reported by kholodenko et al. the earlier time points for erbb <dig> phosphorylation was reported by moehren et al. compared to the other studies while the data reported by kholodenko appears to exhibit a more rapid decline than observed by markevich et al. . it may seem that an increase in the egf binding rate may provide better agreement with the moehren et al. data. however, the rate constant for egf binding was obtained from the literature  <cit>  and is a factor of  <dig> slower than the other steps that lead to erbb <dig> phosphorylation. additional studies may be needed to rectify these seemingly conflicting observations.

differences between the expected behaviors and new data identify areas where our understanding of the system is inadequate and reveal novel aspects of biology  <cit> . in the case of signal transduction models, systematic differences may be used to discriminate between competing mechanistic descriptions, such as between recent bench experiments that report aspects of ras activation  <cit>  or of dose-dependent aspects of egf stimulation  <cit>  and those represented in current models  <cit> . while developing an empirical bayesian approach was the primary focus of this work, two additional points deserve mention in analyzing the egf signaling data. first, the lack of convergence for the ras predictions suggests that additional information, such as a more quantitative estimate for ras expression levels within rat primary hepatocytes, are required before a level of belief can be stated regarding the agreement between the new mechanistic aspect of ras activation and the observed dynamics. second, an underlying assumption is this model is that all of the reactants are in the same compartment . however, a common theme for a variety of signaling receptors suggests that cellular response to receptor stimulation is dependent on the receptor's subcellular location . regarding egf signaling, vieira et al. reported that plcγ exhibited an increase in phosphorylation in cells with defective clathrin-mediated endocytosis  <cit> . this underlying biology may contribute to the inability for the plcγ predictions to converge. similar to the ras predictions, quantitative estimates for plcγ expression levels within rat primary hepatocytes may help improve the model predictions for plcγ. finally, this work helps set the framework for coupling empirical bayesian techniques with information theory, such as the akaike information criterion  <cit>  or bayes factor , to establish a level of confidence with distinguishing between competing mechanisms of information flow.

a recent study by chen and coworkers elevates the standard for developing and testing mathematical models of cell signaling networks  <cit> . in this paper, the uncertainty associated a mathematical model of the canonical pathways associated with the egf family of receptors, erbb <dig> - erbb <dig>  was estimated using a ensemble of parameter point estimates obtained by simulated annealing. while stochastic sampling of parameter space is an essential requirement for an algorithm to navigate successfully within a rugged landscape with multiple local minima, using a model to make inferences about cell signaling networks requires that the observed samples are representative samples derived from the posterior distribution.

while the dimensionality of the model described by chen et al. may be too computationally expensive for a conventional bayesian approach, amcmc techniques, as described here, provide an attractive approach to sample high-dimensional parameter space efficiently. moreover, demonstrating that markov chain is a representative sample of the posterior distribution with a convergence criterion applied to the parameters can make a conventional bayesian approach computationally intractable due to the presence of different timescales. as described here, the convergence criterion can be applied to the model predictions thereby simplifying the computational burden of the approach.

CONCLUSIONS
a common challenge in systems biology is to infer mechanistic descriptions of biological process given limited observations of a biological system  <cit> . in the area of signal transduction, mathematical models are frequently used to represent a belief about the causal relationships among proteins within a signaling network. the simulated dynamics of these models are dependent on the topology of the signaling network and the parameters associated with each protein-protein interaction. an outstanding question is whether a specific topology can capture the observed biology given a range of plausible parameter values. by integrating over the range of plausible parameter values, we may establish a level of confidence with a specific topology. if the level of confidence is low, we may learn that we have left out a critical link within the signaling network. a bayesian perspective may be particularly helpful for determining aspects of the biology that are inconsistent with the stated hypothesis about how information flows within a network. as illustrated by the example developed in this paper, models of signal transduction frequently synthesize information derived from a variety of experimental studies. tools, such as the amcmc algorithm and gelman-rubin convergence metrics, that facilitate evaluating different sources for this information may also be helpful. in summary, an empirical bayesian approach was developed for inferring the confidence in a particular model for describing signal transduction mechanisms and for inferring inconsistencies in experimental measurements.

supplementary material
additional file 1
model definition. a series of tables that define the mathematical model, parameter values, and initial conditions used in this manuscript.

click here for file

 acknowledgements
this work was supported by grants from the phrma foundation, the national cancer institute r15ca <dig>  and the national institute of allergy and infectious diseases r56ai <dig>  the content is solely the responsibility of the author and does not necessarily represent the official views of the national cancer institute, the national institute of allergy and infectious diseases, or the national institutes of health.
