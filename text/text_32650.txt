BACKGROUND
an important aspect of the post-genomic era is to understand the biological effects of inherited variations between individuals. for instance, a key problem for the pharmaceutical industry is to understand variations in drug treatment responses among individuals at the molecular level. a single nucleotide polymorphism  is a mutation, such as an insertion, deletion or substitution, observed in the genomic dna of individuals of the same species. when the snp results in an amino acid substitution in the protein product of the gene, it is called a missense mutation. a missense mutation can have various phenotypic effects although we restrict ourselves here to the simplified task of predicting whether a missense mutation has an effect or no effect on protein function.

the wealth of snp data now available  <cit>  has prompted a number of studies on the functional consequences of snps. for example, wang and moult  <cit>  and ramensky et al.  <cit>  showed that most of the detrimental missense mutations affect protein function indirectly through effects on protein structural stability particularly disruption to the protein hydrophobic core. the evolutionary properties of the mutated residue may also be important determinants of its effect on protein function  <cit> , since conserved amino acids tend to be functionally important or critical in maintaining structural integrity. a number of groups have developed strategies to predict the effects of missense mutations by using structural or evolutionary information, or a combination of both. most of these methods claim prediction accuracies of between  <dig> – 80% although comparison is extremely difficult due to the use of different data sets and criteria for assigning a mutation as having an effect or not. chasman and adams  <cit>  proposed a probabilistic method, and krishnan and westhead  <cit>  evaluated decision trees and support vector machines. herrgard et al.  <cit>  used structural motifs called fuzzy functional forms to predict the effects of amino acid mutations on enzyme catalytic activity. deleterious human alleles were predicted by sunyaev et al.  <cit>  using mostly structural information. by contrast,  <cit>  used purely sequence homology data in their sift  algorithm, although adding structural information resulted in significant improvements  <cit> . subsequent work has compared sift to svms and random forests  <cit> . cai et al.  <cit>  used a bayesian framework to predict pathogenic snps. verzilli et al.  <cit>  applied a hierarchical bayesian multivariate adaptive regression spline  model for binary classification of the functional consequences of snps. within this model, samples from the posterior distribution were used to highlight properties of the mutated residue that are most important in predicting its effect on protein function.

all these methods require either structural or evolutionary data to be available for predictions to be possible. however, there are many proteins that lack any detectable sequence homology to known proteins or a solved 3d structure. in these cases, many prediction methods break down. therefore a method is needed that can combine both structural and evolutionary information but at the same time tolerate the absence of either without manual intervention. with this in mind we have applied bayesian networks to the problem of predicting the consequences of a missense mutation on protein function. bayesian networks are probabilistic graphical models which provide a neat compact representation for expressing joint probability distributions and inference. the representation and use of probability theory makes bayesian networks suitable for learning from incomplete datasets, expressing causal relationships, combining domain knowledge and data, and avoiding over-fitting a model to training data. as such, a host of applications in computational biology  have used bayesian networks and bayesian learning methodologies  <cit> . our detailed evaluation of bayesian network performance in this work is likely to be valuable to many groups working with bayesian networks and biological data.

bayesian networks
our recent primer  <cit>  introduces bayesian networks to the computational biologist. briefly, given a set of variables x = {x <dig> ..., xn}, which are represented as nodes in the bayesian network, a set of directed edges representing relationships between nodes can be defined in a graph structure. to allow efficient inference and learning, a directed acyclic graph  must be formed, which exploits the conditional independence relations between variables. using this model structure, model parameters θ in the form of conditional probability distributions  between the connected variables may be learned. with discrete data, these model parameters take the form of conditional probability tables . throughout this work, we have used the bayes net toolbox for matlab   <cit> . the code used to produce the results presented in this paper is available on request from the authors.

learning from complete data
the bayesian learning paradigm can be summarised as:

p = ∫ppdθ

i.e., the predictive distribution for a new example observation, given a set of training examples d can be calculated by averaging over all possible models θ the likelihood of the example x given the model, multiplied by the likelihood of the model given the training data. for a given model structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ the model θ can be thought of as the model parameters that encode the conditional probability distributions between variables and their parents in s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@.

learning from incomplete data
one advantage of using bayesian networks is that it is possible to learn model parameters from incomplete training data i.e. in cases where variables are missing. to learn from incomplete data, we used the expectation-maximisation  algorithm, which estimates missing values by computing the expected values and updating parameters using these expected values as if they were observed values.

structure learning
a fully connected network structure captures relationships  between all of the variables. a simpler, more compact model may be produced if conditional independencies between variables are learned. to do this, we used the greedy search algorithm from the matlab-based structure learning package   <cit>  with tabular cpds and uninformative dirichlet priors . the greedy search algorithm starts with a graph with no edges between the nodes, and aims to maximise a score function: either the full bayesian posterior or the bayesian information criterion . at each stage, the neighbourhood of the current graph  are considered, and the one with the highest score is chosen, until convergence. we use the notation of heckerman, where s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@h is a model structure hypothesis. from bayes' theorem the posterior distribution for network structures p is proportional to the marginal likelihood of the data p. the full bayesian posterior can be calculated , or the bic approximation can be used, which contains a term to describe how well the maximum likelihood model θ^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwf4oqcgaqcaaaa@2e79@s for structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@h predicts the data d, and a term that punishes model complexity. for a model with d parameters, built from n samples, the bic score is:

ln⁡p≈ln⁡p−d2ln⁡n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaacyggsbabcqggubgbcqwgwbaccqggoaakcqwgebarcqgg8bafimaacqwfse=udaahaawcbeqaaiabdigaobaakiabcmcapiabgiki7kgbcygasjabc6gaujabdchawjabcicaoiabdseaejabcyha8hgaciqb+h7axzaajawaasbaasqaaiabdohazbqabagccqggsaalcqwfse=udaahaawcbeqaaiabdigaobaakiabcmcapiabgkhitmaalaaabagaemizaqgabagaegomaidaaigbcygasjabc6gaujabd6eaobaa@5b51@

inference with missing data
knowledge of the conditional probability distributions between variables allows us to make predictions about the expected states of variables even if some variables are missing from the test data. for example, if structural information about a test missense mutation is not available, we can still infer whether the mutation has a functional effect on the protein or not by marginalising over the unknown variables. this is illustrated in a very simple bayesian network with three nodes, a, b, c, which can take the values {a <dig> ..., ana
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwghbqydawgaawcbagaemota40aasbaawqaaiabdgeabbqabaaaleqaaaaa@308b@}, {b <dig> ..., bnb
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgibgydawgaawcbagaemota40aasbaawqaaiabdkeacbqabaaaleqaaaaa@308f@}, and {c <dig> ..., cnc
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgjbwydawgaawcbagaemota40aasbaawqaaiabdoeadbqabaaaleqaaaaa@3093@} respectively and a structure given by figure  <dig>  the joint probability over all the variables is:

p = ppp

each of the probabilities can be expressed as a conditional probability table in this discrete case. if we wish to infer the value of c given a = ai and b = bj then we can calculate the probability of c taking each of the possible values, c = ck for k =  <dig> ..., nc by p read from cpts. if we wish to infer the value of c given only the value of a, we can marginalise over the unknown variables . thus:

p=∑bj∈bpp
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgwbaccqggoaakcqwgjbwydawgaawcbagaem4aasgabeaakiabcyha8jabdggahnaabaaaleaacqwgpbqaaeqaaogaeiykakiaeyypa0zaaabuaeaacqwgwbaccqggoaakcqwgibgydawgaawcbagaemoaaogabeaakiabcyha8jabdggahnaabaaaleaacqwgpbqaaeqaaogaeiykakiaemicaanaeiikagiaem4yam2aasbaasqaaiabdugarbqabagccqgg8bafcqwghbqydawgaawcbagaemyaakgabeaakiabcycasiabdkgainaabaaaleaacqwgqbgaaeqaaogaeiykakcaleaacqwgibgydawgaaadbagaemoaaogabeaaliabgigiolabdkeacbqab0gaeyyeiuoaaaa@5815@

RESULTS
the systematic unbiased mutagenesis dataset of lac repressor  <cit>  and t <dig> lysozyme  <cit>  were used to train and validate the bayesian networks. classification of 'effect' and 'no effect' mutations was based on that of  <cit>  in which only those mutations resulting in a significant loss of function were considered 'effect' mutations. as a result, our lac repressor dataset consisted of  <dig> effect and  <dig> no effect mutations, and our t <dig> lysozyme dataset contained  <dig> effect and  <dig> no effect mutations.

a total of fourteen variables were used to predict whether or not a missense mutation affects protein function . all these variables have been implicated in previous studies as useful in discriminating 'effect' from 'no effect' mutations. six of the variables are continuous , the rest are discrete binary. the variables  can also be sorted into three groups based on the type of biological information they give: structural, evolutionary, or in the case of nrent structural and evolutionary information.

we used two basic types of bayesian network structure in this study: naïve and learned. in the naïve structure, the effect node is a parent to all the other nodes in the network structure. details of the learned structure are provided later. on each of these structures we performed seven experiments:

• all:all:  <dig> node network trained and tested using all  <dig> variables listed in table  <dig> 

• all: nos:  <dig> node network trained on all variables, tested with evolutionary information only .

• nos:nos:  <dig> node network  trained and tested with evolutionary information only.

• all:noe:  <dig> node network trained on all variables, tested with structural information only .

• noe:noe:  <dig> node network  trained and tested with structural information only.

• all:key:  <dig> node network trained on all variables, tested using three key variables . these key variables were identified by analysing a number of learned structures.

• key:key:  <dig> node network trained and tested using key variables only.

results of these experiments are presented in tables  <dig> and  <dig>  we carried out both homogeneous and heterogeneous cross-validation tests. homogeneous cross-validation was performed on both lysozyme and lac repressor datasets separately, and a mixed set in which the two datasets were pooled. in each case, data were randomised and divided into  <dig> equal parts. one part was used as the test set and the remainder as the training set. this procedure was repeated  <dig> times so that each example  was used exactly once for testing. the mean and standard deviation of the ten results were then calculated. in heterogeneous cross-validation, the data set of one protein  was used as the training set and that of the other protein  was used as the test set.

column:  trained on all variables, tested with all variables observed;  trained on all variables, tested without any structural information  – only evolutionary variables observed;  trained and tested using only five evolutionary nodes;  trained on all variables, tested without any evolutionary information  – only structural variables observed;  trained and tested using only eight structural nodes;  trained on all variables, tested with only key variables observed ;  trained and tested using only the three key variables.

see table  <dig> for column details. note that mcc score or effect rate cannot be shown if all mutations are predicted as 'no effect'.

naïve bayes classifier
all:all
as expected, overall error rates of less than 20% were achieved in all cross validation tests with the all:all model . these results are consistent with previous studies reporting accuracies of  <dig> – 80% on similar datasets using similar variables  <cit> . furthermore, all auc values , including those from heterogeneous cross validation were at least  <dig>  indicating a robust classifier despite the naïvety of the network structure. we therefore used results on the all:all model as a benchmark for the six other experiments.

missing structural information 
performance dropped significantly with a  <dig> node network utilising only evolutionary information , with most auc values reduced by over 10% from the all:all model. in particular, with homogeneous cross validation on lysozyme data auc value decreased from  <dig>  to  <dig> , and mcc value was as low as  <dig> . even when structural information was used in training the network , results were not improved possibly because variables are treated as independent in a naïve structure and so variables with missing values have little influence when they are marginalised over.

missing evolutionary information 
in contrast to results achieved without structural information, there was little or no effect on performance when evolutionary information was either missing during testing  or missing during both training and testing . again, due to the naïvety of the structure, similar results were achieved by the all:noe and noe:noe models with auc values of around  <dig>  and overall error rates below  <dig> .

overall, results suggest that structural information is more important than evolutionary information in predicting the functional consequences of a missense mutation in both lac repressor and t <dig> lysozyme, for the dataset used. indeed, although evolutionary information has some predictive power, utilising only structural information appears to be sufficient for accurate prediction, comparable to that of the all:all model.

a note on structural flexibility
it has previously been suggested that the b-factor and neighbourhood b-factor of the native amino acid are the most important predictors of functional effects of snps  <cit> . however, the need to use b-factor information limits a method to structures from x-ray crystallography; such information is not available for nmr structures . we found that removing the bf and nbf nodes from the all node network made little significant difference to overall performance with aucs ranging from  <dig>  to  <dig>  in homogeneous cross-validation and  <dig>  and  <dig>  in heterogeneous cross-validation . this suggests that accurate prediction is possible without using structural flexibility information, although that is not to say that structural flexibility is not important, rather, other variables have compensated effectively for its loss.

learned structure
using both the bayesian and bic scoring functions employed by the greedy search algorithm we learned structures from lac repressor and lysozyme datasets separately and the two datasets combined . as with the naïve bayes classifier, we evaluated each structure using both homogeneous ten-fold and heterogeneous cross-validation. there was little significant difference in performance between the two scoring functions, or between structures learned on different datasets. the main difference was in the number of edges in the resulting dags. for our mixed dataset, there were  <dig> edges with bic, and  <dig> with full bayesian scoring. using occam's razor, we prefer the simplest of equally good models, and take the bayesian network structure learned from the mixed dataset, using the bic scoring function, as our model structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@, which is illustrated in figure  <dig>  with a harsher penalty for extra edges, the dag learned using the bic scoring function, should contain edges which are more likely to be biologically meaningful. it is important to note that the bayesian networks with learned structure  capture the relationships between all the variables, and are not designed solely to discriminate for classification of a single variable based on the other variables. this is a significant advantage of the bayesian network: we can infer the value of any variable based on the value of any other variable, so we have constructed a model which can not only predict effect/no effect, but can infer the value of any of the attributes.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@. learned bayesian network structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ . key to node labels is shown in table  <dig> 

all:all
little significant improvement in homogeneous cross validation performance was gained from using structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@  over the simple naïve structure . this was because the naïve structure is specifically designed for classification, whereas our learned structure is the 'best' structure for capturing the relationships between all of the variables. the learned structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ performs as well in classification of effect as the naïve structure, but has the added advantage that it can be used to predict the values of any of the variables, from any of the other variables.

structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ appeared to perform worse than the naïve structure during heterogeneous cross-validation, especially when trained on lac repressor and tested on lysozyme data. here, auc decreased from  <dig>  to  <dig>  despite lower effect error rates at the selected threshold . the low auc value of  <dig>  may be deceptive since a significant number of points on the roc curve lie below the convex hull  and as such are non-optimal classifiers  <cit> . therefore, measuring the performance of a classifier which represents a single point on both the roc curve and the convex hull  was more useful in this case. as described in methods, we chose the point at cost ratio  <dig>   as this helps balance the 'effect' and 'no effect' misclassification error rates . at this selected threshold, overall error  and effect error rate  were lower for structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ than the naïve structure . however, mcc value was also lower  and no effect error rate was higher  which illustrates the difficulty in selecting a measure to compare different models not only between different studies but within the same study as well.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@. roc curve for learned structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ trained on lac repressor, tested on lysozyme. the blue line is the roc curve. the red line is the convex hull of the roc curve. the circled point which lies on both of these curves is the classifier with the selected threshold .

missing structural information 
the model learned from all the variables and tested using only evolutionary information  performed poorly achieving auc values less than  <dig>   and error rates above  <dig> . given the number of connections in s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ and the potential for inferring the missing structural information in the test data, the all:nos model was surprisingly worse than the nos:nos model .

there could be a number of reasons for the poor performance of the all:nos model. the model may have learned during training that structural information is more important to prediction than evolutionary information. consequently, without structural information during testing, the model has problems since it has down-weighted the evolutionary nodes relative to the structural nodes. alternatively, it may not be possible to accurately infer values at the structural nodes from evolutionary information. by contrast, it is essential that the nos:nos model makes full use of the evolutionary information since structural information is unavailable in both training and testing. even though cross validation results with nos:nos were worse than the all:all model with auc values ranging from  <dig>  –  <dig>  and overall error rates up to  <dig> , they were better than the all:nos since full weight is given to the evolutionary nodes.

missing evolutionary information 
when marginalising over unknown evolutionary variables , predictions in most cases were comparable to the all:all model. however, poor results were achieved during homogeneous cross validation on mixed data and heterogeneous cross validation, especially training on lysozyme and testing on lac repressor data . in these cases, it appears that values at the evolutionary nodes with missing data could not be predicted accurately from the structural information during testing thus confusing the model. as expected, the noe:noe model trained and tested using structural variables only performed as well as the all:all model across all cross validation tests .

tolerance to incomplete training data
bayesian networks are capable of learning model parameters from incomplete data. here we test the tolerance of the bayesian networks by training on incomplete data. in every training example, we hide n nodes . we do this for the naïve bayes classifier, and the learned structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@, and vary n from  <dig> to  <dig>  the cpts are learned using the iterative em algorithm on the missing values. figure  <dig> shows the results of homogeneous cross-validation when trained on incomplete data from the 'mixed' dataset, and tested when all nodes are observed. note that using this method, different sets of n nodes are chosen to have missing data between different training cases, therefore here we were testing the general ability of the bayesian network to tolerate incomplete data rather than the effect of when certain nodes were missing data in all examples .
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ with parameters learned from incomplete data. the auc  is plotted against the number of nodes  randomly chosen to have missing data within the test examples.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ structures  were robust to incomplete training data, with an area under the roc curve of over  <dig>  maintained even when nine of the fifteen nodes were not observed in every example. with very sparse data , the naïve bayes classifier performed better than the learned structure. this was probably because the conditional probability tables  of the naïve structure only model the relationship of effect with each variable, whereas the cpts of s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ depend on the relationship between multiple variables. from figure  <dig>  we can see that a number of nodes are dependent on three variables in s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@, which perhaps explains the performance decrease when the model is not trained on sets of four or more variables. for example, when  <dig> variables are missing, an auc value of  <dig>  is achieved by s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@, whereas when  <dig> variables are missing, performance decreases to that of random classifier . nevertheless, overall tolerance to incomplete training data by both bayesian networks was encouraging considering the potential sparsity of either evolutionary and structural information for a significant number of proteins, especially structural genomics targets. other machine learning methods such as svms or decision trees are unable to handle incomplete data in this way.

training set size
in order to assess how much data is needed for training the bayesian networks, sequential learning of the model parameters was performed. the 'mixed' dataset was divided into two. one half was used as the test validation set, and the bayesian networks were trained on the other half. figure  <dig> shows a plot of training set size vs. classifier performance, measured using area under the roc curve. the result is as expected. the naïve model  gradually improves its performance as its parameters are sequentially learned, with excellent performance after  <dig> examples . the learned bn structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ has  <dig> free parameters and it out performs the naïve classifier after  <dig> training examples.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ with parameters learned sequentially. the auc  is plotted against the number of training examples.

interpreting the structures
the learned structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ is one of many markov equivalent structures which could have been learned from this data. there are also many other network structures which could suitably encode the relationships between the variables. using markov chain monte carlo  methods, we constructed a set of 'good' model structures, and averaged over these models to form a posterior distribution of model structures. figure  <dig> shows a plot of the frequency of connections made in the set of 'good' structures from ten runs of the mcmc simulation over  <dig> samples, after a 'burnin' of  <dig> samples. the darker squares indicate a higher observed frequency of an edge connecting each pair of nodes. from this, one can identify strong relationships between highly correlated variables.

the use of mcmc methods to study the posterior distribution over networks has the advantage of revealing relationships between the input variables. for instance, in figure  <dig>  the top row shows which variables are most strongly related to effect, and this is used later to develop simplified classifiers.

however, biologically meaningful relationships between the other variables are also revealed. with the exception of the trivial relationship between ac and rac, the second row of figure  <dig> shows strong links between ac, nrent, and rent, reflecting a well known biochemical relationship between solvent accessibility of residues and phylogenetic variability: the solvent exposed surface loops of protein structures show greater evolutionary variability than the unexposed hydrophobic core residues. similar effects that concur with known protein chemistry relate measures of flexibility  to phylogenetic variability. equally understandable are the strong link between g and p residues in turns  and evolutionary conservation at the specific sequence position of g/p  but not to a neighbourhood measure , and the relationship between protein interface positions  and neighbourhood flexibility measures .

from figure  <dig>  one can see that the effect node has the strongest relationships with bur, nbf, ac, uslaa, and uslby . there are very few direct connections between effect and trn, hlx, ifc, cnsd, and ncnsd. as expected, nodes such as bf and nbf, and rent and nrent are highly correlated, which suggests some redundancy within the network and one node could be used to predict the value of the other. both structural and evolutionary information are represented by the nodes most frequently directly connected to effect, although the top three most common nodes, bur, nbf and ac, represent only structural information. this, together with the strong performance of the bayes classifier without evolutionary information , suggests that evolutionary properties of the mutated residue have little direct influence on prediction if structural information is present.

our finding that solvent accessible area of the native amino acid, whether the amino acid is charged at a buried site, and the flexibility of its structural neighbourhood are all important predictors of effect agrees to some extent with chasman and adams  who found that structure based accessibility and b-factor features have the most discriminatory power. the strong performance of accessibility measures probably reflects the finding of  <cit>  and  <cit>  that mutations affecting the hydrophobic core are more likely to destabilise the protein and thus affect function. perhaps mutations on the surface are more likely affect function if they are conserved, as suggested by the strong relationship between accessibility and phylogenetic entropy . conversely, whether or not the mutation breaks either a helix or turn does not appear to be critical to predicting effect although, again, secondary structure information may become more powerful when used in conjunction with other features.

a simplified bayesian network
whilst the nodes directly connected to the effect node are not essential to prediction if certain other nodes are present , in theory, the value of the effect node can be predicted using only the nodes which are directly connected to it in the learned structures. the other variables become d-separated from effect; i.e. with a structure, and the conditional independence relations it implies, the effect node is conditionally dependent on only the nodes it is connected to when they are observed.

we tested this hypothesis by constructing two simple four node networks: a naïve structure , and structure s
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ <dig>  learned using the greedy search algorithm and the bic scoring function as above. these networks consisted of only the three nodes, bur, nbf and ac, with the strongest relationships with effect as shown in figure  <dig> 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfse=uaaa@3845@ <dig> 

across all cross-validation tests, the four node naïve bayes classifier trained and tested using only the three key variables  performed extremely well with only a minor decrease in performance over the all:all model. in homogeneous cross validation, auc values ranged from  <dig>  to  <dig>  and the maximum overall error rate was just  <dig> . in heterogeneous cross validation tests, the auc also remained high  with overall error rates as low as  <dig>  for training on lac repressor and testing on lysozyme data. there were no significant differences between the performance of the four node learned structure  and that of the naïve structure, which suggests little value in the connections between variables.

CONCLUSIONS
we have applied bayesian networks to the task of predicting whether or not missense mutations will affect protein function with comparable performance to other machine learning methods. however, the strength of the bayesian network lies in its ability to handle incomplete data and to encode relationships between variables; both of which were exploited here to derive some biological insight into how a missense mutation affects protein function.

a number of models were learned in this work. due to the unbalanced datasets we analysed roc curves and selected a suitable cost ratio in order to choose a probability threshold for the classifiers. this allowed us to compare classifiers in a meaningful way. from this analysis we concluded that a naïve network structure is sufficient for accurate prediction of the effect of a missense mutation with auc values around  <dig> . we also found that the structural environment of the amino acid is a far better predictor of the functional consequences of a missense mutation than phylogenetic information. this was demonstrated by the more accurate performance of a naïve classifier that just uses structural information compared to that which uses just evolutionary information. there were no significant performance gains when using a learned network structure, however the learned structure did allow relationships between variables to be analysed, in particular by analysing the posterior distribution of model structures, we found the top three strongest connections with the effect node all involved structural nodes. with this in mind, we derived a simplified bayesian network that used just these three structural descriptors  without significant decrease in performance. given the importance of structure, it would be interesting to learn if certain amino acid changes are more predictive of effect than others. for example, both cysteine, which forms disulphide bridges, and proline, with its unique ring structure, are often critical to the integrity of a protein structure so one would expect a mutation involving either of these residues to change the structure significantly. this will provide the basis for future work.

