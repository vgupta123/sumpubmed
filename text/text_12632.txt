BACKGROUND
bioimage informatics has become an increasingly important field in recent years owing to the advances in microscopy imaging technologies  <cit> . automated analysis of microscopy images helps to achieve objective and consistent cell phenotype recognition, subcellular localization, and histopathological classification, which are critical to many biological studies . in particular, classification of biological images is an essential algorithmic component in such computed-aided analyses.

existing bioimage classification methods typically comprise two main components: feature extraction and classification. the descriptiveness of feature descriptors is essential to the bioimage classification performance, and much work has been conducted for bioimage feature extraction. the feature descriptors are often multi-modal, i.e. integrating multiple different types of features. the commonly used features include the traditional approaches such as gabor filters and haralick textures, and more recent descriptors such as the scale-invariant feature transform  and local binary patterns  . to further enhance the discriminative power of feature descriptors for specific problem domains, customized features have been designed manually  or learned automatically .

with these extracted descriptors, supervised classification models such as support vector machine  , subspace learning , multiple instance learning  <cit>  and sparse representation  <cit>  are applied. however, the classification performance is often largely affected by the small number of training data available for bioimage research. for example, in the iicbu  <dig> database  <cit> , each classification task contains only hundreds of images categorized into multiple classes. the small amount of training data would not well represent the feature space characteristics, and the trained classifier could easily overfit especially with high-dimensional descriptors. the applicability and effectiveness of automated feature learning algorithms would be limited as well. dimension reduction techniques are sometimes used in bioimage classification to avoid the over-fitting problem  <cit> . however, the effect of applying these techniques is often not evaluated and the result could actually be negatively affected due to undesirable information loss.

in the broader computer vision field, high-dimensional descriptors, such as the improved fisher vector   <cit> , have become increasingly popular. ifv can be considered as a variation of the bag-of-words  encoding of local descriptors. while bow assigns the descriptors to a pool of visual words and computes the histogram distribution of the visual words, ifv formulates gaussian mixture models from the descriptors and computes the first- and second-order statistics for each feature dimension. with the original ifv, the local descriptors used are dense sift features, and the resultant feature dimension of ifv is much higher than bow. ifv has shown excellent discriminative performance for face recognition, object detection and texture classification .

in addition, descriptors that incorporate spatial pooling of local descriptors   <cit> , gist  <cit>  and census transform histogram   <cit> ) can have high dimensions as well. with spatial pooling, an image is partitioned into a grid or hierarchy of regions and descriptors computed at the region-level are concatenated to form a long overall descriptor, so that the overall descriptor captures the spatial structure of the image to some extent. recent studies show that high-dimensional descriptors often provide more discriminative power than the lower-dimensional counterparts .

with high-dimensional features, especially ifv, the linear-kernel svm classifier is often found to be the most effective and efficient way of training and classification  <cit> . furthermore, it has been reported that better classification results can be achieved with properly designed dimension reduction techniques . for example, a discriminative metric learning method has been proposed to reduce the dimension of ifv features and improve the classification performance for face images  <cit> . dictionary learning methods have also been used to enhance the discriminative power of feature representations. examples include the fisher discrimination dictionary learning   <cit> , and latent dictionary learning   <cit> . they can also be used for dimensionality reduction when the dictionary size is smaller than the descriptor’s dimension.

in this study, we propose a new bioimage classification method. we have two main methodological contributions. first, we find that by combining ifv  with lbp, hog, centrist and gist texture features, the resultant high-dimensional multi-modal descriptor is highly discriminative for a wide variety of microscopy images and classification objectives at the subcellular, cellular, and tissue levels. while ifv has recently been adopted into bioimage classification  <cit> , these approaches are designed for specific problem domains, i.e. hep- <dig> cell classification and ovarian cancer classification, whereas our design is validated on a number of classification tasks and we identify a set of texture features that provide complementary information to ifv.

second, to further enhance the discriminative power of the descriptors, we design a subcategory discriminant transform  method to transform the descriptors before performing classification. sdt has a similar learning objective to linear discriminant analysis  in that the descriptor transform is aimed to minimize the within-class variation and maximize the between-class difference. however, sdt performs feature transformation with learned convolution kernels without dimensionality reduction, and the learning objective is modelled based on subcategories rather than the class-level information. we also compare sdt with the popular discriminative dimension reduction   <cit> , full matrix learning   <cit> ), and dictionary learning  techniques, and show consistent advantage over them.

for evaluation, we use the publicly available iicbu  <dig> database  <cit>  to perform eight different multi-class classification tasks in the areas of subcellular localization, phenotype recognition and histopathological classification. figure  <dig> shows the sample images. we achieve improved classification performance over the current state-of-the-art for six tasks.
fig.  <dig> sample images of the iicbu database. eight datasets from the iicbu  <dig> database are used in this study, including the 2d hela, cho, rnai, muscle aging, terminal bulb aging, lymphoma, liver gender, and liver aging datasets. each image represents one image class, and the class name is annotated under the image. summary of the dataset properties is listed in table  <dig>  comprehensive description of the database can be found in  <cit> 


dic: differential inference contrast; cho: chinese hamster ovary; al: ad-libitum




methods
given a set of training images, we first extract the visual descriptors of each image. the kernels of sdt are learned from this training set and the visual descriptors are transformed with these kernels. a multi-class linear-kernel svm model is then learned based on the transformed visual descriptors. to classify a test image, we extract its visual descriptor, then perform sdt with the learned kernels, and finally obtain its class label with the learned svm model. figure  <dig> illustrates the overall design of our method.
fig.  <dig> illustration of our method design. during the training process, the high-dimensional visual descriptors of the training images are extracted, and the sdt models are learned. the visual descriptors of the training images are then transformed using the learned sdt models, and a linear-kernel svm is learned from the transformed descriptors. during the testing process, the high-dimensional visual descriptor of the test image is extracted, and then transformed using the learned sdt model. the transformed descriptor is then classified using the learned svm to produce the classification output




visual descriptor extraction
to represent the biological images, we propose to use a combination of five visual descriptors: ifv, lbp, hog, gist and centrist. they describe different aspects of the visual characteristics, and we expect their combination to give a more comprehensive image description compared to the individual descriptors. while lbp and hog are often used in bioimage studies  <cit> , the other descriptors have rarely been used in this field.

the ifv descriptor computes the image-level statistics of a dense set of local sift descriptors with fisher encoding. specifically, local sift descriptors are extracted densely at multiple scales with the width of the sift spatial bins set to  <dig>   <dig>   <dig>   <dig> and  <dig> pixels and sampled every two pixels. these 128-dimensional sift descriptors are reduced to  <dig> dimensions using pca and a gaussian mixture model  with  <dig> gaussian components built to obtain the fisher encoding. this process produces a 64×64× <dig> dimensional ifv descriptor.

to compute the lbp, hog and gist descriptors, an image is divided into 4× <dig> grid of cells. for each cell, a 58-dimensional lbp , 31-dimensional hog , and 32-dimensional gist  descriptors are extracted. consequently, the lbp, hog and gist descriptors are of 58× <dig>  31× <dig> and 32× <dig> dimensions, respectively. these descriptors encode the texture and shape features with spatial information provided by the cell subdivision. in addition, a 256-dimensional centrist descriptor is extracted globally for the whole image, and is used to capture the overall structural information. each descriptor is also l <dig> normalized.


block structure of descriptors. we note that each feature descriptor can be partitioned into multiple blocks of feature vectors. for example, the ifv descriptor can be partitioned into 64×2= <dig> numbers of 64-dimensional  vectors. the lbp, hog and gist descriptors can be partitioned by the cells into  <dig> vectors of  <dig>   <dig> or  <dig> dimensions, respectively. the histogram-based centrist descriptor can be artificially partitioned into four 64-dimensional vectors. therefore, the descriptor x can be rewritten as x={x
b:b= <dig> …,b}, meaning that x is constructed by concatenating b=128+16×3+4= <dig> feature vectors and each vector xb∈ℝc× <dig> is of c=  <dig>   <dig>   <dig>   <dig> or  <dig> dimensions. this block-wise representation of descriptors is useful in our proposed sdt algorithm as described in the next section.

subcategory discriminant transform
while the visual descriptors can be directly used to train a classification model and classify the test images, we design a sdt algorithm to further enhance the discriminative power of the visual descriptors. formally, let x∈ℝh× <dig> denote the visual descriptor of an image with h dimensions. the objective of sdt is to transform x to a more discriminative descriptor y∈ℝh× <dig>  to do this, for each feature vector x
b, we compute a transformed vector y
b=x
b∗f
b with ∗ as the convolution operator and f
b as the convolution kernel, and y
b has the same dimension as x
b. then the transformed descriptor is the concatenation of b transformed vectors, i.e. y={y
b:b= <dig> …,b}. figure  <dig> illustrates our design of the sdt algorithm.
fig.  <dig> illustration of the visual descriptor and subcategory discriminant transform. a total of  <dig> convolution kernels are learned with each kernel corresponding to one block of feature vector within the high-dimensional visual descriptor




we design a subcategory-based discriminative learning model to derive the convolution kernels {f
b:b= <dig> …,b}. specifically, a kernel is defined as an d-dimensional vector fb∈ℝd× <dig>  d<c. the kernel is supposed to reduce the within-class feature variation and increase the between-class feature difference, so that the transformed vector y
b is more discriminative. in other words, we consider that generally  images of the same class would show inhomogeneous visual features and can be grouped into subcategories, and  images of different classes could show similar visual features and the subcategories from different classes could overlap in the feature space. by transforming x
b into y
b, we expect that  the subcategories of the same class are closer hence the within-class variation is reduced, and  the subcategories between different classes are more separated hence the between-class difference is increased.

formally, we denote a set of n training images as {x:n= <dig> …,n} with n as the index of the training images. we first cluster the training images into a set of subcategories. each subcategory contains images from the same class and exhibiting similar visual descriptors. the subcategories are denoted as {s
l,k:l= <dig> …,l,k= <dig> …,k
l}, and a subcategory s
l,k is identified by its class label l and index k within the class l, with l denoting the number of classes and k
l denoting the number of subcategories in class l.

the subcategories are generated by first clustering all training data {x:n= <dig> …,n} into l clusters regardless of the class labels using the locality constrained subspace clustering  method  <cit> . the method first constructs an affinity matrix using the locality-constrained linear coding   <cit> , then spectral clustering is performed on the affinity matrix to obtain the clusters. this lsc method is essentially similar to the popular sparse subspace clustering   <cit> , but llc is used in place of the standard sparse representation for efficient computation of the affinity matrix. in our study, we set the sparsity constant to  <dig> and the balance parameter to 1e− <dig> for all datasets. then, for each cluster, the data belonging to the same class are extracted as a subcategory. in this way, we avoid choosing the number of subcategories for each class, but just use l  for clustering. note that because of the large within-class variation and small between-class difference, each cluster typically contains data from a number of classes and the number of subcategories for each class is close to l.

based on the subcategories, we define the within-class variation vwithb  and between-class difference vbetwb  for the transformed feature vectors {y
b:n= <dig> …,n} as: 
  <dig> vwithb=∑l=1l∑k=1kl|sl,k|kl−1∑k′=1klθl,kb−θl,k′b <dig> 



  <dig> vbetwb=∑l=1l∑k=1kl|sl,k|∑l′=1lkl′∑l′=1l∑k′=1kl′θl,kb−θl′,k′b <dig> l′≠l 


where |s
l,k| is the number of descriptors in the subcategory s
l,k. θl,kb is the mean element  of the transformed vectors derived from the subcategory s
l,k: 
  <dig> θl,kb=1c|sl,k|∑m=1|sl,k|∑i=1cyb,i 


where i indexes the feature elements in the c-dimensional vector y
b, and m indexes the descriptors in the subcategory s
l,k.

recall that the transformation is computed by y
b=x
b∗f
b. from an element-wise perspective, it can be considered that the individual element x
b,i∈x
b is transformed to yb,i=hb,itjdfb, where hb,i∈ℝd× <dig> is the vector of length d extracted from x
b with its center at the ith element of x
b. j
d is a d×d dimensional exchange matrix, so that f
b is flipped vertically for computing the convolution. the mean element θl,kb is then equivalent to: 
  <dig> θl,kb=1c|sl,k|∑m=1|sl,k|∑i=1chb,itjdfb 


for simpler notation, we rewrite eq.  as: 
  <dig> θl,kb=γl,kbfb,γl,kb=1c|sl,k|∑m=1|sl,k|∑i=1chb,itjd 


with γl,kb∈ℝ1×d. by substituting eq.  into eqs.  and , we obtain: 
  <dig> vwithb=fbt∑l=1l∑k=1kl|sl,k|kl−1∑k′=1klγl,kb−γl,k′btγl,kb−γl,k′bfb 



  <dig> vbetwb=fbt∑l=1l∑k=1kl|sl,k|∑l′=1lkl′∑l′=1l∑k′=1kl′γl,kb−γl′,k′btγl,kb−γl′,k′bfb,l′≠l 


in these formulations, the terms within the brackets can be readily computed from the training data. in other words, we can rewrite vwithb=fbtuwithbfb and vbetwb=fbtubetwbfb, in which uwithb and ubetwb are obtained from the feature vectors {x
b} and the unknown f
b is isolated. here uwithb and ubetwb are square matrices with dimension d×d.

by optimizing f
b, we expect to maximize the between-class difference and minimize the within-class variation. this is formulated as: 
  <dig> argmaxfbfbtubetwbfbfbtuwithbfb 


the kernel f
b is then derived by solving the generalized eigenvector problem: 
  <dig> ubetwbfb=λuwithbfb 


where λ is the generalized eigenvalue.

with the learned kernels {f
b:b= <dig> …,b}, we obtain the transformed vectors {y
b:b= <dig> …,b}. each vector y
b is then rescaled so that its norm is the same as that of x
b. in this way, the relative magnitudes and contributions of the various vectors in the transformed descriptor y remain unchanged when compared to the original descriptor x. the transformed descriptor y is finally obtained by concatenating the rescaled vectors. based on these transformed descriptors, a linear-kernel multi-class svm classifier is trained and then applied to classify the testing data.

iicbu database and implementation
we used the publicly available iicbu  <dig> database for our experiments. this database contains  <dig> separate datasets of different bioimage classification problems, and represents a broad range of real-life biological problems identified by experimental biologists. each dataset features organelles, cells, or tissues, and includes noisy and unusual images typically present in biological databases. excellent classification performance with near 100% accuracy has been reported for three datasets. we thus focused on the other eight datasets. table  <dig> lists the main properties of the eight datasets. example images are shown in fig.  <dig>  the lymphoma, liver gender and liver aging datasets provide color images. we converted them into grayscale using the standard rgb to grayscale conversion. for datasets with large intensity ranges, we rescaled the image intensities linearly to the range of  <dig> to  <dig>  before extracting the visual descriptors.

for each dataset, we used five-fold cross validation for training and testing; and ten splits were used to measure the overall classification accuracy. for feature extraction and subcategory generation, we performed initial study on the terminal bulb aging dataset and experimented with parameter settings that were commonly used in related literature. the chosen parameter settings for this dataset were then used for all the datasets to maintain a common approach across datasets. the values of parameters have been described in the previous sections on visual descriptor extraction and subcategory generation. for sdt, the only parameter is the size d of the convolution kernel f
b. the kernels applied to different feature modalities can have different sizes, hence there are a total of five size parameters to select. to do this, we specified that d∈ <cit> , and employed a sequential search approach by fixing four size parameters while selecting the best setting for the fifth one. the sequential search was conducted on the training data, by performing multiple runs of four-fold cross validation within the training set and choosing the parameter d based on majority voting from these runs.

RESULTS
classification results using various descriptorsfig.  <dig> classification results comparing various combinations of visual descriptors. results are obtained by classifying the descriptors using  svm only without sdt and  our method that applies sdt before svm classification. the feature names ifv, lbp, hog, centrist and gist are shortened as i, l, h, c and g, respectively. the mean accuracy and standard error are shown




overall, the results suggest that the ilhc descriptor should be used for most datasets ; and ilhcg, which includes the additional gist feature, is more effective for the muscle aging and terminal bulb aging datasets. the gist feature describes the dominant spatial structure of an image. it provides different perspectives of an image compared to ifv, lbp and hog, which summarizes the local visual information with varying encodings. it is also different from centrist, which captures the global shape information. it can be seen from fig.  <dig> that there is little spatial structural information in the 2d hela, cho, rnai, lymphoma, liver gender and liver aging datasets. adding gist could thus introduce noisy data and reduce the discriminative power of feature descriptors. however, for the muscle aging and terminal bulb aging datasets, the spatial distribution of tissues is an important clue for classification, and hence the ilhcg descriptor is more suitable.
fig.  <dig> classification results comparing different descriptors. results are obtained by classifying the descriptors using  svm only without sdt and  our method that applies sdt before svm classification. the features tested include ifv, l/h/c/g , features generated using the vgg-vd model pretrained on imagenet, and features generated using a cnn model trained on each dataset. the mean accuracy and standard error are shown




the high discriminative power of ifv also means that ifv is already highly optimized. as a result, the improvement by sdt on ifv descriptors was relatively low. it can be seen from fig.  <dig> that with ifv features, less than 1% improvement in classification accuracy was typically obtained when sdt was applied. however, the improvement by sdt on the multi-modal descriptor was apparent. for example, on the rnai and lymphoma datasets, the ilhc descriptor was the least effective when classified using svm only, but generated the highest classification performance when sdt was applied. on the 2d hela and terminal bulb aging datasets, the improvement from sdt was minimal when only ifv was used, whereas the improvement was much larger with the ilhc or ilhcg descriptor. we found that this was because the improvement by sdt on the lbp, hog, centrist and gist features was much higher  than that on the ifv feature . since ifv is a learning-based encoding of local descriptors and the other features are computed based on predefined rules, our subcategory-based discriminative learning method could provide much greater enhancement of the other texture features than the ifv feature. these improvements led to the overall improved classification results using the ilhc or ilhcg descriptors. for illustration, we show the results using l/h/c/g  in fig. 5; but similar improvements were obtained for subsets of l/h/c/g.

we also conducted paired t-test to verify the statistical significance of improvement by sdt for different descriptor choices. the classification outputs using svm classification only were compared with the outputs using our method, and the null hypothesis was that both approaches would provide the same results. with ifv as the feature descriptor, the p-value was around  <dig>  for all datasets , indicating that the improvement by sdt on ifv descriptors was not statistically significant. however, using the l/h/c/g descriptors, the improvement was statistically significant on all datasets, with p-values less than  <dig>  . using the ilhc or ilhcg descriptors, the p-values for the 2d hela, cho, terminal bulb aging, and lymphoma datasets were all less than  <dig>  , while the p-values for the rnai and muscle aging datasets were around  <dig> . for the muscle aging dataset, we suggest the insignificant improvement was mainly because the classification performance using svm only was already very high and there was limited scope of improvement for sdt. for the rnai dataset, even with the high degree of improvement by sdt on the l/h/c/g descriptors, the classification performance by the l/h/c/g descriptors was still low, and hence such improvement could only boost the final classification results slightly. also, in the rnai dataset, each image class contains only  <dig> images. this small amount of images would limit the learning capability of sdt, and this could be an important factor affecting the degree of improvement provided by sdt on this dataset.

for further evaluation, we tested two deep learning approaches with convolutional neural network  to generate the feature descriptors, which have recently become popular in biomedical imaging analysis. first, we applied the very deep vgg-vd model  <cit>  that was pretrained on imagenet. we chose the vgg-vd model since it reported the state-of-the-art performance on multiple benchmarks in general computer vision. second, we trained a cnn model for each dataset. the well-known alexnet  <cit>  was adopted as the base network, with two middle convolutional layers removed to reduce the network complexity to better accommodate the much smaller number of images in our datasets compared to imagenet; and affine transformation was used for data augmentation. in both cases, the 4096-dimensional descriptor from the last fully connected layer was used as the image descriptor.

in addition, we also tested various other features, including the more traditional encoding techniques of local sift descriptors  and llc), and surf, gray level co-occurrence matrix  and wavelet features. we found that ifv was more effective than the bof or llc encoded sift descriptors, with on average 15% improvement in classification accuracy. inclusion of surf, glcm or wavelet features degraded the classification performance by  <dig> – 10% compared to using lbp, hog, gist and centrist.

comparison with existing studiestable  <dig> classification accuracies  compared to the existing studies

 <dig>  ± <dig> 
 <dig>  ± <dig> 
 <dig>  ± <dig> 
 <dig>  ± <dig> 
 <dig> 
 <dig>  ± <dig> 
 <dig>  ± <dig> 
 <dig> 
 <dig>  ± <dig> 
 <dig>  ± <dig> 
 <dig>  ± <dig> 
 <dig>  ± <dig> 
 <dig>  ± <dig> 
 <dig>  ± <dig> 
boldface indicates the best result on the individual dataset




for the terminal bulb aging dataset, our classification result was  <dig> % lower than the state-of-the-art  <cit> . the approach  <cit>  involves a multi-level classification model with additional middle classes. it is however unknown how this approach would perform for the other datasets. to further analyze our method performance, we generated the confusion matrices using the svm only approach and our method, as shown in fig.  <dig>  a comparison between the two confusion matrices shows that our method obtained consistent improvement for each class over the svm only results. the first two classes were easily differentiated from the other classes with distinct structural patterns in the tissues  are shown


fig.  <dig> visual results on the terminal bulb aging dataset. the samples show a images of class ‘day 0’ and ‘day 2’ that are correctly classified; b cases where class ‘day 4’  or ‘day 6’  are classified as ‘day 4’  or ‘day 6’ ; and c cases where class ‘day 10’  or ‘day 12’  are classified as ‘day 10’  or ‘day 12’ . in b and c, the two off-diagonal images illustrate the misclassification cases




for the rnai dataset, the compared approaches  <cit>  outperformed our method. in particular, the approach  <cit>  included a shape-based feature design based on the glcm representation, and provided the state-of-the-art classification result with  <dig> % advantage over our method. however, the approaches  <cit>  were less effective for the 2d hela and cho datasets and hence less generalizable when compared to our method. we suggest that the main uniqueness of the rnai dataset is that the images contain many well isolated cells and the spatial structure is not informative when classifying the images. these characteristics thus limited the effectiveness of our feature descriptor for the rnai dataset. figure  <dig> shows the confusion matrix obtained using our method. the lowest classification accuracy was observed for the class of gene cg <dig>  and 20% of it was misclassified as cg <dig>  example of misclassification is shown in fig.  <dig>  it can be seen that while the two genes have different functionalities, i.e. regulation of cell cycles or dna repair, they show similar visual characteristics and could be easily confused to the untrained eyes. based on these comparisons, we suggest that a customized feature design at the individual cell-level could be investigated for further improving the classification performance of the rnai dataset.
fig.  <dig> results on the rnai dataset. confusion matrix of the classification results is shown


fig.  <dig> visual results on the rnai dataset. the left and middle images are of class ‘cg3938’ and the right image is of class ‘cg7922’. the left and right images are correctly classified, but the middle image is misclassified as class ‘cg7922’




it is worth noting that in our experiments we conducted ten runs of five-fold cross validation, while the compared approaches used different cross validation techniques, as summarized in table  <dig>  for example, the approach  <cit>  used a single three-fold cross validation. as a result of the difference in evaluation protocol, their performance advantage of  <dig> % on the terminal bulb aging dataset might not be conclusive.



comparison with dimension reduction
while our sdt method does not involve dimension reduction, it adds an additional step prior to the svm classification, and has some similarity with lda that the transform tries to reduce the within-class variation and increase the between-class difference based on discriminative learning. in addition, considering the small number of images in the datasets, dimension reduction could be useful to address the over-fitting issue. we compared our method with several popular discriminative dimension reduction techniques: lda, gda  <cit> , fml  <cit> , and fddl  <cit> . gda is a kernelized version of lda. fml is based on distance metric learning and demonstrates good performance when coupled with ifv and svm  <cit> . fddl, being a supervised dictionary learning technique with fisher discrimination constraints, can be used for dimension reduction as well. to have direct comparison with sdt, the dimension reduced descriptors from lda, gda and fml were classified using linear-kernel svm. for fddl, we used the integrated sparse representation-based classification technique. the parameters used in these approaches were tuned manually. we also performed the paired t-test to verify the statistical significance of improvement of sdt over lda, gda, fddl and fml.

as shown in fig.  <dig>  when the classification accuracies were near 100% on the cho, liver gender and aging datasets, our method performed similarly to gda and fddl. the improvement provided by our method is more clearly observed for the other five datasets with non-saturated classification accuracies. compared to the svm only approach, fddl and gda resulted in slightly higher accuracies for some datasets while lda and fml often affected the results negatively. the advantage of sdt over lda, gda and fml was statistically significant with p-values less than  <dig>  for all five datasets. the improvement of sdt over fddl was statistically significant for the 2d hela, terminal bulb aging and lymphoma datasets. for the rnai and muscle aging datasets, fddl performed similarly to svm, and the improvement of sdt over fddl on these datasets was insignificant. to explain this insignificance, our previous discussion regarding the improvement of sdt over svm can be applied here as well. overall, these results demonstrate that it is not advantageous to apply dimension reduction techniques on the high-dimensional descriptor before svm classification. compared to the commonly used dimension reduction techniques, our sdt method could retain and enhance the discriminative information of the descriptors to a larger extent. therefore, it is generally more effective by first transforming the feature descriptors with our sdt model before svm classification.
fig.  <dig> classification results comparing our method with the other related techniques. results are obtained using our method , svm only classification, the discriminative dimension reduction techniques , and fdt that replaces the subcategory-based computation in sdt with a class-based formulation. the mean accuracy and standard error are shown




use of subcategories
in sdt, the within- and between-class variations are computed based on subcategories. to analyze the effect of such a subcategory-based model, we compared our results with an alternative approach that replaces the subcategory-level computation by class-level computation. we name this alternative approach fisher discriminant transform . briefly, we change eqs.  and  to measure the within- and between-class variations similarly to the standard fisher discrimination, by computing the distance between data samples and class means, and between class means and the overall mean. this transform also keeps the descriptor dimension unchanged. the subcategory information is not used in fdt, hence result comparison between fdt and our sdt method would indicate the use of subcategories in the feature transform.

regarding the clustering method for generating the subcategories, we chose the lsc algorithm mainly due to its efficiency. for example, on the smallest rnai dataset of  <dig> images, lsc clustering took about  <dig>  seconds while the well-known ssc method took about  <dig> minutes. ssc is similar to lsc except that ssc used the standard lasso for sparse representation while lsc used the highly efficient llc algorithm. such a long computation time prevented us from using ssc for our current study. in addition, k-means clustering is also quite efficient, requiring about  <dig>  seconds for the rnai dataset. however, the classification performance using k-means clustering was on average  <dig> % lower than that using lsc clustering for all datasets.

further analysis of sdt
to further elaborate the effect of our sdt method, we provide analysis of the intermediate results when applying the method. the underlying objective of sdt is to minimize the within-class and maximize the between-class differences. we thus measured the within- and between-class differences before and after applying sdt, and the margins between separation hyperplanes from the trained svms correspondingly. these measures help to demonstrate the use of sdt intuitively.

recall that our ilhcg descriptor x is partitioned into b blocks of feature vectors x={x
b:b= <dig> …,b} with b= <dig>  and the within- and between-class differences vbetwb and vwithb are derived for each vector x
b separately. we computed the ratio between vbetwb and vwithb for each x
b, and concatenated the ratios together as the overall measurement of the feature space distribution. figure 11
a shows the ratios obtained before and after applying sdt on the training set of the terminal bulb aging dataset. it can be seen that the ratios became higher after sdt was applied, indicating larger between-class difference compared to within-class variation. such an observation is aligned with the optimization objective of sdt in eq. . it was also found that in the original feature space, the separation margin between hyperplanes from the trained svm was on average  <dig> . in the transformed feature space after applying sdt, the separation margin became  <dig> . therefore, with sdt the transformed features could be better separated between different classes and more accurate classification became possible.
fig.  <dig> effects of sdt on feature space distribution. ratios between vbetwb and vwithb are computed before and after applying sdt, from a the training set and b the testing set of the terminal bulb aging dataset




on the other hand, an important aspect is if the effect of sdt shown on the training set is transferable to the testing set. to analyze this, we computed the ratios on the testing set. as shown in fig. 11
b, among the  <dig> ratios corresponding to  <dig> blocks of feature vectors,  <dig> ratios became higher after applying sdt. this helps to explain the higher classification performance using our method compared to using only svm without sdt as shown in fig.  <dig>  the overall increase in ratios after applying sdt was lower than those on the training set. this is because similar to the other discriminative learning techniques , the testing set typically exhibits some different feature space characteristics from the training set. such differences would subsequently affect the effectiveness of the learned model when applied to the testing set.

in addition, as a feature transform method, sdt can be considered related to the artificial neural network . specifically, in sdt, the learned kernels are convoluted with the feature vectors to generate the transformed descriptor; and in ann, the outputs of hidden layers are transformed descriptors with the weights optimized based on the classification objective. the main difference of sdt from ann is that our optimization objective is constructed based on the intra- and inter-class variations derived from subcategories and sdt is applied to each block of feature vectors rather than the whole feature descriptors.

to compare our optimization method with ann in a fair manner, we designed the following ann-based approach. first, since in our method we learned  <dig> convolution kernels for the  <dig> blocks of feature vectors, we trained  <dig> anns correspondingly as well. each ann was configured to have an input layer for the feature vectors, one hidden layer as the feature transform layer, and one output layer for classification. the hidden layer had the same number of nodes as the input feature vector, so that the feature dimension remained unchanged after this layer, simulating the effect of our sdt-based feature transform. after training the  <dig> anns, the transformed features generated at the hidden layer were then concatenated together to produce the transformed descriptors, which were then used to learn an svm classifier. for a testing image, the transformed descriptor was derived from the learned anns and then classified using the svm.

the results show that our sdt method outperformed this ann-based method on six datasets, with improvement in classification accuracy of  <dig> % ,  <dig> % ,  <dig> % ,  <dig> % ,  <dig> % , and  <dig> % . the same results were obtained on the liver gender and aging datasets. also, the ann-based method outperformed the svm only classification and dimension reduction techniques with  <dig>  to  <dig> % improvement for five datasets , while we found that a standard ann classification  underperformed the svm only classification by  <dig>  to 9%. this implies the advantage of having block-wise feature transform and the additional svm classification of the transformed descriptors. we suggest that it could be possible to further enhance the ann-based method with convolutional layers, and this would be an interesting direction in our future study.

CONCLUSIONS
in this paper, we present a general method for bioimage classification. the images are represented with highly discriminative high-dimensional visual descriptors comprising multi-modal features. a subcategory discriminant transform  method is designed to further enhance the discriminative power of the descriptors using block-wise convolution processing, in which the convolution kernels are learned based on subcategory-level discriminative modeling. we have evaluated our method on the publicly available iicbu  <dig> database, which contains eight individual datasets of various types of microscopy images and different multi-class classification problems. our method outperformed state-of-the-art approaches for six datasets.

