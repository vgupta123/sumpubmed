BACKGROUND
in this paper we describe a grid solution to the computational challenges arising from the immunogrid project  <cit> . immunogrid is an ambitious project that has, as its primary objective, the development of a human immune system simulator spanning multiple levels – from molecules to organs. two main versions of the simulator are currently under development: the hiv simulator, designed to model responses to hiv- <dig> infection  <cit>  and the simtriplex simulator, designed to model vaccine schedules for the immunoprevention of mammary carcinoma in her-2/neu transgenic mouse  <cit> . both simulators are built on a single code base that is written in c and parallelised. the project has also undertaken research that requires the prediction of the location of major histocompatibility complex  class i epitopes within large sets of microbial sequences  using pre-trained tools, and predicting the location of mhc class ii epitopes using time-consuming molecular dynamics simulations.

many of the characteristics of immunogrid are shared by other biological projects: the involvement of multiple international partners ; the need to run large numbers of computations, both large and small; and the need to provide an easy-to-use interface for a non-technical user base. from this perspective, the approach presented here can be viewed as a case study that demonstrates the relevance and effectiveness of our chosen solution to a much wider range of biological projects.

in the past, many researchers  have had negative experiences attempting to exploit grid resources for scientific computation. however, significant progress has been made in recent years, notably through the development of lightweight grid "upper middlewares"  that insulate users from the underlying access technologies. the framework presented here enables research groups to construct computational grids that are easy to develop, modify and use.

requirements
with respect to our ability to access computational resources, the requirements of immunogrid are as follows:

• to enable the most complex single simulations to be run, requiring access to a large cluster or supercomputer.

• to enable large sets of immune system simulations and epitope predictions to be carried out, both to explore the parameter space of the simulator and to investigate the effects of a given clinical scenario on multiple individuals.

• to support smaller-scale simulations, including runs of the immunogrid educational simulators, for which standards workstations are sufficient.

as foreseen when the project name was chosen, no single partner of immunogrid can guarantee access to sufficient resources to meet these requirements, so a grid-based solution is a practical necessity.

from the end-user perspective, the requirements of our grid solution are as follows:

• access to the underlying computational resources should be transparent, i.e. the user should gain automatic access to the set of resources currently available to him/her without needing to be aware of their underlying organisation. in other words, the user should be insulated as far as possible from issues concerning administrative boundaries, passwords, operating systems, etc.

• given that the potential users of our grid-based simulators are diverse and often non-technical , all relevant resources should be accessible via an easy-to-use interface. from this perspective, a web interface is particularly appealing as it ensures that end-users do not need to install client software on their local machines.

solutions
to meet the requirements outlined above, a key priority for our computational grid is that it maximises the range and number of resources that can be added into it, from local desktop workstations to national/international grid services such as the uk national grid service  <cit>  , the european supercomputer grid deisa  <cit>  , and the us teragrid  <cit> . the full set of computational resources that we potentially wish to access via our grid is listed in table  <dig> 

the desire to access a diverse range of computational resources has two practical implications. firstly, it means we must aim to support all major existing grid middleware and platforms. secondly, it means that the addition of a new grid node needs to be as easy as possible, so that individuals and organisations that have resources that can potentially be incorporated into our grid are not deterred from doing so.

in order to meet the preceding requirements, we have sought to re-use existing solutions wherever possible. at the heart of our solution are two pieces of "upper middleware", the ahe   <cit>  and deshl   <cit> . taken together, these tools provide us with mechanisms for accessing the maximum range of resources whilst shielding us from most of the complexity associated with the underlying grid middlewares, such as globus  <cit> . neither tool on its own is sufficient. for example, we cannot access resources at cineca using ahe, nor local computing resources using deshl.

our framework also allows resources to be accessed via a third mechanism – the web service paradigm. a web service provides an application programming interface  that enables users to seamlessly integrate a remotely-hosted service with other components of the applications they are developing. this approach is becoming increasingly popular in the field of bioinformatics, with many core services provided by organisations such as the european bioinformatics institute  <cit>   already being made available as web services, not just via traditional "point-and-click" web interfaces. for immunogrid, instances of our simulators can be wrapped as web services, deployed on a local machine, and accessed via the grid framework described in this paper.

given a set of available resources linked by the immunogrid framework, specific resources are selected automatically by a simple job broker , or manually . one important feature of our solution is that it allows for the fact that different users will have the right to run jobs on different subsets of available resources. in particular, only users who have the appropriate grid certificate will have the right to access a given national/international grid service .

the final essential ingredient of our grid framework is its web interface. this hides the various underlying middlewares from the user, who  can run multiple simulations on diverse computational resources at various widely-distributed sites.

RESULTS
the grid infrastructure described in this paper has been used to run several contrasting applications for the immunogrid project. the primary scientific aim of immunogrid is to develop and validate a virtual human immune system simulator. during the development of the simulator, large numbers of runs with different versions of the simulator software have been carried out using this infrastructure. in addition, we have undertaken large-scale prediction of class i t-cell epitopes using local installations of the prediction software developed at the center for biological sequence analysis , technical university of denmark , copenhagen, denmark  <cit> . we are also currently developing a new method for class ii t-cell epitope prediction using molecular dynamics simulations.

here we present three representative case studies that show the amount of time saved by using our grid-based approach to handle large-scale applications. all timings are measured in wall clock time, as this represents the most relevant measurement for end-user. the individual jobs varied significantly in their computational intensity; when run on the single birkbeck server, times ranged from fractions of a second for a single cbs prediction to approximately two days for a molecular dynamics simulation. in these case studies, access was restricted to local resources in london , bologna  and boston  that were made available by members of the immunogrid consortium. details of these resources are given in table  <dig> 

class i t-cell epitope prediction
for this case study,  <dig>  influenza protein sequences were analysed using the cbs t-cell epitope prediction software for  <dig> mhc alleles, giving a total of  <dig> , <dig> jobs. we estimate  that running the entire batch on the birkbeck server would take approximately  <dig> hours. using the grid infrastructure presented in this paper, the total number of jobs was split equally between three resources: the birkbeck local cluster, the dana-farber local cluster, and the cineca supercomputer. in this case, the splitting of the jobs over the resources was performed by hand. splitting of jobs over the resources can be completed by the resource broker, so long as an appropriate schedule has been implemented. subsets of the total number of nodes were used at each resource in order to comply with fair usage guidelines. the resource usage and wall times are summarised in table  <dig> 

the total wall time for the birkbeck server is an estimate.

this distribution of jobs proved highly successful, with a wall time saving of approximately  <dig> day  compared to the anticipated execution time with a single machine. one important caveat, however, is that the jobs sent to the cineca supercomputer were not held in a queue for a significant period of time. this is certainly not something that was guaranteed, and other batches of jobs submitted to cineca have not been so fortunate. queue length is something that can only be ascertained by directly logging onto the supercomputer at run time.

assessment of vaccination schedule effectiveness
for this case study, the simtriplex simulator was used to assess the effectiveness of different vaccination schedules in the prevention of mammary carcinoma in virtual mice that represent real her-2/neu transgenic mice  <cit> . we ran a total of  <dig>  vaccine schedules on a population of  <dig> mice, giving a total of  <dig>  jobs. we estimate  that running the entire batch on the birkbeck server would take approximately  <dig> hours.

on this occasion we were unable to use the supercomputer at cineca as it was undergoing maintenance. we therefore submitted the jobs to the two local clusters at dana-farber and birkbeck. we decided to increase the number of nodes we submitted jobs to on the dana-farber cluster, as it was slower than the birkbeck cluster in the preceding case study . the resource usage and wall times are summarised in table  <dig> 

the total wall time for the birkbeck server is an estimate.

again, we achieved a substantial saving in wall time  compared to the anticipated time on a single machine. it is worth pointing out, however, that the relative performance of the dana-farber cluster  was significantly worse than anticipated after the results of the first case study . indeed, had all the jobs been submitted to the birkbeck cluster, the saving would have been significantly higher .

this apparent inconsistency in the performance of the birkbeck and dana-farber local clusters with respect to the two case studies is not easy to explain, but neither is it entirely unexpected. ultimately it may be attributable to a complex interplay between the cpu, memory and io characteristics of the jobs executed, or to other factors we are unaware of . consequently, we cannot guarantee that the timings would be more-or-less the same even if we ran them again with the same distribution of jobs.

towards class ii t-cell epitope prediction
for this case study, we simulated the binding of peptides to class i mhc proteins using namd  abf  software  <cit> . ultimately the aim is to develop a class ii prediction method, but the current lack of class ii data means that we are using class i data during the development stage. we estimate  that running a batch of  <dig> jobs on the birkbeck server would take approximately  <dig>  hours. for this case study we sent half of the jobs to the birkbeck cluster, and half to the supercomputer at cineca. the resource usage and wall times are summarised in table  <dig> 

the total wall time for the birkbeck server is an estimate.

once again we achieved a substantial saving in wall time  compared to the anticipated time on a single machine. here the majority of the cineca time is, in fact, queuing time.

discussion and 
CONCLUSIONS
the number of scientific research projects that would benefit from having access to large-scale computational resources is increasing. with the growing prevalence of large high-throughput data sets, this is true for many sciences, but it is certainly a characteristic of systems biology research, where the need to run large numbers of computationally intensive simulations is commonplace. many projects will have access to sufficient resources in the form of a large local cluster or  a national/international production-quality grid. however, many others will struggle to satisfy their demand for computational power via these routes. moreover, to rely on a single source for access to resources is inherently risky; local clusters can fail, or require maintenance or upgrading at crucial times, and long-term access on demand is rarely available to academic users of production grids. it is also worth emphasising that, given the demand for such resources that lengthy queuing times are commonplace.

in this paper we have presented a lightweight grid framework that aims to provide researchers with a transparent mechanism for accessing a wide range of computational resources. it is particularly appropriate for consortia of research groups that are collaborating on a particular project, as it allows each group to contribute its own local resources to the grid with minimal effort. we have demonstrated that the framework can be used to build a grid capable of accessing a diverse range of computational resources , and have used it successfully to run a range of jobs for the immunogrid project. currently we are further extending the framework to enable job submission to pi2s <dig>  <cit> , the sicilian grid using glite  <cit>  middleware  <cit> .

the hallmarks of our framework are its flexibility, ease of installation, and ease of use. this comes at a certain cost, however. currently it is not feasible to rapidly develop a new grid infrastructure capable of integrating such a diverse range of resources that is of production quality. production grids typically require services to be constantly monitored, sophisticated schemes for handling errors, and the provision of dedicated user support  <cit> . indeed, it is worth noting that even national grid services are typically rather limited in this respect. for example, users of the ngs need to manually interact with individual grid nodes in order to ascertain which resources have the shortest queues. the time required to develop such a grid makes it an impractical proposition for all but the largest and longest-running projects, and no off-the-shelf solution is currently available.  nevertheless, notwithstanding the non-optimal deployment of resources, we believe the grid framework presented here represents a reasonable compromise.

