BACKGROUND
biomedical event extraction is the process of automatically detecting statements of molecular interactions in research articles. using natural language processing techniques, an event extraction system predicts relations between proteins/genes and the processes they take part in. manually annotated corpora are used to evaluate event extraction techniques and to train machine-learning based systems.

event extraction was popularised by the bionlp' <dig> shared task on event extraction  <cit> , providing a more detailed alternative for binary interaction extraction, where each pair of named entities  co-occurring in the text is classified as interacting or not. events extend this formalism by adding to the relations direction, type and nesting. events define the type of interaction, such as phosphorylation, and commonly mark in the text a trigger word  describing the interaction. directed events can define the role of their arguments as e.g. cause or theme, the agent or the target of the biological process. finally, events can act as arguments of other events, creating complex nested structures that accurately describe the biological interactions stated in the text. for example, in the case of a sentence stating "stat <dig> phosphorylation is regulated by vav", a phosphorylation-event would itself be the argument of a regulation-event.

we developed for the bionlp' <dig> shared task the turku event extraction system, achieving the best performance at  <dig> % f-score  <cit> . this system separated event extraction into multiple classification tasks, detecting individually the trigger words defining events, and the arguments that describe which proteins or genes take part in these events. other approaches used in the shared task included e.g. joint inference  <cit> . an overall notable trend was the use of full dependency parsing  <cit> .

in the following years, event extraction has been the subject of continuous development. in  <dig>  after the bionlp' <dig> shared task, we extended our system and improved its performance to  <dig> %  <cit> . in  <dig>  the system introduced by miwa et. al. reached a new record performance of  <dig> %  <cit> .

in  <dig>  we applied the turku event extraction system to detecting events in all  <dig> million pubmed abstracts, showing its scalability and generalizability into real-world data beyond domain corpora  <cit> . to facilitate the ease of use and applications based on the dataset, it has been transferred to the evex database, which also adds several layers of analysis to the data  <cit> .

participating in the bionlp  <dig> shared task  <cit> , we have demonstrated the generalizability of the turku event extraction system to different event extraction tasks by applying what is, to a large extent, the same system to every single task and subtask. following the shared task, we now further improve performance on the id  task, provide a detailed analysis of performance on different corpora with learning curves, and evaluate the suitability of events from the evex database for use as additional training data.

methods
our system divides event extraction into three main steps . first, entities are predicted for each word in a sentence. then, arguments are predicted between entities. finally, entity/argument sets are separated into individual events.

graph representation
the bionlp' <dig> shared task consists of eight separate tasks. most of these follow the bionlp' <dig> shared task annotation scheme, which defines events as having a trigger entity and one or more arguments that link to other events or protein/gene entities. this annotation can be represented as a graph, with trigger and protein/gene entities as nodes, and arguments  as edges. in our graph representation, an event is defined implicitly as a trigger node and its outgoing edges .

most of the bionlp' <dig> shared task tasks define task-specific annotation terminology, but largely follow the bionlp' <dig> definition of events. some new annotation schemes, such as the bracket notation for protein references in the co  task can be viewed simply as alternative representations of arguments. the major new feature is relations or triggerless events, used in the rel , ren , bb  and bi  tasks. in our graph representation, this type of event is a single, directed edge.

some event arguments have a matching site argument that determines the part of the protein the argument refers to . to allow detection of core arguments independently of site arguments, in most tasks we link both core and site arguments directly to proteins . connecting site arguments to the protein instead of the event also reduces the number of outgoing edges per predicted event, simplifying unmerging . however, if several events' core arguments refer to the same protein, the matching of site arguments to core arguments becomes ambiguous, limiting performance on site argument detection, but in most cases maximizing the performance on the core task is preferable.

to further simplify event extraction all sentences are processed in isolation, so events crossing sentence boundaries  cannot be detected. this also limits the theoretical maximum performance of the system .

numbers are for all available annotated data, i.e. the merged training and development sets. event numbers include the resolved equivalencies.

in the provided data an event is annotated only once for a set of equivalent proteins. for example, in the sentence "ubiquitination of caspase  <dig> " a ubiquitination event would be annotated only for "caspase 8", "casp8" being marked as equivalent to "caspase 8". to improve training data consistency, our system fully resolves these equivalences into new events, also recursively when a duplicated event is nested in another event . resolved equivalences were used for event extraction in the bionlp' <dig> ge , id , epi  and bb  tasks, although based on tests with the ge dataset their impact on performance was negligible.

machine learning
the machine learning based event detection components classify examples into one of the positive classes or as negatives, based on a feature vector representation of the data. to make these classifications, we use the svmmulticlass support vector machine  <cit>  with a linear kernel. an svm must be optimized for each classification task by experimentally determining the regularization parameter c. this is done by training the system on a training dataset, and testing a number of c values on a development dataset. when producing predictions for the test set, the classifier is retrained with combined training and development sets, and the test data is classified with the previously determined optimal value of c.

in the bionlp' <dig> shared task we optimized the three main parameters  in an exhaustive grid search against the final metric. due to time constraints, for the bionlp' <dig> shared task, only the recall-adjustment parameter  was optimized against the final metric, edge and trigger detector parameters being optimized in isolation.

following the shared task, we tested again the three-parameter grid search for the ge, epi and id tasks. performance differences were negligible, so with the current system and feature representations we can assume that optimizing trigger and edge detector regularization parameters in isolation produces svm models applicable for the overall task.

syntactic analyses
the machine learning features that are used in event detection are mostly derived from the syntactic parses of the sentences. parsing links together related words that may be distant in their linear order, creating a parse tree .

we used the charniak-johnson parser  <cit>  with david mcclosky's biomodel  <cit>  trained on the genia corpus and unlabeled pubmed articles. the parse trees produced by the charniak-johnson parser were further processed with the stanford conversion tool  <cit> , creating a dependency parse  <cit> .

in the supporting tasks  this parsing was done by us, but in the main tasks the organizers provided official parses which were used  <cit> . all parses for tasks where named entities were given as gold data were further processed with a protein name splitter that divides at punctuation tokens which contain named entities, such as "p50/p65" or "gata3-binding", which would otherwise lead to multiple entities or triggers having the same head token, preventing detection of events between them.

feature groups
to convert text into features understood by the classifier, a number of analyses are performed on the sentences, mostly resulting in binary features stating the presence or absence of some attribute. basic features such as token texts can also be combined into more specific features, such as the n-grams used in edge detection.

token features can be generated for each word token, and they define the text of the token, its porter-stem  <cit> , its penn treebank part-of-speech-tag, character bi-and trigrams, presence of punctuation or numeric characters etc.

sentence features define the number of named entities in the sentence as well as bag-of-words counts for all words.

dependency chains follow the syntactic dependencies up to a depth of three, starting from a token of interest. they are used to define the immediate context of these words.

dependency path n-grams are built from the shortest undirected path of tokens and dependencies linking together two entities, and are used in edge detection. n-grams join together a token with its two flanking dependencies as well as each dependency with its two flanking tokens. each token or dependency has a number of attributes such as text or type, which are joined with the attributes of its neighbours to form the n-gram. while these n-grams follow the direction of the entire path, the governor-dependent directions of individual dependencies are used to define additional token bigrams.

trigger features can be built for trigger or entity nodes already present, i.e. the given gold entities, and also predicted triggers when doing edge detection and unmerging. these features include the types and supertypes  of the trigger or entity nodes, and combinations thereof.

external features are additional features based on data external to the corpus being processed. such features can include e.g. the presence of a word in a list of key terms, wordnet hypernyms, or other resources that enhance performance on a particular task. these are described in detail in section results and discussion.

trigger detection
trigger words are detected by classifying each token as negative or as one of the positive trigger classes. sometimes several triggers overlap, in which case a merged class  is used. such cases are quite rare, for example in the genia corpus development set only  <dig> %  of positive trigger examples belong to a merged class. after trigger prediction, triggers of merged classes are split into their component classes. in practice, examples of merged classes are rarely predicted, except for the most common overlapping classes.

most tasks evaluate trigger detection using approximate span, so detecting a single token is enough. however, this token must be chosen consistently for the classifier to be able to make accurate predictions. for multi-token triggers, we select as the trigger word the syntactic head, the root token of the dependency parse subtree covering the entity.

when optimizing the svm c-parameter for trigger and edge detection, it is optimized in isolation, maximizing the f-score for that classification task. edges can be predicted for an event only if its trigger has been detected, but often the c-parameter that maximizes trigger detection f-score has too low recall for optimal edge detection. a recall adjustment step is used to fit together the trigger and edge detectors. for each example, the classifier gives a confidence score for each potential class, and picks as the predicted class the one with the highest score. in recall adjustment, the confidence score of each negative example is multiplied with a multiplier, and if the result falls below the score of another class, that class becomes the new classification. this multiplier is determined experimentally by optimizing against overall system performance, using the official task metric if a downloadable evaluator is available , or edge detection f-score if there isn't one.

edge detection
edge detection is used to predict event arguments or triggerless events and relations, all of which are defined as edges in the graph representation. the edge detector defines one example per direction for each pair of entities in the sentence, and uses the svm classifier to classify the examples as negatives or as belonging to one of the positive classes. as with the trigger detector, overlapping positive classes are predicted through merged classes . there are usually fewer edge types than trigger types, so merged classes are even less common than in trigger detection, for example in the genia corpus development set only  <dig> out of  <dig> positive edge examples belong to a merged class. task-specific rules defining valid argument types for each entity type are used to considerably reduce the number of examples that can only be negatives.

unmerging
in the graph representation, events are defined through their trigger word node, resulting in overlapping nodes for overlapping events. the trigger detector can however predict a maximum of one trigger node per type for each token. when edges are predicted between these nodes, the result is a merged graph where overlapping events are merged into a single node and its set of outgoing edges. taking into account the limits of trigger prediction, the edge detector is also trained on a merged graph version of the gold data.

to produce the final events, these merged nodes need to be "pulled apart" into valid trigger and argument combinations. in the bionlp' <dig> shared task, this was done with a rule-based system. since then, further research has been done on machine learning approaches for this question  <cit> . in our current system, unmerging is done as an svm-classification step. an example is constructed for each argument edge combination of each predicted node, and classified as a true event or a false event to be removed. tested on the bionlp' <dig> shared task data, this system performs roughly on par with our earlier rule-based system, but has the advantage of being more general and thus applicable to all bionlp' <dig> shared task tasks. the unmerging step is not required for triggerless events which are defined by a single edge.

all of the tasks define varied, detailed limits on valid event type and argument combinations. a final validation step based on task-specific rules is used to remove structurally incorrect events left over from preceding machine learning steps. for example, for the genia corpus development set, this validation step removed  <dig> % of the predicted events which did not conform to task specific structural requirements.

modality detection
speculation and negation are detected independently, with binary classification of trigger nodes. the features used are mostly the same as for trigger detection, with the addition of a list of speculation-related words selected manually from the bionlp' <dig> st corpus.

RESULTS
the bionlp' <dig> shared task consists of five main tasks and three supporting tasks . additionally, many of these tasks specify separate subtasks. except for the ge-task, which defines three main evaluation criteria, all tasks have a single primary evaluation criterion. all evaluations are based on f-score, the harmonic mean of precision and recall. performance of all systems participating in the bionlp' <dig> shared task is shown in figure  <dig>  our system's performance on both development and test sets of all tasks is shown in table  <dig> 

the event types for all tasks, their core arguments used for the primary evaluation and optional arguments for secondary evaluation. superscripts show the arguments and targets limited to a specific task for events present in multiple tasks. starred events have in the epi task a corresponding reverse event  with identical argument types. the plus-sign indicates where multiple arguments of the same type are allowed for one event.

the performance of our new system on the bionlp' <dig> st genia dataset is shown for reference, with task  <dig> omitted due to a changed metric. for ge-tasks, the approximate span & recursive matching criterion is used. in many tasks, the development and test set results differ considerably, which may be partially explained by noise unseen due to lack of cross-validation and by the event distribution not being stratified across the sets.

in this section we also describe the approaches required for adapting the system to the different tasks. the primary adaptation was addition of task specific feature sets, although the majority of features were shared between all tasks. in some tasks, such as epi, the graph representation was slightly altered. as the turku event extraction system deals only with nodes and edges, the modified graph representation affected the system primarily in conversion to or from the shared task format. finally, in tasks where all entities and triggers were given, the event extraction process was started from the edge detection step. all in all, task specific requirements resulted in relatively little additional code, consisting mostly of specialized versions of the generic trigger and edge detection modules.

genia 
the genia task is the direct continuation of the bionlp' <dig> shared task. the bionlp' <dig> st corpus consisted only of abstracts. the new version extends this data by 30% with full text pubmed central articles  <cit> .

our system applied to the ge task is the most similar to the one we developed for the bionlp' <dig> shared task. the major difference is the replacement of the rule-based unmerging component with an svm based one.

the ge task has three subtasks, task  <dig> is detection of events with their main arguments, task  <dig> extends this to detection of sites defining the exact molecular location of interactions, and task  <dig> adds the detection of whether events are stated in a negated or speculative context.

for task  <dig>  speculation and negation detection, we considered the ge, epi and id task corpora similar enough to train a single model on. compared to training on ge alone, example classification f-score decreased for negation by  <dig> pp and increased for speculation by  <dig> pp. overall task  <dig> processing was considerably simplified.

our system placed third in task  <dig>  second in task  <dig> and first in task  <dig>  task  <dig> had the most participants, making it the most useful for evaluating overall performance. our f-score of  <dig> % was within three percentage points of the best performing system , indicating that our chosen event detection approach still remains competitive. for reference, we ran our system also on the bionlp' <dig> data, reaching an f-score of  <dig> %, a slight increase over the  <dig> % we have previously reached  <cit> .

epigenetics and post-translational modifications 
all events in the epi task that have additional arguments  have a single core argument  <cit> . we therefore use for this task a slightly modified graph representation, where all additional arguments are treated as core arguments, linking directly to the event node , thus preserving the core/site argument pairings. the number of argument combinations per predicted event node remains manageable for the unmerging system and full recovery of additional arguments is possible.

eight of the epi event types have corresponding reverse events, such as phosphorylation and dephosphorylation. many of these reverse events are quite rare, resulting in too little training data for the trigger detector to find them. therefore we merge each reverse event type into its corresponding forward event type. after trigger detection, an additional rule-based step separates them again. most of the reverse classes are characterized by a "de"-prefix in their trigger word, so the types of all such triggers are negated, as are the types of triggers whose text contains one of the strings "remov", "loss" or "erasure". on the epi training dataset, this rule-based step determined correctly whether an event was reversed in  <dig> % of cases . using this approach, primary criterion f-score on the development set increased  <dig>  percentage points from  <dig> % to  <dig> %. several previously undetectable small reverse classes became detectable, with e.g. deubiquitination  detected at  <dig> % f-score.

our system ranked first on the epi task, outperforming the next-best system  by over  <dig> percentage points. on the alternative core metric our system was also the first, but the faust system was very close with only a  <dig>  percentage point difference. following the shared task, it was confirmed that we were the only team to attempt detection of non-core arguments, explaining the large difference to other systems on the full task  <cit> .

infectious diseases 
the annotation scheme for the id task closely follows the ge task, except for an additional process event type that may have no arguments, and for five different entity types in place of the protein type  <cit> . our approach for the id task was identical to the ge task, but performance relative to the other teams was considerably lower. primary evaluation metric f-score was  <dig> % vs.  <dig> % for the core metric which disregards additional arguments, indicating that these were not the reason for low performance.

following the shared task, we analysed the results to determine the causes of our system lagging behind on the id task. compared to other participants, performance was especially low on the process events. a closer analysis of the system revealed that our original implementation of the unmerging component did not consider triggers with zero arguments as candidates for events. allowing these process triggers to form events improved performance to  <dig> %.

in the shared task, the teams with better performance succesfully utilized the similarity of the id and ge datasets. the three machine learning systems  <cit>  were trained for the id task on a combination of id and ge data, while the rule-based concordia system  <cit>  was developed to have mostly a single rule set for the ge, epi and id tasks. following these approaches, we added the ge corpus into the training data of the id task trigger and edge detectors, further increasing performance to  <dig> %.

together, these improvements increased our primary criterion performance on the test set by  <dig>  percentage points. compared to the shared task results, our new results place us second, just  <dig>  pp after the leading system.

the new performance of  <dig> % is very close to our system's performance of  <dig> % and  <dig> % on the similar ge and epi tasks, indicating that the system's generally high performing approach is now fully applied also to the id task.

bacteria biotopes 
the bb task considers detection of events about bacteria and their habitats  <cit> . the task defines only two event types but a large number of entity types which fall into five supertypes. all entities must be predicted and all events are triggerless.

unlike in the other main tasks, in the bb task exact spans are required for bacterium-type entities, which usually consist of more than one token . after trigger detection, a rule-based step attempts to extend predicted trigger spans to reach the correct span. starting from the detected trigger head token, it extends the span forwards and backwards as long as each encountered token is a known bacterium name substring. these substrings are derived from the list of prokaryotic names with standing in nomenclature  <cit> . about  <dig> additional rules select for tokens based on common bacteria suffixes  and a further  <dig> rules select for other known bacterium substrings . when extending the spans of bb training set gold entity head tokens, this step produced the correct span for 91%  of bacterium-type entities.

to aid in detecting bacterium-entities the list of bacteria names from the list of prokaryotic names with standing in nomenclature was used as external features, marking for each token as a binary feature whether it has been seen in a known bacterium name. to help in detecting the heterogeneous habitat-entities, synonyms and hypernyms from wordnet were used  <cit> . the development set lacked some event classes, so we moved some documents from the training set to the development set to include these.

the best system in the bb task was by team bibliome, with an f-score of 45%  <cit> . our f-score of 26% was the lowest of the three participating systems, and detailed results show a consistently lower performance in detecting the entities. the large number of intersentence events  also considerably limited performance .

bacteria gene interactions 
the bi-task considers events related to genetic processes of the bacterium bacillus subtilis  <cit> . this task defines a large number of both entity and event types, but all entities are given as gold-standard data, therefore we start from edge detection . all bi events are triggerless.

in this task manually curated syntactic parses are provided. as also automated parses were available, we tested them as an alternative. with the charniak-johnson/mcclosky parses overall performance was only  <dig>  percentage points lower . as with the bb task, we moved some documents from the training set to the development set to include missing classes.

despite this task being very straightforward compared to the other tasks we were the only participant. therefore, too many conclusions shouldn't be drawn from the performance, except to note that a rather high f-score is to be expected with all the entities being given as gold data.

protein/gene coreference 
in the co supporting task the goal is to extract anaphoric expressions  <cit> . even though our event extraction system was not developed with coreference resolution in mind, the graph representation can be used for the coreference annotation, making coreference detection possible. anaphoras and antecedents are both represented as exp-type entities, with coref-type edges linking anaphora-entities to antecedent-entities and target-type edges linking protein-type entities to antecedent-entities.

in the co-task, character spans for detected entities must be in the range of a full span and minimum span. therefore in this task we used an alternative trigger detector. instead of predicting one trigger per token, this component predicted one trigger per each syntactic phrase created by the charniak-johnson parser. since these phrases don't cover most of the co-task triggers, they were further subdivided into additional phrases, e.g. by cutting away determiners and creating an extra phrase for each noun-token, with the aim of maximizing the number of included triggers and minimizing the number of candidates.

the best system in the co task was by university of utah, with a performance of  <dig> %  <cit> . our system placed fourth out of six, reaching an f-score of  <dig> %. coreference resolution being a new subject for us and our system not being developed for this domain, we consider this an encouraging result, but conclude that in general dedicated systems should be used for coreference resolution.

entity relations 
the rel supporting task concerns the detection of static relations, subunit-complex relations between individual proteins and protein complexes and protein-component relations between a gene or protein and its component, such as a protein domain or gene promoter  <cit> . in our graph representation these relations are defined as edges that link together given protein/gene names and entity-type entities detected by the trigger detector.

to improve entity detection, additional features are used. derived from the rel annotation, these features highlight structures typical for biomolecular components, such as aminoacids and their shorthand forms, domains, motifs, loci, termini and promoters. many of the rel entities span multiple tokens. since the trigger detector predicts one entity per token, additional features are defined to mark whether a token is part of a known multi-token name. the texts of the preceding tokens are joined together, and the presence of known multi-token triggers in this string are marked as features. the system still predicts only one token for each trigger, but can this way determine whether that token belongs to a known, larger trigger expression.

our system had the best performance out of four participating systems with an f-score of  <dig> %, over  <dig> percentage points higher than the next. performance for the two event classes was quite close,  <dig> % for protein-component and  <dig> % for subunit-complex.

bacteria gene renaming 
the ren supporting task is aimed at detecting statements of b. subtilis gene renaming where a synonym is introduced for a gene  <cit> . the ren task defines a single relation type, renaming, and a single entity type, gene. all entities are given, so only edge detection is required. unlike the other tasks, the main evaluation criterion ignores the direction of the relations, so they are processed as undirected edges in the graph representation.

edge detection performance was improved with external features based on two sources defining known b. subtilis synonym pairs: the uniprot b. subtilis gene list "bacsu"  <cit>  and subtiwiki  <cit> , the b. subtilis research community annotation wiki.

for the  <dig> renaming relations in the ren training data, the synonym pair was found from the uniprot list in 66% , from subtiwiki in 79%  and from either resource in  <dig> % . for the corresponding negative edge examples, uniprot or subtiwiki synonym pairs appeared in only  <dig> % .

at  <dig> % f-score our system had the highest performance out of the three participants, exceeding the next highest system by  <dig>  percentage points. if uniprot and subtiwiki features are not used, performance on the development set is still  <dig> %, close to the second highest performing system on the task.

learning curves
moving forward after the shared task, it is important for the community to know how best to focus our resources on improving event extraction performance. event extraction systems may benefit from additional optimization and extraction strategies, but on the other hand, many competing approaches have led to roughly similar performance in the bionlp' <dig> and bionlp' <dig> shared tasks.

one source of improvement could simply be additional annotated text. however, annotation is a costly and difficult process. to determine the likely benefit from further training data, we construct learning curves for all bionlp' <dig> shared task corpora, using our event extraction system .

learning curves are made by consecutively reducing the training set size. our system operates on individual sentences, but in the corpora these sentences are usually grouped into documents, often consisting of a related set such as an abstract. sentences within a single document may overlap in content, so to ensure a realistic reduction in training data, entire documents are removed at all steps  <cit> .

machine learning systems often show a logarithmic response to training dataset size. in the shared task corpora, the number of documents can however be quite small, usually in the range of a few hundreds. thus, taking e.g. 1/1000th of the data would not be feasible. therefore, to produce curves that clearly show the impact of the dataset size, we use a binary logarithmic scale, roughly doubling dataset size at each step. all results are predicted for the full development set, using the official shared task evaluation metrics.

we can see from the learning curves that generally a doubling of dataset size is required to maintain a consistent increase in f-score, indicating diminishing gains from more annotated data. however, most corpora show increased performance even at the final points of the learning curve, so some performance could still be gained by additional annotation, if enough data is added.

the bionlp' <dig> corpora are of very different sizes . especially the two bacteria corpora, bb and bi, are very small. the learning curves of these two show considerable variance and in some cases a reduction of training data can even result in an increase in performance. as we know that on the next smallest id task performance is limited by training dataset size ), further development of the bb and bi extraction targets likely depends on more annotated data becoming available.

for the ge  <dig> and  <dig> learning curves, we have used the primary task  <dig> measure. of all tasks, only these two have a directly comparable evaluation metric. it seems that overall, the old ge  <dig> corpus is slightly easier to learn, a result consistent with the inclusion of more heterogeneous full-text articles in the  <dig> corpus. however, when dataset size increases, performance seems to converge, and when using 64-100% of the data, performance is very similar for both corpora.

self-training
self-training is a machine learning technique in which a suitable subset of a system's output is used as additional training data for the same system. in the domain of biomedical nlp, self-training was successfully applied for instance to syntactic parsing  <cit>  and word sense disambiguation  <cit> . we tested the effect of self-training on the ge task , using data from evex, a publicly available database of automatically extracted events produced by applying our bionlp' <dig> shared task system to the entire  <dig> distribution of pubmed citation titles and abstracts  <cit> .

typically, self-training examples are selected based on their confidence score assigned by the system during extraction. low-confidence examples are avoided since they have a higher proportion of false positives and would thus not be likely to provide useful training data. very high confidence events, on the other hand, may not provide sufficiently new information, as the system is already able to extract them reliably. to test the effect of event confidence on its usability as self-training data, we first renormalize the confidence scores of all events in evex to μ =  <dig> and σ =  <dig>  i.e. zero mean with standard deviation one. having observed that the mean event confidence score in evex differs substantially depending on the type of the event, the number of entity arguments, and the number of recursive event arguments, we normalize each subset of evex events defined by these three criteria separately. we then select four sets of evex events for self-training, based on how many standard deviations above or below the mean their normalized confidence score is. we randomly select  <dig>  evex events for each of the four sets: set s <dig> contains events with confidence in the range [- <dig> ,  <dig> ), set s <dig> events with confidence in the range [ <dig> ,  <dig> ), and so forth for sets s <dig> and s <dig> 

for each of the four self-training sets, we measure the performance of the system, with the set included in the training data, and compare it to the baseline performance where no self-training data is used. the results are presented in table  <dig>  the self-training performance surpasses that of the baseline for sets s <dig> and s <dig>  however the overall gain of  <dig>  pp  is only very modest and does not manifest on the test set, where the overall f-score decreases by  <dig>  pp when self-training is used.

performance of the system on the ge subtask  <dig> in terms of f-score on the overall approximate span & recursive matching criterion. random distribution refers to self-training example selection by random sampling, whereas even distribution refers to selection of equal amount of examples for each event type and argument combination. baseline is the performance of the system with no self-training .

in a follow-up experiment, we focus on the fact that the distribution of events is very uneven. first, most events only have a single theme argument and second, event types such as protein catabolism are considerably more rare than for example regulation. this naturally also reflects in the randomly selected self-training sets, which provide little additional data for rare event types and argument combinations. we thus tested a sampling strategy where for each of the  <dig> combinations of event type, number of entity arguments, and number of event arguments, we sampled a maximum of  <dig>  event examples in the confidence range [ <dig> ,  <dig> ), i.e. the range that gave best results in the previous experiment. in addition, for each of these events, we also include all their recursively nested events so as to preserve event structures in their entirety. in total, the self-training set comprised  <dig>  events. this strategy resulted in an increase in f-score of  <dig>  pp  on the development set and  <dig>  pp  on the test set, for ge subtask  <dig>  and is thus a clearly better strategy than a simple random sampling. detailed results are shown in table  <dig>  however, there is no obvious pattern as to which event classes benefit from self-training, likely to some extent due to the small magnitude of the overall gain.

performance of the system on the ge subtask  <dig> in terms of f-score on the overall approximate span & recursive matching criterion. baseline and self-training  results, as well as evaluation event counts are given for the development set. difference  in f-score is given for both the development and test sets.

these results are obtained when both training and evaluating the system on ge subtask  <dig> only. combined training for subtasks  <dig> and  <dig> gives a subtask  <dig> performance of  <dig> % on the test set, the official result of the system in the shared task. this performance is  <dig>  pp higher than the  <dig> % obtained with self-training on ge subtask  <dig> only. further preliminary experiments with self-training for combined ge subtasks  <dig> and  <dig> had so far only a negligible effect on the performance.

while the magnitude of the performance differences does not allow too firm conclusions to be drawn, it is clear that with appropriate selection strategy, self-training does have the potential for a performance gain, as shown both on the development and test sets. with a pubmed-wide event resource with nearly  <dig> million events easily available, it is a direction certainly worth further investigation regarding which exact subset of events to include as self-training data to maximize the gain.

CONCLUSIONS
we have developed a system that addresses all tasks and subtasks in the bionlp' <dig> shared task, with top performance in several tasks. with the modular design of the system, all tasks could be implemented with relatively small modifications to the processing pipeline. the graph representation which covered naturally all different task annotations was a key feature in enabling fast system development and testing. as with the turku event extraction system developed for the bionlp' <dig> shared task, we release this improved system for the bionlp community under an open source license at bionlp.utu.fi.

of all the tasks, the ge-task, which extends the bionlp' <dig> corpus, is best suited for evaluating advances in event extraction in the past two years. for the ge' <dig> corpus, in the bionlp' <dig> shared task we achieved a performance of  <dig> %  and in  <dig> miwa et. al. reached  <dig> %  <cit> . comparing our current system's performance on the ge' <dig> corpus with the ge' <dig> one, we can assume that the two corpora are of roughly equal difficulty. therefore we can reason that since the bionlp' <dig> shared task, event extraction performance has increased about four percentage points, the highest performance on the  <dig> ge-task being  <dig> % by team faust. it appears that event extraction is a hard problem, and that the immediate performance gains have already been found. we hope the bionlp' <dig> shared task has focused more interest in the field, hopefully eventually leading to breakthroughs in event extraction and bringing performance closer to established bionlp fields such as syntactic parsing or named entity recognition.

that our system could be generalized to work on all tasks and subtasks, indicates that the event extraction approach can offer working solutions for several biomedical domains. a potential limiting factor currently is that most task-specific corpora annotate a non-overlapping set of sentences, necessitating the development of task-specific machine learning models. training on multiple datasets could mean that positives of one task would be unannotated on text from the other task, confusing the classifier. on the other hand, multiple overlapping task annotations on the same text would permit the system to learn from the interactions and delineations of different annotations. system generalization has been successfully shown in the bionlp' <dig> shared task, but has resulted in a number of separate extraction systems. it could well be that the future of event extraction requires also the generalization of corpus annotations.

our results on self-training demonstrate that system output can be used to improve performance in some cases. self-training is a promising direction for system improvement, as in addition to performance improvements, it might produce a system more suited for use with heterogeneous real-world data. our continued efforts on pubmed-scale event extraction will in the future provide more data for researchers interested in self-training for event extraction.

as future directions, we will continue to improve the scope and performance of the turku event extraction system. we are continuing our work on pubmed-scale event extraction and the evex dataset, and will use for this project several of the new extraction targets introduced by the bionlp' <dig> shared task.

competing interests
the authors declare that they have no competing interests.

authors' contributions
jb designed and implemented the upgrade of the turku event extraction system for the bionlp  <dig> shared task. fg and jb designed and implemented the learning curve and self-training analyses. jb and fg have drafted the manuscript. ts supervised the study. all authors have read and approved the final manuscript.

