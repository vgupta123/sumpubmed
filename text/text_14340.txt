BACKGROUND
over past decades, the prediction accuracy of protein secondary structure has gained some improvements, largely due to the successful application of machine learning tools such as neural network  and support vector machine . qian and sejnowski designed one of the earliest nn methods  <cit> . rost and sander introduced the alignment profile with multiple sequence alignment into the prediction. their method, named as phd, performed much better than previous ones, because of the use of alignment profile as the network's input  <cit> . jones made an important improvement by pioneering the use of position-specific scoring matrices  to generate the so-called psi-blast profile and developed the method called psipred  <cit> . recently, new advances have been made in developing nn-based prediction methods  <cit> . similarly, svm-based methods were developed for protein secondary structure prediction, first taking the alignment profile as inputs and then being improved to use the psi-blast profile  <cit> . generally speaking, the q <dig> of a modern nn or svm-based method can reach over 76%.

in contrast to nn and svm, probabilistic methods for protein secondary structure prediction such as those based on hidden markov model  have had very limited accuracy  <cit> . most of them were designed for single sequence prediction with prediction accuracy generally less than 70%. recently, two profile-based hmm methods were proposed, which take either the alignment profile or psi-blast profile as inputs  <cit> . both of the methods treat the profile as production from a multinomial distribution with  <dig> possible outcomes , and thus lose the information about the correlation between entries of the profile. as a result, the prediction accuracy of the two methods, which is around 72%, is still much lower than the common level of nn or svm-based methods. it is notable that there is a special hmm-type method, sam-t <dig>  <cit> , which has shown comparable accuracy to nn and svm-based methods. however, with using a neural network for the sequence-to-structure prediction while building the hmm only at the secondary structure level  <cit> , sam-t <dig> should not be regarded as a pure hmm-type method.

it would be interesting to break this apparent asymmetry in accuracy between machine learning-based methods and probabilistic model-based methods. the probabilistic model is of somewhat different nature from machine learning tools, and provides a complement to the latter. thus, combining the two kinds of model is likely to produce a consensus prediction that has better accuracy than the prediction of individual program  <cit> . in addition, the probabilistic model outputs a set of knowledge about the property of secondary structure in an explicit way, including specific correlation structure between neighboring residues, while such information is implicit in nn or svm. hence, the development of an appropriate probabilistic model is interesting for understanding the mechanism by which sequence determines structure.

in this paper we introduce a new probabilistic model, dynamic bayesian network , for protein secondary structure prediction. dbn represents a directed graphical model of a stochastic process, often regarded as a generalized hmm capable of describing correlation structure in a more flexible way  <cit> . a novel feature of our method is the introduction of a multivariate gaussian distribution for the profile of each residue, which takes into account the correlation between entries of the pssm. in addition, our method considers a high-ordered dependency between profiles of neighboring residues and introduces a segment length distribution for each secondary structure state. testing results show that the dbn method has made a significant improvement in accuracy over previous pure hmm-type methods. further improvement is achieved by combining the dbn with an nn, a method named dbnn, which has achieved better q <dig> accuracy than many other popular methods and is competitive to the current state-of-the-arts. the most interesting feature of dbn/dbnn is that a significant improvement in the prediction accuracy is achieved when combined with other methods by a simple consensus.

RESULTS
training and testing datasets
three public datasets are employed for training and testing, i.e. cb <dig>  <cit> , eva  <cit>  common set, and a large dataset containing  <dig>  chains  constructed by g. karypis  <cit> . the first dataset contains  <dig> protein sequences with guaranteed non-redundancy via a strict criterion  for the sequence similarity; this dataset is used independently from two other datasets. the second is obtained from eva server, where several secondary structure prediction servers are evaluated with sequences deposited in pdb  <cit> . in particular, a set labeled as "common set 6"  is selected, which contains  <dig> protein chains and has been used to test several popular prediction methods  <cit> . the third dataset, evatrain, is used in conjunction with evac <dig>  with the former for training and the latter for testing. evatrain has been guaranteed to have less than 25% sequence identities to evac <dig> 

furthermore, we have built a fourth dataset based on the known tertiary structural similarity from the scop  <cit>  database , to evaluate the performance of our methods when dealing with proteins of remote evolutionary relation. one protein domain for each superfamily of the four classes  is selected. the domains of multi-segment, of nmr structure, and of low x-ray resolution  are removed. also, too short  or too long  sequences are removed. the final dataset contains  <dig> protein sequences and is referred to as sd <dig> 

for all the datasets described above, the secondary structure is assigned by dssp program  <cit> , and the eight-state secondary structure is converted to three, according to the rule: h, g, and i to h ; e and b to e ; all others to c .

window sizes
the window sizes, denoted by laa and lss for profile and secondary structure respectively, describe the range of dependency of current site on its neighbors. the correlation between the q <dig> accuracy of dbn and window sizes is studied via a set of seven-fold cross-validation tests of dbnsigmoid  on sd <dig> using different window sizes. due to the limitation in the computational resources, the upper bounds of laa and lss are set to be  <dig> and  <dig>  respectively.

as shown in fig.  <dig>  q <dig> is improved significantly when lss >  <dig>  and saturated when lss >  <dig>  which indicates that there is strong short-range dependency between the profile of a residue and the secondary structure states of its neighbors. a similar phenomenon occurs for profiles' dependency of neighboring sites. note that the model with either laa =  <dig> or lss =  <dig> is a special case of dbn, in which the distribution of the profile of each residue is independent from neighboring profiles or neighboring secondary structure states, respectively. as a result, its topology is different from that of a full-dbn version  due to the removal of ri or di nodes ).

our results are in partial agreement with the conclusions of crooks and brenner, who claimed that each amino acid was dependent on the neighboring secondary structure states but was essentially independent from neighboring amino acids  <cit> . we argue, however, that the psi-blast profile has quite different correlation structure from a single amino acid sequence, from which crooks et al. derived their conclusions. in fact, the dependency between neighboring profiles are significant and helpful for improving the prediction accuracy.

fig.  <dig> also shows that the most accurate model occurs when using the set , for which q <dig> reaches about  <dig> %. however, test shows that this model is very time-consuming. we choose a more economical set  which offers a similar q <dig>  with a big saving in computational cost, for all the dbn models used in current study.

the accuracy improvements through combinations
all the basic dbn- and nn-based models described in methods are tested on the sd <dig> dataset, and the results shown in table  <dig> report the performance of these models, as well as of their combinations. specifically, both dbnlinear  and dbnsigmoid  have significantly improved the performance in all the measures, indicating that the two directions of the sequence  contain complementary information. in addition, the combination of the two different pssm-transformation strategies  also contributes to the accuracy improvement, increasing q <dig> and sov by  <dig> % and  <dig> %, respectively, for dbn-based models. note that for nn-based models, the accuracy improvement by combination is much less evident, indicating that nn is not sensitive to pssm-transformation strategies.

all the eleven models listed in the table are described in methods. the average results of seven-fold cross-validation are shown.

finally, the combination of all the basic dbn- and nn-based models, which produces the resultant dbnn, has achieved further improvement in the accuracy, increasing q <dig> and sov by  <dig> % and  <dig> %, respectively, compared to dbnfinal . this implies that the two types of models are indeed complementary.

secondary structure segment length distributions
to study the significance of the secondary structure segment length distributions introduced in dbn models, we define a degenerate dbn , which has the same structure to dbnfinal except dmax =  <dig> . as described in methods, dmax =  <dig> implies a geometric distribution for the segment lengths. the segment length distributions of the predicted secondary structure by both dbnfinal and dbngeo are calculated and compared to the true distributions observed in the sd <dig> dataset, as shown in fig. 3–. in particular, fig.  <dig> shows that, for helices, the segments of one and two residues are over-predicted, while those of three residues are under-predicted, by both dbnfinal and dbngeo. but longer segments are all predicted correctly by both models. generally speaking, dbnfinal has better performance than dbngeo: the prediction of dbnfinal for segments of  <dig> and 5– <dig> residues is much better than that of dbngeo.

fig.  <dig> and  <dig> show the segment length distributions for sheets and coils, respectively. both dbnfinal and dbngeo have missed a rich population of one residue, and over-predicted segments of 3– <dig> residues, for sheets. dbngeo has predicted a spurious peak for segments of  <dig> and  <dig> residues, which is absent in the true distribution. on the contrary, dbnfinal gives a distribution closer to the observation, in which the peak is located at segments of about  <dig> residues. fig.  <dig> shows that dbnfinal and dbngeo have very similar performance for coils: both under-predict the segments of  <dig> and  <dig> residues and over-predict those of  <dig> and  <dig> residues. however, dbnfinal predicts a much better distribution for long coils  than dbngeo.

it is interesting to study whether we can modify the a priori segment length distribution, gα in eq. , to get a predicted  distribution closer to the observation shown in fig.  <dig>  a calculation is made by using a modified version of dbnfinal, denoted by dbnmod, which is constructed as following: take the a priori segment length distribution directly from the training set, then run the prediction and calculate the posterior distribution, and finally modify the a priori distribution according to the following equation:

  gαnew=max⁡{gαold+ <dig> , 0}, 

where gαold is the a priori segment length distribution before the modification, gαpre is the predicted distribution, gαobs is the observed distribution, α = h, e, or c, and n =  <dig>   <dig>  ... dmax. the quantity gαnew is then normalized to form the new a priori segment length distribution. the eq.  enhances the population of deficient segments and reduces that of over-represented ones, in a linear fashion. all the three models, dbnfinal, dbngeo, and dbnmod, are tested on sd <dig>  and the performance on segment length distributions prediction is measured by "relative entropies", defined by

  hα=∑n=1dmax⁡gαobs⋅log⁡2gαobsgαpre, 

where gαobs, gαpre, and dmax have the same definitions as above, and α = h, e, or c.

the results presented in table  <dig> show that dbngeo has much higher relative entropies indicating a strong deviation of the predicted distributions from the observation, than other two models. note that q <dig> and sov of dbngeo are also much lower than that of dbnfinal , implying that the segment length distributions do have an effect on the prediction accuracy. on the other hand, dbnmod shows the lowest relative entropies for all the three secondary structure states with almost the same q <dig> and sov to dbnfinal , which indicates that eq.  has effectively improved the prediction of segment length distributions.

the seven-fold cross-validation test results on three models with different segment length distributions are explained in the text. the performance is measured by q <dig>  sov, and the relative entropies between the observed segment length distributions from sd <dig> and the model's predictions . clearly, dbnfinal and dbnmod have visible improvement over dbngeo.

comparison between dbn and leading hmm-type methods
the dbn method  developed in this work is also evaluated on the widely used cb <dig> dataset, and its performance is compared to two recently published hmm-type methods, denoted by hmmcrooks  <cit>  and hmmchu  <cit> , respectively, both of which have also been tested on the same or a similar dataset. in comparison, we have calculated the significant-difference margin  for each score, which is defined as the standard deviation divided by the square root of the number of proteins and was used by others  <cit> . the results presented in table  <dig> show that dbnfinal has made improvements for all measures compared to the two methods mentioned above. specifically, dbnfinal improves q <dig> by  <dig> % over hmmcrooks and  <dig> % over hmmchu, and improves sov by  <dig> % over hmmchu. since the errsig for q <dig> and sov are  <dig>  and  <dig> , respectively, the improvements are judged to be significant. matthews' coefficients  <cit>  shown in table  <dig> indicate that dbnfinal is particularly good at the prediction of helices and sheets, compared to above two methods.

dbnfinal and dbndiag are methods developed in this work and their descriptions can be found in the text. entries marked with "--" mean that the data could not be obtained from the literature. hmmchu has been trained and tested on the cb <dig> dataset , while all other methods have been trained and tested on the cb <dig> dataset. the average results of seven-fold cross-validation are shown.

the improvements made by dbnfinal are believed mainly due to the use of a conditional linear gaussian distribution to model the psi-blast profile of each residue, in which the correlation between the  <dig> entries in the profile is considered . in contrast, both hmmcrooks and hmmchu employ a multinomial distribution to model the profile, which lacks the above correlation information  <cit> . the supporting experiment of our conjecture consists in constructing a degenerate dbn model  that has the similar architecture to dbnfinal but only has a diagonal covariance matrix for the distribution of aai , so that the correlation between entries of the profile is ignored. we have tested this model on the cb <dig> dataset, and the results  show that the q <dig> of dbndiag drops down to  <dig> %, similar to those of hmmcrooks and hmmchu, which highlights the importance of the non-diagonal entries in the covariance matrix.

comparison between dbnn and other popular methods
cb <dig> dataset
the best models developed in this work, dbnn, is then tested on the cb <dig> dataset and compared to other popular methods. specifically, the methods svm  <cit> , pmsvm  <cit> , svmpsi  <cit> , jnet  <cit> , spine  <cit> , and yasspp  <cit>  are selected for comparison, because they have been tested on the same  dataset. table  <dig> shows that dbnn has the best q <dig> accuracy among all the methods mentioned above, with improvements ranging from  <dig> % to  <dig> %. since the errsig is  <dig> / <dig> , this indicates that for all methods except yasspp, the improvement made by dbnn is significant. in sov measure, dbnn ranks second, below yasspp but above svmpsi. the comparison of the matthews' coefficients between dbnn and yasspp indicates that the two methods are complementary and may be combined to obtain further improvement in the prediction accuracy: dbnn has a better ch while yasspp has a better cc.

the description of dbnn can be found in methods. entries marked with "--" mean that the data could not be obtained from literatures. jnet has been trained and tested on the cb <dig> dataset , while all other methods have been trained and tested on the cb <dig> dataset. methods marked with "†" have been evaluated using ten-fold cross-validation, while others have been evaluated using seven-fold cross-validation.

eva dataset
dbnn is also compared to some live prediction servers by using the evac <dig> dataset and eva website. the methods selected to compare are: prospect  <cit> , prof_king  <cit> , sam-t <dig>  <cit> , psipred  <cit> , profsec , and phdpsi  <cit> , and their evaluation results on evac <dig> are obtained directly from the eva website  <cit> . because not all sequences are tested against all methods, the evac <dig> dataset is rearranged into five subsets, and the comparison is made between methods that are tested on the same subset .

dbnn and the three consensus methods  developed in this work are compared with other leading methods on five subsets of evac6; each comparison is carried out with maximum number of common sequences. the results of the six existing methods, prospect, prof_king, sam-t <dig>  profsec, phdpsi, and psipred, are obtained directly from the eva website.

the t-tests are also performed for rigorous pairwise comparison between different methods. specifically, we test the hypothesis that "method x" gives a significantly higher mean score than "method y", by calculated t-values as t=d¯/σn, where d = ; x is the accuracy score of "method x", and y is of "method y"; σ=2/n]/, and n = the number of proteins. we have evaluated all the methods on the subset  <dig> of evac <dig> , of which the prediction data of existing methods can be obtained directly from eva website . the results shown in table  <dig> indicate that dbnn has significantly better prediction, in both q <dig> and sov, than prof_king and phdpsi, and has competitive performance to the three state-of-the-arts: psipred, sam-t <dig>  and profsec.

the t-values are calculated for the differences in accuracy scores between "method x" and "method y"  tested on evac <dig> subset  <dig>  the descriptions of dbnn, cm <dig>  cm <dig> and cm <dig> can be found in the text. underlined are where calculated t > tabulated t . the tabulated t =  <dig>  for α =  <dig>  and degree of freedom =  <dig> 

all the above evaluation work shows that prediction accuracy of protein secondary structure by any individual program seems to reach a limit, no better q <dig> than 78% . previous studies  <cit>  show that a simple way to achieve further improvement is to construct a consensus over several independent predictors. the consensus would be effective if the individual predictors are mutually complementary . so, the study of consensus performance is also a way to judge if a new method or program brings in new  information. this study is carried out with a design of three consensus methods  using a simple "weighted vote" strategy to generate the final output: cm <dig> combines the five existing popular methods, prof_king, sam-t <dig>  psipred, profsec, and phdpsi; cm <dig> repeatedly replaces one of the above five methods by dbnfinal, and cm <dig> is the same as cm <dig> except dbnn is in the place of dbnfinal. the weight for the vote of each method is set to be the success rate of the method for each type of secondary structure, which is derived from an individual evaluation of its own. the cm-series are evaluated on the subset  <dig> of evac <dig>  the results shown in table  <dig> indicate that cm <dig> has the top performance and that dbnn brings in complementary information to the family of existing methods. note that cm <dig> ranks second , indicating that the success of dbnn is derived from dbn.

the t-tests between the cm-series and the individual methods are also performed, and the results shown in table  <dig> indicate that a simple combination of the five existing methods does not make significant improvement in accuracy: the individual method sam-t <dig> has competitive q <dig> to cm <dig>  on the other hand, the inclusion of dbn or dbnn  has given rise to significantly better q <dig> than all individual methods including sam-t <dig>  this is further enhanced by a direct comparison between cm <dig> and cm1; significant improvements in both q <dig> and sov are clearly evidenced. finally, let us note that none of the consensus methods shows significant improvement in sov over all individual methods, indicating that sov is particularly hard to improve.

CONCLUSIONS
a new method for protein secondary structure prediction of probabilistic nature based on dynamic bayesian networks is developed and evaluated by several measures, which has shown significantly better prediction accuracy than previous pure hmm-type methods such as hmmcrooks and hmmchu. the improvement is mainly due to the use of a multivariate gaussian distribution for the psi-blast profile of each residue and the consideration of dependency between profiles of neighboring residues. in addition, because of the introduction of secondary structure segment length distributions in the model, dbn shows much better sov than a typical nn.

the essentially different nature of dbn and nn inspires a model that combines the two and forms the dbnn with significant further improvements in both q <dig> and sov. dbnn is shown to be better than most of popular methods and competitive compared to the three state-of-the-art programs. we are then encouraged to explore further with consensus methods that combine all the best existing methods together. this study has demonstrated again the uniqueness of dbnn: the best consensus method is achieved by the inclusion of dbnn. this provides the evidence that dbnn brings in complementary information to the family of existing methods.

an interesting feature of our work here, compared to nn or svm, is that it provides a set of distributions which have specific meanings and which can be studied further to improve our understanding of the model's behavior behind the prediction. an example is provided regarding the secondary structure segment length distributions used by the dbn, which is set to be an a priori distribution but can further be adjusted and improved. this points to a way for further improving the performance of dbn, by including modifications on more distributions, such as the transition probabilities between secondary structure states or the distribution of the profile of each residue. these distributions are also interesting for advancing the understanding of such fundamental problems as protein dynamics and protein folding, for which the information in implicit form in nn or svm is of little use.

it appears that the limits of secondary structure prediction are being reached as no new method over the past decade has shown any major improvement since psipred. all of the top methods are between 77%–80% accurate, in terms of q <dig>  depending on data set used. this implies that the complexity of the sequence-structure relationship is such that any single tool, when it attempts to extract  and to extrapolate  the knowledge, can only represent some facets of this relationship, but not the whole. further hope lies in the possibility that more facets are covered by new models, and that new models are integrated with the existing ones. the consensus methods reported above are just a simple approach in that direction; more sophisticated strategy for combining multiple scores can be sought in the future.

