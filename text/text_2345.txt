BACKGROUND
a general biological principle states that cellular function results from the combined interactions of sub-cellular structures in space and time. interactions typically manifest themselves through statistical dependencies in the spatial distributions of the involved structures. here, we adopt this general definition and we understand interaction as the collection of all effects that cause significant  correlations in the positions of the participating objects.

over the last decades, advances in fluorescent markers have enabled probing interactions of sub-cellular structures in the microscope, either directly or indirectly. the direct approach relies on experiments that generate a signal upon the proximity required for molecular interaction. indirect approaches are based on independently imaging two populations of interest, and searching for clues of interaction in their spatial distributions. this approach is based on the paradigm that spatial proximity  is a hallmark of many types of physical and chemical interactions between sub-cellular structures. if two or more structures interact, their spatial distributions hence appear correlated. the reverse, however, is not necessarily true. presence or absence of significant co-localization does not imply presence or absence of interaction. the reason is that co-localization depends on the specific interaction mechanism: an unobserved third structure may act as a confounding factor , making the observed structures appear co-localized even though they do not interact. furthermore, one can imagine interaction mechanisms that lead to spatial distributions with correlations that are not captured by simple co-localization measures. hence, the interaction has to be statistically inferred from the data.

such inference, however, entails a trade-off between the objectives of pattern discovery and statistical detection power. according to these objectives, two complementary approaches to co-localization analysis can be distinguished: intensity correlation methods capitalize on pattern discovery  <cit> , whereas object-based methods  <cit>  emphasize detection power. intensity correlation methods quantify correlations in the intensities of different color channels on individual pixels. intensity correlation methods are straightforward to implement and use. the results, however, may be difficult to interpret since interactions need to be inferred from correlations in intensity space, which is sensitive to the blurring and noise inherent to microscopic imaging systems  <cit> . object-based methods quantify the spatial relationships between sets of discrete objects. this requires reducing the image to a set of geometric objects using, e.g., image segmentation or fitting of structure models. object-based approaches infer interactions from correlations in physical space, which allows constructing intuitive and simple co-localization measures, such as counting the number of overlapping objects  <cit> .

the intensity-based approach is limited to interactions on a spatial scale on the order of the resolution of the microscope. while the object-based approach is not necessarily limited to any particular length scale , a spatial scale is nevertheless assumed in practice. many object-based co-localization methods rely on a hard threshold for the distances between objects in order to distinguish between "co-localized" and "not co-localized" for each individual pair of objects  <cit> . the choice of distance threshold greatly influences the types of interactions that can be reliably detected. the actual physical or chemical interaction between sub-cellular objects can be of short temporal duration and they can quickly separate thereafter. in such situations, high thresholds can increase the detection power, but only at the expense of increased false-positive rates. when interactions take place over long distances, the choice of threshold implicitly determines a range limit of the analysis.

apart from fixing the interaction scale a priori, using a hard distance threshold also implies a binary distinction of pair-wise distances: either they are below the threshold and hence the objects are assumed to interact - or they don't. a co-localization percentage thus corresponds to an indirect measure for the preference of "interaction" over "non-interaction". this preference reflects the strength of the interaction. however, it also depends on the frequency of possible distances that the population of objects can assume.

more specifically, the cellular context in which the interactions take place is a confounding factor. a high co-localization percentage can, for example, be observed in a cell with densely packed sub-cellular structures of interest, irrespective of their interaction strength. this artifact needs to be considered in statistical tests  <cit>  or corrected for in order to construct an interaction score  <cit> .

taken together, object-based approaches provide intuitive co-localization measures whose statistical interpretation, however, is not straightforward. here, we establish a connection between co-localization and the notion of interaction as used in spatial statistics  <cit> , namely the non-independence of the relative positions of objects under study. this is based on modeling the nearest-neighbor distance distribution between the observed objects. these distances are the result of interactions, measurement inaccuracies, and the geometry of the domain in which the objects are distributed. this modeling provides generic procedures for inferring interaction strengths and quantifying their statistical significance. our approach helps formalizing design decisions in co-localization and interaction studies and shows how they translate to biological hypotheses. standard object-based co-localization analysis is included as a special case, which makes explicit the connections between interaction and co-localization. after developing and characterizing the statistical interaction analysis framework, we exemplify its utility in a biological study of virus entry.

RESULTS
basic scenario: co-localization analysis
we review the basic concepts of classical object-based co-localization analysis and its interpretation in terms of interactions.

object-based co-localization measures are typically constructed for two sets of objects  and . these objects are located in a bounded region Ω ⊂ ℝn with boundary ∂Ω and dimensionality n . each object i  is represented by a feature vector xi  that comprises information about the object's position and, if available, its dimension and shape. these features vectors are extracted from image data by means of image segmentation or fitting of structure models.

suppose one wishes to investigate the interaction between the objects in x and y, one can define for each xi the distance to the nearest neighbor  in y,   

the function d is a suitable distance function in feature space, for example the euclidean distance between point-like objects or the minimum distance between outlines of more complex objects. a nearest-neighbor distance distribution p can then be estimated from the set of distances . the classical overlap or nearest-neighbor-distance co-localization measure ct follows by counting  <cit> :   

where  <dig> is the indicator function and t an application-specific distance threshold. the form of eq.  <dig> implies assumptions about how the objects in x and y interact. the interaction process is considered to be translation- and rotation-invariant since only the distance between interacting objects is taken into account. based on this distance only two categories of positions of the objects in x are distinguished: either they are sufficiently close to any object in y to be considered interacting, or they are not. furthermore, objects in x interact with at most one object in y and they do not experience the presence of any yj unless they cross the distance threshold t. the choice of t reflects an assumption about the length scale of the interaction to be detected.

inferring interactions from an observed co-localization measure ct is not trivial since ct >  <dig> does not necessarily imply any interaction between the objects. this is because spatial correlations can also be caused by confounding factors, such as the cellular context {Ω, y}. even if the objects in x and y do not interact there is a finite probability that any possible distance in an interval Δd about di is observed. we arbitrarily choose y as a reference in order to compute the relative frequency of possible distances  as:   

this density q is determined by the positions, dimensions, and number density of the objects in y . independent random positions will result in a relatively wide density q . with regularly placed objects y, large distances do not occur . clustering increases the frequency of long distances at the expense of short distances . objects with large surfaces or a large number density give rise to shorter distances. in case there are interactions between the objects in x and y, some of the possible distances are additionally favored over others, deforming the density q to p.

the co-localization measure ct is, therefore, not sufficient to separate the contributions from the cellular context and the interactions. information about the interactions is only contained in the deviation from an expected base-level in the absence of interactions. this base level, , is the co-localization measure that would be observed under the hypothesis h0: "no interaction"  = q and numerical evaluation of the integral in eq. 2). but how does a certain deviation from the base level  relate to interactions between the objects, and what deviations can be considered significant? we address this question in the following sections by generalizing co-localization analysis to interaction analysis. ideally, an interaction score is independent of the cellular context and reflects variations of the interaction strength in a monotonous fashion. the first step toward constructing such a score is a precise definition of the term interaction strength in the context of an interaction model.

generalization: interaction analysis
spatial point process analysis  <cit>  is a standard statistical framework for studying the spatial distribution of interacting objects. our interaction analysis is derived from the general binary gibbs process with fixed number of objects. its central component is an effective pair-wise interaction potential Φ. in many applications, "interaction" is an abstraction of the different effects that collectively cause an observed spatial pattern. nevertheless, the mathematical form of the gibbs process corresponds to physical models of interacting objects. the potential associates an energy level with each pair {i, j} of interacting objects. the probability density of the gibbs process for two sets of interacting objects, x and y, has the shape of a boltzmann distribution:   

i.e., states with lower energy occur with higher probability. eq.  <dig> implies mutual independence of the objects within the same set x or y, in agreement with the assumptions formulated in the previous section. for nearest-neighbor interactions, the corresponding interaction potential is given by:   

where the function ϕ specifies the distance dependence of the interaction.

assume a cellular context {Ω, y} is given. the probability density p for the potential in eq.  <dig> then only depends on the di. an inner sum over all j, as in eq.  <dig>  is then not required. the mutual independence within x allows factorizing p into terms that only depend on a single di:   

where, unlike in eq.  <dig>  an explicit dependence of the potential on xi is no longer present.

the probability of observing a certain xi is proportional to exp ). the probability of observing a certain di, however, also depends on how frequently an arbitrary object x is a distance di away from its nn in the given cellular context. this frequency is given by the state density q as stated in eq.  <dig>  straightforward calculations yield:   

the normalization constant z  renders p  a true probability density function.

so far, we have not specified any particular shape for the interaction potential ϕ, which can be a parametric or non-parametric model. a specific choice constitutes a hypothesis or assumption about the range, strength, and distance dependence of the interaction. these three aspects of the interaction are represented independently in our parameterization:   

ϵ is the strength, f encodes the shape, σ defines the length-scale, and t is a shift along the distance axis of the interaction potential. using eqs.  <dig> and  <dig> we find the joint probability density of observations d:   

this is the central class of models that we use to extend co-localization analysis to interaction analysis. all interaction models will be formulated as specific instances of such a model.

the assumptions underlying the simple overlap co-localization measure can, for example, be formalized in a specific interaction potential. only two categories of distances  are distinguished . this implies a step function for the shape f of the interaction potential ϕ :   

using the integral definition in eq.  <dig>  the co-localization measure ct can then be expressed as a function of the interaction strength. inserting eq.  <dig> into eq.  <dig> and eq.  <dig> and solving for ϵ yields an estimator  of the model interaction strength:   

the quantity  corrects for the cellular context and, therefore, fulfills our requirements for a valid interaction score. eq.  <dig> relates the purely descriptive co-localization measure ct to an interaction model between the objects in x and y. it builds a bridge between patterns in the data  and functional relationships  between sub-cellular components.

whether an observed estimate  is indicative of the actual presence of an interaction, however, has to be addressed using statistical inference as presented in the following section.

hypothesis testing and power analysis for the step potential
in the parameterization of our interaction model , the presence of an interaction is equivalent to ϵ ≠  <dig>  since  is an estimator, it is a random variable. even if the hypothesis h0: "no interaction" is true, a non-zero  can occur with finite probability . inference about interactions requires finding a critical estimated interaction strength above which one can reject h <dig> on a prescribed significance level α.

this critical interaction strength is determined by the distribution of  under h <dig> , which depends on the sample size n, q, and the prescribed α. under h <dig>  ctn is binomially distributed with parameters . hence, the critical ct can be computed from the  inverted cumulative distribution function of the binomial distribution. the corresponding critical  follows from eq.  <dig> 

the dependence of the critical ct and  on  and n is shown in fig. 2a and 2b. it can be seen that the minimum significant excess over  varies only weakly with  . obviously, large values of  in conjunction with small n do not allow rejecting h <dig>  even if ct =  <dig>  the critical value of  is highest at the two extremes of  and lowest for  ≈  <dig>  . as for ct, it can be seen that for large  and small n no finite  is sufficiently large to allow rejecting h <dig> 

the curves in fig. 2b show the decision of the statistical test based on the estimated interaction strength . a true interaction with a strength ϵ greater than this critical value does, however, not guarantee that it will always be detected by the test . furthermore, a weak interaction may lead to unwanted rejection of h <dig>  the behavior of the test critically depends on the effect size, which quantifies the departure from h <dig>  here, effect size refers to the true interaction strength ϵ = a >  <dig>  the statistical "power"  quantities the probability of rejecting h <dig> when h1: "ϕ = ϕst, ϵ = a" is true. in fig. 2c, the detection power for a true strength of a =  <dig> is shown as a function of . as expected from fig. 2b, the power is low at the extremes of , eventually dropping significantly below the recommended value of  <dig> , even for n =  <dig>  weak interactions are harder to detect, requiring larger sample sizes to yield a certain power.

in the design of experimental interaction studies, a key objective is to maximize the robustness and reliability of detecting effects of unknown size. power can be increased by optimizing the experimental design or the subsequent statistical analysis. while increasing the sample size might be possible, controlling the cellular context is not feasible in most situations. our analysis is based on the interaction model introduced in the previous section. it allows specifying different shapes f and scales σ of the interaction potential. power could potentially be increased by better modeling the interaction potential. in the next section, we thus quantify the influence of alternative model potentials on statistical power.

improving statistical power with non-step interaction potentials
constructing statistical tests as described above requires assuming a specific shape and scale of the interaction potential. in the absence of prior knowledge, however, this model potential can be arbitrarily different from the true potential of the actual biological interactions under observation. test statistics that are based on a model potential close to the real one may achieve greater power.

we quantify the influence of the discrepancy between the model and the true potential by considering a scenario where n objects {xi} are distributed in the square region Ω containing m randomly placed circular objects {yi} with identical radii r. fig. 3a shows the corresponding state density q. the objects in x interact with the objects in y according to the plummer potential :   

this potential has an overall  <dig> = d-shape, but finite value and slope everywhere. the parameter ϵ again controls the interaction strength . the parameter σ sets the length scale of the interaction  and allows gradually changing ϕ from a step-like shape to a potential that causes significant attraction toward the objects in y over large distances .

for such more general potentials, algebraic expressions for   can in general not be derived. statistical tests for the presence of interactions can nevertheless be constructed using a different statistic. since eq.  <dig> describes a member of the exponential family,   

is a sufficient test statistic for ϵ  <cit> .

for a set of distances d, distributed according to eq.  <dig> with ϕ = ϕpl, a test for the presence of interactions can thus be constructed based on  under h0: "no interaction", where the scale parameter σ is assumed to be known. the null-distribution can be approximated by i.i.d. monte carlo  samples  . an observed value of tpl is then ranked among the . if it ranks higher than ⌈k⌉-th, h <dig> is rejected on the significance level α  <cit> . the statistical power of this test to reject h <dig> when h1: ϕ = ϕpl, ϵ = a" is true, can be estimated with additional mc simulations: for a fixed effect size a >  <dig>  one draws n distances di from p, computes tpl, and conducts the test as described above  <cit> . this procedure is repeated many times and the fraction of tests rejected serves as an estimator of the power.

in order to quantify the influence of the model potential on statistical power, we test h <dig> against h <dig> and h2: "ϕ = ϕst, ϵ = a" on data generated under h <dig> for varying σ . testing h <dig> against h <dig> makes use of the sufficient statistic , which is proportional to ct with t =  <dig>  as opposed to tpl, this statistic only contains information about the signs of the di and should thus yield a less powerful test.

fig. 3c shows the number of samples required to reach 80% power as a function of the strength a of the true interaction potential. it can be seen that the power of a test based on the true interaction potential  is higher than the power of a test based on a step potential . moreover, this difference strongly increases with increasing potential range σ: for σ =  <dig>  using the step model potential requires  <dig> times more samples. if the true potential is close to a step potential , both tests perform comparably well. moreover, the figure also shows that interactions over longer distances are harder to detect. we therefore conclude that one needs to be careful when assuming a step potential . controlling power requires prior knowledge about the interaction potential. such prior knowledge can easily be included in the present framework by choosing t, σ, and f.

example: virus trafficking
the uptake and intracellular transport of virus particles is a complex process that involves temporary association with membrane receptors and multiple organelles of the endocytic machinery, such as early and late endosomes  <cit> . in many cases, fluorescence microscopy allows resolving the involved entities as discrete objects. this has previously motivated the use of object-based co-localization measures to quantify association kinetics and unravel infection pathways. here, we show how the generalized framework of interaction analysis presented above can be applied in a practical experimental situation, and how it enables using a large toolbox of well-known statistical techniques.

we consider a set of  <dig> two-color fluorescence microscopy images of single her- <dig> cells expressing the small gtpase rab <dig> tagged with enhanced green fluorescent protein , recorded in the green color channel. rab <dig> is a regulator of clathrin-mediated endocytosis and a marker for early endosomes. these dynamic, lipid-bounded organelles are formed by invaginations of the plasma membrane. they are the first sorting compartment of clathrin-derived cargo  <cit> . either fluorescently tagged adenovirus serotype  <dig>  or its temperature sensitive mutant  were recorded in the red color channel. images were taken between  <dig> and  <dig> min post infection. the same data have already been used in a previous study  <cit> . virus positions and endosome outlines were extracted from the images as described in the materials and methods section. based on these object representations, the set d of virus-to-nearest-endosome distances and the state density q were computed for each of the imaged cells.

like ad <dig>  ts <dig> is known to enter the cell by clathrin-mediated endocytosis, but the mutation inhibits escape from endosomes  <cit> . this should be reflected in a deviation of the empirical distribution of observed distances d from the null distribution p = q, which is stronger for ts <dig> than for ad <dig>  in our framework, this translates to a non-flat interaction potential between virus centroids and outlines of rab5-positive endosomes.

before modeling an interaction potential, we test h0: "ϕ = 0" against h1: "ϕ ≠ 0" for each imaged cell using a non-parametric statistical test . this test does not assume any specific shape of the interaction potential, which allows detecting any type of interaction, albeit with reduced power. the results are summarized in table  <dig>  the fraction of cells for which h <dig> has to be rejected is significantly higher for ts <dig> than for ad <dig>  irrespective of the significance level and despite the on average smaller sample sizes n. however, ad <dig> exhibits significant interaction with endosomes in half of the cells .

first column: number of cells analyzed; second and third columns: number and percentage of cells for which h <dig> was rejected on the indicated significance levels; forth column: mean and standard deviation of the observed number of virus particles per cell.

these results indicate that the interaction potential is non-zero for many cells. they do not, however, permit any conclusions about the shape or strength of the interaction potential, for which, in addition, no prior information is available. we therefore apply a non-parametric estimation procedure for the interaction potential to get a sketch of its strength and distance-dependence. subsequently we can specify and identify parametric potentials. ignoring, for now, possible variability between cells and virus types, we pool all data and estimate a common non-parametric potential  . the estimated  is shown in fig.  <dig>  its shape is notably different from a step function. the slow decay suggests that viruses interact with endosomes over distances of about  <dig> pixels  from their center.

the estimated non-parametric potential serves as a template for the shape of parametric models. parametric potentials can be identified more robustly from sets of observed distances of individual cells. this allows correlating their parameters with co-variates such as the virus type or the time at which a cell was imaged after infection. we consider four different potentials, two that resemble the shape in fig.  <dig>  and two that are generalizations of the step potential with a plateau below d =  <dig> . for all potentials, we fix the threshold to t =  <dig>  definitions of the potential shapes f are given in the materials and methods section.

the parameters of the potentials are found by maximum likelihood estimation . in order to exclude cell-to-cell variations of the potential range, we do not determine the pairs  for each cell separately. rather, we estimate for a given potential a single scale parameter σk = σ* common to all cells, while the interaction strengths ϵk may vary between cells. the resulting -dimensional estimation problem is solved with a nested ml algorithm . the common scale  and the maximum of the pooled log-likelihood l* for the four potentials are reported in table  <dig>  as a reference, the values are also given for a step potential with distance threshold t =  <dig> 

scale parameters  of potentials as found by maximum-likelihood estimation, and the corresponding maximized pooled log-likelihoods max l* for the different potentials 

the potentials are ranked according to their log-likelihood. it can be seen that the step potential is outperformed by all others. this remains unchanged even if one compares akaike or bayesian information criteria, which take into account the smaller number of free parameters. with a difference in log-likelihood of >  <dig> to second-best fit, the hermquist potential is by far the best fit. it is also subjectively most similar to the non-parametric potential identified above. fig.  <dig> shows an example of an imaged cell, infected with ts <dig>  together with the empirical and estimated distance distributions and the corresponding hermquist potential. the images of ad2-infected cells are visually indistinguishable from those of ts1-infected cells and are hence not shown. despite fitting only one independent parameter , the estimated model distribution captures the features of the data remarkably well.

the estimated interaction strength  of the hermquist potential varies within and between the two groups of infected cells. the within-group variability comprises statistical fluctuations and natural variations between cells. since virus internalization and transport is a dynamic process, the time at which a cell was imaged  is a further source of in-group variability. fig.  <dig> shows the estimated interaction strength of a hermquist potential for all cells infected with ad <dig>  and ts <dig>  as a function of the time post infection. throughout the observation period, the interaction strength for ts <dig> is significantly larger than that for ad <dig>  confirming the trend reported in table  <dig>  furthermore, a temporal maximum of the interaction strength is apparent for ts <dig>  while for ad <dig> no significant variation over time can be resolved. these results indicate that ts <dig> and ad <dig> use different uptake pathways or exhibit significantly different escape kinetics from rab5-positive endosomes.

CONCLUSIONS
we have introduced a statistical inference framework for robustly estimating interaction parameters from experimentally observed object distributions.

this allowed establishing a connection between spatial co-distributions of objects and interaction, by formulating the object-based interaction analysis problem in a spatial statistics framework based on nearest-neighbor distance distributions. the present framework provides generic procedures for inferring interaction strengths and quantifying their statistical significance. standard object-based co-localization analysis is included as a limit case, making explicit the connections between the present framework and more classical approaches.

in the present framework, two novel key quantities emerge:  the state density q, which is the distribution of nearest-neighbor distances expected under the null hypothesis of no interaction, and  the interaction potential ϕ, which defines the strength and distance dependence of the interaction. we have shown that classical co-localization analysis amounts to estimating the parameters of a step potential. this requires a notion of "inside" and "outside", either naturally defined by the physical extent of the objects or imposed through the step function's distance threshold. for point-like objects, or weak correlations between object positions, the choice of distance threshold is arbitrary.

this limitation can be relaxed by affording more general shapes of the interaction potential, which naturally extends co-localization analysis to  co-distribution analysis without requiring any additional assumptions. the additional flexibility allows capturing information about a wider range of sub-cellular interactions. this was demonstrated by statistical power analysis of the classical and generalized measures. our results highlight that the probability of detecting an interaction strongly depends on the cellular context. we furthermore illustrated the influence of the range of an interaction on its detectability. test statistics that include knowledge about the shape of the true interaction potential can greatly reduce the number of samples required to achieve a certain target power. physico-chemical models might provide such prior knowledge. alternatively, a non-parametric phenomenological potential can be estimated from the data as demonstrated here. this potential can then serve as a template for the parametric potentials used in subsequent analyses. in addition, the present framework enables comparison of the likelihoods of different hypothetical physico-chemical interaction models directly on the original image data.

the present approach enables applying a wide range of established statistical tools for analyzing experimental data, from parameter identification to model selection. this workflow was illustrated by studying the spatial patterns of endosomes and viruses infecting live human cells. in this case study, the experimental data were very well explained using only a single free parameter per cell. among the five potentials considered, the step potential  was worst in explaining the data. this highlights the benefit of the present method over classical co-localization analysis. moreover, the fitted potentials provided additional quantitative readouts that could be used in subsequent machine learning analyses.

for simplicity the case study was done on 2d projections of 3d images. the presented approach, however, is equally applicable in three dimensions without any changes, provided three-dimensional object detection and segmentation is available. projecting the data into two dimensions alters the estimated potentials , since it distorts both the distance data d and the state density q. we empirically found that the strengths of the potentials estimated from the projected 2d data may be smaller than those estimated directly on the raw 3d data . although all distances d are systematically reduced by the projection, this effect is overcompensated by the non-linear distortion of q, which is strongest for intermediate distances, but negligible for very small and large distances. besides projection artifacts, errors in the image processing may also influence the estimated co-localization measures. depending on the accuracy of the image segmentation method used, object sizes can be under- or overestimated, or entire objects can be missed altogether. this problem is inherent to all forms of co-localization or distribution analysis. we have assessed the sensitivity of our method with respect to image segmentation errors by successively eroding or dilating the endosomes from the presented case study. the results show that the mean of the estimated strength of the hermquist potential remains unaffected, yet the variance of the estimate increases for strong erosion when entire endosomes start to be missed . this robustness of the present method is due to the state density q correcting for size errors. the classical co-localization measure, naively corrected for the cellular context by subtracting the amount of unspecific co-localization c <dig>  significantly changes when under- or over-estimating object sizes. for strong erosion, leading to very small and frequently missing objects, it even drops to a meaningless value of zero . since image segmentation errors are always present in practical applications, we consider the robustness of our method one of its major advantages over classical measures.

the presented framework is limited by the same assumptions that also underlie classical co-localization analysis:  spatial homogeneity and  isotropy of the interaction within the observation window, and  exclusively nearest-neighbor interactions between objects of different classes. assumption  is, e.g., violated if large areas of the analyzed images do not contain any objects. in this case, estimation of q is not robust. assumption  imposes limits on admissible distances between objects: if objects x are attracted toward objects y, the distances between the objects within the set y need to be larger than the typical interaction range.

all of these limitations could be relaxed by using position-dependent interaction potentials or allowing for many-body interactions as described by general gibbs processes. considering such processes, however, is theoretically and numerically challenging. the presented framework could also be extended by including additional confounding factors, such as imaging artifacts causing spurious co-localization. temporal plasticity of interactions, cell-to-cell variations, and experiment-to-experiment variations could be accounted for through additional co-variates  in the statistical model. already in its present form, the statistical framework can be used to test more general hypotheses, such as "interactions are stronger in strain a than in strain b".

the interpretation of fitted potentials is limited to their relative strengths. in the absence of a mechanistic or physical model of the process that has created the observed spatial pattern, biophysical interpretation of the identified parameter values is difficult or misleading. this is because the fitted interaction potentials reflect the collection of all intracellular phenomena that lead to the observed point pattern. interestingly, however, a relation between the steady-state distribution of a diffusion process with added deterministic forces and the distribution of the gibbs process  exists: if the deterministic force acting between the diffusing objects is given by -∂ϕ/∂d, the two distributions become identical . this fact points a possibility of connecting fitted interaction potentials with biophysical processes.

