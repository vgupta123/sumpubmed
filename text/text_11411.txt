BACKGROUND
a frequent objective of cancer related studies is to detect genes or biomarkers that can predict the outcome of therapy. the hardest criterion of success for therapies is the survival of patients. to identify predictive genes, the expression levels in samples of tumor or normal tissue are measured by dna microarrays before the therapy is applied. then, expression levels are compared to survival data of the patients. usually, tissue samples are only available at distinguished points in time and it can take several years until the full planned sample size is available and follow-up is complete. in such long-lasting studies it would be beneficial to obtain interim results already before their planned end. an early detection of survival related genes within an interim analysis would for example allow their further laboratory validation before the end of the study. in addition, if an interim analysis provides evidence for the early stopping of the study it would save time and costs and would spare further patients to be involved or allow better treatment. classically, interim analyses are performed in studies of group sequential designs. in such designs, interim analyses are performed when a certain fraction of the full planned sample size n has been reached, for example when 1/ <dig> · n and 2/ <dig> · n samples are available. there are numerous articles that deal with group sequential designs in the case of one single feature, e.g.  <cit> . as the repeated testing of the same hypothesis by interim analyses is one form of multiple testing and thus inflates the overall type i error  <cit> , both articles propose reduced significance levels to solve this problem. dna microarray data, however, comprise expression levels for thousands of genes, meaning that there are more features than available samples. for this case of high-dimensional data there have been very few approaches published to date. marot and mayer  <cit> , for example, propose a method for combining p-values from several independent microarray analyses and show that the overall false discovery rate is not inflated when testing repeatedly a large number of hypothesis. a similar result was obtained by posch et al.  <cit> . we make use of these results and apply them for studies in which survival data is correlated with gene expression data in interim analyses. in order to detect the survival related genes, we use gene-wise cox-regression analyses  <cit> .

an important issue in interim analyses of multiple features is the choice of a stopping criterion. in the case of testing one single feature, the study is simply stopped when a significant result is observed. similarly, victor and hommel  <cit>  use gene-wise stopping rules in the case of high-dimensional microarray data. marot and mayer  <cit>  and posch et al.  <cit>  propose to stop the study when a certain proportion of true positive genes has been detected. we pick up the latter idea and derive an estimator for the proportion of true positive findings this new estimator is compared to a variant based on  <cit>  and to an estimator proposed in  <cit> .

the article is structured as follows. the study design, the detection of survival-related genes, the problem of performing interim analyses in the case of multiple hypothesis testing, and the stop criterion are detailed in the methods section. subsequently, a simulation study evaluates the behavior of the false discovery rate and of the estimator for the proportion of true positive detections. the simulation study covers several settings of survival-focused microarray experiments with interim analyses. after presenting the results from these simulations, we apply our methods to gene expression data from a breast cancer study van de vijver et al.  <cit> . parameters from this breast cancer study are used one more simulation presented subsequently. finally, a discussion on the results follows, and further ideas are given.

methods
this section starts with an illustration of the particular study design we are considering in this article. as survival is a special focus of this work, we detail afterwards the methods for the detection of survival-related genes. next, an overview of common methods for multiple hypothesis testing and their application in group sequential interim analyses is given. in this context, we also describe rules for the early stopping of such studies.

first, let us introduce the basic notation. let n be the total number of subjects that would be involved if the study was not stopped after any interim analysis. for each of the relating tissue samples the expression levels of d genes are measured by means of dna microarrays. we denote the complete  data matrix with all genes and all samples by x. as typical for microarray data, each row represents one gene and each column represents one sample. thus, xij denotes the expression level of gene i in the tissue sample of subject j . an overview of all notations is given in table  <dig> 

study design
assume, the whole study is not stopped after any interim analysis. then, the n patients will have individual arrival times a = ', e.g. months after the begin of the study. we denote the arrival time of the last patient to enter the study by l <dig> = max. thus, the first part of the study, during which patients are recruited, will take place in the time frame .

let us consider a second study episode which serves as a follow-up time of length l <dig>  where no new patients enter the study but the patients survival data is still observed and up-dated. thus, the length of the full study without any early stopping would be l = l <dig> + l <dig>  and the time frame of the second study part would be . the study design is visualized in figure  <dig> 

assume further, that m <dig> interim analyses are planned to take place during the first study part. an interim analysis is always performed when  · n new samples are available. this makes sure, that the sample size in each analysis is equal. in addition, m <dig> interim analyses are planned for the second study part, where an interim analysis is always performed when a time proportion of  · l <dig> of the planned follow-up time has passed. thus, we chose to perform analyses at equally spaced time-steps during this second part of the study. we denote the times at which interim analyses are performed by tm .

detecting survival-related genes
disease specific survival is the hardest control for measuring the success of cancer therapies. therefore, we modelled the survival information in dependence on the gene expression data using cox proportional hazard regression as was proposed for example by simon et al.  <cit> . survival information consists for each patient j of a pair , where sj is the observed survival time of the patient, and zj ∈ { <dig>  1} specifies whether the patient has already died or not at the time the analysis takes place. according to the cox-regression model, the survival information is modelled in terms of the hazard function at time t in dependence on gene i by   

where h <dig> is an unspecified function of t, called the baseline hazard. the hazard function h can be interpreted as the patients risk of dying in a short time frame, [t, t + ε), assuming the patient has survived thus far  <cit> . more precisely, the hazard function is defined as   

where t* is the patients observed survival time  <cit> . the influence of gene i on survival can be determined by testing the hypothesis h0i: βi =  <dig> in the related cox model. the d resulting p-values from the gene-wise survival analyses can than be adjusted for multiple testing as described below.

in general, other models than the cox model can be considered to detect survival-related genes. park et al.  <cit>  for example propose to use partial least squares regression to account for the presence of covariates.

interim analyses of multiple endpoints
in each interim analysis one statistical test is performed per gene in order to detect those genes which correspond to the studied response variable .

if the d hypotheses were all true and independent, testing each of them at the same level α, the expected number of false positive detections would be given by α · d. in whole genome microarray experiments, where d is typically about  <dig> , the expected number of false positive detections would be too large to be tolerable. in multiple testing situations, it is therefore common to reduce the number of false positive decisions by controlling a pre-specified type i error rate. note that the notion of type i error rate is not used consistently in the literature. following dudoit et al.  <cit>  we will use the term type i error rate to name the superordinate concept of different types of such error rates, among which there are the family-wise error rate  and the false discovery rate .

in microarray experiments the fdr, introduced by benjamini and hochberg  <cit> , is the most commonly considered type i error rate. the fdr is defined as the expected proportion of false positives among all positive test decisions, i.e. fdr = e, where r >  <dig> denotes the total number of rejected null-hypotheses. the proportion of false positives  among all positives itself is also known as false discovery proportion . in the special case that r =  <dig>  i.e. no positive test decisions were found, the fdp as well as the fdr are defined to be zero.

the fdr can be controlled by adjusting the raw p-values resulting from the gene-wise tests. the adjusted p-values are then compared with a pre-specified level α of the fdr that is desired to be controlled. we denote the unadjusted p-value for gene i by pi and the respective adjusted p-value by . in our simulation study, we consider the adjusting procedure proposed by benjamini and hochberg  <cit> . other adjusting procedures are detailed in  <cit> .

alternatively to comparing the adjusted p-values with the pre-specified fdr-level α, genes can be selected by comparing the raw p-values with adjusted α-levels. according to the procedure in  <cit> , the raw p-values are ordered by increasing size, i.e. p ≤ p ≤ ... ≤ p, and the largest k  for which p ≤  α has to be determined. all hypothesis associated with p to p are then rejected. we denote the adjusted α-level that corresponds to this largest value of k by αbh.

similar as in multiple hypothesis testing, the control of type i error rates is an important issue in group sequential interim analyses. in clinical trials on one single feature , interim analyses have been studied in-depth. as was shown by armitage et al.  <cit> , performing interim analyses increases the probability for making a type i error. in order to avoid such an increase and to maintain a pre-specified type i error, the tests at each interim analysis are performed at lower nominal levels  . the first authors who proposed α-level adjustments for group sequential interim analyses were pocock  <cit>  and o'brien and fleming  <cit> .

at first glance, performing interim analyses in microarray experiments seems to require two adjustments. one to account for the interim analyses and one to account for multiple testing. however, the two recently published articles by posch et al.  <cit>  and by marot and mayer  <cit> , show that the adjustment for interim analyses can be omitted, when d is large, while the fdr remains controlled. the result in  <cit>  is based on the observation that the correlation between a single p-value and the empirical distribution of all p-values approaches zero when d gets large. the results in  <cit>  are based on the findings of storey et al.  <cit>  who proved that, under certain assumptions,   

this holds for each interim analysis independently. let  denote the random interim analysis where the study is stopped. posch et al.  <cit>  proved that equation  holds also at this interim analysis , since   

with equations  and  and the lemma of fatou it follows that the fdr is controlled asymptotically when d gets large.

this argumentation is not valid under the global null hypothesis , but it is also possible to extend the argumentation to the case of the global null hypothesis  <cit> .

when performing interim analyses in experiments with multiple endpoints, one has to decide which data to base the interim analysis on. we decided to use all available accumulated data in each interim analysis.

this approach makes sure, that every analysis uses the maximal available data. however, one has to be aware of its drawback: it requires renormalization of the data in each analysis which leads to inconsistencies across the interim analyses.

stopping rules
one important point in studies with planned interim analyses is the stop criterion. each interim analysis provides the possibility to stop the study prior to its planned end, entailing the mentioned ethical and financial benefits. in studies on one single feature the study is usually stopped if that feature is found to be significant. in studies on a large number of features one could stop the study as soon as a pre-specified fixed number of features has been found to be significant. in the case of microarray analyses, this criterion might be useful when the number of genes that can be further validated by laboratory experiments is limited by costs or time.

here, we follow the approach of marot and mayer  <cit>  who consider as stop criterion the achieved proportion of detected true positives, the so called average power rate    

where d <dig> is the number of non-true null hypothesis. at each interim analysis the achieved apr is estimated and the study is stopped if this estimate exceeds a predefined level, e.g. 80%. in the case of d <dig> =  <dig> the apr is defined to be zero.

we employ a new estimator of the apr, similar to the fdr-estimator of storey and tibshirani  <cit>  which is based on the following relations:   

the three components e, e and e can be estimated as in  <cit>  which is shown in the following. the expectation of r can simply be estimated by the observed number of rejected hypothesis, i.e.   

the estimation of e is based on the fact, that p-values belonging to true null-hypotheses are uniformly distributed within  <cit> . thus, the probability that a p-value which belongs to a true null-hypothesis is smaller than a threshold t  is exactly t. therefore, if the significance level is chosen to be α' = t and d <dig> null hypotheses are true, e can be estimated by   

where π <dig> is the fraction of true null hypothesis. for α' one can for example choose αbh as defined in the previous subsection. the unknown fraction π <dig> of true null hypotheses can be estimated by   

here, ϑ serves as a tuning parameter that balances bias versus variance. for a well chosen ϑ, the p-values in  will belong 'mostly' to true null-hypotheses, and therefore equation  estimates the fraction of true null-hypotheses. again, the argument is the uniform distribution of p-values belonging to true null-hypotheses. see figure  <dig> for graphical illustration.

according to our simulations  setting ϑ =  <dig>  results in a good estimate . other automated ways to choose ϑ have been proposed in  <cit>  and  <cit> . in both cases the estimate of π <dig> is used to estimate not the apr but the fdr. storey  <cit>  calculates the mean squared error of the fdr estimator for a range of values of ϑ and takes the one minimizing this mse. the calculation of this mse thereby is in turn based on a plug-in estimate of the fdr. the method presented in  <cit>  does not need the fdr but is based on  only. the underlying observation is, that the bias in the estimator of π <dig> vanishes in the extreme choice ϑ =  <dig>  thus, the approach there is, to set .

a non-parametric estimator of π <dig> is given by langaas et al.  <cit> . based on this estimator and on the empirical distribution of the p-values marot and mayer  <cit>  construct an apr estimator analog to the beta uniform model presented by  <cit> .

in any case, the expectation of d <dig> can be estimated by   

such that the final estimator for the apr is given by   

we will use  to denote the estimator that results from setting ϑ =  <dig>  and   to denote the estimator which is based on the procedure for estimating π <dig> presented in  <cit> . the estimator by marot and mayer  <cit>  will be denoted by  .

RESULTS
simulation study
data generation and settings
in order to simulate a study of the design proposed in subsection 'study design' we set the following parameters. at first, we chose the total number of samples to be n =  <dig>  furthermore, we set the intended length of the recruitment part of the study  and the length of the follow-up part to be  <dig> months, each, i.e. . we assume that the arrival times a are distributed uniformly during this first part. thus, they were drawn from . the patients' survival times were assumed to follow an exponential distribution exp, where λ is the mean survival time. here, we set λ =  <dig> months.

as we wanted to generate a set of genes which correspond to the simulated survival times, we split the individuals into two groups. the one group comprises the subjects with survival smaller than the specified λ the other group the subjects with equal or longer survival times than λ. the gene expression data for the two groups were then drawn from multivariate normal distributions nd, k =  <dig>   <dig>  expression levels were simulated for d =  <dig> genes. the different mean vectors of the two groups represent the differentially expressed genes between subjects with short or long survival. for both groups, the same covariance matrix Σ with an autoregressive structure was generated. this structure images the fact that some genes are highly correlated among each other while others behave rather independent. in detail, we set   

the expectation vector μ <dig> of the one group was set to be the null-vector, while a fraction of τ = 50% randomly chosen genes was altered in the other group. the alterations in μ <dig> were drawn from 'discretized' normal distributions. larger fold changes were simulated via a higher standard deviation of this normal distribution. the structure of μ <dig> is illustrated in figure  <dig> 

this way we simulate an effect of a fixed size in the gene expression depending on whether the patient belongs to the group of long-term survivors or not. of course in biology the inverse direction is true, i.e. survival is regulated by gene expression. however, we think that for our purpose it does not matter in which order the survival and expression data are generated. in addition, it is typical in biology that a gene is either up- or down-regulated. hence, only the direction of regulation but not the strength of regulation influences the outcome. therefore we chose to model the relationship between single genes and survival by a discrete function and not by some continuous one.

at each interim analysis, a gene-wise cox regression was performed to detect the survival correlated genes, and resulting p-values were adjusted to control the fdr at a level of 5%. following the results of marot and mayer  <cit>  and of posch et al.  <cit> , no additional adjustment for interim analyses was performed. the number of simulation runs was set to  <dig> for each setting. all simulations were performed with the free software r in version  <dig>   <cit> .

we simulated two different setups. one, where  <dig> interim analyses are scheduled for both study parts, i.e. m <dig> = m <dig> =  <dig>  and one where  <dig> analyses are planned to take place per part, i.e. m <dig> = m <dig> =  <dig>  in each simulation run, our power estimator described in section 'stopping rules' was applied and the study was stopped when an estimated apr of 80% was achieved.

as a more extreme setting we additionally simulated in the second setup  the situation of a smaller fraction τ = 5% of altered genes.

adherence to false discovery rate
as no interim-specific adjustments were applied, the question arises, whether the pre-specified fdr-level of 5% was maintained at each analysis. figure  <dig> displays the mean simulated fdr at each interim analysis in two different settings. figure  <dig> represents the case of m =  <dig> analyses , while figure  <dig> represents the case of m =  <dig> analyses. table  <dig> and table  <dig> contain the mean and standard deviation of the fdr for these cases. both results were obtained with the fold changes for the genes between long and short time survivors being generated as shown in figure  <dig>  the pre-specified fdr-level of 5%, indicated by the dashed line, is maintained at nearly each analysis.

the descriptive values for each interim analysis including the number detected genes, the simulated fdr, the real apr, and the apr estimates, all in the setting of τ = 50% altered genes and four interim analyses. these values correspond to figure  <dig>  and figure  <dig> .

the descriptive values for each interim analysis including the number detected genes, the simulated fdr, the real apr, and the apr estimates, all in the setting of τ = 50% altered genes with large fold changes and ten interim analyses. these values correspond to figure  <dig>  and figure  <dig> .

one can observe, that with only a small number of patients available the problem is harder, such that only a small number of genes is detected. in such cases each false positive gets more weight in the calculation of the fdr and one has to expect higher fdr levels. in later interim analyses, the simulated fdr stabilizes at a more conservative level. in the setting of m =  <dig> interim analyses, the fdr is considerably small in the very first interim analysis. this observation can be explained by the fact that the fdr is defined to be zero when no genes are found at all.

in the more extreme setting with only τ = 5% survival related genes, the overall course of the fdr over the interim analyses - as shown in figure  <dig> and table  <dig> - stays the same, but the peak in the early analyses becomes more prominent, and the specified fdr-level is not strictly maintained also during the later interim analyses.

the descriptive values for each interim analysis including the number detected genes, the simulated fdr, the real apr, and the apr estimates, all in the setting of τ = 5% altered genes and ten interim analyses.

average power rate and early stopping
in figure  <dig>  the estimated and true apr at each interim analysis is shown. the corresponding descriptive values can be found in tables  <dig> and  <dig>  again, the cases of m =  <dig> and m =  <dig> analyses are displayed, where half of the analyses took place during the recruitment part of the study and the other half during the follow-up part. in both cases, the estimated and the true apr increases with each interim analysis. in addition, true and estimated power do not diverge dramatically, however our estimation () appears to be slightly liberal. comparable performs the estimator  , where we plug in the π <dig> estimation procedure of storey and tibshirani  <cit> . the power estimation by marot and mayer  <cit>  ) overestimates the real power.

the pre-specified stopping criterion, an estimated apr of 80%, is represented by the dashed line. at average, this criterion is achieved at the 3rd analysis in the case of m =  <dig> planned interim analyses, and at the 7th analysis in the case of m =  <dig> planned interim analyses. in addition, true and estimated apr become not much higher than the desired level of 80%. in particular, the power increases when new samples are included during the recruitment part, but nearly stagnates in the follow-up part, where survival data is up-dated only.

one main interest of our simulation was to find out whether interim analyses can provide an early stopping in such survival studies. figure  <dig> shows for each interim analysis the fraction of simulation runs which could be stopped at this point. both figures show the simulations with m =  <dig> analyses. the fold changes in these two settings were generated either with small effects  or with large effects . in the case of small effects ), only 60% of all simulated studies reached the last planned final analysis while 40% were stopped at an interim analysis. in the case of large effects ), even more than 80% of the simulated studies were stopped before the final analysis.

the average power rate and its estimations in the harder setting with only τ = 5% survival related genes is shown in figure  <dig>  in this setting neither the true apr nor its estimates reach the 80% level, such that no study was stopped at earlier analyses. while the apr  again overestimates the true apr, the other estimators become conservative.

application to breast cancer data
in order to evaluate our method on real data, we analyzed gene expression levels from  <dig> patients suffering from breast cancer  <cit> . the data contain expression levels of  <dig> genes. in this study, patients were recruited between  <dig> and  <dig>  thus, the recruitment part of the study was l <dig> =  <dig> years. exact arrival times were not given in the public available data set, thus, we drew these times randomly from a uniform distribution u. to account for random effects, which might have been introduced by drawing the arrival times, we repeated the simulated study based on this data  <dig> times with newly drawn arrival times and took the average of the resulting error and power estimations.

in the data, minimum and maximum survival was  <dig>  and  <dig> years, respectively. median survival was  <dig> years. we analyzed the data set with m <dig> =  <dig> interim analyses during the recruitment part of the study and m <dig> =  <dig> analyses within the follow-up part. we intended to control the fdr at a level of 5% and to stop the study when the estimated apr exceeds 50%.

the raw p-values from the final analysis are displayed in figure  <dig>  from this figure it can be seen, that the suggested  <cit>  choice of ϑ =  <dig>  is indeed a good choice for this data, as the histogram resembles a uniform distribution very well in the range .

simulation with parameters from breast cancer data
in order to perform the simulation also with different distributional assumptions, we performed an additional simulation, where parameters were taken from the breast cancer data described in the previous section. to this end we simulated the patient data and the gene expression data for  <dig> patients. because the recruitment time of the real study was  <dig> years, the arrival times were again drawn randomly from a uniform distribution u. a weibull, a gamma, and a log-normal distribution were fitted to the survival times of the real data using the fitdistcens function from the fitdistrplus r package  <cit> . we employed the akaike information criterion  to select the fitted log-normal distribution, from which, thus, the survival times were drawn.

we wanted to set the proportion of survival related genes according to the real data set. we, therefore, used the results from the previous section where in the last analysis  <dig> genes were found and the apr was estimated to be 27%. thus, the total number of survival related genes in this data set was estimated to be  <dig> 

to generate the gene expression data, we divided the patients - as before - into two groups along their median survival time. the gene expression data was again multivariate normally distributed in both groups. the mean vector was set to  <dig> in one group. for the other group we used a discretization of the difference between the empirical mean vectors in both groups of the real data set. we chose the discretization grid to consist of steps with a width of  <dig> , which resulted in  <dig> survival related genes.

the distribution of the resulting mean vector of the second group is given in table  <dig> 

in the simulation based on the parameters from the breast cancer data the mean vector of the gene expressions of the long time survivors is set to this discretized version of the difference of the empirical mean of the two groups in the real data.

we estimated the covariance matrix as proposed by schäfer and strimmer  <cit>  using the implementation in the r package corpcor, but that resulted in a maximum power of  <dig> % in the final analysis.

therefore, we employed again a covariance matrix based on equation  as it was used in the other simulations. because the effects in this simulation  were rather small, we reduced the simulated variance by a factor of  <dig> compared to the previous simulations.

the results are shown in figure  <dig> and table  <dig>  as in the simulation setting with τ = 5% survival related genes, the fdr peaks at the 3rd interim analysis, where only  <dig> genes were found on average. the apr estimators are comparable to the apr estimators in the real data, but show an erroneous aberration in the second analysis, even though there are no genes detected in that analysis. this aberration is corrected, though, in the third analysis as soon as there are some genes found.

the descriptive values for each interim analysis including the number detected genes, the simulated fdr, the real apr, and the apr estimates, all in the setting where parameters were taken from the breast cancer data.

discussion
typically, survival studies require long time spans from recruitment of the first patients until the availability of first results. therefore, there is a strong desire to obtain results prior to the planned end of the study, not only for financial aspects but also for ethical ones. classical group sequential designs exhibit a methodology for interim analyses including the potential for an early stopping of a trial. whereas these classical methods concentrate on studies with one single feature, there has little been done for the case of multiple features, particularly the high-dimensional case. however, many survival studies now concentrate on correlating observed survival times with high-throughput data from genomics or proteomics experiments which yield expression levels for thousands of features measured in a small number of samples.

based on the findings of marot and mayer  <cit>  and posch et al.  <cit>  we simulated the possibility of early stopping in interim analyses of survival data in microarray experiments. likewise to these prior findings we observed that a pre-specified false discovery rate is maintained during interim analyses without particular adjustments. i.e., adjustment appears only to be necessary for multiple testing but not additionally for interim analysis. while it was shown in the two mentioned articles that this principle holds asymptotically when the number of tested hypothesis is large, we have seen in further simulations beyond those presented in the section on the simulation that it also works for rather small numbers of hypotheses .

we used the benjamini-hochberg procedure to do the multiple testing adjustment, even though the benjamini-hochberg procedure does not control the fdr under arbitrary dependency structures. however, in our simulations and in the real data example it could be seen that this procedure mostly controlled the fdr. we believe, that in microarray studies a strict control of the fdr is of minor importance, as microarray studies are mainly used for hypothesis generation and, thus, need further validation anyway. in cases where a stricter control of the fdr is required, the more conservative procedure of benjamini and yekutieli  <cit>  might be more appropriate.

an important issue in interim analyses of high-dimensional data is the choice of an adequate stopping criterion. here, we chose the achieved average power rate as stopping criterion which is defined as the proportion of detected false null hypothesis. we derived a new estimator for the average power rate that comes close to the true proportion of true positive findings. however, this estimator behaved slightly liberal when the data contained many survival related genes and conservative when the data contained few survival related genes. we also tried other methods like the more sophisticated ϑ estimator given in  <cit>  and the apr estimation method proposed in  <cit>  which resulted in comparable and worse approximations, respectively. improvements remain therefore necessary. with this criterion we observed that early stopping can be achieved in certain studies, based on the actual proportion of false null hypothesis and the effect sizes .

we applied the methods onto gene expression data from a microarray study on breast cancer. in this analysis we obtained an estimated average power of 20% at the fifth interim analysis  and of 27% at the eighth interim analysis . these estimated proportions seem to be rather small. however, the estimated apr of 27% in the final analysis corresponds to about  <dig> genes detected by cox regression . this set might provide a signature which enables to build a survival predictor of sufficient quality. predictor quality and classification accuracy are other interesting stopping criteria for interim analyses of high-dimensional data. the prognostic value of survival models based on gene expression signatures was for example studied by hielscher et al.  <cit> . evaluation of such alternative stopping criteria remains an open point which we are going to study in our further research.

the necessary sample size another important point in planning microarray studies in combination with survival data. our simulation study provides the basis for such sample size considerations. with certain information from pilot studies, including expected distributions of fold changes and expected survival times, our simulation approach can be used to study the development of the apr in interim analyses. therefore, we made our r-code available as package on the r-cran repository http://cran.r-project.org within the package survgenesinterim.

several extensions of our simulation framework can be considered. in the analysis of microarray experiments normalization is an important pre-processing step to make the single arrays comparable. we therefore intent as methodological improvement to add different normalization approaches to our simulation framework. when making interim analyses, one can for example consider a re-normalization with each set of new array data or use the normalization parameters obtained in a previous interim analysis. another improvement can be considered with regard to the survival analysis. while we have used the proportional hazard model, here, this assumptions may not always be true and other models with time variant effects seem to be more reliable.

CONCLUSIONS
group sequential interim analyses of microarray experiments in survival studies are frequently performed without considering the adherence of the overall error rate. our simulation framework helps to evaluate the behaviour of error rates and power rates in such experiments. the framework also enables to study the developing of results when survival data is up-dated at subsequent times during studies that take several years.

authors' contributions
kj formulated the problem and the study design. al wrote the r-code and performed the simulations and the data evaluation. tb provided important contributions concerning the practical aspects of the study design. all authors were involved in writing the manuscript and have read and approved the final manuscript.

