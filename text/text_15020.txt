BACKGROUND
neuroblastoma  is the most common solid pediatric tumor, deriving from ganglionic lineage precursors of the sympathetic nervous system  <cit> . it shows notable heterogeneity of clinical behavior, ranging from rapid progression, associated with metastatic spread and poor clinical outcome, to spontaneous, or therapy-induced regression into benign ganglioneuroma. age at diagnosis, stage and amplification of the n-myc proto-oncogene  are clinical and molecular risk factors that the international neuroblastoma risk group  utilized to classify patients into high, intermediate and low risk subgroups on which current therapeutic strategy is based. about fifty percent of high-risk patients die despite treatment making the exploration of new and more effective strategies for improving stratification mandatory  <cit> .

the availability of genomic profiles improved our prognostic ability in many types of cancers  <cit> . several groups used gene expression-based approaches to stratify nb patients. prognostic gene signatures were described  <cit>  and classifier proposed to predict the risk class and/or patients' outcome  <cit> . we and other scientific groups have identified tumor hypoxia as a critical component of neuroblastoma progression  <cit> . hypoxia is a condition of low oxygen tension occurring in poorly vascularized areas of the tumor which has profound effects on cell growth, genotype selection, susceptibility to apoptosis, resistance to radio- and chemotherapy, tumor angiogenesis, epithelial to mesenchymal transition and propagation of cancer stem cells  <cit> . hypoxia activates specific genes encoding angiogenic, metabolic and metastatic factors  <cit>  and contributes to the acquisition of the tumor aggressive phenotype  <cit> . we have used gene expression profile to assess the hypoxic status of nb cells and we have derived a robust 62-probe sets nb hypoxia signature   <cit> , which was found to be an independent risk factor for neuroblastoma patients  <cit> .

the use of gene expression data for tumor classification is hindered by the intrinsic variability of the microarray data deriving from technical and biological variability. these limitation can be overcome by analyzing the results through algorithms capable to discretize the gene expression data in broad ranges of values rather than considering the absolute values of probe set expression. we will focus on the discretization approach to deal with gene expression data for patients' stratification in the present work.

classification is central to the stratification of cancer patients into risk groups and several statistical and machine learning techniques have been proposed to deal with this issue  <cit> . we are interested in classification methods capable of constructing models described by a set of explicit rules for their immediate translation in the clinical setting and for their easily interpretability, consistency and robustness verification  <cit> . a rule is a statement in the form "if<premise> then<consequence>" where the premise is a logic product  of conditions on the attributes of the problem and the consequence indicates the predicted output. most used rule generation techniques belong to two broad paradigms: decision trees and methods based on boolean function synthesis.

the decision tree approach implements discriminant policies where differences between output classes are the driver for the construction of the model. these algorithms divide iteratively the dataset into smaller subsets according to a divide and conquer strategy, giving rise to a tree structure from which an explicit set of rules can be easily retrieved. at each iteration a part of the training set is split into two or more subsets to obtain non-overlapping portions belonging to the same output class  <cit> . decision tree methods provide simple rules, and require a reduced amount of computational resources. however, the accuracy of the models is often poor. the divide and conquer approach prevents the applicability of these models to relatively small datasets that would be progressively fractionated in very small, poorly indicative, subsets.

methods based on boolean function synthesis adopt an aggregative policy where some patterns belonging to the same output class are clustered to produce an explicit rule at any iteration. suitable heuristic algorithms  <cit>  are employed to generate rules exhibiting the highest covering and the lowest error; a tradeoff between these two objectives has been obtained by applying the shadow clustering  technique  <cit>  which leads to final models, called logic learning machines , exhibiting good accuracy. the aggregative policy can also consider patterns already included in previously built rules; therefore, sc generally produces overlapping rules that characterize each output class better than the divide-and-conquer strategy. clustering samples of the same kind permits to extract knowledge regarding similarities of the members of a given class rather than information on their differences. this is very useful in most applications and leads to models showing higher generalization ability, as shown by trials performed with sc  <cit> .

llm algorithms prevent the excessive fragmentation problem typical of divide-and-conquer approach but come at the expense of the need to implement an intelligent strategy for managing conflicts occurring when one instance is satisfied by more than one rules classifying opposite outcomes. llm is a novel and efficient implementation of the switching neural network  model  <cit>  trained through an optimized version of the sc algorithm. llm, snn and sc have been successfully used in different applications: from reliability evaluation of complex systems  <cit>  to prediction of social phenomena  <cit> , form bulk electric assessment  <cit>  to analysis of biomedical data  <cit> .

the ability of generating models described by explicit rules has several advantages in extracting important knowledge from available data. identification of prognostic factors in tumor diseases  <cit>  as well as selection of relevant features in microarray experiments  <cit>  are only two of the valuable targets achieved through the application of llm and snn. in this analysis, to improve the accuracy of the model generated by llm, a recent innovative preprocessing method, called attribute driven incremental discretization   <cit>  has been employed. adid is an efficient data discretization algorithm capable of transforming continuous attributes into discrete ones by inserting a collection of separation points  for each variable. the core of adid consists in an incremental algorithm that adds the cutoff iteratively obtaining the highest value of a quality measure based on the capability of separating patterns of different classes. smart updating procedures enable adid to efficiently get a  optimal discretization. usually, adid produces a minimal set of cutoffs for separating all the patterns of different classes. adid and llm algorithms are implemented in rulex  <dig>   <cit> , a software suite developed and commercialized by impara srl that has been utilized for the present work.

blending the generalization and the feature selection strength of llm and the efficiency of adid in mapping continuous variables into a discrete domain with the stratification power of the nb-hypo signature we obtained an accurate predictor of nb patients' outcome and a robust tool for supporting clinical decisions. in the present work, we applied rulex  <dig>  components to the problem of classifying and predicting the outcome of neuroblastoma patients on the bases of hypoxia- specific gene expression data. we demonstrate that our approach generates an excellent discretization of gene expression data resulting in a classifier predicting nb patients' outcome. furthermore, we show the flexibility of this approach, endowed with the ability to steer the outcome towards clinically oriented specific questions.

RESULTS
rulex model
we analyzed gene expression of  <dig> neuroblastoma tumors profiled by the affymetrix platform. the characteristics of the nb patients are shown in table  <dig> and are comparable to what previously described  <cit> . we selected this dataset because the gene expression profile of the primary tumor, performed by microarray, was available for each patient. "good" or "poor" outcome are defined, from here on, as the patient's status "alive" or "dead"  <dig> years after diagnosis respectively.

a the number of patients is  <dig> in the training set and  <dig> in the test set. the data show the total number of patients and the relative percentage in each subdivision.

we previously described the nb-hypo  <dig> probe sets signature that represents the hypoxic response of neuroblastoma cells  <cit>  and used this signature for developing the hypoxia-based classifier to predict the patients' status utilizing adid to convert the continuous probe sets values into discrete attributes and llm algorithm to generate classification rules. both techniques are implemented in rulex  <dig> . the first assessment of the classifier was done on the training set of  <dig> randomly chosen patients, while the remaining  <dig> patients were utilized to validate the predictions . the outcome of the classifier is a collection of rules, in the form if<premise> then<consequence>, where the premise includes conditions based on the probe sets values and the consequence is the patient status. rulex  <dig>  will use these rules collectively for outcome prediction on the validation set.

the generation of the classification rules requires a discretization step because this simplifies the selection of the cut-off values of the probe sets expression . the discretization yielded one cutoff value for each probe set which was sufficient for modeling the outcome. the use of a single cutoff dichotomizes the probe set attributes in low or high expression, drastically reducing the influence of the technical and biological variability present in the models associated with the absolute values of probe sets expression.

furthermore, a test on the maximum error allowed for a rule was defined. the final classification rules were trained with the optimal value of 25% associated with the maximal mean accuracy of 87%, determined by  <dig> fold cross validation analysis . the procedure generated  <dig> rules, numbered from  <dig> to  <dig>  in table  <dig> and based on conditions containing high or low probe sets expression value . rule premises are limited to two conditions with the exception of rule  <dig> that has only one condition. six rules predict good outcome and  <dig> poor outcome; they will be considered together in scoring the class attribution of new patients of the validation set. this is the optimal scenario proposed by rulex  <dig>  to utilize the  <dig> probe sets nb-hypo signature for classifying patients' outcome.

rule
a cond  <dig> and cond  <dig> indicate the conditions into the premises of the rules.

b the covering accounts for the fraction of patients that verify the rule and belong to the target outcome.

c the error accounts for the fraction of patients that satisfy the rule and do not belong to the target outcome.

d fisher p-value quantifies the statistical significance of the rule on the basis of the number of patients correctly and incorrectly classified by a rule and the number of patients of the dataset belonging to each specific outcome.

three parameters, shown in table  <dig>  estimate the quality of the rules: 1) covering, measuring the generality, 2) error, measuring the ambiguity and 3) fisher's p-value measuring the significance of each rule. the statistical significance of each rule by fisher's exact test was very high  providing strong evidence of the excellent quality of the rules. the covering ranged among rules from 48%  to 80%  for good outcome classes and ranged from 57%  to 92%  for poor outcome. error ranged from  <dig> %  to 14%  for good outcome class and ranged from  <dig> %  to 17%  for poor outcome class. these rules have interesting features that will be addressed in detail. the first consideration is that the overall covering of the rules classifying good and poor outcome adds up to 380% and 209%, respectively, indicating overlap among rules. this is a characteristic of the llm method implementing an aggregative, rather than fragmentation, policy as illustrated in the materials and methods section. however, overlap among the rules can lead to a conflict if the probe sets values of a patient satisfies two or more rules predicting opposite outcomes. to investigate whether overlapping among our rules can be source of conflicts we plotted in figure  <dig> each patient's membership to the nine rules. the plot clusters the rules classifying good and poor outcome and the patients belonging to good or poor outcome classes. the results show that each patient is covered by at least one rule. overlaps exist and occur mainly among rules predicting the same outcome , which do not lead to classification conflicts. however,  <dig> patients  were covered by rules pertaining to opposite outcomes and are represented by those present in quadrant 2a  or 2d . rulex  <dig>  overcomes this problem by employing for assigning a class to a new sample a fast procedure that evaluates all the rules satisfied by it and their covering, thus generating a consensus outcome to be assigned to the sample as detailed in the materials and methods section.

a second characteristic of the classification rules is that they include only  <dig> out of  <dig> probe sets of the original nb-hypo signature. the relationship among probe sets and rules is shown in table  <dig>  rulex  <dig>  operated a second feature selection on the original  <dig> probe sets optimized for functioning in a binary profile of low and high probe set expression and gave rise to a modified hypoxia signature that we name nb-hypo-ii.

a affymetrix probe sets belonging to nb-hypo-ii signature.

b rule id indicates the id of the rules in which the probe set occurs .

c relevance measures the importance of the features included into the rules.

the relevance calculated for each outcome is shown.

it was of interest to assess the relative importance of each probe set in the classification scheme leading to identification of poor and good outcome patients. negative or positive values indicate low or high expression associated with the predicted outcome and the relative relevance is measured by the absolute value. ranking the probe sets may be of relevance to pick the genes for further validation on an alternative platform. it is interesting to note that probe set 217356_s_at is the most relevant for the classification of good and poor outcome patients.

a third interesting feature of the rules is the dichotomization of the expression values of each probe set even if rulex  <dig>  had no constraints on the discretization that could lead to multiple cutoffs or overlapping expression values in different rules. we were intrigued by these results and decided to examine the relationship between the cutoff expression values identified by rulex and those obtained by kaplan-meyer analysis. the kaplan-meier algorithm calculates all possible cut- off points of a given probe set in a cohort of patients and selects the one maximizing the distinction between good and poor outcome. the results, shown in table  <dig>  demonstrated a general quite good concordance between the cutoff values of the kaplan-meier and those generated by rulex. in particular, the two measures always differed by less than ±50% in magnitude, but only in some cases  discrepancies are probably related to the capability of adid to exploit the complex multivariate correlation among probe sets. furthermore, we found a concordance also in the relationship between high/low probe set and outcome in rulex derived rules and kaplan-meier plots .

a probe sets id indicates the numerical identified of the probeset.

b nb-hyp-ii indicates the list of probe sets of the nb-hypo signature belonging to the rules.

c overall indicates the survival time between the time of an event or last follow up and the time of diagnosis.

d relapse free indicates the survival time between the first relapse and the time of diagnosis.

e expression cut-off indicates the optimal cut-off point of each probe set resulting from the kaplan-meier scan and from the rules.

f worse indicates whether high or low expression of a given probe set is associated to the worse survival. worse survival are calculated from the kaplan-meier curve or from the conditions included into the rules.

outcome prediction
the ability of the rules in table  <dig> to predict patients' outcome was tested on a  <dig> patients' independent dataset. results are expressed as accuracy, recall and precision, assessing the performance in classifying good outcome, specificity and negative predictive values  assessing the performance in classifying poor outcome patients. the direct evaluation of the performance of the  <dig> rules on the validation set is represented by adid in table  <dig>  by adid+llm in table  <dig>  the base configuration of table  <dig> and by no dataset modification in additional file  <dig>  the results demonstrate a good accuracy comparable to what reported by other algorithms  <cit> . furthermore, the classification of good outcome was superior to that of poor outcome patients as shown for example by a recall of 90% relative to 57% of specificity.

a accuracy is the fraction of correctly classified patients and overall classified patients.

b recall is the fraction of correctly classified good outcome patients and the overall predicted good outcome patients.

c precision is the fraction of correctly classified good outcome patients and the predicted good outcome patients.

d specificity is the fraction of correctly classified poor outcome patients and the overall poor outcome patients.

e npv is the fraction of correctly classified poor outcome patients and the overall predicted poor outcome patients.

f discretization algorithms utilized for comparison/

a accuracy is the fraction of correctly classified patients and overall classified patients.

b recall is the fraction of correctly classified good outcome patients and the overall predicted good outcome patients.

c precision is the fraction of correctly classified good outcome patients and the predicted good outcome patients.

d specificity is the fraction of correctly classified poor outcome patients and the overall poor outcome patients.

e npv is the fraction of correctly classified poor outcome patients and the overall predicted poor outcome patients.

f machine learning algorithms utilized for comparison.

a accuracy is the fraction of correctly classified patients and overall classified patients.

b recall is the fraction of correctly classified good outcome patients and the overall predicted good outcome patients.

c precision is the fraction of correctly classified good outcome patients and the predicted good outcome patients.

d specificity is the fraction of correctly classified poor outcome patients and the overall poor outcome patients.

e negative predictive value is the fraction of correctly classified poor outcome patients and the overall predicted poor outcome patients.

f configuration indicates the specific weights assigned to the outcomes in the weighted classification.

pvca analysis  <cit>  was utilized to estimate the potential variability of experimental effects including batch. the analysis revealed that batch effect explained a moderate 21% of the overall variation in our dataset and a frozen surrogate variable analysis  was employed for removing batch effect. the application of fsva reduced batch effect to less than  <dig> % of the total variation .

we compared the performances achieved by adid and llm on the batch-adjusted dataset and those on the original dataset  to measure the impact of batch effect on classifier performances. performance obtained with the adjusted dataset turns out to be very similar to that obtained with original data as shown in additional file  <dig> demonstrating that batch effect had negligible impact on the performances. therefore, the dataset with no modifications was utilized for subsequent analysis.

we then compared the performance of the adid discretization approach and those of commonly used discretization algorithms, namely: entropy based , modified chi square  <cit> , roc based , and equal frequency . results detailed in table  <dig> showed that the discretization performed by adid produced better accuracy with respect to the others .

we also compared the performance of llm and those of decision tree  <cit> , support vector machines   <cit> , and prediction analysis of microarrays   <cit> , to evaluate the ability of llm to predict patients' outcome with respect to other standard supervised learning methods. results in table  <dig> revealed that adid and llm were able to predict patients' outcome with better performances with respect to the decision tree classifier. the performances of llm, pam and svm classifiers were comparable.

overall, the performance was good but unbalanced datasets tend to bias the performance towards the most represented class  <cit> . in our dataset the good outcome patients were more frequent . therefore, we explored the possibility of utilizing a weighted classification system   <cit>  to improve the classification of poorly represented classes or to force the algorithm to maximize the performance on selected outcomes. the performance of the base configuration was taken as reference.

we performed a weighted classification accounting for the unbalanced class representation in the dataset . the weight was calculated as the inverse proportion of the number of patients belonging to each class, about  <dig> times more weight on the poor outcome class. the major improvement over the base configuration was the specificity whereas all other parameters were similar or worst. configuration w1_ <dig>  was similar to w26_ <dig>  but set the weight of poor outcome  <dig> times higher than that of the good one. interestingly, its performance was very close to that of configuration w26_ <dig> despite the disparity in the weight applied, suggesting that small changes in the relative weight of poor outcome are sufficient to optimize the results. in conclusion, increased weight on poor outcome augmented the percentage of correctly classified poor outcome patients even though a smaller number of patients were included in this class. this correction may be relevant when maximization of the specificity isof primary importance as in the case of using a prudent therapy.

in contrast, configuration w1000_ <dig> sets the weight of good outcome  <dig> times higher than that of poor outcome. the performance parameters were similar or higher than the base configuration with the exception of specificity that was quite low, a situation that appears symmetrical to those observed previously. the recall is almost absolute indicating the exceptional ability to classify good outcome. the drawback of this configuration is a very low percentage of correct poor outcome classification that is 35% of all poor outcome patients. this configuration may be useful in the case of using an aggressive therapy.

in conclusion, wcs can improve performance parameters of classification of poor or good outcome patients and may be particularly relevant in a situation where the dataset contains a major unbalance between classes and/or when clinical decisions may require minimizing false positives or false negatives.

discussion
our study is based on gene expression data derived through the analysis of primary neuroblastoma tumors by microarray on the affymetrix platform. we focused on the expression of  <dig> probe sets comprising the nb-hypo signature that we have previously shown to represent the hypoxia status of neuroblastoma cells  <cit> . the association of hypoxia with poor prognosis in neuroblastoma patients was previously demonstrated  <cit> . we studied a cohort of  <dig> nb patients characterized by clinical and molecular data addressing the question of the potential prognostic value of this signature. rulex  <dig>  suite was used to train a model on a set of patients and validate it on an independent one. we demonstrate that attribute driven incremental discretization and logic learning machine algorithms, implemented in rulex  <dig> , generated a robust set of rules predicting outcome of neuroblastoma patients using expression values of  <dig> probe sets, specific for hypoxia extracted from the gene nb-hypo expression profile.

outcome prediction of nb patients was reported by several groups using a combination of different risk factors and utilizing various algorithms  <cit> . several groups have used gene expression-based approaches to stratify neuroblastoma patients and prognostic gene signatures have been described often based on the absolute values of the probe sets after appropriate normalization  <cit> . affymetrix genechip microarrays are the most widely used high-throughput technology to measure gene expression, and a wide variety of preprocessing methods have been developed to transform probe intensities reported by a microarray scanner into gene expression estimates  <cit> . however, variations from one experiment to another  <cit>  may increase data variability and complicate the interpretation of expression analysis based on absolute gene expression values. we addressed the problem by applying the attribute driven incremental discretization algorithm  <cit>  that maps continuous gene expression values into discrete attributes. interestingly, the algorithm applied to our dataset showed that the introduction of a single cutoff was sufficient to create two expression patterns, operationally defined as low and high, capable of describing the probe set status accurately enough for effective patients classification. this approach minimizes the variability and errors associated with the use of absolute values to interpret microarray gene expression data.

the validity of the discretization implemented by rulex  <dig>  was further documented by an empirical validation where we calculated the optimal cutoff value for each of the  <dig> probe sets tested in a kaplan-meier analysis of the patients' survival. it is noteworthy that such analysis utilized the survival time of the patient as opposed to  <dig> years survival considered by rulex  <dig> . nevertheless, we demonstrated that the cutoff values calculated by either approaches were rather similar, thus supporting the robustness of the adid algorithm to identify relevant discrete groups of expression values. from a technical point of view adid is a multivariate method searching for the minimum number of cutoffs that separate patients belonging to different classes. on the other hand, the kaplan-meier scan is a univariate technique having the aim of identifying the value of the probe set that maximizes the distance among the survival times of resulting groups. it should be noted that these two approaches are independent from each other since they are based on different algorithms and different classifications.

only  <dig> out of  <dig> probe sets of the original signature were considered by llm for building the classifier. this selection has a biological meaning. in fact, the original  <dig> probe sets nb-hypo signature was obtained following a biology driven approach  <cit>  in which the prior knowledge on tumor hypoxia was the bases for the analysis and the signature was derived from hypoxic neuroblastoma cell lines  <cit> . hence, nb-hypo is optimized for detecting tumor hypoxia. the importance of hypoxia in conditioning tumor aggressiveness is documented by an extensive literature  <cit> . however, nb-hypo was not optimized to predict outcome that is dependent on factors other than hypoxia. rulex  <dig>  performed a feature selection by identifying the  <dig> probe sets that were the most relevant in predicting outcome among those of the nb-hypo signature.

one key feature of llm is to implement an aggregative policy leading to the situation in which one patient can be covered by more than one rule. this leads to the advantage of avoiding dataset fragmentation typical of the divide-and-conquer paradigm. furthermore, the robustness of the resulting model is increased; in fact, if a patient satisfies more than one rule for the same output class, the probability of a correct classification is higher.

the same outcome is generally predicted by every rule verified by a given patient. however, there are situations in which a patient satisfies rules associated with opposite outcomes, thus generating a potential conflict. rulex  <dig>  overcomes this problem by adopting a procedure for assigning a specific class on the basis of the characteristics of the verified rules. a conflict should not necessarily be considered as a limit of the proposed approach, but it could reflect a source of ambiguity present in the dataset. if this were the case, any method building models from data would always reflect this ambiguity.

the generation of a predictive classifier based on gene expression obtained from different institutions raised the question of a possible batch effect in the data. we utilized frozen surrogate variable analysis method, a batch effect removal method capable of estimating the training batch, and used it as a reference for adjusting batch effect of other batches. in particular, those for which no information about the outcome is known. this was not possible with other known batch removal methods such as the combating batch effects   <cit> , which adjusts the expression values of both training and test batch  <cit> . we compared the performances achieved by adid and llm on the batch-adjusted dataset and those on the original dataset  to measure the impact of batch effect on classifier performances. performance obtained with the adjusted dataset showed that batch effect had negligible impact on performances. previous studies observed that the application of batch effect removal methods for prediction does not necessarily result in a positive or negative impact  <cit> . furthermore, batch effect removal methods may remove the true biologically based signal  <cit> . for this reasons, the analysis of the present manuscript was performed on the original dataset excluding any modification for batch removal.

the  <dig> rules generated by adid and llm achieved a good accuracy on an independent validation set. we compared the accuracy of our new classifier with that of the top performing classifiers for nb patients' outcome prediction listed in  <cit>  to study the concordance of the performance achieved by the  <dig> rules with that of other previously published classifiers. the classifiers were generated on different signatures and algorithms and the accuracy reported ranged from 80% to 87%. note that those classifiers were validated on a different test set; they utilized different algorithms and signatures. we concluded that the accuracy of our rules and that of other techniques reported in literature were concordant.

adid represents an innovative discretization method that was used in combination with llm for classification purposes. in the present study, adid demonstrated to outperform other discretization algorithms, based on univariate analysis, indicating the capability of adid to exploit the complex correlation structure commonly encountered in biomedical studies, including gene expression data sets. moreover, adid-llm showed better performances with respect to the decision tree classifier. this was somewhat predictable because the lower performance of the decision tree with respect to other approaches has been pointed out in literature  <cit> . the llm rules, svm and pam classifier achieved similar performances. the good performance achieved by adid and llm and the explicit representation of the knowledge extracted from the data provided by the rules demonstrated the utility of adid and llm in patients' outcome prediction.

our dataset suffers of class-imbalance as many other datasets  <cit> . in fact, the good outcome class is over-expressed with respect to the poor outcome class. rulex  <dig>  implements a novel algorithmic strategy that allows setting up different weights to outcomes biasing the assignment to a class towards that of interest. it should be noted that weighting is effective only on class assignment for patients verified by conflicting rules. we have utilized the weight approach to represent the situation in which either poor or good outcome was favored or to address the imbalance between good and poor outcome patients in our dataset. we found that, in the absence of predefined weights, the algorithm generates a good performance somewhat unbalanced towards better classification of good outcome patients. by changing the weights we were in the position of steering the prediction towards a high precision in classifying poor outcome patients or in privileging the specificity. this tool may be or practical importance in the decision making process of clinicians that are confronted with difficult therapeutic choices.

CONCLUSIONS
we provided the first demonstration of the applicability of data discretization and rule generation methods implemented in rulex  <dig>  to the analysis of microarray data and generation of a prognostic classifier. rulex automatically derived a new signature, nb-hypo ii, which is instrumental in predicting the outcome of nb patients. the performances achieved by rulex are comparable and in some case better than those of other known data discretization and classification methods. furthermore, the easy interpretability of the rules and the possibility to employ weighted classification make rulex  <dig>  a flexible and useful tool to support clinical decisions and therapy assignment.

