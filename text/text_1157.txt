BACKGROUND
clustering is widely used for exploratory data analysis, with applications ranging from physics and biology to social sciences and psychology. in data intensive fields of biology, it is important to identify groups or clusters of data showing similar behavior. many methods for clustering have been developed, which fall into two general categories: heuristic algorithms and model based analyses. in heuristic algorithms clustering is obtained either by optimizing a certain target function or iteratively agglomerating  nodes to form bottom-up trees. examples of these approaches include: k-means <cit>  and k-median  <cit>  clustering, fuzzy k-means clustering  <cit> , affinity propagation  <cit> , spectral clustering  <cit> , qt  clustering  <cit>  and density contour clustering  <cit> . in contrast to heuristic methods, model-based clustering methods make inferences based on probabilistic assumptions about the data distribution. gaussian or modified gaussian mixture models  <cit>  use the expectation-maximization algorithm  <cit>  to find the parameters of the distributions that are fitted to the data. then bayesian information criterion   <cit> , akaike information criterion   <cit> , integrated completed likelihood   <cit>  or other criterion is used to select the number of clusters.

flow cytometry  is a commonly used technique to measure the levels of expression of multiple markers, such as specific proteins, in millions of cells. fcm data is typically analyzed by an attempt at visual selection of similar groups of data in  <dig> dimensional projections, a process referred to as gating. the visual identification of similar groups of data points, referred to in fcm as manual gating, is error-prone, non-reproducible, non-standardized, difficult to apply to more than two dimensions, and manpower-intensive, making it a limiting aspect of the technology  <cit> . despite its widespread use, fcm lacks a fast and reliable method for automated analysis to parallel its high-throughput data-generation. the development of a reliable, heuristic clustering approach suitable for large datasets would significantly improve the value of fcm experiments and would have widespread application to other data-intensive biological clustering problems.

automated fcm gating attempts using heuristic methods, such as k-means and fuzzy k-means  <cit>  do not provide stable results. different initial values for the algorithm, i.e. initial locations of the cluster centers, typically result in different clustering results. often, with a poor set of initial values, the minimization of the target function falls into a local minimum and gives an undesirable clustering result. furthermore, these methods work best with spherical or hyperspherical shaped clusters, a distribution often not observed in fcm datasets. several other useful clustering algorithms based on pairwise comparisons, including linkage or pearson coefficients method  <cit>  and the affinity propagation method  <cit> , are computationally too expensive to be used for fcm because the size of the pairwise distance matrix increases on the order of n <dig> with the number of points. classification and regression trees  <cit> , artificial neural networks  <cit>  and support vector machines  <cit>  have also been used in the context of fcm analyses  <cit> , but these supervised approaches require training data, which may not be available and may perform unreliably if the features of the experimental data diverge from the training set. model-based approaches are slow, need user involvement and require assumptions about cluster distributions that limit their general utility  <cit> . a major problem of all practical approaches for unsupervised fcm cluster analysis remains the determination of the number of clusters. the use of bic, aic, icl or other criterion can make the determination of cluster number unreliable .

to overcome these limitations of the above approaches, we have developed a new density contour clustering method that is particularly suitable for fcm data. in the early 1960's boyell and ruston <cit> , working on methods for storing topological data in a manner allowing efficient reconstruction, recognized that contour lines can be represented as a tree structure. this insight led to the idea of density contour clustering by finding the largest cross section of each histogram peak  <cit> . jang and hendry <cit>  used a density contour method for clustering galaxies, that in principle is most similar to our method. their method is a modification of a method proposed by cuveas et al. <cit> . we have developed a new, fast density contour clustering method suitable for large, multi-dimensional datasets that will be compared with jang and hendry's method in additional file  <dig>  the method is unbiased for cluster shape and does not require global optimization of a multi-variable target function like other commonly used clustering methods do. the algorithm run time increases on the order of n. according to the tests on manually gated and simulated data the method provides correct clustering with correct number of clusters.

the misty mountain algorithm can be understood as the computational analogy of an airplane view of histogram terrain that is initially completely immersed in misty clouds. the mist is steadily removed from the top down by the sun, progressively uncovering clusters as peaks that pierce the mist. eventually the merging points of two peaks, the highest saddle, is revealed. from there two peaks form one instead of two holes in the mist. as the level of the mist decreases, more and more summits and saddles are revealed and evaluated to determine the number of statistically distinct peaks and their extent.

RESULTS
misty mountain algorithm
the approach is briefly described here and more extensively in methods. the multi-dimensional data is first processed to generate a histogram containing an optimal number of bins by using knuth's data-based optimization criterion  <cit> . then cross sections of the histogram are created. the algorithm finds the largest cross section of each statistically significant histogram peak. the data points belonging to these largest cross sections define the clusters of the data set.

to illustrate the method, we generated a simulated two-dimensional  <dig> fcm dataset with the respective histogram having four peaks . seven representative locations of the histogram intersection with a lowering plane are shown . each cross section shows group or aggregate of those bins where the bin content is higher than the actual level of the cross section. with decreasing level, the number and size of the bin aggregates increase . then at the level of the highest saddle two bin aggregates coalesce . at one level higher we have the largest, still separated bin aggregates . the data points belonging to these two largest bin aggregates define the first two clusters of the cluster analysis. the blue colored aggregate in figure 1i is shown just before coalescing with the gray colored aggregate. the data points belonging to this aggregate define the third cluster of the analysis. the cross section of the red peak is still separated and largest at frequency =  <dig> . the data belonging to the respective bin aggregate define the fourth cluster.

to realize the steps described above computationally, the algorithm uses a percolation theory based procedure <cit>  by labeling different bin aggregates of a histogram cross section by different integers. then the algorithm comparatively analyzes pairs of consecutive cross sections to recognize coalescing bin aggregates. assigning clusters to the coalescing bin aggregates requires the lp1-ls and lp2-ls relative heights of the two peaks that fuse both be statistically significantly greater than random fluctuations . lp <dig>  lp <dig> and ls are the heights of the fusing first and second peak and the saddle between them, respectively.

in the sample data, the algorithm assigned points to four clusters, requiring  <dig>  seconds cpu time on a standard laptop pc. the characteristic properties of the assigned clusters such as cluster size and reliability of the assignation are shown in table  <dig>  these clusters contain 85% of all the  <dig> data points. misty mountain is a tight clustering method in that it does not force all points into clusters  <cit> .

lp: height of the peak.

ls: height of the highest saddle next to the peak.

c: number of data points in the cluster.

f: measure of separateness of the peak from nearby peak. the parameter estimates the reliability that an element of the cluster belongs to the respective population.

testing misty mountain algorithm
we analyzed a flow cytometry dataset containing  <dig> points representing the side scattering and forward scattering measurements obtained from u <dig> macrophage cells . an expert in flow cytometry would interpret the large oval group as representing intact cells and would form a gate to separate these cells for further analysis from cellular debris. we first used k-median  <cit>  and spectral clustering  <cit>  algorithms. for k-median clustering we used simulated annealing  <cit>  to find the global minimum of the target function, i.e. to find a stable solution of the clustering problem. both of these conventional clustering methods gave similar erroneous results . we next used the misty mountain algorithm to cluster these data. the respective optimal histogram contained  <dig> ×  <dig> bins and there were  <dig> points in the most populated bin. thus during the analysis, cross sections of the histogram were created at  <dig> levels. the elapsed cpu time of the cluster analysis was  <dig>  sec. the result of the cluster analysis is shown in figure 2c. these clusters contain  <dig> % of all the data points, which are assigned at high confidence .



we next compared misty mountain with other state of the art flow cytometry automated gating algorthims using a variety of datasets . the accuracy of these various algorithms was determined using expert manual gating to generate gold standards with  <dig> dimensional and  <dig> dimensional experimental datasets as well simulated  <dig> dimensional and  <dig> dimensional datasets having known cluster numbers. algorithm run time was compared using these datasets as well as additional high dimensional experimental datasets for which a gold standard for accuracy was not generated. the accuracy of misty mountain was superior to that of all other methods tested. the speed of misty mountain was comparable to that of flowjo and orders of magnitude faster than other state of the art published methods. extensive benchmarking suggests that misty mountain provides a significant improvement over the performance of other available methods.

a optimal cluster number: 12

b optimal cluster number: 24

a*optimal cluster number: 15

b*optimal cluster number: 22

c optimal cluster number: 5

c* optimal cluster number: 2

d optimal cluster number: 1

d* optimal cluster number: 4

* optimal cluster number: 8

&to save cpu time a data set, reduced by 80%, has been analyzed by flame, flowclust and flowjo

sens  = /

spec = /

gold standards were independent expert manual clustering for experimental data and specified clusters for simulated data.

the performance of the misty mountain algorithm with a complex flow cytometry dataset consisting varying levels of two fluorophores, apc-cy7-a and pacific blue-a, in  <dig>  u <dig> cells is shown in figure  <dig>  the dataset in figure 3a was generated for a barcoding experiment  <cit>  in which different groups of cells were labeled with different concentrations of each fluorophore. the respective optimal histogram that was analyzed contained  <dig> ×  <dig> bins. the most populated bin contained  <dig> data points. thus during the analysis,  <dig> cross sections of the histogram were created. the elapsed cpu time of the cluster analysis was  <dig>  sec. the results of the cluster analysis are shown in figure 3b. the analysis identified  <dig> large clusters where the reliability of the cluster elements was from  <dig> - <dig> , and  <dig> small clusters with about  <dig>  reliability. these clusters contained 87% of all the data points. the characteristic properties of the assigned clusters are listed in table in additional file  <dig>  the last cluster in the table is a very small one and it is considered as noise . in additional files  <dig> and  <dig> the analysis of an even more complex 3d barcoding experiment is shown.

as another example we analyzed one of the graft-versus-host disease  data sets.

these 4d data sets have been made available  <cit>  and used in a few flow cytometry analysis publications already  <cit> . the individual data files are available at: http://www.ficcs.org/data/data-files/. in our current example we used a data set from gvhd <dig> iso, folder e# <dig> h <dig>  two dimensional projections of the data and the result of the clustering are shown in figure  <dig> and  <dig>  respectively. this data set is an example for overlapping populations. misty mountain algorithm assigned  <dig> clusters to the 4d gvhd data set within  <dig>  sec. the analyzed histogram of the simulated data contained  <dig> bins. since the populations are severely overlapping the assigned clusters contain only 29% of all the data points. table  <dig> lists the characteristics of the clusters assigned by misty mountain. the low f values in table  <dig> show that the histogram peaks belonging to cluster  <dig>   <dig> and  <dig> are seriously overlapping with nearby peak. in each of these cases misty mountain assigns cluster to a histogram cross section that is close to the top of the respective peak and thus the number of histogram bins assigned to these seriously overlapping clusters is low. the above two data sets are also analyzed by state of the art clustering methods in additional file  <dig> and compared with the results of misty mountain clustering.



bin #: number of histogram bins containing the points of a cluster

we also compared the performance of the various gating algorithms using a dataset from 4d bone-marrow derived mouse stromal cells  stained with antibodies for cd <dig>  gr <dig>  mac <dig> and cd <dig>  two experts manually gated this experiment obtaining identical results. misty mountain gave results identical to that of the experts, unlike the other automated gating methods . in order to test algorithm performance we used a variety of other experimental and simulated data sets with biologically interesting populations such as low density, overlapping and non-convex populations. comparisons were made using simulated  <dig> dimensional and  <dig> dimensional data and additional experiments with  <dig> dimensional and  <dig> dimensional data . these results all strongly support the improved accuracy and utility of the misty mountain algorithm relative to other state of the art methods.

studies were done to evaluate the time complexity of the misty mountain algorithm. these simulations revealed that at fixed bin number the overall run time for the composite steps of the algorithm increases linearly by the number of data points. also an increase in the run time was detected with increasing dimensionality of the data space . the number of clusters did not alter the computation time .

the misty mountain algorithm can be applied to analyze other than fcm data when the data set is large enough to construct an adequate histogram. for example in astrophysics it can be used for unsupervised recognition of star/galaxy clusters, or in social sciences to analyze questionnaires and identify groups with common interests/opinions.

implementation
the implementation, instruction and the input data files of all the examples analyzed in this study are available in additional files  <dig>   <dig> and  <dig> 

pca- misty mountain algorithm for high dimensional data
the current version of the misty mountain algorithm software uses direct analysis for data having up to  <dig> dimensions. some flow cytometry datasets may have up to twelve or even more dimensions. one can set the critical dimension higher than  <dig>  however the run time, the number of data points needed for an adequate histogram and the memory requirement for storing the histogram increases super linearly with increasing dimension. as another option, we have combined the misty mountain algorithm with principal component analysis   <cit> . in order to analyze higher than  <dig> dimensional data, we use pca to project the high dimensional data into a  <dig> dimensional subspace. the subspace is spanned by  <dig> eigenvectors belonging to the  <dig> largest eigenvalues of the covariance matrix of the data. then misty mountain analysis is performed on the projected data. finally the points of the assigned clusters are back-projected into their original position in the data space. this procedure is demonstrated on a simulated  <dig> dimensional data set containing points that distributed as the sum of  <dig> distorted-gaussians. the parameters of the distorted-gaussians  are listed in the table in additional file  <dig>  by using pca, the simulated data are projected into the 5d subspace where misty mountain clustering is performed. the points of the assigned  <dig> clusters are back-projected to their original position in the 10d data space. table in additional file  <dig> lists the center coordinates of the assigned clusters. as a demonstration of correct clustering these cluster centers are very close to the means of the respective distorted-gaussians. it is important to note that the projection of the data into the 5d subspace may bring some of the otherwise separated histogram peaks so close to each other that the number of clusters assigned by the misty mountain algorithm becomes less than the true value. this happens with higher frequency when the data histogram contains many, broad peaks. finally it is important to note that the optimal choice for the critical dimension depends on the actual number of the data points, i.e. one should be able to create an adequate histogram from the data at the critical dimension.

advantages and limitations of misty mountain algorithm
advantages:

1) misty mountain algorithm is unbiased for cluster shape.

2) it is robust to noise,

3) it is fast,

4) it is unsupervised. it does not need estimation for cluster number.

5) the computation time linearly increases with the number of data points

limitations:

1) misty mountain algorithm identifies two closely situated populations as one when the respective histogram has only one peak

2) it identifies two populations as one when lp-ls is comparable with the standard deviation of lp-ls. 

3) the computation time, the number of data points needed for an adequate histogram and the memory requirement for the histogram super linearly increase with the dimension of the data space

misty mountain provides a useful, general solution for multidimensional clustering problems. it can be easily adapted to address diverse large dataset clustering problems in computational biology. it is particularly suitable for automated gating of fcm and should improve the ability to interpret experimental data in this field.

CONCLUSIONS
in biology, measurements on a single object  are frequently represented by a point in a multi-dimensional space where the coordinates of the point refer to the measured values. with the advent of high-throughput assays, these experiments can generate datasets comprising millions of points. clusters of points may be thought of as regions of high density separated from other such regions of low density. we describe a fast algorithm that automatically identifies clusters of data points showing similar values. the three major steps of the algorithm are: i) the multi-dimensional data is first processed to generate a histogram containing an optimal number of bins. ii) the cross sections of the histogram are created. iii) the algorithm finds the largest cross section of each statistically significant histogram peak. the data points belonging to these largest cross sections define the clusters of our data set.

while the idea of clustering by using a density histogram is old, the present implementation results in particularly fast clustering that is useful for data-intensive computational biology applications. misty mountain clusters  <dig> data points in 2d data space in about  <dig> seconds on a standard laptop pc. the run time linearly increases with the number of data points. unlike other commonly used clustering methods, misty mountain is not model-based, unsupervised and does not require global optimization of a multi-variable target function. without making strong assumptions, this method provides fast and accurate clustering. the algorithm is general, but was motivated by the need for an unbiased automated method for analysis of flow cytometry  data.

