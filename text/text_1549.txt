BACKGROUND
in pattern recognition, data analysis is used for predicting the behaviour of an unseen test dataset. amongst problems of this kind, two different types can be clearly distinguished. the first is supervised learning or classification, where a labelled training dataset with known categories is involved. the second kind is unsupervised learning or clustering, where no prior information is available for grouping the dataset. the objective of a clustering algorithm is to partition the given data into mutually exclusive and meaningful  <cit>  clusters; this can provide a better understanding of the natural structure of the data. semi-supervised  <cit>  classification, which combines strategies from both supervised and unsupervised methods, has also grabbed attention in various fields of research as it requires less human effort and gives better accuracy  <cit>  than unsupervised learning. in this paper, we focus our attention on the challenges faced by clustering algorithms  <cit> .

there are numerous clustering algorithms discussed in the literature, traditionally clearly distinguished as either hierarchical  <cit>  or partitional  <cit> . hierarchical methods group the objects together layer by layer, based on the closeness of the data points as measured by suitable similarity or distance metrics. hierarchical clustering sequentially partitions the dataset, either by successively dividing an initial single cluster in divisive methods, or by joining together initially unlinked objects in agglomerative algorithms. in a hierarchical method, once two objects are clustered together they remain together at all subsequent levels of the scheme with fewer clusters. contrastingly, partitional clustering algorithms, such as k-means, do not have a layer by layer structure and objects may sometimes move from one cluster to another.

the k-means method iteratively assigns each point to the cluster whose centroid is closest to it, recalculates the cluster centroids, and reassigns the points. this process continues until the assignments no longer change at each iteration. k-means tends to generate approximately equally sized clusters, minimising intra-cluster distances; however, its preference for globular clusters and its failure to reproduce clusters of complex shape are limitations  <cit> .

determining the number of clusters
automatically determining the number of clusters is a major problem in clustering. a set of n objects can be clustered into any number k of clusters  <dig> ≤ k ≤ n by any of the methods we have discussed. identifying the optimal number of clusters involves computing a range of different numbers of clusters k, with the objective of finding the best value of k that gives the optimum clustering. however, there is no agreed or unique definition of optimum in this context. using internal and external validation measures as described in handl et al.  <cit> , one could design a protocol for reaching a decision on the best k. a gap statistic  <cit>  addressed this issue by acting as an internal validation measure, and has been applied in bioinformatics  <cit> . though in principle it is not hard to design a workflow to find the best k, in practice this is not commonly done. this is partly because there is no consensus as to which of the many different possible measures should be used to compare clusterings with different numbers of clusters, a more difficult problem than the comparison of clusterings with the same k. this adds to the difficulty of choosing the best clustering method for finding the structure of a novel dataset.

validation
validation  <cit>  plays an important role in deciding the number of clusters, as well as in assessing the performance of the clustering algorithm. cluster validation is designed to evaluate and compare clustering algorithms by their ability to analyse a dataset. there are many different validation measures. internal measures like silhouette width  <cit>  and dunn index  <cit>  depend on the inherent structure of the data  <cit> . external measures, such as the rand index  <cit>  commonly used to evaluate the noise in biological data, depend on comparison with an externally known gold standard classification of the objects.

here we propose a novel clustering algorithm, pfclust , suitable for use where no prior information about the number of clusters is given. as input, only similarity scores within the dataset are required, and evaluation of the clustering is part of the algorithm. a previous study by akoglu et al.  <cit>  designed a parameter free graph clustering algorithm to find cohesive clusters, pics. they have shown the efficiency of their method using real-world datasets including data from youtube and twitter. our method shares the property of being parameter-free, but is aimed at classifying objects rather than graphs.

as the availability of biological information accelerates, it is necessary to find the natural structure or patterns in data in order to understand biological questions. in bioinformatics, grouping proteins based on sequence  <cit>  or structure is a very common task. classification of novel proteins  <cit>  can be performed by using pattern recognition approaches, built on the assumption that some underlying characteristics are considered, while clustering proteins into superfamilies and families. there are numerous classification schemes for protein sequences including pir-psd  <cit> , a freely available database of protein sequence classification mostly applied for functional annotation, and pfam  <cit> , a classification of functional protein domains based on hidden markov models and multiple sequence alignments.

extending these ideas to three-dimensional  protein structure provides the interesting task of clustering and classifying protein domain folds. during the early 1990s the protein data bank   <cit>  held only a few thousand 3d crystal structures, and several initiatives for protein fold classification were proposed with cath  <cit>  and scop  <cit>  being the best known. these were based on either manual curation  or computer-aided manual curation . common to both approaches is that the human curator has the final word in the classification decision. with the exponential growth of the number of 3d high resolution structures deposited in the pdb during the last decade  <cit> , reaching  <dig>  structures at the beginning of  <dig>  the rate-limiting manual part of the curation process restricts our capacity to understand the full structural diversity of proteins. hence it would be ideal if a fully automated process could classify protein domains and cluster them into structurally similar groups.

methods
here we describe a partitional algorithm that uses the idea that each cluster can be represented as a non-predetermined distribution of the intra-cluster similarities of its members. the algorithm partitions a dataset into a number of clusters that share some common attributes, such as their minimum expectation value and variance of intra-cluster similarity. it is an agglomerative algorithm, meaning that it starts with separated objects and progressively joins them together to form clusters. pfclust is heuristic in the sense that it cannot be described in terms of optimising any single simply-expressed metric over the space of possible clusterings. nonetheless, we demonstrate that, over a number of validations on test datasets, it produces clusterings that closely reflect the structure of the test data, and outperforms many well established algorithms in this regard. we have taken a number of design decisions to optimize the algorithm with respect to time efficiency and result stability.

since we represent each cluster as the distribution of the similarities between its members, we need to have a clustering criterion in order to separate different clusters and find the cluster structure. the criterion used in this algorithm is the expectation value of the similarity distribution between cluster members. in order to select a suitable threshold, we will need a good approximation to the distribution of mean intra-cluster similarities for all possible clusterings. we can then select the most appropriate threshold value given an internal validation measure. the number of unique clusterings for a dataset with n points when we do not know the number of clusters, o, a priori is the bell number:

 on=∑k=0n1k!∑j=0k−1k−jjkjn 

where k is the number of clusters, inclusive of singletons. this number grows rapidly with the number of points in the dataset. hence, instead of attempting an exhaustive search of all possible clusterings, we perform a random sampling, where we randomly decide the number of clusters and randomly assign the initial distribution of points amongst clusters. this sampling approach is necessary for efficiency, but introduces a random element to the algorithm. in order to optimize some of our design decisions for the algorithm, we have internally validated using a synthetic dataset of  <dig> two-dimensional  vector points.

algorithm
the clustering algorithm consists of two parts. the first part is the randomization, and the second part incorporates both the threshold selection and the actual clustering. thus, in the first part ,  <dig> thresholds are estimated  by a randomization process. in the second part , each threshold is used to cluster the data, and the best threshold is selected. this whole process, incorporating both randomization and threshold selection, is carried out four times . if the four resulting clusterings do not agree, the algorithm replaces the least successful of the four runs with a fresh attempt and repeats until convergence. figure  <dig> shows a graphical representation of the clustering algorithm.

threshold estimation
given a dataset Ω = {α <dig> …,αn} with n members, the threshold values are estimated by multiple random clusterings of the data. firstly, a random number of clusters k  is chosen and each data point α is initially randomly placed in one of the clusters. now let xi be the distribution of the similarities between the elements of cluster i. the mean intra-cluster similarity, which is the expectation value of the distribution xi, is defined so as to exclude self-similarities and can be calculated as:

 exi=1ni2∑j=2ni∑q=1j−1sαj,αq 

where xi is the ith cluster, ni is the number of members of that cluster and sαj,αq is the similarity between elements αj and αq. in order to estimate an efficient value for n, the number of randomizations used for threshold estimation, we selected a dataset and simulated the randomization process. we repeated this ten times for a number of different values of n. figure  <dig> shows that using a small number of randomizations greatly affects the shape and sharpness of the distribution. however, increasing n greatly affects the execution time of the algorithm, since the randomization is the most expensive part of the calculation. hence, as a compromise between accuracy and expense, we selected n =  <dig> as the number of randomizations, since this is the smallest number which gives an acceptably sharp distribution of mean intra-cluster similarities. the intra-cluster mean similarities, e, from every individual cluster across the  <dig> randomizations are collated into a single distribution. from this distribution, we retrieve  <dig> threshold values from the 95% - 100% significance levels, that is the 5% of the clusters with the highest mean intra-cluster similarities. we select the ten values corresponding to the following percentiles { <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> % and  <dig> %} and ten further thresholds corresponding to the second to eleventh highest values in the distribution of intra-cluster similarities. using this number of thresholds provides a way of reducing the random element of our sampling.

similarity-based clustering
now, for each threshold value t, the dataset Ω is clustered with a similarity-based clustering. we begin with all elements separated and no clusters defined. the two most similar elements in Ω are placed together to form the first cluster. for each of the  remaining elements, we compute its average similarity e to the elements already in cluster  <dig>  and we now identify the element with the highest similarity. if this value is larger than t, the algorithm considers adding this most similar element to the given cluster. the element is added if and only if the element’s average similarity compared to the members of that cluster is at least p% of t and the overall e of the cluster is also larger than t. this process is repeated until no new element can be added to the cluster without e of the cluster falling below t. at this stage, a new cluster is formed from the two most similar remaining elements, provided that their similarity exceeds t. this process is continued iteratively until all elements of Ω have been clustered, or until the remaining elements cannot form a cluster that has an expectation value of intra-cluster similarity greater than t.

the p% of t cut-off was selected as a way to restrict the intra-cluster variation of the similarities since, in a very tight cluster, outlier members could be included because, even if they are distant from the other cluster members, the total e could still be above t. in order to estimate the value of p, we performed a number of experiments on our dataset with different p values. table  <dig> shows the results, from which we see that a value of p = 85% of t gives the optimal results with respect to the silhouette width as well as the number of clusters, with multi-member clusters and singletons being shown separately.

the table summarizes the performance of the different p values for the threshold inclusion rule. the numbers of multi-member clusters and singletons are given separately, so that the total numbers of clusters at each p value are  <dig>   <dig>   <dig>   <dig>   <dig>  and  <dig>  respectively. the silhouette width and the average of the standard deviations of the distributions of intra-cluster similarities in each cluster are also shown.

after the data are assigned to clusters, a final refinement step is applied to all points that have an average similarity score less than t when compared to the members of the cluster they have been assigned to. each such point has its average similarity calculated with every cluster and is assigned to the cluster to which it is most similar .

selecting the best threshold
for each of the  <dig> different t values, a clustering has been obtained from the algorithm. for each of these clusterings, the mean silhouette width  and the dunn index of the clustering are computed. the silhouette width is defined for each element i as:

 si=bi−aimaxai,bi 

where a is the average dissimilarity, where dissimilarity is 1-similarity, of element i with all the other elements of the same cluster and b the average dissimilarity of element i to all the members of the closest neighbouring cluster. in order to take singletons into account, a negative score  is given to each singleton point in the proposed clustering. the dunn index is defined as:

 dim=min1≤i≤mmin1≤j≤m,j≠iδci,cjmax1≤k≤mΔk∀i,j,k 

where  δ is the inter-cluster distance between the centres of clusters i and j, and max 1 ≤ k ≤ mΔk is the maximum cluster size in the dataset, where cluster size is defined as the mean distance between all members of the cluster and the cluster centroid. the cluster centroid is the element with the maximum similarity to the other members of the cluster. the silhouette width is the main factor used in deciding which threshold produces the best clustering, and the dunn index is used only as a tie-breaker to decide cases where two or more clusterings have the same silhouette width.

convergence
as mentioned above, the algorithm has a random sampling aspect. in order to increase the probability of ultimately finding the best possible clustering, we choose to repeat the process a number of times. in order to decide a number of repetitions that is time efficient and reduces the probability of runs  generating different outputs, we performed a single clustering experiment on our dataset. we ran the algorithm   <dig> times using a dataset of  <dig> 2d vectors and, considering the event a as “seeing the clustering result with the maximum silhouette width”, we found p = 76%. therefore, if we run the algorithm only once, we have a 24% probability of not finding the best solution. ideally, we would like to have a very small probability of such an event. using four repetitions reduces that probability to  <dig> %, which is sufficiently small for our needs. hence, the whole process is repeated four times and the four different clusterings are retrieved and compared. if all four runs give the same clustering, the algorithm is said to have converged and stops. if not, for each of the six different pairs of clusterings chosen from the four clusterings made, a rand index between a pair of clusters is calculated as:

 randindex=a+ba+b+c+d 

where α is the number of cases where two elements are members of the same cluster in both clusterings, b is the number of cases where two elements are members of different clusters in both clusterings, c is the number of cases where a pair of elements are in the same cluster for the first clustering and in different for the second and d is the number of cases where a pair of elements are members of different clusters for the first clustering and members of the same cluster for the second clustering. using all six different pairs of clusterings we calculate the average rand index.

we use the rand index because it is a widely accepted measure of concordance between different clusterings  and not as a maximization metric compared to some original classification. if the average rand index is high , this means that most of the runs report near-identical clusterings with no significant differences. hence, this is enough for the algorithm to converge and report the clustering with the highest silhouette width. as mentioned above we want to be very confident in the resulting clusters, therefore a very strict average rand index of  <dig>   is applied as a cut-off. in the case of an average rand index less than  <dig> , we consider that we have found significantly different clusterings. then, an instance of the clustering with the lowest  silhouette width is removed, even if this outcome has been found two or three times, and another randomization is done. this procedure is repeated until convergence.

pseudocode
i. do four times:

stage 1: calculate d .

 <dig>  do the specified “randomization”  <dig> times:

i. randomly select a number of clusters k.

ii. randomly assign each data point α to a cluster c.

iii. ∀ clusters c, calculate e for the pairwise point-point similarities within c and include this value of e in d.

 <dig>  for each of the ten percentiles { <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> % and  <dig> %} of the distribution d of intra-cluster similarities, and for ten further thresholds corresponding to the second to eleventh highest values, retrieve a threshold value t.

stage 1a: clustering

i. while any α in the dataset remains unclustered:

a. join the two most similar currently unclustered elements to form a new cluster, provided criteria in b. are met.

b. calculate average similarity of each currently unclustered data point to the current cluster and keep adding the most similar available data point as a member as long as both:

– e of the cluster > t, and.

– the average similarity of the new member to the existing members of the current cluster >  <dig> ×t.

stage 1b: clustering refinement

ii. ∀ α ∈ any c, retrieve its average similarity with all the members of its current cluster. if this average similarity < t then:

a. if its average similarity with elements of any other cluster is more than that with the parent cluster, move the point α to this other cluster.

iii. measure the silhouette width, averaged over all points with singletons each contributing − <dig>  and the dunn index for the final clustering for this t value.

 <dig>  return the t value and resultant clustering with the best silhouette width as the result of the run; in the event of a tie, use the dunn index to decide.

ii. repeat until convergence:

 stage 2: convergence 

 <dig>  if average rand index amongst all  <dig> pairs taken from the  <dig> clusterings ≥  <dig> , return the clustering with the best silhouette width as the final result .

 <dig>  if this average rand index <  <dig> , the algorithm has not converged and the clustering with the lowest silhouette width is discarded and we repeat stage  <dig> a single time to generate a new clustering.

validation
in order to validate pfclust, we used a number of synthetic 2d datasets. the first dataset consisted of  <dig> 2d vectors distributed over  <dig> groups; for each of these groups, the probability density function falls off with distance from its centre according to a normal distribution. hence the groups are approximately circular. each group corresponds to the external gold standard definition of a cluster. we also used subsets of  <dig> and  <dig> 2d vectors, respectively composed of two and three out of the  <dig> groups in the  <dig> vector dataset. the second dataset consisted of  <dig> 2d vectors distributed over  <dig> groups, which vary in shape. finally, the third dataset consisted of  <dig> 2d vectors distributed over  <dig> clusters, which all have different member densities. in each dataset, the centres are chosen such that there is no significant overlap between groups, though a handful of outlier points appear within an apparently ‘wrong’ group. we performed three different experiments based on the first dataset, in order to illustrate that the method was not finely tuned for a specific number of clusters or cluster structure. we define similarity as one minus the normalized  euclidean distance between two points. we would therefore expect pfclust to perform optimally where the clusters are approximately circular.

on all our datasets, we run pfclust as well as six other current state-of-the-art algorithms. these are  the hierarchical clustering algorithm hierarchy  <cit> ;  the hierarchical aglomerative nesting   <cit> ,  the partitional k-means clustering algorithm  <cit> ,  clustering large applications   <cit> , which is based on repeated k-mediods clustering of samples,  density-based algorithm for discovering clusters in large spatial databases   <cit>  and  model-based clustering   <cit> . we used available implementations of each of these methods in the statistical software suite r,  <cit>  the relevant packages are listed in additional file 1: table s <dig>  these algorithms cannot all be compared on the ability to find the optimal number of clusters. only pfclust and dbscan amongst the methods considered here can do this, and in fact the latter algorithm requires two parameters to be optimised before it decides the number of clusters. hence, for the five other methods, we will use the externally defined ‘correct’ number of clusters  as a given parameter and compare how well each algorithm clusters the data compared to the original classification. in order to compare the different clustering approaches, we selected the rand index as a measure of agreement between the externally known ‘correct’ clustering and that produced by a clustering algorithm.

we ran k-means and clara  <dig> times each on every dataset and have selected as the final result for each algorithm the one with the best silhouette width. for the epsilon parameter of dbscan, the maximum permitted distance between a point and its closest intra-cluster neighbour, we examined all values between  <dig> and  <dig> with a step size of  <dig> . we also iterated the min-points parameter, the minimum number of members allowed in a valid cluster, using all integer values from  <dig> to  <dig>  this resulted in  <dig> clustering outputs, from which the one with the maximum silhouette width was selected.

as an addendum to the main work, we tested the use of the silhouette width as a characteristic measure from which to decide the correct number of clusters. we ran the deterministic methods once each. we also ran the stochastic clara and k-means algorithms  <dig> times each for every number of clusters, k, between  <dig> and  <dig>  the run with the best silhouette width for a given algorithm was selected, thus deciding the number of clusters to report.

protein fold clustering using polar fourier expansions
a shape-density superposition algorithm based on spherical polar fourier  basis functions has recently been published  <cit> , in which protein shapes are represented as 3d density functions expressed as expansions of orthonormal basis functions:

 ρr,θ,φ=∑nlmnαnlmrnlrylmθ,φ 

where ylm  are the real spherical harmonics, n is the order of the highest polynomial power of the expansion, rnl  are radial functions, and αnlm are the expansion coefficients which are calculated as described previously  <cit> . mavridis et al. proposed in the same paper a novel structure-based indexing for existing classification schemes such as cath  <cit>  and scop  <cit> . their proposed consensus algorithm works well for only some of the cases it was tested on, because of the structural diversity of a number of protein domains assigned to the same superfamilies  <cit> . hence, methods such as spf would greatly benefit from an automated clustering algorithm, such as pfclust, which could identify the structure of a dataset without any prior knowledge or parameter tuning.

in order to illustrate that pfclust could be used to provide such a clustering using the spf descriptors, we performed the following study. we randomly selected  <dig> cath superfamilies, which had in total  <dig> non-redundant representative structures, and used the spf descriptors to calculate the all-against-all similarity matrix of these protein domains. we then used pfclust to cluster the protein domain structures based on these similarities.

RESULTS
original dataset
in this section, we present the resulting clusters from the  <dig> 2d vector dataset that was used for the general parameter set up of the algorithm. figure  <dig> shows the results of each method on the  <dig> 2d vector dataset; the mismatched points for each method are shown in additional file 1: figure s <dig>  from the rand index results in table  <dig>  we see that agnes has the lowest  score; that is mainly because agnes joins two groups into a single cluster . furthermore, we have a singleton as one cluster . rather better levels of performance are achieved by the hierarchy and dbscan algorithms. however, hierarchy has trouble in correctly assigning a number of the boundary cases for some pairs of clusters  and dbscan assigns a large number of points as singletons. finally, the best performing algorithms are pfclust, clara, k-means and mix model, which correctly identify all clusters and boundaries. although the rand indices for these methods are very good, they fail to reproduce perfectly the original ‘correct’ classification because the original dataset has a number of outlier points that lie closer to the centres of different groups, such as purple elements in the cyan and orange groups. so, for example, a purple element is so coloured because in the original grouping it was generated from the normal distribution used to define the purple cluster. however, it is located significantly closer to the centre of the cyan cluster, and thus we believe that a rational classifier should consider it cyan. nonetheless, the gold standard we are using means that we count the rational classification as incorrect.

the table summarizes the comparison between pfclust and the other six clustering algorithms based on the rand index between the clustering predicted by the method in question and the original gold standard clusters.

validation
for the first two experiments with the  <dig> and  <dig> 2d vectors, all methods perform very well, as shown in figure  <dig>  with only the hierarchy clustering performing a little worse in comparison with the rest, as can be seen in table  <dig> with the mismatched points for each method being shown in additional file 1: figure s <dig> 

table  <dig> summarizes the results for all methods on all the different datasets used in this study. we can see from the results that pfclust is the top performing algorithm on average with a mean rand index of  <dig> , though it perfectly reproduces the original clusters for only one dataset .

when we use all  <dig> 2d vectors, pfclust suggests a different number of clusters from that in the original clustering. in this case, pfclust finds that there are  <dig> clusters instead of  <dig> and splits the light green cluster into two, light green and gold. even though pfclust suggests a different number of clusters, its very good rand index means that it still significantly outperforms the other algorithms, except k-means, as can be seen in table  <dig>  hierarchy, clara and dbscan achieve good rand indices, though with some errors on the borderline cases. mix model gives a rand index that is only slightly worse, but assigns a singleton as one of its  <dig> clusters and, as seen in figure  <dig>  splits the greenish cluster at the top centre of the diagram between its purple and blue neighbours; the mismatched points for each method are shown in additional file 1: figure s <dig>  dbscan identifies the correct number of multi-member clusters, although as with all datasets it additionally has a large number of points assigned as singletons, shown as open circles in figure  <dig> 

we also performed another validation study on the bigger dataset of  <dig> 2d vectors distributed over  <dig> clusters, in order also to compare the algorithms using a dataset that had different shapes and sizes of clusters, as well as the larger number of data points. as suggested by visual inspection of figure  <dig>  the rand index results in table  <dig> show that all algorithms perform very well, with mix model having the best rand index. the mismatched points for each method are shown in additional file 1: figure s <dig> 

another challenge for algorithms is describing datasets that have different densities of points in their original clusters; the  <dig> 2d vector dataset consists of  <dig> clusters whose densities vary. this is a subset of the d <dig> dataset taken from  <cit> , which consists of  <dig> circular clusters with  <dig> members each. we then chose a  <dig> cluster subset of the original dataset and from each cluster we randomly selected a different number of members .

in this case, we see from figure  <dig> that pfclust reports  <dig> clusters instead of the original  <dig>  integrating the black group into the navy blue and the light green. of the other methods, only agnes was able to correctly identify all the groups, but due to the misassignment of some borderline cases it did not achieve a very high rand index. dbscan has major problems assigning the correct clusters , joining many clusters together and again leaving a large number of singletons, as seen in figure 7; the mismatched points for each method are shown in additional file 1: figure s <dig> 

pfclust – supervised mode
while we have shown pfclust to be a very powerful and accurate algorithm, it does take longer to cluster a dataset than most alternative approaches. table  <dig> shows how the running time of the algorithm until convergence varies with the number of data points. it is easy to see that the run time grows rapidly as the dataset becomes larger and that the clustering takes considerable amounts of cpu time in order to converge, with most of it being spent in the randomization process.

the table summarizes the timings for convergence of pfclust with the different datasets. the total execution time typically includes four randomization and four clustering runs, but in the case of the  <dig> 2d vectors, the “ <dig>  average rand index between the four clusterings” criterion was not met until a fifth run had been carried out. the second column in the table shows the rand index of the final clustering against the original gold standard cluster definitions.

there exist cases when one wants to cluster large groups of data and time efficiency is very important. for those cases, a supervised mode of pfclust has been implemented in order to significantly speed up the process. the supervised mode of pfclust addresses the cost issue by applying an initial clustering on a training set and estimating a number of thresholds that would finally be applied to full dataset. the training set should be a small subset of the data, representing some coherent groups or clusters amongst the full dataset that we wish to cluster.

pfclust clusters the training set and uses the three best performing thresholds to estimate a total of nine threshold values . then these nine values are applied on the full dataset and the clustering with the best silhouette width is reported. using the supervised version of the algorithm the time-consuming randomization step is removed, which results in a significant speed-up of the total process.

as a validation of the supervised mode, we used the dataset with the  <dig> vectors in  <dig> groups, the dataset of  <dig> vectors distributed over  <dig> groups, and the larger one of  <dig> vectors distributed over  <dig> groups. figure  <dig> shows the original dataset and the split between training and full sets, as well as the resulting classification by pfclust. a training set of  <dig> points was selected and pfclust running using the supervised mode clustered the first two datasets very rapidly, compared to the unsupervised version, and with high accuracy. in more detail, for the dataset of  <dig> vectors the algorithm took only three minutes to complete and gave an identical clustering to the unsupervised method. for the  <dig> vector group it took only  <dig> minutes, again with a very good rand index of  <dig>  compared with the original clustering, and in fact very slightly better than the  <dig>  achieved by the unsupervised pfclust. finally, for the larger group the algorithm used a training set of  <dig> vectors and took  <dig> hour and  <dig> minutes to complete, giving again a very good rand index of  <dig> . in each case, the rand indices achieved by the supervised and unsupervised modes of pfclust are virtually identical, but the supervised mode is much faster.

protein fold clustering using polar fourier expansions
when we used pfclust to cluster the  <dig> protein domains in  <dig> cath superfamilies, as shown in table  <dig>  the program reported  <dig> multi-member clusters and one singleton protein domain, i.e.,  <dig> clusters in total. figure  <dig> shows as a heat map the all-against-all similarity matrix of protein domains, each row is colour coded according to the cath classification and each column according to the pfclust clustering.

the  <dig> cath superfamilies that were selected to be clustered.

the agreement between pfclust and the cath classification is nearly perfect with a rand index of  <dig> . there is only a minor difference between the original classification and the classification of pfclust, where the 1q27a <dig> protein domain was classified as a singleton, whereas cath has it assigned to the  <dig> . <dig>  superfamily. we also tested the other clustering algorithms against this dataset and set the number of clusters to  <dig> for the five algorithms requiring this parameter. table  <dig> summarizes the performance of each method, and figure  <dig> visually shows the agreements and disagreements between the different clustering algorithms. we see that mix model and clara are the top performing clustering algorithms, reproducing the exact cath classification.

finding the correct number of clusters
for each of the aforementioned datasets, we tested the use of the silhouette width as the criterion for identifying the number of clusters – similarly to the way we ran dbscan. since all the algorithms depended on a single parameter k , we varied this number from  <dig> to  <dig> and the results are shown in figure  <dig>  note that these data are considered separately and do not contribute to the main results described previously, for which purpose the ‘correct’ number of clusters was instead passed to hierarchy, agnes, clara, k-means and mix model as a parameter.

we see that in most cases, except for the density dataset, at least one method found the correct number of clusters by using the maximum silhouette width as the stopping criterion. however, no method is consistently able to do this for the different datasets.

CONCLUSIONS
it has been shown that pfclust can accurately group data according to their similarities without the need for any parameter tuning. our clustering results on the synthetic datasets not only show that pfclust provides structurally meaningful clusters, but also that it performs best when compared to six other well-known clustering algorithms. clustering protein domains using a density representation gives excellent agreement with the cath part-manually curated classification. in the future, the full cath database could be automatically clustered based on such density representations of protein domains.

competing interests
the authors have received funding from wada. other than this sponsorship, the authors declare no conflict of interest.

authors’ contributions
lm conceived and implemented the algorithm, carried out the experiments with pfclust and participated in the drafting of the manuscript. lm and nn designed the experimental and validation studies and carried out the comparison of pfclust with the other methods. nn participated in the drafting of the manuscript. jbom participated in the drafting of the manuscript and provided guidance. all authors read and approved the final manuscript.

supplementary material
additional file 1: figure s1
comparison of different clustering algorithms; figure s <dig>  comparison of the  <dig> and  <dig> 2d vector datasets; figure s <dig>  comparison of different clustering algorithms; figure s <dig>  comparison of different clustering algorithms; figure s <dig>  comparison of different clustering algorithms; table s <dig>  r packages used in the comparison of different clustering methodologies.

click here for file

 acknowledgements
this project has been carried out with the support of wada. jbom and nn thank the scottish universities life sciences alliance  and the scottish overseas research student awards scheme of the scottish funding council  for financial support. we thank dr luna de ferrari for her valuable comments on this manuscript.

availability
by request to the corresponding author, lazaros.mavridis.lm@gmail.com.
