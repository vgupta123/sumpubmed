BACKGROUND
classifier complexity and overfitting
the small-sample problems with microarray-based classification have long been recognized  <cit> . the potential number of features  upon which a classifier can be based is extremely large, the potential features consisting of all the gene-expression levels measured on a microarray , and the sample size being the number of microarrays in the study . when the number of features is large in comparison to the sample size, classifier design is hampered by the designed classifier tending to overfit the sample data, which means that the designed classifier may provide good discrimination for the sample data but not for the general population from which the sample data have been drawn.

classifier design involves choosing a classifier from a family of classifiers on the basis of the data by means of some algorithm. in this paper we restrict our attention to the case of two classes. classification involves a feature vector x =  on d-dimensional euclidean space ℜd composed of random variables , a binary random variable y, and a function  ψ: ℜd → { <dig>  1} to serve as a predictor of y. the values,  <dig> or  <dig>  of y are treated as class labels. the error of ψ is the probability, p ≠ y), that the classification is erroneous. classifier error depends on the probability distribution, fx, y, called the feature-label distribution, of the feature-label pair , in particular, the class conditional distributions, fx| <dig> and fx| <dig>  since in practice we do not know the class conditional distributions, a classifier is designed from sample data.

a classifier is optimal in a family g of classifiers if its error, εg, is minimal among all classifiers in g. since a designed classifier depends on the particular sample, it is random relative to random sampling. we would like the expected error, εg, n, of the designed classifier, n denoting sample size, to be close to εg. if g and h are families of classifiers such that g ⊂ h, then εh ≤ εg; however, for designed classifiers, it may be that εh, n > εg, n. that is, the designed classifier may partition the feature space well relative to the sample data but not relative to the full distribution. this phenomenon, called overfitting, is widespread in studies with small sample sizes. to mitigate overfitting, one can choose from smaller classifier families whose classifiers partition the feature space more coarsely. using g instead of h, where g ⊂ h, reduces the design cost, εg, n - εg, relative to εh, n - εh at the expense of introducing a constraint cost, εg - εh. for a fixed sample size n, consider a collection of classifier families, g <dig> ⊂ g <dig> ⊂ g <dig> ⊂ .... the increasing size of the families means increasing classifier complexity. while the smaller families extensively reduce design cost, their constraint is excessive thus creating a situation in which the expected errors of the designed classifiers fall as we utilize increasingly large families but then begin to increase when the design cost grows too much.

while overfitting is often thought of as applying to the complexity of functional structure of a classifier, it also applies to the number of features composing a classifier. a feed-forward neural network classifier with one hidden layer, d features and k hidden nodes, is an operator on d dimensional euclidean space ℜd, as is a linear classifier. their difference in complexity is that the linear classifier partitions ℜd into two classes via a hyperplane, whereas the neural net more finely partitions the space, thereby reflecting greater complexity. another way to increase complexity is to increase the number of features. in this way a linear classifier on d +  <dig> features is more complex than a linear classifier on d variables because the former reduces to the latter by setting one of the variables equal to  <dig>  in this vein, if we consider a sequence, x <dig>  x <dig>  ..., xd, ..., of features, we often first observe a decrease in expected error as d increases and then subsequently an increase in error for increasing d. while this description is idealized and the situation can be more complex, it describes the peaking phenomenon.

in light of the peaking phenomenon, a natural question arises: given a set of potential features, what is the optimal number of features one should use for classifier design  <cit> ? the question is complicated because it depends on the classification rule, feature-label distribution and sample size. figure  <dig> illustrates peaking in terms of sample size n and the number d of features. the surface gives the average error of designed lda classifiers in terms of d and n based on two gaussian class conditional distributions possessing the same covariance matrix. the features are slightly correlated and we see that peaking occurs with very few features for sample sizes  <dig> and below, but then exceeds  <dig> features for sample sizes above  <dig>  a much more serious situation for lda is presented in fig.  <dig>  where the situation is the same except that the features are highly correlated. with a sample size of  <dig>  large for most microarray studies, the optimal number of features is  <dig>  even with a sample size of  <dig>  the optimal number of features is only  <dig> 

in our preceding examples, we knew the distributions and were able to order the features so as not to have to consider all possible feature sets; in practice, the features are not ordered, and a good feature set must be found from among all possible feature subsets. this involves the use of a feature-selection algorithm, which is part of the classification rule. feature selection yields classifier constraint, not a reduction in the dimensionality of the feature space relative to design. for instance, if there are d features available for linear discriminant analysis , when used directly, then the classifier family consists of all hyperplanes in d-dimensional space. but, if a feature-selection algorithm reduces the number of variables to m <d prior to application of lda, then the classifier family consists of all hyperplanes in d-dimensional space confined to m-dimensional subspaces. the dimensionality of the classification rule has not been reduced, but the new classification rule  is constrained.

a standard way of measuring classifier complexity is via the vc  dimension of a family of classifiers  <cit> . the vc dimension is defined in the methods section; here we note that it provides a measure of the degree to which a classifier can separate points, the greater the separation ability the higher the vc dimension and the lower the separation ability the lower the vc dimension. high vc-dimension classifiers have a greater ability to discriminate complex class interaction, at the cost of a greater potential to overfit, than do low vc-dimension classifiers. the vc dimension of a linear classifier with d features is d +  <dig>  whereas the vc dimension of a feed-forward neural network with one hidden layer, d features and k hidden nodes, exceeds d  <cit> . depending on the number of nodes in the hidden layer, this can greatly exceed the vc dimension of a linear classifier.

the potential for overfitting is exhibited by a classical bound on the expected design error. for sample size n, the expected design cost of a classifier chosen from a family g via the empirical-error rule, which chooses the classifier in g that makes the least number of errors on the sample data, can be bounded via the vc dimension of g,

εg,n−εg≤vglog⁡nnc0     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwf1oqzdawgaawcbagaem4rackaeiilawiaemoba4gabeaakiabgkhitiab=v7alnaabaaaleaacqwghbwraeqaaogaeyizim6aaoqaaeaadawcaaqaaiabdafawnaabaaaleaacqwghbwraeqaaogagiibawmaei4ba8maei4zacmaemoba4gabagaemoba4gaaawcbagaem4yam2aasbaawqaaiabicdawaqabaaaaogaaczcaiaaxmaadaqadaqaaiabigdaxagaayjkaiaawmcaaaaa@4743@

where vg is the vc dimension of g and c <dig> is a constant independent of g and n  <cit> . to make the bound small the sample size must greatly exceed the vc dimension. while providing a cautionary warning concerning high vc-dimension classifiers, the bound of eq.  <dig> is not the end of the story when it comes to classifier design. first of all, the bound applies to all possible distributions, and therefore can be very loose. second, and directly to the point of the present study, the design error depends on the classification rule, not only the family from which the classifier is chosen.

given a very large sample, one might pose the rule of thumb that a more complex distribution requires a more complex classifier. the overfitting problem strikes at this rule of thumb when samples are insufficiently large. the rule is not reversed; rather, it breaks down. the vc dimension, or any other measure of classifier complexity, can only provide a loose guideline in the context of a warning against overfitting. the efficacy of a classifier design strategy depends not only on the complexity  of the classifier family, but also on the complexity of the feature-label distribution, or the difficulty of the classification problem. the latter can be rigorously approached by defining a measure of distributional complexity and calculating classifier performance as a function of distributional complexity  <cit> . in that case, when samples are small, as expected we observe that simple classifiers work best for low-complexity distributions, but we also often observe that, when a complex classifier performs well, the distribution is also of low complexity and a simple classifier could just as well do the job. this is why in microarray studies one should use high-complexity classifiers with caution.

the point of this paper is that one should not give up on complex classifiers, in particular, neural networks. as we will see in the current investigation of neural-network design, using an appropriate design strategy can yield good results in a small-sample setting, even for a high vc-dimension classifier. specifically, neural networks can give competitive results for small samples, so long as they are properly trained. the aim of the present study is to examine over a range of models the degree to which variously trained neural networks can provide competitive results for the kinds of sample sizes used in many microarray studies, our emphasis being on training via noise injection.

noise injection
sietsma and dow  <cit>  found that injecting noise into the sample data can lead to neural networks with improved performance, meaning that the designed neural networks can have smaller misclassification errors than those designed without noise injection. since then there has been substantial research on two aspects of noise injection, theoretical proof and implementation. holmsträom and koistinen  <cit>  showed that noise-injection-based design is asymptotically consistent as the sample size goes to infinity provided that the noise is chosen correctly. moreover, rather than relying on heuristic choices, they developed rigorous methods to find the distribution of noise to be injected through gaussian kernel density estimation. by using a taylor expansion, matsuoka  <cit>  claimed that injecting noise to the neural network is equal to adding a regularization term to the error cost function. following a similar but more rigorous approach, a neural network based on a regularized error function was introduced by bishop  <cit>  to avoid the randomness and increasing computation time brought by the injected random noise. the regularization term in the cost function penalizes a fast changing input-output relationship, and is believed to prevent the neural network from overfitting to the individual sample points. however, an  <cit>  pointed out that the second derivative term of the cost function, which is omitted in bishop's derivation, is a perturbation that can be either positive or negative, and can be ignored only when the network function fits well or is very smooth. grandvalet and cano  <cit>  also found that the error perturbation cannot be omitted, and grandvalet  <cit>  found a bound for the perturbation as a function of the amount of noise injected. he therefore claimed that noise injection could be beneficial. in the mean time, reed et al.  <cit>  pointed out that noise injection has similar effects as weight decay, another common method used in neural network training in order to improve performance.

recently, the study of noise injection has moved to improving the training scheme . proposed methods include applying the noise not to the input data but to the hidden nodes  <cit> ; use of elliptic rather than spherical shaped gaussian noise, and basing the noise distribution on the data  <cit> ; noise designed according to the local data distribution  <cit> ; and utilizing a regularized cost function in conjunction with noise injection to alleviate the effects caused by inappropriate variance of the injected noise  <cit> .

the effects of noise injection can be illustrated pictorially. figure  <dig> corresponds to a 2-feature nonlinear model and shows a sample of size  <dig>  with  <dig> points from each class. parts a, b, c, and d of the figure show the optimal classifier, the quadratic discriminant analysis  classifier, the standard neural network , and the noise-injected neural network , respectively. the standard neural network overfits the training data by fitting its decision boundary into the small gaps between samples, either between two classes , or among points of the same class . by filling up small gaps with noise points, noise injection has moved the decision boundary out of these local optimal states, smoothly fit it into the large gap between the two classes, so it is closer to the bayes decision boundary, and hence enhances the classification performance.

RESULTS
in this study, we have conducted an extensive simulation-based comparative study of ninn design by taking advantage of contemporary high-performance computing, a 512-node beowulf cluster, which was not available when many neural-network training procedures were proposed. we consider a number of different feature-label models across various small sample sizes using varying amounts of noise injection. besides comparing ninn design to classical snn design, the paper compares it to a number of other classification rules: lda, qda, the strong feature classifier , the gaussian kernel  classifier, and the 3-nearest-neighbor  classifier. since our applications concern microarray data for expression-based classification for diagnosis and prognosis, we also consider ninn design as it relates to a study of survivability of breast cancer patients. the models and patient data are described in the methods section, as are the classification rules used in the study.

corresponding to each experimental case determined by the model, the degree of correlation, and the number of features, there is a graph of the errors of the classifiers for the sample sizes considered. the complete set of these graphs appear in the additional file  <dig>  with some of them also appearing in the paper to support the discussion. in these comparison graphs, the noise ninn results correspond to largest amount of noise injection available . the amount of noise injected is important. for each situation there is a graph showing the errors for different amounts of noise for each sample size considered, again the full set being given in the additional file  <dig>  a large amount of noise may be required to achieve good noise-injected design and this entails a high computational cost, which can be prohibitive when considering a large number of feature sets. in the case of linear classification, avoidance of this computational burden motivated the introduction of sfc, which produces the effect of noise injection but determines the classifier analytically without the computational cost of introducing random noise. let us proceed to consider the different models.

linear model
simulation results for the linear model are shown in fig.  <dig>  for the linear model, lda is optimal for the feature-label distribution, with its sample-based performance depending on sample size. in the case of  <dig> uncorrelated features, except for the impractical case of n =  <dig>  lda and sfc perform equivalently, with qda in the processing of catching up as the sample size increases, as expected. among the neural nets, ninn performs best and its performance is very close to that of lda. as is the case throughout the experiments, snn does poorly. for  <dig> uncorrelated features, the performance of all the classifiers improves, but now we see the advantage of sfc for very small samples. it outperforms lda for n <  <dig>  after which it stabilizes, whereas lda continues to improve. most interestingly relative to the current study, for n ≥  <dig>  ninn outperforms lda. for both  <dig> and  <dig> features, gk is not far behind ninn and 3nn is not competitive with ninn except for very small sample sizes.

for slightly correlated features, the situation is mostly analogous to the uncorrelated case, except that overall performance is worse, as expected. the main difference is the performance decline of sfc relative to lda and ninn, although sfc still performs better than qda, and snn. matters are quite different for  <dig> features. sfc outperforms lda and ninn for all sample sizes, sfc and ninn significantly outperforming lda for smaller sample sizes. the key point is that lda is only performing slightly better with  <dig> features than with  <dig>  as seen in previous cases, 3nn performs best for very small samples.

when we go to highly correlated features, there is severe performance degradation. for  <dig> features, ninn bests lda for n ≤  <dig>  and their performance is essentially the same for n ≥  <dig>  gk is not far behind. the more interesting observations occur with  <dig> features. except for n ≤  <dig>  when 3nn is better, ninn provides the best performance, and is much better than lda for the smaller sample sizes. ninn is also better than gk, although the latter outperforms lda for n ≤  <dig>  the problem with lda is that it has suffered the peaking phenomenon: it performs worse with  <dig> features than with  <dig>  early peaking for lda with highly correlated features has been previously observed  <cit> . note that in this highly correlated case, as with the previous cases, 3nn performs well for very small samples, but does not improve much thereafter as the sample size increases.

the effect of different amounts of noise is shown in fig.  <dig> for uncorrelated features. classification error is plotted as a function of the amount of noise injection. each line corresponds to a fixed training sample size, from  <dig> to  <dig> for every  <dig> sample points, the highest line corresponding to  <dig> sample points and the lowest corresponding to  <dig> sample points. this graph is typical in that noise injection is more beneficial for smaller samples and there is diminishing return for additional noise. indeed, too much noise can mask the original data to the extent that it ceases to be beneficial. throughout the experiments we have seen that 10-fold noise injection is quite beneficial and typically does not cause loss of performance. note that the graph in fig.  <dig> has substantially more noise injected for smaller samples. this is on account of two reasons: first, more added noise provides greater benefit for smaller samples; second, the computational burden increase in accordance to the sample size. owing to the typical behaviour illustrated in fig.  <dig>  for all other models we defer to the additional file  <dig> for graphs showing the effects of different amounts of noise injection.

low-curvature nonlinear model
simulation results for the low-curvature nonlinear model are shown in fig.  <dig>  as expected, although qda is optimal for the feature-label distribution, lda outperforms qda for smaller sample sizes  owing to its lower complexity. from the perspective here, the key point is that ninn performs best among all classifiers for n ≤  <dig> and second to qda  for n ≥  <dig>  note that gk and sfc are not far behind ninn. an interesting phenomenon occurs with  <dig> features: sfc performs best for n ≤  <dig>  along with this, lda significantly outperforms qda. as for ninn, it is close behind sfc, especially for n ≥  <dig> 

for slightly correlated features, there are some large differences in the performance comparisons. in this case, with  <dig> features sfc performs relatively very poorly. qda overtakes lda earlier and then does much better as the sample size increases. however, once again ninn does well, having the best performance for  <dig> ≤ n ≤  <dig>  and only being bested by qda for n ≥  <dig>  note also that gk performs close to ninn for all n. similar statements hold for  <dig> features, an exception being that qda never overtakes ninn.

with highly correlated features, peaking plays a critical role. it is particularly severe for lda and qda, with error rates higher for  <dig> features than for  <dig>  this is in agreement with the results of previous study as shown in fig.  <dig> <cit> . it even occurs for ninn and gk. otherwise, there are a lot of similarities to the slightly correlated case, with qda overtaking lda and, as in the 5-feature slightly correlated case, ninn outperforming qda for the smaller sample sizes, this time in both the 5- and 10-feature cases. an interesting point regarding neural networks is that, for  <dig> features, gk is not far behind ninn, as we have witnessed before.

high-curvature nonlinear model
simulation results for the high-curvature nonlinear model are shown in fig.  <dig>  it is instructive to compare the results for the low-curvature and high-curvature models. focusing first on  <dig> features, in the uncorrelated model we are struck by the much poorer relative performance of ninn for the high-curvature model in comparison to the low-curvature model. whereas in the low-curvature nonlinear model, ninn is substantially better than snn, and slightly better than gk, in the high-curvature nonlinear model, ninn is bested by gk, although it remains significantly better than snn. its relation to qda is also much different in the high-curvature model, where qda is superior to it for n ≥  <dig>  and much better than it for n ≥  <dig>  whereas in the low-curvature model, qda does not outperform ninn until n ≥  <dig>  and then not very much. for  <dig> features in the uncorrelated case, ninn again outperforms snn across all sample sizes, the gap closing at sample size  <dig>  as in the 5-feature case, ninn is beat by gk. whereas qda never surpasses ninn in the low-curvature model, it surpasses ninn in the high-curvature model for n ≥  <dig>  analogous considerations apply to the different relative performance of ninn compared to gk and qda in the other five high-curvature models.

relative to overfitting the sample data, the key difference between the low-curvature and high-curvature nonlinear models is the increased curvature in the high-curvature model. noise injection has the effect of smoothing the decision boundary and this smoothing has greater benefit when the decision boundary is less curved. as another effect of high curvature, note the strikingly poor performance of 3nn.

equal-mean model
in this model, qda is optimal for the feature-label distribution with the decision boundary being a hypersphere. the main point to be made here is that the comparisons are similar to the high-curvature nonlinear model, the key factor being the high curvature of the decision boundary in the qda-optimal model. another noteworthy observation is that the performance of ninn does not always monotonically improve along with the amount of noise injected. in some certain cases, the classification error can first decrease, then increase after the noise injection surpasses a certain amount. this phenomenon is most prominent in the case of  <dig> highly correlated features.

xor model
the results for the xor model are fairly consistent and clear across all six cases,  <dig> and  <dig> features, and uncorrelated, slightly correlated, and highly correlated models. ninn, gk, and 3nn have very close performances and they perform significantly better than snn. if anything can be said concerning the relationship between ninn, gk, and 3nn, it is that 3nn is insignificantly slightly better than ninn and gk for small sample sizes, with the situation reversing for larger sample sizes.

bimodal model
simulation results for the 5-feature case for the bimodal model are shown in fig.  <dig>  the results for the bimodal model show similarity to those for the xor model in that, as a group, ninn, gk, and 3nn tend to outperform snn for the smaller sample sizes, but with the intra-group performances more spread and the inter-group performance differences tending to dissipate as the sample size increases. among the classifiers ninn, gk, and 3nn, for  <dig> features, ninn is better than gk, which is better than 3nn, with the differences becoming larger for increasing correlation. for  <dig> features, ninn remains the best but 3nn outperforms gk.

patient data
simulation results for the patient data are shown in fig.  <dig>  basically, our simulation on patient data shows that ninn achieves the best performance with 3nn, gk and lda not too far behind.

when we observe the 5-feature results for the patient data, we see a striking similarity with those for slightly correlated features in the nonlinear model. ignoring the fact that qda provides the best performance for larger samples in the nonlinear model, for which it is optimal relative to the feature-label distribution, in both cases ninn performs best across the full range of sample sizes, and even more so with the patient data. in both cases, lda and gk are similar and trail ninn. 3nn is a little better in the real patient data while sfc is a little worse. note that while we have compared the patient-data results to those of the slightly correlated nonlinear model, similar correspondences exist between the patient data and the highly correlated nonlinear model, which is only reasonable since, as pointed out previously, there are many similarities between the slightly and highly correlated nonlinear models.

the salient point regarding using  <dig> features for the patient data is peaking, as it is in the highly correlated nonlinear model. for instance, for n =  <dig>  lda performs worse with  <dig> features than with  <dig> features and qda performs much worse with  <dig> features than with  <dig>  nn shows no improvement with  <dig> features compared with  <dig> features. ninn shows a slight improvement with  <dig> features, indicating later peaking. although ninn pretty much flattens out when n >  <dig>  it again has the best performance across all sample sizes. the most prominent differences between the real patient data and highly correlated nonlinear model are qda and gk. poor performance owing to peaking is particularly evident with gk.

CONCLUSIONS
although neural networks have high vc dimension and can therefore suffer from overfitting the sample data, their performance is highly dependent on the training procedure  employed. this paper has demonstrated that in many instances noise-injected neural network design is superior to classical neural-network design and to the other tested methods, and in almost all cases it does not perform substantially worse than the best of the other methods. this conclusion has importance for the design of classifiers for diagnosis and prognosis based on gene-expression data because sample sizes are often limited and, unless the class conditional distributions are easily discriminated, say by a linear classifier, a higher-complexity classifier must be employed.

