BACKGROUND
the most common technique for establishing a homology between sequences is sequence alignment . numerous algorithms have been developed for aligning sequences. these include exhaustive dynamic programming algorithms as well as faster heuristic algorithms . in these algorithms each alignment is evaluated using some score matrix, wherein the score matrix depends on the expected similarity between the aligned sequences. however, in some cases one needs to compare related sequences that are divergent, or related sequences that are not at all homologous. one example of biologically related sequences that do not share a common ancestor is where two genes with different evolutionary history are found on the same genome. another example is related to sequencing wherein non-overlapping reads originate from the same gene . these cases do not allow for a direct sequence alignment. they may, however, be identified as related using alignment-free sequence comparisons.

alignment-free methods are less accurate than direct sequence alignments. thus, they are only used as a last resort when direct alignment is either impossible or computationally complex. a common method for alignment-free sequence comparison is comparison via the word  counts
 <cit> . in this approach a nucleotide sequence of arbitrary length l is represented by the counts of the 4n different n-mers. it is no surprise that such comparisons are less accurate than the sequence alignment as replacing the sequence with the vector of the word counts results in a loss of information.

high throughput sequencing data analysis is a relatively novel area of computational biology where application of alignment-free sequence comparison is especially desirable. indeed, high throughput sequencing runs generate a vast amount of relatively short  reads. these factors make direct comparison with the reference complex if not impossible. one can think of different applications of alignment-free methods to sequencing data. in particular, an assembly free comparison of genomes using reads generated from them is possible
 <cit> . in the present work we focus on a different application: clustering of reads coming from different genes and possibly different species based on n-mer counts. one case when such a clustering is desirable is a sample of large size, so that the large number of reads makes direct assembly computationally prohibitive. in this case clustering reads rather than randomly splitting the sample is desirable.

in the present study we focus on small values of n, such that l ≫ 4n. in other words, we assume that the compared sequences are sufficiently long to avoid the situation where l ≤ 4n and almost all counts are either zero or one. in particular, for  <dig> bp reads the appropriate value of n is  <dig>  possibly 2; for  <dig> bp reads the appropriate values of n are  <dig>   <dig> or  <dig> 

we implement a word count based clustering algorithm for short nucleotide sequences  <dig>  in this approach each sequence is represented by the vector of the n-mer counts . we focus on a centroid based clustering algorithm because of its linear space and time complexity. in the framework of centroid based clustering each centroid is characterized by its n-mer frequencies. in particular, we study expectation maximization  algorithm, which is a generalization of the k-means algorithm. each individual sequence is attributed to a centroid based on the likelihood of obtaining the observed word counts within a given model. in this context the kullback-leibler  divergence naturally arises as the log likelihood function. we study centroid based algorithms involving other distances as well. we evaluate performance of different algorithms using simulated reads data. in the end we apply our clustering methods to a real sequencing run.

RESULTS
comparison of centroid based algorithms
we study several centroid based clustering algorithms in the context of alignment-free sequence comparison. from the point of view of these algorithms each sequence is represented by the vector of the word  counts. we restrict ourselves to the case of the relatively short sequences, having length typical to sequencer reads. we implement expectation maximization algorithm using kullback-leibler divergence as the log likelihood function. we also consider centroid based clustering algorithms using l <dig> distance  and d <dig> distance
 <cit> . in addition we consider k-means algorithm. k-means algorithm is the l <dig> algorithm with preliminary whitening of data. all these algorithms have an established convergence property. we implement centroid based clustering algorithms using some other distance functions: symmetrized kl divergence,
d2∗ <cit>  and χ <dig> statistic . the latter algorithms do not possess a known convergence property.

we compared the performance of these algorithms using  <dig> randomly chosen subsets of human reference mrna sequences. each subset consisted of  <dig> sequences. for each of the  <dig> subsets we generated several sets of simulated reads, different sets containing simulated reads of different length. for each dataset we performed clustering using different methods. for k-means clustering we used implementation available in scipy package. we evaluated classification performance  for a sequence as the maximal fraction of reads from this sequence which fall into the same cluster. we compared the distribution of the recall rates obtained for each sequence in each of the datasets. the results are presented in tables
 <dig> 
 <dig> 
 <dig>  and
 <dig>  in this simulation the em algorithm showed a higher performance for the word size n =  <dig> with  <dig> or  <dig> clusters, n =  <dig> with  <dig> clusters and n =  <dig> with  <dig> clusters. the l <dig> algorithm showed a higher performance for n =  <dig> with  <dig> or  <dig> clusters and for n =  <dig> with  <dig> or  <dig> clusters. note that the l <dig> and d <dig> algorithms operate with the word frequencies normalized for each read individually, while the k-means and the em methods use the normalization related to several reads ). methods from the first group  generally exhibit a better performance for a larger number of clusters.

mean recall rates and standard deviation for various read lengths and numbers of clusters. for every read length clustering was performed on  <dig> simulated read sets, each set originating from  <dig> randomly chosen human rna reference sequences and having  <dig> reads. word length is n =  <dig> 

mean recall rates and standard deviation for various read lengths and numbers of clusters. for every read length clustering was performed on  <dig> simulated read sets, each set originating from  <dig> randomly chosen human rna reference sequences and having  <dig> reads. word length is n =  <dig> 

mean recall rates and standard deviation for various read lengths and numbers of clusters. for every read length clustering was performed on  <dig> simulated read sets, each set originating from  <dig> randomly chosen human rna reference sequences and having  <dig> reads. word length is n =  <dig> 

mean recall rates and standard deviation for various read lengths and  <dig> or  <dig> clusters. for every read length clustering was performed on  <dig> simulated read sets, each set originating from  <dig> randomly chosen human rna reference sequences and having  <dig> reads. clustering was performed using all distance functions considered in the paper, including those which do not guarantee convergence. results for l <dig> and d <dig> distance are not shown. word length is n =  <dig> 

even though these differences can be considered statistically significant, their magnitude is rather small for practical purposes. based on this fact we recommend using the l <dig> or k-means algorithm for computational efficiency. indeed, the em algorithm involves the computationally expensive evaluation of the natural logarithms; while the l <dig> and k-means algorithms only involve arithmetic operations. this can make the run time of the em algorithm exceed that of the l <dig> and k-means algorithms by more than a factor of  <dig> 

we evaluated the performance of the algorithms involving the symmetrized kl divergence,
d2∗ and the χ <dig> distance on the same datasets for the word length n =  <dig>  the
d2∗ algorithm failed to converge in  <dig> out of  <dig> runs. the results are shown in table
 <dig>  none of the three mentioned algorithms exhibits a performance better than that of the em or k-means algorithm. taking into account the fact that the convergence of these algorithms is not established , our data exhibit no benefits of using these methods.

tables
 <dig> 
 <dig> 
 <dig> and
 <dig> show that the recall rate increases with the increasing read length, number of reads being constant. this conforms to our intuition that with the increasing reads length the word counts increase, resulting in a smaller effect of statistical fluctuations. tables
 <dig> 
 <dig> and
 <dig> show that the recall rate has almost no dependence on the word size n for n =  <dig> , <dig> 

we performed a set of simulations with different number of reads generated from the same source sequences in order to study the dependence of the recall rate on the number of reads. the results are shown in table
 <dig>  for smaller number of reads the recall rate is lower. it is gradually increasing and stabilizing as the number of reads is increasing. our explanation for this fact is that for a small number of reads some of the source sequences have only a few reads, and the recall rate is significantly influenced by the pseudocounts.

mean recall rates and standard deviation for different number of reads. for each of the  <dig> randomly chosen subsets of human reference rna sequences we simulated reads, choosing the specified number of reads. clustering was performed using the em, l <dig> and d <dig> algorithms. word length is n =  <dig>  read length is 200bp. when computing the recall rate for each contig we use pseudocounts, artificially increasing the count of reads in each cluster by one.

we performed a series of simulations for different sequencing error rates. the results are shown in tables
 <dig> 
 <dig> and
 <dig>  as expected, the recall rate decreases monotonically when the sequencing error rate increases for all clustering methods.

mean recall rates and standard deviation for various error rates and numbers of clusters. for every value of the error rate clustering was performed on  <dig> simulated read sets, each set originating from  <dig> randomly chosen human rna reference sequences and having  <dig> reads. word length is n =  <dig>  read length is 200bp.

mean recall rates and standard deviation for various error rates and numbers of clusters. for every value of the error rate clustering was performed on  <dig> simulated read sets, each set originating from  <dig> randomly chosen human rna reference sequences and having  <dig> reads. word length is n =  <dig>  read length is 200bp.

mean recall rates and standard deviation for various error rates and numbers of clusters. for every value of the error rate clustering was performed on  <dig> simulated read sets, each set originating from  <dig> randomly chosen human rna reference sequences and having  <dig> reads. word length is n =  <dig>  read length is 200bp.

soft em clustering
we implemented the soft em clustering algorithm using the kl divergence as the log likelihood function. we performed soft em clustering of simulated viral reads in the human background using single stranded and double stranded dna and rna viruses as well as retroviruses. we generated  <dig> datasets for each read length and built the roc curve for each dataset. the area under the curve  averaged over the  <dig> datasets is shown in figure
 <dig> 
 <dig> 
 <dig> and
4

the auc and its dependence on the read length is determined by the interplay of the different factors. these include the choice of the likelihood function in the em algorithm, uniformity of the sequence composition of the studied viral sequences as well as the choice of the background reads. our results indicate that double stranded viruses as well as single stranded rna viruses generally show higher auc than single stranded dna and retroviruses. note that the lower auc is a consequence of the change of the nucleotide composition across the sequence .

application to real data
a real world scenario where alignment-free sequence clustering is desirable is assembly of an hts run containing a large number of reads. it can be the case that the available computational resources  are not sufficient to perform a direct assembly. in such instances one may need to split the reads into several sets and assemble each set individually, merging the contigs afterwards. we explore the random splitting and the educated splitting using alignment-free clustering of an illumina run containing  <dig> million cdna reads from a nasal swab. the results are shown in table
 <dig>  it turns out that the educated splitting results in a better assembly . the difference between the hard em and the k-means partitioning is rather small, and these two partitionings improve assembly compared to the random splitting. the soft em leads to a better assembly than both hard em and k-means partitioning. the reason for this is that the soft em algorithm allows a single read to be assigned to multiple clusters simultaneously. this provides more possibilities for the reads from the same contig to be clustered together and consequently assembled. in fact, for a small value of the velvet hash length  the soft em partitioning results in more reads mapping back to contigs than assembling the run as a whole. we speculate that the explanation for this observation is that the small value of the hash length results in a larger number of contigs at the cost of specificity. partitioning the reads in an educated way makes assembly of each subset more specific.

statistics of assembly of real sequencing data.  <dig> , <dig> reads from an illumina run on a nasal swab cdna were assembled with and without splitting the sample. splitting into  <dig> or  <dig> clusters was performed randomly as well as using the soft and hard clustering techniques studied in the present work.

afcluster software
we implemented afcluster software for centroid based alignment-free clustering of nucleotide sequences based on word  counts. word counts can be computed using overlapping or non-overlapping n-mers, optionally concatenating the sequence together with its reverse complement. where no reading frame is found we recommend using overlapping n-mer counts and/or stacking with the reverse complement. non-overlapping n-mer counts can be used to compare the codon usage of coding sequences.

implemented clustering algorithms include expectation maximization algorithm, k-means algorithm as well as centroid algorithms using different distance types: l <dig> distance, d <dig> distance,
d2∗ distance, symmetrized kl divergence, χ <dig> distance. one can also perform consensus clustering. in this case regular clustering is performed a specified number of times, and the consensus partitioning is built based on patterns of individual samples clustering together. consensus clustering mitigates the dependence of the resulting partitioning on the random initialization inherent to centroid-based methods. however, this is achieved at the cost of
o time complexity and
o space complexity for input consisting of n sequences.

the software also allows soft em clustering, in which case each sequence is only assigned to each cluster with some probability. this method gives some estimate of the clustering accuracy without the overhead of the consensus clustering. the ability to simultaneously assign the same sequence to several clusters is also useful when splitting a sample before performing assembly.

afcluster software is implemented in c++. it has been compiled and tested using gnu gcc. the tool is open source under the gnu gpl license. it is publicly available from
https://github.com/luscinius/afcluster as a source code together with the documentation. it is also available as additional file
 <dig> 

CONCLUSIONS
alignment-free sequence clustering is a useful tool for sequence analysis. however, it has the limitations found with other clustering algorithms based on word counts. a major potential confound is assumption that for any given gene or organism there is a consistent frequency distribution for counted words. however, there are examples where word frequencies change across the same genome
 <cit> . also, viral genomes are systematically affected by the interaction with the host which leads to the host mimicry
 <cit> . a separate study would be required to address these issues.

centroid based clustering offers the linear time and space complexity, which is critical for large datasets; in particular, hts reads. even though the hard expectation maximization algorithm using the kullback-leibler divergence as a log likelihood function shows superior performance in some cases, it is computationally feasible to use the k-means algorithm as the time gain outweighs the possible accuracy loss. it also turns out that it is sufficient to use short word sizes . soft expectation maximization clustering is more effective than the hard expectation maximization as it allows to assign a read to more than one cluster simultaneously. application to a real dataset shows that the soft em algorithm is especially effective in the context of the hts read assembly.

