BACKGROUND
natural products  are small molecules synthesised by living organisms. in drug discovery, the class of nps termed secondary metabolites that are involved in defence or signalling, are of particular importance because they were optimised during evolution to have effective interactions with biological receptors. they are therefore good starting points for designing new drugs  <cit> . hence, natural product-likeness  of a chemical structure can serve as a criteria in lead compound selection and in designing novel drugs  <cit> . in order to estimate np-likeness of a molecule, prior knowledge such as physicochemical and structural properties of existing natural products have to be captured. in this work, we focus only on identifying structural features typical of natural products, and based on their presence, rank molecules of interest according to their np-likeness.

methods
cdk-taverna version  <dig> <cit>  is an open-source java tool kit to perform cheminformatics tasks, making use of the pipelining technology offered by taverna version  <dig>  <cit> , an open-source workflow management system. the cdk-taverna  <dig> plug-in is based on the chemistry development kit   <cit>  and few other open source java libraries. the individual components required to score a small molecule for np-likeness are implemented as cdk-taverna workflows to be used intuitively by users without programming background. source code for the cdk-taverna  <dig> workers is freely available at https://sourceforge.net/projects/cdktaverna2/.

the scorer is also available as standalone java archive  package to be used as a library component in stand-alone or web applications. the standalone jar and the source code is freely available for download at http://sourceforge.net/projects/np-likeness/.

integration of np-likeness scorer components with cdk-taverna  <dig> 
cdk-taverna  <dig>  <cit>  has drag and drop components  to build cheminformatics workflows ranging from parsing a molecule file via fingerprinting and clustering to more advanced tasks such as reaction enumeration. the full features of the cdk-taverna  <dig>  plug-in, its installation procedure and example workflows are available at http://cdk-taverna- <dig> ts-concepts.de/wiki/. cdk-taverna  <dig> provides a set of workers commonly used in cheminformatics workflows. to provide additional functionality, individual components such as those required to score a small molecule for np-likeness are bundled as sub-packages within the existing cdk-taverna <dig> plug-in. the np-likeness sub-packages comprise workers for molecule curation, fragment generation and fragment scoring; all of which can readily be integrated into other data analysis workflows.

components for molecule curation
before being evaluated for np-likeness, molecules have to be pre-processed to remove small disconnected fragments like counter-ions and fragments containing metallic elements. in previous study  <cit>  commercial tools such as pipelinepilot and molinspiration  <cit>  were used to standardise molecules. these curation workers are now implemented in an open manner within the cdk-taverna  <dig>  plug-in and available under the folder “molecule curation”. to start with, worker checks for the disconnected parts in the molecule. if such are found, the user has an option of configuring the minimum atom-count for a fragment to be retained. as suggested by ertl et al. <cit> , the default minimum atom-count cut-off is set to  <dig> and so, unless modified, disconnected fragments with less than  <dig> atoms will be removed from the molecule. the worker filters molecules, removing those that contain elements other than c, h, n, o, p, s, f, cl, br, i, as, se or b. as another standardisation step, deglycosylation is needed to remove sugar moieties from the molecules. worker identifies all the sugar moieties in the structure and remove the ones that are linked by glycosidic bond to the scaffold. this is done in order to retain core structural features that are more typical of natural products and to omit features like sugar moieties that are less distinctive, albeit commonly present in natural products. removal of sugars is not expected to improve the score but to facilitate classifications based only on chemically interesting structural features.

an example workflow that makes use of all the curation workers is depicted in figure  <dig>  the workflow takes structure data format  file of molecules from the user as input. as soon as the molecules are read, they are assigned an universal unique identifier  before entering the curation step. the uuid tagging is done in order to keep track of molecule fragments generated upon curation. for example, when a sugar ring connecting two different scaffolds of a molecule is removed the molecule is split into two fragments. these fragments will have the same uuid of the parent molecule and will be tracked as single molecule in the scoring step.

component for atom signature generation
the molecule curation workers leave behind curated structures of molecule upon standardisation. down the workflow, they are consumed by another worker that generates its atom signatures  <cit> . atom signatures are structural descriptors – canonical, circular descriptions of an atom’s environment in a molecule. the atom signature of a given atom in a molecule is a directed acyclic graph of its connected atoms, where every node in the graph is an atom and the edges are the bonds between the atoms. the levels of neighbourhood of an atom in a molecule is the signature height of that atom. a molecular signature is the summation of all atom signatures of a molecule. the successful usage of molecular signatures is reported in various studies, ranging from qsar calculations to prediction of enzyme-metabolite and target-drug interactions  <cit> . in their original implementation, ertl et al <cit>  used hose codes, an earlier circular description of atom environments suggested by bremser  <cit>  for the use in nmr spectrum prediction. atom signatures and hose codes capture identical circular description of an atom environment but only differ in their string representation. since we had a well-tested, efficient implementation of signatures in the cdk, provided by torrance  <cit> , we decided to test whether it would give the expected identical results as the hose code-based implementation of the original work by ertl et al <cit> . the worker in the “signature scoring” folder generates atom signatures based on a given structure as input. the worker generates atom signatures of a molecule and tags them with the molecule’s uuid, to keep account of the signatures identity. the signature’s height  is configurable and we used atom signatures of height  <dig>  as it was sufficient in capturing relevant structural features in small molecules. the generated atom signatures for huge training datasets are usually written out to text file and stored for re-use. this feature is shown in figure  <dig> 

component for np-likeness score calculation
the worker in the “signature scoring” folder takes signatures of natural products, synthetic molecules and query compounds as input from text files. the workflow is depicted in figure  <dig>  within this worker, atom signatures of compounds from natural products and synthetic molecules datasets are indexed separately, in order to look up for the frequency of molecule fragments in question. the number of atom signatures generated for a molecule is equal to the number of atoms that make up the molecule. every atom signature independently represent a structural feature/fragment of the molecule, and an individual score for it is calculated using the statistic used in the original implementation. 

  fragmenti=lognpismi∗smtnpt 

in the above calculation of single fragment contribution fragmenti, npi is the total number of molecules in the natural products dataset in which the fragmenti occurs, smi is the total number of molecules in the synthetic molecules dataset in which the fragmenti occurs, smt is the total number of molecules in the synthetic molecules dataset and npt is the total number of molecules in the natural product dataset. individual fragment contributions from a molecule finally add up to give the total score of the molecule as shown in equation . the summed up score is then normalised by the number of the atoms in a molecule  as shown in equation , to give the final np-likeness score for a molecule. here, normalisation prevents molecules containing higher number of atoms from gaining higher score. 

  scoren=∑i=0nfragmenti 

  np−likenessscore=scorenn 

the calculated molecule scores are written out to a text file, tagged with their respective compound uuid. it is possible that non-linear discriminant analysis would work slightly better, but clear advantage of our approach is that it is chemically interpretable, it identifies fragments or substructures that play a role in natural product-likeness and this information may be used then directly in molecule design applications, for example for combinatorial library design or fragment growing. the nonlinear statistical methods provide mostly complex, not easy interpretable numerical solution. further, natural product-likeness is a concept and there is no established standard of value range to compare against. worker, also under the “signature scoring” folder, makes density plots of the scores and writes it out in portable document format . an example workflow making use of the scoring workers described above is shown in figure  <dig> 

RESULTS
the performance of the np-likeness score depends, of course, on the choice of natural products and synthetic molecules in the training dataset. for the analysis of our engine’s performance, natural products, synthetic molecules and query compound collections were all obtained from open access databases only. our first subset of natural products  originates from the chembl database  <cit> , where we selected molecules extracted from the journal of natural products. the second subset of natural products  comes from the traditional chinese medicine database @ taiwan  <cit> . together, the natural product training set comprised  <dig>  non-redundant structures. training set of synthetic molecules comprised  <dig>  clean lead-like compounds selected from the zinc database  <cit> . small molecules from drugbank  <cit>  and the human metabolome database   <cit>  were treated as our test sets. besides that, pubmed abstracts reporting isolation of new nps were text-mined for natural product’s name and the names were converted into smiles using chemical identifier resolver  <cit>  and the resultant set of  <dig> non-redundant nps was used as our test set.

the steps shown in figure  <dig> were repeated for both training and test sets to calculate their atom signatures. to score test sets for np-likeness, steps shown in figure  <dig> were followed. the overall scores obtained in our test study ranged from - <dig> to + <dig>  the more positive the score, the higher is the np-likeness and vice versa. the distribution of scores obtained for the compounds in the test set is shown in figure  <dig>  the distribution of the drugbank compound set overlaps both the synthetic molecule and natural product structural space. this is expected because, in drug design experiments, the drug-like compounds often end up mimicking structural features of metabolites after the optimisation process  <cit> . only one third of the natural products space captured by us overlaps with currently available common drugs. the text-mined natural products, as expected, almost completely overlaps the training natural products structural space occupying small additional structural space.

to validate our scoring system,  <dig> text-mined nps with additional  <dig> synthetics were scored using both our system and the original implementation by ertl et al <cit> . despite the much larger training set of the original system, the scores obtained showed a good correlation coefficient with r-value  <dig> . further, the scores obtained for the test set by replacing the training data in the original system with our open-data, showed very good correlation coefficient with r-value  <dig> . taking into account that two cheminformatics toolkits that have been used to calculate the values, differ slightly in handling of aromaticity, tautomerism, molecule normalisation etc and also slightly different types of substructure fragments, we consider this agreement very good and fully validating the new implementation of np-likeness.

CONCLUSIONS
we have presented an open-source, open-data implementation of a natural-product-likeness scorer originally described by ertl et al. workflows for curation, training and scoring are implemented in the open-source workflow tool cdk-taverna and published at myexperiment.org. a version of the scorer is available as an executable from command-line and as a library for inclusion in stand-alone or web applications. training and test sets where extracted from open access databases such as chembl, tcm, zinc, drugbank and hmdb. we replaced hose codes by faulon’s atom signatures as our circular fingerprint implementation which showed similar performance. with the available open-data and open-source tool-kits, we have implemented a np-likeness scorer engine and successfully demonstrated its capability to differentiate the natural product compound collection from synthetic and drug compound collections identical to what was reported in the original paper. the engine can be used as a filter to remove improbable metabolite structures from chemical spaces generated from computer assisted structure elucidation  or to select natural-product-like molecules from molecular libraries for the use as leads in drug discovery. the open-source, open-data implementation allows other researchers to modify the workflows or to use larger collections of training molecules once they become available.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
pe, with his colleagues from novartis, basel, conceived the original idea of natural product likeness score. pe also provided the text-mined natural product dataset. cs conceived the idea of implementing the scoring engine using open source and open data. at made new developments to the existing cdk-taverna plug-in. kj conducted the study, selected the data, implemented the curation, training and scoring engine and tested it. pm contributed to the development and discussion. all authors read and approved the final manuscript.

