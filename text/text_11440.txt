BACKGROUND
the role of auditory feedback in speech production is a topic of longstanding interest that has been investigated via a number of methods, most recently in studies using functional neuroimaging methods. previous studies using magnetoencephalography  have revealed a phenomenon called speaking-induced suppression : a reduced response in auditory cortex to self-produced speech, compared with its response to externally-produced speech. these studies examined the m <dig> response, also called the n100m response, which is the most significant peak in the magnetic response of cortex occurring approximately 100ms after the onset of an auditory stimulus <cit> , and found a dampened auditory m <dig> response to a person's own voice when speaking compared to conditions in which a person listens to recorded speech being played back to them  <cit> . researchers have also found that when self-generated voice sounds were different from the expected sounds, auditory cortex response was maximal, but if the output during speech production matched the expected sound, cortical activity in the auditory cortex was suppressed  <cit> . heinks-maldonado, nagarajan and houde  <cit>  proposed a precise forward model for speech production. they suggested that a forward model operates in the auditory system during speech production, which caused maximal suppression of the auditory cortical response to the incoming sounds that most closely match the speech sounds predicted by the model. researchers have argued that precise auditory suppression during speech allows the auditory system to distinguish between internally and externally produced speech sounds  <cit> .

a working model of auditory feedback processing for speech that accounts for the sis phenomenon has been proposed by j.f. houde and s.s. nagarajan  <cit> . this model is a version of the kalman filtering approach taken to model motor control in other domains  <cit> . a key part of this model is an internal model that represents the learned associations between vocal motor commands and their resulting acoustic sensory consequences, which we hypothesize is acquired when learning to speak . during speaking, actual  incoming auditory feedback is compared with a feedback prediction derived from efference copy of the motor output commands, creating a feedback prediction error. it is this comparison process that we hypothesize is a principal cause of the speaking-induced suppression phenomenon seen in our meg studies.

if houde and nagarajan's model of auditory feedback processing is correct, one would expect utterance rapidity and complexity to affect sis because temporal misalignment between actual feedback and the prediction only affects the prediction error for dynamic articulations. thus, the goal of this paper is to test the above two model predictions about differences in sis for static versus rapid, dynamic speech targets. we did this by comparing differences between auditory processes during speech production compared to auditory processes during passive listening across three different conditions with various speech targets . if the model is correct, we would expect a maximal difference in magnitude of the m <dig> response between speak and listen tasks in the simplest condition . however, with increasing rate and complexity of utterances , the speaking induced suppression should be reduced, and the difference in magnitude of the m <dig> response between speak and listen amplitudes should be smaller in the complex utterances.

RESULTS
we first examine the auditory evoked field  response to simple tones in all subjects. the aef m <dig> data revealed no hemispheric differences in the amplitude of the response from each hemisphere . however, there was a hemispheric difference for m <dig> latency, p = . <dig>  showing that the right hemisphere processed pure tones more quickly than the left hemisphere across all  <dig> participants.

second, we analyzed the acoustic output amplitude, i.e. volume through the earphones that subjects' heard, in both speak and listen tasks. this analysis revealed significant difference between conditions, p =  <dig> , but not across task. overall, participants produced /a/ more loudly than /a-a-a/ which they in turn produced more loudly than /a - a-a - a/. however, no differences were found in the acoustic amplitude that the subject heard during the speak and listen tasks in these conditions. responses to speech sounds during speaking and listening showed that m <dig> listen amplitudes decreased as the volume of the stimuli decreased .

an analysis of the sensor root mean square  m <dig> amplitude data during speech tasks revealed significant hemisphere and task differences. a repeated measures anova with condition, task and hemispheres as factors revealed significant differences for hemisphere, p = . <dig>  and task, p =  <dig> . in contrast, m <dig> latency data revealed no significant differences.

source space analysis using virtual sensors were used to analyze the m <dig> response arising from auditory cortex in each hemisphere in a  <dig> ×  <dig> ×  <dig> repeated measure anova. the virtual sensor m <dig> amplitude data revealed significant differences for hemisphere, p =  <dig> e- <dig>  task, p = . <dig> and a trend towards significance for the interaction between condition and task, p = . <dig>  to further investigate this interaction between task  and condition , we compared virtual m <dig> amplitude data from the simple speech target  versus dynamic speech targets . this anova revealed significant differences for hemisphere, p =  <dig> , task, p =  <dig> , and a significant interaction between task and condition, p = . <dig>  interestingly, the interaction between hemisphere and task did not reach significance, p = . <dig> suggesting that task effects are similar across the two hemispheres.

additional analyses of the virtual sensor amplitude data revealed differences in the responses between speak and listen tasks between conditions . to specifically examine these differences across conditions independent of task, the percent difference was calculated per condition: /amplitudelisten. results for the left hemisphere were as follows: in condition  <dig> , mean listen amplitude was  <dig>  na-m and mean speak amplitude was  <dig>  na-m, resulting in a percent difference between speak and listen amplitudes of 58%; in condition  <dig> , mean listen amplitude was  <dig>  na-m and mean speak amplitude was  <dig>  na-m, resulting in a percent difference of 50%; and in condition  <dig> , mean listen amplitude was  <dig>  na-m and mean speak amplitude was  <dig>  na-m, resulting in a percent difference of 35%. this analysis showed that the greatest difference between speak and listen amplitudes in the auditory cortex occurred in the simplest speaking condition. as the speech stimuli became more rapid and complex, the differences between speak and listen amplitudes decreased. a similar trend was also observed in the right hemisphere, although there was an overall dampened response to speech stimuli in the right hemisphere: in condition  <dig>  mean listen amplitude was  <dig>  na-m and mean speak amplitude was  <dig>  na-m, resulting in a percent difference between speak and listen amplitudes of 38%; in condition  <dig>  mean listen amplitude was  <dig>  na-m and mean speak amplitude was  <dig>  na-m, resulting in a percent difference of 15%; and in condition  <dig>  mean listen amplitude was  <dig>  na-m and mean speak amplitude was  <dig>  na-m, resulting in a percent difference of 5%. the difference between speak and listen amplitudes was reduced as the utterances became more rapid and complex. to specifically test whether any increase in dynamics of speech target would increase sis, we compared sis percent differences from the simple speech target  versus dynamic speech targets  across hemispheres. this two-way anova with condition and hemisphere as factors, revealed a significant difference for condition, p =  <dig> , but neither hemisphere, p =  <dig> , nor the interaction between hemisphere and condition, p =  <dig> , was significant. therefore, sis modulation for dynamic targets is bilateral, in spite of overall hemispheric differences in both speak and listen tasks.

no differences were observed in sensor rms or virtual sensor m <dig> response latencies in the speech tasks.

discussion
rapidity and complexity of the uttered syllable appears to modulate sis of the m <dig> amplitude. sis percent differences were largest with simple, static utterances in condition  <dig>  smaller with rapid utterances in condition  <dig>  and smallest with complex utterances in condition  <dig>  thus, the greatest difference between speak and listen m <dig> amplitudes was found in the static speech target , compared to the dynamic utterances . these findings are consistent with predictions from our model of speech feedback processing. the greatest speaking induced suppression was observed in condition  <dig> with the simple utterance presumably because the internal representation, or mental model, for that utterance was largely static and therefore easy to produce and match. however, with increasing rate and complexity of utterances , the auditory feedback predictions became more dynamic and more difficult to keep in temporal registry with the incoming auditory feedback, resulting in a poorer match with it, and, thus, a less suppressed response.

the differences in amplitude results across conditions are also in accord with houde and nagarajan's  model of speech feedback processing: one's expectation for a speech sound  is related to the activity observed in the auditory cortex. if a participant spoke /a/ loudly, that participant could predict the sound of that utterance and the auditory cortex will not be "surprised" by the volume of the utterance. in such a scenario, one would expect to observe attenuated activity, or reduced activity in the auditory cortex. if a participant spoke /a-a-a/ at a reduced volume, that participant could still predict the sound of that utterance and one would again expect to observe attenuated activity in the auditory cortex. however, during the listen task, participants could not predict the sound or volume of the auditory stimuli. therefore, the auditory cortex behaved correspondingly: larger amplitudes were observed with louder stimuli, and smaller amplitudes were observed with more quiet stimuli. this is directly related to the prediction/expectancy aspect of the model proposed by houde and nagarajan: if one's internal representation for a speech sound  matches the actual speech sound, then suppression or attenuation of cortical activity in the auditory cortex is observed. this process of matching one's internal representation of a speech sound to the actual speech sound is only possible in the speak task. on the other hand, all stimuli are unexpected during the listen task, and thus response in the auditory cortex should behave solely according to the properties of the auditory stimuli .

given that different auditory stimuli were used in this study, the hemispheric differences in observed responses are noteworthy. several studies have reported no hemispheric differences when tones were used  <cit> . this is consistent with our findings: significant activations were observed in primary auditory cortex  in both hemispheres when pure tones were used  <cit> . in contrast, previous literature suggests that the left hemisphere is dominant during speech and language perception  <cit> . therefore, a more dominant response was expected to occur in the left hemisphere, in other words a dampened response in the right hemisphere was expected, when speech stimuli were used. during both the speak and listen tasks, we observed this overall dampened m <dig> amplitude response in the right hemisphere. however, the effect of condition on sis is the same across both hemispheres. thus, in spite of the overall hemispheric differences in response to speech, the processing of auditory feedback during speaking may be similar across the two hemispheres.

CONCLUSIONS
these findings provide additional support for our conceptual model of speech motor control, and as such provide the impetus to test other predictions from the model. in addition, these findings also provide better insights into the speech motor control system, and the computational role of auditory cortex in transforming auditory feedback. the sis paradigm used in this study may benefit the study of disorders such as schizophrenia, in which patients lack the ability to distinguish between internally and externally produced speech sounds  <cit> . it may also benefit the study of speech production impediments such as stuttering  <cit> , where altered auditory feedback has been shown to be fluency-enhancing.

