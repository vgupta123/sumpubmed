BACKGROUND
tumor classification is performed on microarray data collected by dna microarray experiments from tissue and cell samples  <cit> . the wealth of such data for different stages of the cell cycle aids in the exploration of gene interactions and in the discovery of gene functions. moreover, genome-wide expression data from tumor tissues gives insight into the variation of gene expression across tumor types, thus providing clues for tumor classification of individual samples. the output of a microarray experiment is summarized as an p Ã— n data matrix, where p is the number of tissue or cell samples and n is the number of genes. here n is always much larger than p, which degrades the generalization performance of most classification methods. to overcome this problem, feature selection methods are applied to reduce the dimensionality from n to k with k <<n.

feature selection chooses a subset of the original features  according to the classification performance; the optimal subset should contain relevant but non-redundant features. feature selection can help to improve the generalization performance of classifiers, and to reduce learning time and the time required to classify out-of-sample data. there has been a great deal of work in machine learning and related areas to address this issue  <cit> . in most practical cases, relevant features are selected and kept as input, while irrelevant and redundant features are removed.

although the removed features are redundant and weakly relevant, they contain useful information that can be used to improve prediction accuracy. multi-task learning  is a method of using the redundant information by selecting features from the discarded feature set to add to the target  <cit> . although mtl achieves only limited improvement, it is nevertheless useful for real world cases like medical problems  <cit>  and multivariate calibration problems  <cit> .

previous studies of search methods for multi-task learning mainly used heuristic methods  <cit> , where the number of features selected for the input and/or target is somewhat arbitrary. when the search method is regarded as a combinational optimization problem, random search methods can be used. the genetic algorithm  <cit>  is a simple and powerful method which has obtained satisfactory results for feature selection  <cit> . motivated by this, we proposed the random method ga-mtl   <cit> , but ga-mtl did not consider irrelevant features in the data sets. here we propose an enhanced version of ga-mtl  which codes one feature with two binary bits. the e-ga-mtl algorithm and others are applied to tumor classification on microarray data sets; it is found that e-ga-mtl outperforms all other algorithms considered.

RESULTS
in order to demonstrate the benefits of multi-task learning methods, we have performed the following series of experiments using artificial neural networks  as classifiers.

 <dig>  all is a baseline method; without any selection, all the genes are input to the ann for classification.

 <dig>  ga-fs uses a genetic algorithm to select genes and input selected genes to the ann.

 <dig>  h-mtl uses a heuristic embedded feature selection method to search features, where some of the selected features serve as input to the ann and some of the features are added to the output.

 <dig>  ga-mtl uses a genetic algorithm to search features, where some of the selected features are input into ann and some of the features are added to the output.

 <dig>  ga-mtl-ir uses an embedded algorithm to remove irrelevant features and then uses a genetic algorithm to search features, where some of the selected features serve as input to the ann and some of the features are added to the output.

 <dig>  e-ga-mtl also uses a genetic algorithm to search features, and employs two bits to represent one feature; some features are considered as irrelevant and discarded, some of the selected features serve as input to the ann, and some of the features are added to the output.

the most important parameter of an ann is the number of nodes in hidden layer, m. to reduce the effect of this parameter we ran the experiments with both m =  <dig> and m =  <dig> 

while different data sets, including data sets with only the selected features, need different optimal parameters for different methods, we do not try to find the optimal parameters, because:

 it is infeasible to find the optimal parameters, because this is an np-hard problem.

 we are not interested in obtaining the best performance of one special method on a given data set; instead, we are interested in demonstrating the effect of our proposed framework.

prediction performance
the average bacc values are shown in figures  <dig> and  <dig> for different values of the ann parameters, where all means all the genes are used as input for classification without any gene selection. from figures  <dig> and  <dig>  we conclude that:

 on average and for all the data sets, the multi-task learning algorithms h-mtl, ga-mtl, ga-mtl-ir, and e-ga-mtl perform better than the feature selection algorithms ga-fs and all.

 on average and for almost all the data sets, the genetic algorithm based multi-task learning algorithms ga-mtl, ga-mtl-ir and e-ga-mtl perform better than h-mtl, a heuristic algorithm. only on the leukemia data set, for an ann with m =  <dig> hidden units, does h-mtl perform slightly better than ga-mtl and ga-mtl-ir.

 on average, e-ga-mtl performs the best among all the learning algorithms.

 although ga-fs performs worse than the multi-task learning algorithms, it performs better than those without any gene selection.

detailed statistical values of bacc, correction, sensitivity, specificity, and precision are also listed in tables  <dig> , <dig>  and  <dig>  from which we conclude that:

 although the results for anns with m =  <dig> are better than those for m =  <dig>  we can draw similar conclusions for both series of results in terms of how the different methods compare.

 for all the measures, on average, multi-task learning algorithms including h-mtl, ga-mtl, ga-mtl-ir, and e-ga-mtl perform better than ga-fs and all, and genetic algorithm based multi-task learning algorithms like ga-mtl, ga-mtl-ir, and e-ga-mtl perform better than h-mtl.

 both e-ga-mtl and ga-mtl-ir remove irrelevant genes; both obtain better results than the others for the specificity, precision and bacc measures, on average. but ga-mtl-ir performs worse than ga-mtl for other measures like sensitivity and correction.

 e-ga-mtl performs the best among all the learning algorithms on average, for all the measures. it greatly improves results for the bacc, sensitivity and specificity measures.

the number of selected features
we show the number of features selected by each algorithm in tables  <dig> and  <dig>  which also lists the number of discarded features, input features, and target features. for ga-fs, the features are selected as input or are discarded. for ga-mtl, the features are selected as input or are added to the target; no features are discarded. for h-mtl, ga-mtl-ir, and e-ga-mtl, the features are selected as input, are added to the target, or are discarded.

from tables  <dig> and  <dig>  we can see that:

 for ga-fs, about one third of genes are removed and two thirds are used for classification. furthermore, the ratio of the number of input features to the number of output features for ga-mtl is similar to the ratio of the number of input features to the number of discarded features for ga-fs.

 h-mtl and ga-mtl-ir both use the same prediction risk criterion to discard irrelevant features, so the features discarded are the same and hence the number of discarded features are the same. the number of input features and output features are different, however. h-mtl has a predetermined number of input and output features; one quarter of the selected features are used for the input, and the other three quarters are added to the output. in contrast, for ga-mtl-ir, the features are determined by a genetic algorithm, but the ratio of the number of input features to the number of output features is similar to that of h-mtl.

 for e-ga-mtl, although the number of input, output and discarded features are determined automatically by the genetic algorithm, the ratios among these numbers are similar to those for h-mtl and ga-mtl-ir.

discussions
we have demonstrated that genetic algorithm based multi-task learning  methods perform better than the heuristic methods and feature selection methods, and that e-ga-mtl performs the best of all the methods considered. several questions come immediately to mind:

why does multi-task learning succeed?
in a previous study, caruana et al. gave an explanation  <cit>  of why multi-task learning succeeds. here we combine their results with the framework presented here. yu and liu  <cit>  proposed to categorize the features into four classes, namely:

i: irrelevant features,

ii: weakly relevant and redundant features,

iii: weakly relevant but non-redundant features, and iv: strongly relevant features;

where iii and iv comprise the optimal feature subset and i and ii should be removed using feature selection methods. we have found that ii contains useful information. these features should not be discarded, but rather should be used in the learning process. multi-task learning is a method to use these redundant features to improve the prediction accuracy of the base learning method, which accounts for its improved performance.

why do genetic algorithms perform better than the heuristic method?
our results demonstrate that genetic algorithm based multi-task learning methods outperform heuristic multi-task learning methods. the chief reason why this is so is that the heuristic method considered here uses the feature ranking technique to select features for the input and the target, which does not consider feature redundancy and/or feature interaction. at the same time, is somewhat arbitrary to use a prespecified number of features for the input and the target. this is another factor which reduces the performance of the heuristic method. in contrast, when the genetic algorithm selects features for the input and the target, it simultaneously considers feature redundancy and/or feature interaction. so it automatically determines the number of features for the input and target. in fact, kudo and sklansky proved that genetic algorithms have a higher probability of finding better solutions to naive feature selection problems than other complete, heuristic and random algorithms  <cit> . among the genetic algorithm based multi-task learning methods, e-ga-mtl performs better than ga-mtl-ir. the number of features removed by e-ga-mtl is determined automatically by the genetic algorithm, while the number removed by ga-mtl-ir is prespecified. this is further evidence that genetic algorithm based approaches outperform heuristic approaches.

what effect do irrelevant features have on multi-task learning?
the effect of multi-task learning on irrelevant features can be observed by comparing the results obtained by e-ga-mtl, ga-mtl-ir, and ga-mtl; e-ga-mtl and ga-mtl-ir remove irrelevant features, while ga-mtl does not. here we observed that e-ga-mtl and ga-mtl-ir outperformed ga-mtl, especially for the sensitivity and bacc measures. this shows that irrelevant features will degrade the generalization performance of multi-task learning methods, and reduce the robustness of the methods; they should therefore be removed before the learning process.

CONCLUSIONS
random search methods of multi-task learning , including ga-mtl , ga-mtl-ir  and e-ga-mtl  are shown to improve the accuracy of multi-task learning and to make multi-task learning more convenient to use. experimental results on microarray data sets for tumor classification showed that genetic algorithm based multi-task learning performed better than h-mtl, a heuristic multi-task learning method, and ga-fs, a naive feature selection method based on genetic algorithms. furthermore, our results showed that e-ga-mtl and ga-mtl-ir, which remove irrelevant features, performed better than ga-mtl, which does not. e-ga-mtl, which employs a genetic algorithm with a two bit encoding to remove irrelevant features and select features for the input and output, performed best. since analysis of microarray data sets is a high dimensional problem, our results demonstrate that multi-task learning techniques can be employed to improve prediction performance of tumor classification by using redundant genes. furthermore, our results demonstrate that genetic algorithms can be employed to improve multi-task learning by discarding irrelevant features and by selecting the input and target features automatically; ga-mtl and e-ga-mtl are shown to to improve generalization performance of classifiers on microarray and other high-dimensional data sets.

