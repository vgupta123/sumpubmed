BACKGROUND
integrating biomedical datasets from different data streams  and of different types  is of utmost importance and has become an analysis bottleneck in biomedical research. ideally, one would like to be able to uncover all direct associations between variables and/or perform feature selection and classification tasks using all data. the first task can reveal disease mechanisms and the second can be used to select variables characteristic of disease status, therapy outcome or any other variable of clinical importance. graphical models have been used in the past for both of these tasks, but they are often limited to datasets with discrete-only or continuous-only variables. traditional univariate approaches for feature selection exist as well, but they also often operate on a single data type. in addition, due to the high dimensionality and co-linearity of biological data, markers selected by these standard feature selection algorithms can be unstable and lack biological relevance  <cit> , a problem that has recently been addressed directly  <cit> . many existing models that do integrate different data types make heavy use of prior knowledge  <cit>  and as such are not easily extendable to clinical and other data that are not well studied. as a result, although numerous biomedical data sets exist with genomic, transcriptomic and epigenetic data for each sample, a general framework for integrative analysis of these heterogeneous data is lacking.

in this paper, we study several strategies for learning the structure of graphical models over mixed data types  to produce statistically and biologically meaningful predictive models. we measure the performance of these strategies in synthetic data  and biological data .

the major contributions of this paper are threefold. first, we apply an mgm, proposed by lee and hastie  <cit> , to simulated and biological datasets. these datasets have higher dimensionality and are derived from more complicated network structures than datasets used in previous work with this model. second, we propose the use of a separate sparsity penalty for each edge type in the mgm, which significantly improves performance. third, to assist with setting the sparsity parameters we use a heuristic search based on an existing model selection method   <cit> , that outperforms standard methods.

prior work
graphical models are a natural tool for decoding the complex structure of heterogeneous data and allow for integration of many data types. they learn a network of statistical dependencies subject to a joint probability distribution over the data. mixed graphical models  are graphical models learned over a mixture of continuous and discrete features.

a fully specified conditional gaussian mgm, as characterized by lauritzen & wermuth  <cit>  26 years ago, would require different continuous distribution parameters for every possible setting of the discrete variables. restricting ourselves to “homogeneous” models, which use a common covariance matrix for continuous variables independent of the discrete variable values, is therefore necessary to avoid trying to learn a parameter space that is exponential in the number of variables. similar to pairwise markov random fields over only discrete variables, the main hurdle to the calculation of likelihood in mgms is calculation of the partition function. this computation is intractable with a large number of discrete variables because it requires summing over all possible discrete variable settings. two approaches to get around this partition function calculation are:  learn separate regressions of each variable given all of the others , and  maximize a tractable pseudolikelihood function instead of the actual likelihood  <cit> .

performing separate regressions is a common approach to the mgm learning problem. this class of methods learns a conditional distribution for each node given the rest. examples of this strategy include estimation of the sparse inverse covariance matrix of a multivariate gaussian by meinshausen and bühlmann  <cit> , and estimation of mixed variable networks via random forests  <cit>  or exponential families  <cit> . alternatively, the pseudolikelihood, proposed by brasg  <cit> , is a consistent estimator of the likelihood, and is defined as the product of the conditional distributions of each node given the rest. both of these approaches thus avoid calculation of the partition function for the joint distribution by substituting the conditional distributions of each node into the optimization problem. separate regressions offer flexibility and are easily parallelized, but in both the continuous  <cit>  and mixed cases  <cit>  estimating the parameters by maximizing the likelihood or pseudolikelihood, respectively, has the advantage of better empirical performance. because of this we chose the focus our efforts on the mgm learning approach via pseudolikelihood, as proposed in lee and hastie  <cit> .

although lee and hastie do not test their algorithm on high-dimensional data, we find that their model is well suited for high-dimensional learning due to their inclusion of a sparsity penalty on the parameters. an important issue that we ran into in our experiments was that the model would often select too many continuous-continuous edges and too few edges involving discrete variables. this is likely a combination of the phenomenon observed in  <cit>  where linear regressions have better edge prediction performance than logistic regression between the same nodes and the fact that lee and hastie use the same sparsity penalty on all edges regardless of the type of nodes they connect. lee and hastie use a weighting scheme to take into account discrete variables with differing numbers of categories, but this does not solve this problem. therefore, in this paper we introduce a new regularization method for the lee and hastie’s model that uses a different penalty for each type of edge: continuous-continuous, continuous-discrete, and discrete-discrete. in addition, because this approach creates more parameters for the user to set, we present an edge stability based method for selecting the three sparsity parameters. we call the combination of using separate sparsity penalties with our heuristic search stable edge-specific penalty selection .

methods
mixed graphical models
lee and hastie  <cit>  parameterize a mixed graphical model over p gaussian variables, x, and q categorical variables, y, as a pairwise markov random field. here we briefly summarize their model: px,y,Θ∝exp∑s=1p∑t=1p−12βstxsxt+∑s=1pαsxs+∑s=1p∑j=1qρsjyjxs+∑j=1q∑r=1qϕrjyryj 

in this model βst represents the interaction between two continuous variables, xs and xt,ρsj is a vector of parameters that correspond to the interaction between the continuous variable xs and the categorical variable yj indexed by the levels  of the variable yj, and ϕrj, is a matrix of parameters indexed by the levels of the categorical variables yj, and yr. in the continuous only case, this model reduces to a multivariate gaussian model where the βst parameters are entries in the precision matrix. in the categorical only case, this model is the popular pairwise markov random field with potentials given ϕrj; and it could parameterize an ising model as in the binary-only case, for example,. thus the model serves as a generalization of two popular uni-modal models to the multi-modal regime.

in order to avoid the computational expense of calculating the partition function of this model, lee and hastie optimize the negative log pseudolikelihood, which is: l˜Θ|x,y=−∑s=1plogpxs|x\s,y;Θ−∑r=1qlogpyr|x,y\r;Θ 

to ensure a sparse model, l˜ is minimized with respect to a sparsity penalty, λ: minimizeΘl˜Θ+λ∑t<sβst+∑s,jρsj2+∑r<jϕrjf where Θ is a shorthand for all of the model parameters. the parameter matrices β and ϕ are symmetric, so only half of each matrix is penalized. lee and hastie use an accelerated proximal gradient method to solve this optimization problem.

a standard way of handling a categorical variable with l levels is to convert the variable to l- <dig> indicator variables where the last level is encoded by setting all indicators to zero, this is necessary to ensure the linear independence of variables in the regression problem. lee and hastie’s mgm approach, uses l indicator variables  and ϕrj) to improve interpretability of the model, and enforces a group penalty to ensure the indicator coefficients sum to zero.

to perform our experiments we adapted the matlab code provided by lee and hastie .

separate sparsity penalties
our main modification to the lee and hastie model itself is that we use different sparsity penalties for the three edge types: edges connecting two continuous nodes , edges connecting a continuous and discrete node  and edges connecting two discrete nodes . with these penalties, the new optimization problem becomes: minimizeΘl˜Θ+λcc∑t<sβst+λcd∑s,jρsj2+λdd∑r<jϕrjf 

methods for model selection
k-fold cross-validation   <cit>  splits the data into k subsets and holds each set out once for validation while training on the rest. we use k =  <dig> and average the negative log-pseudolikelihood of the test sets given the trained models. the akaike information criterion   <cit>  and bayes information criterion   <cit>  are model selection methods that optimize the likelihood of a model based on a penalty on the size of the model represented by degrees of freedom. to calculate the aic and bic, we substitute the pseudolikelihood for the likelihood and we define the degrees of freedom of the learned network as follows.

in the standard lasso problem, the degrees of freedom is simply the number of non-zero regression coefficients  <cit> . so, in the continuous case, the degrees of freedom of a graphical lasso model is the number of edges in the learned network. in the mixed case, edges incident to discrete variables have additional coefficients corresponding to each level of the variable. lee and hastie’s mgm uses group penalties on the edge vectors, ρ, and matrices, ϕ, to ensure that all dimensions sum to zero. so, in the model, an edge between two continuous variables adds one degree of freedom, and edge between a continuous variable and a categorical variable with l levels adds l- <dig> degrees of freedom, and an edge between two discrete variables with li and lj levels adds  degrees of freedom.

we compare these model selection methods to an oracle selection method. for the oracle model, we select the sparsity parameters that minimize the number of false positives and false negatives between the estimated graph and the true graph. while we do not know the true graph in practice and none of the other methods use the true graph, this method shows us the best possible model selection performance under our experimental conditions.

aic, bic, and cv all require calculating the pseudolikelihood from a learned model so to optimize over separate sparsity penalties for each edge type, we perform a cubic grid search of λcc, λcd, and λdd over {. <dig>  . <dig>  . <dig>  . <dig>  .04}.

stability for model selection
here we briefly present the stars procedure  <cit>  reformulated in terms of λ rather than Λ = 1/λ as was originally described. given a dataset with n samples, stars draws n subsamples of size b without replacement from the set of nb possible subsamples. an mgm network is learned for each subsample over a user specified set of values and a single sparsity parameter, λ. the adjacency matrices from these learned models are used to calculate, θ^stλ, the fraction of subsample networks that predict an edge from node s to node t. using this value we can then calculate edge instability, ξ^stλ=2θ^stλ1−θ^stλ, which is the empirical probability of any two subsample graphs disagreeing on each possible edge at each value of λ. liu et al. define total instability of the graph, d^λ, as the average of ξ^stλ over all edges: d^λ=∑s<tξ^stλp+q <dig>  very low values of λ will result in very dense but stable graph, which is not desirable. to avoid this, stars monotonizes the instability: d¯λ=supλ≤td^t and selects λ^=infλ:d¯λ≤γ with γ being a user defined threshold . in other words, starting with a large value of λ that produces an empty graph, we reduce λ until the total instability hits the given threshold.

stable edge-specific penalty selection 
we modified the stars procedure to accommodate selection of separate λ for each edge type. we now define the total instability over each edge type instead of the entire graph: d^ccλ=∑ccξ^stλp <dig>  d^cdλ=∑cdξ^stλpq, d^ddλ=∑ddξ^stλq <dig>  given these separate estimates of total instability, we then perform the rest of the stars algorithm for each λ. this approach does not require any additional model learning, the only extra computations in this approach compared to the standard, single penalty stars are the additional averages, which are trivial to calculate. because the subsample network learning uses the single penalty mgm, this procedure is linear in the size of the parameter search space. based on the suggestions in  <cit> , and the default parameters in the r implementation of stars  <cit> , we use n= <dig> b=10n, and γ = . <dig> 

simulated network data
we generated  <dig> scale-free networks of  <dig> variables each, based on the framework of bollobás et al.  <cit>  but ignoring edge direction. so, given a number of nodes to connect, we start with an edge between two nodes and the rest of the nodes unconnected, we iteratively add edges until all nodes are connected. at each edge addition, we connect two non-zero degree nodes with probability .3; and we connect a node i with degree  <dig> to a node j with non-zero degree with probability  <dig> . in each case, the non-zero degree nodes are selected randomly with probability proportional to their degree: degreej∑k∈vdegreek.

for each network we simulated two datasets of  <dig> samples with  <dig> continuous and  <dig> categorical variables. each categorical variable had  <dig> levels. the parameters in one dataset were set so that discrete-continuous and discrete-discrete edges had approximately linear interactions, while the other dataset did not have this constraint. each edge, from node s to node t is given a weight, wst, drawn uniformly from . for continuous-continuous edges we chose a sign with even probability and set βst = wst or βst = − wst. to ensure the β matrix is positive definite, we set the diagonal elements the largest value of the sum of the absolute value of the edge weights over each node. for continuous-discrete edges, in the linear dataset we set ρst =  and in the non-linear data we set ρst = perm, where perm is a random permutation of the elements in the vector. for discrete-discrete edges we set the diagonal of ϕrj to wst and the rest to -wst, while in the non-linear data we randomly set one parameter in each column and row to wst and the rest to -wst..

lung chronic disease data
the lung genomics research consortium  contains multiple genomic datasets and clinical variables for two chronic lung diseases: chronic obstructive pulmonary disease  and interstitial lung disease . we used two data types from lgrc: gene expression profiles  and clinical data for  <dig> patients . to expedite the execution time and avoid sample size problems, we only used the  <dig> most variant expression probes and  <dig> clinical variables: age, height, weight, forced expiratory volume in one second , forced vital capacity , gender, cigarette history, and diagnosis . age, height, weight and the spirometry variables  were divided into tertiles. diagnosis was used for classification experiments.

graph estimation performance
non-zero mgm edge parameters correspond to a prediction of the presence of that edge. for edges with multiple parameters,  and ϕrj) if any of the parameters are non-zero we predict the edge is present. we use accuracy, precision and recall to evaluate edge recovery in our predicted graphs: precision is the ratio of true edge predictions to all edge predictions; recall is the ratio of true edge predictions to all edges in the true graph; accuracy is the ratio of true predictions to all predictions ; and the f <dig> score is the harmonic mean of precision and recall. in addition we consider the matthews’ correlation coefficient   <cit>  which provides a correlation between the presence of edges in the true and predicted graphs. mcc is formulation of pearson’s correlation for two binary variables so values of  <dig> correspond to perfect agreement between the variables, − <dig> to all disagreements, and  <dig> to random guessing. this measure is robust to unbalanced nature of the problem where in the true, sparse graph edge absence is much more frequent than edge presence.

functional enrichment and classification
for evaluation of the performance of various mgms and other models on real data we used functional enrichment analysis of external databases and classification analysis over specific variables in the network, including disease diagnosis .

gene annotations were retrieved from the gene ontology  database  <cit>  and we used the hypergeometric test to determine if sets of selected genes were overrepresented for any of these annotations .

given the parameters learned from training data, Θ^train, we make predictions on any categorical variable, ytarget, in a testing dataset given the rest of the variables by selecting the category minimizes the negative log pseudolikelihood of the test data given the trained model: y^target=argminltargetl˜Θ^train;xtest,ytest\target,ytarget=ltarget 

we use this approach to predict lung disease diagnosis in a test dataset with an mgm trained with a training dataset.

we used 8-fold cross validation to determine the optimal classification settings of λ for mgm and lasso, and which kernel to use for support vector machines . we used the built-in matlab implementations of lasso and svms for these experiments.

RESULTS
synthetic data
separate sparsities versus single sparsity parameter
we applied lee and hastie’s method for learning an mgm to datasets simulated from a scale-free network. initial experiments found that using a single sparsity penalty for all edge types produced many false positive continuous-continuous edge predictions, while missing many true discrete-discrete edges. we first present an example of this behavior on a single dataset of  <dig> samples over  <dig> four-level discrete variables and  <dig> continuous variables generated from a scale free network structure. figure 1a shows the adjacency predictions of the learned mgm compared to the true graph using a λ selected by the oracle to minimize the number of edges present in one graph but not the other. this observation leads us to introduce separate sparsity penalties for each edge type. figure 1b shows the adjacencies learned by an mgm with separate sparsity penalties for each edge type. for the sparsity parameters, the oracle searched over a range of  <dig> values evenly spaced on a log scale from . <dig> to . <dig> fig.  <dig> example adjacency matrices predicted by an mgm, with sparsity selected using the oracle. a single sparsity penalty λ = . <dig> b split sparsity penalties λ
cc = . <dig>  λ
cd = . <dig>  λ
dd = .13



figure  <dig> shows the matthews correlation of the edge predictions over the range of sparsity parameters, both overall and separated by edge type. for this example dataset, edge recovery of discrete-discrete edges had the highest mcc at λ = . <dig> while correlation of recovery of continuous-discrete edges was maximized at λ = . <dig> and continuous-continuous edges at λ = . <dig> fig.  <dig> matthews correlation between edge predictions and the true graph versus sparsity for the dataset from fig.  <dig>  calculated for each edge type, cc for continuous-continuous, cd for continuous-discrete, dd for discrete-discrete, and over all edge predictions



selecting an optimal value for a single λ can be challenging, and the addition of two more sparsity parameters made it necessary to develop an efficient selection strategy. other methods with multiple sparsity parameters search over a grid of models learned on all possible combinations of the parameters  <cit> , but for our model the complexity of this selection would be cubic in the number of parameter values tested. many model selection methods rely on calculating some likelihood over the training data, and it is not clear how to divide up this calculation by edge type. we do expect the presence of edges to remain relatively constant for a given edge sparsity parameter setting, so we extended a recent subsampling technique for model selection, stars  <cit> , to select three edge-type specific sparsity penalties by assuming independence between edge types. this assumption allows for a linear rather than cubic search over possible sparsity parameters. thus, our method, steps, selects three sparsity penalties for lee and hastie’s mgm learning using a modified stars approach for subsampling over different edge types.

steps outperforms other methods for model selection
table  <dig> summarizes graph prediction results for mgms trained using sparsity penalties chosen with different model selection procedures over the  <dig> simulated non-linear datasets. oracle, aic, bic, and cv evaluated models over a three dimensional grid of all possible combinations of λcc, λcd, λdd ∈ {. <dig>  . <dig>  . <dig>  . <dig>  .04}. for stars, models were trained using a single sparsity penalty over the same range of values, and then either a single λ was selected based on the average instability over all edges or λcc, λcd and λdd were selected based on the average instability of each edge type.table  <dig> comparison of model selection methods

stars –  <dig> λ
steps –  <dig> λ

aic akaike information criterion, bic bayesian information criterion, cv cross-validation, oracle: best possible prediction performance 

mean  of classification performance over  <dig> datasets simulated from scale-free networks



our results show that aic, bic and cv produce overly dense models in the high-dimensional setting. even when restricted to the single sparsity model, stars significantly outperforms these traditional model selection methods. these results agree with what liu et al. observed in their model selection experiments with the graphical lasso  <cit> . in addition, our modification of stars with separate sparsities outperforms stars with a single sparsity. neither stars model selection with  <dig> penalties nor the oracle model selection output a model where all three sparsities were equal in any of these experiments. both methods always set λdd = . <dig> while the other parameters were always in the set { . <dig>  . <dig>  .16}. these results confirm the effectiveness of separating the mgm sparsity penalty into three λ values.

the original stars procedure uses a subsampled dataset to make final edge predictions because the instability calculations are made on subsamples. we found, however, that in all cases the final edge prediction performance is higher if we use all samples compared to predictions from a model using a subsampled dataset. this improved performance is observed for all three metrics: accuracy, mcc, and f <dig>  so, for all results presented below we used all samples to learn the mgm and make edge predictions with stars selected sparsities.

it is important to note that because our method, steps, selects each sparsity parameter independently, so it incorrectly assumes that the instability of each edge type is independent of parameters of the rest. without this assumption, we would have to perform stability experiments on all combinations of the sparsity parameters. to test if this assumption is reducing the edge recovery performance of steps, we ran stars on the non-linear datasets using all  <dig> possible settings of λcc, λcd, λdd ∈ {. <dig>  . <dig>  . <dig>  . <dig>  .16}. this search space was chosen because all of the values selected by the oracle or either of the other stars methods fell in the set {. <dig> . <dig> .16} and additional intermediate values were needed to compare the relative performance of these methods. this experiment posed a new problem of how to monotonize and select the total instability over three dimensions rather than one. in addition, this experiment showed that the number of predicted edges in the graph does not always increase when one of the λ parameters decreases, even when the other two are held constant. we found that simply choosing the model with monotonized total instability closest to the user-specified γ threshold produced poor results. taking into account the number of edges predicted across all subsamples for each parameter setting, as described below, was essential to producing usable results.

we fist looked at the total instability of the whole graph with all edge types pooled together, d^allλccλcdλdd. we monotonized this 3-dimensional matrix across each dimension: d¯allλccλcdλdd=supλcc,λcd,λdd≤t <dig> t <dig> t3d^t <dig> t <dig> t <dig> and selected the setting of λcc, λcd, λdd that produced subsampled networks with the most edges such that d¯allλccλcdλdd≤γ=. <dig>  surprisingly, this approach performed worse than steps on all measures. mcc, for example, was significantly worse . we found that the networks produced by this method were too dense in the continuous-continuous edges and too sparse in continuous-discrete edges . this is the result of averaging the instability of all edge types: the selected models were too stable for some edge types and too unstable for other types. to fix this, we separated the instability as before into d^ccλccλcdλdd, d^cdλccλcdλdd and d^ddλccλcdλdd, and monotonized as before. then we choose λcc, λcd, λdd that produced networks with the most edges such that maxd¯ccλccλcdλdd,d¯cdλccλcdλdd,d¯ddλccλcdλdd≤γ=. <dig>  on  <dig> of the  <dig> datasets tested, this approach selected the same sparsity parameters as our proposed linear parameter search method. for the three runs where the two methods selected different parameters, the cubic search made better choices than the heuristic. averaging over all runs the cubic search performed better than the heuristic but these results are not significant . these results indicate that the independence assumption made by our heuristic is reasonable and that steps performs only slightly worse than a more theoretically sound cubic search while requiring much less computation.

comparison to scggm
an important potential application of mgms is in identifying expression quantitative trait loci  based on the predicted dependencies between single nucleotide polymorphisms  and mrna expression. the sparse conditional gaussian graphical model   <cit>  is a method that addresses this problem specifically. like many methods for finding eqtls, the scggm assumes a linear relationship between the number of variant alleles and the mrna expression level. thus, the scggm is not technically a mixed graphical model because it treats the snp allele counts as continuous variables. another difference is that scggm does not predict discrete-discrete edges, which is also common among methods for finding eqtls. like steps, scggm also adopts a strategy of using a separate sparsity penalty for each edge type. scggm uses cross-validation to search over a two dimensional grid of parameter values in order to optimize prediction of continuous values given the discrete values.

first, we examined how our stability method can be used in scggm parameter selection instead of cross validation on our synthetic data and we found that steps resulted in significantly higher mcc  for recovery of both continuous-continuous and continuous-discrete edge types. to perform a comparison between mgm and scggm edge predictions we used two sets of  <dig> mixed datasets generated from the same set of  <dig> scale-free networks but with parameters that resulted in either linear or non-linear interactions between discrete and continuous variables. figure  <dig> shows the results of this experiment with steps selected sparsity parameters. as expected, mgm learning performed similarly on the linear and non-linear datasets because it does not assume linearity. the scggm had similar performance on continuous-continuous edge recovery with both datasets, but significantly worse performance on continuous-discrete edge recovery in the data with non-linear cd interactions, which resulted in worse overall performance in that setting.fig.  <dig> comparison of edge recovery performance of mgm and scggm on continuous-continuous , continuous-discrete  and both edge types. matthews correlation is averaged over  <dig> simulated datasets with linear continuous-discrete interactions and  <dig> datasets with non-linear interactions with error bars ± one standard error. sparsity parameters for both methods selected by steps



for these tests we found that when allowing the selection of  edge type specific sparsity penalties, scggm chose the same penalty for the cc and cd edges in  <dig> out of the  <dig> datasets; and steps chose the same penalty for the cc and cd edges in  <dig> out of the  <dig> datasets, but a different dd penalty in all  <dig> cases.

performance of mgm on lung disease data
it is difficult to evaluate the edge recovery performance of mgm in real clinical datasets since the ground truth  is not generally known. alternatively, we evaluate mgm performance indirectly, by  recovering the small number of interactions that are known,  using external datasets  to see if connected genes have similar function,  performing classification on a target variable in the network .

we applied our mgm learning approach to the lgrc biomedical data . on this data steps selected the same value of λcc, λcd = . <dig> for an average instability threshold of γ = . <dig> and λcc, λcd = . <dig> for γ = . <dig>  the selection of λdd proved more problematic. even with γ = . <dig>  λdd was selected to be so high that only one edge was selected . this issue is likely caused by the fact that there are only  <dig> possible edges between the  <dig> clinical variables, and we expect that many of these variables are connected. because of this and the fact that the experiments we perform below depend more on the continuous-discrete edges, we set all three penalties to the same value for our parameter searches in this section.

recovering known interactions
figure  <dig> shows part of the network learned over the lung  dataset with λcc, λcd, λdd = . <dig>  we only show the nodes adjacent to the clinical variables most relevant for lung disease: diagnosis, spirometry tests and cigarette smoking are shown in this graph. this model found a very strong connection between the fev and fvc variables. a number of relevant gene expression variables are linked to diagnosis in this network. il <dig> is part of the family of interleukin signaling molecules, which are associated with inflammatory response to tissue damage, and copd is an inflammatory disease. we also see a link between diagnosis and mmp <dig>  a previously discovered biomarker for idiopathic pulmonary fibrosis which is categorized as ild  <cit> . a link between diagnosis and azgp <dig>  another previously studied marker for copd  <cit> , was also recovered. fgg and cyp1a <dig> were found to be linked to cigarette smoking history. cyp1a <dig> is known to convert polycyclic aromatic hydrocarbons, found in cigarette smoke, into carcinogens  <cit> , and fgg codes for fibrinogen, a marker for inflammation, which is positively correlated with risk of mortality and copd severity  <cit> .fig.  <dig> learned sub-network of gene expression and clinical features connected to lung disease diagnosis, lung tests and cigarette smoking. nodes are colored by data type, blue for gene expression, red for clinical variables. edges were filtered by weight with a threshold of . <dig>  node size is proportional to the diagonal of the β matrix for continuous variables and ϕ
yyf for each categorical variable, y



recovering functional relationships
we also compared the functional relevance of mgm networks learned with steps and those learned by qp-graphs  <cit>  which is another method for learning networks over mixed data. like scggm, qp-graphs do not attempt to learn edges between two discrete variables, but qp-graphs do not make a linearity assumption about the discrete variables. to assess the biological relevance of networks learned at different levels of sparsity, we performed enrichment analyses on genes with expression variables linked to each clinical variable. for each group of genes linked to each clinical variable we counted go terms with an uncorrected enrichment p < . <dig>  these counts are shown in fig.  <dig>  since each clinical variable represents a phenotype, we would hope that genes linked to those variables share similar biological function as measured by functional enrichment. we would like to choose a value of λ that maximizes the number of enriched go terms.fig.  <dig> counts of go terms with uncorrected p < . <dig> for groups of genes with expression variables linked to each discrete clinical variable in a mgm networks at different values of λ and b qp-graphs at different values of q. edge thresholds for qp-graphs were chosen to select similar numbers of connected genes to an mgm network with λ = .1



the setting of λ = . <dig> recovers the most annotations for diagnosis, fev <dig> and fvc, and also corresponds to an instability threshold of γ = . <dig>  qp-graphs output a “non-rejection rate” for each edge, which corresponds to the number of different conditional independence tests that rejected the presence of each edge. to predict edges, this output needed to be thresholded, so we chose thresholds that produced similar numbers of edge predictions to λ = . <dig>  while qp-graphs perform comparably well to mgms in this test, we found the learning procedure to be very computationally expensive. on a quad-core laptop, learning a qp-graph with q =  <dig> took over 3 h  while learning an mgm took  <dig>  min on average when the iteration limit was reached.

evaluating mgm in classification tasks
we also evaluated mgms on how well a trained model can predict the status of a given target variable and we chose the lung disease diagnosis as a clinically relevant target variable. the mgm was compared to svm and lasso. we optimized the settings of svm, lasso and our mixed models to maximize the 8-fold cross-validation accuracy of predicting lung disease diagnosis using the  <dig> expression variables and  <dig> clinical variables. for svm, we found that a linear kernel worked best on this data. for lasso and mgm, the parameter scan found that λ =  <dig>  maximized this accuracy. figure 6a shows a comparison between the optimized classification accuracies of these three methods. for mgm classification, we expected similar results to lasso because the conditional distribution of a discrete variable in the mixed model reduces to a  logistic regression. it is interesting to see that the generative mgms are not significantly different from discriminative lasso and svm models in this experiment. while λ = . <dig> maximized the cross-validation accuracy for mgms, fig. 6b shows that the stars selected sparsity values of λ = . <dig> and . <dig> do not perform significantly worse than λ = . <dig>  Ιn addition, we ran experiments using steps with settings of  =  and  =  which correspond to instability thresholds of γ = . <dig> and γ = . <dig>  respectively, and found that these changes did not significantly alter classification performance.fig.  <dig> 
a 8-fold cross validation accuracies for copd/ild classification using different methods b regularization effects on classification accuracy 



discussion
learning graphical models over variables of mixed type is very important for biomedical research. the most widely used types of genomic data include continuous  and discrete  variables. similarly, clinical variables can be either continuous or discrete . we are interested in learning graphical models from these heterogeneous data to identify significant interactions between variables and uncover important biological pathways. as an added advantage, a learned network and joint probability can be used to ask an arbitrary number of classification questions over the data without the need for retraining each time  <cit> . these models would be broadly applicable to biological network inference, biomarker selection and patient stratification. although calculating the mgm requires certain distributional assumptions about the data, the two distributions that make up the model in this work, a multivariate gaussian for the continuous variables and a pairwise markov random field for discrete variables, are well studied and have been successfully applied to many types of data. additionally, using gaussian copula methods  <cit>  in conjunction with mgm learning would allow users to relax the normality assumption for the continuous data.

our simulation study strongly supports the need for separate sparsity penalty for each edge type when learning an mgm. in addition we show the effectiveness of our extension of the stars procedure, steps, to select these penalty terms. by using instability estimates from the single sparsity parameter model to select parameters for the three parameter model, we are making the assumption that each edge type set is independent from the others. we showed that steps performance under this independence assumption is comparable to a stability selection procedure that does not make this assumption. the payoff for steps is that we can select three parameters in linear time  rather than cubic time. steps is a general methodology, which can be applied to a variety of mixed distribution settings, and will be especially useful in problems with many different edge types.

one could argue that steps substitutes an arbitrary setting of λ for an arbitrary setting of the instability threshold, γ. as liu et al. point out, γ has a more intuitive meaning than λ, and we feel that setting this threshold compares to the common practice of setting an arbitrary significance threshold for rejecting the null hypothesis. our results from applying steps to mgms highlights the fact that the same setting of γ applies well to all edge types while different, edge type specific settings of λ are required for accurate edge recovery. although it is possible to set the sparsity parameters based on some prior knowledge of the expected number of edges in the network, the data driven methods we present here allow for wide application of mgms to domains where such knowledge is not available.

furthermore, we show that our approach to mgm learning is competitive with a state-of-the-art eqtl learning method, scggms. although scggms run faster that our mgms due to the fact that it treats all variables as continuous, we showed that mgms have a clear advantage when the discrete variables have non-linear relationships with the continuous variables. the assumption of linearity is common in eqtl learning and it makes sense in the haploid yeast datasets  used in the scggm study. in more complex organisms, however, an mgm that can handle non-linear interactions may be necessary.

while we had difficulty setting the discrete-discrete edge penalty in the lung dataset, we were still able to show the utility of mgm based analysis on biological data. also, results from our classification experiment were robust to variation in the setting of this parameter. we do not expect mgms to perform better than standard classification methods because the latter minimize the error of the classification problem  directly. the pseudolikelihood optimization in the mgm, however, must take into account the relationships between all of the variables. despite this handicap, our results show, however, that the mgm-based classification is comparable to standard methods while offering two key advantages:  the same trained mgm can be used to make predictions about any variable without additional learning, and  the graph structure allows us to look at the second neighbors of the target variable and beyond for possible functional significance.

CONCLUSIONS
mixed graphical models are becoming popular in the statistics and machine learning literature, and there is a lot of potential for their application to high dimensional biological data. we have broached that potential in this study. we showed that mgms can accurately learn undirected graphical models over a mixture of discrete and continuous variables in a high dimensional setting. in addition, we showed that using a separate sparsity parameter for each edge type in a graph can significantly improve edge recovery performance. these separate parameters can account for the differences in both the difficulty of learning such an edge and differences in the sparsity of edge types in the true graph. finally, we showed that stability based methods are well suited for model selection in this setting and that our method steps allow us to perform a search over the sparsity penalties in linear time.

declarations
publication charges for this article were funded by the national institutes of health  under award number r01lm <dig>  u54hg <dig> and u01hl <dig> for pvb, and award t <dig> eb <dig> for ajs and rmd. the content is solely the responsibility of the authors and does not necessarily represent the official views of the nih.

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2016: selected articles from statistical methods for omics data integration and analysis  <dig>  the full contents of the supplement are available online at http://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-17-supplement- <dig> 

abbreviations
aicaikike information criterion

bicbayesian information criterion

cvcross-validation

eqtlexpression quantitative trait loci

mccmathews correlation coefficient

mgmmixed graphical model

scggmsparse conditional gaussian graphical model

stepsstable edge-specific penalty selection

svmsupport vector machine

competing interests

the university of pittsburgh has filed a provisional patent application, which is based partly on the algorithm described here .

authors’ contributions

ajs and pvb developed the idea, designed the study, interpreted the results and wrote the paper. ajs developed the algorithm with input from rmd. ajs, rmd and is produced the results. all authors have seen and approved the paper.

