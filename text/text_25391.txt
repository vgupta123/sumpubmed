BACKGROUND
factor analysis  as well as principal component analysis  is used to describe a number of observed variables by a smaller number of unobserved variables. unlike pca, fa also includes independent additive measurement errors on the observed variables. fa assumes that the observed variables become uncorrelated given a set of hidden variables called factors. it can also be seen as a clustering method where the variables described by the same factors are highly correlated, thus belonging to the same cluster, while the variables depending on different factors are uncorrelated and placed in different clusters.

fa has been successfully used in a number of areas such as computer vision, pattern recognition, economics and more recently in bioinformatics  <cit> . the suitability of fa for gene expression analysis is also the motivation of this work. genes are transcribed into mrnas which in turn are translated into proteins. some of these proteins activate or inhibit, as transcription factors , the transcription of a number of other genes creating a complex gene regulatory network. the number of transcription factors is much smaller than the number of transcribed genes and most genes are regulated only by a small number of transcription factors. hence, the matrix that describes the connections between the transcription factors and the regulated genes is sparse. using microarrays, mrna levels of thousands of genes can be measured simultaneously, but no direct information is obtained about tf activity. our aim is two-fold: to identify the genes regulated by a common tf, that is, to reconstruct the connectivity structure and weights in a two-layer network, and to reconstruct the activity profile of each tf.

liao et al.  <cit>  have suggested the use of a network component analysis  algorithm for reconstructing the profiles of the tfs , while boulesteix and strimmer  <cit>  have used an approach based on partial least squares regression. they have both shown that such methods can faithfully reconstruct the expression profiles of the tfs. however, both methods rely heavily on the availability of connectivity information. nonzero positions in the factor loadings matrix, which describes the connections between the factors and the genes, need to be specified in advance. the algorithms then estimate the values at these positions . this is a strong limitation since often only little information about genes regulated by specific tfs is available. fa models are faced with a much harder task where both the structure of the factor loadings matrix and the activity profiles of the factors have to be reconstructed. independent component analysis  has also been widely used in bioinformatics . this approach assumes that the transcription factors are statistically independent. a comparison of nca and ica can be found in liao et al.  <cit> , and thus ica will not be considered further here. a further advantage of the bayesian fa models is that any information about the underlying structure can be easily incorporated through priors. this improves performance, but is not required for the algorithms to be applicable in the first place, as in the case of nca and its generalisations.

hinton et al.  <cit>  first introduced an em algorithm for factor analysis in order to model the manifolds of digitised images of handwritten digits. later ghahramani and hinton  <cit>  presented an exact em algorithm for both factor analyzers and mixtures of factor analyzers. more recently, utsugi and kumagai  <cit>  used a gibbs sampler instead of the em algorithm suggested by ghahramani and hinton  <cit>  for mixtures of factor analyzers. west  <cit>  was the first to introduce bayesian factor analysis in the bioinformatics field. to accommodate the required sparsity regarding the connections between the factors and the genes, he suggested the use of a mixture prior on the factor loadings matrix. as is shown in the results section, the predicted factor loadings matrix has the desired sparsity, at the expense of increasing computing time as the number of hidden variables increases. recently, sabatti and james  <cit>  have used the framework by west  <cit>  for the reconstruction of transcription factor profiles. in order to avoid the computational burden of estimating the factor loadings matrix at each step of the gibbs sampler and to facilitate the reconstruction process, they set a large number of entries to zero based on information obtained from the vocabulon algorithm  <cit> . this algorithm scans dna sequences for multiple motifs and associates with each transcription factor a probability of binding to a specific site. this approach resembles the approach of liao et al.  <cit> , and boulesteix and strimmer  <cit>  where the structure of the factor loadings matrix is given in advance.

note that the algorithms of ghahramani and hinton  <cit> , and utsugi and kumagai  <cit>  have not previously been applied to biological data, and that the algorithm of sabatti and james  <cit>  is an adaptation of the algorithm of west  <cit>  with the difference that an informative prior is used for the factor loadings matrix. also, sabatti and james  <cit>  applied the fa model to yeast and e. coli data, while west  <cit>  applied his algorithm to cancer data.

in this paper, we suggest the use of fokoue's algorithm  <cit>  as an alternative to west's algorithm  <cit> . this algorithm utilises a gamma prior distribution on the variance of the factor loadings matrix that imposes the required sparsity but, at the same time, avoids the computational burden introduced by the use of a mixture prior  <cit> . since this algorithm avoids the combinatorial problem of west's algorithm, a prior knowledge on the underlying model is not required. at the same time, we give a thorough review of all fa algorithms mentioned above and examine the applicability of those algorithms to biological data. to the best of our knowledge such a comparison of fa algorithms in the scope of analyzing microarray data has not been presented before. moreover, we extend these algorithms by suggesting a further factor rotation analysis which produces additional sparsity of the factor loadings matrix. this additional sparsity not only facilitates the interpretation of the results, but it is also useful in a biological context where a very sparse matrix is required. finally, we show that merging the information provided by each algorithm to obtain a combined result leads to better performance. the algorithms are compared based on their ability to reconstruct the underlying factor loadings matrix and the profiles of the transcription factors.

the comparison is done on both simulated data where the true answer is known and on experimental data. we evaluate the performance of the algorithms on the hemoglobin data obtained by liao et al.  <cit>  and on the escherichia coli  data in kao et al.  <cit> . although time series data show correlation that is ignored in a factor analysis, which in fact assumes independence across data points, we used these data sets for comparison of our results with that in liao et al.  <cit> , kao et al.  <cit> , and boulesteix and strimmer  <cit> .

factor analysis model
let us assume that we have a random observed vector variable x of p dimensions, x = '. we denote an instance of this vector with a superscript n and we assume that we have n such instances, xn where n =  <dig> ..., n. similarly, f = ' is a vector of k hidden variables, known as factors. note that the number k of factors is always smaller than or equal to the number p of observed variables. the factor analysis model states that the observed variables are a linear combination of the factors plus a mean and an error term. for case n

xn=μ+Λfn+εn     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaafaqaaeqaiaaaaaqaamaaxababagaemieag3aawbaasqabeaacqwgubgbaaaabagaeiikagiaemiuaalaeeiiaaiaey41aqraeeiiaaiaegymaejaeiykakcabeaaaoqaaiabg2da9aqaamaaxababaaccigae8hvd0galeaacqggoaakcqwgqbaucqqggaaicqghxdatcqqggaaicqaixaqmcqggpaqkaeqaaagcbagaey4kascabawaacbeaeaacqqhboatasqaaiabcicaoiabdcfaqjabbccagiabgena0kabbccagiabduealjabcmcapaqabaaakeaadawfqaqaaiabdagamnaacaaaleqabagaemoba4gaaaqaaiabcicaoiabduealjabbccagiabgena0kabbccagiabigdaxiabcmcapaqabaaakeaacqghrawkaeaadawfqaqaaiab=v7alnaacaaaleqabagaemoba4gaaaqaaiabcicaoiabdcfaqjabbccagiabgena0kabbccagiabigdaxiabcmcapaqabaaaaogaaczcaiaaxmaadaqadaqaaiabigdaxagaayjkaiaawmcaaaaa@6626@

where μ = ' and εn = ' are column vectors of dimension p with elements corresponding to the mean and the error of the p observed variables. the vector μ is the same for all cases. Λ is the unobserved transition matrix also referred to as the factor loadings matrix. the factor loadings matrix has p × k dimensions. that is, each column corresponds to a factor and each row corresponds to an observed variable. the entries of the factor loadings matrix indicate the strength of the dependence of each observed variable on each factor. for example, if λpk is zero, then variable xp is independent of factor fk. in matrix form equation  <dig> is

x=m+Λf+e     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaafaqaaeqaiaaaaaqaamaaxababagaemiwagfaleaacqggoaakcqwgqbaucqqggaaicqghxdatcqqggaaicqwgobgtcqggpaqkaeqaaagcbagaeyypa0dabawaacbeaeaacqwgnbqtasqaaiabcicaoiabdcfaqjabbccagiabgena0kabbccagiabd6eaojabcmcapaqabaaakeaacqghrawkaeaadawfqaqaaiabfu5ambwcbagaeiikagiaemiuaalaeeiiaaiaey41aqraeeiiaaiaem4saskaeiykakcabeaaaoqaamaaxababagaemorayealeaacqggoaakcqwglbwscqqggaaicqghxdatcqqggaaicqwgobgtcqggpaqkaeqaaagcbagaey4kascabawaacbeaeaacqwgfbqrasqaaiabcicaoiabdcfaqjabbccagiabgena0kabbccagiabd6eaojabcmcapaqabaaaaogaaczcaiaaxmaadaqadaqaaiabikdayagaayjkaiaawmcaaaaa@60be@

where x = , f = , e = , m = μen with en an n dimensional row vector of ones. fa models assume that the error terms εn are independent, and multivariate normally distributed with mean zero and covariance matrix Ψ, εn ~ n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaat0uy0hwztfgdpnwy1egaryqthrhal1wy0l2yhvdaiqaacqwfnevtaaa@383a@, where Ψ = diag. thus the probability distribution of x for each observed case n has a multivariate normal density given by

p=n=−p/2|Ψ|−1/2×exp⁡′Ψ−1)     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaafaqaaegadaaabagaemicaanaeiikagiaemieag3aawbaasqabeaacqwgubgbaagccqgg8bafcqwgmbgzdaahaawcbeqaaiabd6gaubaakiabcycasiabfu5amjabcycasggaciab=x7atjabcycasiabfi6azjabcmcapaqaaiabg2da9aqaamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnagabaiab+1q8ojabcicaoiabdiha4naacaaaleqabagaemoba4gaaogaeiifawnae8hvd0maey4kasiaeu4mdwkaemozay2aawbaasqabeaacqwgubgbaagccqggsaalcqqhooqwcqggpaqkaeaaaeaacqgh9aqpaeaacqggoaakcqaiyagmcqwfapaccqggpaqkdaahaawcbeqaaiabgkhitiabdcfaqjabc+caviabikdayaaakmaaemaabagaeuiqdkfacaglhwuaayjcsdwaawbaasqabeaacqghsislcqaixaqmcqggvawlcqaiyagmaagccqghxdatcygglbqzcqgg4baecqggwbacdaqadaqaaiabgkhitmaalaaabagaegymaedabagaegomaidaaiabcicaoiabdiha4naacaaaleqabagaemoba4gaaogaeyoei0iae8hvd0maeyoei0iaeu4mdwkaemozay2aawbaasqabeaacqwgubgbaagccuggpaqkgaqbaiabfi6aznaacaaaleqabagaeyoei0iaegymaedaaogaeiikagiaemieag3aawbaasqabeaacqwgubgbaagccqghsislcqwf8oqbcqghsislcqqhboatcqwgmbgzdaahaawcbeqaaiabd6gaubaakiabcmcapagaayjkaiaawmcaaaaacawljagaaczcamaabmaabagaeg4mamdacagloagaayzkaaaaaa@98b9@

or in matrix notation

p=n=−n/2|Ψ|−1/2×exp⁡′Ψ−1])     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaafaqaaegadaaabagaemicaanaeiikagiaemiwaglaeiifawnaemoraykaeiilawiaeu4mdwkaeiilawcccigae8hvd0maeiilawiaeuiqdklaeiykakcabagaeyypa0dabaacdagae4xdx7kaeiikagiaemiwaglaeiifawnaemyta0kaey4kasiaeu4mdwkaemoraykaeiilawiaeuiqdklaeiykakcabaaabagaeyypa0dabagaeiikagiaegomaijae8hwdanaeiykakyaawbaasqabeaacqghsislcqwgobgtcqggvawlcqaiyagmaagcdaabdaqaaiabfi6azbgaay5bslaawia7amaacaaaleqabagaeyoei0iaegymaejaei4la8iaegomaidaaogaey41aqragiyzaumaeiieagnaeiicaa3aaewaaeaacqghsisldawcaaqaaiabigdaxaqaaiabikdayaaacqqg0badcqqgybgccqggbbwwcqggoaakcqwgybawcqghsislcqwgnbqtcqghsislcqqhboatcqwggbgrcuggpaqkgaqbaiabfi6aznaacaaaleqabagaeyoei0iaegymaedaaogaeiikagiaemiwaglaeyoei0iaemyta0kaeyoei0iaeu4mdwkaemoraykaeiykakiaeiyxa0facagloagaayzkaaaaaiaaxmaacawljawaaewaaeaacqai0aanaiaawicacaglpaaaaaa@8d88@

where tr is the trace, the sum of the diagonal elements. in the methods section, we discuss in detail the prior and posterior probabilities of the parameters f, μ, Λ and Ψ, as well as algorithms for their estimation.

identifiability problems
as shown in equation  <dig> in the methods section, the complete density of the data, when factors are integrated out, is given by a normal distribution with covariance matrix ΛΣf Λ' + Ψ. there is a scale identifiability problem associated with Λ and Σf. in order to avoid this problem, we could either restrict the columns of Λ to unit vectors or set Σf to the identity matrix. the second approach is often preferred in factor analysis.

there is also an identifiability problem associated with equation  <dig>  let us assume that we have an orthogonal matrix q of dimensions k × k with qq' = q'q = ik. then we can have

Λf = Λqq'f = Λ*f*

with cov = cov. that is, it is not possible to distinguish between Λ and all its possible orthogonal transformations Λ* based on knowledge of the product Λf only. however, as we show in the results section, if the loadings matrix underlying the data generating process is sparse enough, it can often be reconstructed. this can be done either by using sparsity priors on the entries of the loadings matrix in a bayesian setting or by orthogonal rotations enforcing sparsity .

note that orthogonal transformations also include permutations of the factors. factors could be ordered by the amount of variance explained. or, as in the case of regulatory networks, we would have to map known tfs to the inferred factors. in sabatti and james  <cit> , the factors are constrained by assigning a priori zero values to the factor loadings matrix. here, we map the tfs to the inferred factors based on previous knowledge about their activity profiles, as for example reported in kao et al.  <cit> .

RESULTS
we compare the algorithms by ghahramani and hinton  <cit>  , utsugi and kumagai  <cit>  , fokoue  <cit>  , and west  <cit>   on simulated and real biological data. algorithm w is based on updating hidden indicator variables representing network connections. for a full exploration of the posterior probability, all possible combinations of hidden values need to be evaluated, thus an exponential number of combinations of these variables. we therefore suggest and test a version  of the algorithm with independent updates of hidden variables. we also compare these bayesian fa algorithms with classical fa ).

in order to evaluate the strengths and weakness of such algorithms we simulate comparatively 'easy' data  to be able to focus on the question how far sparsity in the connectivity allows identification of the loadings and the factor matrix. moreover, as shown in the phd thesis by pournara  <cit>  the assumption of linearity is not a severe one given the small amount of data and the significant amounts of noise present in microarray data, especially after taking logarithms of mrna abundance levels or ratios . in a second step, instead of resorting to simulated nonlinear data, which would have invited questions about the choice of particular nonlinear functional forms, we apply the algorithms to real microarray data and evaluate their performance there directly.

simulated networks
we test the algorithms on simulated networks. for the generation of random networks we start with a description of network characteristics such as the indegree distribution of genes and outdegree distributions of tfs, which we take from known regulatory networks of e. coli. for each tf, we then select random genes subject to these constraints. the activity levels of the factors f are drawn from a gaussian distribution with zero mean and covariance matrix i. the vector μ of means is set to zero. all non-zero loadings are set to  <dig>  a noise term ep is added in each dimension p with zero mean and variance ψp2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwfipqedaqhaawcbagaemicaahabagaegomaidaaaaa@3109@ as

ψp2=σp2snr
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwfipqedaqhaawcbagaemiuaafabagaegomaidaaogaeyypa0zaasaaaeaacqwfdpwcdaqhaawcbagaemicaahabagaegomaidaaagcbaacbagae43camnae4nba4mae4ncaihaaaaa@3a74@

where σp2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwfdpwcdaqhaawcbagaemicaahabagaegomaidaaaaa@30fe@ is the variance of the data in dimension p, and snr is a signal to noise ratio. we evaluate the performance of the algorithms by calculating the mean of squared error  for the predicted factor loadings matrix Λ and the factor matrix f. we identify the labels of the factors by choosing the column permutation of f that gives the smallest mse.

as discussed above, the loadings and factor matrices are only identifiable up to a rotation. sparsity of the true loadings matrix helps to overcome this lack in identifiability. in algorithms f and w the parameters are estimated by imposing sparsity on the loadings matrix directly. others, not imposing any prior sparsity, cannot be expected to find the correct solution without further processing, for example, by orthogonal transformations to a sparse form. results can be improved by normalising the column vectors of the loadings matrix before the transformation, that is, by dividing each vector by its euclidean length. the inverse of the orthogonal transformation of the loadings matrix is used to transform the factor matrix correspondingly. finally, in order to assess how successful a factor analysis is independently of the identifiability problem for orthogonal transformations, we apply a procrustes orthogonal transformation  of the column vectors of the reconstructed loadings matrix onto the column vectors of the true loadings matrix. such rotation is possible since in the case of simulated data the true loadings matrix is known.

simulated e. coli networks
we assume that there are only a few tfs in e. coli that control the expression profiles of most genes. this assumption is also supported by the connectivity matrix as inferred from regulondb  <cit>  and the current literature in kao et al.  <cit> . the matrix is reproduced in figure  <dig>  it is very sparse with most genes regulated by  <dig> to  <dig> tfs, and with only a few tfs regulating a larger number of genes as shown in figures  <dig> and  <dig> 

we generated random networks consisting of  <dig> genes and  <dig> tfs. since the performance of the algorithms depends on the number of nonzero entries in the loadings matrix Λ, we generated networks with densities ranging from  <dig> to  <dig> percent of nonzero entries. figure  <dig> shows the distributions of the genes and tfs for three networks with densities of  <dig>   <dig> and  <dig> percent. networks with density less than  <dig> have distributions similar to that in the e. coli network of figure  <dig> 

two more tests were performed to investigate the behavior of the fa algorithms on datasets of different size  and data generated with different values of snr . note that the classical fa algorithm uses the covariance matrix of the data and thus the number of cases must be greater than the number of variables. that is, the factoran script was not run for datasets of  <dig> cases. these two tests were applied to networks with density  <dig>  figure  <dig> shows again that algorithms z and u perform similarly regardless of the number of cases in the dataset. moreover, algorithms f and w also perform similarly and have a much smaller mse than the other algorithms. once the varimax rotation is applied, all the algorithms give a similar performance with a smaller mse achieved as the number of cases increases. for sparse networks with small densities even a very small dataset is enough to reconstruct the factor loadings matrix. the procrustes rotation indicates that algorithm w produces a factor loadings matrix which, if properly rotated, is very close to the true matrix for very sparse networks.

summarising, algorithms f and w perform better on sparse matrices than algorithms z, u and m because they implicitly capture the required sparsity on the factor loadings matrix. however, if an appropriate orthogonal rotation of the matrices Λ and f is applied, the performances of all the fa algorithms are enhanced and become comparable.

biological data
we further compare the fa algorithms to two biological datasets; the hemoglobin dataset from liao et al.  <cit> , where the connectivity matrix and the profiles of the factors are known to some degree, and the e. coli dataset, where the tf profiles and some interactions have been suggested by kao et al.  <cit> .

hemoglobin dataset
the absorbance spectra of seven hemoglobin solutions  were measured in liao et al.  <cit> . each spectrum is the outcome of a linear combination of the concentrations of three components: oxyhemoglobin , methemoglobin  and cyano-methemoglobin . this dataset consists of  <dig> measurements for each of the seven hemoglobin solutions.

we first compared the algorithms by fokoue  <cit> , west  <cit> , and by tran et al.  <cit>   fixing the positions of zeros in the loadings matrix. note that the algorithm by tran et al.  <cit>  requires this connectivity matrix as an input and is unlikely to work properly without this information. tran et al.  <cit>  have presented an extension of the nca algorithm  <cit> , the gnca  algorithm. for details regarding the different versions of the gnca algorithm see  <cit> . we present the results for versions gnca and gncar. each algorithm was run  <dig> times. for algorithms gnca and gncar, we consider the run with the least mse, while for the fa algorithms we consider the average of these runs.

as shown in figure  <dig>  the mse in the estimation of Λ is approximately equal for all algorithms except gncar, and it is very similar before and after procrustes rotation. this figure indicates that fixing the zero loadings simplifies the task of identifying the underlying factor loadings matrix considerably. figure  <dig> shows the mse in the estimation of the factor profiles, and these profiles are plotted in figure  <dig>  the mse for the reconstruction of the factor profiles is close to zero for all the algorithms except the algorithm gncar. we used the inverse of the rotation matrix returned for Λ by the procrustes method to rotate the factors. the rotation increases the mse of the factors since the best rotation for Λ is not necessarily the best rotation for f. however, it is still considerably small.

we also evaluated the algorithms without providing prior information about the underlying structure of the factor loadings matrix. this can, of course, only be done for the fa algorithms. figure  <dig> shows the mse of Λ as given by each algorithm. it also shows the mse after performing varimax, quartimax, equamax, tanh, and procrustes rotation. most fa algorithms perform equally well in predicting the values of the loadings of Λ. this is probably due to the fact that the hemoglobin factor loadings matrix is not sparse enough. algorithms z and u depend less on sparsity and match the performance of algorithms f and w on this dataset. however, once we perform varimax rotation the performance of all the algorithms improves.

the classical fa algorithm  performs best according to the mse of Λ. however, comparing the mse of the factors ) its performance is worse. this is also apparent by looking at the factor profiles . classical fa optimises the joint likelihood of the loadings matrix and noise covariance matrix , which amounts to integrating out the factors. all other algorithms  represent the factors explicitly. this explains why classical fa is doing better in reconstructing the loadings but worse in reconstructing factors compared to the other algorithms.

algorithm f and w perform quite well on both the reconstruction of the Λ and the factor profiles. their performance is also improved by using any of the four rotation methods. varimax rotation also improves the performance of algorithms z and u. again procrustes rotation shows that we can rotate the estimated Λ to match the true Λ very closely. however, as shown again by the mse on the factors, the best rotation for Λ is not necessarily the best rotation for the factors.

escherichia coli dataset
we evaluated the fa algorithms as well as the algorithm by boulesteix and strimmer  <cit>   on an e. coli dataset from kao et al.  <cit> . these data consist of  <dig> time points for  <dig> genes. the first time point was ignored since all the values are zero. a matrix that indicates possible interactions between  <dig> tfs and the  <dig> genes has been suggested by kao et al.  <cit>  based on regulondb  <cit>  and the current literature. we will refer to this matrix as the kao connectivity matrix. this matrix also indicates whether a tf inhibits or activates a given gene. each fa algorithm is run  <dig> times. the following results refer to an average value over these runs. the classical fa is not used in this analysis since the number of cases  is smaller than the number of observed variables .

since the gnca algorithm requires prior knowledge of zeros in the factor loadings matrix, for comparison we also run the fa algorithms of fokoue  <cit>  and west  <cit>  providing prior information on zeros in the factor loadings matrix. here, algorithms f and ws treat the connectivity matrix simply as indicating whether there is a relationship or not between a gene and a tf and ignore the information on activation or inhibition. however, one could also include a more detailed prior information. we consider two different prior matrices for the gnca algorithm: one where a simplified connectivity matrix that only indicates whether an interaction exists or not, and one with extra information on inhibition and activation. for each of the two different prior matrices, we run the gnca algorithm  <dig> times, and we only consider the run with the least sum squared error.

mses of the reconstructed factor matrices from the factor matrix obtained from gnca with activation and inhibition information. the second column contains the mses when the zero positions in the loadings matrix are fixed. the third column contains the mses when no information regarding those positions is given. – indicates that the algorithm was not tested.

finally, we analyse the inferred factor loadings matrix in greater detail. such an evaluation is complicated by the fact that the true connectivity matrix is not fully known. for evaluating the learned loadings matrix, we treat the kao connectivity matrix ) as showing true interactions and true missing interactions. however, we should keep in mind that the latter is based on partial biological information and not necessarily complete. figure  <dig> shows a roc curve for each algorithm. the true positive  rate is the proportion of entries above a specified cutoff among entries which are nonzero according to the kao connectivity matrix. the false positive  rate is the proportion of entries above a specified cutoff among entries which are zero according to the kao connectivity matrix. on average all algorithms give very similar performance. the lack of differences between the algorithms that implicitly consider sparsity, f and ws, compared to the algorithms that do not, z and u, could be due to the lack of detailed information in the kao connectivity matrix. that is, this matrix has only  <dig>  entries and actually some of the  <dig> entries could be very close to zero or exactly zero and in contrast some zero values could be nonzero. figure  <dig> also shows a roc curve that is based on merging the information gain by each algorithm. that is, we derive a combined factor loadings matrix by averaging the loading matrices derived by each algorithm. this combined loadings matrix gives a roc curve that is better than any other roc curve alone.

we also plot, in figure  <dig>  the roc curve of each algorithm after applying procrustes rotation to the factor loadings matrix. here, we use the kao connectivity matrix as the target matrix for the procrustes rotation. the roc curves have greatly improved indicating that an appropriate rotation of the learned loadings matrix for each algorithm can lead to a connectivity matrix that is very close to the kao connectivity matrix. again the combined loadings matrix gives a roc curve that outperforms each of the roc curves given by the fa algorithms.

CONCLUSIONS
we discussed and compared the performance of five factor analysis algorithms presented previously in the literature. only one of these algorithms has been previously applied to biological data. we investigated the applicability of the algorithms on microarray data from e. coli, on data from hemoglobin spectroscopic measurements and on simulated data. in a gene regulatory context, we aim to identify regulatory relationships between genes and tfs and to reconstruct transcription factor activity profiles. that is, the expression levels of regulated genes are the observed variables and the tfs are the unobserved variables. even after imposing a correlation structure on the factors, this is still an underdetermined problem. if, however, we assume that the connectivity matrix is sparse, that is, that most genes are regulated by a small number of tfs and most tfs regulate only a small number of genes, estimation of tf profiles and loadings becomes possible.

the sparsity requirement is implicit in the algorithms by fokoue  <cit>  and west  <cit> , and thus these algorithms are shown to perform very well on sparse simulated networks where the underlying relationships are linear. however, we show that the performance of the algorithms by ghahramani and hinton  <cit> , and utsugi and kumagai  <cit>  is also very satisfactory after an orthogonal rotation of the loadings matrix. on the e. coli data, we see that all the fa algorithms reconstruct the factor loadings matrix and the factors profiles equally well. moreover, we show, using the e. coli data, that such algorithms can reconstruct the underlying tf profiles to an acceptable degree even without any prior knowledge of the connectivity structure. in contrast, algorithms such as the gnca algorithm of tran et al.  <cit> , depend heavily on prior connectivity information. finally, we show that integrating results from several fa algorithms results in a connectivity matrix which has a better true positive rate given a specified false positive rate than each algorithm separately. our analysis demonstrates the usefulness of fa algorithms for biological problems where prior information regarding the system under study is not fully available.

the fa algorithms discussed here ignore any time series information. we are currently working on an extension of the above methods to integrate time correlation. we expect that such correlation will smooth tf activity profiles further.

