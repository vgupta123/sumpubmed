BACKGROUND
even though the first works in automatic text summarization date from the middle of the last century  <cit> , research in biomedical summarization has started only recently. biomedical summarization works typically adapt existing methods from domain-independent summarization to deal with the highly specialized biomedical terminology. to this end, they make use of external knowledge sources to represent the texts as sets of domain concepts and relations. this produces a richer representation than the one provided by traditional term-based models and results in better quality summaries.

a pioneer work in biomedical summarization is found in  <cit> . they propose the use of semantic predications provided by semrep  <cit>  and information from the unified medical language system â“‡ <cit>  to extract biomedical entities and relations, and generate semantic-level abstracts, which are presented in graphical format. ling et al.  <cit>  focus on a narrower domain, genomic, and present a gene summary system that ranks sentences according to three features: the relevance of six gene aspects, such as the dna sequence, the relevance of the documents where the sentences are taken from, and the position of the sentences in the document. reeve et al.  <cit>  use the frequency of the umls metathesaurus concepts found in the text and adapt the lexical chaining approach  <cit>  to deal with concepts instead of terms. their system is used to produce single-document extracts of biomedical articles.

more sophisticated is the work of yoo et al.  <cit>  for multi-document summarization. they represent a corpus of documents as a graph, where the nodes are the meshâ“‡s <cit>  descriptors found in the corpus and the edges represent hypernymy and co-occurrence relations between them. they cluster the mesh concepts in the corpus to identify sets of documents dealing with the same topic and then generate a summary from each document cluster. biosquash  <cit>  is a question-oriented extractive system for biomedical multi-document summarization. it constructs a semantic graph that contains concepts of three types: ontological concepts , named entities and noun phrases.

more recent is the work of shang et al.  <cit> , where the aim is to combine information retrieval techniques with information extraction methods to generate text summaries of sets of documents describing a certain topic. to do this, they use semrep to extract relations among umls metathesaurus concepts and a relation-level retrieval method to select the relations more relevant to a given query concept. finally, they extract the most relevant sentences for each topic based on the previous ranking of relations and the location of the sentences in different sections of the document. however, no details are given about how the location scores are calculated.

methods
in this section, we first present the different positional strategies for sentence selections. we next describe the two different summarizers  that have been developed to test such positional strategies.

positional strategies for sentence selection
in order to test our hypothesis that the position of a sentence in the different sections of the document is an indication of the importance of the sentence for inclusion in a summary, and that traditional positional strategies are not appropriate for summarizing biomedical literature, we have defined the following positional criteria: 

•distance to the beginning : sentences close to the beginning of the document are supposed to the deal with the central topic of the document, and so more weight is assigned to them. thus, a score begin_pos∈=11= <dig>  for the second sentence begin_pos=12= <dig> , and so on. 

  begin_pos=1mj 

•distance to the beginning and end : sentences close to the beginning and the end of the document are considered highly relevant. therefore, a score begin_end_pos∈=max{ <dig> −1+1}= <dig>  for the second sentence begin_end_pos=max{ <dig> −2+1}=12= <dig> , and for the last sentence begin_end_pos=max{ <dig> −10+1}= <dig>  

  begin_end_pos=max1mj,1m−mj+ <dig> 

•section in the document : we consider the following section classes or clusters:  introduction,  background,  materials and methods,  results and discussion, and  conclusions and future work. following the methodology used in  <cit> , we have first analyzed the typical structure of biomedical articles and manually grouped the section headers in these five section groups based on the similarity of their contents. lexical variants of the same section header are included in the same class. for example, method and methods are clustered in the materials and methods class. similarly, sections that differ in their title but refer to the same content are included in the same class . we calculate a score section_pos∈, backg, m&m, r&d, and c&f are equal to  <dig> if the sentence sj belongs to each of the five section groups, respectively, and  <dig> otherwise. the values of γ, δ, θ, σ, and Π vary between  <dig> and  <dig>  and need to be empirically determined. 

  section_pos=γ×intro+δ×backg+θ×m&m+σ×r&d+Π×c&f 

graph-based summarizer
we use the graph-based summarization method presented in  <cit> . this method is based on the representation of the document as a conceptual graph, using the umls  <cit>  as the knowledge source, and the use of a degree-based clustering algorithm for detecting salient concepts within the graph. the original summarizer has been modified to incorporate more advanced positional strategies in the sentence selection step. the system architecture is illustrated in figure  <dig> 

the method consists of the  <dig> main steps, which are briefly explained below : 

•the first step, concept identification, is to map the document to concepts from the umls metathesaurus and semantic types from the umls semantic network. we run the metamap  <cit>  program over the body section of the document to obtain the metathesaurus concepts that are found within the text. metamap is invoked using the word sense disambiguation option . this flag implements the journal descriptor indexing  methodology described in  <cit> . umls concepts belonging to very general semantic types are discarded since they have been found to be excessively broad and do not contribute to summarization. these types are quantitative concept, qualitative concept, temporal concept, functional concept, idea or concept, intellectual product, mental process, spatial concept and language.

•the second step, document representation, is to construct a graph-based representation of the document. to do this, we first extend the umls concepts with their complete hierarchy of hypernyms  and merge the hierarchies of all the concepts in the same sentence to construct a sentence graph. the upper levels of these hierarchies are removed, since they represent concepts with excessively broad meanings. next, all the sentence graphs are merged into a single document graph. this graph is extended with two further types of relations: relations between concepts in the umls metathesaurus and relations between semantic types in the umls semantic network . finally, each edge is assigned a weight in  <cit>  as shown in equation  <dig>  the weight of an edge e representing an is_a relation between two vertices, vi and vj , is calculated as the ratio of the depth of vi to the depth of vj from the root of their hierarchy. the weight of an edge representing any other relation  between pairs of leaf vertices is always  <dig>  

  weight=β 

 whereβ=depthdepthiferepresents anis_arelationβ=1otherwise 

•to illustrate this process, figure  <dig> shows the document graph for the following text from  <cit> : 

interactions among lrf- <dig>  junb, c-jun, and c-fos define a regulatory program in the g <dig> phase of liver regeneration. in regenerating liver, a physiologically normal model of cell growth, lrf- <dig>  junb, c-jun, and c-fos among jun/fos/lrf- <dig> family members are induced posthepatectomy. in liver cells, high levels of c-fos/c-jun, c-fos/junb, lrf-1/c-jun, and lrf-1/junb complexes are present for several hours after the g0/g <dig> transition, and the relative level of lrf-1/junb complexes increases during g <dig>  we provide evidence for dramatic differences in promoter-specific activation by lrf-1- and c-fos-containing complexes. lrf- <dig> in combination with either jun protein strongly activates a cyclic amp response element-containing promoter which c-fos/jun does not activate.

•the third step, topic recognition, consists of clustering the umls concepts in the document graph using a degree-based clustering method similar to that used by  <cit> . the aim is to construct sets of concepts strongly related in meaning, based on the assumption that each of these clusters represents a different topic in the document. we first compute the salience of each vertex in the graph, as the sum of the weights of the edges that are linked to it. next, the nodes are ranked according to its salience. the n vertices with a highest salience are labeled as hub vertices. the clustering algorithm then groups the hub vertices into hub vertex sets . these can be interpreted as sets of concepts strongly connected and will represent the centroids of the final clusters. the remaining vertices  are iteratively assigned to the cluster to which they are more connected. the output of this step is, therefore, a number of clusters of umls concepts, each cluster represented by the set of most highly connected concepts within it . in this way, for instance, the top five salient concepts in the document graph represented in figure  <dig> are: cells, lrf, cjun, c-fos, and growth.

•the last step, sentence selection, consists of computing the similarity between each sentence graph and each cluster, and selecting the sentences for the summary based on these similarities. to compute sentence-to-cluster similarity, we use a non-democratic vote mechanism so that each vertex of a sentence assigns a vote to a cluster if the vertex belongs to its hvs, half a vote if the vertex belongs to it but not to its hvs, and no votes otherwise. the similarity between the sentence graph and the cluster is computed as the sum of the votes assigned by all the vertices in the sentence graph to the cluster. a single score for each sentence is calculated, as the sum of its similarity to each cluster adjusted to the cluster’s size . 

  sem_sim=∑cisimilarity|ci| 

finally, this semantic similarity is normalized in the  <cit>  interval and combined with each of the positional criteria explained in the previous section using a linear function . the n sentences with higher score are then selected for the summary. 

  score=α×sem_sim+β×position; 

α and β can be assigned different weights between  <dig> and  <dig>  their optimal values must be determined empirically.

concept frequency-based summarizer
the second summarizer is based on the frequency of umls concepts in the document. it consists of  <dig> steps: 

•the first step, concept identification, is to map the document to concepts from the umls metathesaurus and semantic types from the umls semantic network using metamap, as explained for the graph-based summarizer.

•concept frequency representation: following luhn’s theory, we assume that the more times a word  appears in a document, the more relevant become the sentences that contain this word. in this way, if {c <dig> c <dig> ...,cn} is the set of n metathesaurus concepts that appear in the document d, and fi is the number of times that ci appears in it, then the document may be represented by the vector d={f <dig> f <dig> ...,fn}. similarly, we build the vectors representing each sentence in the document, sj, and compute a concept frequency score, cf, as the sum of the frequency of all the concepts in the sentence multiplied by the frequency of those concepts in the document. this cf score is normalized in the  <cit>  interval.

•in this way, the text modeled as a conceptual graph in figure  <dig> is represented by the following document and sentence vectors: 

 d={lrf= <dig> c−jun= <dig> c−fos= <dig> liver= <dig> cell= <dig> promoter= <dig> complexes= <dig> program= <dig> regeneration= <dig> growth= <dig> lrf−1= <dig> transition= <dig> c−complex= <dig> combination= <dig> junprotein= <dig> protein= <dig> element=1}s1={lrf= <dig> c−jun= <dig> c−fos= <dig> program= <dig> liver= <dig> regeneration=1}s2={lrf= <dig> c−jun= <dig> c−fos= <dig> liver= <dig> cell= <dig> growth=1}s3={lrf= <dig> c−jun= <dig> c−fos= <dig> liver= <dig> cell= <dig> lrf−1= <dig> transition= <dig> complexes=2}s4={lrf= <dig> promoter= <dig> c−complex=1}s5={lrf= <dig> promoter= <dig> combination= <dig> junprotein= <dig> protein= <dig> element=1} 

•the concept frequency scores of the sentences are, respectively, cf= <dig>  cf= <dig>  cf= <dig>  cf= <dig> and cf= <dig> 

•sentence position: different position scores are calculated for each sentence in the document, according to the positional strategies described previously in the article.

•the last step, sentence selection, consists of extracting the most important sentences for the summary up to the desired length. having computed the different weights for each sentence, the final score for a sentence ) is calculated according to the equation  <dig>  finally, the n sentences with higher score are extracted for the summary. 

  score=α×cf+β×position 

•α and β can be assigned different weights between  <dig> and  <dig>  their optimal values must be determined empirically.

evaluation methods
the most common approach to evaluating automatically generated summaries of a document  is to compare them against manually-created summaries  and measure the similarity between their content. the more content that is shared between the peer and reference summaries, the better the peer summary is assumed to be. to the authors’ knowledge, no corpus of model summaries exists for biomedical articles. for this reason, in this work we use a collection of  <dig> biomedical scientific articles randomly selected from the pmc open access subset  <cit> . when collecting the articles, we made sure that they present, at least, the five main following sections: introduction, background, methods, results and discussion, and conclusions and future work. the abstracts for the articles are used as model summaries, since they condensate the most relevant content in the articles and have been written manually.

the rouge metrics  <cit>  are used to quantify the content similarity between the automatic summaries and the reference ones. rouge is a commonly used evaluation method for summarization which uses the proportion of n-grams between a peer and one or more reference summaries to compute a value within  <cit> . higher values of rouge are preferred, since they indicate a greater content overlap between the peer and the model. the following rouge metrics are used: rouge- <dig> and rouge-su <dig>  rouge- <dig> counts the number of bigrams that are shared by the peer and reference summaries and computes a recall-related measure. similarly, rouge-su <dig> measures the overlap of skip-bigrams , using a skip distance of  <dig>  both rouge- <dig> and rouge-su <dig> have shown high correlation with the human judges gathered from the document understanding conferences  <cit> . however, it must be noted that rouge metrics present two important limitations:  they depend on the length of the peer summaries , and  since they use lexical matching instead of semantic matching, peer summaries that are worded different but have the same semantic information may be assigned different rouge scores. thus, these metrics should only be used in a comparative fashion on the same dataset and should not be interpreted as absolute measures.

automatic summaries are generated by selecting sentences until each summary reaches the same number of sentences than its corresponding model summary . we generate summaries using both the graph-based and the frequency-based summarizers and the three positional strategies for sentence selection, and assigning different weights to the different parameters of the summarizers. for these experiments, different combinations of values for α, β, γ, δ, θ, σ, and Π were tested. however, for the sake of brevity, only the combinations that produced the best rouge scores are presented. a wilcoxon signed ranks test with a 95% confidence interval is used to test statistical significance of the results.

RESULTS
we first evaluate the adequacy of the begin-pos positional strategy. the results of evaluating the automatic summaries generated when different weights are assigned to such criterion are shown in table  <dig>  as it may seen from this table, giving greater weight to sentences at the beginning of the document improves the quality of the automatic summaries compared with not using positional information, but only when the weight assigned to the position of the sentences is low . the improvement achieved is higher for the frequency-based summarizer than for the graph-based one, and it is only statistically significant for the first one.

significance is calculated with respect to the non-positional information baseline , and shown using the following convention: * = p <. <dig> and no star indicates non-significance. the best scores per summarizer are shown in bold.

we next evaluate the effect of the begin-end-pos criterion, which attaches greater weight to sentences close to the beginning and end of the document. it may be seen from table  <dig> that this strategy does not benefit the quality of the graph-based summaries, regardless of the weights assigned to the different criteria, but slightly improves the performance of the concept-frequency based summarizer when β is set to  <dig> . however, once again, the improvement achieved is not significant.

significance is calculated with respect to the non-positional information baseline , and shown using the following convention: * = p <. <dig> and no star indicates non-significance. the best scores per summarizer are shown in bold.

we finally examine the summaries generated when different weights are assigned to sentences depending on the section in which they appear . these results are shown in table  <dig>  for both summarizers, there exist a combination of weights that produces significantly better summaries compared with the non-positional information summaries . in particular, the best rouge scores are reported when the introduction and the conclusions and future work sections are given a weight of  <dig> , no weight is assigned to sentences from the related work section, and the sentences from the methods and material and the results and discussion sections are given a weight of  <dig> . this configuration allows for improvements of over 17% in rouge- <dig> for the graph-based summarizer and over 20% for the frequency-based summarizer.

significance is calculated with respect to the non-positional information baseline , and shown using the following convention: * = p <. <dig> and no star indicates non-significance. the best scores per summarizer are shown in bold.

it is also worth mentioning that the experiments showed that weights for the background and conclusion and future work sections  above  <dig>  produce very poor summarization results, and that γ values  upper  <dig>  decrease performance as well. in contrast, the best results are reported when the methods and material and the results and discussion sections are assigned high weights.

finally, table  <dig> compiles the best results for each positional strategy and summarizer. for comparison purposes, this table also shows the rouge scores for the summaries generated using lexrank  <cit> . lexrank is the best-known graph-based method for summarization. it models documents as undirected graphs in which each node corresponds to a sentence, represented by its tf-idf vector, and the edges are labeled with the cosine similarity between the sentences. it may be seen from table  <dig> that the best rouge scores are obtained when the graph-based summarizer is combined with information about the position of the sentences in the different document sections. these scores are significantly better than those of the frequency-based summarizer and than those of lexrank.

rouge results for different summarization approaches. the best scores per summarizer are shown in bold.

discussion
the results in tables  <dig>   <dig>  and  <dig> confirm our hypothesis that traditional positional strategies are not appropriate when summarizing biomedical scientific articles, as opposed to summarizing other types of documents, such as news items. our experiments have shown that awarding sentences close to the end of the document decreases summarization performance comparing with not using any positional information, while awarding sentences close to the beginning of the document only improves the quality of the summaries for the frequency-based summarizer.

in contrast, we have found that it is possible to improve summarization by taking into account the section in the article in which the different sentences appear, and attaching greater relevance to sentences from the appropriate sections. in particular, it has been found that sentences from the methods and material and results and discussion sections are more relevant for inclusion in the summary, since they are more related to the main topic of the document that sentences in other sections, such as introduction and conclusions and future work. sentences in the related work section seem to be secondary, and therefore are not usually included in a summary. these results confirm what may be observed in the abstracts of the articles. if we examine such abstracts we realize that the information considered as important by the authors of the articles is mostly related to the method and results being described in the article, while the remaining sections are given less credit.

an interesting finding is that, in general, the frequency-based approach takes more advantage from the information about the position of sentences in the document than the graph-based one. we think this is due to the fact that the use of the frequency of the concepts alone is not enough to capture the salience of the sentences, and so the use of the positional criteria helps to bias the selection of sentences toward the most relevant information.

in contrast, the graph-based method captures better the importance of the different sentences, and thus produces better quality summaries even when no positional information is used. however, it still presents some limitations that will be addressed in future work. the first limitation is to do with the coverage of the umls. while general clinical terms are quite well covered, other vocabulary, specially that related to genomic, is not well supported  <cit> . as a results, automatic summaries of genetic and proteomic articles present low rouge values.

the second limitation is to do with the accuracy of the metamap mappings and the ambiguity in the umls metathesaurus. even using the -y disambiguation option, the precision of the jdi algorithm is reported to be around  <dig>  when evaluated against a set of  <dig> ambiguous terms from the nlm-wsd corpus  <cit> . this precision, however, is expected to be lower for genomic entities, such as protein and gene names, where ambiguity is more frequent.

third, our summarization and evaluation methods assume that all users have the same information needs, and that these needs are reflected in the authors’ abstracts. however, different users may have different interests. in future work, we plant to extend the summarizer to produce query-based summaries that take into account the readers’ information needs as specified in a user’s query. to this end, the similarity of each sentence in the document to the user’s query may be computed and uses as a feature for sentence selection.

CONCLUSIONS
this work explores the utility of the position of the sentences as a feature for automatic summarization of scientific articles. toward this goal, we have developed two different summarizers, one based on semantic graphs and the other using concept frequencies, which implement three different positional strategies: the first gives more importance to sentences at the beginning of the article, the second prefers sentences both at the beginning and end, and the third weights sentences according to the section in which they appear. the summaries generated are evaluated and compared with non-positional summaries.

overall, the results suggest that it is possible to improve summarization by taking into account the section in the article in which the different sentences appear, and attaching greater relevance to sentences from the appropriate sections. in contrast, traditional strategies that attach greater weights to sentences at the beginning and end of the document are not suitable when summarizing biomedical scientific articles. we believe that our results are of great interest since they may guide nlp tasks involving extraction of salient information in biomedical literature.

as future work, we plan to investigate the importance of the specific location of sentences within the different sections of the article. in this way, for instance, the last sentences of the introduction section may be more relevant that the first sentences in the same sections, since they usually anticipate the content of the document.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
lp designed and developed the graph-based method for automatic summarization and performed the evaluation. jcda designed and developed the concept frequency-based summarizer and performed the evaluation. lp and jcda drafted and reviewed the manuscript. both authors read and approved the final manuscript.

