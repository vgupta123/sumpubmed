BACKGROUND
scientific workflows are becoming much more widely used to concisely describe the activities required to execute scientific experiments  <cit> . while workflows found in the business world typically represent a consistent, repeatable sequence of events that operates on homogeneous data and conducts simple computations, scientific workflows represent a scientific experiment which can contain numerous unknowns, may have variability in execution sequence, operate on heterogeneous data, contain complex computations, and require a considerable degree of intuition on the part of the researcher to be executed successfully. scientific workflows are complex, dynamic, and contain a high degree of variation.

the workflow for a scientific experiment can be modeled at two levels of abstraction; the implementation level and the conceptual level. currently, most workflows created for scientific experiments are constructed using a scientific workflow environment such as kepler  <cit> . scientific workflow environments result in the creation of an implementation-level workflow, a workflow that represents how the system will carry out the tasks in the workflow. an implementation model of a workflow is tied to a specific set of tools to be used to carry out an experiment. in contrast, a conceptual model of a workflow focuses on the user's viewpoint without being tied to a particular set of underlying implementation tools. a conceptual model of a scientific workflow captures the researcher's understanding of what an experiment does and how it works without capturing implementation details. as such, a conceptual model highlights the data that flow through the workflow. the separation of the conceptual model from the implementation model allows the steps of the experiment process to be understood without constraints imposed by the implementation. in addition, once a conceptual model has been validated, it can be mapped to different implementations in different experiment environments.

the conceptual model describes what steps need to be taken in order to complete an experiment. for instance, in the analysis of biomolecules using nmr spectroscopy, one step in the conceptual model might be analyze sample which encompasses the tasks of construct spectrum, analyze spectrum, assign peaks, post process sample, and validate data/results. the implementation model describes the execution of the steps used to carry out the process described in the conceptual model. the implementation model describes the software, network access, database, and other resources needed to implement the workflow. for instance, the construct spectrum step of the conceptual model could be implemented using the nmrpipe tool  <cit> .

experienced scientists usually have a mental conceptual model of the workflow of an experiment. however, there is no widely acceptable method of codifying that model by writing it down in concrete format. such an explicit conceptual model is essential for validating the correctness of the process of an experiment, and consequently ensuring the correctness of the experimental results. the conceptual model also serves numerous purposes from educating others about the process  <cit> , sharing experimental procedures with other scientists, providing the opportunity to simulate new steps and processes, to streamlining experiments. the conceptual model can be used as the baseline for modifications to the system and for sharing tacit knowledge with other scientists. the development of this conceptual model is our current focus.

the process of conducting biomolecular analysis using nuclear magnetic resonance  spectroscopy is a prime example of a scientific process which is complex, painstaking, time consuming, and requires a high degree of insight on the part of the researcher conducting the experiment. a typical experiment using nmr spectroscopy to determine the structure of a macromolecule involves the use of numerous stand-alone applications that use multiple different file formats requiring the nmr scientists to spend a significant amount of time formatting data for each application  <cit> . due to the non-integrative nature of the scientific applications, most of the experimental data are fragmented and distributed across a variety of file formats risking the potential loss of valuable data. the variability of the nmr experimental process means that the detailed steps taken when conducting an experiment are rarely fully documented. this lack of documentation leads to difficulty in the reproducibility of experiments.

the approach to modeling the workflow for nmr experiments is part of a major research effort focused on the integration of numerous heterogeneous nmr applications  <cit> . the connecticut joint university research  project is a multi-institutional effort investigating the construction of a framework to support the experimental process of employing nmr to determine protein structure. one of the goals of the connjur project is to improve the efficiency of the experimental process and to provide data management support for that process by integrating existing tools and an underlying database to form an integrated environment to support nmr experiments.

scientific workflow systems
due to the need for automating the execution of scientific experiments, numerous scientific workflow management systems have emerged in the academic community with leading systems including kepler  <cit> , taverna  <cit> , triana  <cit> , and woodss  <cit> . these scientific workflow management systems typically provide a distributed environment for scientists to conduct experiments by  connecting to existing collaborative scientific data sources to access and store relevant experiment data.

kepler  <cit>  is an open source scientific workflow management system designed to implement and execute scientific workflows in a distributed environment. kepler uses grid-based computing to access remote resources. kepler uses actors to model the individual steps of an experiment at a fine-grained level of detail.

the workflow-based spatial decision support system   <cit>  was originally built as a workflow system and has been extended to support other scientific applications such as agro-environmental planning and bioinformatics. the main components of woodss are a geographic information system  and a workflow repository.

specifically designed for the construction and execution of bioinformatics workflows, taverna  <cit>  is a scientific workflow management system that supports experiments which access numerous local and distributed information sources. a scientist constructs a scientific workflow at the functional level using the simple conceptual unified flow language .

triana  <cit>  is a workflow-based graphical problem-solving environment which was originally developed by scientists conducting the geo <dig> gravitational wave experiment. triana uses tasks  to construct functional workflows.

these scientific workflow management systems all provide useful functionality for representing workflows at various levels of abstraction. however, none of these tools supports high-level conceptual modeling of the workflow for an entire scientific experiment nor do they provide guidance as to how to create such a conceptual model in the first place.

modeling notations for scientific workflows
a variety of modeling notations has been used to capture aspects of scientific workflows. traditional computing notations have been used including uml, ontologies, petri nets, and data flow diagrams. notations constructed specifically for the scientific domain have also been created including an sql-like language  <cit> , the abstract grid workflow language   <cit> , plan  <cit>  and virtual data language   <cit> . we begin by discussing the use of traditional notations.

uml  <dig>  is an object oriented modeling methodology commonly used for software engineering  <cit> . uml  <dig>  activity diagrams have been utilized to model computational and business processes from an object-oriented perspective. semantics used in activity diagrams are based on petri nets and are defined in terms of token flow  <cit> . control flows and data flows can be represented with uml  <dig>  activity diagrams.

ontologies provide a description of a concept or domain, but do not model the active aspect of a workflow that shows how data move through a series of steps to accomplish a task. ontologies provide a common naming and functionality convention for work-flow components used to construct scientific workflows. ontologies are currently being explored as an approach to constructing workflows from disparate data or software components  <cit> .

petri nets have frequently been used to model the workflows of biological systems and medical processes  <cit> . since petri nets represent a view of the processes and flow of resources in a system, they are a natural modeling representation for scientific workflows.

data flow diagrams have been used effectively to model how data move through the steps in a workflow. in one related work, an approach called participatory workflow analysis  <cit>  was used for domain analysis with a multi-disciplinary group of scientists to capture the different types of problems and solutions scientists encounter in their experiments for a problem solving environment. data flow diagrams are used as the notation for the workflows.

the main advantage to using traditional notations for capturing scientific workflows is that the graphical notations are relatively easy for the lay person to understand. the drawback to such notations is that they frequently only represent the workflow from a single view point such as the flow of data through the workflow. in addition to traditional computing notations, a variety of workflow modeling languages s has been developed to compose functional workflows for execution.

in a foundational work, castro et al  <cit>  define a comprehensive set of components and operators that are commonly used in scientific workflows. built on this theoretical base, the gpipe prototype is a workflow generator which allows users to create workflows consisting of these components and operators using a gui.

a wml which is tightly integrated with sql and uses sql syntax has been developed as a framework for modeling and executing workflows  <cit> . scientists construct the workflow by generating commands using the wml to create programs, inputs, and outputs.

the abstract grid workflow language   <cit>  is an xml-based language that can be used to construct scientific workflow applications at the functional level of the workflow. plan  <cit>  is a similar xml-based wml used to represent scientific workflows that analyze or search for data. specifically designed for bioinformatics workflows, plan can be used to search and return results across numerous heterogeneous systems. virtual data language  is a declarative language for modeling workflows. vdl uses a c-like language to represent the datasets and procedures of a workflow  <cit> .

the main drawback to these wmls is that they have a steeper learning curve than do most traditional computing notations. the wmls require the user to learn some form of a text-based language. workflow components are constructed using these work-flow modeling languages, which are then instantly executed to produce the results. in most instances, the languages do not support a graphical representation of the workflow itself.

while there are many widely accepted approaches for designing business workflows  <cit> , the process by which scientific workflows are determined up to now has been ad hoc, or at least undocumented. we present a process for constructing workflows for scientific applications at the conceptual level.

RESULTS
several factors motivated our selection of approach to modeling the workflow for biomolecular analysis using nmr spectroscopy. our first goal was to construct a comprehensive and accurate conceptual model of the workflow, a broad model that encompasses the entire process, rather than looking in depth at any one phase of the nmr experiment process. a second goal was to manage complexity by providing different perspectives on the experiment process. the use of three phases of modeling supports discrete views of the experiment without loss of detail. yet another goal was to use a modeling notation easily understandable to the nmr experimenter. in order to ensure the accuracy of the model, the model must be easily validated by the domain expert who may not clearly understand the intricacies of a complex text-based workflow modeling language. similarly, we chose to use a component-based modeling approach that could represent scientific workflow components in a generic format. the use of a generic component-based modeling technique allows for ease of transformation to other modeling methodologies. another factor that influenced our approach was time constraints. since nmr researchers would typically prefer to spend their time conducting experiments, they have limited time to devote to modeling exercises. we needed to choose a proven, efficient and effective modeling approach for capturing the scientific workflows.

due to the complex nature of the scientific domain, current state of implementation of the nmr applications, time constraints, and enormous data requirements, a top-down, iterative, design approach was chosen to capture the scientific workflows. the approach starts by looking at the highest level of the experiment and decomposing the steps in the experiment until a reasonable level of detail has been obtained. in order to capture the complexity of the experiment process, the approach includes three different models, as shown in figure  <dig>  the process consists of the following modeling phases: the context model which provides a structural view of the processes and their sub-processes; the supplier-input-producer-output-consumer  model which highlights the data that are produced and consumed and how it flows during an experiment; and the control-flow model which demonstrates the flow and ordering of the steps in the experiment. each phase consists of a two-step process of design and validation of the model. each phase involves facilitated work-shops with subject matter experts . since the three models provide three different perspectives on the workflow, the development or refinement of one model may result in the identification of missing elements in another modeling phase.

the process of constructing the conceptual level model of the scientific workflow for biomolecular analysis using nmr spectroscopy was performed collaboratively using facilitated workshops with three participants. one participant was an expert in business process modeling and served as the knowledge engineer, the second was a subject matter expert, in our case an expert nmr spectroscopist, and the third participant facilitated the meetings and documented the meeting proceedings. the nmr spectroscopist is a seasoned author on topics in the biomolecular analysis using nmr spectroscopy arena and has expertise in the area of molecular, microbial and structural biology. at the beginning of each workshop, models captured in the previous workshop were validated with our subject matter expert to ensure completeness before advancing to the next step in the modeling process.

a subset of the steps in the workflow for the process of conducting biomolecular analysis using nmr spectroscopy was captured. this subset was sufficient to completely exercise each of the three modeling levels and to express several processes at the finest level of granularity. over the course of four months, we conducted eight workshops with a total duration of fifteen hours. twenty four models were constructed during the workshops, using our process for conceptual-level workflow construction. we constructed the context model, all of the high-level sipoc models for the top level processes, and constructed all but one of the scientific workflows for one of the more-complex sub-processes.

the context model
the first step in the process of determining the nmr experiment workflow model is to create a context model. a context model is a proven tool used in system analysis to identify the scope and boundaries of a system being modeled, while setting the top level of abstraction for capturing the workflow  <cit> . a context model provides a structural perspective on the experiment process by describing the main processes and their sub-processes in a workflow. a single top-level context model is constructed for the system being modeled. due to the complex nature of the experiment, we chose to focus only on the high-level processes in order to gain a common understanding of the scientific domain  <cit> . we modeled the experiment using a hierarchical view of the high-level processes. the hierarchical view of the high-level process is used to demonstrate the super-workflow  and sub-workflow  relationship.

context model components
a context model consists of different kinds of a component called a process. a process in the context model represents a high-level activity that must be done to complete the experiment. the context model for the process of biomolecular analysis using nmr spectroscopy is shown in figure  <dig> 

the context model defines a hierarchy of processes that describes the workflow at the highest level with no explicit ordering. processes are represented by rectangles. only a single root process for each context model is allowed. the root process represents the main experiment being modeled and is typically located at the top of the diagram. the root process in our context model is conduct biomolecular analysis using nmr spectroscopy. top-level processes are those located directly below the root process such as collect sample parameters and analyze sample. each process that has processes connected below it is a parent or super process and the processes that are below the parent process are known as child or sub processes.

there are five separate sub-processes to analyze sample. construct spectrum is the process of converting nmr data describing the bulk magnetic oscillations of the sample to a spectral frequency map of those oscillations. analyze spectrum is the process of identifying the frequencies at which oscillations occur. assign peaks is the process of identifying which nuclei in the sample are responsible for the observed signals. post process sample encompasses a whole host of various processes which are workflow dependent, such as the identification of components of biomolecular secondary structure. validate data/results is the process of confirming that the processed data are accurate.

a two-hour workshop was conducted to capture the context model. an initial process model  <cit>  demonstrating the current state of nmr data analysis was used as the starting point. an initial context model containing the main process of conduct biomolecular analysis using nmr spectroscopy and its five sub-processes was constructed. the analyze sample and deploy result processes were then further decomposed due to the complexity of these two processes. the context model was considered to be complete once the domain expert indicated that all high-level processes had been identified. verification of the completeness was performed via a walk-through of the context model that verified that every step in the experiment was accounted for at the highest level of abstraction. at the conclusion of the workshop, the context model was distributed to all workshop participants who performed a final, independent validation. minor changes were incorporated into the model at the beginning of the subsequent meeting.

the context model identified the areas of the workflow that required further analysis in subsequent workshops to capture the sipoc models and workflow models. the context model also helped us make judgments about whether a process/activity was out of scope for the process of conducting biomolecular analysis using nmr spectroscopy.

the sipoc model
once a complete context model is constructed and modeling participants have gained an understanding of the major processes and their decompositions, the second phase of modeling may begin. in the second phase of modeling, the processes identified in the context model are further detailed with corresponding producers, data inputs and outputs, and consumers using a sipoc model. the sipoc model is based on the supplier-producer-customer  chain, a proven modeling technique used in continuous process improvement  <cit> . the sipoc model was chosen as it allows both data and process flows to be captured  using a single diagram notation. while modeling the scientific workflows, we retained the label "consumer" for the customer link in the spc chain as output in a scientific experiment is typically consumed by a connecting process or entity. sipoc modeling is performed for all processes identified in the context model. any complex processes identified during sipoc modeling may be further refined by applying sipoc modeling to those processes. the goal of the sipoc phase of modeling is to iteratively apply the sipoc model to various processes until a model that is sufficiently low-level as to be modeled using a flow chart is obtained. only the top level of the context model is always included in the sipoc model. the decision as to whether to model lower levels of the context model in the sipoc model is based on the complexity of those levels of the context model.

we decided to model a high-level view of the five top level processes identified in the context model. figure  <dig> shows the sipoc model for the analyze sample top-level process identified in the context modeling phase.

as shown in figure  <dig>  the top level process, analyze sample, collects input data from many types of suppliers. three of the suppliers are other processes which occur elsewhere in the work-flow: collect sample parameters, collect nmr data and segregate control data. other input comes from human sources: the nmr data analyzer process, as well as information deposited in reference literature and the internet. all of this data is required by the analyze sample process to produce a final result, which is composed of both the unrefined, resultant data as well as the refined solution for publication and deposition.

sipoc model components
the sipoc model uses five different kinds of components:

 supplier – an entity that supplies the input to the process. suppliers may be other entities, represented by rectangles or rounded rectangles with arrows flowing outwards towards the process to which the supplier supplies input. the collect sample parameters process is a supplier to the analyze sample process shown in figure  <dig> 

 input – any information, data, or objects supplied to the process, represented by arrows flowing in the direction towards the analyze sample process. sample parameters, nmr data, and standard nmr chemical shifts are inputs consumed by the analyze sample process shown in figure  <dig> 

 process – an activity required to transform an input to an output, represented by a rectangle with arrows flowing into and out of the rectangle. in figure  <dig>  the analyze sample process consumes input from the collect nmr data process, among others, and produces output to be consumed by the deploy result process.

 output – any information, data, or object created by the process, represented by arrows flowing out of the main process being modeled. in figure  <dig>  the analyze sample process has two outputs, finish solution and result data which serve as input to the consumer deploy result process.

 consumer – an entity that uses the output from a process, represented by a rectangle with arrows flowing into the rectangle. in the analyze sample process shown in figure  <dig>  the deploy result process consumes the finished solution and result data outputs from the analyze sample process.

sipoc models were constructed for five top-level processes identified in the context model . the process used to capture the sipoc models for the process of biomolecular analysis using nmr spectroscopy involved three two-hour workshops and one one-hour workshop with workshop participants performing independent reviews of the models produced in a workshop in-between meetings. validation of a sipoc model consisted of verifying that all producer, inputs, outputs, and consumers were captured correctly in the model.

in order to demonstrate the iterative application of the sipoc model, the analyze sample sipoc model was further decomposed into a more-detailed sipoc model. one two-hour workshop and one one-hour validation workshop were conducted to capture the analyze sample-level  <dig> sipoc model. the first workshop began with a validation of the five higher level sipoc models created in the previous workshop. to gain an understanding of the analyze sample process, we constructed a context model for the analyze sample process. we used the context model as a tool to identify all the major sub-processes that compose the analyze sample process. our context model identified the construct spectrum sub-process as needing further decomposition.

the analyze sample context model and analyze sample – level  <dig> sipoc model were validated for completeness during the one-hour validation workshop. validation consisted of a walkthrough that ensured that all supplier and consumer inputs/outputs had been identified for the process. in addition, the model was checked to ensure that all parent sipoc components were captured within all the sub-process sipocs.

the sipoc models proved to be a useful visual tool for providing the team with a common understanding of the process of conducting biomolecular analysis using nmr spectroscopy. once the analyze sample – level  <dig> sipoc model was validated, we proceeded to the next phase in the process to construct the control flows.

the control-flow model
once the major processes of the conceptual model of the workflow have been identified and described using the sipoc model, the third phase in our process is capturing the flow of control of the scientific workflow. we chose to use a notation that closely resembles flow charts to represent our scientific workflows as flow charts are a proven modeling technique used to accurately capture the control flow of processes. in addition, flow chart modeling is a simple and efficient method to identify the activities required to execute a process  <cit> . decomposition of flow charts is possible if the primary process does not provide sufficient detail for the analysis of the process. the lowest decomposition we needed to capture for our conceptual-level view of the process was four levels of decomposition.

the analyze sample process was chosen for decomposition as it includes a significant portion of the complex computations and analysis found in the process of biomolecular structure determination and is a process that would highly benefit from automation. figure  <dig> shows the control flow for the analyze sample process.

the analyze sample process control flow illustrates the iterative decision making characteristic of scientific workflows. the flow starts with a reconstructed spectrum to be analyzed. the spectrum is analyzed for both accuracy  and quality . once the spectrum meets both requirements, the observed peaks are assigned iteratively, often requiring tentative assignments to be fed to later stages of the flow to be confirmed/denied, or even to retreat to prior stages of the flow and collect more data. once all peaks have been assigned, and all post-processing results validated, the control flow ends.

control flow modeling uses the following components:

 start – a circle is used to represent the starting point of the process.

 end – an oval is used to indicate the termination point of the process.

 activity – a task or step required to execute the process, represented by a plain rectangle. some examples of activities used in our workflows are construct spectrum and analyze spectrum.

 external process – a process that is external to the workflow which indicates the control flow of the workflow exits to an external process. represented by a double barred rectangle, one example of an external process shown in figure  <dig> is collect nmr data.

 flow – the sequencing order of the activities of the process, represented by a directed arrow. an example of flows used in our workflows is the connection between the construct spectrum and analyze spectrum activities.

 decision – a decision point in the flow of control where control could go one of two directions, represented by a diamond. the decision identifies the direction in which the flow moves based on the answer to a true/false question. for example, peaks are assigned until the results are validated as shown by the results validated? decision.

three two-hour workshops were conducted to capture the control flow for the analyze sample process. since the validation for the analyze sample context model and analyze sample level  <dig> sipoc models was already complete, we immediately started constructing the control flow for analyze sample based on the sub-processes identified in the analyze sample context model.

in addition, the control flows for the construct spectrum, analyze spectrum, and post process sample processes were captured in the first of the three workshops using the same process we used to construct the analyze sample context model and control flow. validation of the analyze sample, construct spectrum, analyze spectrum, and post process sample control flows were conducted at the beginning of the second workshop. the analyze sample – level  <dig> sipoc model and the process context models served as excellent references to verify that correct information was being captured in the control flows. by studying the inputs and outputs identified in the analyze sample level  <dig> sipoc model, the nmr domain expert was able to identify potential gaps in the lower level control flows. context models were used to verify that all processes were represented in the lower level workflows. the context models served as a tool for determining which sub-processes needed to be decomposed. control flows for seven additional processes were modeled and validated in the two last workshops.

validation of approach
in order to determine the soundness of the approach used to construct conceptual-level workflows for the analysis of biomolecules using nmr spectroscopy, we validated the models produced and the approach used with an outside, highly respected nmr spectroscopist. the validation approach was based on best practices used in systems analysis and design, software engineering, and business process modeling. due to the size of the problem space and the number of models produced, we identified a subset of the models to use during validation of our approach. the construct spectrum process  was chosen because it is the first step in analyze sample parent process and because its sub-processes decompose to a relatively detailed level.

the validation approach involved three steps. first, criteria for validation of the models were developed in the form of a checklist. criteria were chosen in the hopes of identifying potential gaps or errors in the process. the spectroscopist responsible for validation was provided all checklists and we solicited feedback from the spectroscopist to ensure our criteria were complete. second, the documents were provided to the reviewer for independent review. lastly, one three-hour workshop was conducted where the spectroscopist responsible for validation verbally walked through all of the models for the construct spectrum process, checking for completeness and correctness of the models.

validation proceeded in a top-down manner as the workshop began with a brief introduction to the construct spectrum context model. the context model was validated using the validation criteria checklist. the sipoc modeling constructs were explained to the spectroscopist responsible for validation and the sipoc models for the construct spectrum process were validated. lastly, the control flows for the analyze sample, construct spectrum, convert data from time domain to frequency domain, set order for execution of functions, and configure values for functions processes were validated using the validation criteria. the ease of understanding our modeling approach was highlighted when, during the review of the control flows, the spectroscopist responsible for validation spontaneously began constructing a flow chart to represent an additional decision branch for the convert time domain to frequency domain control flow.

only minor issues were identified during the validation workshop. for example, a process name was changed to more accurately describe the functionality provided by the process, the outputs and consumer on the analyze sample sipoc model. the control flow of the analyze sample process was slightly modified to include an additional decision. the nmr spectroscopists agreed that our models served as an accurate representation of the conducting biomolecular analysis using nmr spectroscopy experiment. the full documentation of validated workflows is available at  <cit> .

discussion
at the beginning of the procedure to determine the conceptual workflows for conducting an experiment to determine the structure of a molecule using nmr spectroscopy, the nmr spectroscopist expressed doubt that it would be possible to capture the steps of the experiment in an accurate and complete manner. this doubt arose from an understanding of the complexity of the experiment process, the variability that may occur during an experiment and the intuition required on the part of the scientist conducting the experiment. however, the clarity, correctness, and completeness of the models produced using our approach clearly indicated that the approach was a success.

nonetheless, there were several factors that limited the process developed to capture scientific workflows at the conceptual level. first, the complexity of the process of biomolecular analysis using nmr spectroscopy required that a simple modeling approach be used. an uncomplicated approach and correspondingly straightforward notation are necessary for all participants in the process to understand the nature and details of the workflow steps.

the limited number of nmr scientists imposed another restriction on the development of our approach. based on the number of nmr scientists available in our area, our user base for capturing the workflows was limited to one expert spectroscopist with a second nmr spectroscopist available for validation purposes. because of the nmr scientist's credentials and maturity in the field, we believe we were able to construct a set of reasonable and implementable scientific workflows for the process of conducting biomolecular analysis using nmr spectroscopy.

overall, the results of our process to construct conceptual-level workflows for the experiment of conducting biomolecular analysis using nmr spectroscopy appear to be correct and precise. however, they are not necessarily unique. in analogy to the association rules of addition, there may be many alternate paths to the same correct conclusion. such prejudices of researchers towards pet methodology must be addressed during the validation phase. codifying scientific processes in terms of workflows ultimately allows scientists to share, compare and critique their individual, disparate solutions to common problems. we were able to document numerous processes accurately in a short amount of time. the iterative nature of the process gave us the flexibility to re-use modeling techniques from previous phases, which reduced the time required to capture the models. during the validation of the process, we encountered only minor issues with the models. the nmr spectroscopist responsible for validation agreed that the models provide an accurate visual description of the control flow for conducting biomolecular analysis using nmr spectroscopy experiment. we did observe that when workshops were conducted on a regular basis , the modeling process flowed more smoothly and was more productive, resulting in a greater number of models generated per meeting. for instance, when workshops were held on a weekly basis, we were able to construct twice the number of models than when workshops were held less frequently, since the methodology and models were fresh in our minds. we recommend identifying a subset of models to capture for a process and then, if possible, schedule the meetings at least once a week until the subset of models is constructed.

benefits of the approach
the process of capturing and designing the workflows for the analysis of biomolecules using nmr spectroscopy highlighted several resulting benefits. the modeling process itself caused the nmr spectroscopist to gain a more-comprehensive understanding of the steps of the experiment process and their relationships to one another. the process of determining the context, sipoc, and control flow models permitted the spectroscopist to focus on the experiment process at various levels of abstraction and from various viewpoints, allowing the spectroscopist to gain new understanding of how the steps in the experiment are related. this understanding has the potential to lead to new and hopefully more-efficient approaches to conducting experiments.

the nmr spectroscopist also benefits from the codification of the steps taken to conduct an nmr experiment. currently much of the information about how to construct an experiment for determining the structure of macromolecules using nmr spectroscopy is transferred among spectroscopists verbally or with minimal text in an informal manner. the set of models created can serve as common foundation for spectroscopists to discuss, compare and evaluate their experiment process, as well as serving as a base for teaching new spectroscopists how to conduct an experiment.

codifying the workflow also provides a conceptual foundation for modeling the intermediate, derived data which are produced during biomolecular analysis. having such a data model in place is critical for the aforementioned goal of connjur, to provide an integrated desktop environment for biomolecular nmr data processing. this, in turn, will allow the workflow to be expressed intuitively in the design of the integration application and thereby allowing the application to convey the experimental methodology to the novice spectroscopist.

the ease with which the models are understood makes the approach straightforward to learn and use by nmr spectroscopists. in the case of the validating spectroscopist, that spectroscopist started constructing control flow models with only forty-five minutes exposure to the overall approach. in addition, the minimal amount of rework required for the validated models demonstrates that our models accurately represent the conducting biomolecular analysis using nmr spectroscopy experiment.

the simplicity of the models and the iterative approach that focuses on control flow used in our approach allows precise models to be constructed relatively quickly. in comparison to a similar effort, participatory workflow analysis  <cit> , our models contain more detail about the experiment and provide a high-level view of the entire process, including activities, inputs, outputs, suppliers and consumers.

CONCLUSIONS
the approach to the creation of conceptual scientific workflows described in this paper uses a broad model intended to encompass the entire experiment process. the goal is to accurately model the workflow of an nmr experiment which contains the complexity representative of other scientific experiments. based on our experience with modeling the intricate experiment of biomolecular analysis using nmr spectroscopy, we believe that our approach can be used by the broad audience of the general scientific community.

our future work in the area of conceptual modeling of scientific workflows has two main directions. one topic for further investigation is the implementation of the existing workflow models for the process of conducting biomolecular analysis. this implementation could be carried out using a scientific workflow management system, such as kepler  <cit> , taverna  <cit>  or triana  <cit> . once a system is chosen, conceptual models would be converted into corresponding implementation models, starting with the decomposed analyze sample process. uml  <dig>  activity diagrams would allow the modeling of both the control flow and data flow in one model diagram and uml is the de facto industry standard for software requirements. in addition, conceptual-level workflows can easily be converted to uml  <dig>  notation.

a second area of investigation is the application of the process to the modeling of scientific workflows in other domains. the application of our process to domains such as biochemistry, chemistry, and biology would demonstrate the utility of our approach across a range of scientific workflows.

additional areas of investigation would be workflow sharing, collaboration, connectivity with instruments and data stores and the design of user interfaces. the use of the workflow for aiding students learning the nmr experiment process is also being explored.

