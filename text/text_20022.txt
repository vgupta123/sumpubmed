BACKGROUND
the potential of high-throughput sequencing as a tool for exploring biological diversity is great, but so too are the challenges that arise in its analysis. these technologies have made possible the characterization of very rare genotypes in heterogeneous populations of dna at low cost. but when applied to a metagenomic sample, the resulting raw data consist of an unknown mixture of genotypes that are convolved with errors introduced during amplification and sequencing.

there are two broad approaches to high-throughput sequencing of metagenomes: in amplicon sequencing  a pool of dna for sequencing is produced by using pcr to amplify all the variant sequences in a sample that begin and end with a chosen pair of primers  <cit> , frequently targeting hypervariable regions of the 16s ribosomal rna gene  <cit> ; in de novo genome assembly total dna is sequenced without amplification and reads are clustered into “species bins”, each providing the material for genome assembly by shotgun methods .

by trading off a broad survey of gene content for greater sequencing depth at the sampled loci, amplicon sequencing has the potential to detect the rarest members of the sampled community, but errors interfere more profoundly. unlike genome assembly projects, where one needs only to determine the consensus base at each locus or decide whether a snp is present in a population, the space of possible distributions for the sample genotypes and frequencies is effectively infinite. as a result, ambiguities in genome projects can usually be resolved by increasing the amount of data, whereas increasing depth  increases the number of both real and error-containing sequences and makes the challenge of distinguishing minority variants from errors only greater under amplicon sequencing. greater depth therefore calls for progressively more sophisticated methods of analysis.

the analysis of amplicon sequence data typically begins with the construction of otus , clusters of sequences that are within a cutoff in hamming distance from one another. otus serve to collapse the complete set of sequences into a smaller collection of representative sequences – one for each otu – and corresponding abundances based on the number of reads falling within each cluster. otus were developed as a tool for classifying microbial species, but have also been repurposed to the task of correcting errors; the sequences within an otu are typically interpreted as a taxonomic grouping without specifying whether the variation within an otu represents errors or real diversity on a finer scale than that chosen to define the otu. if the scale of the noise is smaller than that of the clusters, then the construction of otus will appropriately group error-containing sequences together with their true genotype. however, as sequencing depth increases, low probability errors outside the otu radius will start to appear, and will be incorrectly assigned to their own otu. early studies using this approach on high-throughput metagenome data sets reported large numbers of low-abundance, previously unobserved genotypes that were collectively dubbed the rare biosphere <cit> . later, analyses of control data sets indicated that the diversity estimates in such studies tends to be highly inflated  <cit>  and that results may lack reproducibility  <cit> . the dual purpose of otus for correcting errors and for taxonomic grouping is appropriate when the diversity is being sampled at a coarse level, e.g. the frequency of different phlya. however, when probing finer-scale diversity, otu methods have intrinsically high false positive and false negative rates: they both overestimate diversity when there exist errors larger than the otu-defining cutoff and cannot resolve real diversity at a scale finer than that  cutoff.

in response, a variety of approaches to disentangling errors from actual genetic variation have been proposed recently  <cit> . these include multiple rounds of otu clustering with different hierarchical methods  <cit> , utilizing sequence abundance information implicitly by starting new clusters with common sequences  <cit> , and replacing otu clustering with an expectation-maximization  approach  <cit> . accuracy has steadily improved, but all methods still fall short of maximizing the information acquired from metagenome data sets.

we believe that the way forward is to model the error process and evaluate the validity of individual sequences in the context of the full metagenomic data set, crucially including the abundances  corresponding to each sequence. major progress in this direction has been made recently by quince et al <cit> . in the specific context of pyrosequencing, often used for metagenomics, strings of the same nucleotide  are problematic, and quince et al incorporated a model of the distribution of homopolymer light intensities into an expectation-maximization  algorithm, pyronoise, which infers the homopolymer lengths of sequencing reads  <cit>  . later, quince et al released ampliconnoise, an extension of pyronoise that includes rates of single-nucleotide substitution errors obtained from training data  <cit>  . these methods were shown to more accurately infer the underlying sample genotypes than other approaches, demonstrating the worth of explicitly modeling errors. however, the methods of quince et al. have several shortcomings that we would like to rectify:  as the size of sequence data sets grows, ampliconnoise becomes too slow to use in many applications;  estimation of error rates relies on the existence of training data specific to the pcr and sequencing chemistries used;  differentiation of fine-scale diversity is limited because read abundances are not fully utilized when calculating the distance between sequences and clusters;  the parameters that determine how conservative the algorithm is in inferring real diversity are ad hoc and cannot be tuned without experiment-specific training data.

we build on the error-modeling approach pioneered in ampliconnoise by developing a novel algorithm, dada, to denoise metagenomic amplicon sequence data that addresses the concerns raised above  <cit> . we start with a parametric statistical model of substitution errors. we incorporate this error model into a divisive hierarchical clustering algorithm that groups error-containing reads into clusters consistent with being derived from a single sample genotype. finally we couple this clustering algorithm with the inference of the error parameters from the clustered data, and perform each step in alternation until both converge. this method is presented below, and is shown to outperform previous methods in both speed and accuracy on several control data sets.

RESULTS
model and algorithm
we introduce a first-order model of the error process by assuming  each sequence read originates from a distinct dna molecule in the sample, and therefore that the presence of errors on different reads are statistically independent, and  errors on different sites of the same read are also statistically independent events. the independence of errors across different reads relies on the independence of the pcr replication histories of those reads, a condition that holds when the total number of reads is significantly smaller than the total number of dna molecules present in the initial environmental sample and there are no strong amplification biases for sequences with errors.

under these conditions, the numbers of reads  of the error-containing sequences derived from a sample genotype follow the multinomial distribution, and the abundance r of each particular sequence is binomially distributed  with a probability λdetermined by the particular combination of errors in that sequence and a number of trials ρ given by the total number of reads of its sample genotype. these facts allow us to establish two statistics to evaluate the hypothesis that a collection of sequencing reads derives from a single sample genotype. the abundance p-value determines when there are too many reads of the same sequence to be consistent with the error model, and the read p-value determines when a sequencing read is too far away to be an error from an inferred sample genotype.

these statistics serve as the basis of a sequence-clustering algorithm in which  reads are assigned to clusters,  a putative sample genotype is inferred for each cluster,  reads are reassigned to the cluster for which they are most likely to have resulted as errors from the inferred sample genotype,  the two p-value statistics are computed given the inferred sample genotypes and the clustering of the sequences  additional clusters are created if the clustering is statistically inconsistent with the error model .

the full algorithm  combines this probabilistic sequence clustering with the estimation of substitution-error probabilities that are used to compute the p-values. the algorithm begins by assuming all reads derive from a single sample genotype and estimates initial error probabilities given this assumption. it then alternates between clustering the reads and re-estimating the error probabilities until it converges to a final set of mutually consistent clusters and error probabilities.

the p-values
we introduce two statistics for deciding that particular sequences did not arise from errors. the read p-value is the probability of having observed at least one read of a sequence that is as improbable as the most improbable sequence amongst the observed reads. this statistic treats each read as a separate event  and therefore does not utilize sequence abundance. it results in a hard cutoff, λ∗, below which reads are decided not to be errors by dada. this cutoff is set by the choice of a significance threshold Ωr, the probability of having observed at least one read more unlikely than λ∗. the abundance p-value, which is computed for each sequence individually, is the probability of having observed at least as many identical reads as we did of each sequence . the conservativeness of this measure is set by a significance threshold Ωa, the probability that at least one sequence should have been as overabundant as the most overabundant sequence. the abundance p-value gives dada significantly greater sensitivity than previous methods.

for both the real and simulated data, the abundance p-value does a good job of tracking the form of the abundances of the errors, and the read p-value sits to the right of all observed data. for the real data, a small number of errors sit on or above the abundance discrimination line. such errors were individually not expected to be observed at all, but ended up with a small number of reads larger than one. this pattern was observed across many clusters, and we believe that it reveals the presence of small violations of our assumption of the independence between reads. in particular, in a regime where the ratio of the number of error-free reads to the number of dna molecules in the sample that act as the basis for amplification is of order one or larger, then errors during early stages of pcr may be sampled multiple times in the sequence data. as a result, the distribution for the number of reads of these errors may fall off much more slowly than what our model suggests. to deal with this effect in this paper, we lowered the Ωathreshold using an ad hoc method  to prevent excess false positives. doing so did not affect dada’s ability to detect the genuine diversity in the data analyzed in this paper, which was typical of the data analyzed in many microbial metagenomics studies, but the sensitivity that is lost by using very small values of Ωa could be limiting for samples with even finer-scale diversity. further analytics that model pcr as a branching process improve this current ad hoc threshold .

treatment of insertions and deletions
dada does not attempt to explicitly model the indel error processes, and indels do not contribute to the determination of whether sequences are related to each other via errors. instead, sequences are aligned to each putative sample genotype, and are assigned to clusters on the basis of substitutions. during the computation of p-values, we sum together the reads of sequences within each cluster that have the same set of substitutions . the number of reads of each of these indel families, rather than those of the raw sequences, are the basis of our p-values .

treating indels in this way does not affect the accuracy of dada for the test data sets analyzed here, as the sample genotypes all differed from each other by at least one substitution, and these provided enough information for dada to distinguish between them. however, dada cannot distinguish between sequences that differ only by indels. in such cases, if the amplicons being denoised are coding regions, frame information should be used for making decisions about whether particular indels are real or errors, but in order to denoise non-coding regions with pure indel diversity, dada is not sufficient in its current form.

preclustering
prior to our probabilistic sequence clustering we divided the raw data into coarse 3% single-linkage clusters , subsets for which each sequence is ≤3% from at least one other sequence in its cluster and >3%from all sequences in other clusters. due to its speed, we employed the esprit algorithm for this task  <cit> . single-linkage’s propensity for chaining was advantageous in this circumstance, as all error-containing sequences are very likely to be in the same cluster as their originating sample genotypes; for a sample genotype and one of its errors to end up in different clusters, the error would have to be ≥3% from the nearest error clustered with that genotype, corresponding to a large gap in the error cloud, which is unlikely under our error model.

clustering
each precluster is partitioned into sets of sequences that are conjectured to contain all errors arising from different sample genotypes. this partition is initialized to a single cluster containing all sequences. two procedures then alternate. first, the indel family most unlikely to have resulted from errors is split off into a new cluster. sequences then move between clusters based on the probability that they were generated as errors by each one, and the consensus sequence for each cluster is updated until there are no remaining reassignments that can improve the probability of the data. this second step is analogous to the assignment and update steps of standard k-means clustering. this alternation stops when the partition of the sequences fits with the current error model at the significance levels provided by the user.

accuracy
we evaluated the accuracy of dada by denoising three of the data sets in q <dig> used to demonstrate ampliconnoise’s accuracy relative to the earlier slp and denoiser algorithms. these data are derived from mixtures of known clones that were amplified together and sequenced on the  <dig> platform, and consisted of different hypervariable regions of the 16s rna subunit of bacterial ribosomes , which are commonly used as a proxy for phylogenetic diversity in metagenomic studies  <cit> . two of the data sets, divergent and artificial, with  <dig>  and  <dig>  reads, were sequenced with the gs-flx chemistry and were truncated at  <dig> nucleotides. they were constructed by amplifying the v <dig> region of the 16s rrna gene from  <dig> and  <dig> clones, respectively, isolated from lake water. the divergent clones were mixed in equal proportions and are separated from each other by a minimum nucleotide divergence of 7%, while the artificial clones were mixed in abundances that span several orders of magnitude, with some of the clones differing by a single snp. the other data set, titanium, with  <dig>  reads, was sequenced with the newer titanium chemistry and was truncated at  <dig> nucleotides. it contains v4- <dig> 16s rrna genes from  <dig> clones isolated from arctic soil with varying abundance and genetic distances, similar to the artificial set.

all data sets had undergone filtering of reads deemed to be of low quality prior to application of ampliconnoise in q <dig>  so for purposes of comparison, we denoised the same set of filtered reads. the presence of a small number of low-quality reads in  <dig> data has been previously demonstrated  <cit> , and as we do not expect these to be well described by our error model, we encourage the use of such quality filtering before applying dada to non-test data. as slp and denoiser were already demonstrated to be less accurate than ampliconnoise on these data, we include here dada’s performance only relative to that of ampliconnoise. there were six other data sets presented in q <dig> of v <dig> regions from a gut microbial community, but these had such an overwhelming number of chimeric sequences , which neither dada nor ampliconnoise attempts to address, that we opted not to include these data sets in our analysis.

tuning algorithmic sensitivity
dada employs two tunable parameters that determine how conservative or liberal the algorithm is to be in deciding whether particular sequences could have resulted from errors: Ωa, and Ωr, the significance levels for its abundance and read p-values. decisions about singletons, the sequences represented by a single read, depend on Ωr, whereas decisions about sequences with several reads depend on Ωa. the two values may be tuned independently to match the priority being placed on capturing the rarest and more common diversity.

due to early-stage pcr effects discussed above, it was necessary to use Ωa significance levels lower than typical values. in order to select such values, we first performed a loose clustering of each data set with larger values of Ωa and Ωr and then made histograms of the Ωathresholds that would be required for each cluster to be reabsorbed into some other cluster . if there are errors with moderate statistical deviations from our model, we expect that these will show up as a tail of increasingly small p-values that will disappear smoothly as we lower the Ωa threshold. thus, we looked for the first large gap in these histograms that would suggest all such model departures had been captured. such a gap occurs at Ωa=10− <dig> for the divergent data, Ωa=10− <dig> for the artificial data, and Ωa=10−100for the titanium data. we used these values in the analysis that follows, but also clustered all three data sets with Ωa=10−100and found that the results were unchanged . this suggests that Ωa=10−100is a reasonable default value to use when clustering diversity at this scale, even though higher resolution may be achieved by the method outlined above. for non-test metagenome data that is more diverse and less oversampled, we have seen evidence that using much larger values of Ωa may be possible without compromised accuracy, but in such cases it is always advisable to make histograms of the type above to ensure that there is not an excess of clusters that would vanish if Ωa were lowered slightly.

we did not observe any significant departures in these data from our model that would affect the read p-values, and it was therefore possible to maintain the interpretation of Ωras a significance threshold. as a result, for these data, which contain ≤ <dig> preclusters that were clustered separately by dada, we set Ωr=10− <dig> so that the probability of having a false positive would be ≤5%for each data set.

false negatives and false positives
the purpose of dada  is the inference of the genotypes present in the underlying sample from a set of noisy  sequencing reads. there are two types of errors that such an algorithm can make: false positives in which a sample genotype is inferred that was not present in the sample, and false negatives in which the algorithm fails to infer a sample genotype that was present in the sequencing reads. the tradeoff between false positives and false negatives in the two algorithms can be controlled by the algorithmic parameters, depending on which type of error presents more of a problem to the user.

we present, in table  <dig>  a comparison of the false positives and false negatives for dada and ampliconnoise applied to the control data sets described above. note, however, one important detail: these algorithms are designed to remediate substitution and indel errors, not all possible errors. in particular, we found that contaminants, chimeras, and pathological homopolymer errors contributed to these sequencing data sets. using ad hoc methods, discussed in appendix appendix 1: chimeras, contaminants, and missing or incorrect sanger sequences, we accounted for these additional error sources, and did not penalize either algorithm for them.

dada is more accurate in its inference of the sample genotypes than is ampliconnoise on every data set. the difference is especially strong among false negatives, where dada successfully identifies virtually all sample genotypes; dada’s only two false negatives, both in the artificial set, result from pathological alignment issues between sequences that differ only in the last two bases.

the differences in the nature of the false positives and negatives made by dada and ampliconnoise are shown in figure  <dig>  dada produces one false positive in the artificial data: a sequence with  <dig> reads that is a single substitution from a sample genotype with only  <dig> reads. due to its vast abundance, this is unlikely to be an error, and we speculate it may represent a polymorphism that arose early in the growth of this clone. ampliconnoise produces eight false positives in the artificial data: all sequences with 1- <dig> reads that are  1- <dig> substitutions away from clusters with a few hundreds of reads. although these sequences were not atypical errors as judged by dada due to their low abundances, ampliconnoise calls them real as a result of setting a narrow error radius that is needed to prevent additional false negatives. the differences between the errors made by the two algorithms is less clear in the titanium data set, but dada outperforms ampliconnoise in both fps and fns. we included ampliconnoise’s results for the titanium data set for both parameter settings included in q11: the σ=. <dig> clustering , which produces only four false negatives, leads to many false positives similar in nature to those of the artificial clustering – low abundances and a small number of substitutions away from large clusters; the σ=. <dig>  clustering produces many fewer false positives but misses nine sample genotypes.

speed
we evaluated the speed of dada applied to the artificial data, which was used to profile ampliconnoise in q <dig> . esprit was run on a single core of an amd phenom ii  <dig> ghz running ubuntu and dada was run on a macbook pro with an intel core  <dig> duo  <dig>  ghz.

the cpu times for a few significant subroutines within dada and esprit as well as their total cpu times.

dada is currently written in matlab, but sequence alignments and the construction of indel families were bottlenecks that we reimplemented as mex  c programs. the majority of the time to run our denoising pipeline on the artificial data set is spent on esprit’s performance of pairwise alignments during the single-linkage pre-clustering step . a newer version of esprit promises to be released soon that may dramatically lower this time  <cit> . if additional speedups are needed as data sets grow, it should be possible to replace the global alignments of esprit by banded alignments that would be guaranteed to produce the same clusters if the width of the band is equal to the cluster radius, and would have have roughly linear  rather than quadratic, running time. nonetheless, for these data dada already gives a 60-fold speedup over ampliconnoise, making jobs that required a 64-core cluster to run ampliconnoise appropriate for a laptop running dada.

as read lengths continue to grow, we expect the time complexity of dada to be affected in two primary ways. first, because the complexity of the needleman-wunsch alignment algorithm used by both dada and esprit scales with the product of the lengths of the input sequences  <cit>  there will be a quadratic slowdown with increasing read length unless heuristics are employed. on data sets comparable to those analyzed here, alignments consume the majority of algorithmic time and this scaling will dominate in the near future. second, our current implementation for computing read p-values has both time and space complexity that grows very rapidly with read length ). this was not strongly limiting for these data, but in case it should become so as reads become longer, we have explored the use of a continuous approximation for the error probabilities that may alleviate this problem.

pcr substitution probabilities: symmetries and nearest-neighbor context-dependence
dada not only infers sample genotypes, it also infers the substitution error probabilities caused by the amplification and sequencing processes. the substitution error probabilities inferred by dada for all three data sets exhibit an approximate symmetry under complementation of the two bases involved. for example, the a→g probability is close to the t→c probability. this symmetry is expected if substitution errors predominantly arise during pcr amplification because substitution errors during pcr can be the result of either of two different mis-pairing events , and complementary substitution errors share causal mis-pairing events . as it was not imposed, and the identities of the original genotypes were not known to the algorithm, the symmetry is a highly non-trivial check on dada’s ability to learn error probabilities without training data. additionally, the inferred substitution probabilities were similar across the data sets, and especially so between divergent and artificial, which were generated with the same pcr protocols.

we also found that the nearest-neighbor nucleotide context affects the probability of substitution errors. we therefore introduced context-dependent substitution probabilities into dada, allowing for dependence on the nucleotides immediately preceding and following the substituted nucleotide. such probabilities are expected to exist in reverse complementary pairs for the same reason given for the context-independent case ; the lir→ljr probability is expected to be similar to the lir¯→ljr¯ probability where lir¯ denotes the reverse complement of lir. the degree of symmetry in the inferred probabilities, both context-independent and context-dependent, are shown in figure  <dig> for all three data sets.

the magnitude of context-dependence for these data was moderate  as seen in the spread of points along the diagonal in figure 6d,e,f . as a result, maintaining separate probabilities for different contexts did not affect the inferred sample genotypes. nonetheless, that dada was robust to significant variation in its parameters is a strong check on the stability of its sample inference.

we have worked with data for which context-dependence is large and has a strong effect on clustering. therefore, we leave use of context-dependence as an optional feature of dada, either as a consistency check, or when justified by the amount and nature of the data. but a caution is in order: with modest sized data sets, or if the sequences are too similar, use of the context-dependent rates could result in over-fitting and calling too many errors. however if this did occur, the expected complementarity symmetry of the inferred error probabilities would be unlikely to obtain unless the sequences were read in both directions.

discussion
dada explicitly incorporates read abundance when deciding whether sequences are genuine or errors; if there are many identical reads of a sequence, dada will be more likely to infer an underlying sample genotype, even if individually those reads would be consistent with being an error from a nearby genotype. furthermore, dada implicitly assumes, via the error model, that reads near highly abundant sequences are far more likely to be errors. in contrast, previous methods have typically treated each read independently. ampliconnoise partially incorporates abundance by weighting the prior probability that a read belongs to a cluster by the frequency of that cluster, but this is weaker than dada, where dependence on cluster size shows up in a binomial coefficient , especially for high-abundance errors. by using both sequence identity and abundance in this way, dada is able to disentangle real diversity from errors at finer scales than previous methods, even when tuned to be very conservative.

however, full incorporation of abundance information makes dada sensitive to early-stage pcr effects and the mis-estimation of error probabilities. the problem of early-stage effects is particularly pronounced in these data: when clustered with Ωa=Ωr=10− <dig>  the artificial data produces  <dig> false positives . the majority of these sequences have 2- <dig> reads and 2- <dig> errors. such problems would be typical of moderately oversampled pcr, the regime in which initial sample molecules are typically sampled multiple times, allowing a single error during early stages to show up in more than one read.

in lieu of an abundance statistic that appropriately compensates for this affect, we deal with this problem by lowering the sensitivity of the algorithm by tuning down Ωa. further, because the probability given to each sequence scales as the error probabilities to the power of the number of reads , if certain error parameters are larger than estimated in certain contexts, then the statistical significance of an error with many reads can be substantially overestimated. this problem gets progressively worse for deeper data sets, as all one-away errors begin to take on many reads. in anticipation of this problem, we have introduced nearest-neighbor context-dependence of error rates . these had no impact on the final clustering for the test data presented, but in other data sets with larger context-dependent effects, we found a reduction in diversity estimates when context-dependence was included .

dada is a divisive hierarchical clustering algorithm: all sequences are assigned to a single cluster that is subdivided until the clustering fits an error model. previous methods, including ampliconnoise and simple otu-clustering, have predominantly taken the opposite, agglomerative approach, which starts with too many clusters and merges them until some condition is met. this gives dada a practical advantage, as the computational and space requirements  scale with the square of the initial number of clusters  <cit> . the typical problem of divisive methods – that the number of possible splittings is too large – is handled in dada by seeding new clusters with sequences that are identified as not being errors and allowing other sequences, e.g. errors associated with the new clusters, to relocate if they become more probable by doing so.

finally, dada uses unsupervised learning to acquire error probabilities from the data that it is given. as pcr protocols vary in their choice of polymerase and number of rounds, these parameters vary by data set, perhaps greatly. this makes the universality of dada’s approach especially attractive, and will be important as new sequencing methods come into use such as longer read-length and paired-end illumina that commonly make substitution as well as indel errors  <cit> . while it now relies on training data to establish error parameters, ampliconnoise could be embedded in the same procedure of estimating error probabilities after each successive round of clustering, but this would multiply the computation requirements by a factor of the number of rounds of re-estimation, compounding the problem of its slower speed.

CONCLUSIONS
otus serve as a rough analogue for microbes of the more clearly defined taxonomic groups of higher organisms. however, the repurposing of the otu concept to the problem of inferring sample genotypes from error-prone metagenomic sequence data has serious and inherent shortcomings. the absence of an error model causes estimates of diversity, especially species richness, to depend strongly on experimental variables such as the size of the data set, the length of the region sequenced, and the details of the pcr/sequencing chemistry. these shortcomings are not amenable to simple fixes; it is not possible to separate real diversity from errors using an otu approach when the diversity and the errors exist at similar scales , as is the case in many metagenomic studies. pyronoise and ampliconnoise have demonstrated the usefulness of denoising sequence data with statistical, physically-based error models. these methods are based on the classical statistical technique of expectation-maximization. we have presented an alternate approach, dada, which is more targeted to the particular task of producing conservative estimates of diversity from noisy sequence data. it is much faster and more capable of resolving fine-scale diversity while maintaining a lower false positive rate.

we did not achieve our goal of complete freedom from ad hoc parameters in this work. even though Ωa, our input parameter, has a simple probabilistic meaning that is data set independent, there are corrections to our pcr model, and as a result Ωa takes on an ad hoc quality in this analysis. nonetheless, Ωacan be coarsely tuned from the data itself in the way shown. alternatively, for conservative diversity estimates, Ωa may be set to very small values , and the resolution of the algorithm may be directly quantified. dada not only guesses what is there, but knows what would have been missed if it were present, making Ωa ad hoc but not arbitrary.

much work remains to be done, and it is not yet clear how the algorithms will fare with extremely rich fine-scale diversity as occurs for the antibody repertoire of b-cells and t-cells of the human immune system  <cit> . dada must be equipped with statistics that correctly describe the abundance distribution of sequencing errors when a realistic model of pcr is used in which some reads are the result of shared lineages. more sophisticated methods for chimera detection that explicitly parameterize the chimera formation process analogously to the substitution and indel processes are also needed. finally, these methods must be fully adapted and tested on sequencing platforms other than roche’s  <dig> 

