BACKGROUND
bottom-up attention has been widely studied in physiology, psychology, neural science and computer vision. it is usually attributed to early vision, such as to the manner by which the primary visual cortex  encodes low-level features and forms a saliency map. numerous studies have explored theories and computational models to provide an efficient input to the saliency detection. for example, treisman and gelade  <cit>  developed the feature integration theory to explain how primary visual features are processed and represented with separate feature maps and are later integrated into saliency maps. koch and ullman  <cit>  proposed a biologically plausible computational framework to model the process that attracts focus-of-attention to the most conspicuous areas. several other hypotheses were tested, including a saliency map created in the primary visual cortex that does not need any feature combination by zhaoping li  <cit> , and those models motivated by an imitation of information processing mechanisms in the v <dig>  <cit> . in addition, the effects of overcomplete bases on encoding a bottom-up saliency map is of current interest  <cit> . while more and more neurobiological properties of the v <dig> have been accepted, the way they modulate saliency detection remains unclear  <cit> . models limited to simple cell simulations cannot sufficiently satisfy neurobiological constraints  <cit> . the receptive fields of simple cells are too small to place competing stimuli inside them, which is an important condition for attentional experiments  <cit> . thus, attentional modulation is most prominent in higher cortical areas, where receptive fields are wide and where several objects can compete inside a single receptive field. simple cells alone contribute less strongly to selective attention, especially for complex tasks . the involvement of simple cells with other mechanisms in the primary visual cortex, such as complex cells or synchronised oscillation, to account for the pre-attentive process seems to be more biologically plausible. the effects of other mechanisms beyond simple cells on bottom-up attention are worth exploring.

recently, deep learning by a hierarchical network has been thoroughly researched and successfully used in object recognition  <cit> . the approach is regarded as an ideal model to simulate the information representation of humans. it provides a hierarchical learning mechanism that gains invariant representation, and can be generalised to include other coding strategies.

this paper aims to introduce deep network into saliency detection and to investigate the influences of different coding factors. we discuss two central questions and a series of related ones. first, does invariant representation in the v <dig> affect bottom-up attention? if so, what is the optimal network structure to obtain invariance? second, what is the effect of overcomplete representation on saliency detection? another issue is the concomitant feature combination on such a massive scale. the overall scheme of our model is given in figure  <dig> 

traditional bottom-up attention models vs. our model
bottom-up attention models extract multi-dimensional features from an image and combine these features into a saliency map where the most salient object will be perceived.

in the feature extraction stage, computational models motivated by imitation of the primary visual cortex often use gabor filters to extract orientation information at different scales. properties of gabor filters resemble simple cells’ receptive fields and can provide input to the bottom-up saliency map. similar methods also use gaussian pyramids  <cit> , fourier transformation, or wavelets decomposition  <cit>  to extract features similar to the responses of cells. one of representative models proposed by itti et al.  <cit>  adopted gaussian pyramids to extract color, intensity, and orientation features at different levels. grigorescu et al.  <cit>  simulated complex cells and nonclassical receptive field inhibition to detect salient contours. by pooling the responses of two simple cells with orthogonal phases, complex cells are insensitive to exact positions of edges. nonclassical receptive field inhibition benefits by suppressing homogeneous texture and thus salient contours are perceived easily.

our model differs from these traditional attention models in several aspects. first, we learn overcomplete basis sets from natural images to extract features for saliency detection, while they directly use gabor function or other wavelets as filters. second, we design a hierarchical architecture to learn some invariance, while most attention models use a single layer to perform filtering and thus do not consider any invariance except the invariance to positions. the invariance to positions is obtained in some models  <cit>  by summing energies of a pair of gabor functions with orthogonal phases. our model learns a broader range of invariance by first obtaining the overcomplete topological basis set and then pooling the responses of topological bases in the neighborhood. third, our model can detect global salient structures besides local salient points. these two different kinds of saliency detection are related with the definitions of saliency in space-based attention and object-based attention.

computationally, an explicit representation of saliency in most models implements centre-surround differences. a type of local spatial selection is presumed to be necessary for preattentive feature detection  <cit> . neurons in the retina, lateral geniculate nucleus, and in the early visual cortical areas are tuned to local contrast such as intensity contrast and colour opponency  <cit> . for example, the response of a retinal neuron tuned to the intensity of the centre-surround contrast can be computed by convolving the luminance channel of the input image by a difference of gaussians  filter. another view of saliency is provided with a global structure rather than with local points. typically, gestalt psychologists defined saliency as whether a structure respects certain perceptual organisation rules such as proximity, good continuity, and closure  <cit> . once local primitives form a structure satisfying these rules, they will be perceived as a whole. in the experiments, we tested our model using two kinds of saliency detection tasks.

v <dig> coding models
coding models simulating information processing mechanism in the v <dig> are often motivated by an imitation of the properties of simple cells and complex cells. the gabor function is regarded as best describing the classical models of simple cells and complex cells. a single gabor wavelet is similar to the receptive field of a simple cell. the square sum of the responses of a pair of gabor wavelets with orthogonal phases is similar to that of a complex cell.

however, this classical picture is incomplete. cells in v <dig> show much more receptive field properties compared with what this simple model can explain. for example, they show end-inhibition  <cit>  and over-representation of the orientation and frequency  <cit> .

several coding models learned statistical properties from natural images and developed receptive field properties that match those of cortical cells to describe the complete picture. hyvarinen and hoyer proposed the independent subspace analysis   <cit>  and the topographic independent component analysis   <cit> . both models can extract the phase invariant and shift invariant features similar to the responses of complex cells. most recently, deep learning  <cit>  was deeply researched and successfully used in object recognition. wang et al.  <cit>  developed a more computationally efficient model based on pairwise cumulant based methods for independent component analysis . it captured the topological relationships by the pairwise cumulant to obtain results similar to isa and tica with fast convergence. obtaining invariance by pooling is not new for object recognition. some works discussed advantages and disadvantages of different pooling operations  <cit> , but this is beyond our topic here.

overcomplete representation is another important property in the primary visual cortex. despite our recognition of its usefulness in early vision, we have not fully understood its role in forming a saliency map  <cit> . information processing in v <dig> provides input to all subsequent cortical areas, to fully represent a complex scene and satisfy different needs. to describe all features including structure and details at different levels, the amount of basis vectors should be large. several coding models considering overcomplete basis sets have been proposed, such as tica and opcica . opcica is successfully used in object recognition. recently, some saliency detection models using matrix decomposition learned overcomplete bases from color images  <cit> . although it uses sparse coding and overcomplete bases, it is less biologically motivated and more mathematically implemented.

RESULTS
to investigate the influences of different coding factors on bottom-up attention, we compared the results of two kinds of datasets: synthetic images widely used in cognitive psychological experiments on visual attention and natural images. the experimental environment was simulated in matlab that ran under intel core i <dig>  <dig>  ghz cpu. the details of the training and testing processes are summarized as follows:

training
 <dig>  obtaine  <dig>  image patches of 16× <dig> from the training dataset and preprocess them with whitening and dimension reduction.

 <dig>  initialize the basis set to be a random matrix and orthogonalize it.

 <dig>  update the basis set according to the rule defined in the pcica algorithm.

in training, we used the grey image dataseta, which is the standard dataset used in ica and in sparse coding models to learn an overcomplete basis set by the pcica.

testing
 <dig>  using the learnt overcomplete topological bases by the pcica, we extracted primitive features by convolving an input image with each filter.

 <dig>  the outputs in step  were then rectified by the sigmoid and the absolute function.

 <dig>  the primitive features, when in step , were pooled and refined to form invariant features descriptors. as the pcica algorithm has organized the bases into a topographic array, pooling is defined in a neighbourhood by subsampling. that is, horizontal and vertical intervals between two neighbourhoods/pools are constants. the pooling in this study was implemented by the square root of the sum of the squares of those units belonging to the same pool. refinement was implemented by computing the similarity  between pairs of filter responses and then selecting the ones whose similarity exceeded the threshold.for the model without invariant feature coding, this pooling was not needed because the conspicuous maps were obtained by directly performing dog suppression on the feature maps from step .

 <dig>  for the model with invariant feature coding, the conspicuous map was obtained by performing surround inhibition on each invariant feature map when in step . the surround inhibition was implemented as a convolution of the invariant feature with the dog .

 <dig>  the conspicuous maps were combined into a final saliency map by iteration strategy . when the number of conspicuous maps was large, we used the k-means algorithm to organise the maps into n clusters, iterated each cluster centre according to formula , and then obtained the saliency map by summing up the n iterated results.

we also build a fully connected network  and a randomly connected network  for a more detailed comparison.

the optimal network structure for invariant representation
for experiments in this part, we learned  <dig> filters of size 16× <dig> by pcica. the filter size was changed to 8× <dig>  12× <dig> and 20× <dig> when we compared the models performances under varying layer  <dig> rf sizes. then we got  <dig> invariant features by pooling filters in 5× <dig> neighborhoods over the 14× <dig> topographic array. two neighborhoods are overlapped by two filters both horizontally and vertically. the neighborhood size was changed to 3× <dig> and 7× <dig> when we compared the models performances under varying layer  <dig> rf sizes.

to make our results comparable to those of the model by itti and koch , we only selected an orientation channel for salience tool to form saliency maps. salience tool used the default parameters setting of  <dig> gabor filters at  <dig> scales,  <dig> orientations, and  <dig> phases. it produces  <dig> feature maps .

the first experiment is similar to the ”visual search” tasks designed by treisman and gelade  <cit> . according to their results, one target among distractors with orthogonal orientations easily pops out. the tasks become more difficult if orientation noises are added to distracters  <cit> . we designed distractors with noises between  to test the performances of different models under different parameters. the results are shown in figure  <dig>  

we generated synthetic images as follows. target positions and orientations were randomly determined. distractors orientations were orthogonal to that of the target and were disturbed by orientation noises . we generated  <dig> synthetic images where the target’s position and orientation was random in every image, and showed an example in figure  <dig>  the correct detection rate is defined as the ratio between the times that the maximal in a saliency map refers to the target and the total experimental times.

the fully connected network could not detect the target in all the experimental conditions. the randomly connected network performed better than the fully connected network. the attention model without invariant features coding could produce acceptable results when parameters were tuned properly. the model with invariant features coding performed best though it was also affected by model parameters.

the first layer receptive field  size is highly relevant to the resolution of the image and to texture density. for images with fine and close textures, a small rf size is enough. for images with coarse contours or pieces, the rf size should be larger. as seen in figure  <dig>  we tested four layer  <dig> rf sizes, namely 8× <dig>  12× <dig>  16× <dig>  and 20× <dig>  as long as the size was greater than the interval between the adjacent bars in figure  <dig>  it would not significantly affect the performance. an rf size of 8× <dig> was too small, leading to relatively low detection rates. the sizes greater than 16× <dig> did not produce substantial changes in performance ).

we also tested three layer  <dig> rf sizes, namely 3× <dig>  5× <dig>  and 7× <dig>  note that for the model without invariant features, the layer  <dig> rf size is always  <dig>  for the fully connected network, this size is always the total number of filters in layer  <dig>  thus, this comparison only works for the model with invariant features and the randomly connected network. for the former, the performance is not affected in such a simple task ). however, in the extreme condition, i.e., layer  <dig> rf size of 1× <dig> , the correct detection rate drops when orientation noises lie in the  range. however, when no orientation noises are added ), the correct detection rate of the model without invariant features is the same as that of the model with invariant features. this indicates the layer  <dig> rf size  influences robustness to noises. for the randomly connected network, correct detection rate rises with the layer  <dig> rf size.

the coefficient α, which adjusts suppression strength, affects performance. generally, a very small coefficient cannot completely suppress responses to distractors, and a very large one probably suppresses all responses including those to the target. both of these situations lead to low detection rates ). however, a larger α results in a better detection rate for the randomly connected network.

finally, we evaluated the influence of the excitation bandwidth σex and the inhibition bandwidth σinh in the dog used within the feature competition. for simplicity, we kept the ratio σinhσex constant, and we only changed σex to three values:  <dig>   <dig>  and  <dig>  this change indicates that σex is one, three, and five times as large as the layer  <dig> rf size. for the model without invariant features, the detection rate dropped with the increase of σex. no substantial change was observed for both the model with invariant features and the randomly connected network ).

the second experiment involved a visual search task in composite stimuli. as shown in figure  <dig>  the attention model with invariant features outperforms the model without invariant features, fully connected and randomly connected networks, and itti’s saliency tool  <cit> . the model with invariant features can detect salient objects when parameters vary greatly, whereas the model without invariant features cannot work. the results of our model with several different parameters are shown in appendix  <dig>  a detailed behavioural analysis for parameter values is also presented. 

we further tested the models’ performances in detection of global salient structures. as mentioned in the background section, the global saliency is defined by gestalt psychologists as whether a structure respects certain perceptual organisation rules such as proximity, good continuity, and closure  <cit> . the s curve, an illusory contour and a noisy version were taken as testing images. results are shown in figure  <dig>  

as indicated in figure  <dig>  objects as solid curves are perceived clearly. when objects are illusory contours, the attention model with invariant features coding can detect continuous contours. as parameters vary within the range mentioned in the figure caption, detection results are always continuous contours, while the attention model without invariant features coding detects discrete end points. as parameters vary within the range mentioned in the figure caption, detection results are always discrete ends. when the object is a noisy s curve, differences in saliency detection results between the two attention models are greater. in the model with invariant coding, segments forming the s curve can be detected and most segments forming background can be suppressed. for the model without invariant coding, segments in the background are more salient than those in the s curve. itti’s saliency tool performed better than the attention model without invariant features coding, but for the illusory contour, it performed even worse. we give a detailed comparison and analysis for the fully connected network and the randomly connected network on the illusory contour in appendix  <dig> 

our purpose in the next experiment was to test the saliency detection performances on a collection of ”real images” under different snrs. we constructed testing images from pairs of real images. three fruitsb were selected as the objects -), and six natural texture images from the mit media lab texture database were selected as the background -). the canny edge detector was applied to each object and texture background to yield edge images -, -).

a testing image was constructed by inserting an object edge image into the center 32× <dig> region in a 64× <dig> edge image of a texture ). an object consists of approximately  <dig> segments, and a texture edge image was undersampled at different scales to produce the background patterns consisting of different numbers of segments. we kept the number of segments in the objects fixed and changed the number of segments in the background to obtain testing images with different snrs. under each snr, we estimated the correct detection rate on  <dig> images  to get statistical results of saliency detection on these real images.

given a testing image in which the object consists of m edges, we defined a correct detection as the object edges which account for not less than 70% the first m most salient edges. it can be computed by a bernoulli binomial probability distribution that the random probability is not higher than  <dig> c. several examples were shown in figure  <dig>  it was a challenging task for the segments along the object silhouettes were not uniformly spaced and the segments in backgrounds were correlated . the model with invariant coding outperformed the model without invariant coding at the same set of parameters. when the rf size was tuned to be larger in the model without invariant coding, it performed better. saliency tool failed in the task. a quantitative comparison of the correct detection rates versus snrs was plotted in figure  <dig> 

from the plot, we can see that the correct detection rate first slightly increases and then dramatically decreases as the snr decreases. this result can be explained in the following way. when the number of background segments is small, their saliency may surpass that of objects, for the background segments are so sparse that they contrast strongly with surroundings. when the number of background segments is large, their probabilities of forming collinear or continual structures increase, leading to many local maxima in saliency maps and thus disturbing the detection of objects. in our experiments, the correct detection rate reaches the maximum at the snr of 1: <dig> 

why did the attention models differ in saliency detection? this could be caused by two factors. first, the sizes of receptive fields are enlarged after pooling, so some discrete segments or end points are easier to be perceived as a whole. it has been reported that larger receptive fields facilitate object search in complex scenes  <cit> . an improvement in the performance of the model without pooling  but with a larger rf size also supports this factor.

second, invariant coding makes smoothly varying stimuli evoke consistent responses, thereby enhancing the contour and facilitating perception of a structure from a cluttered background. this phenomenon is known as the contour completion, which is achieved since neurons with similar preferred orientations enhance each other when they are collinear , and suppress each other when they are nearly orthogonal  <cit> . this kind of interaction is also reported in  <cit>  as neuronal responses that are modulated by the presence of stimuli outside of classical receptive fields.

effects of overcomplete representation on saliency detection
in this section, we used natural image datasets to test the effects of overcomplete representation on saliency detection. the experiments include two parts. in the first part, we learnt filters from the same set of images as used in the last section. the numbers of filters are set to be  <dig>   <dig>   <dig>  and  <dig>  respectively, and correspondingly  <dig>   <dig>   <dig>  and  <dig> invariant feature descriptors by pooling the filters are selected. the testing dataset is collected by bruce et al.  <cit> , which includes  <dig> color images and eye movements from  <dig> observers when they view these images. the human eye tracking data can be used as a physiological basis to compare with the saliency maps obtained from attention models. in the second part, we selected  <dig> images from the weizmann dataset  <cit>  and collected eye tracking data of  <dig> viewers on these images. all these images show targets in cluttered texture backgrounds.  <dig> images out of the  <dig> images were used for training filters, and the  <dig> images left were used for testing.

each image was scaled to 341× <dig> pixels. in learning by the pcica, we chose, for example,  <dig> neighbourhoods  of 5× <dig> from the 10× <dig> topographic array  where the size of each filter is 16× <dig>  two neighbourhoods were overlapped by two filters both horizontally and vertically. torus grid was used, that is, next to the nethermost filter is the corresponding uppermost filter, and next to the rightmost filter is the corresponding leftmost filter. in testing, we used the  <dig> sets/pools of filters  to extract features from the 341× <dig> images. next, we pooled the  <dig> filtered results in each set into a separate feature map. after performing surround suppression within each feature map,  <dig> conspicuous maps with a size of 341× <dig> were obtained. finally, a single 341× <dig> saliency map was formed by combining  <dig> conspicuous maps.

the model of itti and koch  uses gaussian pyramids with  <dig> scales and  <dig> orientations. at each orientation angle, this model pools primitive features obtained from gabor filters of different phases. in this experiment, we set the number of phases at  <dig> for computational efficiency and also because this value does not substantially influence the results. to make the results of the two models comparable, we only selected the intensity and orientation channel for the saliency tool to form saliency maps. altogether, saliency tool uses  <dig> filters and combines  <dig> feature maps . this setting is close to that of our model with  <dig> filtersd and  <dig> feature maps to be combined. several examples are given in figure  <dig>  

we computed the receiver operator curve area , a common measure in signal detection  <cit> , to compare the performances of different models with observations from humans, and list the roc scores in table  <dig>  the larger score means better consistency with human observers. considering that the filters and invariant feature descriptors are learned from gray images and no color information is encoded, the attention models based on these can detect saliency mainly caused by intensities and orientations. therefore, in the bruce dataset, we transformed the color images into gray images and removed the images whose saliency was only caused by color contrasts. the remaining  <dig> images were used for testing. in the weizmann dataset, all images are gray images. 

as shown in table  <dig>  the saliency detection accuracy improves as the number of the basis set increases. this may be explained by the fact that the more overcomplete basis set describes features of images  more adequately  <cit> . it covers almost the whole frequency  space for natural images and encodes primitive features as well as shape features.

in the bruce dataset, when the number of a basis set is too small, such as  <dig> bases pooled into  <dig> invariant feature descriptors, they cannot describe an image adequately, resulting in great divergence from human detection. when the number of a basis set reaches  <dig> or  <dig>  the change of roc scores is tiny. this indicates that a  <dig> basis set  is near saturation. the roc score at this point is above that of saliency tool, which indicates the superiority of overcomplete basis set. the performances of the randomly connected network and the fully connected network are also listed. the performances of these networks are worse than our model, though the differences are not as significant as those in synthetic images.

the possible reasons for the difference between the results of synthetic and natural images can be analyzed from two aspects. first, synthetic images and natural images have different structures. in bruce dataset, a considerable number of images have no uniformly distributed distractors and almost no collinear or cocircular segments. saliency detection results for such images do not differ significantly in different networks.

second, our model with invariant features yields a saliency map coding intensity and orientation, whereas the fully and randomly connected networks produce saliency maps only coding intensity. for the synthetic images used in our experiments, the intensity cannot differentiate targets from distractors∖noises . without orientation information, the targets cannot pop out. for most natural images from the bruce dataset, the intensity contrast alone contributes greatly to the final saliency maps. therefore, saliency detection results on the bruce dataset do not differ substantially between our model and the fully∖ randomly connected networks.

however, our model with invariant features does show distinctions in certain kinds of natural images: the images where orientation plays a major role in saliency detection and the images where targets are in cluttered texture.

an example showing orientation plays a major role in saliency detection is given in figure 10-. in the fully connected and randomly connected network, only the vertical line  is detected as salient. in our model, the triangle sign  is also detected as salient, which is more consistent with human perception. the vertical line is salient because of the strong intensity contrast between its left and right regions, whereas the orientation contrast contributes more than intensity to the saliency of the triangle sign.

an example showing the pop-out of targets in cluttered texture is given in figure 10-. in the fully connected and randomly connected network, a pedestrian  in a cluttered background cannot pop out. in our model, the pedestrian is detected as salient, which is more consistent with human perception.

as to the bruce dataset that is used in our experiments, the images where our model with invariant features shows distinctions are few. thus the roc scores have no substantial differences among different networks.

we selected  <dig> images from the weizemman dataset. all the selected images show targets in backgrounds with cluttered texture.  <dig> images were used for training filters, and the  <dig> images left were used for testing. the filters learned from this training set was given in figure  <dig>  the performance of different networks on this dataset were listed in table  <dig>  we can observe divergences between our model and other networks. this experiment shows that our model with invariant coding can fully exhibit its superiority, given special datasets and filters learnt from task-dependent images.

relations between our model and itti’s saliency tool
our model uses orientation and intensity feature to produce saliency maps and calculates within feature competition and combination in a similar manner to that of saliency tool. from this point of view, our model may be mistaken as a special case of itti and koch’s model . however, the two models are distinct from one another, as mainly manifested in the following aspects:

first, the model by itti and koch constructs gabor filters in  <dig> orientations and  <dig> scales to extract primitive features, whereas ours learns an overcomplete set of filters from training images. thus, saliency tool works in an unsupervised way, while our model can be extended to a supervised one. though the learning process is more complicated and time consuming than direct construction, it greatly improves performance for it provides a specialised model . such works  <cit>  have been reported to improve saliency detection by learning feature descriptors and by training classifiers with positive and negative samples.

second, the orientation features obtained by the two models are different. at each orientation angle , saliency tool pools gabor filtering results at different phases by a sum of the absolutes . the default is pooling a pair of gabor responses with orthogonal phases. in our model, filters in the same pool have similar orientations  and different phases. hence, our model obtains orientation invariance aside from location invariance. when the orientation of input data slightly changes, the features do not change greatly. by contrast, saliency tool is location invariant but not rotation invariant. a small change in input orientation changes the representation significantly. to test this point, we reused the visual search task mentioned in figure  <dig> and listed the performances of the two models in figure  <dig>  when all distractors were in the same orientation, the two models performed equally well. as the orientation noises in the distractors increased , the correct detection rate of saliency tool greatly dropped, whereas the correct detection rate of our model did not change significantly. the two models differ in terms of the robustness to minor orientation variations in input. in saliency tool, the orientation contrast of the target to distractors is likely to be weakened or be entirely lost  when variations among distractors increase. it is so sensitive to this variation that the correct detection rate drops.

however, a feature descriptor designed to be insensitive to great variations in input may be unfavourable especially for object recognition.

CONCLUSIONS
to determine which factors in coding models affect saliency detection, we construct a coding model satisfying neurobiological constraints to provide input to the bottom-up attention model. the model is plausible in neurobiology because it provides an overcomplete representation for stimuli, obtains invariant features by a hierarchical structure, and models feature competition through nonclassic receptive field inhibition. we analyse different network structures and parameters as well as the underlying factors that influence performance.

more specifically, invariant coding improves robustness to noises and distractors, and improves the ability of detecting salient structures, such as colinear and co-circular structures that satisfy some perceptual organization rules. this phenomenon is also found in physiological experiments  <cit> . we have also shown that overcomplete basis set encodes a rich repertoire of natural image features, so it can improve saliency detection accuracy.

given the state-of-the-art models by itti and koch, and zhaoping li, our work provides a learning scheme that can be easily extended to a supervised model. by learning from examples of targets to be detected in a specific task, the performance is expected to be improved. the detailed behavioural analysis for parameters is of reference to a construction of a good system.

in summary, our results suggest that hierarchical invariant coding and overcomplete representation are general principles in visual attention and possibly in other perceptual systems. in the future, we will extend our attention model to work in a supervised method, involve multiscale techniques for adaptation to image resolutions, and to include other features  into saliency detection.

