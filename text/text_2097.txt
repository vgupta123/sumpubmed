BACKGROUND
next generation sequencing  enables whole genome sequencing or targeted sequencing of a large fraction of the genome. restriction enzyme digestion can be used for reducing the complexity of the genome by a reproducible selection of genomic regions. in combination with ngs this has resulted in different strategies for cost effective genome-wide marker discovery and genotyping  <cit> . the genotyping by sequencing  strategy, as developed by elshire et al.  <cit> , uses a single restriction enzyme for the digestion of genomic dna, followed by the ligation of adaptors that allow pcr amplification and subsequent sequencing on an ngs platform. the illumina machines are currently the most used ngs platforms. during this procedure fragments of an amplifiable length are selected and the complexity of the target is reduced . this results in a reproducible selection of genomic dna fragments. to date, this cost-reduction method has been used for genotyping a variety of species, including maize, barley, soybeans, oats, watermelon, and cattle  <cit> .

however, the restriction based reduction introduces low-diversity in the initial bases of the sequencing library due to every sequence starting with an identical restriction digest recognition sequence. this interferes with  cluster identification, resulting in a significant loss of data  <cit> . a solution to this problem is the addition of in-line barcodes  with different lengths. the length difference ensures that the restriction digest recognition sequence, present in all library fragments, is not read in the same sequencing cycle.library types with in-line barcode. the gbsx demultiplexer can handle barcode only sequences, sequences starting with barcode and restriction enzyme site , and those that also end with rs and/or common adapter.



illumina uses barcodes  that reside in the adaptor and not in the sequence read. hence, illumina’s demultiplexing software does not natively support in-line barcodes for sample identification. the tassel software  <cit>  provides a pipeline for analyzing single read gbs data, but has some restrictions. tassel  <dig>  evaluates reads for the presence of an expected barcode, without the presence of restriction enzyme cut sites. if a barcode is found, the barcode is trimmed from the read, which is further trimmed to  <dig> bases, retaining the enzyme cut site. when a second cut site is found, a polya tail replaces the remaining sequence. the output of this demultiplexing step is a set of binary files that can only be used within the tassel framework, as they cannot be converted to standard fastq or fasta formats. an alternative analysis strategy for gbs data is to process them as restriction site associated dna  data. although the rad and gbs protocol are very similar the resulting data have distinct properties . gbs fragments will always start and end with a restriction site whereas rad fragments will only have a single restriction site sequence. furthermore, gbs sequences for a certain restriction site will always start and end at the same position whereas this is variable for rad sequences. the stacks  <cit>  package can handle rad data, including the ability to input paired-end fastq files and output fastq files per sample via their demultiplex tools . however, stacks currently can not handle barcodes of different lengths and does not utilize the restriction site for trimming. byrne et al.  <cit>  have used an adjusted stacks pipeline to analyze gbs data that utilizes sabre for demultiplexing. reads are checked for adaptors and, if found, the reads are removed. otherwise, reads are trimmed to a length of  <dig> basepairs  and stacks is used to complete the analysis. therefore, this pipeline can handle barcodes of different lengths, but it has limited trimming capabilities. theoretically, the hard cut-off of  <dig> basepairs could be avoided and trimming improved by incorporating more dedicated trimming modules into these pipelines, such as the fastx-toolkit  <cit>  or trimmomatic  <cit> . to improve upon current gbs pipelines we set out to develop a pipeline that would provide easy-to-incorporate text-format output using: 1) more sensitive algorithms for demultiplexing and 2) a more sophisticated trimming method that searches for and trims on the restriction site containing adaptor sequence, as opposed to throwing out adaptor containing reads, hard-trimming to  <dig> bases, or ignoring the restriction site during trimming.

here we present a suite of tools that can aid in the design of gbs experiments and facilitates the initial processing. it provides the following tools:
an in silico digest for evaluating restriction enzymes

in-line barcode design, given a selected restriction enzyme

a sequencing demultiplexer based on in-line barcodes

supplemental tools for data simulation and post-sequencing barcode discovery



implementation
gbsx is developed as an easy to use toolkit for both users and developers. the in silico digest script is written in perl and requires bioperl modules  <cit> . all other tools are part of a single java program. for memory reasons biojava is not used for handling fastq or fastq.gz input and output files. instead a buffered reader/writer is used. all scripts are licensed under gnu general public license  version  <dig> 

in silico digest
the gbsx digest script performs an in silico digest of a reference genome in fasta format utilizing a user provided enzyme restriction digest sequence. this is done by using bioperl modules to identify all restriction cut site positions. then, based on an adjustable fragment size range and read length parameters, report statistics are calculated on the number and distribution of sequencable fragments. a bed format file is also generated of predicted sequenced bases, which can be viewed in genome browsers or used for annotation, such as overlap with known variants or repetitive elements. the same analysis can also be performed using two enzyme restriction sites, with statistics provided on the joint and separate cut sites. if requested, the number of sequencable fragments containing a third digest site can also be reported.

barcode design
the barcode generator designs random self-correcting barcodes based on hamming codes as described in bystrykh et al.  <cit> . generated barcodes vary in length and have an equal representation of the different nucleotides at every position in order to mitigate possible problems due to a low diversity of the library. the algorithm creates a random barcode set given the restriction enzyme of choice and the desired number of barcodes. our implementation uses hamming  codes . during the design process shorter barcodes are extended with a polya sequence in order to be compliant with the hamming code. for the experimental use of the barcodes the polya sequences are removed. as such the self-correcting nature of the hamming code is retained. the barcodes differ by at least a hamming distance of three and up to one substitution error can be corrected. additional constraints are that barcodes cannot contain restriction enzyme recognition sites and that shorter barcodes cannot be identical to a partial sequence of a longer barcode. the combination of the smallest barcode and restriction enzyme recognition site must have a hamming distance of  <dig> or more compared to the start of all other barcodes and restriction enzyme recognition sites. figure  <dig> illustrates two barcodes designed with a restriction site. due to the design constraints these two barcodes have a hamming distance of  <dig>  if the same barcodes would be used without the enzyme, or without the constraints, both barcodes will be demultiplexed as being the shortest barcode . using another restriction enzyme could introduce a smaller hamming distance, resulting in the same misassignment. hence, for optimal usage, the designed barcodes can only be used in combination with the corresponding restriction enzyme. using different or no restriction enzyme in combination with these barcodes will result in incorrect demultiplexing, and hence incorrect results .the importance of a good barcode design. this image shows  <dig> barcodes, completed with the restriction enzyme recognition site of apeki. if these barcodes are used for the demultiplexing of gbs or rad data with the apeki enzyme, the software will recognize the correct barcodes  because of the hamming distance between the barcodes. when the barcodes are used with another or no enzyme, the barcodes will have a different distance. this could result in misdemultiplexing of the reads.



gbsx demultiplexer
the gbsx demultiplexer is capable of demultiplexing fastq files with a variety of in-line barcode format sequences , including:
in-line barcode sequences without a restriction enzyme cutsite at the start of each read

rad sequences: in-line barcode and restriction enzyme cutsite at the start of each read and an optional adaptor at the read end

gbs sequences: in-line barcode and restriction enzyme cutsite at the start of each read and an optional restriction enzyme cutsite and adaptor at the read end



the gbsx demultiplexer requires one  or two  fastq files and a parameters file as input. the fastq files can be gziped. the parameters file is a tab-delimited text file with the sample names and their associated barcode and enzyme. the start of the fastq sequence is checked for the barcodes provided in the parameters file. the barcode and enzyme  are trimmed from the sequence and corresponding quality values. the end of the sequence is then trimmed for a possible enzyme cut site followed by the start of the common adapter  . demultiplex statistics are generated, including the number of mismatches in the barcodes, percentage of the sample in the data, number of basepairs, and overall quality.

since gbs often uses short insert sizes read  <dig> and read  <dig> of read pairs, when using paired-end sequencing, often overlap. therefore we include an additional analysis step for paired-end read trimming to include a consistency check across both reads. read  <dig> is trimmed as described earlier and the read  <dig> sequence is trimmed for the enzyme cut site followed by the complement of the barcode. trimmed read  <dig> and read  <dig> sequences are then assessed if adaptor trimming was performed similarly in both reads. trimming is corrected in case of any inconsistency. there is an inconsistency when one or both reads are trimmed, but both reads do not have the same length. in this case, the start of the longest read has to be the complement of the end of the shortest read. this is checked for the restriction enzyme cutsite and the first bases of the longest read . when there is a match, the longest read is trimmed to the length of the shortest read. when this does not match, the trimming of the longest read is assumed to be correct. the shortest read is corrected by trimming the original read.

barcode discovery
the barcode discovery tool counts all possible barcodes  in a fastq file. this can be used when a large portion of the demultiplex is undetermined. the barcode discovery tool can also be used to identify unexpected barcodes, or global sequencing errors. this tool utilizes a standard restriction enzyme annotation file, based on the stacks standard enzymes. the annotation file can be replaced by a new file or extended by the user. the output of this tool are tab-delimited files. one file is created per barcode length. in these files are the following columns: the barcode sequence, the name of the restriction enzyme associated with the barcode, and the number of reads identified with the barcode.

gbsx simulator
the gbsx simulator tool was developed for evaluating the performance of the gbsx toolkit by generating simulated gbs data with sequencing errors. the simulator uses a fasta file and a barcode file as input. the fasta file contains sequences of different lengths, all starting and ending with the restriction enzyme recognition site. the barcode file contains all barcodes that need to be included in the simulation.

for every fasta sequence, reads are simulated of the specified length completed with the common adapter . the reads per locus parameter determines the total number of reads that will be generated per sample. an equal number of reads is generated from both ends of the fasta sequence. the tool supports the generation of both single-end and paired-end reads. while generating the reads errors are introduced under the simple assumption that sequencing will generate a single error every  <dig> bases. the error base is changed to another random base . the simulation generates statistics containing the number of correct barcodes and barcodes with one mismatch.

RESULTS
gbsx is a toolkit developed to enhance gbs experimental design and analysis. to compare demultiplexing and trimming to existing tools we simulated data based on chromosome  <dig> of the human reference genome . the chromosome  <dig> fasta file was used as input for the in silico digest script using the restriction enzyme apeki, sequence gˆcwgc. the additional non-default parameters used were minimum fragment size of 60bp and maximum fragment size of  <dig> bp. the resulting bed file was adjusted so all fragments include the complete restriction cutsite . this bed file was used, together with the fasta file of the reference, as input for the bedtools bedtofasta toolkit  <cit> . this resulted in a fasta file with all target restriction fragment sequences. this fasta file was then used as input for the gbsx simulator. ten experimental data sets of paired-end reads were generated. each data set generated used different barcodes targeting a different number of samples . the barcodes were generated per experiment with the barcode generator with a variable length between  <dig> and  <dig>  in addition, the simulated data sets had varying synthetic read depths for restriction targeted loci . the simulation was done twice for each experimental group, either including or not including the apeki restriction enzyme sequence. for single end synthetic data the first read file of paired-end reads was used.

gbsx and stacks, tools which can both demultiplex and trim simultaneosly, were evaluated with these datasets. in addition we evaluated the demultiplexing of sabre, which is part of a published gbs pipeline  <cit> .

demultiplexing was evaluated with the simulated data using three different tools: gbsx, stacks, and sabre. the datasets with apeki restriction enzyme sequence were demultiplexed with gbsx  and stacks . the datasets without restriction enzyme sequence were demultiplexed with gbsx , stacks , and sabre. for all tools  <dig> mismatch was permitted in the barcode and  <dig> mismatch was permitted in the restriction enzyme recognition site . for gbsx and stacks tools the common adapter  was supplied. tassel was not included in this performance analysis since its demultiplexing results in a binary file, of which no tools are available to convert to a text format file  per sample.

overall, we find gbsx demultiplexing to be equal to sabre and more sensitive than stacks . gbsx and sabre use a similar barcode recognition algorithm explaining their near identical results when not including the restriction enzyme recognition site. for gbsx, the demultiplexing of gbs and rad data also use the same algorithm, as indicated by identical results. in addition, the barcode needed for the demultiplexing is always in the first read of paired-end data which means paired-end and single end read data should have identical results. this is true for gbsx and sabre, but not stacks. stacks does filter reads on quality which could potentially cause discordance between single and paired-end analyses. stacks had no misassigned barcodes, while gbsx misassigned less than one in a million reads when using a restriction enzyme recognition site and about four in a million without a restriction enzyme recognition site . though gbsx introduces a very small number of misassigned barcodes, it does provide much greater sensitivity  which we believe provides a more favorable demultiplexing tool.the tools and their options comparing the percentage of demultiplexed reads and correctly trimmed reads from total reads, for paired-end and single end data. the gbsx option na indicates an enzyme was not provided for demultiplexing. sabre does not perform trimming and therefor has no trimming values in the plot.
demultiplexing statistics as an accumulation of ten synthetic data sets, totaling  <dig> , <dig> synthetic reads



paired end reads
re
# misassigned
misassignments
# correctly
sensitivity
site
per million reads
demultiplexed

single end reads
re
# misassigned
misassignments
# correctly
sensitivity
site
per million reads
demultiplexed
na indicates an enzyme was not provided to gbsx for demultiplexing. the re site column indicates if the reads did or did not contain the apeki restriction enzyme sequence.



trimming was evaluated for gbsx and stacks . evaluation was done by counting if the length of the trimmed demultiplexed reads matched the length of the original sequences. if the trimmed sequence had a different length than the original, because it was trimmed too short or was not trimmed at all, the sequence was counted as incorrect. overall, the gbsx gbs demultiplexed and trimmed reads performed best with the lowest errors and highest sensitivity for both paired-end and single end reads . this performance can be explained in that the gbsx gbs demultiplexer and trimmer is the only tool that uses the extra restriction enzyme recognition site before the adapter for the trimming procedure. for paired-end reads, all gbsx analyses had less errors and higher sensitivity than stacks, likely due to the correction made possible by looking at the overlap between read pairs. the only case where equivalent data was trimmed better by stacks was analyzing single end reads with restriction enzyme sequence for the gbsx rad demultiplexer compared to stacks’s process_radtags, though the gbsx gbs demultiplexer still performs better than both. the gbsx gbs performs better than gbsx rad, mostly due to the missing of the enzyme site recognition in rad data. in conclusion, the gbsx trimming process provides overall higher sensitivity and less errors than stacks.trimming statistics on demultiplexed reads from table
1



paired end reads
re
# correctly
sensitivity
triming errors
site
trimmed
per thousand reads

single end reads
re
# correctly
sensitivity
triming errors
site
trimmed
per thousand reads
the re site column indicates if the reads did or did not contain the apeki restriction enzyme sequence. sabre does not perform read trimming and is therefor not included in this comparison.



CONCLUSIONS
gbsx provides an easy to use suite of tools for designing and demultiplexing of gbs experiments. this includes evaluating restriction enzymes with the in silico digest tool and creating in-line barcodes with gbsx’s barcode generator. post sequencing, gbsx provides tools to demultiplex and trim reads with, in a majority of cases, enhanced performance over existing tools. these demultiplexed and trimmed fastq files can then be mapped with traditional next generation sequencing tools and snps called using existing tools such as gatk  <cit>  or freebayes  <cit> . in conclusion, gbsx fills in the gaps and enhances current modules for a complete gbs pipeline, from experimental design to final analyses.

availability and requirements
poject name: gbsx

poject home page:https://github.com/genomicscoreleuven/gbsx

operating system: platform independent

programming language: java and perl

other requirements: java  <dig>  or higher, bioperl

license: gnu gpl v3

any restrictions to use by non-academics: no



competing interests

the authors declare that they have no competing interests.

authors’ contributions

jkjvh and jrv conceived the project. kh designed the barcode design, demultiplexer, and supplemental tools. msh designed the in silico digest tool. all authors contributed to evaluation of the tools and writing the manuscript. all authors have read and approved the manuscript.

