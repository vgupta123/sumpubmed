BACKGROUND
in the last decade, next-generation sequencing methods have emerged, become widely used and are today a prerequisite for most large-scale studies. one of the early next-generation techniques is roche  <dig> sequencing  <cit> , currently extensively used as it produces longer sequence reads compared to other platforms. the first  <dig> sequencing platform was introduced in  <dig> with the gs <dig> instrument. the gs <dig> brought a huge leap in performance over present sanger based techniques and produced around  <dig> million bases  per run  <cit> . since then, the  <dig> technique has repeatedly evolved and the gs flx titanium instrument, introduced in  <dig>  produces as much as  <dig> mb per run in reads of approximately  <dig> bases per read  <cit> . the long reads produced by  <dig> sequencing are particularly useful when scaffolding is difficult, where homology to known sequences is distant or absent and overlaps may be ambiguous or non-existent, e.g. de novo genome sequencing or metagenomics. the underlying methodology involved in sequence comparisons as well as in most subsequent data processing steps, is sequence alignment, which is an important fundament for the entire field of bioinformatics.

the global sequence alignment problem and a method for solving it was first proposed by needleman and wunsch in  <dig>  <cit> . the suggested algorithm utilised a technique called dynamic programming. dynamic programming solves a problem by dividing the problem into solvable sub-problems. in  <dig>  smith and waterman defined the local alignment and proposed a slightly modified algorithm to solve it  <cit> . finally, one year later gotoh proposed the usage of non-linear gap penalties and the corresponding modified algorithms  <cit>  for solving the global and local alignment problems. while a single sequence alignment is quickly calculated, a full database search using smith-waterman-gotoh would be unnecessarily time consuming as most comparisons would render non-significant alignments. to address this issue, heuristic methods such as fasta  <cit>  and blast  <cit> , which greatly reduces the number of sub-problems solved, were developed. later, methods using improved heuristics, and/or innovative implementations to more efficiently utilise computer memory and processors have been proposed, e.g. megablast  <cit> , ssaha <dig>  <cit> , blat  <cit>  and smith-waterman-gotoh alignments utilising simd instructions  <cit> .

the characteristics of  <dig> data are to some extent different from most other sequencing technologies.  <dig> sequencing is a pyrosequencing method where the nucleotide reagents for detecting thymine , adenine , cytosine  and guanine  are repeatedly cycled over each dna template fragment, while elongating the complementary strand. the intensity of each flow of nucleotide reagents is recorded, as a so-called flowpeak and collected in a flowgram  <cit> . as the homopolymer length is estimated from peak intensity, occasional uncertainties occur  <cit> . these uncertainties are challenging for downstream analysis algorithms. current analysis methods do not provide any modification to the alignment algorithm, but instead recommend using adjusted heuristics, e.g. a shorter indexing word length, to cope with this problem. while a majority of the homopolymer insertions/deletions  are placed correctly by gaps, close to the end of the read, the alignments are often truncated and sequential undercall-overcall  are often mistaken for a single nucleotide polymorphism . the first method that addressed the problem of homopolymer uncertainties was flat  <cit> , which performs probabilistic flowgram matching. the downside of using flowgram matching is the limited possibility to detect non-identical matches, apart from homopolymer dissimilarities. another tool that addressed this problem was pangea  <cit> , which uses a modified smith-waterman-gotoh alignment algorithm, which allows gaps at the end of long homopolymer runs at a lowered cost. faast  <cit>  was developed in order to combine the benefits of both methods. faast utilises the flowgram to dynamically set a position specific homopolymer gap penalty relative to the uncertainty at that position, regardless of homopolymer length.

since nucleotide alignments fail to produce significant results for remote homologues, homopolymer indel recover methods based on nucleotide alignments are not applicable at remote homology. due to this limitation of nucleotide alignments, many metagenomic studies  <cit>  perform translated homology searches , using translated blast, e.g. tblastx or blastx  <cit> . however, blast does not handle frame-shifts and at best presents two alignments in each frame, and these may be either significantly separated or overlapping, depending on the similarity in the shifted frame. these issues add complexity to metagenomic analysis, and potential homopolymer inaccuracies often have to be corrected manually to preserve an open reading frame . there are methods that do allow nucleotide-protein alignments with frame-shifts, for example the method proposed by states and botstein  <cit> . however, none of these methods take homopolymer errors into account, which makes them unsuitable for  <dig> data or other data where homopolymer reading errors occur, for example the ion torrent platform  <cit> .

to address these problems, i here introduce a new tool named haxat . this new tool solves the protein-nucleotide alignment through a novel dynamic programming algorithm. the algorithm is based on the smith-waterman-gotoh algorithm, and is capable of handling frame-shifting matches. it considers homopolymer inaccuracies through position-specific penalties, provided by a pre-calculated query profile. through this novel algorithm, haxat produces more sensitive results with  <dig> data, even in the absence of flowpeak information .

RESULTS
the new tool haxat addresses the issues of frame-shifts in translated nucleotide – protein alignments while considering an underlying  <dig> data model. haxat utilises a novel alignment algorithm, with corresponding parameters, to perform these alignments. haxat was designed to be useful in the analysis of metagenomic data, but it can be used in various genome sequencing and annotation tasks, such as orf annotation.

the alignment output format
haxat produces its output in a new alignment format to incorporate the protein-nucleotide alignments as well as annotations for the frame-shifting positions. an alignment of a simulated  <dig> titanium read from orf <dig> of torque teno sus virus  <dig>  gu <dig> against add <dig> is shown in figure  <dig>  all frame-shifting insertions/deletions  are marked in the alignment output by lowercase characters. furthermore, for each nucleotide gap caused by a frame-shift, an inserted nucleotide is predicted based on a maximum likelihood estimate . apart from reporting the score, identities, positives, coverage and gaps, the output format also specifies the number of frame-shifting gaps .

evaluation of homopolymer insertion/deletion recovery
a representative benchmark set with different degrees of difficulty was constructed from a single coding seed sequence. the orf <dig> of torque teno sus virus  <dig>  gu <dig>  was chosen as seed sequence. a torque teno virus sequence represents a likely single read sequence, resulting from a metagenomics or genome project, as tt viruses are highly heterogeneous  <cit> . the nucleotide sequence was first translated into its corresponding amino acid sequence. six additional sequences, one for each degree of difficulty, were subsequently constructed through mutations in protein space down to  <dig>   <dig>   <dig>   <dig>   <dig> and 40% identity. each mutation position was randomly chosen, while the substitution frequencies were proportional to the corresponding blosum substitution probabilities  <cit> , where x denotes the clustering identity threshold used. in order to supply query sequences,  <dig>   <dig> titanium reads were simulated by the “454sim” simulator  <cit> , using default settings. finally,  <dig> parameter combinations were evaluated for all seven degrees of difficulty, by alignment of the  <dig>  reads to each of the seven target protein sequences. the complete study design and evaluation scripts are available in additional file  <dig>  haxat was executed using the blosum <dig> substitution matrix and a gap open/ext cost of  <dig> and  <dig> .

parameter evaluation setup
the predicted homopolymer inaccuracies were extracted from the alignment and the prediction accuracy was evaluated. a correct prediction was defined as either a correctly predicted deletion  or correctly placed insertion  of a particular nucleotide. consequently, an incorrect prediction is an incorrectly placed deletion/insertion or an insertion of an incorrect nucleotide at a particular position. homopolymer indels not covered by the alignment were not regarded as a false negative prediction . the parameters evaluated were; the cost for opening a single  or a double  frame-shifting gap. the  <dig> model specific parameters evaluated were; the homopolymer frame-shifting gap penalty fraction  which determines the minimum single and double frame-shifting penalties , the relative flowpeak deviation allowed  and whether to use insertion validation or not , see methods. haxat can be executed in three different modes by applying two different alignment models. the first mode uses a neutral  alignment model . consequently, for this model, only a single  and double frame-shifting gap cost  is applicable. this model is highly similar to the method previously proposed by states and botstein  <cit> , while it provides the possibility to set different costs to single and double gaps. the second mode uses a  <dig> aware alignment model without flowpeak information, i.e. running haxat with fasta input . finally, the last mode applies the same  <dig> aware alignment model but uses flowpeak information through sff input . the prediction accuracy was evaluated by the matthews correlation coefficient   <cit>  for all three modes.

evaluation of parameters for a neutral alignment model
a total of  <dig> parameter combinations employing the neutral alignment model were evaluated , for each of the seven degrees of difficulty . from that set, the maximum prediction accuracy  for each tested value of the single gap penalty was extracted, see figure  <dig>  the results showed that the accuracy peaked at the smallest allowed single gap penalty  for high identity targets, while at lower identity the accuracy peaked at a slightly higher penalty . the mean accuracy of all targets peaked at  <dig>  the double gap penalty was evaluated at  <dig>  to  <dig> times the single gap penalty, but the experiments showed a peak accuracy for a double gap penalty that was  <dig>  or  <dig> times the single gap penalty. however, due to the rarity of double homopolymer under/overestimates, the double gap penalty does not affect the results significantly in these evaluations, see additional file 2: figure s <dig>  table  <dig> describes the prediction accuracy obtained for the highest scoring parameter combinations of each target.

the table describes the matthews correlation coefficient of the homopolymer insertion/deletion recovery for various gap costs using a non- <dig> data model. the results are evaluated using alignments against database entries of a protein identity ranging from  <dig> to 100%, not including potential sequencing errors. the best results are highlighted in bold for each column. the parameter set which results in the best mean mcc is highlighted in bold.

evaluation of parameters for homopolymer aware alignments
a total of  <dig> parameter combinations for each of the seven degrees of difficulty  were evaluated, see figure  <dig>  as seen in figure 3a, the h-value was chosen so that the homopolymer single gap penalty  assumed the lowest allowed value. as the parameter combinations evaluated did not span all possible values of all parameters, some combinations received lower mcc-values than expected. the single gap penalty, figure 3b, assumed different peak values for different target identities. for low identity targets a high single gap penalty was favoured and at high identity targets a low single gap penalty was favoured. the single gap penalty determined the cost of introducing a frame-shifting gap in regions where long homopolymers are not present. at high identity, it was favourable to introduce gaps to keep high identity while at low identity, the risk of introducing invalid gaps were higher and thus better results were produced with an increased gap cost. finally, figure 3c, shows the flowpeak deviation parameter, k. as with the single gap penalty, high identity targets allowed a more aggressive penalty decrement at increased homopolymer length compared to low identity targets. for example, at 100% identity, the highest value evaluated,  <dig> , produces the peak accuracy while at the mean identity, the peak was at k =  <dig> .

as with the neutral  model, the double/single gap penalty ratio was evaluated from  <dig>  to  <dig>  where values around  <dig>  were found to be favoured slightly, see additional file 3: figure s <dig>  table  <dig> describes the prediction accuracy obtained for the highest scoring parameter combinations of each target. we note from the table that flow-order insertion validation  generally performs worse. a substantial increase in accuracy was achieved using a homopolymer aware model, where for instance the mcc-value at 100% rose sharply to  <dig> , while at 40% a mcc-value of  <dig>  was obtained, compared to  <dig>  and  <dig>  for the neutral model.

the table describes the matthews correlation coefficient of the homopolymer insertion/deletion recovery for various parameter combinations using a  <dig> model without flowpeak information . the results are evaluated using alignments against targets of a protein identity ranging from  <dig> to 100%, not including potential sequencing errors. the best results are highlighted in bold for each column as well as the parameter set which results in the best mean mcc.

evaluation of parameters for a flowpeak aware alignment model
the final parameter evaluation was performed using the same 454-model but by using flowpeak information through feeding haxat with raw  <dig> data input , see figure  <dig>  just as without flowpeak information a low homopolymer single gap penalty was found to be favoured, figure 4a, while in general due to more complete flowpeak information a higher single gap penalty was preferred, see figure 4b. figure 4c shows the flowpeak deviation parameter, k. at low identity, a lower value of k  was found to be optimal, while at high identity, large values of k produced the best results. the double/single gap penalty ratio was again evaluated from  <dig>  to  <dig>  and values around  <dig>  were slightly more favourable, see additional file 4: figure s <dig>  finally, table  <dig> describes the prediction accuracy obtained for the highest scoring parameter combinations of each target. it is noteworthy that using flow-order insertion validation  produced better results for all cases except at 100% target identity. the increase in accuracy, compared to using no flowpeak information, was most notable when the target identity was low, due to the absence of biological information . for example, at 40% identity with flowpeak information, a mcc-value of  <dig>  was obtained, compared to  <dig>  without the aid of flowpeak information. for a complete list of the parameters examined, for all three evaluations, and a list of all true/false positives/negatives, see additional file  <dig> 

the table describes the matthews correlation coefficient of the homopolymer insertion/deletion recovery for various parameter combinations using flowpeak information . the results are evaluated using alignments against targets of a protein identity ranging from  <dig> to 100%, not including potential sequencing errors. the best results are highlighted in bold for each column as well as the parameter set which results in the best mean mcc .

in conclusion, due to the low number of homopolymer indels  in comparison to non-mutated positions  even with only a few misplaced indels, each resulting in a consequent fp and fn, it is difficult to reach a high mcc-value. this is clear from table  <dig> where, even at 100% protein identity and using flowpeak information, the best mcc-value was  <dig> . when examined, many of the indels were placed just a few bases away or even at the same location but with the adjacent homopolymer corrected, for example “ttttaa” instead of the correct “tttaaa”. as seen in the tables  <dig>   <dig> and  <dig>  in all cases, the novel  <dig> model significantly outperformed the previously existing neutral  alignment model, regardless of the parameter settings.

effects of using the haxat alignment algorithm on real data
to further test the alignment models, alignments were performed against real sequences homologues to gu <dig> . the database sequences used are described in table  <dig>  the alignments were performed using a neutral  model and a 454-model with/without insertion validation , and with/without flowpeak information. as with previous tests, a  <dig>  reads were simulated from each homologue and homopolymer errors were estimated from the resulting alignments towards the seed sequence. true/false positives/negatives were defined as before and the algorithm efficiency were evaluated by mcc, see figure  <dig>  the alignments were performed using haxat and default parameters for a  <dig> model  as well as the optimal parameters for the neutral  model on 454-data, -s <dig> -d <dig> . the results from this test, largely, agree with the tests using simulated data. consistently for all targets, a  <dig>  to  <dig>  mcc-value gap was found between the previously available neutral model and the  <dig> aware model using flowpeak information and insertion validation. a slight increase in accuracy was achieved with insertion validation when using flowpeak information, while a slight decrease in accuracy was found when flowpeak information was unavailable.

the table shows the blast statistics for some homologues to gu <dig>  the blastx program was used with default parameters against with the nucleotide gu <dig> orf <dig> sequence as query. the identities given are averages for the aligned region while partial alignments relevant for the alignment of a read may obtain both a lower and higher identity.

implementation and availability
the homopolymer aware cross alignment algorithm has been implemented in a tool called haxat . haxat was implemented in c++ and it compiles using gnu gcc or intel icc on both linux/windows. the tool is open source under the gnu gpl licence and available as source code and pre-compiled binaries at http://www.bioinfo.ifm.liu.se/454tools/haxat. haxat is able to use a wide range of parameters for adjusting the alignment model. more information about parameters, usage and input/output formats can be found in the documentation at the webpage. the webpage also provides a web-version of haxat which can either align two sequences using haxat alone or search a query sequence against a database using blastx heuristics.

discussion
as shown above, haxat introduces a novel sensitive tool for performing frame-shifting nucleotide-protein alignments using dynamic programming. the increased sensitivity stems from retaining flowpeak information and applying a  <dig> model to the frame-shifting gap-penalties. haxat is intended as an alignment refinement tool in down-stream studies of sequences suspected to contain homopolymer indels. at this moment, haxat does not implement any heuristics and performs full dynamic programming calculations for each query-database combination. this greatly reduces the speed at which a full database scan can be performed using this tool alone. on the other hand, a full alignment ensures that the optimal local alignment, given the alignment model, is found. furthermore, many tools implement excellent heuristics, for example alignment search tools like tfasta  <cit> , blastx  <cit> , and these can be used to reduce the number of alignments computed by haxat. an example on how to achieve haxat results with the aid of blastx heuristics , using four simple commands, is available at the webpage . the webpage also provides the possibility to run haxat directly, both for alignment of two sequences and running a query against a database using blast heuristics. even though haxat is developed to handle  <dig> data, the algorithm could also be applied to other pyrosequencing methods or any sequencing method or data where homopolymer inaccuracies occur, for example the ion torrent platform  <cit> .

the haxat alignment algorithm is optimised to isolate the most costly calculations so that these can be performed beforehand and stored into a query profile. the calculations of each cell introduces four additional evaluations and references at a maximum i- <dig>  j- <dig>  see methods, in comparison to the smith-waterman-gotoh algorithm which references i- <dig>  j- <dig>  this implementation is computationally efficient, but it has some limitations. most notably; no homopolymer correction tracking is employed and thus it is possible to correct a homopolymer position several times. for example, if the single frame-shifting gap penalty is smaller than half of the double gap penalty, two single gaps would be favoured in cases where the double gap is between two codons . furthermore, the single gap penalty plus the double gap penalty should be larger than the penalty for introducing a gap in protein space, to ensure that protein gaps are not substituted with homopolymer corrections. the haxat tool solves these issues by not allowing such penalty combinations. while this limited model for correcting homopolymer indels cannot stably correct homopolymer lengths at low costs, the results show that this model still provides a significant step forward in performance compared to the previously available neutral  model.

the haxat algorithm has been put to the test, both in alignments against simulated data where the identity is fairly uniform  and in alignments with real homologues containing low identity regions and complicating gaps. the algorithm provides a substantial gain compared to the previously available neutral model. additionally, the ability to place homopolymer indels correctly does not only increase accuracy but also the query coverage  in these alignments as fewer alignments are terminated by the frame-shift.

CONCLUSIONS
sequencing methods based on sequencing by synthesis without a terminator, for example  <dig> sequencing, suffer from frequent homopolymer indels. as most previous sequencing techniques do not show this type of error, many bioinformatics algorithms are not well adapted to handle homopolymer indels. this paper presents a novel algorithm for solving the fundamental problem of sequencing alignment between a nucleotide sequence, potentially burdened by homopolymer reading errors, and a protein sequence. this algorithm has been implemented in an open source tool called haxat, available as source code, pre-compiled binaries and through a web-based tool. the results show that haxat provides a significant improvement in alignment accuracy for this type of data. the improved accuracy stems from the homopolymer aware algorithm which makes use of raw flowpeak values to further improve homopolymer length predictions. haxat provides an important step forward in solving the ‘homopolymer problem’ which faces bioinformaticians when working with  <dig> data. haxat will be useful in metagenomic and genomic analysis, not only for  <dig> sequencing data, but also for emerging methods, such as the ion torrent platform. the haxat tool is open source under the gnu gpl licence and is publically available as source code, pre-compiled binaries and as a web-based tool at http://www.bioinfo.ifm.liu.se/454tools/haxat.

