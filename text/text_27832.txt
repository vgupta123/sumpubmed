BACKGROUND
with the advancement of technology, genome sequencing projects are moving from the study of single genomes to the examination of genomes in a community. this new study, metagenomics, allows culture-independent and sequence-based studies of microbial communities. a metagenomic project generally starts by using whole genome shotgun  sequencing  <cit>  on environmental samples to acquire sequence reads, followed by assembling sequence reads, gene prediction, functional annotation and metabolic pathway construction. a necessary step in metagenomics, which is not required in single genome sequencing, is called binning. the binning process sorts contigs and scaffolds of multiple species, obtained from wgs sequencing, into phylogenetically related groups . the resolution of phylogenetic grouping can vary from high levels such as domain, down to low levels such as strain of a given microorganism, depending on a number of factors such as the binning method used, community structure and sequencing quality and depth  <cit> . as the community structure is inherent in the nature of the environmental sample collected, and sequencing quality and depth rely heavily on the sequencing technology and sample size, the major focus of computational techniques is on the method of binning.

a number of binning methods are currently available that fall into two broad categories: sequence similarity-based and sequence composition-based. sequence similarity-based binning methods, for example blast, classify sequences based on the distribution of blast hits of predicted genes to taxonomic classes. composition-based methods discriminate genomes by analysing the intrinsic features of sequence encoding preferences, such as gc content  <cit> , codon usage  <cit>  or oligonucleotide frequencies  <cit> , for different genomes. different approaches to extracting sequence features have been proposed. some composition-based binning methods include k-mer  <cit> , oligonucleotide-frequency-based clustering with self-organising maps   <cit> , phylopythia  <cit>  and tetra  <cit> , all of which have yielded promising results. the k-mer method calculates the oligonucleotide frequencies of all sequence fragments and compares them to a reference set of completed genomes. clustering of oligonucleotide frequencies with som can be used in two different ways: directly clustering the metagenomic sample sequences using different oligonucleotide frequencies as features  <cit> , or building an som classifier using sequences in existing databases and then assigning metagenomic samples to the closest matching node in the classifier  <cit> . phylopythia follows a similar approach to the som classifier method, building a classification model from the sequenced genomes and assigning sample sequences to phylogenetic clades according to the similarity of metagenomic sequence patterns. two types of phylopythia models can be built, a generic model and a sample-specific model. the generic model is constructed using the sequence patterns of a reference set of isolated genomes and the sample-specific model includes additional marker-gene labelled sequence fragments  from each of the dominant population of the sample. in contrast, tetra follows the som clustering approach that does not require reference genomes. it bins the species-specific sequences by comparing the pairwise tetranucleotide-derived z-score correlations of every sequence.

the above named binning methods can be divided, from a machine learning perspective, into supervised and unsupervised learning methods. methods that build a classifier using the knowledge of completed genomes belong to the class of supervised learning methods, such as blast, k-mer and the som classifier approach. however, considering the current amount of known genomes, which is insufficient to represent the almost limitless microbial genomes  <cit> , the resultant under-representation of training samples can be detrimental to these supervised-learning methods. phylopythia, being the latest of the binning methods addressed here, has been shown to be able to classify sequence reads with great accuracy when training data is highly relevant to the species being assigned, which can either be from a set of reference genomes or using  <dig> kb length marker-gene labelled sequences, which may not always exist to build a sample-specific model. unsupervised-learning methods do not have this dependence on training data. tetra falls into this category, but it focuses on the long fosmid-sized  <dig> kb fragments and its all-verse-all pairwise comparison matrix can quickly become intractable for a large number of sequences. the direct clustering approach with som was tested to separate  <dig> kb and  <dig> kb sequence fragments derived from  <dig> bacteria and  <dig> eukaryotes, but only the  <dig> kb tests showed clear species-specific separations  <cit> . nevertheless, without the reference sequences, the results lack identifiable labels to resolve for the clusters of sequence reads produced.

to circumvent dependence on the knowledge of completed genomes and provide meaningful clustering results, we propose a semi-supervised seeding algorithm that uses very small amounts of labelled samples, extracted from metagenomes, for binning metagenomic sequences. the seeding method is a post-processing method that can be applied to an unsupervised clustering algorithm of choice, for example, som, growing self-organising maps   <cit> , or the incremental grid growing neural network  <cit> , which also provide dimensionality reduction for data visualisation. comparing the clustering qualities of som and gsom indicated that gsom has a better clustering quality and faster computational speed, as confirmed by experiments conducted on other datasets  <cit> .

therefore, in this paper, we implemented the proposed seeding method on gsom, henceforth abbreviated as s-gsom. the performance of s-gsom for binning metagenomic sequences is demonstrated by applying it to simulated metagenomic datasets. preliminary tests were first conducted to compare s-gsom with the following well-known semi-supervised learning methods: constraint-partitioning k-means , constrained k-means, seeded k-means and the transductive support vector machine  on the task of separating sequence fragments created from  <dig>   <dig> and  <dig> prokaryotic species randomly sampled from the ncbi genome database. for a comparison with current binning methods, further tests of s-gsom were performed using the recently published simulated metagenomic datasets that were created to evaluate the fidelity of metagenomic processing methods  <cit> .

methods
datasets preparation and data pre-processing for separation of prokaryotic dna sequences
generation of independent simulated metagenomic data
the most updated ncbi database contains  <dig> completed archaea/bacteria genomes  <cit> . genome sequences of  <dig>   <dig> and  <dig> species were randomly selected from the ncbi database. in total, seven random sets were independently drawn from the database, replacing them each time. three sets were drawn for each of the  <dig> and  <dig> species datasets and only one set for the  <dig> species dataset, considering the limitations imposed by the available computing resources. . for convenience, we abbreviated the datasets created in the form of 'xsp-sety' where 'x' denotes number of species and 'y' represents the number in the draw. for example, the first dataset containing  <dig> random species is denoted by the abbreviation '10sp-set1' and the other datasets are named in the same manner. the lists of species in all datasets can be found in supplementary material .

the first step in pre-processing involves the extraction of fragments from the collected datasets. since rrna and trna sequences are very similar among different species  <cit> , they are likely to interfere with the clustering process when nucleotide frequencies are used as training features. therefore, we take the precaution to reduce noise by avoiding the inclusion of these rna sequences. this noise reduction is also valid in practical situations because all these rna sequences can be easily identified based on sequence similarity against public databases, e.g. rdp  <cit>  or genbank  <cit> , and are excluded from the clustering process. figure  <dig> contains an example for preparing unlabelled input vectors and seeds for clustering algorithms, and a typical extraction process is as follows:

 <dig>  identify all rrna and trna sequences in the genome.

 <dig>  obtain 8– <dig> kb length seeding sequences that flank the 16s rrna sequence . ensure that a seeding sequence does not overlap other rrna and trna sequences.

 <dig>  remove all rrna, trna and seeding sequences from the genome to obtain initial sequence fragments.

 <dig>  divide initial sequence fragments into non-overlapping 8– <dig> kb  length fragments. fragments shorter than  <dig> kb are discarded.

 <dig>  nucleotide frequencies of each sequence fragments are computed using a sliding window of the size of  <dig> bases . a total of  <dig>  combinations of nucleotide usages are represented in the vector form of  <dig> dimensions.

 <dig>  since the sequences were cut into random lengths between  <dig> kb and  <dig> kb fragments, the tetranucleotide frequencies also need to be normalised relative to the length of the fragments .

the choice to use  <dig> kb as the minimum sequence length is because the number of chimeric sequences becomes significant for sequence length <  <dig> kb using the currently available assemblers  <cit> , and the chimeric sequences will reduce the binning accuracy leading to meaningless results. the same reason explains the use of ≥  <dig> kb sequence length for the simulated metagenomic datasets created in mavromatis et al.  <cit> . the restriction of having seed length between  <dig> kb and  <dig> kb is applied to provide a standardised rule for all sequences, whether seed sequence or not, in the generation of the artificial datasets. the choice to use tetranucleotide frequency follows that of the work of abe et al.  <cit> , who tested di-, tri- and tetranucleotide frequencies as training features and found that tetranucleotide frequencies have a better species separation among the three oligonucleotide frequencies. in addition, sandberg et al.  <cit>  showed that a longer nucleotide sequence represents the genome more specifically and teeling et al.  <cit>  mentioned that the correlations of tetranucleotide usage patterns is high between intragenomic fragments and low between intergenomic fragments. therefore, the tetranucleotide frequency was used as the training feature for the clustering algorithms.

justification and identification of seeds
prior to applying the semi-supervised learning methods for binning of metagenomic sequences, small amounts of labelled data or seeds are to be identified from the datasets of sequence fragments for training the classifiers. however, exact taxons are only known for experimentally identified species, which are still few in number, making them impractical for metagenomic applications. additionally, the labelled data should not depend on the existing completed genomes and should be informative enough to describe the species. in this work, we employed the flanking sequences of conserved 16s rrna as seeds for the semi-supervised clustering algorithm. the flanking sequences of highly conserved 16s rrna sequences can be easily obtained from sequencing results or by southern blot hybridisation in the wgs genomic dna library and sequencing the positive clones. these flanking sequences are often similar within, and only within, the closely related species in terms of nucleotide frequency. this property allows the discrimination of species at a medium resolution level, although it is insufficient for binning strains of the same species.

it is well known that 16s rrna sequences are highly conserved over the evolution of organisms, such that the difference between various species can be identified from the small number of base changes in the 16s rrna sequences. some papers have exploited these small base changes to estimate the number of species in their environmental sequencing samples  <cit> . due to the highly conserved nature of 16s rrna sequences, that have highly similar nucleotide frequencies even across different species, they cannot be directly used to distinguish species by clustering that uses nucleotide frequency as the training feature. we verified this by clustering sequence fragments with the inclusion of 16s rrna fragments, and all 16s rrna sequences tended to group together to form a separate cluster . however, the flanking sequences of the 16s rrna sequences, which can be generated by sequencing with universal primers, do possess enough variations in nucleotide frequency to be used to identify different species, even for new species. hence, the flanking sequences of the 16s rrna sequences are excellent candidates for seeds. nevertheless, due to the nature of pre-processing for obtaining sequence fragments, if there are other rrna and trna sequences in a genome within the length of the sequence fragment on either end of a 16s rrna sequence, the seeding sequences will be discarded and the genome becomes unseeded. figure  <dig> shows one case where the right hand-side seed is discarded because the length of that flanking sequence is shorter than the pre-determined length . the  <dig> kb length of the left-hand-side seed was also determined by a random generator. on the other hand, all seeds in the benchmark datasets were generated using a length of  <dig> kb.

self-organising maps and growing self-organising maps
the self-organising map   <cit>  was originally developed to model the cortexes of more developed animal brains. subsequently, it became a very popular technique in the field of data mining. it is an unsupervised clustering algorithm which can visualise unlabelled high dimensional feature vectors into groups on a lattice grid map. this is achieved by projecting high dimensional data onto a one-, two- or three-dimensional feature map that has a pre-defined regular lattice structure. in the map, every lattice point represents a node whose weight vector has the same dimension as the input vectors. the mapping preserves the data topology, so that similar samples can be found close to each other in the grid map. the grouping of samples can be visualised by comparing the distance between nodes or simply the inputs that have been projected onto the same node. som uses a static lattice structure where the size and shape of the lattice is defined before training and remains unchanged throughout training. the som algorithm separates training into three phases: initialisation of the map, ordering and fine tuning. the initialisation method of node weight vectors and choice of number of nodes in the map can be crucial to achieve a good quality clustering result for som. generally, the principle components analysis  is used for initialising the som  <cit>  by positioning a fully unfolded map on the plane formed by the first two principle vectors in the input space. the number of nodes, which represents the resolution of the map, needs to be determined by the user. in the ordering and fine tuning phases, each input is presented to the map and a 'winning' node, which has the smallest euclidean distance to the presented input, is identified. the weight vector of the winning node and its neighbouring nodes are updated by

  w = w + α × h × . 

where w is the weight vector of the node, x is the input vector , k is the index of the current input vector, α is the learning rate and h is the neighbourhood kernel function.

the growing self-organising map   <cit>  is an extension of som. it is a dynamic som, which overcomes som's weakness of a static map structure. as with som, gsom is used for clustering high dimensional data and employs the same weight adaptation and neighbourhood kernel learning as som, but has a global parameter of growth named growth threshold  that controls the size of the map.

the growth threshold is defined as

  gt = - d × ln. 

where d is the dimensionality of data and sf is the user-defined spread factor that takes values  <cit> , with  <dig> representing minimum and  <dig> representing maximum growth.

there are four phases of gsom training: initialisation of the map, a growing phase and two smoothing phases. in the initialisation phase, the gsom is initialised with a minimum single 'lattice grid', depending on whether the rectangular or hexagonal network topology is used. for gsom that uses a rectangular topology, the minimum single lattice grid consists of four nodes that are connected as a rectangle; when using the hexagonal topology initialisation, there are seven nodes that form a hexagon with an additional node in the centre that is linked to all six other nodes. more details of initial lattice are described in  <cit> . during the growing phase, every node keeps an accumulated error counter and the counter of the winning node  is updated by

  ewinner  = ewinner  + ||x - wwinner || 

if the winning node is at the boundary of the current map and ewinner exceeds gt, new nodes will be added to the surrounding vacant slots of the winning node. weight vectors of the new nodes are created by interpolating or extrapolating the weight vectors of existing nodes around the winning node. in the case when ewinner exceeds gt and the winning node is not a boundary node, ewinner is evenly distributed outwards to its neighbouring nodes. the two smoothing phases are for finetuning the weights of nodes and no new node will be added to the map. in this paper, the 2-d hexagonal lattice was used for gsom, since the hexagonal lattice yields better data topology preservation  <cit> .

seeded growing self-organising map algorithm
it is sometimes difficult to visually identify clusters in a trained gsom. previously, we developed a region identification algorithm to automatically identify the clusters based on the distance map which visualises the distance of the weight of node to each of its neighbour nodes  <cit> . however, in binning, sequence fragments of closely related species will most likely have homologous sequences present in between the clusters that occlude the cluster boundaries in the distance map and cause the region identification algorithm to fail to identify the correct clusters. therefore, a method is needed to identify clusters in gsom to make the clustering approach of gsom practical for binning.

the core concept of seeded gsom  is to automatically identify clusters in the feature map using the already-available labelled samples . the algorithm consists of three core procedures. firstly, the very small amounts of available or selected seeds  are combined with other unlabelled samples . secondly, the combined samples  are presented to gsom for training in which the seeds are treated the same as the unlabelled data. finally, after the normal phases of gsom training, s-gsom performs an extra phase, the cluster identification phase, as post-processing. this phase identifies clusters based on the locations of seeds in the trained gsom and the specified amount of nodes to be clustered. the flowchart of the overall s-gsom algorithm is given in figure 2a.

in the cluster identification phase, the seeded nodes, which are nodes that contain the seeds, are identified in the gsom. an assigning process will then assign the un-clustered nodes  to clusters. figure 2b provides the pseudo-code of this assigning process. the number of nodes that will be assigned to a cluster is specified by the clustering percentage , which is the percentage of the number of clustered nodes to the total number of nodes. the assigning process first creates the initial set of clusters with only the seeded nodes . the process then iteratively assigns the un-clustered nodes, one by one, to clusters. in each iteration, a set of un-clustered nodes that is adjacent to the clustered nodes is identified. the node within the set that is the shortest euclidean distance from its neighbouring clustered node will be assigned to the cluster of that clustered node . however, nodes that do not contain any sample can exist, and these empty nodes most likely represent the boundary of clusters. therefore, a penalty factor greater than one is multiplied to the actual distance when calculating the distance from an empty node to clustered nodes . this will force the algorithm to avoid clustering empty nodes, thus favourably completing the assignment of its own cluster before jumping into other clusters. it is empirically observed that clustering results are not very sensitive to the penalty factor between values of  <dig> and  <dig>  and a penalty factor value of  <dig> was used in all our experiments.

to illustrate the role of s-gsom in binning, figure  <dig> shows the schematic diagram that explains how s-gsom fits into the whole binning process. in the binning process, a bin is created for all seeds that are obtained from one 16s rrna that is the phylogenetic marker of a taxon. a bin may contain multiple seeded nodes that contain the seed sequences. in the process of assigning labels to unlabelled nodes, the taxon label of the seeded node needs to be determined. it is straightforward when there are only seeds that come from the same taxon. however, when seeds in a node come from different taxa, the node will have the label of the taxa which the majority of seeds belong. and if the numbers of seeds for all taxa are the same, e.g.  <dig> seeds are in the same node, one belongs to taxon a but the other one belongs to taxon b, all seeds are discarded.

another semi-supervised version of gsom was also developed by two of the co-authors, focusing on the creation of a gsom classifier from data with up to 40% of missing labels and 25% of missing attribute values  <cit> . two other methods are also conceptually related to s-gsom. seeded region growing  is a concept known in computer vision in which an image was grown from the known seeds . for example, in adams and bischof  <cit> , segmentation of images was achieved by incrementally assigning pixels to the seeds. although the fundamental concept in srg is similar to s-gsom, there are the following major differences:

• the locations of the seeds in nodes are determined by the base clustering algorithm in s-gsom, in contrast to srg where the pixels are selected on the image as seeds. therefore, seeds with the same label will all be connected in srg whereas in s-gsom they, and the resulting clusters, may scatter.

• s-gsom compares the euclidean distance between the border nodes of the clustered sets with its adjacent nodes of the un-clustered sets. srg compares the mean grey value of the clustered sets with the surrounding pixels in the un-clustered sets.

• s-gsom provides cp as the stop condition but srg stops when all pixels have been allocated to clustered sets .

the other related method is a semi-supervised som algorithm based on label propagation in a trained high resolution som  <cit> . it assigns a label vector, which contains the probabilistic memberships of the node in the available clusters, to each node. batch training is performed by updating label vectors of all nodes through a probabilistic transition matrix, derived from distances between nodes. the algorithm stops when there is no change to any labels of unclassified nodes and all unlabelled nodes are classified to one of the clusters. this algorithm is similar to s-gsom in that it also determines the unlabelled samples by propagating the label information from small amounts of labelled samples. however, the objectives of the two algorithms are different, leading to the following additional differences between the algorithms:

• the intention of s-gsom is to stop the assignment before reaching the cluster boundaries, which is potentially the mixing region of classes in the application of binning. alternatively, s-gsom can be considered as a filtering algorithm to filter out the noisy sequences which were clustered by the normal gsom. the label propagation of som stops when all nodes are clustered and the algorithm converges.

• the partial assignment feature of s-gsom assists users to identify clusters without seeds , whereas the label propagation of som will form the clusters only with the given labelled samples, and thus cannot be used to identify sequences of unknown organisms.

• s-gsom requires less computational power because it only operates on the surrounding nodes of clustered nodes in each step, whereas the label propagation of som updates all nodes in each iteration.

RESULTS
semi-supervised learning methods for binning
in order to provide an independent evaluation from the datasets used in mavromatis et al.  <cit> , we generated datasets that have different sizes, species and equal abundance to mimic multi-class datasets for testing other semi-supervised clustering algorithms on species separation. genomic sequences were randomly sampled from the  <dig> completed microbial genomes available in the ncbi genome database , which include both archaea/bacteria genomes  <cit> . the sampled prokaryotic sequences were cut into random fragments between  <dig> kb and  <dig> kb in length, and the tetranucleotide frequency of each fragment was computed . a total of  <dig> datasets were created, three of which contain  <dig> species, another three contain  <dig> species and one contains  <dig> species .

four other well-known semi-supervised clustering algorithms, cop k-means  <cit> , constrained k-means  <cit> , seeded k-means  <cit>  and the transductive support vector machine   <cit>  , were used here alongside s-gsom as a feasibility study of semi-supervised methods for binning metagenomic sequences. some features of the proposed s-gsom are explained here. other semi-supervised algorithms are described in the literature. the choice of the user-defined parameter called clustering percentage , used to specify the percentage of nodes that will be assigned to the seeded clusters relative to the total number of nodes in the feature map, was determined through experimental studies. as in our previous investigation  <cit>  and in abe et al.  <cit> , it was identified that ambiguity of sequence fragments for closely related species occurred mostly at the cluster borders. an appropriate level of cp is necessary in order to avoid assigning too many sequence fragments and consequently incorrectly assigning fragments that are highly ambiguous. preventing the algorithm from assigning ambiguous fragments is desirable, since it is very unlikely, even in a highly sophisticated procedure, to gather enough information to confidently distinguish the species of these highly ambiguous fragments. it was noted in our binning experiments that the clustering performance of s-gsom declined when the cp was higher than 55% . while more than 80% of sequence fragments were assigned in most cases at cp = 55%, there was little reason for increasing the cp further to assign the remaining small amounts of fragments and taking the high risk of fragments being assigned inaccurately. therefore, cp = 55% was used throughout the experiments in this paper. it should be noted that it is the intrinsic nature of gsom that not all nodes have the same number of clustered sequence fragments. furthermore, in the node assignment process of s-gsom, nodes that clustered the highest density of sequence fragments tend to be assigned first. therefore the amount of clustered sequences is not linearly proportional to the clustering percentage cp. a graph in section  <dig> of the supplementary materials is provided to illustrate this . one can also consider cp as a confidence threshold, analogous to the p-value in phylopythia, and opt to use a higher cp value to assign more contigs to bins with lower confidence, where a high cp value is equivalent to a low p-value in phylopythia.

the clustering performance of the semi-supervised algorithms were measured by two popular methods of evaluating multi-class clustering quality to ensure the validity of the results: adjusted rand index  <cit>  and weighted f-measure  <cit>  . these methods compare the clustering performance of algorithms by the calculated indices. a higher index indicates a better clustering accuracy, and a larger difference between indexes means more significant results. the implementation and settings for semi-supervised algorithms were as below :

 <dig>  cop k-means, constrained k-means and seeded k-means, which were implemented as described in literature, were trained with k-centres where k equals to the number of species in the dataset.

 <dig>  tsvm used the processing methods laid out for multi-class problems  <cit>  and svm-light package  <cit> .

 <dig>  s-gsom was implemented as described in this paper. all results in this paper were obtained using the gsom settings as specified in section  <dig> of the supplementary materials .

in the above methods, different runs of random initialisation of the cop k-means and s-gsom can lead to varied results. as constrained k-means and seeded k-means use the labelled sample for initialisation, there is no such issue with these methods. the results for cop k-means are the best results in  <dig> runs, with different random initialisation of the k cluster centres. different initialisations for gsom can result in different maps. however, the initialisation effect on gsom is much less than on som, as gsom starts with a minimum node structure in the beginning of training to allow for fast unfolding of the map; twisted maps are less common in gsom  <cit> . furthermore, different initialisation in gsom will only have a minor effect on the final cluster formation for a well-ordered map. in all experiments reported in this paper, initialisation for all normalised dimensions was fixed at the mid value  <dig>  to ensure repeatability.

the results of binning artificially generated fragments using different semi-supervised learning methods are tabulated in table 1; the algorithm with the highest values of the two measures is shown in bold. the s-gsom visualisation of binning sequences of 10sp_set <dig>  20sp_set <dig> and 40sp_set <dig> are provided in figure  <dig> 

s-gsom showed consistently superior performance on both measures of clustering quality in all datasets tested, with the exception of constrained k-means on the ari measure for the 10sp_set <dig> dataset. tsvm has shown considerably worse performance among the algorithms. although proposed as a semi-supervised method, tsvm is derived from the fully supervised algorithm svm. therefore, we suspect that insufficient labelled data significantly reduces its capability to classify correctly. the superior performance of s-gsom is also attributed to the use of the variable cp, offering the flexibility of not assigning highly ambiguous fragments that are likely to overlap with other species. at cp = 55% between 75% and 90% of sequence fragments were assigned, whereas when all fragments were assigned  the performance was similar to other algorithms. this shows s-gsom's ability to filter out the noisy fragments to achieve better clustering performance.

we have also considered the 20-species datasets as an example to analyse the resolution of binning with s-gsom. in the 20-species results, an average of 82% of fragments were assigned at cp = 55% and of these an average of 92% were correctly assigned to their seeds . the distribution of sequence fragments according to species is shown in figure 5b. nodes that contain fragments from more than one species are coloured grey and numbered with the number of species it represents. the fact that fragments that belong to different species were being clustered to the same node is attributed to the high similarity of nucleotide frequencies between very closely related species. for example, species nc_ <dig>  and nc_ <dig>  are represented by nodes with labels 'c6' and 'c7' respectively. there was a significantly higher number of grey nodes at the boundary of these two species than of others. further taxonomical examination of these two species revealed that they are phylogenetically close and are in the same family, pasteurellaceae. it is also identified that all seeds of these species are located near the boundaries of clusters. this further highlights the importance of obtaining seeds in non-boundary regions.

a prominent advantage of the proposed seeding method compared with other semi-supervised clustering algorithms is the ability to identify a small number of species that do not have any fragment that qualifies as a seed. in order to demonstrate this advantage, an iso-cp  plot is shown in figure 6a, generated with sequence fragments from  <dig> species  in which there is one unseeded species and the seeds of other species are represented by unique colours. nodes in charcoal colour represent nodes that will be assigned when cp = 27% and dark grey nodes at the recommended value of cp = 55%, light grey at cp = 77%, and white at cp = 100%, respectively. figure 6b shows the allocation of nodes to seeds at cp = 55%, where it can be seen together with figure 6a that there is a largely unassigned region at this point, and increasing the cp to 77% will result in a rapid assignment of many nodes. this situation is most likely when a species is relatively abundant, but does not have a seed to allocate nodes to. however, incorrect assignment of nodes can sometimes occur at a low cp. for instance, figure 6b shows a protrusion of species '1' into the unassigned region, which actually belongs to species '5' that does not have any seed.

binning of sequence fragments in simulated metagenomic datasets
three simulated metagenomic datasets, which vary in relative abundance and number of species, were created by mavromatis et al.  <cit>  and placed in the fames database  <cit>  to facilitate benchmarking of metagenomic data processing methods, which include, but are not limited to, binning methods. the sequence fragments in the simulated datasets were assembled using three commonly used sequence assembling programs, arachne  <cit> , phrap  <cit>  and jazz  <cit>  at the u.s. department of energy, joint genome institute. in this section of the experiment, we test the performance of s-gsom in binning against three binning methods: blast, k-mer and phylopythia, reported on the datasets .

the three simulated datasets can be designated as being low complexity, medium complexity or high complexity. the low complexity  dataset is dominated by one near-clonal species together with a few low-abundance ones, which simulates a microbial community in an environment such as a bioreactor  <cit> . the medium complexity  dataset mimics a community structure similar to an acid mine drainage biofilm  <cit> , which consists of more than one dominant species flanked by low-abundance ones. the high complexity  dataset, on the other hand, does not include any dominant populations.

two considerations were taken into account when comparing the reported binning results with s-gsom. firstly, the results in  <cit>  showed that sequence reads assembled by jazz produce a very small number of binned contigs compared to the other two assemblers arachne and phrap, which greatly diminishes the purpose of binning. therefore, fragments assembled by jazz are excluded from our analysis. secondly, the simhc dataset resembles a complex community structure that can be common in reality and a well-performing binning method is therefore highly desirable. however, a complex community that has no dominant populations has insufficient dna sequences of the same species to form longer contigs. as reported in literature  <cit> , composition-based analysis requires a sufficiently long sequence length to ensure the accuracy of binning. therefore, the simhc dataset is also excluded from our analysis.

like other binning methods, s-gsom can form bins at different taxonomic levels. bins that have different labels in a lower taxonomic level may belong to the same higher taxonomic level and can be combined to form higher taxonomic bins, thus accuracy is higher but at the cost of lower taxonomic resolution. for the purpose of fair comparison, all methods need to be compared at the same taxonomic level of binning. binning at a very high level clearly has no significance, therefore the results are compared at the order level here and results for comparing at other taxonomic levels are included in the supplementary materials .

two confidence settings of s-gsom  were tested on the simulated metagenomic data, which correspond to two different settings of phylopythia. the s-gsoms were trained using parameters specified in section  <dig> of the supplementary materials and the final feature maps are also provided there. the binning results of s-gsom, phylopythia, k-mer and blast on the simlc  and the simmc  datasets are reported here. at the order level, the results for a different complexity dataset are shown in two separate tables, one for binning contigs greater and equal to  <dig> kb length and one for binning contigs with at least  <dig> reads. details of the datasets at different taxonomic levels are also provided in section  <dig> of the supplementary materials. performance evaluation results at class level and family level are included in section  <dig> of the supplementary materials and all contig assignments can be downloaded from s-gsom homepage  <cit> . note that the published results in mavromatis et al. used simple averages of all bins regardless of their sizes . here we use a weighted average that gives higher weighting to larger bins to better reflect the amount of correctly binned contigs. the exact step of performance evaluation is provided in section  <dig> of the supplementary materials.

total#contigs: total number of contigs in the dataset; %ofbincontigs: the percentage of contigs binned; #ofprednotinact: the number of contigs predicted as a taxon that is not present in the dataset, which are treated as the un-binned contigs; wsp: weighted specificity; wsn: weighted sensitivity.

the results in table  <dig> and  <dig> showed that s-gsom performed reasonably for binning contigs longer than  <dig> kb, where it is more accurate than all settings of k-mer and blast binning methods, but was outperformed by phylopythia in both confidence settings  regardless of data complexity and the assembler used. nevertheless, s-gsom still performed better than phylopythia for the simmc, particularly in terms of sensitivity, i.e. having a higher true positive rate, when the reference taxonomic level was at the family level  of the supplementary materials). while phylopythia performed best for all ≥  <dig> kb tests, the s-gsom binning method was the best-performing method when used to bin contigs that have at least  <dig> reads .

considering the fact that s-gsom is the only one amongst the tested binning methods that does not use any knowledge of completed genomes, this performance has demonstrated its feasibility, particularly for analysing metagenomic communities that contain a number of unknown organisms that lack similarity to known genomes. s-gsom has proved itself competitive with the most sophisticated binning algorithm, phylopythia, over which s-gsom has an advantage in being easier and faster to build because it does not require the use of reference genomes for training.

discussion and 
CONCLUSIONS
recently, the importance of environmental genomics has been recognised  <cit>  and more environmental sequencing projects are being undertaken  <cit> . most of the current methods of binning compare a sequence fragment to a reference set of genomes. such methods tend to have high assignment accuracy particularly when a strong similarity exists between the reference and environmental genomes, either at the compositional or sequence level. however, the performance declines drastically if the best match found is still quite different from the one being queried. on the contrary, extracting reference sequences from within the metagenome, as in s-gsom, will ensure better matching. it also facilitates a means of exploring species that have not yet been identified. in fact, such reference information from longer assembled contigs was used in the binning process of two prominent metagenomic projects for low-diversity microbial communities  <cit> .

s-gsom enables the clustering of sequence fragments, with phylogenetical meaning, by using very small amounts of sequence fragments around the highly conserved genes as seeds. it has several advantages over other unsupervised and semi-supervised clustering algorithms. first and most importantly, due to its visualisation property, s-gsom allows the user to visually identify one or two relatively abundant species that do not have any seed, as demonstrated earlier, by using cp contour display together with inter-node distances. however, to identify unseeded clusters, it is necessary to have a larger number of fragments in the unseeded cluster, at least as many as the seeded clusters, i.e. the species is relatively abundant. if the unseeded clusters have far less samples than the seeded clusters, the unseeded cluster may be assigned to the other clusters at low cp values, or be considered as part of the border of neighbouring clusters and thus become hardly detectable. on the other hand, if the unseeded clusters have far more samples than the seeded clusters, the clustering percentage needs to be set to a lower value, as otherwise there will be more incorrectly assigned samples.

the other advantages of s-gsom include: 1) sequences can easily be reassigned without retraining by varying the cp that is directly related to confident assignation, 2) the ability to function as a fully automated clustering process, 3) the potential use as a visualisation aid for identifying unseeded clusters with iso-cp contours and inter-node distances, and 4) this technique can easily be applied to other applications when the labelled data is very limited but a lot of unlabelled data is available.

for the discovery of sequences of unknown microorganisms using s-gsom, it should be expected that seeds may or may not be available. when seeds are available, s-gsom allows the identification of relationships between the seed-associated clusters based on phylogenetic analysis of 16s rrna sequences. if the phylogenetic analysis detects any unknown microorganism, we will be able to discover the sequences associated with this organism from its 16s rrna sequences using s-gsom. this function will be of great assistance in sorting sequences in metagenomic datasets according to phylogenetic relationships. however, if no seed is available, it means that there is no 16s rrna sequence in the datasets for assessing the phylogenetic relationships of this cluster. in such circumstances, we can still obtain the sequences in the possible bins, which were identified by using the iso-cp contour map, then compare the sequences with existing databases by blast searching. if any marker gene is detected, such as elongation factors and/or cytochrome oxidase, then we may assess whether these sequences are from unknown or known microorganisms by phylogenetic analysis.

in the results section, we noted a major requirement for the optimal performance of s-gsom. as the proposed algorithm uses seeds to assign more unknown fragments, poor seeds that are at the boundary of clusters, which are ambiguous in themselves, can greatly affect the resulting assignment. this problem can possibly be solved by observing the data distribution density around the seeds. for instance, if a seed occupies a single node and its surrounding nodes also contain fewer samples, the seed has a high chance of being a poor seed. while a choice of cp = 55% yielded accurate sequence assignment in the two separate tests, a user can opt to use a lower clustering percentage  for more confident assignment at the cost of fewer sequences being assigned. conversely, using a higher cp value  will assign more fragments but risk incorrect assignments. this feature is also evident in the tests that use simulated metagenomic data, where s-gsom traded specificity for an increase in sensitivity.

although these composition-based binning methods have shown good results, currently they are hindered by the requirement of long sequence length. this limitation of length is partially due to the occurrence of chimeric sequences from cloning procedures of experiments and from the incorrect assembly of sequences. the former source of chimeric sequences can be reduced by improving the sequence strategy, such as using a roche  <dig> genome sequencer flx, which excludes sequence cloning and hence generates less chimeric sequences. the latter source of chimeric sequences is caused by the incompatible design of the currently available assemblers. these assemblers are commonly designed to assemble all reads into one single genome. however, this application does not satisfy the requirement of metagenomic datasets which are of poor coverage depth and contain multiple genomes. as a result, it leads to the occurrence of chimeric sequences when highly conserved stretches of sequences, e.g. transposases, are shared by multiple species or strains. mavromatis et al.  <cit>  tested the simulated metagenomic datasets and showed that the numbers of chimeric sequence became significant for the assembled short sequences, e.g. <  <dig> kb, and led to the low quality of binning. therefore, if the number of chimeric sequences is reduced, the required sequence length can also be reduced. to help the reduction of chimeric sequences, we suggest including the compositional information in the assembling level.

authors' contributions
ckkc designed the overall study, implemented the algorithm and performed analysis. alh contributed to the design of the experiments and interpretation and analysis of results. slt designed the algorithm with ckkc and biological aspects of the study. all authors contributed to the preparation of the manuscript, also read and approved the final manuscript.

supplementary material
additional file 1
supplementary materials. this file is in pdf format. section  <dig> provides details of all datasets used in this article. section  <dig> gives descriptions of the semi-supervised algorithms, cop k-means, constrained k-means, seeded k-means and transductive svm, used in the comparison with s-gsom. section  <dig> describes two popular clustering quality measures, the adjusted rand index and weighted f-measure, in detail. sections 4– <dig> provide more information on the experiments with simulated metagenomic datasets. section  <dig> briefly describes the three binning algorithms, blast, k-mer and phylopythia. section  <dig> describes the steps used to evaluate binning results of methods. section  <dig> shows the detailed results of s-gsom for all individual bins at different taxonomic levels for the simulated metagenomic dataset tests. section  <dig> provides the training parameters used for s-gsom and the resultant trained gsom maps. section  <dig> shows an example graph to demonstrate the relationship between cp value and the percentage of assigned samples. finally, section  <dig> lists the references used in the supplementary material.

click here for file

 acknowledgements
we thank k. mavromatis of doe joint genome institute for evaluating the results for s-gsom on their simulated metagenome data and for discussing and clarifying their results.
