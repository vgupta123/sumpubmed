BACKGROUND
large repositories of gene expression data are currently available and serve as online resources for researchers, including the gene expression omnibus , the center for information biology gene expression database , the european bioinformatics institute's arrayexpress and the stanford tissue microarray database  <cit> . repositories for gene expression data such as geo allow for widespread distribution of gene expression measurements in order to:  validate experimental results,  enable progressive accumulation of data that may support, modify or further develop prior work, and  facilitate use of archived measurements to generate novel hypotheses that naturally develop from continuous updating of accumulated data. although geo contains a vast amount of measurements from numerous samples, the link between measurements and phenotypic characteristics of each individual sample, including the sample's disease and tissue type, is not readily accessible because they are encoded as free text. furthermore, there are no standardized documentation rules, so phenotypic and/or protocol information resides in multiple documents and physical locations. such information may be included as text describing the experiment or protocol, sample and sampling descriptions, or may be found only in the published journal article that may accompany the submission. in order to increase utility and improve ease of use of this resource, data should be readily available and easily comprehensible, not only for researchers, but also for automatic retrieval. in particular, the data have to contain sufficient detail to allow for appropriate combination of similar experimental subjects and protocols that may then collectively facilitate the verification, support, or development of new hypotheses.

many centers have focused on re-annotating biomedical data with the goal of increasing utility for researchers. the promise of fast-paced annotation amid rapid accumulation of data has spurred great interest in progressive development of automated methods  <cit> . to date, manually annotated data is the de facto gold standard for most annotation efforts  <cit> . therefore, it becomes critical to ensure that manually annotated data are accurately described and evaluated.

several attempts directed specifically at annotation of gene expression data have been performed and remain the subject of ongoing work. in particular, geo datasets  are being developed to systematically categorize statistically and biologically similar samples that were processed using a similar platform within a single study  <cit> . the process typically begins with a geo series , defined as an experiment deposited into geo that contains descriptions of the samples within the experiment, usually provided by the investigator. a gse is then characterized into a data set. this phase is performed manually, with reviewers adjudicating whether or not experiments are comparable, which of them should belong in a dataset, and what axis differentiates samples from each other within a dataset. some commonly used axes include the disease state and the cell line. table 1a illustrates common descriptions that are given for samples within a gse that correspond to various axes. there are  <dig> distinct axes that are currently in use. each gds, however, only utilizes a few axes, at the discretion of the curators. in addition, while the axes used to group samples are controlled, the values corresponding to these axes are typically provided as free text. the vocabulary used to describe the values within an axis is neither standardized nor controlled. to illustrate, breast cancer is entered as a value for a "disease state", whereas breast tumor is entered as a value for "cell line" in the sample excerpted in table 1b. moreover, the reference to breast tumor is ambiguous under "cell line" because this axis should specifically refer to breast cancer instead of tumor, given that these cell lines refer to models of neoplastic diseases.

it is not surprising, therefore, that re-annotating geo and other large microarray data repositories is the focus of several groups. in particular, automatic text processing is being used to capture disease states corresponding to a given sample from gds annotations. in a recently published article in which the objective was to identify disease and control samples within an experiment, the gds subsets were analyzed using representative text phrases and algorithms for negation and lexical variation  <cit> . although this algorithm was successful in identifying 62% of controls, the study was evaluated using only  <dig> samples, and it highlighted an urgent need for a methodical solution for annotating geo using a controlled vocabulary. another study performed re-annotation of the stanford tissue microarray database using the national cancer institute  thesaurus  <cit> . they were successful in representing annotations for 86% of the samples with 86% precision and 87% recall, but the study was evaluated using only  <dig> samples. while diagnosis remains as one of the most useful annotation points for a given experimental sample, there are many more categories of interest to investigators and users. for example, treatment interventions, sample demographics , and various phenotypic information that affects gene expression. a re-annotation of these rapidly growing repositories has to take into account all these variables and the use of a controlled vocabulary for identifying sample variables and values.

we therefore describe a large-scale manual re-annotation of data samples in geo, including variable fields derived from the nci thesaurus and corresponding values that also utilize primarily controlled terminology  <cit> . the objective is to create an annotation scheme for various disease states that is flexible, comprehensive and scalable. we subsequently present a framework for evaluating the annotation structure by measuring coverage and agreement between annotators.

methods
three sections below specifically:  enumerate the iterative process used for developing an annotation structure,  describe the annotation tool and the annotators' characteristics, and  describe the framework for evaluation.

an iterative process was designed for identifying the variables selected for annotation, as follows:

 <dig>  variable generation – human experts develop a list of variables for annotation. this procedure is based on guidelines and publications that are related to the disease category. variables were then trimmed based on consensus among three physicians.

 <dig>  supervised domain annotation – a trained annotator was instructed to start annotating the given variables under physician supervision. whenever a variable deemed important was identified, it was listed for further deliberation. the process was then repeated – back to number  above, until no further variables were identified or the amount of samples for preliminary annotation was reached .

 <dig>  unsupervised annotation – a trained human annotator then performed unsupervised annotation independently, after receiving a standardized, written instruction protocol. instructions were specifically developed for each disease category. two human annotators were assigned to code each data sample. randomized assignment between annotators was performed by disease category to minimize the occurrence of two coders being assigned to annotate the same disease category  repeatedly.

 <dig>  disagreement and partial agreement identification – after the human annotators finished coding their assigned experiments, the data was compiled and the assigned values were compared to measure agreement. the method to assess agreement is further described below.

 <dig>  re-annotation – finally, the samples containing values that were not in agreement initially were re-annotated and the correct annotation was determined by a majority vote. in the event of a three-way tie, one of the investigators performed a manual review and final adjudication.

to ensure consistency of terminology, the nci thesaurus was utilized for the disease domains annotated, consistent with prior annotation initiatives  <cit> . this ensures that the concepts utilized all readily mapped to the unified medical language system   <cit> . therefore, scalability for using variables and values was preserved, which is valuable for future research initiatives. figure  <dig> below shows a graphical illustration of the variable and values that were utilized to annotate breast cancer.

the variable "tissue" was assigned several different values, one of which was "breast." this assignment provided flexibility, allowing for addition of other tissue types, whenever the disease domain changes. there was also sufficient granularity to allow for actual interrogation into the database for future hypothesis generation or validation. a full description of the web-based annotation tool and the quantity of samples annotated over time is described in a separate paper  <cit> .

evaluation of annotations
there were a total of six annotators, including four senior biology students, one graduate student in the biological sciences field, and one physician. as noted previously, each sample had at least two annotators assigning values to variables. the annotation task was to provide phenotypic information for each data sample that was available in geo for breast and colon cancer, ibd, dm, sle, and ra. thus, it was critical to obtain standardized values for most of the annotation variables to ensure that the annotations would be consistent. this entailed a review of data descriptions listed in various sources – the data sets , series information  and sample information . in addition, information was available in supplementary files and in published scientific articles, which are not in geo. manual review of all these data sources was necessary to obtain sufficient variable coverage. coverage was defined as the percentage of non-'unknown' values that were assigned to a variable. specifically, it can be represented as:

coverage = x/y, where x represents the number of variables with values that are not "unknown." y represents the total number of variables that were annotated.

to validate the reliability of the annotation scheme, we computed the percentage of agreement between annotators, defined as the number of variables for which both annotators gave the same value, divided by the total number of variables that were annotated. we calculated percentage agreement for each level of similarity across all disease categories.

RESULTS
data description
a substantial fraction of geo, including  <dig> platforms,  <dig>  studies, and  <dig>  samples were extracted into the analytical database. among them, several disease categories are represented, but only  <dig>  samples  are included in various gds subsets. over a period of five weeks,  <dig>  samples  from a limited set of disease categories were annotated, as shown in table  <dig>  many of these did not have annotations in gds.

in addition, for each disease category, a comprehensive and controlled set of phenotypic variables were provided, as shown in table  <dig>  for each disease category, between  <dig> and  <dig> variables were identified . to our knowledge, this constitutes the largest re-annotation initiative performed on gene expression data to date.

past breast cancer
cd classification
the next goal was to provide adequate coverage for as many variables that were identified. table  <dig> shows the top  <dig> most commonly annotated variables and their coverage. as shown in table  <dig>  the sample tissue, cell line and disease states were most frequently annotated and were rarely "unknown". these were probably the most pertinent variables and likely the subject of most re-annotation initiatives. therefore, it was critical that values for these variables were available and actually annotated.

inter-annotator agreement results are shown in table  <dig>  there is  <dig> % strict agreement. there was a  <dig> % difference between strict and semantic agreement in this study. further improvement in agreement  was observed when partial similarity was measured.

overall, there was excellent inter-annotator agreement across multiple disease domains. table  <dig> shows examples of the most common types of disagreements that we observed between annotators. most commonly, one annotator labels a sample variable  as "unknown," while another annotator labels the same variable with the value "no" .

discussion
repositories for gene expression data such as geo are expanding very rapidly  <cit> . however, the critical details necessary for understanding the experiments and sample information are encoded as free text and are not readily available for analysis. we described a large scale re-annotation performed on a substantial portion of the geo consisting of  <dig>  samples. our large scale re-annotation was accomplished within a reasonable amount of time – completed within only five weeks. in addition, we were able to accomplish annotations of samples in great detail. the annotations used controlled terminology from the nci thesaurus, with the advantage of allowing generalizability of the annotations for other research applications.

this study's re-annotation evaluation was performed on sample quantities that are two orders of magnitude higher than most prior reports  <cit> . a major contribution of this research effort includes the massive amount of well-annotated data, with substantial coverage for a large number of phenotypic information and with excellent accuracy, particularly at the semantic level.

we also described the methodology used for identifying relevant variables in each disease category. this iterative process is efficient and provided a mechanism for identifying relevant variables for domain categories. this technique provides a framework for inducing structure of a specific domain in an iterative and consultative manner. excellent inter-annotator agreement confirmed that the annotation variables were robust and easily identifiable.

finally, we provided a framework for measuring inter-annotator agreement. apart from strict agreement measured using exact string matching between variable values, we defined and considered two other similarity categories that were known to be especially useful for annotations that relied heavily on free text. we showed an improvement in agreement using these more lenient similarity measures. the degree of improvement was mitigated by the very controlled terminology from the nci thesaurus that annotators utilized, and was augmented by the annotation tool. several studies use semantic similarity as a measurement of agreement in annotation of microarray data  <cit> . several other studies use partial agreement, especially when annotated text contains fragments that are not exactly similar  <cit> . manual curation is usually the gold standard and determines whether terms that were used are semantically appropriate or not  <cit> . our results show better strict, semantic, and partial agreement compared to most other re-annotation studies  <cit> .

CONCLUSIONS
phenotypic annotations and data sample information are critically important for translational research. in particular, it is important to have good coverage for vital information, specific to clinical domain, as well as providing accurate annotations. we show that it is possible to perform manual re-annotation of a large repository in a reliable and efficient manner.

competing interests
the authors declare that they have no competing interests.

authors' contributions
all authors were involved in designing the study, and developing the annotation structure. likewise, all authors were involved in the annotation system design and development of the user interface. after the initial pilot phase, rl, ch and lom were involved in further variable identification and selection. ch was involved in performing some of the annotation in the pilot phase. rl was involved in supervising the entire annotation process and evaluating annotation quality. all authors contributed to preparation of this manuscript and read and approved the final version.

