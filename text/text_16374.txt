BACKGROUND
recent advances in sequencing technologies have enabled the completion of a growing number of individual genomes, including several cancer genomes . while whole-genome sequencing provides a near-complete catalog of variants and individual genotypes, sequencing of mrna transcripts  is becoming the method of choice for studying functional implications of genetic variability  <cit> . in particular, rna sequencing is an important source of information for studying the effect of genetic variation on transcription regulation and establishing causal relationships between mutations and disease. for cancer research, comparison of rna-seq data generated from normal tissue and tumor samples can provide the information needed to discover driver mutations or to find new therapy targets  <cit> .

analysis of rna-seq data poses several challenging computational problems  <cit> . first, eukaryotic mrna transcripts are typically the result of splicing, whereby non-coding regions called introns are removed from the pre-mrna molecule. this makes the use of tools for mapping of dna reads to the reference genome like maq  <cit>  or bowtie  <cit>  not suitable for finding the genomic location of reads spanning splicing sites. several methods based on spliced alignment have been proposed to identify splicing sites and assemble full transcripts  <cit> , however these methods incur a high computational cost and require very high sequencing depth, typically with paired reads. even when accurate read mapping is achieved, differences in transcription levels result in unequal sequencing depths of different transcripts, making it difficult to identify variants in regions transcribed at low levels. although it is possible to overcome this difficulty by sequencing both genomic dna and mrna and identifying variants from the genomic dna reads using standard methods, when the interest is in expressed variants it is significantly more cost effective to identify them directly from mrna reads  <cit> .

our main contributions are an efficient strategy for accurate mapping of mrna reads and a new method for single nucleotide variant  detection and genotyping. note that we use the term snv instead of the better known term snp  because snps are normally defined relative to a population and imply a minimum minor allele frequency whereas we are interested in finding and genotyping in an individual all sequence variants that do not match the reference genome sequence, regardless of their frequency in the population. to improve the success rate and accuracy of read mapping, we map mrna reads against both the reference genome and a transcript library such as the consensus coding sequences  database  <cit>  and then combine mapping results using a simple rule set. our method for snv detection and genotyping is based on computing, for each locus, conditional probabilities for each of the ten possible genotypes given the reads, and then choosing the genotype with highest posterior probability using bayes' rule. the underlying probabilistic model assumes independence among reads and fully exploits the information provided by base quality scores. unlike other widely used bayesian methods  <cit> , we consider all four possible alleles at each position, and do not apply a separate test of heterozygosity.

we validated our methods on publicly available illumina rna-seq datasets generated from blood cell tissue of three individuals extensively genotyped as part of the international hapmap project  <cit> . the results indicate that the combined mapping strategy yields improved genotype calling accuracy compared to performing genome or ccds mapping alone and that our snv detection and genotyping method is more sensitive than existing methods at equal levels of specificity. we also assess the effect on sensitivity and specificity of commonly used data curation strategies such as read trimming, filtering of read copies to correct for variable transcription levels and pcr artifacts, and imposing minimum allele coverage thresholds.

methods
mapping strategy for mrna reads
mapping mrna reads against the reference genome using standard mapping programs such as bowtie  <cit>  or maq  <cit>  does not require gene annotations but leaves reads spanning exon junctions unmapped. spliced alignment methods such as  <cit>  could theoretically overcome this difficulty but in practice they are computationally intensive and not well suited for very short reads. on the other hand, mapping against a reference transcript library like the consensus coding sequences database   <cit>  recovers reads spanning known splicing junctions but fails to recover reads coming from unannotated genes.

we decided to map reads both against the reference genome and the reference transcript library and to implement a custom rule set for merging the two resulting datasets. we implemented two approaches that we called hard merging and soft merging. for hard merging, we require unique alignments against both references and agreement between them while in soft merging we relaxed the uniqueness constraint by requiring a unique alignment to at least one reference and keeping that alignment. for both approaches we keep reads that map uniquely to one reference and do not map to the other one. table  <dig> summarizes the decision rules applied to each read by each approach, depending on how the read mapped on each reference and on the concordance between the two alignments. one important issue is how to deal with reads aligned to genes with multiple isoforms. after mapping onto the reference transcriptome, multiple alignments can be reported for some reads not because there exist different genomic locations where the read could come from but because the same genomic location is shared by several different transcripts. after mapping against the transcripts database, our module transfers each alignment to absolute genomic coordinates, splicing accordingly if the alignment spans multiple exons, and then checks for each read with multiple alignments if all of them fall into the same genomic location. if that is the case, just one alignment is kept as unique.

snv detection and genotyping
our new bayesian method, named snvq, computes for each locus the posterior probability of each of the ten possible genotypes given the reads. for a locus i we let ri denote the set of mapped reads spanning this locus. in all bayesian methods, the posterior probability of a genotype is calculated from its prior and conditional probabilities by using the bayes rule, p=ppp. the main difference between models lies in the way conditional probabilities are calculated  <cit> . both maq and soapsnp use a different model to calculate probabilities of homozygous and heterozygous genotypes. maq uses a binomial distribution on the alleles having the two highest counts while soapsnp uses a rank test to determine heterozygosity. soapsnp also assumes as prior information that the homozygous reference genotype is the most likely one and calculates conditional probabilities based on illumina specific knowledge about the reads  <cit> . we decided instead to use a uniform set of assumptions for calculating conditional probabilities of all genotypes. assuming independence between reads, the conditional probability of genotype gi can be expressed as a product of read contributions, i.e., p= ∏r∈rip. for a mapped read r ∈ ri let r be the base spanning locus i and εr be the probability of error sequencing the base r, which we estimated from the quality score q calculated during primary analysis using the phred formula εr = 10-q/ <dig>  <cit> . we discarded allele calls with quality scores zero and one. let hi and hi′ be the two real alleles at locus i, or in other words, let gi=hihi′. the observed base r could be read from either hi or hi′. if there is an error in this read, we assume that the error can produce any of the other three possible bases with the same probability. thus, the probability of observing a base r given than the real base is different is εr/ <dig> while the probability of observing r without error is  <dig> - εr.

if gi is a heterozygous genotype  and the observed allele r is equal to hi this outcome could be due to two possible events. either r was sampled without error from the haplotype containing hi or r was sampled from the haplotype containing hi′ but an error turned it to be equal to hi . assuming that both haplotypes are sampled with equal probability, the first event happens with probability )/ <dig> while the second happens with probability εr/ <dig>  using the fact that for homozygous genotypes the probability of observing each possible base does not depend on the haplotype from which the reads are sampled, we obtain the following formula for computing the probability of observing read r for each possible genotype:

 p=1-εr,ifhi=h′i=rεr <dig> ifhi≠r∧h′i≠r12-εr <dig> otherwise 

note that no matter which is the genotype gi, the sum of the probabilities p over the four possible values of ri is equal to one. we complete the model by setting prior probabilities based on the expected heterozygosity rate h as follows :

 p=1-h <dig> ifhi=h′ih <dig> otherwise 

finally, a variant is called if the genotype with highest posterior probability is different than homozygous reference. in the next section we show a comparison of results among these methods by reanalyzing a publicly available dataset.

software and performance issues
we implemented mapped read merging strategies and snvq in java  <dig>  and we packed both programs with a few additional utilities in a single jar file. the open source code, released under the gnu general public license, is available at http://dna.engr.uconn.edu/software/ngstools/.

in order to enable integration with other analysis tools we use the sam format  <cit>  for both the input and the output of mapped read merging. we also sort alignments by chromosome and absolute position to enable efficient processing in subsequent modules and fast merging of results from different lanes if available. sam files produced by the merging module can be used directly as input for the samtools package  <cit>  to produce run statistics, pileup information, and for variants detection. we recommend to run the merging process lane by lane because it needs to load all unique alignments in memory in order to sort them at the end of the process. we used space efficient data structures that allow us to process more than ten million reads in a few minutes, using up to  <dig> gb of memory. the code implementing snvq is able to receive as input either alignments in sam format or pileup information in the format described in the samtools package. the pileup format is recommended because it enables faster processing and reduces the memory requirements. our experiments indicate that snvq is able to process a whole transcriptome pileup file in about  <dig> minutes using a single processor and up to  <dig> gb of memory.

RESULTS
experimental setup
we tested the performance of the combined mapping strategies and snv detection methods on three publicly available illumina rna-seq datasets generated from lymphoblastoid cell lines derived from three individuals extensively genotyped as part of the international hapmap project  <cit> : a female with northern and western european ancestry , a yoruban male , and a yoruban female . sequencing and read mapping statistics  for the three datasets are provided in table  <dig> 

genotype calling accuracy was assessed using as gold standard hapmap snp genotype calls for the three individuals. to measure accuracy of genotype calling, we defined as true positive a correctly called heterozygous or homozygous non reference snp and as false positive an incorrectly called homozygous snp. we did not consider as error a heterozygous snp called homozygous or not called because this can be due to lack of read coverage for one or both alleles. we consider that one method is more accurate than another when it is able to detect more true positives for the same number of false positives, or conversely if it detects the same number of true positives with fewer false positives. when assessing snv detection accuracy, we define as true positive a detected heterozygous or homozygous non reference snp, no matter which is the actual genotype call, and as false positive a homozygous reference snp marked as having a variant. thus, calling as heterozygous a homozygous not-reference snp is considered a true positive for snv detection, because the variant was detected, but a false positive for genotype calling because an inexistent reference allele is being called.

comparison of read mapping strategies
we used bowtie  <cit>  to map the reads against both the human reference genome  and the ccds transcript library  <cit> . table  <dig> gives read mapping statistics for the compared methods on the na <dig> dataset.

to assess the effect of various mapping strategies on genotyping accuracy, we ran snvq on datasets consisting of na <dig> reads mapped uniquely onto the ccds transcript library and onto the reference genome, respectively reads mapped by the hard and soft merging strategies presented in the methods section. since for reads mapped on transcripts it is only possible to detect snvs in annotated exons included in the ccds database, we excluded from this comparison all hapmap snps located outside of annotated ccds exons. figure  <dig> shows that our merging strategies produce more accurate results than just genome or transcripts mapping for the na <dig> data. although this comparison suggests that genome mapping could be more sensitive than the merging strategies for some specificity levels, we confirmed by repeating the comparison on the full set of hapmap snps that merging methods dominate for all levels of specificity . since the performance of the hard and soft merging strategies is very similar, further results are presented only for the former method.

comparison of snv calling and genotyping methods
we compared our snvq method with soapsnp  <cit>  and maq  <cit> , two widely used bayesian methods implemented in the samtools package  <cit> . we also experimented with the snv detection method for mrna reads of  <cit> , called pma, which is based in careful filtering of aligned reads and a binomial test equivalent to setting up a minimum coverage threshold to make a variant call relative to the total locus coverage. the trade-off between sensitivity and specificity of this method is controlled by the maximum p-value required to discard the null hypothesis of absence of a variant allele. in terms of outcome, both soapsnp and maq have the a-priori advantage of not just pointing out the loci with variant alleles but also inferring the most likely genotype at each locus. the bayesian methods also provide for each locus posterior probabilities of having an allele different than the reference and of the genotype itself. we ran all methods on the na <dig> reads aligned using the hard merge method. figure  <dig> shows that all bayesian methods have significantly better snv detection accuracy than pma and snvq is slightly more sensitive than soapsnp and maq at different specificity levels obtained by varying the threshold on the genotype probability reported by each method. figure  <dig> shows that the accuracy gain of snvq over soapsnp and maq is more pronounced for genotyping accuracy. we confirmed this behavior by running the bayesian methods on the set of reads mapped uniquely onto the genome reference . our results indicate that the binomial tests of heterozygosity employed by maq and soapsnp result in under-calling true heterozygous loci. these heterozygous loci are found by snvq thanks to its unified model based on computing conditional probabilities for every possible genotype.

comparison of strategies for data curation
in practice snv detection is the problem of separating allele calls that are different from the reference because of sequencing errors from calls that are different from the reference because they were sampled from a variant locus. with the current sequencing error rates, if sequencing errors were randomly distributed, it is not difficult to show that any of the discussed methods would have high accuracy. unfortunately, each sequencing technology has different error patterns which can break the randomness assumption. in this section we describe three systematic error patterns characteristic to illumina sequencing and assess how common ways to solve these issues performed in our testing data.

a well-known error bias for illumina reads  is that base calling errors tend to accumulate toward the 3' end of the reads due to a phenomenon referred to as de-phasing  <cit> . to test for this effect, we developed a module which calculates for a set of aligned reads the distribution of mismatches per read position from the 5' to the 3' end. in absence of any bias, this distribution should be close to uniform. as shown in figure  <dig>  the proportion of mismatches sharply increases towards the 3' end of the na <dig> reads. after observing this pattern in the mismatches rate, we decided to apply a filter on the aligned reads by disregarding the first base and the last  <dig> bases of each aligned read for snv detection. although this trimming strategy is equivalent to throwing away one third of the aligned bases for the na <dig> dataset, figure  <dig> shows that this correction improves the specificity of the final calls without loosing sensitivity. trimming aligned reads instead of raw reads is better because the bases sampled correctly in the trimmed region are still used to locate the correct location where the read must be aligned.

another common source of false positive results are pcr amplification artifacts that produce a large number of copies of the same read  <cit> . one usual way to deal with this issue is to allow just one read to start at each possible locus. this filter eliminates artificial high coverage at every locus, which can be beneficial not only for discarding reads generated by pcr artifacts but also for normalizing biases produced by variable transcription levels. the main drawback of this strategy is that it can throw away useful read data, thus affecting sensitivity. an intermediate approach consists on allowing a small number x of different reads per start locus as described in  <cit> . figure  <dig> shows that selecting just one read per start locus is indeed too restrictive for the na <dig> dataset but the three reads filter of  <cit>  did not affect sensitivity and even improved it for stringent specificity requirements. although the improvement is not as consistent as the one obtained by trimming aligned reads, we consider that this filter may be generally useful for correcting coverage biases without loosing sensitivity.

finally, to control for the presence of correlated errors within individual lanes, a natural approach to increase specificity is to call a variant allele only if it seen in at least x different lanes, where x is a user specified parameter. we used na <dig> dataset, consisting of reads from seven different lanes, to assess the effect of the detection threshold on sensitivity and specificity. figure  <dig> shows that after requiring a minimum of three lanes out of seven, the loss of sensitivity is larger than the improvement in specificity. we compared also the simple rule of keeping variants passing the threshold of observing at least two times the non reference allele with the more stringent rule of observing the non reference allele in at least two different lanes. we found that the first filter produced slightly better accuracy for this dataset.

effect of coverage on genotyping accuracy
rna-seq reads are sampled from transcripts roughly proportionally to their relative expression levels, resulting in uneven coverage of different variants. to assess the effect of this uneven coverage on genotyping accuracy we calculated the average coverage for every exon in the ccds database based on the hard merged alignments. average exon coverages are proportional to the rpkm  values more commonly used to report expression levels for rna-seq data, and indeed can be inferred from rpkm values by taking into account exon lengths and the total number of mapped reads. figure  <dig> shows genotyping accuracy achieved by soapsnp, maq, and snvq on the na <dig> and na <dig> datasets, computed for several bins of variants grouped according to the average coverage of the exon to which they belong. as expected, all methods have poor sensitivity for variants with low coverage. owing to improvements in sequencing data quality, all methods have improved genotyping accuracy on the more recent na <dig> data compared to na <dig>  snvq consistently outperforms the other two methods, with most pronounced gains at intermediate coverage depths and on the lower quality na <dig> data.

genotype calling from heterogeneous samples
a main motivation for this work has been identification of expressed non-synonymous somatic mutations in cancer tumors, a subset of which are predicted to yield tumor-specific epitopes with significant immunotherapeutic potential  <cit> . in addition to uneven expression levels, variant calling from cancer rna-seq data is further complicated by the typically heterogeneous nature of these samples. in order to assess the effect of such heterogeneity on genotyping accuracy in a context in which the true genotypes are well characterized we performed experiments on pooled rna-seq reads from the three hapmap individuals. as shown in figure  <dig>  the relative performance of the three compared genotyping methods on the pooled data is similar to that observed for individual samples ; with snvq having a small advantage over soapsnp and maq. however, at a fixed specificity level, the sensitivity achieved by all methods is significantly reduced on the pooled sample. this is illustrated in figure  <dig>  where tradeoff accuracy curves are plotted for snvq calls made from individual rna-seq datasets as well as rna-seq reads pooled from the two yorubans, and all three hapmap individuals, respectively. while the accuracy drops with increased heterogeneity, snvq retains the ability to call a significant fraction of variants with high specificity.

CONCLUSIONS
second generation sequencing of mrna is becoming the method of choice to investigate the behavior of human cells and to reveal the functional effects of variation. in this paper we introduced a mapping strategy for mrna reads which fully utilizes the information contained in both the reference genome sequence and reference databases of known transcripts such as ccds. we also presented a bayesian model for snv detection and genotyping called snvq that seeks to improve genotype calls by fully exploiting the information contained in base quality scores. finally, we performed a comparison among commonly used mapping, snv detection and genotyping methods, and data curation strategies with the aim to select the most effective methods to identify expressed single nucleotide variants from rna-seq data, taking advantage of the availability of rna-seq data for hapmap individuals with well characterized genotypes. our experiments indicate that the double reference mapping and merging strategy yields improved snv calling and genotyping accuracy compared with methods based on mapping to a single reference. the experiments further suggest that snvq achieves improved accuracy over existing methods, and retains its power to detect variants with a high specificity even from heterogeneous rna-seq samples.

in future work we seek to integrate our tools with methods for estimating isoform expression levels  <cit>  and to extend our model by incorporating allele specific expression of isoforms  <cit> . we also plan to integrate additional transcript annotation sources such as dbest and ucsc, and to integrate our methods in a bioinformatics pipeline enabling personalized cancer immunotherapy based on tumor transcriptome sequencing.

competing interests
the authors declare that they have no competing interests.

authors' contributions
pks and iim conceived and supervised the study. jd designed and implemented the algorithms, conducted the experiments, and drafted the manuscript along with iim. all authors have read and approved the final manuscript.

