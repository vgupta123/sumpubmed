BACKGROUND
modern sequencing experiments usually involve the shearing of dna or cdna into relatively short fragments for processing on a high-throughput sequencing device, such as the illumina hiseq. in the analysis of the resulting data, one of the first steps is to align the reads representing these partially-sequenced fragments to a set of target sequences. this procedure identifies locations within the target sequences from which each fragment may have originated using a threshold on mismatches and insertions or deletions , thus reducing the focus of downstream analysis to only highly probable loci. numerous read mappers exist to solve this problem with various features and performance characteristics, the most popular of which are based on the burrows-wheeler transform  <cit> .

a common problem in downstream analysis of the resultant alignment data is that fragments often map ambiguously to multiple target sequences. for example, in the case of rna-seq, a given fragment might align to multiple isoforms of a gene as well as to multiple genes within a gene family. this ambiguity makes it difficult to measure the abundance of transcripts, especially those with few unique regions. a similar problem occurs with chip-seq data, where fragments align to many regions of the genome, complicating the problem of peak finding for determining binding sites  <cit> . another example is in metagenomics, where researchers wish to detect the presence and relative abundance of various closely related species of microorganisms in a pooled sample of dna  <cit> .

previous approaches
the earliest solution for this problem was to ignore any fragments that align ambiguously. however, such techniques discard large amounts of useful information and can lead to significant biases in the analysis. for example, we observe over 25% of fragments being ambiguous in recent rna-seq experiments in the human transcriptome.

mortazavi et al. <cit>  provided a better solution the problem, later named the rescue method by  <cit> , through which ambiguous fragments are assigned proportionally to their potential origins based on the initial gene abundances computed from unique fragment counts. this idea, based on the oft-valid assumption that genes that generate more unique fragments are likely to also generate more ambiguous fragments, was extended by both li et al. <cit>  and trapnell et al. <cit>  after noting that rescue was equivalent to a single iteration of the expectation-maximization  algorithm for a simple model of rna-seq. in the full version of the algorithm, reads are probabilistically assigned in an expectation  step based on the current abundance  parameter estimates. these estimates are then updated in a maximization  step to those that maximize the likelihood given the assignments, which in this case is proportional to the number of assigned fragments per target. these steps are repeatedly alternated and are guaranteed to improve the likelihood at each iteration. since the likelihood function in this simple model is concave, the estimates will eventually converge to the maximum likelihood solution.

several probabilistic models have since been introduced, building off still earlier models  <cit>  to include parameters for features such as fragment length, indel and substitution errors, and sequence-specific biases  <cit> . furthermore, various methods have been proposed for optimizing the likelihood of a given model. the em solution has been the most successful to date, but has failed to scale with the rapid growth in the size of typical sequencing datasets  <cit> . the large number of iterations required for convergence of the em algorithm means that details of the alignments must be stored in memory for fast access, since reading them from disk thousands of times would be intractable. in response to the large memory requirements, heuristics and approximations have been introduced to reduce the footprint of these methods.

the developers of rsem <cit>  have been conservative in their attempts to scale. by ignoring fragments that align ambiguously to a large number of transcriptomic locations , the memory requirements are somewhat reduced, along with the number of iterations. using what is essentially the full batch em algorithm allows rsem to retain high accuracy when ignoring deficiencies in bias modeling  but makes scaling a challenge.

express <cit>  follows a model very similar to rsem, but side-steps the scaling issue by using an alternative optimization procedure–the online em algorithm–that only requires a small, constant amount of memory. the online–or streaming–em algorithm approximates the batch algorithm used in rsem without the need to consider any of the alignments more than once. therefore, the alignments of each fragment can be read from disk once, processed, and then discarded from memory. this leads to huge reductions in the memory and time requirements of the method, but causes it to be somewhat less accurate than rsem when express’s bias modeling is disabled .

another approach to reducing the memory footprint is to exploit the structure of the likelihood function.  <cit>  introduces the ambiguity graph for a set of transcripts, or other target sequences. in this graph, each target is represented as a node and edges are added between pairs of nodes when an ambiguous fragment aligns to both. targets in different components of this graph are independent of each other, providing a decomposition of the likelihood function. thus, the fragments and targets making up separate components can be optimized serially, reducing the memory requirements to the size of the largest component. while this component often makes up a significant fraction of the data, graph partitioning techniques can be used to further reduce the block sizes with little effect on accuracy. however, this method has not been implemented as part of a batch em solution to date.

cufflinks <cit>  approximates the above procedure by partitioning the fragments and targets based on their genomic positions. since ambiguous alignments in rna-seq are commonly due to equivalent mappings to multiple isoforms, cufflinks determines the maximum likelihood assignment of fragments for each non-overlapping genomic locus using the em algorithm. fragments that are not unique to a single locus are initially assigned uniformly to the ambiguous loci, but can be better assigned in a second iteration of cufflinks using a method similar to rescue. this method essentially assumes that the components of the ambiguity graph strictly correspond to genomic loci, which is, in fact, rarely the case  <cit> . in  <cit> , we provide evidence that this assumption is the source of cufflinks lower accuracy on simulated data when compared to rsem and express.

to the cloud
while these solutions have all used algorithms and heuristics to deal with bounded computer memory resources, another approach is to handle the increasing size of datasets by scaling up the compute resources. it is currently infeasible for every small lab to purchase machines with enough ram to fully analyze today’s datasets using the batch em algorithm. however, large clusters of compute nodes are now available for relatively low cost with pay-by-use cloud platform services, such as amazon’s elastic compute cloud   <cit> . developing software to take advantage of the distributed resources on clusters of commodity machines is nontrivial, as issues such as failure recovery and communication must be dealt with  <cit> .

mapreduce is an abstraction that allows developers to access the power of large distributed commodity clusters without having to explicitly handle details such as data partitioning, work scheduling, and software and hardware failures. the mapreduce programming model involves a series of calls to primitive map and reduce methods, with reordering and grouping allowed between. mapreduce was originally conceived by google  <cit>  in conjunction with the google file system   <cit> , a fault tolerant, distributed file system–the “disk” that mapreduce utilizes. both inspired open-source counterparts that compose the core apache hadoop project: hadoop mapreduce and the hadoop distributed file system   <cit> .

myrna <cit>  applies hadoop mapreduce to the analysis of rna-seq data, using hadoop to count the unique alignments in an experiment. the map phase iterates through the alignments, emitting a tuple identifying the transcript that each fragment is aligned to. in the reduce phase, the unique alignments for each transcript are accumulated to produce the total counts. since the fragments can be processed independently in the map phase, hadoop can distribute the fragments randomly to multiple nodes. in the reduce phase, hadoop can be set to automatically assign tuples for each transcript to the same node, allowing the accumulations to occur in an independent, distributed manner.

this method could be also extended to handle ambiguous mappings by implementing the em algorithm using many iterations of mapreduce. the map phase would correspond to the e-step, in which a tuple is emitted for each alignment specifying the target and the probability that it is origin of the fragment based on the likelihood model and a set of global parameter estimates. the reduce phase would correspond to the m-step, in which the probabilistic assignments would be accumulated and the values normalized to produce the updated maximum likelihood parameters estimates for use in the subsequent iteration.

however, the problem with implementing the em algorithm using hadoop mapreduce is that the system is not tailored for data reuse. in hadoop, the dataset being scanned is re-read from disk before every map step, and the results of intermediate computations are written to temporary files after the map. in our application, em would be implemented as a map task. this means that the alignments would have to be loaded from disk before the e-step and a partial set of probabilistic assignments written to disk after the m-step. then, on a single node executing a reduce task, partial sets of probabilistic assignments would be fetched from the temporary files on map nodes and loaded into memory for rendezvous and normalization. an on-disk file containing likelihood parameters would also be updated during this reduce step. thus, numerous disk operations done in hadoop’s map and reduce tasks would create a significant bottleneck.

the approach we take instead is to use apache spark, an open-source framework that provides in-memory, fault-tolerant cluster computing by implementing resilient distributed datasets   <cit> . spark is an alternative compute engine to hadoop that implements the mapreduce abstraction by allowing users to apply map and reduce functions over rdds. in conjunction with the a distributed file system–such as gfs, hdfs, or amazon’s s3–spark handles all issues of fault tolerance and partitioning across the cluster nodes. unlike mapreduce, however, once a subset of the data is read from the filesystem into memory, it can be made to persist in the ram of the compute nodes, allowing an application to efficiently scan it throughout many iterations.

furthermore, spark provides two types of shared variables based on common use cases that are well-suited for the workflow of the em implementation: broadcast variables and accumulable variables. a broadcast variable is a read-only piece of data that is distributed to all worker nodes. an accumulable variable references an append-only data structure that is updated by each worker node’s local process and then fully combined by the process running on the master node. broadcasted and append-only data structures both persist in-memory. express-d utilizes these shared variables to distribute and update parameters and accumulate probabilistic assignments. the following section contains more detail on the implementation.

when given enough ram, consecutive map executions, broadcasts, and accumulations can avoid disk spilling, which makes spark particularly appropriate for the em algorithm  <cit> . by implementing the em algorithm for ambiguous fragment assignment using spark, in conjunction with amazon s <dig> for persistent storage, we can easily scale the method to very large datasets by combining the resources of multiple compute nodes, providing in-memory storage of alignment data while also taking advantage of large-scale parallel computations.

method
model
our implementation maximizes the likelihood of the generative model presented in  <cit> .

preprocessing with express
by default, the distributed file system partitions a dataset stored as text using line breaks to delineate discrete units of processing. in our case, a discrete unit is the collection of alignments of a single fragment for the alignment file, and the name and sequence of a single target for the target file. since the commonly used formats for alignments and targets  do not conform to this standard, we must pre-process the files to produce inputs that can be partitioned by the file system. at the same time, we wish to make our input files as small as possible to reduce the time required for network transfers.

to achieve these goals, we modified express–which has already been optimized for parsing the standard sam and fasta files–to produce input files compatible with our method. the format of these new files are newline-delimited, serialized protocol buffers, which are encoded in base <dig> to ensure no newline characters appear in the serialization itself. the protocol buffer specification is shown in table  <dig> for both alignments and targets. we have avoided including any unnecessary or redundant information and compressed nucleotide sequences to byte arrays, requiring approximately  <dig> bits per nucleotide. the resulting files are significantly smaller than the original binary sam  and fasta files.

express pre-processes the input data  and converts it to a format that is compatible with the distributed file system’s partitioning scheme. the information for each target and fragment are put into a space-efficient protocol buffer–retaining only the information necessary for optimization–, which is then serialized and encoded in base <dig>  each target or fragment takes up exactly one line in the file created for input into express-d.

once the input files are loaded into hdfs or s <dig> on the cluster, our application can be run on spark to begin fragment assignment. figure  <dig> outlines the procedure, which is described in more detail in the following subsections.

preprocessing on spark
the input files are parsed by spark and loaded into the memory of the slave nodes as rdds. the per-alignment indices for accessing the relevant elements of the error and bias markov chain parameter matrices are then precomputed and stored in a transformed rdd. each partition of the transformed rdd is approximately  <dig> megabytes and stores about  <dig> million fragments.

processing without bias correction
the algorithm for processing without bias correction is depicted at the top of figure  <dig>  the current target abundance, error substitution markov chain, and fragment length distribution estimates  are broadcast to the slave nodes storing alignment rdds. given these distributions, the fragments on each slave are probabilistically assigned to the aligned targets using the likelihood function from  <cit> . the appropriate categories of the latent distributions are incremented by the posterior probabilities of the assignments at each slave node to produce new empirical distributions. these counts are then accumulated by the master node and laplace smoothing is applied before they are normalized. the updated parameter estimates are then broadcast to the slave nodes and the procedure is repeated until convergence is detected .

processing with bias correction
previous work demonstrates that significant improvements in accuracy can be attained by modeling sequence-specific bias  <cit> . we have included a bias correction mode  to take advantage of these improvements, as illustrated at the bottom of figure  <dig>  the primary algorithm remains the same as outlined above with the addition of markov chain parameters modeling the sub-sequences surrounding the 5’ and 3’ fragment ends. estimates of these parameters are broadcast to the slaves, used in the likelihood calculation, and updated empirically, similar to the other hidden parameters. instead of probabilities, the bias parameters are ratios of the observed to expected frequencies of these sub-sequences and are used as weights in the likelihood function. the observed frequencies are accumulated empirically along with the other parameters as described above, but the expected frequencies must be computed by sliding windows along the target sequences and counting the occurrences of various sub-sequences weighted by the current target abundance and fragment length parameter estimates. to make these repeated updates efficient, we broadcast the current model parameter estimates to the slave nodes storing target rdds and have them compute local frequencies based on sliding windows over the rdds in memory. the frequencies are then accumulated by the master node, allowing the bias weights to be updated before the next iteration.

freezing of auxiliary parameters
we define the auxiliary parameters to be all parameters of the model except for the target abundance parameters, which are the main parameters of interest. there are two reasons for freezing the auxiliary parameters after a suitable number of iterations: 

 <dig>  the auxiliary parameters can be estimated accurately much earlier than the target abundance parameters since they are fewer in number. otherwise the algorithm will be wasting a significant amount of time unnecessarily updating their distributions at later iterations.

 <dig>  the model is only convex given fixed auxiliary parameters. since we repeat the em steps until convergence is reached, we want a guarantee that processing will not continue indefinitely. given fixed auxiliary parameters, the likelihood function is log-linear, and the em algorithm is guaranteed to converge to the maximum likelihood solution.

we therefore have chosen to use the following auxiliary parameter update scheme: the parameters are updated at every iteration for the first  <dig> and are then only updated every  <dig> iterations until  <dig> iterations are reached, at which point they are frozen.

numerical stability
to avoid underflow, all probabilities distributions are logged before being used in likelihood computations, which has the extra benefit of allowing the use of faster additions instead of multiplications. the assignment probabilities are exponentiated before incrementing the empirical distributions, since there is no concern of numerical instability in the update step.

convergence detection
we halt the algorithm when convergence of the target sampling probabilities is detected in a manner similar to  <cit> . the parameters are considered to have converged when all targets with a sampling probability of at least 10− <dig> have a relative change of no more than 10− <dig> between two consecutive iterations.

RESULTS
test data
in order to test the performance of express-d in terms of both speed in accuracy and in comparison with previous methods, we chose to use the two synthetic datasets from  <cit> . both datasets contain a billion fragments that were simulated according to the generative model described in  <cit> , one including sequence-specific bias and one without. alignment was done with bowtie v <dig> . <dig> and tophat v <dig> . <dig> , allowing for three mismatches in both cases.

method comparisons
we compared performance of express-d with rsem, cufflinks, and express using the results generated in  <cit> . in that analysis rsem v <dig> . <dig>  cufflinks v <dig> . <dig> , and express v <dig> . <dig> were used. we have provided scripts for repeating this analysis in conjunction with the data published with  <cit>  additional file  <dig> 

cluster and experiment setup
for running experiments, we used amazon ec <dig> clusters comprising m <dig> xlarge instances, each of which has  <dig> virtual cpus and  <dig> gb of memory  <cit> . a virtual cpu is rated at  <dig>  ec <dig> compute units , which is roughly equivalent to a  <dig> – <dig>  ghz  <dig> xeon processor  <cit> . even though  <dig> gb may seem excessive, we found that it was necessary to avoid full, costly garbage collection runs by the java virtual machine  that scala runs on, which could delay each iteration by tens of seconds.

a cluster was launched for subsets of various sizes of each test dataset. starting from  <dig> slave nodes used for  <dig> million and fewer fragments, the number of slave nodes used increases proportionally with the dataset size, until we reach  <dig> slave nodes used for  <dig> billion fragments. each set of fragments is broken down to partitions of approximately  <dig> million fragments, the size of which is  <dig> mb when stored on disk and  <dig> mb when stored in memory as java objects. the partitions are stored using amazon’s s <dig> persistent store, and for express-d executions is cached on a slave assigned by the spark scheduler. to measure how runtimes scale with increasing dataset sizes and cluster resources, we executed express-d four times on each cluster for every dataset and report the average of those runs on that cluster. furthermore, runs over datasets simulated with and without bias were done sequentially on the same cluster. we also used only trials where no spark processes were interrupted due to disconnected instances, or other machine component failures.

performance comparisons
in terms of speed and resource use, figure  <dig> shows that express-d can provide constant runtime if the number of nodes are increased linearly with the size of the input datasets. we found that with one cpu core per  <dig> million fragments, express-d could execute  <dig> iterations in approximately  <dig> minutes without bias correction and  <dig> minutes with bias correction. there is also a constant  <dig> minute total overhead for learning the bias model during the first  <dig> iterations. each run requires approximately  <dig> iterations to converge, meaning that only  <dig> hours would be required to process a billion fragments using  <dig> slave nodes . this is only twice is long as is taken by the online em algorithm of express.

although it is impossible to directly compare timings across different machines, we note that we previously found rsem to be unable to complete the processing of more than  <dig> million fragments on a typical desktop server with  <dig> gb of ram or  <dig> million fragments on a server with  <dig> gb of ram  <cit> , which is more than is available to many labs. also, we show in table  <dig> that express, cufflinks, and rsem scaled with slopes that range from  <dig>  minutes per million fragments  to  <dig> mpmf on datasets that were successfully processed in  <cit> . since express-d runs in the cloud, it is not limited by the resources on a single machine and can easily scale to a billion reads with essentially no change in the time required.

we computed the mean slope between the runtime samples from figure  <dig> from of this manuscript and figure  <dig> of  <cit>  to compare the scaling of the four methods in units of minutes per million fragments . while express, cufflinks, and rsem were all run on the same machine fixed at  <dig> cores and  <dig> gb ram, the resources of express-d were increased at a rate of  <dig>  cores per million fragments , allowing it scale in approximately constant time.

CONCLUSIONS
the distributed implementation of express-d allows us to combine the full model of express with the batch em algorithm of rsem to provide the best results in the least amount of time for large datasets. a simple extension to express-d that also parallelizes the read alignment and pre-processing steps–similar to what is done in myrna and crowssbow <cit> –would greatly improve performance and move the full analysis pipeline to the cloud.

as more genomic data moves to the cloud for storage, tools that are able to take advantage of distributed environments and frameworks–such as spark–will become more widely used and help remove the barriers to large-scale integrative analysis of high-throughput sequencing projects.

availability and usage
express-d and spark are open source software that can be downloaded from their respective websites, http://github.com/adarob/express-d and http://spark.incubator.apache.org/. for ease of use, the express-d source code includes a copy of a spark script that allow users to launch, setup and manage ec <dig> clusters running spark and hdfs. the script can be used to launch all nodes in the cluster using a customized amazon machine image –a type of templated operating system  <cit> –that is preloaded with express-d source and binaries. target and fragment datasets can then be loaded into hdfs or s <dig> for distributed execution. the express-d wiki page includes more detail about using the script to launch clusters, as well as notes on cluster configuration and tuning.

competing interests
the authors declare that they have no competing interest.

authors’ contributions
ar developed the method. ar and hf implemented the method and analyzed the results. ar, hf, and lp wrote the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
this script contains procedures for repeating our analyses in comparing rsem, cufflinks, and express. to reproduce our results, use in conjunction with the synthetic data available at bio.math.berkeley.edu/express/simdata.

click here for file

 acknowledgements
we thank matei zaharia, kristal curtis, and reynold xin for discussions on the feasibility of the spark implementation. adam roberts was supported by an nsf graduate research fellowship. lior pachter was partially supported by nih hg <dig> 
