BACKGROUND
chemogenomics, the genome-wide analysis of the effects of chemical compounds, is a valuable approach to elucidate the mechanism of action of small molecules by identifying their cellular targets and target pathways  <cit> . recent applications of chemical genomics in yeast include haploinsufficiency profiling and homozygote profiling of barcoded deletion collections  <cit> , exploration of essential genes using temperature-sensitive mutants  <cit> , molecular barcoded open reading frame libraries  <cit> , decreased abundance by mrna perturbation  <cit> , multi-copy suppression profiling  <cit>  and gene function and drug action analysis using the relationships between gene fitness profiles and drug inhibition profiles  <cit> , to name a few.

we used chemogenomic profiles obtained from experiments that utilized the yeast saccharomyces cerevisiae gene deletion collections  <cit> , which include heterozygous and homozygous diploid deletions and haploid deletions. these screens measure growth of individual strains in a mixed population in the presence of diverse small molecules. in these screens, a decrease in the strain’s fitness can reflect that the deleted gene is the target of the chemical compound present  or part of an affected pathway .

in practice, a genome-wide chemical-genetic profile comprises the fitness of each strain relative to a mock treatment control profile. as each chemical compound produces a unique profile of gene sensitivities, comparing the profiles helps understand the similarity between the modes of action of compounds  <cit> . this “guilt-by-association” approach can suggest therapeutic applications for known compounds as well as the mode of action of novel compounds  <cit> . because most chemical profiles display a range of fitness defects, identifying similarities between chemical profiles requires a way to define similar fitness defect profiles. as part of this comparison, the method must emphasize those genes with highest fitness defect values, i.e. the strains most sensitive to treatment.

to analyze chemical genomics on a large scale  a robust, extensible means to correct for variation is needed. this variation can come from many sources; including operator, laboratory, sample preparation and date  <cit> . taken together, many profiles will cluster based on these non-biological parameters, into “batches”, which confounds any biological conclusions  <cit> . furthermore, as throughput increases, and the method is adopted by different laboratories and platforms, batch effects will increase. these non-biological variation in results  <cit> , are well recognized  <cit>  and hinder the progress of 1) global analysis across different chemogenomic datasets and 2) efforts to integrate this data with orthologous genomic data. although many batch effects  <cit>  can be recorded for each experiment, one cannot account for all variation. one example of an effect that is not always recorded is the level of training, which varies over time, of the person performing the experiment. another example is the temperature which affects all next generation sequencing experiments  <cit> .

due to batch effects, genomic profiles often display uninformative similarity according to these effects rather than the similarity of the underlying chemical biology  <cit> . comparison algorithms, many of which do not consider batch effects, provide an inaccurate similarity mapping of profiles. some algorithms require defining the variables that affects the results for an accurate comparison  <cit> , yet these variables, and their relative impact are not always known.

to find similarity between experiments in a way that accommodates such uncertainty, we devised a method which finds correlation between experiments without the need to define the batch effects variables. this method is based on scaled ranks, which are scored according to a levelled scoring matrix, which provides a score for each gene-drug comparison. we evaluated the method using chemogenomic profiles , and compared the method to other existing correlation methods, including pearson  <cit> , spearman  <cit> , and kendall  <cit>  correlations, which also do not require prior knowledge of the variables that affect the results. finally, we explored the extensibility of the bucket evaluations  algorithm on other microarray data and barcode sequencing data . by statistically evaluating results of the be analysis compared to other correlation methods, we demonstrate its performance and illustrate its application to a variety of data types. we created software and a user interface, which is freely available such that the be method can be applied for diverse experimental comparisons.

RESULTS
the be algorithm is based on ranking and comparing a large number of columns within a dataset, and was initially applied to chemogenomic profiles. for a broader understanding of how the algorithm works, consider this analogy which equates chemogenomic profiles with spider habitats; there are over  <dig> species of spiders living in a variety of habitats from hot deserts to artic regions  <cit> . similar habitats should have similar groups of spider species, adapted to their environment. to evaluate similarity between spider habitats, one should compare the groups of successful  species, rather than comparing the single most successful species because in very similar habitats a and b, the most successful species in a is not necessarily the most successful in b. a better way to measure habitat similarity is to ask, for example, if the most successful species in habitat a is, the top fifty most successful species in habitat b, because such a rank is still very high considering there are  <dig> species.

similar to the world of spiders, comparing the effect of chemical compounds requires examining the groups of genes affected by the chemical compounds rather than the top gene alone. there are many differences between profiles, such as scale of results, standard deviation, and a changing rank of gene values, even when the experiment was performed with the same compound at the same dosage, but on different days . these differences require analysing the ranking, not by comparing specific ranks, but by comparing groups of ranks. a pure rank comparison, meaning the highest value in one profile against the highest value in another profile and so on, gives poor results because it does not take into account the variability of ranks between genome-wide profiles. we addressed this problem in chemogenomic data using section comparisons, dividing each profile’s gene scores into sections, defined as buckets. the algorithm creates a weighted scoring system by ranking sections separately, and holding a higher score for highly ranked gene scores compared to lower ranked gene scores. each section, or “bucket”, is defined as a subgroup of ranked scores, which are scored according to significance. the gene deletion strains with the highest fitness defect scores are considered the most significant for comparing profiles, as these deletion strains are the most influenced by the chemical compound. therefore, we define the bucket sizes in each experiment according to significance, i.e. smaller buckets contain the most significant genes , whereas larger buckets contain the least significant genes . after the genes of each profile are parsed into buckets, we used a levelled scoring matrix  with weighted scores for scoring similarity between profiles, and evaluate a summed similarity score .

the levelled scoring matrix guidelines award a higher similarity score to genes located in lower buckets , and to genes located in closer buckets . to implement the levelled scoring matrix guidelines, we devised a scoring matrix formula  which meets the requirements of the levelled scoring matrix . these guidelines allowed us to find resemblance between profiles in addition to identifying profiles of repeated conditions.

tag <dig> barcode microarray dataset
we ran the be method on a dataset of tag <dig> barcode microarray results , which included novel platinum based chemical compounds, in addition to well characterised compounds, such as cisplatin. the dataset was created by screening these compounds against a pool of ~ <dig>  barcoded deletion strains of saccharomyces cerevisiae,  <dig> essential genes as heterozygous diploids and  <dig> non-essential genes as homozygous diploids to producing unique genome-wide profiles  <cit> . we used several correlation methods, including pearson  <cit> , spearman  <cit>  and kendall  <cit> , for finding similarities between the compound profiles. we then assessed their performance according to batching by date, an unwanted cluster outcome, versus batching by chemical compound, a desired cluster outcome . the results showed the be method performed better than other methods, as measured by the statistical significance of the distribution of scores. we statistically assessed the distribution of similarity scores generated by each of the algorithms by using the wilcoxon test   <cit> . typically, when clustering experiments to evaluate similarity, one would like to see experiments cluster according to experimental factors, i.e. chemical compound or mechanism of action, and not according to the date of the experiment, for example. to assess whether the date of the experiment had an effect in batching the scores, we used a two-sided wilcoxon test on two vectors. the first vector contained the similarity scores of pairs of experiments performed on the same date, and the second vector contained scores of pairs of experiments performed on different dates. the graphs represent the distribution of similarity scores of both vectors . these differences demonstrate a statistically significant shift in the distribution of scores between the two vectors when pearson, spearman or kendall algorithms are used , indicating a strong unwanted effect of the experiment’s date on the outcome. in contrast, the be algorithm was not significantly affected by date . indeed, the statistical evaluation confirmed that, compared to these other methods, the be algorithm was least influenced by the date of the experiment, visualized as a highly similar distribution of scores for same dates and different dates. this is because be compares groups of genes, rather than single gene ranks. . we next evaluated whether the chemical compound used in an experiment had an effect in batching the scores, using the wilcoxon test. we used two vectors: the first contained similarity scores for pairs of experiments performed with the same chemical compound, and the second contained scores of experiment pairs performed using different compounds . repeated experiments, using the same chemical compound, received higher similarity scores compared to experiments using different chemical compounds. the graphs represent the distribution of similarity scores of both vectors, and demonstrate a statistically significant shift in distribution for all algorithms used, indicating all methods used are affected by the chemical compound present. this was notable for the be algorithm, which attained the lowest p-value  compared to the other methods , confirming that the chemical compound has the strongest effect on the batching of scores rated by the be method, and seen where the distribution of scores for different compounds is much lower than the distribution of scores for identical compounds . to summarize this application of the be algorithm, be showed a clear difference in the distributions of scores between date and chemical compound, showing date has less effect on the be method , while chemical compounds have a strong effect on the be method . on the other hand, the differences in score distribution for each one of the correlation methods other than be, look similar for both date and chemical compound, which means that experiments performed on the same date receive a score distribution nearly as high as experiments where the same chemical compound was used .

tag <dig> microarray  <dig> pnas dataset
in order to evaluate the be method on other types of datasets, we tested the method on a dataset which included  <dig> published microarray results for  <dig> different fda approved drugs  <cit> . the assay used haploinsufficiency profiling, which comprises all  <dig> diploid heterozygous yeast strains that can manifest sensitivity to compounds that inhibit the product of the heterozygous locus. this dataset consisted of  <dig> to  <dig> replicate experiments for each drug  <cit> . the be algorithm successfully located similarity between drugs , recapitulating the previously reported similarity between three drugs: alverine-citrate, dyclonine, and fenpropimorph , demonstrating the accuracy of the algorithm  <cit> . in the original study, the similarity between drugs was found using a parametric method that set a threshold to ignore genes with low fitness defects   <cit> , the be method is non-parametric and did not ignore any genes for scoring similarity between experiments. we assessed the similarity results using other methods, including pearson, spearman and kendall correlations, which all found similarity between these drugs. however, be was the only method which found these three drugs as most similar to one another . all methods found the replicate experiments as most similar to one another, scoring the drug itself within the top two most similar drugs.

gene expression  dataset
having shown be works on barcode data from different studies, we next evaluated the be method on an entirely different data type, genome-wide expression profiles from yeast. in this instance, gene expression is the measurement of transcript abundance, which is used as a proxy to measure the relative transcriptional activity of genes. using microarrays, this process allows analyzing thousands of genes at once, providing a global picture of transcript abundance. for this analysis we selected the widely cited study of gasch et al. which contains microarray results for  <dig> environmental stress experiments for all ~ <dig> genes  <cit> . this data was composed of gene expression abundances of saccharomyces cerevisiae to diverse environmental conditions such as heat shock, oxidative and reductive stress, osmotic shock, nutrient starvation, dna damage and extreme ph. in this dataset, high correlation scores between genes, represented by the transcript abundance measured, are indicative of a shared response to stress. these data were initially analyzed using fuzzy k-means  <cit> , a method that differs from the standard k-means, as it provides a membership value for each gene to a centroid. such membership permits each gene  to belong to more than one centroid, which is critical because each gene may be co-regulated with several groups. gasch and co-workers used prior knowledge about the data to select the k value according to the expected number of clusters, and chose the initial centroid locations according to known regulatory elements; we therefore used this analysis as a benchmark. the be method positions the most affected genes, those with the highest score represented by transcript abundance, in the top significant buckets, providing a high score for comparing buckets among experiments with shared top genes, which resulted in a high correlation score specifically between groups of highly affected genes, confirming the previously reported group of ~ <dig> specific genes which were found to be strongly affected throughout all stress treatments . this group of environmental stress response genes represent a common gene expression response to stress  <cit> . the affected genes received statistically significant greater scores than the less affected genes where p < 2e- <dig> . these findings suggest that one can use the be algorithm to locate unique groups of genes that display similar pattern of expression in certain experimental conditions, i.e. stress conditions or in the presence of chemical compounds. the be method was found to perform as well as other correlation methods, which also display a significantly higher score for the reported genes , including pearson, spearman and kendall, for locating groups of similarly affected genes, presenting an additional application of the be method.

high throughput sequencing dataset
next generation sequencing is rapidly being adopted and applied by applications previously dominated to microarrays, such as assessing abundance of yeast deletion strains using barcodes  <cit> , full genome sequencing  <cit> , transcriptome profiling  <cit>  and epigenetics studies  <cit> . accordingly, we evaluated the be method on high throughput sequencing data of chemogenomic profiles performed in a manner similar to the barcode microarray data . for this method, the sequencing results consist of counts of barcode sequences representing the abundance of strains for each experiment  <cit> . the fitness defects are expressed as a log <dig> ratio of the strain specific barcode counts of strains grown in the present of a drug versus strains grown without the drug, for calculating the differences between the treatment and control. these results build a sequencing result matrix of strain fitness, a table of fitness defect scores for each strain in each experiment, that provided a dataset for using the be. we ran the algorithm on  <dig> experiments which included  <dig> repeated experiments for each of  <dig> different drugs. the be method successfully identified the experiments where repeated conditions clustered together according to the drug . experiments performed using the same drug had a statistically significant higher scores than different drug experiments where p =  <dig> e- <dig> . the be method performed better than the pearson correlation method , and as well as non-parametric methods including spearman and kendall correlations . such findings are significant as they confirm that one can use the be method to compare different chemical compounds using data originated from high throughput sequencing experiments.

CONCLUSIONS
rigorous evaluations on several datasets, which included tag <dig> microarrays, tag <dig> microarrays, high throughput barcode sequencing and gene expression microarrays, show that the be algorithm overcomes most batch effects . we confirmed that the be algorithm outperforms other well-established methods by statistically validating the differences of score distributions and comparing these differences between the be method and other methods . clustering of results showed the be algorithm successfully identified similar conditions for microarray and sequencing data . the be method performed as well as other methods by successfully locating the group of key genes as most sensitive to environmental changes, attaining the highest similarity scores .

having tested the be method on data collected from different technology platforms, we conclude that the method is applicable to other datasets where correlation between values is needed. for example, fine tuning the be variables for different datasets, e.g. for high throughput sequencing data required modifying the first bucket size to be  <dig> % of the total number of genes, and setting the maximum amount of buckets to  <dig> . in general, achieving accurate correlation of results may involve similar fine-tuning. the general approach of bucket-weighted scores can therefore be applicable to both groups of highly similar profiles, and diverse matrices, according to the definition of the variables. this method may also be applicable to data collected from emerging technologies, such as new next generation sequencing applications, as finding correlation between results will continue to be beneficial  <cit> .

we note that despite being applicable to many dataset models, like any algorithm it cannot satisfy all datasets. when considering whether to use the be method or other methods, one should take into account several factors. first, whether the data is significant for both positive and negative values. as the be method evaluates scores according to rank, datasets that are significant for both positive and negative values are not analyzed properly. this occurs due to negative values appraised as insignificant relative to positive values. for example, a genomic expression dataset can hold positive scores for induced genes and negative scores for repressed genes, represented by transcript abundance. therefore both positive and negative values are significant, as they both show a change in cell response to the conditions measured in the experiment. one way to surmount this problem, which we used in our study, is to split the original dataset into two with the first dataset containing positive values, and the second containing only the absolute values of the original negative values. running separate analysis for positive and negative values can then identify affected genes, represented by their transcript abundance.

the second factor is whether there is prior data that is relevant to the dataset which the user wishes to incorporate when assessing similarity between experiments. an example is the work done by gasch and co-workers , in which they wished to filter out highly regulated genes. gasch and co-workers used the fuzzy k-means method, which uses prior knowledge about the expected number of clusters, and regulatory elements . as a result many genes that are highly co-regulated, based on prior knowledge of the regulation factors, were filtered out. if the user wishes to ignore subsections of the dataset, the be method is not suitable, as it is specifically designed to avoid the need of prior knowledge about the dataset, and to use an entire-dataset analysis approach.

we implemented the be method so that it is available in a graphical user interface environment program. the application loads an input dataset, provided by the user, and produces a similarity matrix according to the be variable definitions. the software is available for download  along with sample input and output files   <cit> .

