BACKGROUND
a huge gap exists between the number of protein sequences and the number of proteins with a known structure and function. the exponential growth in sequence information means that better methods to automatically annotate new sequences are needed. current methods include ab initio structure prediction, sequence-structure comparisons, and sequence comparisons  <cit> . ab initio methods try to predict the native protein structure from the amino acid sequence. the protein can then be annotated by comparing the predicted structure to those of proteins with known structure and function. sequence-structure comparisons, or threading methods, try to fit the protein sequence to known structures. compared with ab initio predictions, threading limits the candidate solutions to those structures already known. sequence comparisons are based on the assumption that similar sequences share a common ancestor – that is, they are remote homologues – suggesting structural and functional similarities. several good solutions exist when the level of sequence similarity is high, but when the sequences are highly divergent it is still difficult to distinguish remotely homologue sequences from sequences that are similar by chance.

early solutions to the problem of finding remote homologues, such as the smith-waterman algorithm  <cit>  and heuristic alternatives like blast  <cit>  and fasta  <cit> , looked for sequence similarity between pairs of proteins. later solutions used aggregated statistics of related proteins to generate more complex models that a protein with unknown function could be compared to. these methods, including profiles  <cit>  and hidden markov models   <cit>  used only related sequences for model generation.

the most successful recent methods have been discriminative. classifiers are trained on both related and unrelated proteins to recognize what distinguishes the related proteins from the unrelated ones. kernel methods such as the support vector machine  <cit>  have proven to give particularly good results, and several groups have introduced different types of kernel functions  <cit> . most of these kernel functions are typically either based on profiles and sequence alignments, or based on the occurrences of discrete motifs.

kernels based on profiles and sequence alignments
the fisher kernel  <cit>  was the first method that used support vector machines. this method trains profile hmms on related proteins and produces feature vectors from sequences by aligning them to the hmms. another alignment-based kernel is svm-pairwise  <cit> , which represents each sequence as a vector of pairwise similarities to all sequences of the training set. the svm-i-sites method  <cit>  compares sequence profiles to the i-sites library of local structural motifs for feature extraction and this method has also been improved to take into account the order and relationships of the i-site motifs  <cit> .

a relatively simple but efficient kernel is the mismatch kernel  <cit>  in which the feature space consists of all short subsequences of length k, called k-mers. a k-mer is said to be present in a sequence if the sequence contains a substring that has at most n mismatches to the k-mer. in the profile kernel of kuang et al.  <cit> , the mismatch kernel is combined with profiles; a k-mer is said to be present in a sequence if the sequence contains a substring that when aligned to the profile gives a score above a given threshold. later methods, such as the la-kernel  <cit>  and svm-sw  <cit>  are also alignment-based, but instead of representing the sequences as a vector of features they calculate the kernels directly by an explicit protein similarity measure. the la-kernel uses all optimal gapped local alignment scores for all possible subsequences of two sequences, while svm-sw uses the optimal local alignment that maximizes a direct profile-profile score.

kernels based on discrete sequence motif content
motif kernels are based on the idea of using motif content to measure sequence similarity. protein sequence motifs describe some common sequence pattern that is conserved over greater evolutionary distance than the rest of the sequences. focusing on sequence motifs therefore means focusing on the most conserved parts of a sequence, where remote homologues are most likely to share similarities.

although there are many databases of sequence motifs available  <cit> , these databases were created in a supervised way to have motifs that characterize different known protein families, domains, or functional sites. consequently, a motif kernel based on these databases will be biased towards correctly classifying known functions or families. this also makes such motif kernels inappropriate in benchmark studies. the emotif kernel of ben-hur and brutlag  <cit>  avoids these problems by using motifs extracted with the unsupervised emotif method  <cit>  from the eblocks database  <cit> . the emotif kernel has good performance when classifying sequences in classes for which several motifs are available, but the performance decreases when related sequences share few or no motifs  <cit> .

an alternative to using motifs from an existing database is to generate the motifs from the available data. we introduce a motif kernel where genetic programming is used to find discriminative sequence patterns matching the positive training set sequences while not matching the negative training set sequences. the motifs are made from a simple regular expression-like grammar and the resulting matches against the data set is used to build feature vectors for a support vector machine.

we benchmark our gpkernel on updated versions of two commonly used benchmarks  <cit>  based on the scop database  <cit>  and compare its performance with the emotif, mismatch, svm-pairwise, and la kernels as well as the psi-blast method. we find that our method achieves performance similar to the la-kernel method and gives significantly better results than all of the other methods. we also find, when comparing the gpkernel to related motif methods, that motifs trained on the different classes of negative sequences are vital for the method's predictive power.

RESULTS
genetic programming for protein motif discovery
there are several methods that use genetic programming   <cit>  to evolve prosite motifs  <cit>  from multiple unaligned  <cit>  or aligned  <cit>  sequences. genetic programming has also been used to create stochastic regular expression motifs  <cit> . we use gp to evolve the discrete sequence motifs that serve as a basis for our methods.

our gp algorithm is trained on a positive and negative training set and the fitness of a candidate solution is a function of its matches in the two sets. more specifically, the fitness is

f=12= <dig>      
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgmbgzcqgh9aqpdawcaaqaaiabigdaxaqaaiabikdayaaadaqadaqaamaalaaabagaeeivaqlaeeiuaafabagaeeivaqlaeeiuaalaey4kasiaeeoraykaeeota4eaaiabgucarmaalaaabagaeeivaqlaeeota4eabagaeeivaqlaeeota4kaey4kasiaeeoraykaeeiuaafaaagaayjkaiaawmcaaiabg2da9maalaaabagaegymaedabagaegomaidaaiabcicaoiabdofatjabdwgaljabgucariabdofatjabdchawjabcmcapiabcycasiaaxmaacawljawaaewaaeaacqaixaqmaiaawicacaglpaaaaaa@524b@

where tp, fp, tn, and fn are the number of true and false positives and true and false negatives, and se and sp are the rates of correctly classified positives and correctly classified negatives. these rates are also known as the sensitivity and specificity. the fitness evaluation is accelerated by special purpose search hardware  <cit> , which reduces the training time.

the hardware supports several regular expression-like operators, but we use only a small subset of these. our solution language is formally defined for the dna alphabet elsewhere  <cit> , but is here modified to handle the protein alphabet of amino acids. the amino acid symbols are the language's basic elements. the language then uses several operators to build patterns from the basic amino acid letters.

concatenation is the simplest operator and it gives patterns that match sequential occurrences of amino acids; for example, the pattern glaa matches the sequence glaa. the logical disjunction operator allows alternative amino acids at specific positions, such that the pattern gl  a matches sequences glaa and glca. the wildcard operator is a special variant of the logical disjunction operator as it matches any amino acid. thus, the pattern gl.a matches glaa, glca, glda, and so forth.

the final operator, called the "p-of-n" operator, can be used to specify the minimum number of amino acids that must match within its sub-pattern. to illustrate, the pattern {glaa : p ≥ 3} will match glaa, gcaa, and elaa, but not match glcc, and gcaq. note that the p defines the minimum number of amino acids that must match. the "p-of-n" operator is also called the hamming distance operator, as it matches sequences that have a hamming distance below n – p, where n is the sub-pattern's number of amino acids.

by using the hamming distance operator, we can specify patterns that have a certain number of mismatches. furthermore, by combining the hamming distance and the logical disjunction operators, we can boost the importance of amino acid residues at specific positions in the pattern. to illustrate, the pattern {gaa : p ≥ 4} gives a double weight to leucine at the second position. this pattern will only match sequences that contain leucine and at least two of the three other amino acids, such as alaa, glca, and glae. we can use this position specific weighting of amino acids to define patterns that approximate position weight matrices.

the gpkernel uses diverse motifs
the problem of protein remote homology detection has an inherent structure as all proteins can be grouped according to evolutionary relations and structure. this is the basis for the scop hierarchy  <cit> . our method uses this inherent structure to create a kernel based on a rich set of motifs that tries to capture information about all related sequences in a particular dataset.

to compute the kernel, we use the gp algorithm to produce motifs from the scop training set. the kernel is therefore referred to as the gpkernel. for each superfamily classifier, we make both positive and negative motifs; that is, motifs trained to match the classifier's superfamily as well as motifs trained to match the other superfamilies. correspondingly, we do the same for the fold benchmark and train motifs for all folds. this is done based on the hypothesis that motifs trained to recognize the different aspects of the negative data will increase the discriminative power of the gpkernel.

the basic positive training sets for gp include all members of a superfamily or fold, except for the sequences forming the positive test set. this means that all the motifs will have to cover a large taxonomic distance. to narrow the structural range each motif has to cover, we also split the positive training set into subsets, as shown and explained in figure  <dig>  for the superfamily benchmark, the subsets exclude one family from the superfamily sequences. for each such subset, we make ten motifs. we also make ten motifs trained on all the sequences of the superfamily. as a consequence, the number of motifs produced for each superfamily will be ten times the number of families in the superfamily. in total, this produces  <dig> motifs for each classifier in the superfamily benchmark.

for the fold benchmark, it is not practical to generate motifs for all subsets excluding one superfamily because of the very high number of superfamilies in each fold. as we still want several motifs representing different subsets within a fold, we adopt a slightly different solution. the sequences of a fold are grouped into superfamilies and ten sets are made for each fold such that each set exclude one tenth of the sequences. this gives  <dig> motifs for each classifier in the fold benchmark.

for both benchmarks, we run gp with a population of  <dig> candidate solutions for  <dig> generations. the resulting motifs are matched against all the sequences to produce a matrix of binary feature vectors. this matrix contains a  <dig> at position  if sequence i contains motif j and a  <dig> otherwise. the gram matrix is then produced by taking the dot product between the vectors of the sequences; see methods for additional details.

as described above, training the gpkernel basically amounts to 1) training motifs on each superfamily and fold for the respective problems, 2) joining the motifs into a common kernel, and 3) training an svm to recognize each of the superfamilies or folds. although this is how one would train final versions of classifiers for predicting superfamily or fold membership, training one set of motifs from the complete database is not appropriate for a benchmark study that tries to assess the method's predictive power. when estimating the predictive performance of any machine learning method, it is essential that one evaluates the trained models on test sets that are independent of the training set; that is, none of the sequences in the test set should be part of the training set. otherwise, the performance estimates will be biased. to ensure that our performance estimates are unbiased, we therefore create a separate set of motifs for each of the superfamily and fold test sets. consequently, we create  <dig> *  <dig> =  <dig>   <dig> and  <dig> *  <dig> =  <dig>   <dig> superfamily and fold motifs for the scop  <dig>  benchmarks, which means we will have evaluated about  <dig> *  <dig> gp patterns. each gp run takes under  <dig> seconds on our pentium  <dig>  <dig>  ghz system accelerated by one search chip. this gives a total runtime of about  <dig> days for both scop  <dig>  benchmarks, but as we run several parallel processes on different subsets of the benchmarks, we can reduce the evaluation time to get complete results within one day. as a simple comparison, nrgrep  <cit>  uses about  <dig> minute to evaluate  <dig> expressions with varying numbers of amino acids and mismatches.

boosted classifiers and an extended emotif kernel
the gpkernel uses motifs made from genetic programming as a basis for a support vector machine kernel. we also propose another method in which we use the gpboost program  <cit>  to build boosted classifiers  <cit> . each such classifier is based on  <dig> weighted sub-motifs where each sub-motif is made by running genetic programming on the scop training sets with a population of  <dig> candidate motifs for  <dig> generations. a boosted classifier's prediction is a sum of the predictions from the weighted sub-motifs. in addition,  <dig> boosted classifiers are made and combined so that the final prediction for a new sequence will be the average of  <dig> boosted classifiers. the setup is explained in figure  <dig> 

the emotif kernel has shown good performance on protein families that share many emotifs, but the performance decreases for families that are not covered well by the emotifs. we propose to extend the emotif kernel with an additional small set of gp motifs in hope that this will give a better performance when classifying the sequences that share fewer emotifs. the extended emotif kernel, called gpextended, is made from an emotif kernel with additional gp motifs trained to target the positive training set. the motifs are made in exactly the same way as for the gpkernel, but only the positive motifs – the motifs trained on the subsets of a given classifier's training set – are added to an emotif kernel to create the gpextended kernel.

the gpkernel performs significantly better than the other motif-based methods
as figure  <dig> shows, the gain of adding additional motifs to the emotif kernel is more evident on the fold benchmark. because most of the emotifs are relatively specific, the sequences that belong to a fold will on average share few emotifs, giving a very sparse kernel. this might explain the huge performance drop for the emotif method compared with its performance on the superfamily benchmark. if the emotif method lacks a suitable set of emotifs for fold detection, the additional motifs made for the gpextended kernel can compensate for this. both the gpextended kernel's roc and roc- <dig> scores are significantly better than the emotif scores .

the gpkernel has a very good performance on fold detection compared to the other motif methods . the key to the gpkernel's increased performance are the motifs trained on the different negative folds. when we tested a kernel that consisted of an equal number of positive motifs only, the average roc- <dig> score fell by 30% . similarly, gpboost and the gpextended kernel only use motifs trained to recognize the positive training set and are less accurate than the gpkernel is. as the above experiments have shown, the negative motifs are more useful on the fold than on the superfamily recognition problem. because the positive sequences are more similar on the superfamily than on the fold benchmark, methods that only focus on recognizing the positive sequences can more easily find motifs that characterize the positive sequences than they can on the fold benchmark. on the fold benchmark, the motifs that characterize the positive sequences do not confidently predict a protein's correct fold, but an absence of motifs common to some of the negative folds may complement the occurrence of positive motifs. this complementarity probably explains the gpkernel's higher relative performance on the fold than on the superfamily benchmark compared to the other motif methods. to further put the gpkernel's performance in perspective, we benchmarked the gpkernel on liao and noble's scop  <dig>  superfamily benchmark set  <cit> . the only difference from their original benchmark to our setup is that we randomly assign individual families instead of individual sequences to the test set. table  <dig> summarizes the gpkernel's average roc-score on the  <dig> test sets and compares this average to the averages of the other motif-based methods that have been benchmarked on the set. we have also included a recent method that uses latent semantic analysis  on three different motif kernels  <cit> . dong and colleagues showed that using lsa on the best motif kernel, which consisted of χ <dig> selected patterns extracted by the teiresias algorithm  <cit> , gives a performance comparable with the performance of the la-kernel  <cit> . both the basic and lsa-optimized kernels  have lower average roc-scores on the scop  <dig>  superfamily benchmark than the gpkernel has.

sf and fold are the average roc-scores on the scop  <dig>  superfamily and fold benchmark sets  <cit> ; dash  represents an unreported value; source is the sources of results other than ours. for reference, the table's lower part shows the best average roc-scores reported by rangwala and karypis  <cit> . these methods do not use discrete sequence motifs.

apart from the lsa, dong and colleagues' approach differs from ours in that the teiresias algorithm is an unsupervised approach that finds all patterns occurring more than a specified number of times in the input sequences. although this approach should give a similar good coverage of the complete training set as the gpkernel, the feature selection biases the initial set towards patterns that discriminate between the positive and negative training sets. this procedure also likely removed the patterns that discriminate between the different subgroups in the negative set. our results show that such patterns are crucial for the gpkernel's performance.

the scop  <dig>  benchmark set differs from our scop  <dig>  set in two ways. first, liao and noble used a slightly more stringent criterion to filter out similar sequences. second, and more importantly, the set includes all sequences that pass the similarity filter, whereas our set only includes sequences from superfamilies that have at least one family with ten or more sequences and one or more additional families that together have at least ten sequences. the result is that for our benchmark set, the gpkernel consists of motifs from all superfamilies, whereas for the scop  <dig>  set, some superfamilies will not have any motifs. thus, if the negative motifs are important for the gpkernel's performance, the gpkernel should have a lower performance on the scop  <dig>  set than on the  <dig>  set. nevertheless, the average roc scores on the two sets are almost identical . similarly, when we benchmarked the gpkernel on rangwala and karypis' scop  <dig>  fold benchmark  <cit> , the gpkernel's average roc score was  <dig> ; the average roc score on the scop  <dig>  benchmark was  <dig> . although results on the scop  <dig>  and  <dig>  cannot be compared directly, these results suggest that the negative motifs may not be that important for the gpkernel's performance, which is contrary to what the initial results suggest. the average roc- <dig> scores on the superfamily and fold benchmarks show, however, that the negative motifs are important. even though the gpkernel's overall performance is comparable on the two benchmark sets, the gpkernel has a large drop in average roc- <dig> score on the scop  <dig>  set . this large drop is likely caused by the gpkernel missing negative motifs from some of the superfamilies and folds, which then led to these superfamilies and folds being overrepresented as false positives among the high-scoring sequences.

the gpkernel has better overall performance than most existing methods
to further assess the gpkernel's performance, we evaluated the performance of four other popular methods for remote homology detection: psi-blast and the la-kernel, mismatch, and svm-pairwise kernels. figure  <dig> summarizes the performances of the five methods on the superfamily benchmark. the gpkernel is significantly better than the other methods, except the la-kernel, in terms of roc scores . the gpkernel also has significantly higher roc- <dig> scores than mismatch and psi-blast , but the gpkernel and svm-pairwise's roc- <dig> scores are not significantly different . the la-kernel is, however, the best method in terms of both performance measures .

the table shows the average roc and roc- <dig> scores obtained by the different methods on the superfamily benchmark and the fold benchmark.

although the above results confirm the gpkernel's high performance on the fold benchmark, the results do not prove that the gpkernel is a useful tool for remote homology detection and fold prediction. the gpkernel and la-kernel have similar performance, and the gpkernel may, for instance, simply make similar predictions as the la-kernel. if this was the case, one could argue that the gpkernel is a redundant method, as one could simply use the la-kernel to get the same predictions as the gpkernel. to investigate this, we plotted the gpkernel's roc- <dig> score against the la-kernel's roc- <dig> score on all the test sets for the superfamily and fold benchmarks . these graphs show that the two methods are not redundant, but complementary. although this result may not be surprising – the two methods rely on distinct features – it suggests that the two methods can be combined to create an improved method for remote homology and fold detection  <cit> .

our scop  <dig>  benchmark set uses a less stringent sequence similarity filter than the scop  <dig>  benchmark. to ensure that this higher sequence similarity had not influenced the results for the gpkernel and la-kernel, we repeated the gpkernel and la-kernel fold experiments on a modified version of the scop  <dig>  set. using the same e-value threshold of 10- <dig> as for the scop  <dig>  benchmark gave  <dig> sequences and  <dig> superfamilies to use as test sets. the average roc scores for the gpkernel and la-kernel were  <dig>  and  <dig> ; the average roc- <dig> scores were  <dig>  and  <dig> . the differences in roc scores are significant .

motif based classifiers for fold detection perform better with many motifs of low specificity
one of the scop superfamilies  that participate as a test set in the fold detection benchmark is classified well with the gpkernel method  but achieves a lower score with the emotif method . even though there seems to be a mild correlation  between the number of emotif matches for a fold and the roc- <dig> score achieved, the training and test sets for this superfamily do not have significantly fewer emotif matches than other superfamilies. more important is the number of emotifs shared between sequences. this number varies a lot between different pairs of sequences, but if we calculate the average number of emotifs shared between sequences in the b. <dig> fold, we find that the sequences on average share  <dig>  emotifs. this is less than the average for all folds  which again is much less than the average shared between sequences of a superfamily . this shows that because sequences at the fold level have a very low sequence similarity, and because of the specificity of most of the emotifs, the number of emotifs shared between sequences in a fold will also be low. this will in turn influence the performance of the emotif kernel.

the table shows examples of the motifs evolved by the genetic programming process targeting the scop b. <dig> fold. in addition to amino acid characters, the motifs are also made from the disjunction operator , the wildcard operator , and the hamming distance operator {: p >= x} that specifies the minimum number of characters that must match in the pattern. for each motif, the table shows the relative percentage of sequences matched in the training and test sets. the positive training set has  <dig> sequences, the negative  <dig> sequences. the positive test set also has  <dig> sequences; the negative set has  <dig> sequences.

when looking at all motifs made, we find that each motif on average matches nearly a fourth of all sequences. all sequences will therefore share many gp motifs. if we compare the related sequences of a superfamily or fold with randomly chosen sequences, we find that related sequences share more motifs than randomly chosen sequences do. on average, sequences in a superfamily have a higher correlation in their motif matches than other sequences on the positive motifs  and they also correlate more on the other motifs . this means that related sequences share more of all motifs than unrelated sequences, explaining the gpkernel's performance.

another kernel that also has a good performance on fold detection is the mismatch kernel. this kernel is based on a much larger feature set of even more unspecific patterns than the gpkernel is. for the mismatch kernel, the generality of the patterns ensures that the whole solution space of sequences is covered and that most sequences share at least a few patterns. the gpkernel achieves good coverage by training a certain amount of motifs for each superfamily or fold. the gp motifs, while not being too specific, are still more tuned to discriminate between sequences of different folds than the completely general mismatch k-mer patterns. this suggests that to capture the small sequence similarity that exists at the fold level, motif-based classifiers benefit from motifs that are general enough to match a significant number of the weakly similar sequences of a fold. in summary, it seems that good motif-based classifiers on the fold recognition problem need to strike a balance between specificity  and generality . the gpkernel is one step in that direction.

CONCLUSIONS
we have introduced a motif kernel with discrete sequence motifs trained with genetic programming. motifs are evolved using a subset of regular expressions to describe sequences in a superfamily or fold, and discriminate between these and sequences in other superfamilies . the method gives very good results on two scop benchmarks when compared to other relevant methods.

in addition, we have established two new and updated benchmark sets. these sets, which are nearly twice as large as previously used benchmarks, should prove useful for future studies on remote homology detection.

