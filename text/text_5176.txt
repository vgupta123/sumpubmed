BACKGROUND
to enable the adaptive behaviour that is required when developing software for research an "informal" data management strategy is often needed. by informal we mean there is a need to rapidly develop and adapt software infrastructures to unforeseen and  unspecified requirements. we have found that the use of a distributed data management system  gives us the required flexibility, while still allowing for the development of the level of formalization that is required for robust software development.

advances in computer science have pushed what can be achieved with data management systems, and conversely these advancements have driven the increase in demands for richer functionality. the computer science research advancements have involved both hardware and software, with faster processor speeds enabling other innovations to become feasible. the way in which data management systems are built, and extended, has also changed. these changes in software engineering and design include: the methodology through which software is constructed ; the technology used to allow for distributed computing ; and the ideology that is used to define the process through which software is built . these advances are continuing to occur, and will have an effect on the next generation of data management and distribution tools .

a number of companies, and academic institutions, have marketed integration and data management solutions for the life sciences. these enterprise data integration  management systems have evolved over the last  <dig> years. this evolution has been from single database based solutions to open, distributed, interoperable data management solutions . this change has been driven by demands for rapid development, high levels of interoperability and increases in data volume and complexity. there has been a natural progression with these integration systems, as they generally follow the traditional approaches to software designs and technologies that are prevalent at the time. there are numerous examples of the application of technical innovations being the focus of a specific integration product, for example: in  <dig> srs  <cit>   advocated external indexing to link between numerous gene and protein data sources; in  <dig> the discovery center  used corba  <cit>  based distributed components to provide bespoke integration products; in  <dig> the alliance framework  promoted an n-tier application server distributed system, which used linked domain specific modules; in  <dig> the metalayer  utilized xml message passing; in  <dig> discoverylink  <cit>   provided a federated database solution which linked across different databases and flat files; in  <dig> the genomics knowledge platform  marketed an object integration solution based solely on ejbs; in  <dig> the i3c  specified the use of an identity driven approach to integration; in  <dig> the lsp  advocated the use of web services; in  <dig> ipa  and metacore  used a knowledge base to provide a solution for the mining of networks of integrated data; in  <dig> cabig  <cit>   adopted a mda  approach, built using a j2ee and web service based solution, to standardize their community integration efforts; in  <dig> cancergrid  delivered a resource framework based web service system to bridge between diverse data sources; and in  <dig> cagrid  <cit>   provided a stateful web service and registry system for loosely coupled data and analysis services.

one common characteristic of these "technology first" efforts is that they developed solutions that were designed to work with static "finished" data, not research information. this style of system works well within publishing scenarios, where information is to be made available throughout an enterprise as a static resource. when actually working within a research institution , where new technologies and ideas are continually being developed, such static publishing approaches are not appropriate. instead, a flexible analysis and access system is required that allows for the rapid introduction and integration of many types of data. with the advent of systems biology, the recognized need for integration solutions has reached a high level of urgency.

this article is principally focused on the design of software to support systems biology investigations, rather than a discussion of a specific application. the software that has been developed, and made available to the community, is discussed to illustrate the advantages of the advocated designs. the importance of design can be over looked in scientific computing  <cit> , due to the complexities with the development of software for research which arise due to the constantly changing requirements and individual project-centric approaches. in this article we discuss how we designed a flexible and maintainable data management solution which can be readily adapted to meet the requirements of a complex research environment. the reasoning behind this non-traditional design is that a research software infrastructure must be highly adaptable to change. the required change could arise from new technologies being introduced, a change of research focus, or a change in resourcing . how we integrate the data management system with other tools and data sources has been discussed previously  <cit> .

RESULTS
there are two major requirements on software design within a rapidly changing research environment: the ability to develop and integrate software rapidly, and the assurance that the resulting systems are adaptable to new and unpredictable needs. the methodologies and technologies used within research environments must be able to satisfy these requirements. methodologies that depend on formalized specifications for processes and data structures are not appropriate for research software infrastructure. an understanding of current informatics technologies, and their suitability, is essential to ensure that research driven software projects can be delivered on schedule. over the last few years there have been many advances in enterprise computing which means that data management technologies can now effectively be used in rapidly evolving research environments. these advances mean that, through an appropriate choice of technologies, it is now possible to deliver, within a minimal timeframe, a distributed system which is robust, standardized, loosely coupled and interoperable. enterprise components can now be rapidly configured to deliver a range of rich functionality . however, inappropriate technology choices can result in an architecture which is not adaptable and difficult to maintain.

technologies that require a static structure  are constrained in their usage as they require "top-down" design. this means that the usage of traditional application server based technologies may not be appropriate for many aspects of research, as they are largely designed for tasks where: there exists reasonably stable information which can be structured in a dbms or similar; business logic is required to operate closely on the data; or integration logic or data transformation is required. alternatively, less structured data management, such as content repositories, can be suitable within research environments. content repositories serve a different purpose to application servers, but can be used to serve out similar types of information using the same protocols. content repository technologies can be more appropriate  as they provide a high level of flexibility when dealing with unstructured content .

we are not suggesting that more formalized representations of data do not have their place in research environments. many data sources can be represented as static snapshots, and when it comes to presenting the research results, the use of more standard software development practices and tools is essential. these static approaches become unsuitable in situations where software is being developed to directly support on-going research projects, with unpredictable life cycles and unforeseen requirements.

layered content management
to manage data arising from ongoing research experiments we adopted an approach using distributed content repositories. content repositories allow for the development of a formalized structure that can be associated directly with resources .

the content can be organized into a  hierarchical structure. the nodes in the hierarchy can have an arbitrary complexity  and also have extensible property sets associated with them. the content system also provides for the usual array of horizontal services .

we extended the java content repository  standard  to provide a distributed data management system that consists of a series of interlinked content management systems . such a federated content repository solution offers three major advantages over a standard database/application server solution:

• easy to adapt. within a research environment it is generally difficult to hammer down requirements. the requirements change over time, and new functionality is often required at short notice. content repository systems have a high degree of flexibility, as the content graph can be extended to meet new requirements, and the annotations can be dynamically updated. this means that when new requirements become apparent, the structure can be changed or modified easily, without having to rewrite a schema or change an object layer implementation. it is interesting to note that other disciplines, in particular the design community, have found that such flexibility is invaluable in enabling creativity  <cit> .

• easy to understand. any data management solution will involve a high level of complexity, especially in a distributed research environment. this complexity, however, does not mean that the principles of how the system operates cannot be easily understood. allowing the end-users to build up a good mental model of how the system works removes many obstacles with adoption. by portraying the content management as "an intelligent file system" positive transfer of knowledge can occur, so that the system is natural and intuitive to use.

• easy to access. within a research environment there is little time to be spared for learning  informatics systems. this fact coupled with the low level of formal software training of most researchers , means that convenient access to the data is of paramount importance. content management systems in general, and the system discussed in this paper in particular have little complexity in terms of object models and data access protocols, as the storage structure is simply a hierarchy and a large number of access protocols can be supported .

unfortunately, a single content repository system does not offer the level of flexibility that is required. research groups interact with different facets of research information in diverse ways and with a plethora of goals. this diversity means that the data must be organized in different ways, for example: when capturing the data it may be appropriate to organize information by group and date; when processing the information it can be organized by runs and tool information; and when exploring the information it can be organized by biological significance .

to allow for this diversity  we deployed a series of loosely coupled distributed content repositories. the resulting distributed content management system  allows for different people to interact with the systems directly. to identify resources we have previously  <cit>  used urns . we have since migrated to using http uris, as we feel that http uris have advantages over urns for linking between data sources: uris  can support human readable hierarchical naming schemes; representational state transfer  operations can be easily appended to uris; they do not require additional overhead for resolution; and uris are better aligned with current community efforts.

the main aim of the distributed content repository system is to enable the researchers, and associated research tools, to easily be able to store, manipulate and retrieve the information they want in the form they desire. one repository can span multiple installations, and multiple repositories can reside within one instance. the system we use is designed around a three-layer architecture:

• instrumentation layer. this layer is used to capture experimental information. the layer models information in a way that makes storage of the information simpler, so that laboratory scientists can easily add and annotate information that is captured from a variety of instruments. typically each type of experiment has a distinct  repository.

• conceptual layer. this layer is designed to provide a means to generically interact with the information through the use of high level abstract operations. these operations include the aggregation and retrieval of information, and do not necessitate an understanding of the actual information content. information which is required for manipulating the data is provided as metadata through properties attached to the data files.

• organizational layer. this layer provides a project  based view on the information, and therefore is designed to have a "biological focus". typically the content is organized by factors such as disease, organism or molecule. each different research, or research group, can individually organize and annotate the data to suit their individual requirements.

it is interesting to note that "three layer" architectures commonly occur in many software architectures  <cit> . this pattern consistently has a lower level that directly reflects the underlying data storage, a middle layer which provides an abstract conceptualization of the data, and an upper level which presents the data in a convenient manner to external applications .

instrumentation layer
the instrumentation layer is designed to be the interface point with high throughput equipment and laboratory instrumentation. data is captured by laboratory scientists via equipment and is stored in a repository that is structured for their convenience. in this way the repository is closely aligned to the experiment and experimenter.

the repositories are mainly used to automatically capture information from high throughput experiments. we are currently using instrument repositories to capture information for high throughput imaging, proteomic and genomic experiments. the instrumentation repositories generally have relatively simple structures as their purpose is to facilitate data collection. for example, when capturing information for genomics, the data is organized in the repository in a fairly flat hierarchy containing a simple name and time stamp mechanism.

we specify a limited set of properties  which are used to describe the metadata associated with the experiment. this controlled metadata is used in conjunction with any ad-hoc or free text annotations which may be added at a later time by the experimenter. where possible the metadata is gathered directly from the instrumentation  otherwise, we interface with bespoke sample tracking systems . the ontologies used are based upon community standards .

once the data is collected within a repository the content can be queried, browsed and retrieved using standardized mechanisms, such as: object protocols  to develop distributed systems; webdav to provide document-based browsing ; xpath query for structured and unstructured searching/retrieval; and rest/json protocols for programmatic access from any language that supports http connectivity . to meet our needs we have also provided two additional retrieval mechanisms:

• uri based retrieval – all referenceable items within the repository can be retrieved using a http uri. this interface was built both to provide convenient access to the data and also to allow for the construction of semantic web based applications on top of a jcr instance.

• standardized access components for retrieving, searching and publishing information to a jcr instance. we have used these components in a variety of applications, including: within a servlet based web application; within a standalone application; and as part of a genepattern  <cit>  tool set.

conceptual layer
the conceptual layer is designed to aid in tasks that are fundamental in many different research investigations. the layer provides a high level of abstraction for tasks associated with data processing  and data manipulation . these systems provide common  functionality, and are designed to be used by analysis and data processing subsystems.

analysis subsystem
the analysis system is used to manage, audit and store the results of data analysis/processing pipelines. each pipeline run is associated with a node in the analysis content repository. each node defines a structure that accommodates the required inputs, the outputs and logs produced by the process, and the current status  of the execution. in this way, each processing pipeline is loosely coupled with the analysis system, which provides a searchable storage area for input/output files and a logging service. other software  can query the repository to find the status of any distributed processes that has been registered. a process that uses the analysis system typically has the same life cycle: when a processing pipeline starts it queries the content repository to find the required input node ; status information is sent to the status node ; and output data is stored in the output nodes. due to the loose coupling and simplicity we have used the analysis system to support numerous data processing pipelines, in particular in processing genomics data .

the genomics normalization pipeline  uses the analysis system directly. as the analysis system uses a standard pattern of nodes , any analysis pipeline can make use of it. the genomics normalization pipeline simply retrieves the required input parameters, executes the job , and then writes the results. by using a pattern of nodes, applications can be loosely coupled so that they can be developed independently of each other with few dependencies e.g. a job submission application can write to the input nodes , and a separate processing application can receive notifications that are triggered through changes to the status nodes . the loose coupling provides a high level of reliability and allows for rapid application development. the content system also provides for a means to ensure that we are able to achieve reliable auditing of each analysis run. the basic functionality for reading/writing information is provided through standardized modules.

aggregation subsystem
one of the aims of the computing advances over the last few years has involved the concept of run time aggregation. this approach is epitomized by the semantic web, where people can mash information from a variety of data sources into a single graph. we provide a run time aggregation system specifically for the aggregation of data from different analyses.

when an analysis run has finished, the observing aggregation system can trigger an indexing operation on the results. the indexing is controlled using the properties that are associated with the output of the analysis run . as all data within any of the content systems is indexed it is possible to retrieve data items using free text or direct querying. if, for example, an application  wished to aggregate all data pertaining to a specific gene then it can query the aggregation subsystem  so that the aggregated information retrieved would consist of both nodes  and specific rows  from data files. in this way, it is possible to extract relevant information from a variety of data sources upon demand. for example, the end results of the normalization pipeline shown in figure  <dig> is a large replicate combined data matrix of how mouse gene expression levels change over a myriad of conditions; the aggregation system can be used to automatically extract subsets of this information directly so that they can be presented in a uniform manner .

the web portal  uses the standardized service to retrieve information from a number of repositories. the service can query multiple repositories, so that the portal makes a request to retrieve uris for specific types of information that match a specific gene id. this information is then displayed in each of the portlets. the aggregation system is used to allow for each part of data files  to be uniquely identified by uris.

organization layer
the organization layer allows for the construction of a scientific oriented façade on top of the other repositories. this façade provides a scientific project based structure, which allows for the custom organization of data and for project specific annotations to be added. this layer provides more than a materialized view of the other layers, as extra information and data relevant to the specific scientific project can be added, searched and retrieved. as all "referenceable" items within any of the repositories are exposed as http uris, these can be used to provide links between distributed repositories. as all items are uniquely addressable, a generic browsing application can be built  which provides a view on the interlinked repositories. the browsing application can be used to annotate items so that they can be searched, to organize distributed items, and to link items from different repositories.

the endpoint for uri addressed items is a rest web service implemented as a series of servlets. these web services are used to provide interoperable operations on both individual repositories  and items within a repository . any domain model representations  are formatted using javascript object notation . the json syntax is fast becoming a standard for communications over rest apis as: it is lightweight; easily extensible and human readable; is easy to interpret from many programming languages; and can conveniently be transformed to other formats such as xml. we use this api to rapidly develop applications for specific users or research groups on top of repositories that  directly reflect the "mental model" that the users have about the organizational structure of their information. an example of how we have used this layer is an application that is used to organize the results of proteomics experiments . this application simply mirrors how a research group used to organize their data locally . the application has richer functionality as the items can now be annotated, is accessible  and the relationships between items can be browsed .

operation
the organizational repositories represent a domain-oriented view on the information. the information can be securely retrieved using a variety of protocols . the flexible access to the data means that: scientists can browse, arrange and annotate the information using a hierarchy that best suits their needs; and domain-centric applications can be rapidly developed using these structures as interfaces to the data.

discussion and 
CONCLUSIONS
the adaptive data management system we have discussed has been designed to meet the competing requirements for data management that arise within a research organization. as with many aspects of research informatics, flexibility and rapid development are key issues as requirements change unexpectedly and frequently. we advocate the view that, to be of lasting use to research, any data management system must be able to:

support the evolution of ideas
the development of integration strategies for systems biology is problematic due to both the nature of science and the organization of scientists. it is typical that the means to which a specific experimental technology will be used, and the methods used to analyze the resulting data, is unknown at software design time. this is because scientific understanding continually evolves. any scientific data management system has to support such evolution, meaning that traditional approaches to development and design are frequently inappropriate. as neither the data usage nor analysis is known a priori an easily adaptable solution is needed.

allows for flexible working
there is a fundamental requirement for scientists to easily access, query and manipulate data to suit their needs. science led investigations require an infrastructure to support ongoing data driven discovery processes. this means that: the data should be accessible through a variety of mechanisms, including multiple computer languages and applications; the data should be detached from the system, so that scientists are not "tied down" to any specific object model or way of working; and flexible navigation through the data using a variety of approaches should be supported.

delivers the salient information
many aspects of biomedical research can be thought of as an information centered science  <cit> . information must be established in the context in which it has been requested, that is to say, the view on the information changes depending upon the question that was asked. therefore, the structure of the data management system must change depending upon the requirements. in a research environment, the views on the data typically involve: high throughput instrumentation, where information must be pushed from machines quickly and reliably; computational informatics processing applications, which require auditing and annotations; and research projects, which require domain oriented and flexible views upon the data. this requirement for adaptable data management is encountered in many avenues of research across all of the biomedical sciences.

rather than build a de novo system, we have extended standardized open source systems. by extending community standards we are able to deliver production systems quickly and are able to overcome boundaries due to resource limitations. we have found that the standardized systems we have adopted are generally not directly suitable for research informatics, and therefore had to be extended to allow for the required flexibility. these extensions are designed to: ensure that the system integrates well within a research enterprise; allow for customization of the system to ensure it has a richer semantics; and provide workflows to ensure the system can work robustly with high throughput instrumentation.

we have found that the three-tier distributed data management system provides the flexibility and adaptability that we required. as we adopted and extended standards, the system was put together with a minimal of fte effort, while still providing a robust and scalable architecture. most importantly, as science moves along at a rapid pace, and opinions are always multi-faceted and divided, this system allows us to evolve the structure of the data management to meet new requirements without having to continually rewrite or wrap out of date or inappropriate legacy code.

availability and requirements
we have made a number of tools available to the community. these tools are built upon the web services we have developed that allows for the interoperability and distribution of content across a number of jcr instances. for these tools to function one or more jcr compatible instances must first be installed, and then the rest services must be deployed within tomcat. further instructions are given below, and more details  are given on the main download site.

project information
project home page: ;

operating system: platform independent;

programming language: java; other requirements: java  <dig>  or higher, tomcat  <dig>  or higher, working jcr instance  <dig> . <dig> or higher.

licence: apache;

any restrictions to use by non-academics: no restrictions.

installation instructions
detailed instructions on how to install the main data management system, which consists of the rest services and the project explorer web application are given with the downloads section of the project web site. to install the main software the following steps must be undertaken:

1) ensure you have java  <dig>  installed and a working tomcat  instance

2) download the main services file  from the downloads site.

3) install a jcr instance to create a repository . to install jackrabbit: download the jcr-instance-jar-with-dependencies.jar into an execution working directory ; edit the jcr-instance.properties file and copy the file into the jcr_home.

4) start the repository by executing the jar using "java – jar jcr-instance-jar-with-dependencies.jar jcr-instance.xml".

5) copy the addama-rest.war file into your tomcat/webapps directory and then fill in the addama-rest.properties file and copy it to your tomcat/libs directory.

6) copy the addama-html.war file into your tomcat/webapps directory, make sure the addama-rest war file started up properly first.

7) in your browser go to 

available tools
the following tools are available from the main download site.

• rest/uri extensions. this is the main set of services that must be installed before any of the additional tools. the services allow for the retrieval of content using a uri.

• jcr browsing system. a javascript/json/ajax based web application for browsing and editing the contents of a jcr instance. this is included as part of the main distribution.

• access components: components are provided to support the development of applications that require jcr retrieving, searching and publishing functionality. this are provided as a separate download and code is provided for r, matlab, perl, python, java and ruby.

• data feeder. a utility for loading a jcr from a sample tracking system. this utility demonstrates how annotations and data items can be mapped into a repository; and how simple monitoring can be used to mirror the contents of an existing sample tracking system in the jcr. this is built against the freely available slimarray sample tracking software  <cit> .

• file loader. a utility for loading file data directly into a jcr instance using xpath information to define the position. this runs as an executable java jar.

authors' contributions
jb designed the system, managed the development team, and drafted the manuscript. hr worked on the design and implementation of the rest interfaces, the aggregation system and the portal. cc contributed to the manuscript, provided implementations for key services, and standardised the metadata for the services. db principally worked on the analysis pipelines and associated services, and also contributed significantly towards the design. sk worked on the pipelines and web applications, and also project managed the team. is instigated and guided the project. all authors read and approved the manuscript.

