BACKGROUND
in recent years, the bi-multivariate analyses techniques  <cit> , especially the sparse canonical correlation analysis  , have been widely used in brain imaging genetics studies. these methods are powerful in identifying bi-multivariate associations between genetic biomarkers, e.g., single nucleotide polymorphisms , and the imaging factors such as the quantitative traits .

witten et al.  <cit>  first employed the penalized matrix decomposition  technique to handle the scca problem which had a closed form solution. this scca imposed the ℓ1-norm into the traditional cca model to induce sparsity. since the ℓ1-norm only randomly chose one of those correlated features, it performed poorly in finding structure information which usually existed in biology data. witten et al.  <cit>  also implemented the fused lasso based scca which penalized two adjacent features orderly. this scca could capture some structure information but it demanded the features be ordered. as a result, a lot of structured scca approaches arose. lin et al.  <cit>  imposed the group lasso regularizer to the scca model which could make use of the non-overlapping group information. chen et al.  <cit>  proposed a structure-constrained scca  which used a graph-guided fused ℓ2-norm penalty for one canonical loading according to features’ biology relationships. du et al.  <cit>  proposed a structure-aware scca  to identify group-level bi-multivariate associations, which combined both the covariance matrix information and the prior group information by the group lasso regularizer. these structured scca methods, on one hand, can generate a good result when the prior knowledge is well fitted to the hidden structure within the data. on the other hand, they become unapproachable when the prior knowledge is incomplete or not available. moreover, it is hard to precisely capture the prior knowledge in real world biomedical studies.

to facilitate structural learning via grouping the weights of highly correlated features, the graph theory were widely utilized in sparse regression analysis . recently, we notice that the graph theory has also been employed to address the grouping issue in scca. let each graph vertex and each feature has a one-to-one correspondence relationship, and ρij be the sample correlation between features i and j. chen et al.  <cit>  proposed a network-structured scca  which used the ℓ1-norm of |ρij|uj) to pull those positively correlated features together, and fused those negatively correlated features to the opposite direction. the knowledge-guided scca   <cit>  was an extension of both ns-scca  <cit>  and s2cca  <cit> . it used ℓ2-norm of ρij2uj) for one canonical loading, similar to what chen proposed, and employed the ℓ <dig> -norm penalty for another canonical loading. both ns-scca and kg-scca could be used as a group-pursuit method if the prior knowledge was not available. however, one limitation of both models is that they depend on the sign of pairwise sample correlation to recover the structure pattern. this probably incur undesirable bias since the sign of the correlations could be wrongly estimated due to possible graph misspecification caused by noise  <cit> .

to address the issues above, we propose a novel structured scca which neither requires to specify prior knowledge, nor to specify the sign of sample correlations. it will also work well if the prior knowledge is provided. the gosc-scca, named from graph octagonal selection and clustering algorithm for sparse canonical correlation analysis, is inspired by the outstanding feature grouping ability of octagonal selection and clustering algorithm for regression   <cit>  regularizer and graph oscar   <cit>  regularizer in regression task. our contributions can be summarized as follows 1) gosc-scca could pull those highly correlated features together when no prior knowledge is provided. while those positively correlated features will be encouraged to have similar weights, those negatively correlated ones will also be encouraged to have similar weights but with different signs. 2) our gosc-scca could reduce the estimation bias given no requirement for specifying the sign of sample correlation. 3) we provide a theoretical quantitative description for the grouping effect of gosc-scca. we use both synthetic data and real imaging genetic data to evaluate gosc-scca. the experimental results show that our method is better than or comparable to those state-of-the-art methods, i.e., l1-scca, fl-scca  <cit>  and kg-scca  <cit> , in identifying stronger imaging genetic correlations and more accurate and cleaner canonical loadings pattern. note that the pma software package were used to implement the l1-scca  and fl-scca  methods. please refer to http://cran.r-project.org/web/packages/pma/ for more details.

methods
we denote a vector as a boldface lowercase letter, and denote a matrix as a boldface uppercase letter. mi indicates the i-th row of matrix m=. matrices x={x1;…;xn}⊆ℝp and y={y1;…;yn}⊆ℝq denote two separate datasets collected from the same population. imposing lasso into a traditional cca model  <cit> , the l1-scca model is formulated as follows  <cit> : 
  <dig> minu,v−utxtyv,s.t.||u||22= <dig> ||v||22= <dig> ||u||1≤c <dig> ||v||1≤c <dig>  

where ||u||1≤c <dig> and ||v||1≤c <dig> are sparsity penalties controlling the complexity of the scca model. the fused lasso  can also be used instead of lasso. in order to make the problem be convex, the equal sign is usually replaced by less-than-equal sign, i.e. ||u||22≤ <dig> ||v||22≤ <dig>  <cit> .

the graph oscar regularization
the oscar regularizer is firstly introduced by bondell et al.  <cit> , which has been proved to have the ability of grouping features automatically by encouraging those highly correlated features to have similar weights. formally, the oscar penalty is defined as follows, 
  <dig> ||u||oscar=∑i<jmax{|ui|,|uj|},||v||oscar=∑i<jmax{|vi|,|vj|}. 

note that this penalty is applied to each feature pair.

to make oscar be more flexible, yang et al.  <cit>  introduce the goscar, 
  <dig> ||u||goscar=∑∈eumax{|ui|,|uj|},||v||goscar=∑∈evmax{|vi|,|vj|}. 

where eu and ev are the edge sets of the u-related and v-related graphs, respectively. obviously, the goscar will reduce to oscar when both graphs are complete  <cit> .

applying max{|ui|,|uj|}= <dig>  the goscar regularizer takes the following form, 
  <dig> ||u||goscar=12∑∈eu+12∑∈eu,||v||goscar=12∑∈ev+12∑∈ev. 

the gosc-scca model
since the grouping effect is also an important consideration in scca learning, we propose to expand l1-scca to gosc-scca by imposing goscar instead of l <dig> only as follows. 
  <dig> minu,v−utxtyvs.t.||xu||22≤ <dig> ||yv||22≤ <dig> ||u||1≤c <dig> ||v||1≤c <dig> ||u||goscar≤c <dig> ||v||goscar≤c <dig>  

where  are parameters and they could control the solution path of the canonical loadings. since the s2cca  <cit>  has proved that the covariance matrix information could help improve the prediction ability, we also use ||xu||22≤ <dig> and ||yv||22≤ <dig> other than ||u||22≤ <dig> ||v||22≤ <dig> 

as a structured sparse model, gosc-scca will encourage ui≐uj if the i-th feature and the j-th feature are highly correlated. we will give a quantitative description for this later.

the proposed algorithm
we can write the objective function into unconstrained formulation via the lagrange multiplier method, i.e. 
  <dig> ℒ=−utxtyv+λ1||u||goscar+λ2||v||goscar+β12||u||1+β22||v||1++γ12||xu||22+γ22||yv|| <dig> 

where  are tuning parameters, and they have a one-to-one correspondence to parameters  in gosc-scca model  <cit> .

taking the derivative regarding u and v respectively, and letting them be zero, we obtain, 
  <dig> −xtyv+λ1l1u+λ1l^1u+β1Λ1+γ1xtxu= <dig>  

  <dig> −ytxu+λ2l2v+λ2l^2v+β2Λ2+γ2ytyv= <dig>  

where Λ <dig> is a diagonal matrix with the k1-th element as 12||uk1|| <dig>  and Λ <dig> with the k2-th element as 12||vk2||1; l <dig> is the laplacian matrix which can be obtained from l1=d1−w1; l^ <dig> is a matrix which is from l^1=d^1+w^ <dig>  l <dig> and l^ <dig> have the same entries as l <dig> and l^ <dig> separately based on v.

in the initialization, both w <dig> and w^ <dig> have the same entry with each element as  <dig> except the diagonal elements. but w <dig> and w^ <dig> become different after each iteration, i.e., 
  <dig> wij=12|ui−uj|,ŵij=12|ui+uj|. 

if ||ui−uj||1= <dig>  the corresponding element in matrix w <dig> will not exist. so we regularize it as 12||ui−uj||12+ζ  when ||ui−uj||1= <dig>  we also approximate ||ui||1= <dig> with ||ui||12+ζ for Λ <dig>  then the objective function regarding u is ℒ∗=∑i=1p. it is easy to prove that ℒ∗ will reduce to problem  regarding u when ζ→ <dig>  the cases of ||vi||1= <dig> and ||vi−vj||1= <dig> can be addressed using a similar regularization method.

d <dig> is a diagonal matrix and its i-th diagonal element is obtained by summing the i-th row of w <dig>  i.e. di=∑jwij. the diagonal matrix d^ <dig> is also obtained from d^i=∑jŵij. likewise, we can calculate w <dig>  w^ <dig>  d <dig> and d^ <dig> by the same method in terms of v.

then according to eqs. , we can obtain the solution to our problem with respect to u and v separately. 
  <dig> u=+β1Λ1+γ1xtx)−1xtyv, 

  <dig> v=+β2Λ2+γ2yty)−1ytxu. 



we observe that l <dig>  l^ <dig> and Λ <dig> depend on u which is an unknown variable, and v is also unknown which is used to calculate l <dig>  l^ <dig> and Λ <dig>  thus we propose an effective iterative algorithm to solve this problem. we first fix v to solve u; and then fix u to solve v.

algorithm  <dig> exhibits the pseudo code of the proposed gosc-scca algorithm. for the key calculation steps, i.e., step  <dig> and step  <dig>  we solve a system of linear equations with quadratic complexity other than computing the matrix inverse with cubic complexity. thus the whole algorithm can work with desired efficiency. in addition, the algorithm is guaranteed to converge and we will prove this in the next subsection.

convergence analysis
we first introduce the following lemma.

lemma 1
for any two nonzero real numbers ũ and u, we have 
  <dig> ||ũ||1−||ũ||122||u||1≤||u||1−||u||122||u|| <dig>  

proof
given the lemma in  <cit> , we have ||u~||2−||u~||222||u||2≤||u||2−||u||222||u|| <dig> for any two nonzero vectors. we also have ||ũ||1=||ũ|| <dig> and ||u||1=||u|| <dig> for any two nonzero real numbers, which completes the proof. □

based on lemma  <dig>  we have 
  <dig> ||ũ′−u′||1−||ũ′−u′||122||ũ−u||1≤||ũ−u||1−||ũ−u||122||ũ−u|| <dig>  

  <dig> ||ũ′+u′||1−||ũ′+u′||122||ũ+u||1≤||ũ+u||1−||ũ+u||122||ũ+u|| <dig>  

when |ũ′−u′|, |ũ−u|, |ũ′+u′| and |ũ+u| are nonzero.

we now have the following theorem regarding gosc-scca algorithm.

theorem 1
the objective function value of gosc-scca will monotonically decrease in each iteration till the algorithm converges.

proof
the proof consists of two parts.

 part 1: from step  <dig> to step  <dig> in algorithm  <dig>  u is the only unknown variable to be solved. the objective function  can be equivalently transferred to 
 ℒ=−utxtyv+λ1||u||goscar+β12||u||1+γ12||xu|| <dig> 

according to step  <dig> we have 
 −u~txtyv+λ1u~tl~1u~+λ1u~tl1^~u~+β1u~tΛ1u~+γ1u~txtxu~≤−utxtyv+λ1utl1u+λ1utl1^u+β1utΛ1u+γ1utxtxu  where u~ is the updated u.

it is known that utlu=∑wij||ui−uj|| <dig> if l is the laplacian matrix  <cit> . similarly, utl^u=∑wij||ui+uj|| <dig>  then according to eq. , we obtain 
  <dig> −u~txtyv+2λ1∑wij||ũi−ũj||122||ui−uj||1+2λ1∑ŵij||ũi+ũj||122||ui+uj||1+β1∑||ũi||122||ui||1+γ1u~txtxu~≤−utxtyv+2λ1∑wij||ui−uj||122||ui−uj||1+2λ1∑ŵij||ui+uj||122||ui+uj||1+β1∑||ui||122||ui||1+γ1utxtxu 

we first multiply 2λ <dig> on both sides of eq.  for each feature pair separately, and do the same to both sides of eq. . after that, we multiply β <dig> on both sides of eq. . finally, by summing all these inequations together to both sides of eq.  accordingly, we arrive at 
 −u~txtyv+2λ1∑wij|ũi−ũj|+2λ1∑ŵij|ũi+ũj|+β1||u~||1+γ1||xu~||22≤−utxtyv+2λ1∑wij|ui−uj|+2λ1∑ŵij|ui+uj|+β1||u||1+γ1||xu|| <dig>  

let λ1∗=2λ <dig>  γ1∗=2γ <dig> β1∗=2β <dig>  we have 
  <dig> −u~txtyv+λ1∗2||u~||goscar+β1∗2||u~||1+γ1∗2||xu~||22≤−utxtyv+λ1∗2||u||goscar+β1∗2||u||1+γ1∗2||xu|| <dig>  

therefore, gosc-scca will decrease the objective function in each iteration, i.e., ℒ≤ℒ.

 part 2: from step  <dig> to step  <dig>  the only unknown variable is v. similarly, we can arrive at 
  <dig> −u~txtyv~+λ2∗2||v~||goscar+β2∗2||v~||1+γ2∗2||yv~||22≤−u~txtyv+λ2∗2||v||goscar+β2∗2||v||1+γ2∗2||yv|| <dig>  

thus gosc-scca also decreases the objective function in each iteration during the second phase, i.e., ℒ≤ℒ.

based on the analysis above, we easily have ℒ≤ℒ according to the transitive property of inequalities. therefore, the objective value monotonically decreases in each iteration. note that the cca objective utxtyvutxtxuvtytyv ranges from , and both utxtxu and vtytyv are constrained to be  <dig>  thus the −utxtyv is lower bounded by - <dig>  and so eq.  is lower bounded by – <dig>  in addition, eqs.  imply that the kkt condition is satisfied. therefore, the gosc-scca algorithm will converge to a local optimum. □

based on the convergence analysis, to facilitate the gosc-scca algorithm, we set the stopping criterion of algorithm  <dig> as max{|δ|∣δ∈}≤τ and max{|δ|∣δ∈}≤τ, where τ is a predefined estimation error. here we set τ=10− <dig> empirically from the experiments.

the grouping effect of gosc-scca
for the structured sparse learning in high-dimensional situation, the automatic feature grouping property is of great importance  <cit> . in regression analysis, zou and hastie  <cit>  have suggested that a regressor behaviors grouping effect when it can set those regression coefficients of the same group to similar weights. this is also the case for structured scca methods. so, it is important and meaningful to investigate the theoretical boundary of the grouping effect.

we have the following theorem in terms of gosc-scca.

theorem 2
let x and y be two data sets, and be the pre-tuned parameters. let u~ be the solution to our scca problem of eqs. . suppose the i-th feature and j-th feature only link to each other on the graph, ũi and ũj are their optimal solutions, thus sgn=sgnholds. the solutions to ũi and ũj satisfy 
  <dig> |ũi−ũj|≤2λ1wijγ1+1γ <dig> 

where ρij is the sample correlation between features i and j, and wi,j is the corresponding element in u-related matrix w <dig> 

proof
let u~ be the solution to our problem eq. , we have the following equations after taking the partial derivative with respect to ũi and ũj, respectively. 
 ũi=xityv,ũj=xjtyv. 

we know that features i and uj are only linked to each other, thus dii=djj=aij=wij for those intermediate matrices. besides, we also know that sgn=ũi|ũi|, sgn=sgn, xitxi=ρii= <dig> and xjtxj=ρjj= <dig>  then according to the definition of l <dig>  l^ <dig> and Λ <dig>  we can arrive at 
  <dig> λ1wijsgn+λ1ŵijsgn+β1sgn+γ1ũi=xityv,λ1wijsgn+λ1ŵijsgn+β1sgn+γ1ũj=xjtyv. 

subtracting these two equations, we obtain 
  <dig> γ1=2λ1wijsgn+tyv 

then we take ℓ2-norm on both sides of eq. , apply the triangle inequality, and use the equality ||||22= <dig>  
  <dig> γ1|ũi−ũj|≤2λ1wij+2||yv|| <dig> 

we have known that our problem implies ||yv||22≤ <dig>  thus we arrive at 
  <dig> |ũi−ũj|≤2λ1wijγ1+1γ <dig> 

□

now the upper bound for the canonical loadings v can also be obtained, i.e. 
  <dig> |v~i−v~j|≤2λ2wij′γ2+1γ <dig> 

where ρij′ is the sample correlation between the i-th and j-th feature in v, and wij′ is the corresponding element in v-related matrix w <dig> 

theorem  <dig> provides a theoretical upper bound for the difference between the estimated coefficients of the i-th feature and j-th feature. it seems that this is not a tight enough bound. however our bound is slack since it does not bound much more the pairwise difference of features i and j if ρij≪ <dig>  this is desirable for two irrelevant features  <cit> . suppose two features with very small correlation, i.e. ρij≪ <dig>  their coefficients do not need to be the same or similar. so we do not care about their coefficients’ pairwise difference, and will not set their pairwise difference a tight bound. this quantitative description for the grouping effect makes the goscar penalty an ideal choice for structured scca.

RESULTS
we compare gosc-scca with several state-of-the-art scca and structured scca methods, including l1-scca  <cit> , fl-scca  <cit> , kg-scca  <cit> . we do not compare gosc-scca with s2cca  <cit> , sscca  <cit>  and cca-sg   <cit>  since they require prior knowledge available in advance. we do not choose ns-scca  <cit>  as benchmark either, due to the following two reasons.  ns-scca generates many intermediate variables during its iterative procedure. as the authors stated, ns-scca’s per-iteration complexity is linear in , and thus the complexity becomes o when it is in the group pursuit mode.  its penalty term is similar to that of kg-scca which has been selected for comparison.

there are six parameters to be decided before using the gosc-scca, thus it will take too much time by blindly tuning. we tune the parameters following two principles. on one hand, chen and liu  <cit>  found out that the result is not very sensitive to γ <dig> and γ <dig>  so we choose them from a small scope . on the other hand, if the parameters are too small, the scca will reduce to cca due to the subtle influence of the penalties. and, too large parameters will over-penalize the results. therefore, we tune the rest of the parameters within the range of {10− <dig> − <dig> − <dig> , <dig> ,103}. in this study, we conduct all the experiments using the nested 5-fold cross-validation strategy, and the parameters are only tuned from the training set. in order to save time, we only tune these parameters on the first run of the cross-validation. that is, the parameters are tuned when the first four folds are used as the training set. then we directly use the tuned parameters for all the remaining experiments. all these methods use the same partition for cross-validation in the experiment.

evaluation on synthetic data
we generate four synthetic datasets to investigate the performance of gosc-scca and those benchmarks. following  <cit> , these datasets are generated by four steps: 1) we predefine the structures and use them to create u and v respectively. 2) we create a latent vector z from n. 3) we create x with each xi∼n where jk=exp−|uj−uk| and y with each yi∼n where jk=exp−|vj−vk|. 4) for the first group of nonzero features in u, we change half of their signs, and also change the signs of the corresponding data. since the synthetic datasets are order-independent, this setup is equivalent to randomly change a portion of features’ signs in u. now that we change the sign of both coefficients and the data simultaneously, we still have x′u′=xu where x′ and u′ indicate the data and coefficients after the sign swap. we do the same on the y side to make our simulation more challenging  <cit> . in addition, we set all four datasets with n= <dig>  p= <dig> and q= <dig>  they also have different correlation coefficients and different group structures. therefore, the simulation is designed to cover a set of diverse cases for a fair comparison.

the estimated correlation coefficients of each method on four datasets are contained in table  <dig>  the best values and those are not significantly worsen than the best values are shown in bold. on the training results, we observe that gosc-scca either estimates the largest correlation coefficients , or is not significantly worse than the best method . gosc-scca also has the best average correlation coefficients. on the testing results, gosc-scca also outperforms those benchmarks in terms of the average correlation coefficients, though kg-scca does not perform significantly worse than our method. for the overall average obtained across four datasets, gosc-scca obtains the better correlation coefficients than the competing methods on both training set and testing set.
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
the estimated correlation coefficients and their mean are shown. ’nan’ means a method fails to estimate a pair of canonical loadings. ’ <dig> ’ means a very small correlation coefficients. ’avg.’ denotes the mean across all four datasets. the best values and those that are not significantly worse than the best ones  are shown in bold


fig.  <dig> canonical loadings estimated on four synthetic datasets. the first column is for dataset  <dig>  and the second column is for dataset  <dig>  and so forth. for each dataset, the weights of u are shown on the left panel, and those of v are on the right. the first row is the ground truth, and each remaining row corresponds to a specific method:  ground truth.  l1-scca.  fl-scca.  kg-scca.  gosc-scca



evaluation on real neuroimaging genetics data
data used in the preparation of this article were obtained from the alzheimer’s disease neuroimaging initiative  database . the adni was launched in  <dig> as a public-private partnership, led by principal investigator michael w. weiner, md. the primary goal of adni has been to test whether serial magnetic resonance imaging , positron emission tomography , other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment  and early alzheimer’s disease . for up-to-date information, see www.adni-info.org.
table  <dig> real data characteristics



shown in table  <dig> are the 5-fold cross-validation results of various scca methods. we observe that gosc-scca and kg-scca obtain similar correlation coefficients on every run, including the training performance and testing performance. besides, they both are significantly better than l1-scca and fl-scca, which is consistent with the analysis in  <cit> . this result shows that gosc-scca can improve the ability of identifying interesting imaging genetic associations compared with l1-scca and fl-scca.
 <dig> 
 <dig> 
 <dig> 
 <dig> 
the estimated correlation coefficients and their mean are shown. the best correlation coefficients and those that are not significantly worse than the best ones  are shown in bold


fig.  <dig> canonical loadings estimated on the real dataset. each row corresponds to a scca method:  l1-scca.  fl-scca.  kg-scca.  gosc-scca. for each row, the estimated weights of u are shown on the left figure, and those of v on the right



discussion
in this paper, we have proposed a structured scca method gosc-scca, which intended to reduce the estimation bias caused by the incorrect sign of sample correlation. gosc-scca employed the goscar  regularizer which is an extension of the popular penalty oscar. the gosc-scca could pull those highly correlated features together no matter that they were positively correlated or negatively correlated. we also provide a theoretical quantitative description of the grouping effect of our scca method. an effective algorithm was also proposed to solve the gosc-scca problem and the algorithm was guaranteed to converge.

we evaluated gosc-scca and three other popular scca methods on both synthetic datasets and a real imaging genetics dataset. the synthetic datasets consisted of different ground truth, i.e. different correlation coefficients and canonical loadings. gosc-scca was capable of consistently identifying strong correlation coefficients on both training set and testing set, and either outperformed or performed similarly to the competing methods. besides, gosc-scca successfully and accurately recognized the signals which were the closest to the ground truth when compared with the competing methods.

the results on the real data showed that both gosc-scca and kg-scca could find an important association between the apoe snps and the amyloid burden measure in the frontal region of the brain. kg-scca performs similarly to gosc-scca on this real data largely because of the strong correlations between the variables within the genetic data, as well as those within the imaging data. in this case, the signs of the correlation coefficients between these variables tend to be correctly calculated, and so kg-scca does not have the sign directionality issue. on the other hand, if the correlations among some variables are not very strong, the performance of kg-scca can be affected by the mis-estimation of some correlation signs. in this case, gosc-scca, which is designed to overcome the sign directionality issue, is expected to perform better than kg-scca. this fact has already been validated by the results of the second synthetic dataset.

the satisfactory performance of gosc-scca, coupled with its theoretical convergence and grouping effect, demonstrates the promise of our method as an effective structured scca method in identifying meaningful bi-multivariate imaging genetic associations. the following are a few possible future directions.  note that the identified pattern between the apoe genotype and amyloid deposition is a well-known and relatively strong imaging genetic association. thus one direction is to apply gosc-scca to more complex imaging genetic data for revealing novel but less obvious associations.  the data tested in this study is brain wide but targeted only at apoe snps. another direction is to apply gosc-scca to imaging genetic data with higher dimensionality, where more effective and efficient strategies for parameter tuning and cross-validation warrant further investigation.  the third direction is to employ gosc-scca as a knowledge-driven approach, where pathways, networks or other relevant biological knowledge can be incorporated in the model to aid association discovery. in this case, comparative study can also been done between gosc-scca and other state-of-the-arts knowledge-guided scca methods in bi-multivariate imaging genetics analyses.

CONCLUSIONS
we have presented a new structured sparse canonical analysis  model for analyzing brain imaging genetics data and identifying interesting imaging genetic associations. this scca model employs a regularization item based on the graph octagonal selection and clustering algorithm for regression . the goal is twofold:  encourage highly correlated features to have similar canonical weights, and  reduce the estimation bias via removing the requirement of pre-defining the sign of the sample correlation. as a result, it could pull highly correlated features together no matter whether they are positively or negatively correlated. empirical results on both synthetic and real data have demonstrated the promise of the proposed method.

alzheimer’s disease neuroimaging initiative, data used in preparation of this article were obtained from the alzheimer’s disease neuroimaging initiative  database . as such, the investigators within the adni contributed to the design and implementation of adni and/or provided data but did not participate in analysis or writing of this report. a complete listing of adni investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/adni_acknowledgement_list.pdf


