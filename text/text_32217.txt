BACKGROUND
mathematical models and simulations are central to systems biology. any mathematical model is only as reliable as the numerical values assigned to its biological parameters, however. since biological parameters, such as kinetic constants, can often not be measured directly, they must be determined indirectly with parameter estimation  methods. naturally, results obtained with pe are the more precise, the greater the information content of the experimental data.

it is well-known that there may exist parameters in a model that cannot be estimated by pe at all, i.e. not with a finite error . we can asses whether the unknown model parameters of a model may be determined by pe by testing the identifiability of the model. a number of useful definitions of identifiability exist . two concepts are important in the context of the present paper: structural identifiability and at-a-point identifiability. unfortunately, methods for structural identifiability testing  are not widely used for large nonlinear models due to either the computational complexity or the lack of mature computer implementations  <cit> .

methods for at-a-point identifiability testing  <cit> , in contrast, are easier to implement and more widely applicable. due to their size and nonlinearity, the models treated in the present paper are analyzed with methods for at-a-point identifiability. in the remainder of this article the term identifiability therefore refers to at-a-point identifiability . we use the eigenvalue method for identifiability testing, because it proved to be computationally efficient and precise in a recent comparison to other methods  <cit> .

if a mathematical model turns out not to be identifiable for the accessible experimental data, we may attempt to replace it by a simplified one. many methods for model reduction of nonlinear models exist. these include but are not limited to techniques based on detecting and decomposing different time-scales  <cit> , sensitivity analysis  <cit>  and balanced truncation  <cit> . in this contribution our aim is to find an identifiable model. therefore we propose an identifiability based approach for model reduction. our iterative approach is not fully automated, but depends on a biologically skilled modeler. even though the modeler is guided by the proposed approach, he remains in control of the simplification throughout the entire procedure. the work flow proceeds by the following steps: 1) parameter estimation, 2) ranking estimated parameters according to their identifiability, 3) calculating parameter variances, and 4) simplifying those parts of the model that contain the least identifiable parameters. these steps are iterated until an identifiable model results. in the presented case study typical simplifications in step 4) amount to lumping and neglecting reactions that do not affect any model output. we stress that we do not enforce identifiability by merely fixing unidentifiable parameters to values that result in good fits. while parameter fixing helps investigating which parameters could be estimated within the set of unknown parameters  <cit> , the resulting parameter values must be considered with great care. after all, the values of the fixed unidentifiable parameters remain to be unknowns in these models. in fact, the unidentifiability of these parameters implies infinite error bars have to be assigned to them. parameter estimation with random number based optimization algorithms can be particularly misleading in this context. these algorithms often suggest there exists an optimization result, even if the pe did not converge to a solution that fulfills optimality criteria. the convergence properties of gradient based optimization methods are, in contrast, closely linked to identifiability. briefly speaking, convergence of gradient based methods is usually tested by checking first order optimality conditions, which are a necessary condition for local optimality of the least squares parameter estimation to gaussian approximation and, in turn, a necessary condition for local identifiability of the model .

the proposed model simplification work flow is applied to a model of janus kinase  and signal transducer and activator of transcription  signaling, which is based on the model by yamada et al.  <cit> . the jak-stat pathway is of biological importance because it is involved in several key cellular processes such as inflammation, cellular proliferation, differentiation and apoptosis.

we do not use the full jak-stat model proposed by yamada et al.  <cit> , since even when all species can be measured noise-free and at high frequency, the full model is by far too detailed  <cit> . this problem would be even more pronounced if realistic experimental conditions were assumed. instead of the original jak-stat model we use a truncated model that focuses on the early signaling phase  of the jak-stat signaling pathway before transcriptional feedback occurs.

we apply the proposed iterative work flow to the jak-stat model until eventually an identifiable model results. after each iteration step the unknown parameters of the resulting new model are determined by maximum likelihood estimation using simulated data. consequently, the work flow is applicable in silico, i.e. the proposed identifiability analysis may be carried out based on experimental data, but it may also be applied prior to any laboratory measurements. if no experimental data are available, it is important to still incorporate realistic assumptions on which biological quantities could in principle be measured, as well as on which measurement error must be expected. these assumptions on the availability and precision of data are crucial, since they heavily influence identifiability properties.

the proposed work flow extends our earlier work  <cit> , where we assumed that all state variables of the model can be measured to arbitrary precision. for the purpose of the present paper it is important to incorporate stricter assumptions on measurability, sampling rates, and error bars. moreover, the identifiability criterion used in  <cit>  arguably is difficult to interpret. here we use a variance based criterion that is both concise and easy to interpret. finally, the work flow proposed here improves upon the identifiability method used in  <cit>  in that it is iterative and uses multi-start parameter estimation to mitigate the problem of convergence to local minima in non convex least squares optimization.

methods
the first part of this section focuses on the description of the jak-stat model. after introducing the general system class, the particular jak-stat model used throughout the paper is discussed. the second part of the section introduces the proposed model simplification workflow and its building blocks.

modeling
system class
we consider mathematical models m of the form

  x˙=f,p,u), x=x0y=h,p,u), 

where x∈ℝnx, p∈ℝnp, u∈ℝnu and y∈ℝny denote state variables, parameters, inputs and outputs, respectively. the functions f and h are assumed to be smooth. one component pi of the parameter vector p corresponds to one biological parameter such as a kinetic constant. in our example the initial conditions are known. note that this might not always be the case. the proposed approach can easily be extended to systems with estimated initial conditions .

we stress that it is important to distinguish state variables x from output variables y. in a simulation, all state variables x can be determined to practically arbitrary precision and recorded at arbitrary sampling rates. in a laboratory experiment, in contrast, many state variables can usually not be measured at all, or not within any relevant precision. moreover, some state variables can often not be measured separately from others, but only as a sum or, more generally, linear or nonlinear combination. the output variables y are introduced to distinguish those quantities that could in principle be measured in a laboratory from the state variables. in terms of the output values y, a laboratory or computer experiment results in values at successive points 0=t0<t1<⋅⋅⋅ <tnt in time

  y, y˜  i= <dig> ...,nt, 

where y and y˜ denote simulated output data and experimental data, respectively. up to measurement noise the outcomes of both a real and a simulated experiment are uniquely determined by the initial conditions x = x <dig> and the values of the inputs u from t =  <dig> to the final time t=tnt. input functions u, t∈ also have to be chosen with care, since computer simulations can be run for a much larger class of functions u than can be realized experimentally. both the choice of u and y are discussed in detail below.

we use a variable order, variable step size, backward differentiation formula based numerical integrator  to obtain the solution of equation . the derivatives of the state variables with respect to the parameters are calculated by adding so called sensitivity equations to equation  and integrating the extended equations system .

we focus on the events that occur in the jak-stat pathway during the first  <dig> minutes after receptor activation. this assumption results in a considerable simplification, since transcriptional feedback and protein synthesis need not be modeled. the model of the jak-stat pathway published by yamada et al.  <cit>  serves as a reference in the present work. figure  <dig> sketches those parts of the model published by yamada et al.  <cit>  that are needed to describe the first  <dig> minutes after receptor activation. its signaling steps can briefly be described as follows. in the first step, the receptor  associates with jak to form the r_jak complex. binding of interferon-γ  to r_jak creates the receptor complex ifn_r_jak, which is able to dimerize. jak may phosphorylate the dimerized receptor complex, and, as a consequence, the active receptor complex  is formed. either sh <dig> domain-containing tyrosine phosphatase  <dig>  or cytoplasmic stat <dig>  can bind to this complex. in the first case, the receptor complex is deactivated. in the latter case, the activated receptor phosphorylates stat1c, a prerequisite to stat1c dimerization. the remaining parts of the model describe how stat1c can be dephosphorylated by the cytoplasmic phosphatase ppx, and how stat1c monomers and dimers can be converted into each other.

choice of input function u and outputs yi
as pointed out above, input functions u, t∈ and output variables yi have to be chosen with care, whenever simulations are supposed to mimic conditions of realistic laboratory experiments. we choose an input function u that represents a pulse stimulation with ifn,

  u=ifn={ <dig> if t≤7min <dig> else, 

where ifn is assumed to be removed completely from the medium in a washing step at time t =  <dig> min. throughout the paper we consider the outputs y <dig> ...,y <dig> illustrated in figure  <dig>  since it is reasonable to assume that these outputs could be measured every minute in a laboratory experiment, we record their simulated values at times t =  <dig> min,...,  <dig> min and discard all other simulated values. the outputs y <dig> ...,y <dig> correspond to the following quantities: the sum of the concentrations of all phosphorylated stat <dig> molecules regardless of their binding or dimerization status ; the concentration of all activated jak molecules regardless of their binding status ; the concentration of all stat <dig> dimers regardless of their binding and phosphorylation status ; and the concentration of all stat <dig> monomers regardless of their binding or phosphorylation status .

by choosing the output y <dig> as defined in figure  <dig> we implicitly assume that phosphorylated stat <dig> molecules can experimentally be distinguished from unphosphorylated ones. similarly, measuring y <dig> requires distinguishing phosphorylated jak molecules from unphosphorylated ones. we assume the necessary measurements can be accomplished by western blotting with phospho-specific antibodies. furthermore, output y <dig> requires distinguishing dimeric from monomeric stat <dig> species. this can be accomplished, if we assume that native sds-gels, which separate dimers from monomers, are used for western blot measurements. finally, we assume that relative phosphorylated jak and phosphorylated stat <dig> concentrations from western blot measurements can be converted into absolute concentrations by immunoprecipitating the phosphorylated protein with phospho-specific antibodies and parallel western blotting of a calibrator protein  with known concentration.

since western blot experiments are known to have large standard deviations, we choose a standard deviation of 20%  <cit> . the choice of experimental noise is critical for the results of any analysis of the parameter variance, since larger measurement noise leads to larger parameter variance.

we stress again that all values yi are simulated values throughout the paper. as pointed out above, however, it is crucial to choose outputs, inputs, and the sampling time which mimic real experimental conditions.

model simplification work flow
the model simplification work flow combines delocalized parameter estimation, identifiability testing, variance analysis, and goodness of fit testing. we describe these building blocks first. note that the work flow can either be started with simulated data from a reference model or with real data.

parameter estimation
assuming normally distributed and independent observational noise, the solution of the optimization problem

  p^=arg minpχ <dig> where 

  χ2=∑i=1ny∑j=1nt−yiσij) <dig>  

results in the maximum likelihood estimate  of p, where σij denotes the standard deviation of data point y˜i. the mle is calculated numerically with the gradient-based sequential quadratic programming software npsol <dig>   <cit> . since gradient-based solvers for nonlinear problems are attracted by local optima, results strongly depend on starting values. we solve the optimization problem for many starting values on a parallel computing cluster to mitigate this well-known problem. starting values are sampled with latin hypercube sampling   <cit>   for an efficient coverage of the sampled space. we will refer to this delocalized parameter estimation approach as multi-start estimation for short.

identifiability
the parameter pk of the model  is called globally  structurally identifiable, if for any admissible input u, p' and p" , the equality y = y implies pk'=pk". global  at-a-point identifiability is defined analogously for a fixed reference parameter p'. in the remainder of the article the term identifiability refers to at-a-point local identifiability.

it can be shown that identifiability of a model implies uniqueness of the minimizer p^ of   <cit> . conversely, a model is not identifiable if p^ minimizes  and there exists a p˜≠p^ such that χ2=χ <dig>  we can locally test if such a p^ exists by analyzing the neighborhood of a minimum p^ of  using the quadratic approximation of χ2

 χ2≈12ΔpthΔp, 

where Δp=p˜−p^ and h is the hessian matrix of . in gaussian approximation the components of h read

  hkl=∑i=1ny∑j=1nt∂yi∂pk1σij2∂yi∂pl=kl, 

where w is the inverse of the measurement variance matrix. for any fixed ϵχ <dig> the inequality ΔpthΔp≤ϵχ <dig> describes an ellipsoid that approximates the ϵ-uncertainty region around p^. the axes of this ellipsoid coincide with the eigenvectors ui of h. the length of the ith axis is proportional to 1/λi, where λi denotes the eigenvalue of eigenvector ui. since h is positive semidefinite in gaussian approximation, its eigenvalues are nonnegative. if any eigenvalue is close to zero, λi ≈  <dig>  the ellipsoid that represents the ϵ-uncertainty region is very elongated in the direction of the corresponding eigenvector ui. this implies that parameters far apart have almost the same χ <dig> value, or equivalently, at least one parameter has a large variance. correspondingly, the existence of one or more λi =  <dig> implies at least one parameter has an infinite variance.

we use an eigenvalue-based approach  <cit>  to rank model parameters pi according to their identifiability. this method identifies the smallest eigenvalue λmin of h, searches for the largest component in the corresponding eigenvector umin, and fixes the corresponding parameter to its estimated value p^i. the hessian is then recomputed with respect to all parameters that have not been fixed, and the procedure is repeated until all parameters are fixed. the order in which parameters are fixed by this approach corresponds to the identifiability ranking of parameters, with the first parameter in the ranking being the least identifiable and the last parameter being the most identifiable one  <cit> . we stress that, in contrast to  <cit> , we do not use the eigenvalue method to distinguish the identifiable parameters from the unidentifiable ones, but only to rank all parameters according to their identifiability. the resulting ranking is then used in the variance analysis as described in the next section. in particular we do not need a cutoff value for small eigenvalues as introduced in  <cit> .

variance analysis
intuitively, a model is locally identifiable at p^ if the variances σ <dig> of all parameters are small. a variance based identifiability criterion, which is stated more formally below, is preferred over the eigenvalue criterion  <cit>  for its simplicity and intuitive interpretation. variances σ <dig> are determined by running parameter estimations  for a large number n of starting points in the neighborhood of a minimizer p^ of , accepting those estimation results with small p-value , and calculating the mean and variances for the resulting set of accepted parameter estimates. more precisely, we consider an estimation result to be acceptable if it has a significantly small p value  <cit>  , where p is the probability for the value of the χ2-distribution with v = nynt - np degrees of freedom to be less than an observed χ2-value. the variance σ <dig> and the mean p¯i are calculated from the accepted estimates according to the following equations.

  σ2=1naccept−1∑j=1naccept <dig>  

  p¯i=1naccept∑j=1nacceptqij, 

where naccept is the number of accepted estimates, and q is an np × naccept-matrix that contains these estimates in its columns. since both variance and standard deviation depend on the mean, we use the scale invariant coefficients of variation v=σ/p¯i <cit>  to compare variances of different parameters. a v value of  <dig>  for example, implies the estimates for parameter pi exhibit 100% standard deviation relative to the mean.

we consider a model to be identifiable, if the coefficients of variation v, i =  <dig> ...,np, of all parameters are smaller than a bound v¯ . a value of v¯= <dig>  implies the parameter estimation step resulted in a relative standard deviation of 1% or less for all parameters. note that this interpretation of v¯ is only valid if the parameter estimation runs cover a sufficiently large neighborhood of the candidate value p^. if the sampled neighborhood is too small, the estimates converge to parameter values that cover the entire sampled neighborhood, and the neighborhood needs to be enlarged. in this case, v is a not an upper but a lower bound for the coefficient of variation of parameter pi. clearly, this lower bound cannot be used to infer identifiability, but it can be used to infer that a model is not identifiable.

both v¯ and the significance level for p obviously are adjustable parameters. as discussed in the section entitled results and discussion, however, the chosen values are not critical. furthermore, we stress that their values  are conservative in any case.

testing goodness of fit
the purpose of the identifiability tests and variance analyses is to ensure a balance between the number of unknown parameters that need to be estimated on the one hand, and the information content of the data used for their estimation on the other hand. essentially, we consider model complexity and information content of data to be balanced, if the parameter estimation yields unique values for all estimated parameters, or equivalently, sufficiently small coefficients of variation for all estimated parameters. besides guaranteeing the uniqueness of the estimated parameters, however, we also need a measure for the quality of the fit of different models to the data. we use a criterion based on akaike's information criterion  <cit>  for this purpose. the aic reads

  aic=−2ln|​data) + 2np, 

where l | ​data) is the likelihood of model g given the measurement data. for brevity l | ​data) will be denoted as l. in case of χ2-parameter estimation l is given by

 l=∏i=1ny∏j=1nt12exp−yi)22σij2), 

and ln can be written as

  ln=ln12)−∑i=1ny∑j=1nt−yi)22σij2=c−χ2/ <dig>  

for a review see  <cit> . in equation  c denotes a model independent constant, which can be neglected when comparing aic values of different models. combining equations  and  the aic criterion can be written as: aic = χ <dig> + 2np. we use a variant of aic, aicc  <cit> , which is well suited for estimation with small data sets. the results obtained with aicc are compared to other established variants of the aicc criterion, the aicc difference Δk and the aicc weights wk. Δk describes the difference between the aicc value of model mk and the model with the smallest aicc value. the value of wk can be interpreted as a probability that model mk is the best model within a set of alternative models. a closer description of the criteria can be found in additional file  <dig> supplementary text  <dig>  the interested reader is referred to  <cit>  for more details.

model simplification work flow
having presented its building blocks, the work flow for the model simplification can now be stated. we assume the original model m <dig> is of the form . this implies the output variables y have been chosen and initial conditions x = x <dig> are given.

 <dig>  choose the input function u, t∈ choose an upper bound v¯ for the coefficients of variation as introduced in the section variance analysis. set the iteration counter to k =  <dig> 

 <dig>  calculate an estimate p^k for mk as described in the section entitled parameter estimation.

 <dig>  calculate the hessian matrix of mk evaluated at the estimated values p^k and rank the parameters of mk as described in the section entitled identifiability analysis.

 <dig>  for i ∈ { <dig> ..., np} in the order that results from step  <dig>  calculate the coefficient of variation v as described in the section entitled variance analysis. if v>v¯, go to step  <dig>  if v≤v¯ for all i =  <dig> ..., np, terminate.

 <dig>  simplify those parts of the model that involve the least identifiable parameters. refer to the resulting model as mk+ <dig>  increment k, and go to step  <dig> 

as pointed out before  <cit> , the model simplification step  cannot be carried out automatically, but requires insight into the model. typical simplifications consists of lumping of reactions or introducing simplified reactions.

note that the work flow does not guarantee that the smallest eigenvalue of the hessian matrix increases in every iteration. since the model structure is changed in step  <dig>  the size and structure of the hessian matrix changes. consequently, there is no one-to-one relationship between the eigenvalues of the hessian before and after a simplification step. in practice, however, a decrease of the smallest eigenvalue from one simplification step to the next rarely occurs. in the example treated here, a decrease occurs in step  <dig> from λmin =  <dig>  · 10- <dig> to λmin =  <dig>  · 10- <dig>  this decrease is outweighed by an overall increase from λmin =  <dig>  · 10- <dig> before the first simplification step to λmin =  <dig>  · 10- <dig> after the last simplification step. our improved work flow differs from the one proposed in  <cit>  in several aspects. most importantly, the single step model simplification described in  <cit>  is extended to a multi-step model simplification work flow in which new estimations are carried out after each model simplification step. secondly, we propose a variance based stopping criterion, which can be interpreted more easily than the eigenvalue bound used in  <cit> . for both the parameter estimation and the variance analysis step we use multi-start estimations. finally, the parameter estimation and identifiability analysis are carried out with respect to outputs y here. in contrast, trajectories of all state variables xi, i =  <dig> ..., nx were assumed to be available in  <cit> . as pointed out above it is crucial to consider outputs instead of state variables if simulations are supposed to mimic realistic laboratory experiments.

RESULTS
in this section we apply the model simplification work flow to the truncated jak-stat model from figure  <dig>  we refer to this model and the parameter values published by yamada et al.  <cit>  as m <dig> and p^ <dig>  respectively. we carry out a sequence of  <dig> simplification steps. the resulting models and the corresponding estimated parameter values are denoted by m <dig> ...,m <dig> and p^ <dig> ...,p^ <dig>  respectively. all models are described in detail in additional file  <dig> 

all parameter estimation steps  are carried out in logarithmic parameter space to efficiently cover a large search area . when estimating parameters pk for model mk, estimated parameter values p^k− <dig> of mk- <dig> serve as reference values. more specifically, starting points pk for multi-start parameter estimation are sampled within four magnitudes around these reference values, i.e., in the ranges p^ik−1⋅10−2≤p^ik≤p^ik−1⋅ <dig>  i =  <dig> ...,np. data points for the estimation are generated once by simulating the reference model m <dig> and recording the output values y, t <dig> =  <dig> min,..., t <dig> =  <dig> min. as proposed in  <cit>  we assume 20% standard deviation for experiments and set σij in eq.  accordingly. for the variance analysis  we choose the boundaries p^i⋅10−12≤pi≤p^i⋅ <dig> and a significance level for p of  <dig> %. both choices turned out not to be critical as discussed at the end of the section. finally, we set v¯= <dig> . this implies we consider two values for the same parameter to be equal, if these values deviate by a relative error of 1% or less.

in each parameter estimation step and in each variance analysis step  <dig> starting points are sampled. the best estimates of each work flow iteration are listed in additional file  <dig> table s <dig> 

simplification 1: neglecting the stat1phos reassociation to the activated receptor
values p^ <dig> for the parameters of model m <dig> are available from the literature  <cit>  and therefore need not be estimated in the first simplification step. the work flow is therefore started with the identifiability analysis . this analysis yields a smallest eigenvalue of  <dig>  · 10- <dig>  which is a strong indication that the model is not identifiable. this result is corroborated in step  <dig> , where a high coefficient of variation, v ≥  <dig> , results for the least identifiable parameter, kf <dig>  we can only infer a lower bound on v, since the parameter values estimated in the variance analysis step span the entire range of the starting values. as discussed in the section entitled variance analysis, this indicates that starting values would have to be sampled from a larger parameter space region, if a value for v rather than a lower bound was necessary. a lower bound suffices, however, to infer that the variances are too large to consider the model to be identifiable. consequently, we attempt to simplify those parts of the model that involve the least identifiable parameter kf <dig> .

the parameter kf <dig> describes the association of phosphorylated stat1c to the activated receptor complex . as discussed in  <cit>  this re-association of previously phosphorylated stat1c can be neglected, since the phosphorylation of cytoplasmic stat1c by the activated receptor is the key event rather than the reassociation of the previously phosphorylated stat1c. moreover, phosphorylated stat1cphos monomers are driven to homodimerize. therefore, only few such monomers are available to react with the activated receptor. by removing kf <dig>  the backwards reaction with kinetic constant kd <dig> and the species ifn_r_jakphos_2_stat1cphos become obsolete and are removed, too. since the initial value of ifn_r_jakphos_2_stat1cphos is zero in the reference model, this species can be removed from the model without the need for adjusting the initial values of the remaining state variables. we reduce our model from  <dig> states and  <dig> parameters to  <dig> states and  <dig> parameters. we refer to the resulting model as m <dig> 

simplification 2: neglecting the dissociation of high affinity complexes before phosphorylation or dephosphorylation can occur
the unknown model parameters of the simplified model m <dig> are determined by carrying out a multi-start parameter estimation . the best estimate of the multi-start parameter estimation p^ <dig> results in a χ <dig> value of  <dig>  · 10- <dig> and an aicc value of  <dig> . both values will be discussed below once a comparison with other models is possible.

the first parameter in each ranking is the least identifiable one. no χ <dig> and aicc values are given for m <dig>  since the model parameters are not estimated but taken from the literature  <cit> .

the identifiability analysis  and the variance analysis  result in a smallest eigenvalue of  <dig>  · 10- <dig> and in v ≥  <dig>  for the least identifiable parameter kd <dig>  respectively. note that a lower bound results for v for the same reason as in the first simplification step. again this lower bound suffices to infer that the model is not identifiable. since the identifiability analysis ranks parameter kd <dig> as the least identifiable parameter, we attempt to simplify the reactions that are affected by kd <dig> 

the parameter kd <dig> describes the dissociation of unphosphorylated stat1c from the activated receptor complex. we assume the affinity of stat1c for the active receptor complex is high  <cit> , or equivalently, phosphorylation of stat1c on average takes place much faster than the dissociation of unphosphorylated stat1c. therefore, the dissociation of unphosphorylated stat1c is a rare event and the dissociation reaction can be removed from the model.

the same line of argumentation holds for the parameters kd <dig>  kd <dig>  and kd <dig>  kd <dig>  describes the dissociation of stat1cphos  from ppx before ppx-induced dephosphorylation occurs. kd <dig> belongs to the dissociation reaction of shp <dig> from the activated receptor that takes place before shp <dig> dephosphorylates the activated receptor. for consistency we renamed kf <dig>  kf <dig>  kf <dig>  and kf <dig> to k <dig>  k <dig>  k <dig>  and k <dig>  respectively. since no state variables are removed in this simplification the initial conditions are taken from the previous model.

while the resulting model m <dig> has the same number of state variables  as the previous model m <dig>  the number of parameters dropped from  <dig> in m <dig> to  <dig> in the new model m <dig> 

simplification 3: assuming jak and the receptor are preassociated
the third simplification step starts with the multi-start estimation of the parameters p <dig> of m <dig> . these parameters can be estimated with a χ <dig> value of  <dig>  · 10- <dig>  the aicc value decreases from  <dig> to  <dig>  which implies that the simplifications made so far improve the balance between model detailedness and quality of fit .

the identifiability analysis  indicates, however, that m <dig> with a smallest eigenvalue of  <dig>  · 10- <dig> is not identifiable. this result is confirmed by the variance analysis , which results in v ≥  <dig>  for the least identifiable parameter kd <dig> . this value of v is a lower bound only; see the discussion in the first and second simplification steps. the results of the identifiability analysis and the variance analysis suggest to simplify the reactions that kd <dig> is involved in.

kd <dig> is the kinetic constant for the dissociation of jak from a single receptor. if we remove this reaction, we can further simplify the model by assuming that jak and the receptor are associated before the signal arrives  <cit>  . the changes amount to assuming that the association of jak to r is much faster than the dissociation of r_jak. under this assumption jak  =  <dig> nm) and r  =  <dig> nm) initially only exist in complex, therefore the initial condition of r_ jak has to be changed from  <dig> to  <dig> nm.

the resulting model, which we refer to as m <dig>  comprises  <dig> state variables and  <dig> parameters.

simplification 4: omitting ppx mediated stat1cphos dephosphorylation
the multi-start parameter estimation for the parameters p <dig> of m <dig> results in χ <dig> =  <dig>  · 10- <dig> . the aicc value improves from  <dig> to  <dig> . step  <dig> of the work flow ranks kf <dig> as the least identifiable parameter with a corresponding eigenvalue of  <dig>  · 10- <dig>  which indicates that kf <dig> is not identifiable. step  <dig> yields a lower bound on the variation, v ≥  <dig>  for kf <dig>  . while providing only a lower bound, this result corroborates that the model needs to be simplified with respect to the reaction kf <dig> is involved in.

kf <dig> describes the rate of the dephosphorylation of stat1cphos_ <dig> by ppx . stat1cphos can be dephosphorylated by ppx along either of two routes. 1) ppx binds and dephosphorylates the phosphorylated stat1c monomer or 2) ppx binds the homodimer of phosphorylated stat1c and subsequently dephosphorylates one of the two stat1c proteins. in the second case the heterodimer stat1c_stat1cphos is created. since neither stat1c heterodimers, nor the phosphatase ppx have been experimentally validated we propose to considerably simplify this part of the model . we remove ppx and all its complexes from the model and assume a simple first order kinetic for the conversion of the phosphorylated dimer into two unphosphorylated monomers. we stress that removing ppx is a drastic simplification. the fact that the ppx-related parameters kf <dig> and k <dig> are ranked third and fourth in the identifiability ranking  further suggests that this simplification is reasonable. since not only ppx  =  <dig> nm) but also all ppx bound species are removed, the initial conditions of the remaining species do not change.

the resulting model m <dig> is visualized in figure  <dig>  the number of state variables and parameters drops from  <dig> to  <dig> and from  <dig> to  <dig>  respectively.

simplification 5: combining stat1c-receptor complex formation, stat1c phosphorylation, and complex dissociation
the parameter values p^ <dig> can be estimated with a χ <dig> value of  <dig>  · 10- <dig> . the aicc value improved from  <dig> to  <dig> .

the identifiability analysis ranks k <dig> as the least identifiable parameter with a corresponding eigenvalue of  <dig>  · 10- <dig> . the variance analysis results in v ≥  <dig>  for k <dig> , where v is a lower bound for the same reason as in the previous simplification steps. both the smallest eigenvalue and the large lower bound on v indicate that model m <dig> is not identifiable.

the reactions around k <dig> are simplified as sketched in figure  <dig>  essentially, this simplification combines the formation of the stat1c-receptor complex, the phosphorylation of stat1c, and the dissociation into one step. as a consequence, stat1c phosphorylation is modeled as a second order kinetic in which the activated receptor is not consumed. this implies that stat1c phosphorylation still depends on the concentration of both the activated receptor complex and stat1c. in this simplification the species ifn_r_jakphos_2_stat1c is removed. since initially this species does not exist, its removal has no influence on the remaining initial conditions.

the resulting model m <dig> has  <dig> state variables and  <dig> parameters.

simplification 6: cytoplasmic stat1cphos dephosphorylation can be omitted when modeling only the first  <dig> minutes of signaling
the multi-start parameter estimation for the parameters p <dig> of model m <dig> yields a χ <dig> value of  <dig>  · 10- <dig> . the aicc value improves from  <dig> to  <dig> .

the identifiability analysis  yields a smallest eigenvalue of  <dig>  · 10- <dig> caused by k <dig>  which indicates that model m <dig> is not identifiable. this result is corroborated by a large lower bound on the coefficient of variation of k <dig>  v ≥  <dig>  . step  <dig> does not yield a value but only a lower bound for the coefficient of variation for the same reasons as discussed in the previous simplification steps. k <dig> is the kinetic constant of the dephosphorylation of stat1c, a reaction we cannot simplify further. in order to create an identifiable model, we must remove this reaction , which amounts to ignoring cytoplasmic dephosphorylation in the first  <dig> minutes of signaling. we stress this does not imply that no cytoplasmic dephosphorylation exists. note that a similar conclusion was drawn by yamada et al.  <cit> , who state that dephosphorylation of cytoplasmic stat by ppx is of minor importance in the pathway. since no states are removed in this step, the initial conditions are taken from the previous model. while the number of state variables remained unchanged, the number of parameters of m <dig> decreased from  <dig> to  <dig> 

properties of the final model m6
the multi-start parameter estimation of parameter values p <dig> of model m <dig> results in a χ <dig> value of  <dig>  · 10- <dig>  note this is the same value as in the previous simplification step. the aicc value, in contrast, improved from  <dig> to  <dig> .

model m <dig> turns out to be identifiable in steps  <dig> and  <dig> of the work flow. while this is not clearly apparent from the smallest eigenvalue , the coefficient of variation is now bounded above by v¯= <dig>  for all parameters. this implies that the multi-start parameter estimation conducted in the variance analysis resulted in parameter values that are within 1% of the respective mean p¯i for all parameters pi of the model.

assessing the quality-of-fit with akaike's information criterion and related criteria
the aicc values improved in all simplification steps. in addition to the aicc values we report aic values, aicc differences Δk, and aic weights wk in table  <dig>  in summary, all criteria agree in the sense that the final model m <dig> is the best model with respect to the quality-of-fit. we note that the improvement of aic and aicc values is quite low in the last step while the last model has a drastically higher probability of being the "best" model in the sense of its aic weight w <dig> = 78%. we conclude that the resulting simplified model m <dig> is both identifiable, and it balances quality-of-fit and model detailedness.

choice of parameter boundaries and the significance level in the variance analysis
the starting values for the variance analysis must be chosen from a sufficiently large parameter space neighborhood of the nominal values p^. if the estimates obtained in the variance analysis converge to values that cover the entire sampled neighborhood, the coefficient of variation may be limited by the choice of the neighborhood. since enlarging the sampling neighborhood can only result in equal or larger coefficients of variation, the v values obtained with a too small neighborhood are lower bounds on the actual coefficients of variation.

in all but the last simplification step the estimates obtained in the variance analyses cover the entire sampling neighborhood . the resulting lower bounds on v are sufficiently large , however, to infer that the respective models are not identifiable. in the final simplification step, in contrast, the sampling neighborhood is large enough, since the estimates obtained in the variance analysis are tightly packed within 1% error  around the nominal value. the choice of the sampling neighborhood is therefore not critical in any simplification step.

we claim the chosen value of  <dig> % for the significance level is not critical, either, since the work flow terminates after the same number of simplification steps for a large range of values, which can be seen as follows. the values of v do not change if the significance level is increased in the sixth simplification step, since all converged parameter estimates for p^ <dig>  are already accepted for the chosen value  <dig> %. smaller, i.e. stricter, significance levels well below 10- <dig>  on the other hand, do not change the coefficients of variation, either . in the first through fifth simplification step, the coefficients of variation increase for larger, i.e. less strict, significance levels than  <dig> %. since the coefficients of variation are large enough to necessitate another iteration through the work flow already, increasing the significance level does not have any effect. on the other hand, decreasing the significance level to value as low as 10- <dig> does not affect the v-values for m <dig> through m <dig> .

CONCLUSIONS
we presented a work flow for model simplification and demonstrated its use by simplifying an overly detailed model for jak-stat signal transduction. here, we used simulated data from a model taken from the literature. however, the method is intended to be used with real data. the work flow can be used to ensure model quality with respect to two important aspects: 1) given an unidentifiable model, the work flow results in a model that is locally identifiable with small coefficients of variation for all parameters and 2) the balance of goodness of fit and the amount of model detail - as quantified by aic based criteria - is better than for the original model and intermediate models that arise in the work flow. the first point is addressed by applying both local identifiability analysis and local multi-start parameter estimation with subsequent analysis of the variance of acceptable estimates. the second point is taken care of by calculating the information criteria, based on aic, that provide a measure for the information loss that occurs when reducing model complexity.

the proposed work flow does not automate model simplifications, but the identifiability ranking of parameters only provides hints on which parts of the model are too detailed. nevertheless, the simplifications found for the jak-stat example lend themselves to biologically sound interpretations. for example, dissociation reactions are neglected for high affinity complexes in the simplified model , jak and the receptor  are assumed to be preassociated , and cytoplasmic dephosphorylation turns out not to be relevant for the considered outputs .

several other authors suggest to fix unidentifiable parameters and to estimate the remaining ones  <cit> . in contrast, the model structure is simplified in the present paper until all parameters are identifiable, i.e. all parameters can be estimated with small error bars. note that changes in the model structure imply structural changes of the hessian matrix. due to these changes together with the local character of the identifiability method, λmin may decrease after one simplification step. in practice, however, this is a rare event, which is outweighed by a prominent increasing trend of λmin.

we stress that the presented case study involves generating output data points yi by simulations carried out with the unidentifiable reference model m <dig>  in this sense, both parameter fixing, and the approach as used in the case study involve unidentifiable models. however, when real experimental data is used, our approach is free of this defect. while more involved than parameter fixing, the proposed approach results in models with a complexity that is consistent with the experimentally accessible data.

authors' contributions
tq and mm developed the presented work flow. tq implemented the method and accomplished the model simplification of the jak-stat pathway. ad and fs designed the hypothetical experiments and made critical suggestions to model simplifications. all authors drafted, read and approved the final manuscript.

supplementary material
additional file 1
the supplementary pdf file accompanying this article contains the supplementary tables s1-s8b and supplementary texts 1- <dig> 

click here for file

 acknowledgements
we gratefully acknowledge funding of tq and ad by deutsche forschungsgemeinschaft  under grant mo 1086/5- <dig> and grant scha 785/4- <dig>  respectively.
