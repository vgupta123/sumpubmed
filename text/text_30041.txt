BACKGROUND
microarray based analysis of immunoprecipitated chromatin  constitutes a powerful technique to detect the interaction of dna with regulatory proteins over large segments of chromatin  <cit> . with advances in microarray fabrication, high-density tiling arrays are now being employed for genome-wide chip-chip studies  <cit> . in chip-chip, immunoprecipitated chromatin is amplified, fluorescently labeled and hybridized to a tiled dna microarray. fluorescent signal detected from hybridization to several oligomers representing a contiguous region is graphically depicted as a "peak" and is suggestive of a protein binding site. although putative binding sites can be individually validated using complementary strategies, comprehensive, genome-wide identification of high confidence peaks constitutes a major challenge for chip-chip studies.

several methods have been developed to detect peak regions  <cit> . cawley et al.  <cit>  and keles et al.  <cit>  applied the wilcoxon rank sum test and t-test, respectively, to generate test-statistics for sliding windows. cawley et al. used a fixed p-value cutoff to select peak regions. whereas keles et al. employed the benjamini and hochberg step-up procedure  <cit>  to control false discovery rate . in addition to the requirement for experimental replicates, gottardo et al.  <cit>  identified the absence of powerful multiple testing adjustment methods as a limitation of these methods. li et al.  <cit>  proposed a hidden markov model  approach to identify peak regions, assuming model parameters could be estimated from previous experiments. ji et al. <cit>  used a modified t-statistic with a more robust estimate of variance to measure probe-level binding signal, then used either moving window averaging or hmm to estimate window-level binding signal, and finally estimated local false discovery rate  of each peak region  <cit> . estimation of lfdr requires dissection of the mixture distribution of chip-chip signals, which includes the distribution of chip enriched signals  and the background  distribution. ji et al. <cit>  estimated the mixture distribution by unbalanced mixture subtraction, which requires additional information to construct the unbalanced mixtures. instead of concentrating exclusively on the strengths of binding signals, zheng et al.  <cit>  identified peaks using both signal strength and signal pattern. specifically, they modeled the dna fragmentation process with a poisson point process and concluded that if the binding signal is transformed to log scale, isolated "peaks" should exhibit a triangular shape allowing development of a double regression method, mpeak, to identify triangular patterns from chip-chip data.

two recent studies  <cit>  have employed bayesian hierarchical models to identify protein binding sites from chip-chip data. a major advantage of bayesian hierarchical models is that the information across probes can be shared; this is especially important when analyzing a limited number of replicates. however, the difficulty of fitting the complicated bayesian hierarchical models poses a heavy computational burden. despite their common characteristics, several attributes distinguish these two approaches. keles's method <cit> , hgmm , adopted a hierarchical gamma-gamma model  <cit> . hgmm is able to detect peak regions of different sizes. however, its constant coefficient of variation assumption can have an undesired effect in the presence probe outliers  <cit> , and it assumes at most one peak per genomic region, so that the genome has to be partitioned  into smaller regions before applying hgmm. gottardo et. al.'s method  <cit> , bac , is based on approaches used for gene expression studies  <cit>  with some additional modifications to exploit the spatial dependence between neighboring probes and to improve the robustness for chip-chip studies. however, bac, as it is currently implemented, cannot be applied to a single sample.

in this paper, we propose a mixture model approach to identify peaks from chip-chip data. our method builds on the important observation made by buck et al.  <cit>  that the signals from chip-chip data are not symmetric. when transformed into log scale and represented as a histogram, the signal density often has a heavier right-tail reflective of the presence of true positive signals. it is reasonable to assume that the majority of the left-tail of the signal density arises from background noise, which defines the null distribution. based on the additional assumption that the null distribution is normal with mean of  <dig>  buck et al.  <cit>  used negative signals to construct the null distribution and then evaluated the p-values of tested regions. following buck et al.  <cit> , we assume that the null distribution is symmetric, but we allow the null distribution to be non-normal and allow its center to deviate from  <dig>  we estimate the local false discovery rate   <cit>  for each peak based on a nonparametric approach to dissect the null distribution  and alternative distribution . as pointed by zheng et al.  <cit> , omitting auto-correlation structure of nearby probes leads to bias in estimating the significance level of each peak. in this study, we adopted the poisson point process used by zheng et al.  <cit>  to estimate auto-correlation and incorporate auto-correlation into the lfdr evaluation procedure.

compared with the existing methods, our method does not rely on potentially restrictive assumptions, such as a normal null distribution  <cit> , or prior knowledge, such as the availability of model parameters <cit> . our major assumption is that the null distribution is symmetric, which can typically be achieved after appropriate normalization . importantly, our method permits analysis in the absence of replicates, a situation that often arises in exploratory chip-chip studies <cit> . in addition, our method functions well with abundant peak regions, which is common in the increasing popular epigenetic studies  <cit> .

our method also alleviates the burden of cross array normalization. in large scale studies, a number of arrays are often needed to cover the entire region of interest. signal differences between arrays may due to technical effects  or relevant biological differences. if prior knowledge implies that there is no systematic biological difference across arrays, it may be more appropriate to combine those arrays prior to the application of peak finding methods. for example, in nimblescan, the software provided by nimblegen, the raw data  is normalized by subtracting a robust estimate of the sample median. in other words, the data from different arrays are aligned by their medians. however, in practice, it may be difficult to know whether biological differences contribute to systematic differences across arrays. our method uses the signals derived from one array to identify peaks thereby avoiding the potential problem of cross array normalization. peaks from different arrays can then be compared by their lfdrs.

in raw data, the null distribution reflecting background noise may not be symmetric and may be heterogeneous depending on the gc-content of the probes  <cit> . therefore, within-array data normalization is crucial to the success of our mixture distribution method. song et al.  <cit>  proposed a normalization method, ma2c , that normalizes data by assuming the log-intensities of the two channels follow a bivariate distribution with gc-specific means and variances. song et al. have shown that ma2c standardizes data from different samples more efficiently than other existing methods. although ma2c works well in many situations, sometime ma2c normalized data still have nonhomogenous null distributions across gc-contents. to overcome this issue, our method uses a lowess smooth curve to capture the gc-content specific information.

our mixture model approach is general enough to be applied to one-color arrays , two-color arrays , and high throughput sequencing data. however, since the normalization method pertains to two-color arrays, we focus on its application for two-color arrays. we have implemented our method into an r package, mixer, which can be downloaded from .

methods
data normalization
let the x2i and x1i be log <dig> and log <dig> of the i-th probe with gc content k, and let μ2k and μ1k be the expected value of x2i and x1i, respectively. ma2c normalizes data by calculating

  

where  and  are robust estimates of μ2k and μ1k, respectively, and  is a robust estimate of the standard deviation of x2i - x1i - . considering  = x1i + as a predictive value of x2i based on the linear model log <dig> = log <dig> + b <dig>  where b <dig> is estimated by  - . then x2i - x1i -  is the residual from the baseline model log <dig> = log <dig> + , and the ma2c normalized value is simply a variance-standardized residual of this linear model with a slope of  <dig> . the underlying assumption of this baseline model is that log <dig> - log <dig> is constant given gc content. although this assumption may be sufficient for some samples, the channel differences of log-intensities may depend on the intensities themselves. for example, analyzing previously published array data  <cit> , we found that the channel difference in one array is negative when log <dig> and log <dig> are small, but approaches  <dig> as log <dig> and log <dig> become larger ). this variation justifies the use of a fully parameterized linear model: log <dig> = b <dig> + b <dig> × log <dig> as the baseline model. therefore, an improvement over the ma2c normalization would be to assume a linear relation between log <dig> and log <dig> and estimate both intercept and slope from data in a robust way, for example, using median regression. however, we found that the relation between log <dig> and log <dig> may be non-linear, and not fully captured by median regression , sup. figure  <dig> in additional file 1). to accommodate non-linear intensity-dependent patterns, we normalized data by lowess curve fitting conditioning on gc-content. the lowess normalization is able to account for either linear or non-linear relation and it is robust to outliers. specifically, given gc-content, let zi = g be the lowess fit , the normalized log ratio difference is calculated as

   

where mi is the median of x2i - zi. we found this lowess normalization better captured the relationship between signal intensities , and sup. figure  <dig>  sup. figure  <dig> in additional file 1). although lowess normalization has been applied to gene expression microarray data  <cit> , to the best of our knowledge, this is its first application to chip-chip data.

mixture models of chip-chip data
chip-chip data analysis represents a combined mixture model problem. observed probe-level data are sampled from the mixture distribution of background signals  and chip-enriched signals . in addition, peaks can be detected by moving windows of various lengths. therefore there are two mixture model problems: one at the probe level and one at the window level. let f <dig> and f <dig> be the probe level density functions of the null and alternative distributions respectively, and let π <dig> and π <dig> be the corresponding mixture proportions respectively, then the observed probe-level data follows the mixture distribution

  

we define a window as a fixed length region around a probe. let the window-level density functions for null and alternative distributions be g <dig> and g <dig> respectively. we use x to denote the window level signal strength to distinguish it from the probe level signal strength x. let the corresponding mixture proportions be κ <dig> and κ <dig>  then the observed window-level data follows mixture distribution

  

probe-level analysis
we first consider the probe level distribution fobs = π0f <dig> + π1f <dig> similar to the approach of buck et al.  <cit> , we utilize lower  signals to infer the null distribution f <dig> or g <dig> . we assume that the null distribution is symmetric but place no constraint on the function form or the location of the null distribution.

let μ <dig> be the center of the null distribution, which is approximately the π0/ <dig> percentile of the whole distribution assuming that the vast majority of the signals smaller than μ <dig> arise from the null distribution. this is a reasonable assumption because most chip-enriched signals are higher than the majority of the background signals. then in order to estimate π <dig>  we just need to estimate μ <dig>  based on the assumption that the null distribution is symmetric with center μ <dig>  it is reasonable to assume that μ <dig> is the mode of the entire distribution, or one of the two modes if the chip-enriched signals also form a mode  <cit> . therefore, in order to estimate μ <dig>  we identify the mode of the observed density fobs = π0f <dig> + π1f1

we first rounded all the probe level signals to a given precision, for example,  <dig>  or  <dig>  to facilitate subsequent computation. the precision is chosen so that little or no information is lost. we estimate the signal density function by kernel method   <cit> . if the estimated density function has two or more modes, we refer to the highest one as the major mode and the others as minor modes. for simplicity, if there is only one mode, we also refer to it as the major mode. a mode cannot be μ <dig> if it is bigger than the overall median, otherwise

  

specifically, we estimate μ <dig> based on the following procedure.

 <dig>  if the major mode is smaller than the overall median, we take it as μ <dig> 

 <dig>  if the major mode is bigger than the overall median and there is one and only one minor mode in 20th – 50th percentile of the observed signal , we take the minor mode as μ <dig> 

 <dig>  in all the other situations, we make a conservative estimation of the mode location of the null distribution. specifically, we iterate all the signal strengths within 20th – 50th percentile  and choose the greatest one so that the estimated null distribution is below the overall distribution, i.e., π0f <dig> ≤ fobs in practice, if such a conservative estimation has to be made, the resulting lfdr is an upper bound instead of an unbiased estimation of actual lfdr.

the major mode can be simply identified as the point with the highest density estimation. the minor mode can be identified as the point where the corresponding 1st derivative of the density function is  <dig> and the 2nd derivative is negative. we estimate the 1st and 2nd derivatives of the density function by savitzky-golay smoothing filters  <cit> . because there are fewer observations at the tails of a density curve, the kernel estimations there may have bigger variations. this variation could result in "small" modes at the tails that happen by chance. in order to avoid these potentially artifactual modes, we assume μ <dig> is within 20th – 50th percentile of the observed signal, which is equivalent to assuming the proportion of null signals is between 40% and 100%. this range is wide enough to accommodate the vast majority of the chip experiments. for experiments with even smaller proportions of null signals, pattern reorganization methods that capture chip-enriched signals in segments may be more appropriate  <cit> .

after identifying the mode of the null distribution , hence π <dig>  we take all the data points smaller than μ <dig>  denoted as d <dig>  all the data points equal to μ <dig>  denoted as d <dig>  and all the data points generated by flipping d <dig> around μ <dig>  denoted as d <dig>  merge them together  to estimate the null distribution f <dig> by kernel method   <cit> . finally the probe level lfdr, i.e., the posterior probability that one probe level signal arises from f <dig> is

   

where p <dig> indicates the probability that x is from the null distribution. in practice, kernel estimation of density functions may be unreliable at the tail area, due to limited number of observations. as a result, the lfdr estimates fluctuate. to circumvent this problem, we order those x where the lfdr is evaluated in ascending order x ≤ x ≤ ... ≤ x and update p0) by

  

therefore the estimation of p <dig> is smoothed and decreases or remain the same as x increases. a similar strategy has been used to define q-value from fdr estimates <cit> .

window-level analysis
the window-level signal strength x, which can be defined as mean or median , is a function of window size and the probe-level signals within the window. in this study, we assume the window size is pre-determined. let the probe-level signals within one window be x <dig>  x <dig> ..., xn, we calculate x as

   

where  is the average of probe-level signals and  is the standard error of  under null distribution. in other words, x measures the distance between  and μ <dig>  in terms of the standard error , which is generally bigger than  because there are auto-correlations between nearby probes even for background signals. we estimate  by

   

because we estimate  under null distribution,  depends only on the number of probes in the window and the distances between them, but not the particular probe level signals. this estimation in equation  has the same form as the one used by zheng et al.  <cit> . however, based on the underlying assumption that the vast majority of the signals are from the null distribution, zheng et al. used all the data below a threshold to estimate both var and corr. in order to accommodate a relatively large proportion of chip-enriched signals, we use different approaches to estimate var and corr. specifically, we estimate var using the data d = {d <dig>  d <dig>  d3} and estimate corr as follows. we model the signal strength at probe j by

   

where ωij is the probability that there is no break up of the dna sequence between probe i and j, and eij indicates the signal strength at probe j due to the dna segments not harboring probe i. xi and xj are measured based on a large number of sequence segments bound to the probe i and j, respectively. equation  can be understood as a summation of the contributions from all the sequence segments captured by probe j from an expectation perspective. since eij is independent with xi,

   

because we are modeling the correlation structures in the background signals, var = var = var, hence corr = ωij. in order to estimate ωij, we modeled the sonication process by poisson point process  <cit> . suppose, on average there is one break up of dna sequence per k bp, the incident rate in the poisson point process is λ = 1/k, and ωij = exp, where dij indicates the distance between probe i and j. therefore given the parameter λ , we can estimate ωij, hence corr, and then we can calculate the window-level statistics x. usually, the parameter λ  can be obtained from the experimental setting for the dna sonication process. for sequencing studies, ωij can be simply estimated from the distributions of sequence fragment lengths  <cit> .

next, the window level mixture distribution gobs = κ0g <dig> + κ1g <dig> can be dissected similarly to the analysis of the probe level data. finally, the window level lfdr, i.e., the posterior probability that one window-level statistics x is from the null distribution is

   

where q <dig> indicates the probability that x is from the null distribution. similarly to the probe-level analysis, we smooth the lfdr by updating q0) as

  

here x ≤ x... ≤ x are the window-level signals where the lfdr are evaluated.

peak identification
after probe-level and window-level analyses, we identify peaks by the following steps. first, "peak windows" with elevated signal strengths are identified using a window-level lfdr cutoff, e.g., lfdr ≤  <dig> . second, overlapped "peak windows" are separated into discrete peak regions. third, each resulting peak region is evaluated by further restriction on the number of probes within it and the signal strengths of those probes. a typical rule could be "a peak region should harbor at least  <dig> probes", or "a peak region should harbor at least  <dig> probes with probe level lfdr ≤  <dig> ". the third step is optional but recommended since "isolated peaks" composed of only one or two probes are unlikely to represent true sites of protein-dna interactions. similar rules have been used in other chip-chip data analysis methods  <cit> .

RESULTS
we compared the results of our peak detection strategy with other published algorithms using three datasets. we focused on two common conditions that were typically not evaluated during the development of the existing peak detection algorithms: the absence of experimental replicates and the presence of abundant peak regions.

spike-in data
we initially evaluated our method using the data set from a recent spike-in study  <cit> . in this benchmark study comparing chip-chip conditions, human genomic dna was combined with defined cloned regions  over a wide range of concentrations to reflect the enrichment ratios often observed in chip experiments. the use of an experimental spike-in data set allows definitive knowledge of the regions that are enriched. although multiple tiling array designs were tested, since the current implementation of our normalization method is for two-color arrays, we analyzed the data generated from seven nimblegen arrays. the original data in "pair" format, which includes signals from both cy <dig> and cy <dig> channels, were downloaded from ncbi geo database. four arrays  were hybridized to dna spiked with specific unamplified fragments. the other three arrays  were hybridized to dna spiked with fragments that had been amplified. each array harbors  <dig>  probes spanning  <dig> encode-selected regions <cit> .  <dig> or  <dig> regions were spike-in with unamplified and amplified dna, respectively, at various concentrations from  <dig>  fold to more than  <dig> fold. a complete description of these data can be found in johnson et al.  <cit> .

in the original data, the peak regions were sparse . we simulated data with increasingly abundant peak regions by replacing the signals from non-spike-in regions with the signals from spike-in regions. to better mimic the original data and more faithfully replicate the flanking contexts, we replicated each spike-in region  including  <dig> bp on either side  as a unit, which we refer to as a peak-containing region. lengths of such peak-containing regions vary from  <dig>  bp to  <dig>  bp, with median of  <dig>  bp. we split the remaining non-peak-containing regions into  <dig>  segments of  <dig>  bp. we then used the peak-containing regions to replace  randomly selected non-peak-containing segments. in the first augmented data set, we replicated each peak-containing region  <dig> times, resulting in  <dig> / <dig>  peak-containing regions  in the unamplified/amplified dna samples, respectively. in the second augmented data set, we replicated each peak-containing region  <dig> times, resulting in  <dig> / <dig>  peak-containing regions  in the unamplified/amplified dna samples, respectively.

analysis of spike-in data
using the native and augmented spike-in data, we compared the efficacy of our peak detection method, which we named mixer, with three other methods: ma2c, tilemap, and hgmm. these methods were selected because they are frequently used and/or they also aim to dissect the mixture distributions of chip-chip data. bac by gottardo et al.  <cit>  was not compared as it requires experimental replicates. mpeak by zheng et al.  <cit>  was also not compared because mpeak assumes that the peaks have triangular shapes. however, the signals from spike-in regions exhibit rectangular patterns.

we used the java version of ma2c software with the default normalization option . other options led to similar or inferior results . after normalization, the median was used by ma2c to identify peak regions with a bandwidth  of  <dig> bp and at least  <dig> probes per peak region. a bandwidth of  <dig> bp was chosen based on the lengths of the spike-in regions. other bandwidths  produced inferior results .

for the implementation of mixer, as with ma2c, we used "half-width of the sliding window of  <dig> bp with at least  <dig> probes" as the criteria to select peak regions. we set the average sonicated sequence length as  <dig> bp  to estimate the correlation between nearby probes. substitution of values from  <dig> bp to  <dig> bp did not significantly change the results. in order to demonstrate the difference between lowess and ma2c normalization, we tested mixer with data normalized by both methods.

we employed cisgenome <cit>  for tilemap calculation. log <dig> transformed data were pre-normalized using the quantile normalization option in cisgenome. tilemap summarizes window-level signals by either moving average or hmm. the significance of each peak is measured by an lfdr estimated from unbalanced mixture subtraction . we used hmm because it yields superior results in terms of higher power given an lfdr cutoff. two parameters  must be provided to ums to enable selection of probes  from the overall distribution to construct the null/alternative distributions. we used either the default values  or adjusted values based on the knowledge of true proportion of spike-in signals. specifically, we set p =  <dig>  and q =  <dig>  for the original data with ~ <dig> % of spike-in probes; p =  <dig>  and q =  <dig>  when ~ <dig> % of the probes are from spike-ins; p =  <dig>  and q =  <dig>  when ~ <dig> % of the probes are from spike-ins.

the r package r/hgmm was used for hgmm calculation. hgmm can take into account a distribution of peak sizes. we generated this distribution based on the actual lengths of the spike-in regions. in most experiments, however, this information can only be estimated. raw data  were log <dig> transformed and normalized using the preprocess function of r/hgmm before applying the hgmm function.

we examined the influence of the proportion of null signals on mixer's performance. figure  <dig> shows the estimated densities of probe and window-level signals from the original and two simulated dataset from one array. as the number of spike-in regions increases, the right tail of the window-level signal density becomes heavier. the increased signal density enhances accuracy and robustness to dissect the mixture distribution. similar patterns were also observed for other arrays.

we then evaluated mixer, ma2c, tilemap and hgmm using the spike-in data. first, given a fixed cutoff of either fdr ≤  <dig>   or lfdr ≤  <dig>  , we compared the power and actual fdr of these methods . the discovery of a peak region was counted as a true discovery  if its center was within a spike-in region; otherwise it was counted as a false discovery. although an alternative comparison would examine the top k peaks identified by different methods, we based our comparison on fixed lfdr/fdr. this approach is more relevant since the number of binding sites is typically unknown.

the first four samples, gsm <dig>  gsm <dig>  gsm <dig>  and gsm <dig> were spiked with unamplified dna, while the last three samples gsm <dig>  gsm <dig>  and gsm <dig> were spiked with amplified dna. among the total of  <dig>  probes, about  <dig>  of them are from spike-in regions. we did not obtain results of hgmm for some arrays  due to failure of function hgmm.

see main text for the simulation methods. approximately  <dig> % of the probes are from spike-in regions.

see main text for the simulation methods. about  <dig> % of the probes are from spike-in regions.

we compared the results of mixer after data normalization by lowess or by ma2c. for the original data when the spike-in regions are sparse, in general, mixer performs much better with lowess normalization than with ma2c normalization. mixer with ma2c normalization often includes many false discoveries resulting in a high fdr . as spike-in regions become more abundant, the normalization method makes less difference . dissection of the mixture distribution becomes easier with additional data to estimate the alternative distribution, which may overcome the differences attributable to the normalization methods.

we then compared the performance of the peak detection algorithms on the original and augmented data sets. hgmm was computationally intensive, requiring more than  <dig> hours to analyze one array. in contrast, the other methods we tested completed the analysis of a single array in less than  <dig> minutes. with the original data, , hgmm failed for four arrays due to errors in numerical optimization. although the use of initial values other than the defaults may avoid such errors, we did not explore this due to the high computational cost. in the augmented data sets , hgmm did not fail for any array. however, hgmm was often over-conservative missing 30–50% of spike-in regions .

at the default parameters of p =  <dig>  and q =  <dig>  , tilemap was over-conservative and had limited power, especially when the proportion of spike-in regions is high. tilemap performed much better when provided appropriate values for parameters p and q based on the true proportion of alternative distribution . however, in actual applications, the alternative distribution is typically unknown. for example, for amplified dna samples when there are  <dig>  spike-in regions, with lfdr smaller than  <dig> , tilemap identifies ~70–80% of the spike-in regions if p =  <dig>  and q =  <dig> , but only ~60% of the spike-in regions with the default parameters, p =  <dig>  and q =  <dig> .

both mixer and ma2c have better power than tilemap and hgmm. as shown in tables  <dig>   <dig> and  <dig>  mixer has lower fdr than ma2c for original data with sparse spike-in regions and has slightly better power than ma2c with abundant spike-in regions. however, a straightforward comparison between mixer and ma2c is confounded by the fact that, unlike other methods, ma2c provides fdr estimates rather than lfdr estimates. since lfdr and fdr cutoffs are not directly comparable, we employed roc -like curve to compare mixer and ma2c . unlike a typical roc curve, these roc-like curves plot / on the y-axis against / on the x-axis in order to accommodate the large number of true negatives in chip-chip data,  <cit> . to simplify the plots, we averaged across samples for amplified/unamplified dna respectively. fdr and lfdr cutoffs were set between  <dig>  to  <dig> . mixer outperformed ma2c when the spike-in regions were abundant . however, when the spike-in regions were sparse, ma2c outperformed mixer if an appropriate fdr cutoff was chosen.

analysis of ctcf-binding data
we also evaluated our method using the chip-chip data from a study of the zinc finger insulator protein ctcf  in imr <dig> human fibroblast cells <cit> . this dataset includes  <dig> arrays each with about  <dig>  50-mer probes tiling the non-repetitive sequences of the human genome in  <dig> bp resolution. the original pair data  and cy <dig> ) were obtained from the ren laboratory website . each of the  <dig> arrays was analyzed separately. the results of different peak-finding algorithms were compared to the results of an independent chip-seq based analysis that identified  <dig>  ctcf binding sites in human cd4+ t cells  <cit> .

hgmm was not evaluated due to its high computational cost. model parameters were similar to those described above. for tilemap, window-level signals were summarized by hmm, and the lfdr of each peak region was estimated from unbalanced mixture subtraction  with default parameters . for ma2c, default options were used to normalize data  and summary window-level signals . in mixer, the average dna fragment length was set to  <dig> bp .

although true ctcf binding sites are unknown, to permit a systematic evaluation of the various peak detection strategies, we compared the peak regions identified by each method with the  <dig>  ctcf binding sites reported from a chip-seq study by barski et al.  <cit> . since experimental variation will likely result in differences between chip-chip and chip-seq data, chip-seq data serves as a common and independent source for comparison, rather than a perfect standard. a common site was called when the center of the chip-chip peak was located within the chip-seq peak. without the knowledge of all true ctcf binding sites we are unable to compare fdrs, as we had done for the spike-in data. therefore, we examined a fixed number of high confidence peak regions and compared the proportion of overlap. specifically, we examined the overlap between the chip-seq reported sites and  <dig> ,  <dig> , or  <dig>  peak regions with the highest confidence  identified by each peak detection algorithm. peaks identified by mixer consistently demonstrate a greater overlap with chip-seq peaks than those identified by ma2c and tilemap .

in each cell, the number of overlapped peak regions and the percentage among the top k peak regions are shown, where k =  <dig> ,  <dig> , or  <dig> .

analysis of faire data
we also compared mixer, ma2c, and tilemap on array data produced by hybridization of dna enriched by formaldehyde-assisted isolation of regulator elements  <cit> . briefly, faire identifies open chromatin regions using organic extraction of formaldehyde crosslinked chromatin. dna recovered in the aqueous phase is fluorescently labeled and hybridized to arrays. faire typifies the data from epigenetic studies where relevant features are expected to be abundant genome-wide. faire-chip thus provides an appropriate application for mixer. for this analysis, faire was performed on chromatin isolated from human foreskin fibroblasts and hybridized to a 1% encode tiling array at 38-bp resolution  <cit> .

four arrays hybridized with faire-selected chromatin were normalized individually. after averaging identical probes across the arrays mixer was applied. ma2c and tilemap were run using their default options for replicate analysis. since hypersensitivity to endonucleases is a standard method to identify open chromatin regions, we compared the results with  <dig>  open chromatin regions identified by dnase i hypersensitivity-chip in lymphoblastoid cell lines  <cit> . the faire regions identified by each of the three methods share ~40% overlap with dnase sites, indicating similar specificities for the various methods. since different techniques and different cell lines are compared, this overlap likely represents an underestimate of specificity. however, mixer offers increased sensitivity as it identifies more peaks  at the same specificity. at a local fdr  or fdr  cutoff of  <dig> , mixer identifies  <dig> peaks  whereas ma2c identifies  <dig> sites , and tilemap identifies  <dig> sites . at a local fdr/fdr cutoff of  <dig> , mixer identifies  <dig> peaks ; ma2c identifies  <dig> ; and tilemap identifies  <dig> .

a local fdr less than  <dig>  is a much more stringent cutoff than fdr less than  <dig> . the former means that the highest fdr for any one of the peak regions is  <dig> , whereas the latter indicates that the average fdr is  <dig> . averaging the local fdr less than  <dig>  results in an estimated fdr for mixer or tilemap of less than  <dig> . because it uses a less stringent fdr cutoff, ma2c is expected to identify more peaks. the actual identification of fewer peaks by ma2c suggests the introduction of bias by ma2c normalization. to test this hypothesis, we supplied ma2c with mixer-normalized data and observed a significant improvement of its sensitivity;  <dig>  peaks  were identified at fdr less than  <dig> , still fewer than the  <dig>  peaks identified by mixer with an estimated fdr of less than  <dig> .

discussion
we have developed a mixture model approach to dissect the mixture distributions of chip-chip data: the null distribution  and the alternative distribution , at both probe and window levels. this approach builds on the method of buck et al.  <cit>  to estimate null  distribution of chip-chip signal data and utilizes the poisson point process assumption proposed by zheng et al.  <cit>  to model dna fragmentation. an advance over most existing peak detection strategies, our approach is less dependent on key assumptions and prior knowledge. our method takes into account the auto-correlation structure of nearby probes, permits a relatively large proportion of chip-enriched signals in the mixture distribution, and does not require cross-array normalization. after dissecting the mixture distribution, both probe-level and window-level lfdrs are provided to evaluate the statistical significance of the identified peaks. using three data set representing widely divergent experimental conditions, we demonstrated that our method performs comparably or better than several representative existing methods, especially when the true peak regions are abundant. our method also applies lowess fit data normalization to capture the non-linear relationship between log and log signals from two-color arrays. mixer emphasizes the identification of abundant short peak regions rather than extended binding regions. we have recently developed a different method to identify broad signal patterns  <cit> .

despite mixer's advances, areas for improved performance remain. we smooth the lfdr estimate so that it decreases as probe-level/window-level signals increase. this smoothing strategy avoids major fluctuations of lfdr estimates when observations are limited . a similar strategy has been used to define q-value from fdr estimates  <cit> . however, smoothing may lead to under-estimates of the lfdr, especially for small lfdr. to improve the lfdr estimates, both signal strength and signal pattern  could be incorporated, a strategy we are currently evaluating.

the use of high throughput sequencing based chromatin identification  has become increasingly common. however, determination of sufficient sequencing depth remains a significant challenge, especially for abundant epigenetic events. chip-chip remains a valuable method for pilot experiments and to cross validate results, a particularly appropriate application of mixer. mixer could also be adapted to dissect mixture distributions from sequencing data. tag counts derived from unfractionated input control could model a null distribution  <cit> . we are currently testing this approach.

CONCLUSIONS
in summary, we have developed a method that combines improved data normalization and peak detection for chip-chip studies. mixer offers several advantages including lfdr determination and enhanced performance when peak regions are abundant, a common scenario for genome-wide studies of chromatin organization and epigenetics  <cit> .

availability and requirements
we have implemented our method in an r package mixer, which can be freely downloaded from . the source code can be redistributed and/or modified under the terms of the gnu general public license as published by the free software foundation.

authors' contributions
all authors have read and approved the final manuscript. ws, ijd and mjb conceived this study. ws implemented the methods and analyzed the data. ws, ijd, mjb and mp wrote the paper.

supplementary material
additional file 1
supplementary materials for "improved chip-chip analysis by mixture model approach". supplementary results demonstrating different data normalization methods.

click here for file

 acknowledgements
we thank paul g. giresi and jason d. lieb for providing the faire data. ws is supported, in part, by the united states environmental protection agency grant . however, the research described in this article has not been subjected to the agency's peer review and policy review and therefore does not necessarily reflect the views of the agency and no official endorsement should be inferred. ijd is supported in part by the national cancer institute , the v foundation for cancer research, the rita allen foundation, and the corn-hammond fund for pediatric oncology.
