BACKGROUND
before we proceed to discuss the algorithmic details, we present some general background materials which include the introduction of the concept of graphs and chemical structures as graphs.

graphs
a labeled graph g is described by a finite set of nodes v and a finite set of edges e ⊂ v × v. in most applications, a graph is labeled, where labels draw from a label set λ. a labeling function λ: v ∪ e → Σ assigns labels to nodes and edges. for the label set Σ we do not assume any structure of Σ now; it may be a field, a vector space, or simply a set. following convention, we denote a graph as a quadruple g =  with aforementioned v, e, Σ, λ,. a graph g =  is a subgraph of another graph g′ =, denoted by g ⊆ g′, if there exists a 1- <dig> mapping f : v→ v′ such that

• for all v ∈ v, λ = λ′ )

• for all  ∈e, , f ) e′

• for all  ∈e, λ  = λ′ , f)

graph modeling of chemical structures
chemical compounds are organic molecules that are commonly modeled by graphs. in our study, we adopt the 2d connectivity map where we use nodes in a graph to model atoms in a chemical structure and edges to model chemical bonds in the chemical structure. in the representation, nodes are labeled with the atom element type, and edges are labeled with the bond type . the edges in the graph are undirected, since there is no directionality associated with chemical bonds. figure  <dig> shows one sample chemical structure and its graph representation.

methods
here we investigate the utility of graph kernel for chemical similarity measurement. towards that end, we first give a overview of g-hash. we then briefly outline a graph kernel  <cit> , which we will use, to define similarity of chemical structures. in particular, below we introduce details of the feature extractiion process, the index structure for fast similarity query and the kernel function for similarity measurement.

algorithm overview
the flowchart of g-hash is show in figure  <dig>  for graphs in database, we first extract node features with a graph wavelet analysis . we then build a hash table to store such features. for query graphs, we perform the same preprocessing techniques. based on the hash table, we calculate distances between query graph and graphs in the database. finally, top k nearest neighbors are reported.

in particular, the application of g-hash to chemical databases follows the below steps. in index construction, we utilize the following steps:

• for each chemical in the chemical database, extract node features for each atom in the chemical

• using graph wavelet analysis, extract local features for each atom in the chemical 

• discretize the combined features and hash the atoms in a hash table.

in the query processing phase, we utilize the following steps:

• for the query chemical, extract node and local features for each atom in the chemical

• discretize the combined features and hash the atoms in a hash table using the same procedure in index construction

• compute distances of the query chemical to the rest of chemicals

• report the k nearest neighbors.

node feature extraction
to derive an efficient algorithm scalable to large graphs, our idea is to use a function Γ: v → ℝ n to map nodes in a graph represented a chemical compound to a n-dimensional feature space that captures not only the node label information but also the neighborhood topological information around the node. two steps involve this process: first node feature extraction through which we extract features associated with a node, and second local feature extraction through which we extract features in a local region centered at the specific node.

we use the following node  features: atomic number, the histogram of atom types of immediate neighbor of the node, the local functional group information, and the histogram of the  chemical bond information. the atom type of the node is a single number. for histogram of neighboring atom types, we collect information for c, n, o, s, and group the rest atom types to "others" to save space. we have a total of five numbers in the histogram. for local functional group information, we collect whether the node is in part of a 5-node ring, a 6-node ring, a high-order ring, a branch, or a path, as did in  <cit> . we have a single number for this feature. for the histogram of the  chemical bond information, we have three numbers corresponding to single, double, and aromatic bonds. in the previously mentioned node extraction method, we ignore the neighborhood topology information of the chemical compound by focusing on atom physical and chemical properties. to add neighborhood topology information, we utilize a technique called the graph wavelet analysis, as originally presented in  <cit> . the output of the wavelet analysis is a vector of local feature averages, with the size of the vector controlled by a diffusion parameter d. further details of the analysis can be found in  <cit> .

structure matching kernel
with the feature extraction methods, we designed a structure kernel, specified below, to measure the similarity of graphs:   

k can be any kernel function defined in the co-domain of Γ. we call this function km a structure matching kernel. we visualize the kernel function by constructing a weighted complete bipartite graph: connecting every node pair  ∈ v ×  v with an edge weighted by k, Γ). in figure  <dig>  we show a weighted complete bipartite graph for v = {v <dig>  v <dig>  v <dig>  v4} and v = {u1;u <dig> u3}.

from the figure we see that if two nodes are quite dissimilar, the weight of the related edge is small. since dissimilar node pairs usually outnumber similar node pairs, in our design, we use the rbf kernel function, as specified below, to penalize dissimilar node pairs.   

where ||x||

similarity search with hash functions
to support effectively indexing, here we use a hash table where the key is the related node feature vector and the value is the node. two chemicals are similar, if they share a lot of nodes that are hashed to the same cell since each node is represented by a feature vector which contains the local atomic and topological information. since node features and local features may contain numeric value, we discretize each feature vector and map the feature value to an integer. after discretization, we hash all nodes in a chemical to the related hash table. we show an example of such hash table below.

example  <dig> for simplicity, we apply the hash process to the single graph shown in figure  <dig> whose nodes are numbered with p <dig>  p <dig>  p <dig>  p <dig>  p <dig>  p <dig> shown in figure  <dig>  we assume d= <dig> and each node has 10features. for example, the feature vector for node with the label of 's' is  <cit>  since its atomic number is '16'; it has only one neighbor with node label 'c', zero neighbor with node label 'n', zero neighbor with node label of 'o', zero neighbor with node label of 's', and zero neighbor with node label of other atom symbol; it is in a path; and it connects with the neighbor through only one single bond. the feature vectors for all nodes are also shown in the figure  <dig> and the sample hash table is shown in the bottom panel of figure  <dig> 

with the feature vector discretization and hash table, we revise the structure matching kernel using an hash-based approximation as described below since only similar nodes are involved into the kernel calculation and k, Γh) ≈  <dig> if rbf kernel is used.   

where simi is the set containing the nodes from graph g that are hashed to the same cell as the node v does. |simi| is the number of nodes in the set of simi. in other words, we only count the number of common nodes, belonging to the graph g and g' in this version.

finally we compute the distances between the query chemical and chemicals in a chemical database to obtain the k nearest neighbors of the query chemical. the idea is to compute the euclidian distance of two objects between their embeddings in the related hilbert space according to the kernel function.

RESULTS
experimental setup
we have performed a comprehensive evaluation of our method by evaluating the classification effectiveness and scalability for large chemical databases. we have compared our method with other similarity measurements including the daylight fingerprints  <cit> , wavelet alignment kernel  <cit> , c-tree  <cit> , graphgrep  <cit> , gindex  <cit> . for g-hash, we extract  <dig> features for each node. we used the openbabel software package to compute daylight fingerprints  <cit>  and k-nearest neighbors. for wa, we set the diameter d =  <dig> and use haar wavelet function. for c-tree, graphgrep and gindex, we use default parameters. all methods, except c-tree, were implemented using the c++ programming language and compiled using g++ with -o <dig> optimization. c-tree was developed in java and compiled using sun jdk <dig> . <dig>  we performed our experiments on a linux cluster where each node has a dual-core intel xeon em64t  <dig> ghz processor and 4g memory running centos  <dig> 

data sets
we chose a number data sets for our experiments. the first five data sets are established data taken from jorisson/gilson data sets  <cit> . the next seven data sets are manually extracted from bindingdb data sets  <cit> . the last one is nci/nih aids antiviral screen data set  <cit> . for the jorissen data sets, there are five proteins for which  <dig> chemical structures are selected with  <dig> chemical structures clearly bind to the protein and the other  <dig> ones similar to the active ones but clearly not bind to the target protein. see  <cit>  for the further details. for the bindingdb database, we manually selected  <dig> proteins with a wide range of known interacting chemicals . for the purpose of classification, we convert the real-valued binding activity measurements to binary class labels. this is accomplished by dividing the data set into two equal parts according to the median activity value . table  <dig> shows the characteristics of the data sets. in the same table, positive compounds refer to those with higher activity values or binding to the target protein and negative compounds refer to those with lower activity values or not binding to the target protein.

#s: total number of compounds, #p: number of positive compounds, #n: number of negative compounds, #v: average number of nodes, #e: average number of edges.

we use the nci/nih aids antiviral screen data set, which contains  <dig>  chemical compounds retrieved from dtp's drug information system, as a large chemical database. there is a total  <dig> types of atoms in this data set; the most frequent ones are c, o, n and s. the data set contains three types of bonds: single-bond, double-bond and aromatic-bond. we randomly sampled  <dig> chemicals as the query data set.

similarity measurement evaluation with classification
we have compared classification accuracy using k-nn classifier on the  <dig> jorissen data sets and bindingdb data sets with different similarity measurements. for the wa method, we first obtain kernel matrix, and then calculate distance matrix to obtain the k nearest neighbors. for subgraph indexing methods such as gindex and graphgrep, we sketch one way to use them for similarity search. this method contains three steps:  randomly sample subgraphs from a query,  use those subgraphs as features and compute the occurrences of the subgraphs in graph databases, and  search for nearest neighbors in the obtained feature space. clearly, the accuracy depends on the number of features. here we pick  <dig> features for gindex. we use standard 5-fold cross validation to obtain classification accuracy. we have tested different k values ranging from  <dig> to  <dig> in classifications. the quality of the results are similar and we only report results with k =  <dig> 

the accuracy of the classification is shown in figure  <dig>  the average precision and recall are shown in table  <dig> and table  <dig> respectively. the accuracy results statistical information is shown in table  <dig>  from figure  <dig>  we know that c-tree and daylight fingerprints show the worst performance. theyjust are a little better than the random guess. wa method is better than them since it similarity measurement is based on kernel function. gindex based similarity measurement and g-hash has similar classification performance with about 78% of average accuracy and outperform others. g-hash outperforms c-tree and daylight fingerprints on all twelve data sets, with at least 18% improvement on most of data sets. the average accuracy difference between g-hash and c-tree and daylight fingerprints are around 23% and 22% respectively. the precision of c-tree and daylight fingerprints are lower than 50% for almost all data sets. g-hash is comparable to gindex on precision and recall, too.

asterisk  denotes the best precision for the data sets among g-hash, wa, c-tree, daylight fingerprint and gindex methods.

asterisk  denotes the best recall for the data sets among g-hash, wa, c-tree ,daylight fingerprint and gindex methods.

here we use  <dig> features for gindex. as we mentioned before, the performance of gindex depends on the use of feature set. figure  <dig> shows the accuracy comparison among different feature sets with  <dig> features,  <dig> features, <dig> features and  <dig> features for the method of gindex. from figure  <dig>  we know that more features provide better classification performance than less features do. here,  <dig> feature set provides the best classification results,  <dig> features set and  <dig> features set have a middle performance,  <dig> features set has the worst performance.

chemical enrichment study
in this section, we perform the enrichment study of g-hash, daylight fingerprints and c-tree. towards this end, we randomly picked  <dig> chemical compounds from  <dig> inhibitors of focal adhesion kinase  <dig>  with aid <dig> from pubchem  <cit> . we call those  <dig> chemicals as test data set. we augment this test data set to the nci/nih aids antiviral screen data set to form a new database. then we pick one chemical from these  <dig> chemicals as the query chemical to search the new database and retrieve  <dig> nearest neighbors. according to these  <dig> results, we calculate the "precision" curve. specifically, for the top k similarity compound, we compute precision as the percentage of chemicals in the top k compounds belongs to the true  <dig> hits and plot the change of precision along with the number of retrieved chemicals. obviously, the high precision shows good performance. after repeating the above steps for  <dig> times, we calculate the average precision curve shown in figure  <dig>  although daylight fingerprints performs better than c-tree, both of them show the low precision. g-hash performs much better than daylight fingerprints and c-tree. from figure  <dig>  we see that the precision is about  <dig>  when the total number of retrieved chemicals is equal to  <dig> for g-hash which means that  <dig> chemicals in the test data are contained in the top  <dig> nearest neighbors of the query chemical. the result is as same as what we expected. edit distance based similarity measurement used by c-tree prefers large graphs. for daylight fingerprints, two graphs sharing more small substructures or patterns are considered to be similar. but as we all know, the connection or position of these substructures also determines the similarity of graphs. our method, g-hash, not only consider the number of common small substructures but also consider the connection between them through using features reflecting local topological information and chemical information.

robustness
in this section, we evaluate the robustness of g-hash by using four different feature sets for the enrichment study mentioned above. in the first set of features, we use  <dig> features as discussed in the subsection of node feature extraction. for other three data set, we use wavelet analysis to extract features from the local region centered at the particular node. we use d =  <dig> with  <dig> additional features, d =  <dig> with  <dig> additional features and d =  <dig> with  <dig> additional features. so we have  <dig> feature sets with sizes  <dig>   <dig>   <dig> and  <dig>  the average precision curves over  <dig> queries and the optimal precision curve are shown in figure  <dig>  we draw the optimal precision curve in this way: if the number is retrieved chemicals is less than  <dig>  the precision is equal to 1; otherwise, the precision is equal to  <dig> over the number of retrieved chemicals. g-hash with  <dig> features shows the worst performance which is similar to that of c-tree and daylight fingerprints shown in figure  <dig>  g-hash with  <dig> features has a much improvement. g-hash with  <dig> features gives the best performance which is near to the optimal performance. g-hash with  <dig> features has a little worse performance than g-hash with  <dig> features. with less features, more nodes pairs are hashed into the same cell. this case prefers those graphs sharing too many small subcomponents. with more features, just a few nodes pairs are hashed into the same cell. this case prefers those small graphs. therefore the distance between graphs can not accurately represent their similarity with too large or small feature sets.

scalability
index construction we compare index size and average index construction time for different methods. towards that end, we have sampled different number of graphs ranging from  <dig>  to  <dig> . figure  <dig> shows the index construction time in milliseconds with respect to the size of database for g-hash, c-tree, graphgrep, gindex and daylight fingerprints. the construction time for g-hash is much faster than those for other four methods with a speed-up up to three order of magnitudes.

query processing time there is no direct way that we could compare g-hash and subgraph indexing methods such as gindex and graphgrep and we use the way that we outlined before . clearly the overall query processing time depends on the many subgraphs we sample. to estimate the lower bound of the overall query processing time, we randomly sample a single  subgraph from each of the  <dig> querying graph and use subgraph indexing method to search for the occurrence of the subgraph. we record the average query processing time for each query. this query processing time is clearly the lower bound since we use only one subgraph from the query graph. figure  <dig> shows the average query time  of different index methods in milliseconds with respect to the size of database. gindex is the worst one. fingerprints do the query faster than c-tree and graphgrep which are comparable. g-hash is the fastest one. when the size of database increases, g-hash scales better than daylight fingerprints, with around an order of magnitude speedup. g-hash performs better than c-tree, with two orders of magnitude speedup.

discussion
feature set influences one of the key factors for determining both the accuracy and efficiency of the g-hash method is the feature extraction methods r that maps nodes to a high-dimensional feature space. in order to evaluate the results, we have compare five sets of features. in the first set of features, we use two features as discussed in the methods section. in the second set, we use  <dig> features described in the subsection of node feature extraction. in the third feature set, we dropped the immediate chemical bond information from the first set and obtained seven features. in addition, we use wavelet analysis to extract features from the local region centered at the particular node. we use d =  <dig> with  <dig> additional features and d =  <dig> with  <dig> additional features. so we have  <dig> feature sets with sizes  <dig> ,  <dig>   <dig>  and  <dig> 

we have tested the classification accuracy with different feature sets. the average accuracy on  <dig> datasets is shown in figure  <dig>  when more features are used, we can obtain better results. the largest difference happens between  <dig> features and  <dig> features which means that the histogram of atom types of immediate neighbors and the local functional group information make a big contribution to improve classification performance. another relatively large difference happens between  <dig> features and  <dig> features which means the topological information of neighbors with hop distance equal to  <dig> much more makes sense. the difference between  <dig> features and  <dig> features is very small which shows that the histogram of the immediate chemical bond information plays little role in improving classification accuracy. we also tested the query processing time difference between different feature sets shown in table  <dig>  both too less features and too more features will speed up query processing. with too less features, many modes are likely hashed into the same cell so that the hash table is too short and less query processing time is needed. with more features, nodes are more likely to be hashed to different cells. if too more features are used so that the nodes of the query graph just are hashed into a few cells and hence we could speed up query processing time.

so to obtain both good classification performance and fast query processing, relatively more features should be used. in our case, the feature set with  <dig> features is the best choice.

CONCLUSIONS
in summary, similarity search plays a critical role in cheminformatics. efficient similarity query processing method for large chemical databases is challenging since we need to balance running time efficiency and similarity search accuracy. here we applied our previous similarity search method, g-hash, combining hash based indexing and graph kernel function, and applied it into the similarity search in the large chemical databases. the key features of g-hash are that the k-nn query time is scalable to large databases and has better classification accuracy. we have compared our method with commonly used methods such as daylight fingerprints  <cit>  and c-tree  <cit>  and have demonstrated the utility of our method.

competing interests
the authors declare that they have no competing interests. 

authors contributions
xw developed methods, implemented the software, and drafted the manuscript. jh offered the research idea, participated in the discussion during research, and wrote part of the paper. as was involved in testing the data set and helped revise the manuscript. gl provided advices on the chemical aspect of the work.

