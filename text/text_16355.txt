BACKGROUND
sequence motifs are short patterns that occur in dna with certain frequency and that often have some sort of biological distinct function. in most cases, that function is to serve as a binding site for proteins. when these proteins are transcription factors , the corresponding motifs are called transcription factor binding sites . knowing these tfbs gives a better understanding of how transcriptional regulation works, and therefore the discovery of tfbs is one of the most fundamental problems in molecular biology research  <cit> . historically, a wide variety of methods have been applied to this problem, computational methods being currently the prevailing approach. the computational problem consists of discovering motifs by searching for overrepresented  dna patterns in sets of functionally related genes, such as genes with similar functional annotation or genes with similar expression patterns. the number of different computational approaches to tackle this problem is constantly growing as computational techniques evolve. one of the most recent techniques, which, to the best of our knowledge, to this date has not yet been applied to motif discovery, is known as topic models  <cit> .

topic models
topic models are statistical algorithms, based on natural language processing and machine learning, which try to discover the structure of a set of documents according to the abstract topics contained in them by hierarchical bayesian analysis  <cit> . these algorithms allow examining a set of documents and determining the existing topics and their distribution among the documents based on the statistical properties of the words of a specific vocabulary in each one of them. where l application of topic models to the motif finding problem.

as far as we know, there is no literature about the application of topic models to motif finding algorithms. the first method here proposed tries to fill that gap and prove that topic models are a suitable method to the motif finding problem. in order to do so, it represents genetic sequences as documents and the k-mers contained in them as words, so that the patterns shown among these k-mers would be considered as motifs. figure  <dig> shows a graphic representation of a topic model and how our algorithm would adapt to it.fig.  <dig> representation of a topic model adapted to the motif finding problem. representation of a topic model adapted to the motif finding problem. this figure shows the basic structure of a topic model . the terms specific for the case of the motif finding problem are stated in red under the original ones in blue, showing that the motif finding problem can be represented by a topic model by describing the dna sequences as documents, the instances of each given motif as words in those documents, and the motifs as clusters of words or topics




the algorithm, as a topic model, would therefore examine a set of sequences to determine the hidden structure of the patterns contained in it. as this is totally consistent with the motif finding problem, it seems likely that the algorithm should be able to correctly discover motifs.

addition of topic models to a previously developed algorithm 
previously to this study of topic models applied to the motif finding problem, we developed another algorithm with the structure of a ga, which used statistical coefficients as a fitness measurement  <cit> . from this point, we will refer to this algorithm as the statistical ga algorithm. the statistical ga algorithm was proven to effectively find overrepresented motifs in sets of sequences. however, it had the main drawback of reporting an excessive number of false positives. along with the algorithm based entirely on topic models, in this study we also research how the use of topic models can be applied to improve the previously developed algorithm and reduce the number of false positives.

methods
how topic models work
the main problem, from a computational point of view, of topic modeling is to infer a concealed topic structure from the examination of the documents.

a topic is formally defined as a multinomial distribution over a fixed vocabulary. in other words, topic models consider that a document could, conceptually, be generated from a set of topics, each one of them being a set of words related to that topic. so that, to create a document, the words would be selected iteratively from the topics that we desire to appear in it. for example, if we want a document that is two thirds about stem cells and one third about cancer, we would create two topics  as sets of words typically related to them, and then construct the document by selecting two thirds of the words from the stem cells set and one third from the cancer set.

topic modeling consists of reversing this conceptual approach, considering that the topics of a document  can be inferred from the proportions of the words contained in them.

the intuition behind this algorithm is that all of the documents in the collection share the same set of topics, but each one of them in a different proportion, which is reflected in the distribution of the different words among them.

the inputs of a topic model are a set of n documents  and the number of topics k that are expected to be contained in the documents. for each one of the documents d
i to be analyzed, the most basic algorithm would process the words in a two-stage process.choose a random distribution of the document over the topics .

for each word w
j in the document:choose a random topic t
r from the distribution over topics previously generated.

once w
j is assigned, for each one of the topics t
m in the current set of topics, compute the proportion of words in the document d
i that are currently assigned to the topic t
m, p, and the proportion of assignments to the topic t
m over all of the documents that come from the word w
j , p and then reassign w
j to the topic that gives the best probability p * p.







a stable set of assignments will be reached after repeating the above steps for several iterations.

the benefit of the use of topic models is that they offer an automated solution to the organization and annotation of large text archives. however, this is not their only utility, and they can be applied to many other fields, such as the subject in question here, bioinformatics.

creating a motif finding algorithm based on topic models from scratch
algorithm structure
the first problem that arises when adapting a topic model to the motif finding problem comes from the idea that, whereas the words in a text document are clearly separated by spaces, in the case of a genetic sequence a mechanism to select the k-mers that will form the vocabulary must be defined.

a typical topic model, as a first step, usually creates a vocabulary from the words in the documents by discarding meaningless words , such as “the” or “of” in documents written in english, as well as words that are not repeated frequently, since in both cases they would not help to find the hidden topics and they would instead add noise to the algorithm. again, this is consistent with a motif finding algorithm, so in this case an initial vocabulary would need to be created similarly, but in this case by selecting k-mers that are overrepresented in the set of sequences.

from this a new problem arises, which is the impossibility to select all of the possible overrepresented patterns in a reduced amount of time. in order to deal with that, a genetic algorithm   <cit>  structure was chosen as the basis of the algorithm here presented, being the topic model the approach for selecting the best possible solutions in the fitness function.

algorithm implementation
the method here proposed is a heuristic algorithm, that is, it gives an approximate  solution, and it is also stochastic, so that each time it is run with the same set of sequences it will likely produce different results. it searches only for ungapped motifs, so that patterns which contain gaps might be predicted split into several separate motifs. also, in contrast with other motif finding algorithms, which usually suppose that there is at least an instance of the motifs in every sequence of the data set, it makes no assumptions about how the motifs are distributed among the sequences.

the algorithm is implemented as a classic ga. in other words, it starts by creating a population of possible solutions  for our problem and then it iterates over them, keeping the best  solutions of every iteration, discarding the worst ones, and creating new solutions based on the fittest ones for the following iteration, until an optimum solution is found or a given number of iterations is reached.

therefore, the only aspects that need to be defined are how the population is represented, how it is evaluated , how the fittest individuals are selected in every iteration, how new individuals are generated by the surviving ones  and when the algorithm will stop iterating and report a final solution .

representation
each individual of the population is a set of m k-mers which can be contained in any of the sequences of the data set. the k-mers can be of any length between a minimum and a maximum passed as a parameter. the initialization works as follows:

given a set of sequences, a minimum k-mer length k
min, a maximum k-mer length k
max, a minimum number of repetitions for each k-mer in the data set c
min, a population size n, and a number of words per individual in the population n. for each one of the n individuals, iterate until an initial set of n k-mers is reached:choose a random word length k within the range k
min
: k
max.

choose a random sequence from the data set.

choose a random position p in that sequence between  <dig> and l - k, being l the sequence length.

select the word w starting in the position p with length k.

count the number of occurrences c of the word w in the given sequence, allowing for 25% of mismatches.

shuffle w into w
s and count the number of occurrences c
s of the word w
s in the given sequence, allowing for 25% of mismatches.

if c - c
s is greater or equal than c
min, add the k-mer to the set for the given individual.




evaluation
the fitness calculation is the more crucial step in a ga. it is at this moment when the topic models must be applied and provide a way to obtain solutions for the motif finding problem.

the type of topic model chosen was a correlated topic model   <cit> , since it takes into consideration the correlation between topics, and, biologically speaking, motifs also usually show correlation, given that transcription factors which have correlated biological functions bind to them. a ctm makes use of a logistic normal distribution, which, through the transformation of a multivariate normal random variable, allows for a general pattern of variability between the components of the distribution  <cit> . more specifically, the ctm contained in the r package topicmodels  <cit>  was the method used for the construction of the ctm in every iteration.

for each one of the individuals of the population, its set of k-mers, along with the original set of sequences, is fed to a ctm as the vocabulary and the documents respectively. then the perplexity of the resulting model is measured and returned as the fitness of the given individual.

perplexity
the perplexity of a probabilistic model is a measure of the accuracy with which its distribution predicts a sample. it is the standard used in natural language processing to evaluate the accuracy of the model. the lower the perplexity, the better the model fits the data. the perplexity is calculated by splitting the dataset into two parts: one for training and one for testing, and then measuring the log-likelihood of the unseen documents. as the perplexity is calculated using the corresponding function provided by hornik and grün for their ctm implementation, the mathematical formula for perplexity used in this method follows their same definition  <cit> : perpω=exp−logpω∑d=1d∑j=1vnjd 


where n
 refers to the frequency with which the jth word appears in the document d.

selection
the algorithm tries to give an optimum set of solutions by minimizing the perplexity. therefore, for the selection of the fittest candidates, an elitist approach is used. in other words, after all of the fitness measurements have been done for a specific generation of individuals, these are selected in random pairs, in which the fittest individual  survives and the less fit individual is eliminated from the population. after this stage, n/ <dig> fit individuals remain in the population.

so the next step is generating new individuals by the use of the crossover function to create a new population of n individuals.

crossover
the crossover step is performed after the evaluation and selection step to generate new individuals in the population for the next generation.

first, two individuals are randomly selected from the population to act as parents.

the crossover function in this case is a classic one-point crossover in which two children are generated by swapping the data beyond a randomly selected crossover point between both parents. in this case, the crossover point is an index in the array of k-mers of the individuals, which indicates which k-mers to select from each one of the parents .

the two newly formed children are added to the population and the process is repeated until the population contains n individuals again.

mutation
mutation happens randomly, according to a parameter that defines the frequency. it is also applied to random individuals. the mutated individual will have a random number of its k-mers slightly shifted from their original position .

post processing
once the ga is terminated, the fittest individuals are sorted by perplexity  and selected accordingly as solutions depending on a parameter set by the user that defines how many motifs are expected to be found in the data set. for each one of these solutions, the ctm is generated once again and each one of the resulting topics is returned as a motif of the data set.

improving the statistical ga algorithm by the use of the perplexity measurement
the statistical ga algorithm  <cit>  works as a ga in which the fitness function takes three steps to discard unfit solutions based on three different coefficients  <cit> , in which the main method to select the final candidates is the mann-whitney u-test  <cit> . each candidate is a k-mer of a fixed length defined as a parameter represented as a position in a supersequence, which is a concatenation in a random order of all the genetic sequences received as an input. to simplify the calculations, this supersequence is divided in a set of subsequences so that in each iteration the fitness is calculated for a given subsequence and the candidates which show no overrepresentation in the given segment are swiftly discarded without further computations.

adding the use of topic models
the main drawback of the statistical ga algorithm is that it reported a big amount of false positives, and one of the main reasons for this was that it had no way to measure the confidence of the results reported. thus, it reported at least one motif for every data set, ballasting that way the overall performance of the algorithm. the solution here proposed for that problem consists of taking the final set of instances provided by the algorithm, creating a ctm with them, and measuring the perplexity. then, only in the cases in which the perplexity is lower than certain threshold the motifs returned by the algorithm are reported . the impact of this was that now the statistical ga algorithm only reports motifs for those data sets in which there is a ctm that fits well the solutions found. that way, the algorithm is able to measure the confidence of the solutions obtained by the main ga. the threshold was set at  <dig> for experimental reasons, given that in the tests performed the motifs reported with a perplexity higher than  <dig> tended to be false positives.fig.  <dig> statistical ga algorithm workflow after including the use of topic models. this figure describes the updated flow of the statistical ga algorithm after adding the perplexity measurement for the selection of solutions. there are four steps in this flow: the first one, in which the candidate instances are selected by the original statistical ga; the second one, in which these instances are clustered attending to their similarity calculated by their hamming distance; the third one, which consists of building the ctm and measuring its perplexity, and the last step, which consists of reporting the motif if the perplexity calculated in the previous step is lower than 100




assessment
several studies  <cit>  concluded that evaluating the performance of a motif finding tool has been proven to be a difficult task, and there is no method to compare tools that can give a definitive conclusion about which one is the best and which one is the worst. keeping this idea in mind, both of the two methods here presented were tested making use of the assessment proposed by tompa et al.  <cit>  in their study to evaluate the performance of several motif finding tools by the scores obtained in eight different statistical coefficients. it is worth mentioning that only the accuracy of the tools predicting binding sites is evaluated, and other aspects such as the running time of each method, are not measured. the benchmark provided by the assessment, which is the same one used in this study, is formed by  <dig> data sets, which belong to four different species  and also  <dig> negative controls to sum a total of  <dig> data sets. these  <dig> data sets are also divided into three different categories: data sets of type real, which correspond to the real promoter sequences that contain the original sites that the different tools will try to locate; data sets of type generic, which correspond to promoter sequences generated randomly from the same genome, and data sets of type markov, which correspond to synthetic sequences generated by a markov chain. the original assessment compared the efficiency of  <dig> different tools  . each one of those tools was allowed to report only one  motif per data set. the format in which this motif was reported was as a list of instances and their corresponding positions in the sequences of the data set. then, the accuracy of how well these instances match the real instances of the motif is studied both at nucleotide and site level. at site level, a predicted site is considered to match the known site if it overlaps at least one quarter of it. with this information, the following eight statistics are used to measure the accuracy of each one of the methods:
nsn : nsn=ntpntp+nfn 



nppv : nppv=ntpntp+nfp 



nsp : nsp=ntnntn+nfp 



npc   <cit> : npc=ntpntp+nfn+nfp 



ncc   <cit> : ncc=ntp×ntn+nfn×nfpntp+nfnntn+nfpntp+nfpntn+nfn 



ssn : ssn=stpstp+sfn 



sppv : sppv=stpstp+sfp 


sasp   <cit> : sasp=ssn+sppv <dig> 





the coefficients starting by n are statistics at nucleotide level, and the coefficients starting by s are statistics at site level. tp, fp, tn and fn refer to the number of true positives, false positives, true negatives and false negatives respectively.

both of the two methods here described were tested using the methodology presented in this assessment and compared to the  <dig> methods with which it was originally carried out.

RESULTS
the ctm algorithm was run with the following parameters:motif width between  <dig> and 30

population size: 50

number of generations: 90

number of instances per individual: 1000

maximum number of solutions: 10

mutation rate:  <dig> 




as for the statistical ga algorithm, it was run with the same parameters as in the original study  <cit> . after adding the perplexity measurement in the post processing stage, a new restriction was included: only the motifs reported with a perplexity lower than  <dig> were considered as solutions.

all of the tests were run in a laptop computer with a  <dig>  ghz intel core i <dig> processor and an  <dig> gb  <dig> mhz ddr <dig> memory.

figure  <dig> summarizes the average values of the statistics previously defined for each one of the  <dig> tools originally analyzed in the assessment and for both of our proposed tools. figure  <dig> shows the average values grouped by organisms.fig.  <dig> average statistical values for all  <dig> data sets. this figure shows the average scores obtained by each one of the tools studied for each one of seven different statistics for all the  <dig> data sets of the benchmark. the statistical ga method is shown as ga approach, and the ctm method as ctm approach


fig.  <dig> average statistical values for each organism. this figure shows the average scores obtained by each one of the tools studied for each one of seven different statistics grouped by the four different species contained in the data sets. the statistical ga method is shown as ga approach, and the ctm method as ctm approach




to calculate the average values, we followed the same process as in the original assessment. in a first step, the values of ntp, nfp, nfn, ntn, stp, sfp and sfn obtained for each one of the data sets are summed. then, each one of these summed values is considered as the given score of a large data set, and the eight statistics are calculated for that large data set , obtaining that way the average scores.

discussion
as previously stated, none of the statistics analyzed should ever be taken as an absolute measurement of the quality of the methods. the authors of the assessment  <cit>  themselves indicate several factors that affect the results and might give a wrong impression about the performance of the different algorithms:this assessment, as any other possible method, can never be considered a standard method to measure the biological significance of the studied tools, since it is still unknown how the subjacent biology works.

as each one of the algorithms was required to predict only one  motif for each data set, there might be an arbitrary component in the candidate selected by each tool.

the assessment requires each tool to report only one  motif for each data set. however, it is known that, especially in the case of the data sets of type real, they are likely to contain more than one motif.

many of the known binding sites are longer than  <dig> bp. our tools, as well as most of the others, were run for motifs no longer than  <dig> bp in the case of the ctm algorithm and  <dig> bp in the case of the statistical ga algorithm. this affects the performance at nucleotide level even if the performance at site level is high.

the assessment relies on transfac  <cit>  as its only source of known binding sites. as the information obtained from transfac is not contrasted with other sources, it might as well contain errors.

the above explained method used to compute the average scores of every tool tends to penalize those tools that make wrong predictions more than those that make no predictions at all, as  <dig> is the default value for the cases in which no motifs are reported.




as long as all these factors are not forgotten, some important conclusions regarding the performance of the different methods can still be inferred from the use of the benchmark proposed in the assessment.

first of all, the ctm method shows levels in sensitivity  only outperformed by our other method, the statistical ga . it also shows a remarkable average site performance  and, regarding the rest of statistics, even though the numbers obtained are not especially satisfying, they are comparable to most of the other methods.

thus, we can already reach the conclusion that topic models are a perfectly valid method to design motif finding algorithms.

as for the statistical ga method, fig.  <dig> shows the improvement in all of the average statistics after narrowing down the results reported according to the perplexity shown in the ctm. all of the scores for the different statistics are practically doubled after filtering out the motifs for which the perplexity of the corresponding ctm is higher than  <dig> fig.  <dig> comparison of statistics for the statistical ga method before and after filtering by perplexity. here it is shown the improvement in the average scores of the statistical ga method for seven different statistics obtained by the addition of the filtering of the results by their perplexity in the corresponding ctm




this tool now clearly outperforms most of the other methods, showing levels of nsn, ssn, and sasp to which any of the other tools can hardly be compared . this further proves the usefulness of topic models for motif discovery tools.

given the nature of both methods, and the high number of true positives shown , it seems clear that both succeed in predicting many of the sites but lack of a mechanism to detect false positives. in other words, as the high scores in sensitivity and average site performance show, both methods can correctly report most of the known motifs, but they locate too many instances of them in the input sequences, so that the number of false positives reported in the assessment, especially at nucleotide level, appears too large, in spite of the correctness of the consensus or the score matrix given by the algorithms as a result. we therefore believe that the high number of false positives is due, to a large extent, to the nature of the assessment. this drawback was considerably reduced in the new version of the statistical ga algorithm, thanks to the use of the perplexity measurement to avoid predicting wrong motifs, but we believe the number of false positives in the assessment could still be shortened if some sort of method was used in the post processing step to obtain only the correct instances of the known site by the use of a weight matrix based on the consensus sequence, instead of simply reporting all of the candidate instances found as both tools currently do. for the ctm method, a way to filter out the results which are not reported with high confidence is required as well.

the method that gives the best overall statistics after the statistical ga method is weeder. as the authors of the assessment clarify  <cit> , one of the main reasons for that is the way in which it was run. the author of the tests decided to pick a cautious mode, that is, to predict a motif only if there is a high confidence of its existence. that explains, therefore, the great improvement in the statistics for our statistical ga tool, and that it is mostly due to the way to calculate the average statistics proposed in the assessment.

as for the running time, as stated before, it is not an object of the assessment, which is focused on the accuracy of the sites predicted. however, it is worth mentioning that the ctm method slows down considerably when the number of input sequences is bigger than three. therefore, some solution for this problem, such as dividing the data sets into subgroups of three or fewer sequences, will be required. the statistical ga method, on the other hand, is able to report the results of data sets of any size in a matter of minutes.

CONCLUSIONS
dna motif finding still remains as one of the most challenging tasks for researchers, and so it is the task of comparing the performance of the different existing tools, given that each one of them has been designed using very heterogeneous algorithms and models, and that there is still little known about the subjacent biology. therefore, we must insist on the fact that nowadays it is impossible to define a standard quality measurement to evaluate the performance of the different tools.

most of the studies on the performance of motif finding algorithms  <cit>  conclude

that the best option for biologists to try to predict sites in a set of sequences is never to rely on a single tool, but better to use a few complementary tools and combine the top predicted results of each one of them.

in line with this, we believe that the methods here described, despite their drawbacks, can perfectly be part of those tools that biologists use in combination with others to predict de novo binding sites in sets of biological sequences. especially in the case of the ctm method, there are still many improvements to be done. however, given the results, it can already be used as a ground for future tools based on the use of topic models as a reliable method for motif finding.

additional file

additional file 1: this table shows the tools that were studied in the original assessment by tompa et al.  <cit>  and the two methods presented in this study, each one with a short description of their underlying methodologies. 




abbreviations
aspaverage site performance

ctmcorrelated topic model

fnfalse negative

fpfalse positive

gagenetic algorithm

tfbstranscription factor binding site

tntrue negative

tptrue positive

