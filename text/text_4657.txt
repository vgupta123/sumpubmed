BACKGROUND
while ion mobility  spectrometry  is an established technology to detect volatile organic compounds  in the air or exhaled breath, the more recent combination with multi-capillary columns  has opened new applications in biotechnology and medicine, consider koczulla et al. <cit>  and armenta et al. <cit> . the analytes, metabolites present within exhaled breath, are pre-separated using the mcc, analogously to gas chromatography  before mass spectrometry .

vocs from the human metabolism in exhaled breath may hint at certain diseases. applications to diagnosis of lung cancer, chronic obstructive pulmonary disease  or sarcoidosis have already been reported  <cit> .

mcc/im measurements and peaks
a single measurement with an im spectrometer takes about  <dig> ms, using nitrogen or synthetic air as drift gas. for a sample pre-separated by an mcc, an im spectrum is captured periodically at several different retention times, e.g. each  <dig> ms for up to  <dig> minutes. the retention time r is the time a compound needs to pass the mcc.

the drift time t′ is the time a compound needs to drift through the im spectrometer and is influenced by parameters such as drift tube length, intensity of electric field as well as temperature or ambient pressure. figure  <dig> illustrates a it is thus advantageous to consider a normalized quantity: the reduced inverse mobility t in vs/cm <dig>  each im spectrum  provides a signal intensity  for each value of t.

we obtain a two-dimensional im spectrum-chromatogram  s:r×t→ℤ with retention times r∈r, inverse mobilities t∈t, and signal intensities s∈ℤ . in practice, we have equidistant points on both retention time and inverse mobility axes; therefore we may assume that r={ <dig> …,m} and t={ <dig> …,n}, where these index values correspond to actual times and reduced inverse mobilities.

we call a single row or spectrum s
r
 at a retention time r an im spectrum. a single column s·,t
 at a certain t is called im chromatogram. the whole matrix s is the imsc. regions of s with a high signal intensity are called peaks. an imsc can be visualized as a heat map . in every imsc, the reactant ion peak  produced by the ionisation of the drift gas is visible as a high-intensity chromatogram at a reduced inverse mobility of approximately t= <dig>  vs/cm <dig>  when additional analyte ions occur, the rip is reduced and may even disappear if the analyte concentrations are extremely high. we describe each peak with a set of  descriptors, which are its coordinates  and signal intensity s . additional parameters may describe the peak shape or alternative signal values. for typical analysis algorithms, a triple  suffices, where we use s as shorthand for any signal at or around , how ever computed.

the position and intensity of peaks indicates the presence and concentration of certain vocs. peaks behaving differentially  in two classes of measurements  may represent potential biomarkers that can hint at specific diseases.

the need for automated peak extraction: our contributions
the fundamental step of peak extraction from a raw imsc is the basis for all subsequent data mining classification steps  <cit> . given a set of measurements, a domain expert assisted by visualization software  is able to interactively pinpoint peak locations within a few minutes. an experienced expert can often distinguish weak signals from noise.

mcc/ims technology has matured to a point where it is applied to automated monitoring  <cit>  and moves towards a high-throughput domain. here, interactive expert-driven and computer-assisted peak extraction is no longer possible. to a lesser extent, the same situation holds true in exploratory medical studies, where the amount of available measurements increases beyond human analysis capabilities. therefore, automated peak extraction methods are urgently required. as another advantage, they offer better reproducibility and increased speed. however, they may make certain assumptions about the data and lack in adaptability.

we here provide a modular automated peak extraction framework. the task of peak extraction is divided into four steps that are performed in sequence. each step allows us to use different specific methods, implemented as separate modules. each  combination of modules, together with individual module parameters, specifies a concrete peak extraction pipeline and transforms an imsc into a list of peaks.

in section ‘methods’ we introduce the peak extraction framework, enumerating briefly all pipeline steps. the next three sections ’) explain the available modules for each step in detail. information on the software architecture and implementation is given in section ‘architecture and implementation’. results on a complete dataset are provided in section ‘results’, where we compare the different combinations with each other and to manual peak extraction. section ‘discussion and conclusion’ concludes.

methods
framework overview
we present a framework for automatic processing of an mcc/im measurement  to discover and quantify all present peaks. the peak extraction process is divided into four steps . each step can be implemented by different modules represented by the yellow boxes containing an abbreviation for each module name. each resulting pipeline requires a single imsc as input and outputs a list of peaks. each peak is represented at least by the following information: name of the measurement, an automatically given peak id, reduced inverse mobility, retention time, signal value, reduced inverse mobility index and retention time index. knowing the name of measurement for each peak is convenient when comparing several peak lists from different measurements. we now discuss the four distinct steps.

preprocessing transforms a  imsc into another  imsc, i.e., no data reduction or peak extraction takes place. raw imscs are noisy and include the confounding rip. to remove both noise and the rip, we describe three modules: baseline correction , de-noising  and smoothing ; every module’s input and output is an imsc. baseline correction  handles the rip , removes it, and uncovers underlying peaks. de-noising  estimates the probability of a data point belonging to noise in order to remove the noise. smoothing  applies a smoothing filter. the order of execution is commutable, but none of these modules can be omitted. figure  <dig> shows a measurement before and after preprocessing.

peak candidate detection finds a list of potential peaks within the preprocessed imsc. we implemented two alternative modules called local maxima  and cross finding . the input of either module is a processed imsc, and the output is a list of candidate peaks, which is further refined in the next step. local maxima finds local maxima within the two-dimensional imsc, while cross finding searches for zeros in the first partial derivatives with respect to both retention time and reduced inverse mobility.

peak picking examines the proposed candidates and generates the final list of extracted peaks. we have so far implemented three modules. all three methods calculate a representative peak for a set of peaks whose positions are too close to be considered distinct. merging by signal intensity  is a basic method considering the distance between two candidates; it picks highest signal peak from a set of close peaks within a surrounding box of given size. cluster editing  discovers peak clusters by solving the cluster editing problem and returns the peak with highest intensity of each cluster as representative. em clustering  works similarly, but discovers peak clusters using the em algorithm.

peak modeling is an optional final step that can be used to estimate additional peak parameters, describing the shape and position more precisely. a module called peak model estimation  has been implemented. in time-critical applications this step is generally omitted by using the “empty module” ε.

the peak modeling step can be exchanged with the peak picking step, meaning that each candidate peak is modeled, and picking is done on the modeled candidate peaks. in the following section we discuss all modules in detail and introduce several parameters, remark that user adjustable parameters are emphasized.

mixture models and expectation maximization
several of our modules use mixture modeling, i.e., the data is viewed as a realization of a mixture distribution

 f=∑c=1cωcfc, 

where c indexes the c different components, θ
c
 denotes all parameters of distribution f
c
, and θ= is the collection of all parameters. we allow that the distributions f
c
 are of different types, e.g., a uniform and a gaussian one. the mixture coefficients ω
c
 satisfy ω
c
≥ <dig> for all c, and ∑cωc= <dig> 

the goal of mixture model analysis is to estimate the mixture coefficients ω= and the individual model parameters θ
c
 . since this maximum likelihood problem is non-convex, iterative locally optimizing methods such as the expectation maximization  algorithm  <cit>  are frequently used. the em algorithm consists of two repeated steps: the e-step  estimates the expected membership of each data point x in each component and then ω, given the current model parameters θ. the m-step  estimates maximum likelihood parameters θ
c
 for each parametric component f
c
 individually, using the expected memberships as hidden variables that decouple the model. as the em algorithm converges towards a local optimum of the likelihood function, it is crucial to choose reasonable starting parameters for θ. for details, we refer to our previous work on peak modeling ), where we describe how to apply the em algorithm to a mixture of inverse gaussian distributions to infer peak shape parameters  <cit> .

modules for preprocessing
baseline correction 
intuitively and informally, a baseline spectrum b=
t∈t
 is defined such that b
t
 is a typical or insignificantly high value at reduced inverse mobility t when considering the whole measurement. formally, for each reduced inverse mobility t, we consider a histogram h
t
 with bin size  <dig> of the chromatogram s·,t
, i.e., h
t,i
 is the number of data points with intensity i in the chromatogram. bader  <cit>  presented a method that assumes a log-normal model as baseline and estimate its parameters before subtracting from spectrum. we developed a new method since baders method does not erase the whole rip. in a typical chromatogram , no rip), most intensities are at noise level, so the most prominent peak in the histogram indicates that level. in a rip chromatogram, the most prominent peak corresponds to the rip level . in both cases and in the intermediate ones as well, we model the most prominent peak of the histogram by a gaussian distribution and the remainder by the uniform distribution between lowest and highest observed intensity.

thus, we describe the histogram h
t
 by a heterogeneous two-component mixture model  and estimate its parameters  by the em algorithm, as outlined above. to start the em iteration, we set μ to the location of the maximum of h
t
 and σ2:= <dig>  while ωg is immediately estimated in the e-step. after convergence, having estimated μ and σ, we say that all intensities up to μ+2σ belong to the baseline and adjust the chromatogram as follows: sr,t′:=max{sr,t−,0} for all r∈r. after repeating this step for every t with individually estimated μ, σ, the baseline b
t
=μ+2σ has been removed.

de-noising 
the goal of de-noising is to subtract a substantial amount of noise from the imsc s by estimating whether the intensity s at coordinates  belongs to a peak region or can be explained by background noise. in previous work on de-noising bader  <cit>  uses a wavelet transform but applies it only spectrum wise. our novel approach is similar to the baseline correction  module in the sense that the em algorithm is used, but the model is more complex and the subtraction works differently.

the method is not applied to s directly, but to the locally averaged

 ar,t:=12·∑r′=r−ρr+ρ∑t′=t−ρt+ρsr′,t′, 

where ρ is the smoothing_radius parameter. the spectra in our measurements consist of  <dig> data points with a maximum reduced inverse mobility of t= <dig>  vs/cm <dig>  with respect to tolerance Δt:= <dig>  vs/cm <dig>  we obtain a tolerance of / <dig> ≈ <dig> index units. we chose ρ= <dig> to avoid taking noise into consideration for smoothing.

considering a histogram of all a-values also with bin size  <dig> , we identify three components: the noise component  is modeled as a gaussian distribution, the signal component  is modeled as an inverse gaussian distribution and a background component  is modeled as a uniform distribution over all measured intensities. this yields a three-component heterogeneous mixture model , whose parameters are again estimated by the em algorithm.

after convergence and a final e-step, we obtain the expected membership values w
r,t
  of each data point  in the noise component. we adjust the original imsc such that only the non-noise fraction remains, i.e., sr,t′:=sr,t· for all r∈r,t∈t.

smoothing 
the smoothing module consists of two consecutive methods. the first method is a lowpass filter. the imsc is transformed from the time/signal domain into the frequency/signal domain by a two-dimensional fast fourier transform . all frequencies above a given frequency threshold  are removed, i.e., set to zero intensity. the inverse transformation of the filtered matrix is done using the inverse two-dimensional fast fourier transform .

the result is smoothed by a savitzky-golay filter   <cit>  on local windows using smoothing_radiusρ= <dig>  around each data point. to handle the boundaries of the measurement, we expand the data matrix with a margin containing only zero values. since the data at the boundary of the measurement does not contain important data, this procedure is uncritical. the sgf computes a weighted average across the window.

modules for peak candidate detection
we discuss two modules to find peak candidates. both use parameter i  as signal intensity threshold.

local maxima 
this module reports a peak candidate for every local intensity maximum with intensity at least i in a surrounding area. to report a point , we require  that  is a local maximum in the sense the each of its eight direct neighbors has a lower or equal signal intensity than s
r,t
 but of at least i, and  that the contiguous area around  with signal intensity at least i is of sufficient size. in other words, we discard points where the surrounding high-intensity area size consists of too few points. the required number of points is controlled by a parameter a≥ <dig> .  and its eight neighbors always account for nine points; the parameter a can be used to impose stricter conditions.)

cross finding 
the basic idea of cross finding is to find maxima based on the ideas by fong et al. <cit> . to avoid overlooking peaks at the borders, the matrix’s borders are temporarily padded by zeros.

we construct two auxiliary matrices dr and dt, both with the same dimensions |r|×|t|. in dt, discrete derivatives of spectra are stored , dr,tt:=sr,t+1−sr,t; analogously derivatives of chromatograms are stored in dr. we describe how dt is analysed.

in each derived spectrum , we mark downward zero crossings; these are indices t with dr,t−1t≥ <dig> and dr,tt< <dig>  the resulting indices t are called active positions for retention time r.

while we scan through the spectra, we maintain two data structures. the first one is an active set containing lists of active positions connected across several spectra. the second one is a finalized set, where lists from the active sets are moved when they have been processed. initially both sets are empty.

we want to connect active positions between consecutive retention times, i.e., we want to find active positions for spectrum r+ <dig> corresponding to active positions in spectrum r ). to find the correspondences, we use a variant of global alignment between the two sorted integer lists a and a+ containing the active positions. the score of aligning a to a+ depends on the distance between a and a+. we use the following score function: score:=−1∈. to prevent that two positions with a high distance are aligned, we introduce a gap score γ= <dig> . as we will thus never align positions a and a+ with distance larger than  <dig> index units, we can solve the alignment problem very efficiently by only considering  with |a−a+|≤ <dig> 

three scenarios are possible between the aligned position pairs: 

 if a+ is not aligned to any a, it is a “new” active position, and a new list, containing only a+ is inserted into the active set.

 if a+ is aligned to some a, the corresponding list containing a is already in the active set and extended by a+.

 each a that is not aligned to any a+ finalizes its corresponding active list, and the list is moved into the finalized set.

after processing all spectra and finalizing each remaining list, we obtain several position lists pointing out consecutive maxima throughout each spectrum; see figure  <dig> 

the same procedure is analogously performed with matrix dr. we report the intersection of positions found from both matrices . if more than one position overlap is found between two lists, the position with the highest signal is reported. each reported point whose signal exceeds i is a candidate for a peak location.

modules for peak picking
the previous step, peak candidate detection, considers each potential peak location separately. two peak candidates may be called close to each other, e.g., by detecting two local maxima of the same underlying peak that arise because of noise in the data.

thus, not every peak candidate corresponds to a voc from the breath sample, and the purpose of peak picking is to thin out the candidate list.

bödeker  <cit>  introduced a minimum distance in retention time and in reduced inverse mobility such that two peaks exceeding those distances belong to distinct compounds. we write Δt for the necessary distance in reduced inverse mobility and Δr for that in retention time. we use a constant Δt:= <dig>  vs/cm <dig>  for the reduced inverse mobility. in retention time we use an affine-linear Δr:=p·r+c for a peak at position , where c:= <dig> s  and p:= <dig>  , as suggested by hauschild et al. <cit> . we now describe three modules for peak picking.

merging by signal intensity 
we sort the n peak candidates by descending signal intensity into a list 
i= <dig> …,n
 with s1≥s2≥⋯≥s
n
, resolving ties arbitrarily. first we mark each candidate as unmerged. iterating the list, we skip merged candidates and report each unmerged candidate we encounter. when this happens for candidate , we find which peaks fall into the box × and mark them as merged, and continue iterating through the list. in this way, we greedily pick peaks with highest signal as representative for all peaks in the surrounding box. this method  <cit>  was used by hauschild et al. <cit> .

cluster editing 
we find clusters of peaks, from which we pick a representative , by solving an instance of the weighted cluster editing problem  <cit> : let g= be a weighted, undirected graph without loops with a symmetric similarity weight function w:v2→ℝ, such that e={{u,v}:w≥0}. the graph can be modified by adding a non-existing or removing an existing edge {u,v}, which incurs a cost of |w|. the costs for several modifications are added to yield the total cost. the objective is to find a set of edge modifications with minimum cost such that the resulting graph consists of disjoints cliques .

every candidate peak is a vertex u=. the similarity w between two vertices u,v depends on their distances on the r- and t-axis. we use the distance measure

 d2:=12tu−tvΔt2+ru−rvΔr <dig> 

and the similarity weight function 

 w:=2b)−1ifd2≤ <dig> −d2otherwise. 

the range for w is therefore [ −∞, 2
b
−1]. if the distance between the two candidates u and v is zero, then the edge  has the maximum weight 2
b
− <dig>  if the t- and r-distances of u and v are equal to Δt and Δr, respectively, then d2= <dig> and w= <dig>  for larger distances, the weights are negative. parameter b called ce_weight_exponent can be set by the user.

the weighted cluster editing problem is solved with the yoshiko  <dig>  software .

em clustering 
this module uses the em algorithm once again. initially, each peak candidate represents a component. during the course of the algorithm, components can be merged. the remaining components will represent the picked peaks.

each component is a two-dimensional gaussian distribution with independent dimensions, i.e., diagonal covariance matrix. initially, the mean of every component is the  location of the corresponding peak candidate. the standard deviation on the r- and t-axis is set to Δr/ <dig> and Δt/ <dig>  respectively, since 6σ covers most of a gaussian bell curve. in the e-step, the hidden membership coefficients of each peak to each component are estimated. when a peak candidate is close to another one, the probability that the first model also  describes the second candidate is comparatively high. in the maximization phase, the parameters of each component are re-estimated based on candidate membership using maximum likelihood estimators. in the case of two close candidates, the mean of both components moves towards their middle. when the distance between the means of two components drops below a given threshold, the components are merged: the component of the candidate with lower signal is removed, and its weight is added to the remaining model. the e- and m-steps are repeated until convergence.

when updating the variance by maximum likelihood estimation, we must be aware that the variance of a component described by only one peak tends to zero, which leads to a singularity in the gaussian density function. therefore, we restrict the estimated standard deviation to values above the threshold τ:=10− <dig> vs/cm <dig> 

a module for peak modeling 
peak modeling is an optional step that estimates a parametric model of a peak shape. we have so far implemented one module ) using shifted inverse gaussian distributions, consider  <cit> . if it is not desired to model the peaks, the empty ε module  can be used instead. it outputs the peak list without any modification.

a whole imsc is interpreted as a sample from a mixture model of different shifted inverse gaussians plus a uniform background noise model. each component  can then be described by seven parameters . the challenge is to estimate the parameters correctly, especially when peaks overlap. again, the em algorithm is utilized for this purpose.

for efficiency, each component model is evaluated only in a surrounding box enclosing the peak. starting from the picked peak location, the box borders are expanded in all four main directions until the signal intensity drops to zero in each direction. the parameter expansion_size determines how much the box around the peak is expanded additionally.

when two boxes intersect, both boxes are merged into their convex hull. after that process we have a set of boxes containing at least one peak. now we can apply em to each box independently, with the advantage of processing smaller boxes in contrast to the whole signal matrix. starting parameters for each component are estimated from the locations of picked peaks and additional assumptions: the parameters are chosen such that their modes correspond to the known  values, the mean is set slightly higher , and the standard deviation is set to  <dig> index unit in both dimensions. as the model parameters have a rather technical interpretation, they are translated back into mode, mean and standard deviation of the distribution, which are conveniently compared and interpreted.

architecture and implementation
the framework consists of a number of classes representing input, output and parameters and, importantly, four function interfaces, one for each major step of a pipeline. the steps have unified interfaces to guarantee the modularity of the framework and the exchangeability of the modules with future ones.

input
the standardized .csv format for mcc/ims measurements is described by vautz et al. <cit> . a more efficient binary format  has also been developed internally. an abstract class imsfile provides the interface for loading and storing those formats, and the classes imsfilecsv and imsfileims instantiate the interface for the respective format.

the class imsmeasurement stores a sequence of retention time points r, a sequence of drift time points and  reduced inverse mobilities t. additionally, a measurement_parameters map stores all meta information e.g. date, time, name or various sample information of a measurement. it also stores an imsmatrix that represents an imsc , i.e., it contains the raw intensity values as a matrix.

output
the class imspeak describes a single peak. it stores the name of the originating measurement, its peak name , retention time and reduced inverse mobility of the peaks mode, the signal intensity and the volume . the indices of both retention time and reduced inverse mobility are also stored. a map peak_parameters may store additional parameters, e.g., parameters estimated for inverse gaussian distributions, as described in section ‘a module for peak modeling ’.

the class imspeaklist stores the resulting list of such peaks found by the candidate detection, picking and modeling steps. every imspeaklist contains a list named parameter_names, which stores the names of additional parameters for every peak. these are the keys for the above mentioned peak_parameters map.

the output format is a .csv file with one line per peak containing the peak’s measurement name, peak name, retention time, reduced inverse mobility, signal, volume, retention time index, reduced inverse mobility index and additional parameters.

module parameters
a map called parameter_map stores all peak extraction parameters .

all computations were repeated for three different values of intensity_threshold.

function interfaces
using unified function interfaces for each step ensures the modularity of the pipeline. a preprocessing function takes an imsmeasurement and a parameter_map manipulating the provided matrix.

a candidate detection method requires an imsmeasurement and a parameter_map and returns an imspeaklist. the picking functions take those results as input and return an imspeaklist that contain a subset of the input list. finally the modeling step requires again the imsmeasurement in addition to an imspeaklist and the parameter_map. this step returns an imspeaklist of the same size as the input one. to augment a particular step with a new module, these interfaces must be used.

RESULTS
dataset
we tested the framework on a dataset of  <dig> measurements, of which  <dig> are from different patients suffering from the same disease and  <dig> from a control group not showing corresponding symptoms. the disease is known but irrelevant with respect to this article and cannot be disclosed due to confidentiality agreements within the clinical study approved by the state ethics committee. our dataset has been anonymized and serves as an illustration of the framework. for every of the  <dig> measurements, a manually annotated peak list was provided by an expert annotator.

evaluation of pipelines
by combining the implemented modules, we obtain  <dig> individual pipelines. we name the pipelines by concatenating the shortcuts of the used modules in order. for example, the pipeline using  the modules smoothing, de-noising, baseline correction, local maxima, em clustering and no modeling is named s-dn-bc-lm-emc-e. there are not  <dig> pipelines because of redundancy between pipelines using the empty module as fourth step and those using it as third one . we used the parameters shown in table  <dig> 

to evaluate each pipeline, we compare the final obtained peak list with one that was manually annotated by an expert mcc/ims development engineer. for the comparison, we considered only peaks with a retention time above  <dig> s and an inverse reduced mobility above  <dig>  vs/cm <dig>  as is standard practice.

agreement of an automatically obtained peak list with that obtained by a domain expert is generally considered favourable. however, one should be aware that manually annotated list may also be incomplete or contain extraneous peaks. nevertheless considering the manually annotated peaks ground truth, we compute the following quantities. peaks detected by both methods, manual and automatic, within one measurement are true positives .  accordingly, manually annotated peaks that are not detected by the pipeline are false negatives  and automatically detected peaks not found in the manual annotation are false positives . we compute the sensitivity sens:=tp/ and the positive predictive value ppv:=tp/. their geometric mean g:=sens·ppv summarizes both measures, which is referred to as fowlkes-mallows index  <cit> . further, the jaccard index between two peak lists is j:= tp /  ∈. from this, we derive a distance measure d:=1/j−1∈. the distance and geometric mean are calculated separately for each combination of pipeline and measurement. to determine these quantities for a particular pipeline, we average over all measurements of the dataset. it remains to define what it means that “the same” peak has been detected by both methods, since the location parameters  may differ slightly. all peak picking modules can be used for this decision, and we chose “merging by signal intensity” . imagine a box around every manually annotated peak  of widths 2Δr and 2Δt, respectively. then we successively count each box containing at least one automatically found peak and delete it. in case of two or more peaks within the box we count the closest one.

ranking the pipelinesi = 5


g
d


g
d


g
d
we find that, for the candidate detection step, almost all pareto-optimal pipelines use cross finding . the picking step is best done by cluster editing . for every signal intensity threshold i, the pipelines split into two groups. the first group has both relatively low sensitivity and low positive predictive value, while the second one has high values for both measures. we note that the pipelines of the first group are characterized by the utilization of the pme module . by modeling, the peak coordinate moves slightly, yielding larger average differences to the manual annotation based on grid coordinates. so module pme seems to be unnecessary. however, the volume of a peak may contain important information , and we cannot infer it only from the position and intensity at those coordinates alone. concerning the threshold i, the choice of  <dig> yields the best results. for both i= <dig> and  <dig>  the best pipeline is dn-s-bc-cf-ce. the top values in table  <dig> range around  <dig>  for the geometric mean. we note that individual measurement properties  were not taken into consideration for choosing the module parameters. an additional step of parameter estimation from global measurement properties would likely improve the results.

discussion and 
CONCLUSIONS
we presented the first framework for fully automatic processing of mcc/im measurements, consisting of different modules for four distinct computational steps. the presented framework processes a single im measurement and outputs a peak list within a few seconds. a domain expert, who before had the time consuming task of peak annotation, may now quickly verify the resulting automatically generated peak list and manually reject a few potential false positives. in practice, higher sensitivity of the automated pipeline  may be desirable, as this type of error can be better compensated by statistical learning methods during classification than a false negative rate.

the best pipeline achieves a geometric mean of sensitivity and positive predictive value of  <dig>  when compared to a manual expert manual annotation, without tuning the parameters for the single modules. since the manually extracted peaks are annotated by one single expert, one cannot be certain whether that solution is fully correct; some of the peaks evaluated as false positives in our pipelines might in fact be false negatives of the expert, but this is difficult to quantify, except on a case-by-case basis.

hauschild et al. <cit>  discovered that the hand picking method by domain experts yields the best results for classification compared to automatic peak picking methods . thus it is reasonable to compete with a domain expert.

the state of the art today is that every expert has his own “manual procedure” for peak extraction, based on certain human-observable features of the visualized data matrix. we observed two experts and attempted to infer their “internal algorithm” and express this knowledge as our parameters. the most significant parameter values were determined by domain experts’ experiences, i.e. the tolerances used for the picking step. the intensity threshold from the candidate detection step was tested empirically. we reported on three different values . for the other parameters we used values selected by our own experience. these last parameters do not influence the results as much as the previously mentioned ones.

future work
our future work will consist in the effort to estimate as many of the algorithms’ parameters as possible from the given data matrix. the tolerance parameters of the picking step were so far determined by domain experts’ experiences, and it will be difficult to learn them purely from the data. for some other parameters it may be possible, e.g. for the signal intensity by determining the noise level of the background noise. also, for the area size of the peak candidate detection step and the expansion size for the peak modeling we see a possibility to automatically determine the values.

the framework with the modules described in this article are available at http://www.rahmannlab.de/research/ims, as well as the anonymized datasets and additional file 1: supplement.

competing interests
we declare that we have no competing financial interests. we do declare that jib is chairman of a company, b & s analytik gmbh that builds and sells ion mobility spectrometers. however, our algorithms are general-purpose and not restricted to their instruments.

authors’ contributions
mda and dk developed and implemented the framework including every presented module. mda, dk and sr drafted the manuscript and jibb provided the data for evaluation. all authors read and approved the final manuscript.

supplementary material
additional file 1
supplement.

click here for file

 acknowledgements
mda, dk, jibb, sr are supported by the collaborative research center   <dig> “providing information by resource-constrained data analysis” within project tb <dig> . we thank benjamin yip and henning funke for preliminary work.
