BACKGROUND
the location  of a protein within cells is an important attribute that can be largely independent of its structure, enzymatic activity, or level of expression. systematic and comprehensive analysis of subcellular location is therefore needed as part of systems biology efforts to understand the behavior of all expressed proteins. work in this area can be divided into experimental determination and computational prediction. of course, the accuracy and utility of prediction methods is dependent on the accuracy, coverage and resolution of determination methods. this is because experimentally determined locations are the starting point for the machine learning methods at the heart of prediction systems  <cit> . subcellular location is most frequently determined by visual interpretation of fluorescence microscope images, but such interpretations can be highly variable from observer to observer. we have therefore developed automated systems to recognize major subcellular patterns  <cit>  and to learn new patterns directly from fluorescence microscope images  <cit> . these systems utilize high resolution images and have been shown to be able to distinguish similar patterns better than visual examination  <cit> .

automated interpretation of subcellular patterns in micrographs
the automated location determination systems consist of machine classifiers  and sets of informative numerical features  to describe protein distributions in the cell. this process is illustrated in figure 1a. the starting point is the collection of many images of two  different protein patterns. regions containing single cells are identified either automatically or manually, and background fluorescence is subtracted. a number of different types of slf are then calculated for each cell, including morphological features that describe the number, distribution, size and shape of fluorescent objects in the cell and texture features that describe the pixel-to-pixel variation in intensity. a feature matrix is then created in which each row shows the values of each feature for a given cell along with the type  of the protein that was labeled in that cell. this matrix is used to train a classifier so that it can learn a mapping between the slf and the classes. for each new  cell, the process of segmentation, background subtraction, and feature calculation is repeated, and the feature vector is supplied to the classifier to assign the cell to one of the known classes.

using large collections of hela cell images containing ten distinct subcellular patterns, our systems have achieved classification accuracies as high as 92% and 98% for 2d and 3d single cell images, respectively  <cit> . the patterns of dissimilar classes can be distinguished quite well; however, there is still room to improve the classification accuracy for similar classes .

in order to improve the classification accuracy, one strategy is to incorporate additional or improved features and another is to combine more than one classifier using voting methods. the performance improvements we have obtained for 2d hela images, from 83% using a library of  <dig> features and a neural network classifier  <cit>  to 92% using a library of  <dig> features and a majority-voting ensemble  <cit> , resulted from implementing both of these strategies. a majority-voting ensemble combines the results from many different classifiers into a single decision, as illustrated in figure 1b.

these improvements were obtained while considering the classification of patterns in single cells. an additional strategy is to utilize information from more than one cell from the same sample. for example, when sets of hela cells from the same slide were individually classified and allowed to vote for a single classification for the entire set, overall accuracy improved from 83% to 98%  <cit> . the penalty for this improvement is that we give up the ability to identify more than one pattern in a given set. a possible improvement on this approach is therefore to first estimate the number of classes that are present from the frequencies of the classes , and then assign each cell to one of the remaining classes.  so that we can decide which classes to rule out, we assume that the "true" classes are present in roughly equal proportions. in this paper, we first evaluate this simple strategy. we then describe more sophisticated approaches that construct a graphical model representing pattern information for more than one cell in a field so that improved classification accuracy can be achieved while retaining the ability to classify each cell individually .

graphical models
graphical models have been extensively applied to problems in the computer vision field, such as image segmentation and object recognition, where the pixels in an image can be segmented or classified into two  or more classes  <cit> . many classification problems where the labels of related objects must be consistent with each other, such as hypertext classification  <cit>  and identification of protein functions in the protein-protein interaction network  <cit> , can also utilize graph-based methods. to our knowledge, graphical models have not previously been applied to the recognition of subcellular patterns in multi-cell images. large numbers of such images are increasingly being acquired both in projects aimed at determining the subcellular location of all proteins  <cit>  and in drug screening by high-throughput microscopy  <cit> . part of the motivation behind the work we describe here is the need to classify fields of cultured cells that may be expressing different tagged proteins . an additional motivation is the desire to classify individual cell patterns in tissues that may consist of more than one cell type.

the problem to be solved using a graphical model is to infer the posterior probability of each class for each node  using information about the likely classes of other nodes . for some graphical models, an exact solution can be found using the belief propagation  algorithm  <cit> . however, bp can only calculate the posterior probability correctly on trees or forests, that is, on graphs where there is at most one path between any two nodes. if there are loops in the graph, the junction tree algorithm  <cit>  can be used to convert a loopy graph into a tree by clustering nodes together. exact inference can then be achieved by applying bp on the converted tree, but the running time is exponential in the size of the largest cluster in the converted graph. we therefore need approximate inference methods for cases where the size of the largest cluster is large. a commonly used approximate method is loopy belief propagation , which iteratively applies belief propagation updates on a graph with loops. lbp often gives good approximate inference when it converges  <cit> , and often runs very quickly, but can fail to converge on some graphs. other approximate inference algorithms, such as variational methods  <cit>  and monte carlo methods  <cit> , are also widely used. running times for these approximate inference methods can be prohibitive for large graphs.

a graphical model consists of an algorithm for constructing the graph itself and an algorithm for making inferences given the graph. in this paper we describe how to construct graphs for the problem of subcellular location classification, and also present a novel algorithm, which we term prior updating, that permits inferences to be made for the  resulting graphs.

RESULTS
problem statement: at the outset, we formalize our problem by describing our assumptions about the process used to create cell images. we assume that the process of creating a slide  for imaging starts by creating a mixture of any number of cells from each of many possible classes. we further assume that cells are randomly distributed over the slide at some time tplate before imaging, that the cells divide with an average generation time of tg, and that the class of a cell is stably inherited by its daughters . lastly, we assume that we have accurate methods for segmenting multi-cell images into regions containing single cells, and classification methods that provide a likelihood for each possible class for each segmented cell. the task is: given an image of a field containing a number of cells meeting the assumptions above, assign a class to each cell as accurately as possible.

equal-sized class model
as discussed above, performance of a single cell classifier on a multi-cell image can be improved if the assumption can be made that all cells in the field should show the same pattern. this can be done by assigning the most frequent class in the image to all cells  <cit> . while this assumption may be true in some cases, it is quite restrictive. the goal of the work in this paper is to improve the analysis of multi-cell images without the drastic assumption of homogeneity. we begin by considering a variation on this assumption, namely that each multi-cell image is composed of a small number of classes with roughly equal numbers of cells. in this case, one strategy is to decide upon the number of classes using a threshold on the observed frequencies of each class. we define tn = 1/ + β, where n is the number of classes and β is an adjustable parameter that ranges from - <dig>  to  <dig> . to find the number of classes, we find the smallest n for which the frequencies of exactly n classes are greater than tn and record which classes those are. this definition is based on the assumption that the true classes are present in roughly equal proportion, and hence that the percentage of each should be greater than the expected frequency of a class if one more true class was present . we consider an example to illustrate the approach. using β =  <dig>  results in t <dig> =  <dig>  and t <dig> =  <dig> . given a field with three classes with frequencies , we would choose n =  <dig>  however, if the frequencies were , n =  <dig> would be chosen. once n is chosen, each cell in the trial field is assigned to the one of those classes that has the largest likelihood for that cell . note that this might not be the class with the highest likelihood if that class was not retained during the selection of the number of classes. if no n meets the criterion, we simply keep the classification results from the single cell classifier. note that as β decreases to - <dig> , we increasingly favor finding only one class, and as β approaches  <dig>  we increasingly favor making no changes to the original class assignments.

evaluation scheme
to illustrate and test approaches to multi-cell classification, we need multi-cell images in which the class of each cell is known with certainty. since it is nearly impossible to collect such images , we have simulated them by combining images from a large library of single cell images . the library contains images of ten subcellular pattern classes, and to classify individual cells we have used a multi-class support vector machine classifier whose outputs were converted to probabilities for each class.

for the first tests, we created synthetic images consisting of  <dig> cells randomly drawn from only two classes such that the number of images in one class varied from  <dig> to  <dig>  average accuracies over  <dig> repeated trials were determined for the  single-cell classifier and for the equal-size class model described above. this process was repeated for all possible pairs of classes and for different mixtures of images from the two classes, and the average classification accuracy across all of these conditions was determined for various values of β. figure 2a compares the overall classification accuracy across all mixtures between the base classifier and the equal-sized class model. the best average accuracy  is obtained for β = - <dig> . figure 2b compares the classification accuracy for β = - <dig>  between the base classifier and the equal-sized class model as a function of n <dig>  the number of cells in one of the two classes. the classification accuracy is only better than that of the base classifier for the set consisting of only one class, but in all other cases the classification accuracies are either lower or equal. the results also indicate that cases of different mixtures need different optimal β s to achieve the best accuracy improvement . for example, when n <dig> =  <dig>  the accuracy can be improved up to  <dig> % over the base classifier for β = - <dig> , but the average accuracy across all mixtures is much worse . the best accuracy improvements for cases with n <dig> =  are  with β = . however, for cases with n <dig> = , no matter how the β is tuned, the best possible average accuracy can only be the accuracy from the base classifier. this is expected since the assumption used to derive the method was that whatever classes are present are approximately equal in frequency. all these results suggest that the equal-size method should not be used when the mixture of classes is unknown.

construction of graphical models
we next consider what information may be available about the likely class of a cell given information about its neighbors in the field, and how we can construct a graphical model to use that information. two limit cases can be considered. these limits are based on the relative magnitudes of the constants tplate and tg defined in the problem statement above.

feature space model
the first possibility is that tplate is short relative to tg such that cells would not have time to undergo significant cell division prior to their being imaged. in this case, the proximity of cells does not provide any information about their likely similarity . the only clues that we have about the number of classes present  are the similarities between cells in the slf feature space. in this case, we initially construct an undirected graph in which each cell is represented by a node and edges are created between each pair of nodes with length equal to the z-scored euclidean distance between the feature vectors of the corresponding cells.

physical space model
if, however, the amount of time that elapses between plating and imaging is significantly greater than the generation time , each original cell is expected to have divided a number of times and we may consider it likely that the class of cells adjacent to one another is the same. the rate  at which daughter cells move away from each other relative to the rate at which they divide becomes the determining factor. thus, if vtrans is high, we may consider physical proximity to be of little predictive value and are forced to use the feature space model described above. if, on the other hand, vtrans is low, we can construct an undirected graph using the euclidean distance between the centers of cells in the field.

pruning
initially, the graphs for both model types are fully connected. each edge suggests the two nodes it connects should influence each other's labels. since we can assume that they should not influence each other if the distance between them is too large , edges whose length is greater than a free parameter dcutoff are removed. note that the units of dcutoff are different for the two types of models.

inference by prior updating
given a graphical model of either of these types, the task becomes inferring the class labels. this requires an algorithm to describe how the label at each node is influenced by information from each of its nodes. as described in the introduction, exact and even approximate inference methods can be extremely compute intensive for models with many connected nodes. since our goal is to apply graphical models to fields with many cells, we need an efficient method for inferring the most likely class for each cell given the results of the single cell classifier for it and its neighbors. we therefore developed a new method, which we term prior updating , that we believed could give improved classification accuracy in realistic compute times. the principle behind the method is simple: we allow each cell to have its own set of prior probabilities for all possible classes and adjust them to reflect the likely classes of its neighbors. we start by setting all prior probabilities equal, and then determine the posterior probability of each class for each cell using the output of the svm classifier and bayes rule. we then iteratively adjust the prior probabilities of all classes for each cell based on the labels of its neighbors and recalculate the posterior probabilities . a free parameter α controls the extent to which the prior probabilities are adjusted at each iteration . the method terminates when no class labels change during an iteration. each cell is allowed to change its label at most once, and its confidence is set to zero after the label changes. we designed this strategy because cells whose labels are easily changed are expected to have high uncertainty, and should not influence other cells after their labels change. this strategy also guarantees that the iteration will converge in constant time. similar results are obtained if priors for each node are initialized outside the loop and if labels are allowed to change more than once .

feature space model
to evaluate the accuracy of the graphical models and the prior updating method, we used synthetic multi-cell images as described above. we first consider the feature space model, which is directly comparable to the equal-size scheme since neither considers physical position of a cell. we calculated classification accuracy for various values of the two free model parameters: α and dcutoff. figure  <dig> shows results for fields of  <dig> cells each for two classes for the best dcutoff for each of various values of α. the best results were obtained with α =  <dig>  and dcutoff =  <dig>  we evaluated three metrics: overall accuracy , average accuracy for similar classes , and accuracy for dissimilar classes . compared with the results for the base classifier , the accuracy of similar classes is much improved , and the accuracy of dissimilar classes is also improved . the overall accuracy is improved by over  <dig> percentage points . the overall accuracy of  <dig> % obtained with an svm classifier combined with pu is higher than the best previous accuracy for the 2d hela collection of  <dig> %, which was obtained using a much more complicated majority-voting classifier  <cit> .

when α is zero, the priors are not updated so that cells do not influence each other. as α increases, the priors of classes that are present in the field are increased while others are decreased. as seen in figure  <dig>  classification accuracy also increases as α increases but roughly plateaus at α near  <dig> . the results suggest that a large α usually gives good improvement in classification accuracy; however, the best α has to be found by applying cross-validation methods.

the dcutoff parameter is designed to determine the neighbors of a cell. if dcutoff is very small, the cell does not have any neighbors to influence and be influenced by. as dcutoff gets larger, the cells start to be influenced by other similar cells, and so the classification accuracy can be improved. if dcutoff is set to infinity, all the cells are connected to each other in the graph and so contribute to the updates of each other's priors. in this case, some dissimilar cells will affect each other's priors and the classification accuracy could be worse than when the best dcutoff is used. the best dcutoff can be found by applying cross-validation methods.

encouraged by these results, we evaluated trial fields with two classes of varying numbers of cells in the feature space field . for the n <dig> =  <dig> case, where there is only one class of cells present in the field, the best dcutoff and α are both infinite, so that all the cells can be classified into one class just as the equal-sized class scheme does. the best dcutoff is  <dig> for all other cases. this implies that the z-score distances among similar cells of 2d hela images in the slf <dig> feature space are on average less then  <dig>  no matter how many cells the classes are composed of. the best α ranged from  <dig>  to  <dig>  for different cases . the results in figure  <dig> were obtained with α set to  <dig> , and this value was used for all subsequent experiments. as the sizes of the two classes become more asymmetric , the accuracy improvement of similar classes still remains in the range of  <dig> to  <dig> percentage points, while the accuracy improvement of dissimilar classes decreases from  <dig> to  <dig> percentage points. this is because smaller numbers of "minority" classes affect the estimated priors to a lesser degree, and a small change in priors is more likely to affect the labels of similar classes than of dissimilar ones. for the n <dig> =  <dig> and n <dig> =  <dig> case, the accuracy of similar classes are higher than for the other cases, which confirms that it is easier to determine which of similar classes a cell is more likely to be when the cells are more homogeneous in the field.

physical space model
we also evaluated a model in which the physical positions of cells in the field are used to influence classification. synthetic fields of cells were created by simulating the processes of cell division and movement for clones derived from two cells of different classes initially at a distance d from each other. figure  <dig> shows results for applying the graphical models on fields generated with various values of d. when d =  <dig>  the two clones overlap in space but in most cases, the accuracies for similar and dissimilar classes are still improved over the base classifier. this is expected, since this case is very similar to the feature space model evaluated above. the classification accuracy improves as the separation of the two clones increases , also as may be expected. the results demonstrate the important conclusion that our graphical models can result in significant improvement in classification accuracy for the task of classifying a mixed population of cells under a variety of test conditions.

multiple classes test
given that our method performed well for multiple cells from two different classes, we next examined the cases where more than two classes of cells were present. we therefore performed experiments using the feature space model on one to five classes with each class having six cells. the results are shown in table  <dig>  as the number of classes increases, the overall accuracy decreases from around  <dig> percentage points above to around  <dig> percentage points below that of the base classifier. since it is more likely that there are cells from both of two similar classes in the field as the number of classes increases, this is expected. the observation that the transition from improvement to degradation occurs after  <dig> out of  <dig> classes are present loosely suggest that the maximum number of classes that can be simultaneously present in a field and still see improvement from a graphical model is around 40% of the number of possible classes.

the accuracy of classification using feature space graphical models were evaluated for all possible mixtures composed of various numbers of classes drawn from ten classes. results shown are averages over  <dig> trials.

effect of training set size
we also examined the effect of training set size on the prior updating scheme as a way of examining the improvement possible for a less accurate base classifier. various training set sizes were used to train base classifier svms and then these were applied to fields of two classes of equal sizes. the results in table  <dig> show that the base svm classifier decreases its accuracy with fewer training data , but that the prior updating scheme can still improve its accuracy by between  <dig> and  <dig> percentage points. the smaller the amount of the training data, the more the prior updating method can improve and compensate the accuracy. it is especially impressive that the combination of the prior updating scheme and the svm classifier with only  <dig> training data per class can do a similar job to the svm classifier alone with  <dig> training data per class. the results also indicate that at least for this subcellular pattern classification task, the svm classifier joined with the prior updating scheme does not need a lot of training data in order to attain a fair classification performance.

base svm classifiers were trained using various numbers of cells for each of ten classes. graphical models using those base classifiers were then tested on fields containing six cells each from two classes and results were averaged over  <dig> trials each for all possible pairs.

discussion
our work has particular implications for classification of patterns in images obtained by high-throughput microscopy. since high-throughput systems typically use low magnification, the number of cells per field is often high and the accuracy of single-cell classifiers is usually not perfect. by applying this method on multi-cell images made of real single cells and synthesized locations, we are able to verify that our scheme can be used for such systems to achieve significantly better performance.

since we have proposed a new approximate inference algorithm, it is important to identify when this method works better than other approximate inference methods. this method is very fast compared to previously described graphical model algorithms: its runtime is linearly proportional to the number of cells in each trial field and to the number of classes it needs to choose from. whether this method has better classification performance under different circumstances will be examined in future work. we anticipate that the method can be made more general so that it can be used for other applications, both for biomedical applications like classification of cell types in tissue images and for other applications like internet link analysis.

CONCLUSIONS
this paper addresses a supervised learning problem in the domain of protein subcellular location determination. we have proposed a novel graphical representation where multiple cells in a field influence each other. assuming that these cells are only composed of a small number of classes, the classification accuracies are improved by manipulating the prior distributions of classes. the improvement is largest for groups of classes which would be difficult for the base classifier to distinguish from one another.

we have also shown the robustness of our prior updating scheme. the accuracies for different classes were always improved under different assumptions about the distribution of cells in the field, different sizes of the two classes of cells present in the field, different numbers of classes, and different training set sizes.

the results are very encouraging since the prior updating method improves the overall accuracy from the base classifier by around  <dig> percentage points and the accuracy of similar classes by around  <dig> percentage points. the combination of the prior updating method and the base single cell classifier outperforms the majority voting classifier that with an accuracy of  <dig> % had the best prior reported performance on this dataset  <cit> .

