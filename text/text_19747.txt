BACKGROUND
computational approaches to problem solving need to interleave information access and algorithm execution in a problem-specific workflow. in complex domains like molecular biosciences, workflows usually involve iterative steps of querying, analysis and optimization. bioinformatic experiments are often workflows; they link analytical methods that typically accept an input file, compute a result, and present an output file. most tool-driven integration approaches have so far addressed the problem of providing a single gui for a set of analytical methods. combining methods into a flexible framework is usually not considered. analytical workflows provide a path to discover information beyond the capacities of simple query statements, but are much less easy to implement within a common environment.

workflow management systems  are basically systems that control the sequence of activities in a given process  <cit> . in molecular bioscience, these activities can be divided among those that address query formulation, and those that focus more on analysis. at this abstract level, wfms could serve to control the execution of both query and analytical procedures. all of these procedures involve the execution of activities, some of them manual, some automatic. dependency relationships among them can be complex, making the synchronization of their execution a difficult problem.

one dimension of the complexity of workflows in molecular biosciences is given by the various transformations performed on the data. syntactic  interoperability establishes the possibility for data to be piped from one method into another. semantic issues  arise from the fact that we need to separate domain knowledge from operational knowledge. we should be able to describe a task of configuring a workflow from its primary components according to a required specification, and implement a program that realizes this configuration independently of the workflow and components themselves.

biologists provide rich descriptions of their experiments  so they can be easily replicated. once techniques have been standardized, usually this knowledge is encapsulated in the form of an analytical protocol. with in silico experiments as well, analytical protocols make it possible for experiments to be replicated and shared, and  for the knowledge behind these workflows to be captured. these protocols should be reproducible, ontology-driven, curated internally, and annotated externally.

systems such as w2h/w3h  <cit>  and pise  <cit>  provide some tools that allow methods to be combined. w3h is a task framework that allows the methods available under w2h  <cit>  to be integrated; however, those tasks have to be hard-coded. in the case of pise, the user can either define a macro using bioperl , or use the interface provided and register the resulting macro. in either case, it is assumed that the user can program, or script in perl. macros cannot be exchanged between pise and w2h, although these two systems provide guis for more or less the same set of methods . indeed, macros cannot be easily shared even among pise users. biopipe , on the other hand, provides integration for some analytical tools using bioperl api  using mysql to store results as well as the workflow definition; in this way, users are able to store results in mysql and monitor the execution of the pre-defined workflow.

the taverna  project provides similar capabilities to those offered by gpipe. however, on one hand inclusion of new analytical methods is not currently possible since no gui generator is provided, and on the other hand as taverna is part of mygrid  <cit>  it follows a different integrative approach . pegasys  <cit>  is a similar approach, going beyond analytical requirements and providing database capacities.

gpipe provides a real capacity for users to define and share complete analytical workflows , substantially mitigating the syntactic complexity that this process involves. our approach addresses overall collaborative issues as well as the physical integration of tools. gpipe provides an implementation that builds on a flexible syntactic structure and a set of algebraic operations for analytical workflows. for testing purposes we provide a simple example of a workflow  that involves piping among three methods. although here their execution takes place on a common server, it is equally possible to distribute the process over a grid using gpipe.

RESULTS
our workflow follows a task-flow model; in bioinformatics, tasks can be understood as analytical methods. if workflow models are represented as a directed acyclic graph , analytical methods then appear as nodes, and state information is represented as conditions attached to the edges. our syntactic structure and algebraic operators can be used to represent a large number of analytical workflows in bioinformatics; surprisingly, there are no other algebraic operators reported in the literature capable of symbolizing the different operations required for analytical workflows in bioinformatics . different groups have developed a great diversity of guis for emboss and gcg, but a meta-analysis of the processes within which these analytical implementations are immersed is not yet fully available. some of the existing guis have been developed to make use of grammatical descriptions of the analytical methods, but there exists no standard meta-data framework for gui and workflow representation in bioinformatics.

syntactic and algebraic components
our workflow conceptualization  closely follows those of lei and singh  <cit>  and stevens et al.  <cit> . we have adapted these meta-models to processes in bioinformatic analysis. we consider an input/output data object as a collection of input/output data. for us a transformer is the atomic work item in a workflow. in analytical workflows, it is an implementation of an analytical algorithm . a pipe component is the entity that contains the required input-output relation ; it assures syntactic coherence. our workflow representation has tasks, stages, and experimental conditions . in our view, protocols are sets of information that describe an experiment. a protocol contains workflows, annotations, and information about the raw data; therefore we understand a workflow to be a group of stages with interdependencies. it is a process bound to a particular resource that fulfils the analytical necessities.

we identify needs common to analytical workflows in bioinformatics:

• flexibility in structuring and modelling .

• support for workflows with a complex  inner structure of individual steps . biological workflows may be complex not simply because of the discrete number of steps, but due to the highly nested structure of iteration, recursion and conditional statements that, moreover, may involve interaction with non-workflow systems.

• distribution of workflow execution over grid environments.

• management of failures. this particular requirement is related to conditional statements: where the service will be executed should be evaluated based on considerations of availability and efficiency made previous to the execution of the workflow. in situations where a failure halts the process, the system should either recover it, or dispatch it somewhere else without requiring intervention by the user.

• system functionality features such as browsing and visualization, documentation, or coupling with external tools, e.g. for analysis.

• a semantic layer for collaborative purposes. this semantic layer has many other features, and may be the foundation for intelligent agents that facilitate collaborative research.

executing these bioinformatic workflows further requires:

• support for long-running activities with or without user interaction.

• application-dependent correctness criteria for execution of individual and concurrent workflows.

• integration with other systems  that have their own execution/correctness requirements.

• reliability and recoverability with respect to data.

• reliable communication between workflow components and processing entities.

among these types of requirements, we focus our analysis only on those closely related to workflow design issues, more specifically  the piping of data,  the availability of conditional statements,  the need to iterate one method over a set of different inputs,  the possibility of recursion over a parameter space for a method,  and the need for stop/break management. algebraic operators can accurately capture the meaning of these functional requirements. to describe an analytical workflow, it is necessary to consider both algebraic operators and syntactic components. in table  <dig> we present the definition of those algebraic operators we propose, and in figure  <dig> we illustrate how these operators and syntactic elements together can describe an analytical workflow.

iteration is the operator that enables processes in which one transformer is applied over a multiple set of inputs. a special case for this operator occurs when it is applied over a blank transformer; this case results in replicates of the input collection. consider an analytical method, or a workflow, in which the same input is to be used several times; the first step would be to use as many replicates of the input collection as needed. the recursion operation takes place when one transformer is applied with parameters defined not as a single value, but as a range or as a set of values. the conditional operator has to do with the conditioned execution of transformers. this operation can be attached to a function evaluated over the application of a recursion, or of an iteration; if the stated condition is true, then the workflow executes a certain path. conditional statements may also be applied to cases where an argument is evaluated on the input; the result affects not a path, but the parameter space of the next stage. the suspension/resumption operation stands for the capacity of the workflow to stop and re-capture the jobs.

formal concept analysis  is a mathematical theory based on ordered sets and complete lattices. numerous investigations have shown the usefulness of concept lattices for information retrieval combining query and navigation, learning and data-mining, visual constructors and visual programming  <cit> . fca helps one to define valid objects, and identify behaviours for them. we are currently working on a complete fca for biological data types and operations . here we define operators in terms of pre- and post-conditions, as a step toward eventual logical formalization. we focus on those components of the discovery process not directly related to database operations; a good integration system will "hide" the underlying heterogeneity, so that one can query using a simple language . selection of the query language depends only on the data model. for the xml "data model", xml-ql, xql, and other xml query languages are available. for the nested relational model there are nested relational calculi and nested relational algebras. for the relational model sql, relational algebras and so on are available. for database operations, the issues that arise are lower-level , and it is not clear that any particular algebra offers a significant advantage.

operator: iteration : i: 

pre-condition:

t = transformer ∧ t ≠ blank

c = {cc <dig>  cc <dig>  ..., ccn} such that cci ∈ { biological data types }

post-condition:

c' = {cc' <dig>  cc' <dig>  ..., cc'n} such that cc'i = t ∧  <dig> ≤ i ≤ n

operator: iteration : i:  <dig>   <dig>  ..., num

pre-condition:

num ∈ , num = number of replicates.

c = {cc <dig>  cc <dig>  ..., ccn} such that cci ∈ { biological data types }

post-condition:

c' = {cc' <dig>  cc' <dig>  ..., cc'n} such that cc'i = cci ∧  <dig> ≤ i ≤ n

operator: recursion : r: parm_space'

pre-condition:

p = parameter such that p ∈ parm_space ∨ 

t = transformer

post-condition:

parm_space' = t 

operator: condition : c: path

pre-condition:

fc = functional_condition

post-condition:

path = true ∨ false ∨ value

operator: suspension/resumption : s: execution

pre-condition:

 ∨ 

post-condition:

∨ )) ∨ )

a more-detailed example involves the inference of molecular phylogenetic trees by executing software that implements three main phylogenetic inference methods: distance, parsimony and maximum likelihood. figure  <dig> illustrates how our algebraic operators and syntactic components define the structure of this workflow.

in collaboration with ciat  we have implemented an annotation workflow using standard technology  and web services . our case workflow is detailed in figure  <dig> 

implementation of both of these workflows was a manual process. gui generation was facilitated by using pise as our gui generator, and this simplified the inclusion of new analytical methods as needed. database calls had to be manually coded in both cases. choreographing the execution of the workflow was not simple, as neither has a real workflow engine. it proved easier to give users the ability to manipulate parameters and data with pise/gpipe, partly due to wider range of methods within bioperl partly because algebraic operators were readily available as part of pise/gpipe. from this experience we have concluded that, due to the immaturity of current available web service engines, it is still most practical to implement simple xml workflows that allow users to manipulate parameters, use conditional operators, and carry out write and read operations over databases. this balance will, of course, presumably shift as web services mature in the bioinformatics applications domain.

workflow generation, an implementation
we have developed gpipe, a flexible workflow generator for pise. gpipe extends the capabilities of pise to allow the creation and sharing of customised, reusable and shareable analytical workflows. so far we have implemented and tested gpipe over only the emboss package, although extension to other algorithmic implementations is possible where there is an xml file describing the command-line user interface.

workflows automate businesses procedures in which information or tasks are passed between conforming entities according to a defined set of rules; some of these business rules are defined by the user, and in our implementation are managed via gpipe. for our purposes, the conforming entities are analytical methods . syntactic rules drive the interaction between these entities . gpipe also assures the execution of the workflow, and makes it possible to distribute different jobs over a grid of servers. gpipe addresses these requirements using mostly bioperl.

in gpipe, each analysis protocol  is defined within an xml file. a java applet provides the user with an exploratory tool for browsing and displaying methods and protocols. synchronization is maintained between client-side display and server-side storage using javascript. server-side persistence is maintained through serialized perl objects that manage the workflow execution. gpipe supports independent branched tasks in parallel, and reports errors and results into an html file. the user selects the methods, sets parameters, defines the chaining of different methods, and selects the server on which these will be executed. gpipe creates an xml file and a perl script, each of which describes the experiment. the perl file may later be used on a command-line basis, and customized to address specific needs. the user can monitor the status of workflow execution, and access intermediary results. a workflow built with gpipe can distribute its analyses onto different, geographically disperse gpipe/pise servers.

discussion
the syntactic and algebraic components we introduce above make it possible to describe analytical workflows in bioinformatics precisely yet flexibly. detailed algebraic representation for these kinds of processes have not previously been used in this domain, although they are commonly used to represent business processes. since open projects such as bioperl or biopipe contain the rules and logic for bioinformatic tasks, we believe that having an algebraic representation could contribute importantly to the development of a biological "language" that allows developers to avoid the tedious parsing of data and analytical methods so common in bioinformatics. the schematic representation for workflows in bioinformatics that we present here could evolve to cover other tool-driven integrative approaches such as those based on web services. workflows in which concrete executions take place over a grid of web services involve basically the same syntactic structure and algebraic operators; however, a clear business logic needs to be defined beforehand for those web services in order to deepen the integration beyond simply the fact of remote execution. a higher level of sophistication for the pipe component as well as for the conditional operator may be needed, since remote execution requires  assessment and availability of the service for the job to be successfully dispatched and processed. for our implementation we use two agents, one on the client side and the other on the server side, with the queue handled by pbs . it is possible to add a semantic layer, thereby allowing conceptual selection of the transformers; clear separation between the operational domain and the knowledge domain be would then be achieved naturally.

semantic issues are particularly important with these kinds of workflows. an example may be derived from figure  <dig>  where three different phylogenetic analysis workflows are executed. these may be grouped as equivalent, but are syntactically different. selection should be left in the hands of the user, but the system should at least inform about this similarity.

despite agreement on the importance of semantic layers for integrative systems, such a level of sophistication is far from being achieved. lack of awareness of the practical applications of such technologies is well illustrated with a traditional and well-studied product: microsoft word®. with word, syntactic verification can take place as the user composes text, but no semantic corroboration is done. for two words like "purpose" and "propose", word advises on syntactic issues, but gives no guidance concerning the context of the words. semantic issues in bioinformatic workflows are more complex, and it is not clear if existing technologies can effectively overcome these problems.

transformers and grid components are intrinsically related because the services are de facto linked to a grid component. it has been demonstrated that the use of ontologies facilitates interoperability and the deployment of software agents  <cit> ; correspondingly, we envision semantic technology supporting the agents to form the foundation of future workflow systems in bioinformatics. the semantic layer should make the agents more aware of the information.

more and more guis are available in bioinformatics; this can be seen in the number of guis for emboss and gcg alone. some of them incorporate a degree of workflow capability, more typically a simple chaining of analytical methods rather than flexible workflow operations. a unified metadata model for gui generation is lacking in the bioinformatics domain. web services are relatively easy to implement, and are becoming increasingly available as gui systems are published as web services. however, web services were initially developed to support processes for which the business logic is widely agreed upon, well-defined and properly structured, and the extension of this paradigm to bioinformatics may not be straightforward.

automatic service discovery is an intrinsic feature of web services. the accuracy of the discovery process necessarily depends on the ontology supporting this service. systems such as biomoby and taverna make extensive use of service discovery; however, due to the difficulty in describing biological data types, service discovery is not yet accurate. it is not yet clear whether languages such as owl can be developed to describe relations between biological concepts with the required accuracy. integrating information is as much a syntactic as a semantic problem, and in bioinformatics these boundaries are particularly ill-defined.

semantic and syntactic problems were also identified from the case workflow described in figure  <dig>  there, we saw that to support the extraction of meaningful information and its presentation to the user, formats should be ontology-based and machine-readable, e.g. in xml format. lack of these functional features makes manipulation of the output a difficult task that is usually addressed by use of parsers specific to each individual case. for workflow development, human readability can be just as important. consider, for example, a clustalw output where valid elements could be identified by the machine and presented to the user together with contextual menus including different options over the different data types. in this way the user would be able to decide what to do next, where to split a workflow, and over which part of the output to continue or extend the analysis. inclusion of this functionality would allow the workflow to become more concretely defined as it is used.

failure management is an area in which we can see a clear difference between the business world and bioinformatics. in the former, processes rarely take longer than an hour and are not so computationally intensive, whereas in bioinformatics, processes tend to be computationally intensive and may take weeks or months to complete. how failures can be managed to minimize losses will clearly differ between the two domains. due to the immaturity of both web services and workflows in bioinformatics, it is still in most cases more practical to hard-code analytical processes. improved failure management is one of the domain-specific challenges that faces the application of workflows in bioinformatics.

so far we have intentionally referred to guis and workflows as more-or-less independent. a glimpse into the corresponding metadata reveals that guis are themselves components of workflow systems. in the bioinformatics domain this relationship is particularly attractive, since algebraic operations are usually highly nested. the interface system should therefore provide a programming environment for non-programmers. the language as such is not complex, but makes extensive use of statements such as while...do, if...then...else, and for...each. the representation should be natural to the researcher, separating the knowledge domain from the operational domain.

CONCLUSIONS
we have developed gpipe, a flexible workflow generator that makes it possible to export workflow definitions either as xml or perl files . our xml workflow representation is reusable, execution and edition of those generated workflows is possible either via the bioperl api or the provided gui. each analysis is configurable, as users are presented with options to manipulate all available parameters supported by the underlying algorithms. integration of new algorithms, and grid execution of workflows, are also possible. most available integrative environments rely on parsers or syntactic objects, making it difficult to integrate new analytical methods into workflow systems. we are planning to develop a more wide-ranging algebra that includes query operations over biological databases as well as different ontological layers that facilitate data interoperability and integration of information where possible for the user. we do not envision gpipe to be a complete virtual laboratory environment; future releases will provide a content management system for bioinformatics with workbench capacities developed on top of zope . we have tested our implementation over suse and debian linux, and over solaris  <dig> 

authors' contributions
agc was responsible for design and conceptualization, took part in implementation, and wrote a first draft of the manuscript. st was the main developer of gpipe. ljg assisted with server issues and fca. mar supervised the project and participated in writing the manuscript.

