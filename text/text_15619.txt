BACKGROUND
the past decade has seen great progress in the field of biomedical text mining . this progress has been stimulated by the rapid publication rate in biosciences and the need to improve access to the growing body of textual information available via resources such as the national library of medicine's pubmed system  <cit> . in recent past, considerable work has been conducted in many areas of bio-tm. basic domain resources such as biomedical dictionaries, ontologies, and annotated corpora have grown increasingly sophisticated, and a variety of novel techniques have been proposed for the processing, extraction and mining of information from biomedical literature. current systems range from those capable of named-entity recognition to those dealing with e.g. document classification, information extraction, segmentation, and summarization, among many others  <cit> .

while much of the early research on bio-tm concentrated on technical developments , in recent years, there has been an increasing interest in users' needs  <cit> . studies exploring the tm needs of biomedical researchers have appeared  <cit> , along with practical tools for the use of scientists  <cit> . however, user-centered studies are still lacking in many areas of research and further evaluation of existing technology in the context of real-life tasks is needed to determine which tools and techniques are actually useful  <cit> .

in this article we will focus on one active area of bio-tm research - textual information structure of scientific documents - and will investigate its practical usefulness for a real-life biomedical task. the interest in information structure  stems from the fact that scientific documents tend to be fairly similar in terms of how their information is structured. for example, many documents provide some background information before defining the precise objective of the study in question, and conclusions are typically preceded by a description of the results obtained. many readers of scientific literature are interested in specific information in certain parts of documents, e.g. in the general background of the study, the methods used in the study, or the results obtained). accordingly, many bio-tm tasks have focused on the extraction of information from the relevant parts of documents only. classification of documents according to the categories of information structure has proved useful e.g. for question-answering, summarization and information retrieval  <cit> .

to date, a number of different schemes have been proposed for  sentence-based classification of scientific literature according to categories of information structure, e.g.  <cit> . the simplest of these schemes merely classify sentences according to section names seen in scientific documents, for example, the objective, methods, results and conclusions sections appearing frequently  in biomedical abstracts  <cit> . some other schemes are based on components of scientific argumentation. a well-known example of such a scheme is the argumentative zoning  scheme originally developed by teufel and moens  <cit>  which assumes that the act of writing a scientific paper corresponds to an attempt of claiming ownership for a new piece of knowledge. including categories such as other, own, basis and contrast, az aims to model the argumentative or rhetorical process of convincing the reviewers that the knowledge claim of the document is valid.

also schemes based on conceptual structure of documents exist - for example, the recent core scientific concepts  scheme  <cit> . coresc treats scientific documents as humanly readable representations of scientific investigations. it seeks to retrieve the structure of an investigation from the paper in the form of generic high-level concepts such as hypothesis, model, and experiment . furthermore, schemes aimed at classifying statements made in scientific literature along qualitative dimensions have been proposed. the multi-dimensional classification system of shatkay et al.  <cit> , developed for the needs of diverse users, classifies sentences  according to dimensions such as focus, polarity, certainty, evidence and trend.

different schemes of information structure have been evaluated in terms of inter-annotator agreement, i.e. the agreement with which two or several human judges label the same element of text with the same categories. some of the schemes have been further evaluated in terms of machine learning: the accuracy with which an automatic classifier trained on human-annotated data is capable of assigning text to scheme categories, e.g.  <cit> . also evaluation in the context of bio-tm tasks such as question-answering, summarization, and information retrieval has been conducted  <cit> . these evaluations have produced promising results. however, evaluation in the context of real-life tasks in biomedicine has been lacking, although such evaluation would be important for determining the practical usefulness of the schemes for end-users.

in this paper, we will investigate the usefulness of information structure for cancer risk assessment . performed manually by human experts , this real-life task involves examining scientific evidence in biomedical literature  to determine the relationship between exposure to a substance and the likelihood of developing cancer from that exposure  <cit> . the starting point of cra is a large-scale literature review which focuses, at the first instance, on scientific abstracts published on the chemical in question. risk assessors read these abstracts, looking for a variety of information in them, ranging from the overall aim of the study to specific methods, experimental details, results and conclusions  <cit> . this process can be extremely time consuming since thorough risk assessment requires considering all the published literature on a chemical in question. a well-studied chemical may well have tens of thousands of abstracts available . cra is therefore an example of a task which might well benefit from annotations according to textual information structure.

our study focuses on three different schemes: those based on section names, az and coresc, respectively. we examine the applicability of these schemes to biomedical abstracts used for cra purposes. since az and coresc have been developed for full journal articles, our study provides an idea of their applicability to tasks involving abstracts. we describe the annotation of a corpus of cra abstracts according to the three schemes, and compare the resulting annotations in terms of inter-annotator agreement and the distribution and overlap of scheme categories. our evaluation shows that for all the schemes, the majority of categories appear in scientific abstracts and can be identified by human annotators with good or moderate agreement . interestingly, although the three schemes are based on entirely different principles, our comparison of annotations reveals a clear subsumption relation between them.

we introduce then a machine learning approach capable of automatically classifying sentences in the cra corpus according to scheme categories. our results show that all the schemes can be identified using automatic techniques, with the accuracy of 89%, 90% and 81% for section names, az and coresc, respectively. this is an encouraging result, particularly considering the fairly small size of the cra corpus and the challenge it poses for automatic classification.

finally, we introduce a user test - conducted by experts in cra - which evaluates the usefulness of the different schemes for real-life cra. this test focuses on two schemes: the coarse-grained scheme based on section names and the finest-grained coresc scheme. it evaluates whether risk assessors find relevant information in literature faster when presented with unannotated abstracts or abstracts annotated  according to one of the schemes. the results of this test are promising: both schemes lead into significant savings in risk assessors' time. although manually annotated abstracts yield biggest savings in time , considerable savings are also obtained with automatically annotated abstracts . interestingly, although coresc helps to save more time than section names, the difference between the two schemes is so small that it is not statistically significant.

in sum, our work shows that existing schemes aimed at capturing information structure can be applied to biomedical abstracts relatively straightforwardly and identified automatically with an accuracy which is high enough to benefit a real-life task.

the rest of this paper is organized as follows: the methods section introduces the cra corpus, the annotation tool, and the annotation guidelines, together with the automatic classification methods and the methods of direct and user-based evaluation. the results section describes first the annotated corpus. the results of the inter-annotator agreement tests, comparison of the schemes in annotated data, the automatic classification experiments, and the user-test are then reported. the discussion and conclusions section concludes the paper with comparison to related research and directions for future work.

methods
the three schemes
full journal articles are more complex and richer in information than abstracts  <cit> . as a distilled summary of key information in full articles, abstracts may exhibit an entirely different distribution of scheme categories than full articles. for the practical tasks involving abstracts, it would be useful to know which of the existing schemes are applicable to abstracts and which of them can be identified in them automatically with sufficient accuracy. we chose three different schemes for our investigation - those based on section names, argumentative zones, and core scientific concepts:

â€¢ section names - s1: the first scheme differs from the other two in the sense that it is actually developed for abstracts. it is based on section names found in some scientific abstracts. we use the 4-way classification from  <cit>  where abstracts are divided into objective, method, results and conclusions. hirohata et al. show that this 4-way classification is the most frequently used classification in medline abstracts  <cit> . they also provide a mapping of the four section names to their synonymous names appearing in medline. table  <dig> provides a short description of each category and its abbreviation . for example, the objective category  of this scheme aims to capture the background and the aim of the research described in abstracts.

â€¢ argumentative zoning - s2: the second scheme is based on argumentative zoning  of documents. az provides an analysis of the rhetorical progression of the scientific argument. it follows the knowledge claims made by authors. teufel and moens  <cit>  introduced az and applied it first to computational linguistics papers. mizuta et al.  <cit>  modified the scheme for biology papers. more recently, teufel et al.  <cit>  introduced a refined version of az and applied it to chemistry papers. as the recent refined version of az is too fine-grained for abstracts  and is not directly applicable to biomedical texts , we adopt the earlier version of az developed for biology papers  <cit> . from the ten categories of mizuta et al., we select seven which  actually appear in abstracts: those shown in table  <dig>  note that we have re-named some of the original category names, mostly for improved annotation accuracy.

â€¢ core scientific concepts - s3: the third scheme is the recent concept-driven and ontology-motivated scheme of liakata et al.  <cit> . this scheme views papers as written representations of scientific investigations and aims to uncover the structure of the investigation as core scientific concepts . like az, coresc has been previously applied to chemistry papers  <cit> . the coresc is a 3-layer annotation scheme but we only consider the first layer in the current work. the second layer pertains to properties of the categories . such level of granularity is rare in abstracts. the 3rd layer involves co-reference identification between the same instances of each category, which is also not of concern in abstracts. we adopt the eleven categories in the first layer of coresc . s <dig> is thus the most fine-grained of our schemes.

data: cancer risk assessment abstracts
we used as our data the corpus of cra abstracts described in  <cit> . it contains medline abstracts from  <dig> biomedical journals  which are used frequently for cra purposes and which jointly provide a good coverage of the main types of scientific evidence relevant for the task. from these  <dig> journals, all the abstracts from years  <dig> to  <dig> which include one of the following eight chemicals were included:  <dig> -butadiene, benzopyrene, chloroform, diethylnitrosamine, diethylstilbestrol, fumonisin b <dig>  phenobarbital, and styrene. these chemicals were selected by cra experts on the basis that they  are well-researched using a range of scientific tests  and  represent the two most frequent mode of action types : genotoxic and non-genotoxic. a moa is an important concept in cra: it determines the key events leading to cancer formation. chemicals acting by a genotoxic moa induce cancer by interacting with dna, while chemicals acting by a nongenotoxic moa induce cancer without interfering directly with dna . we selected  <dig> abstracts  from this corpus for our work:  <dig>  sentences and  <dig>  words in total.

annotation of abstracts
annotation guidelines
we annotated the 1000-abstract version of the cra corpus according to each of the schemes. we used the annotation guidelines of liakata for s <dig>  and developed the guidelines for s <dig> and s <dig> ourselves. the new guidelines were developed via trial annotations and discussions. they provide a generic description of each scheme and its purpose, define the unit of annotation , introduce all the scheme categories, provide advice for conflict resolution , and include examples of annotated abstracts. each guideline is  <dig> pages long. we have made them available at http://www.cl.cam.ac.uk/~yg244/10crab.html.

annotation tool
we used the annotation tool of korhonen et al.  <cit>  for corpus annotation. this tool was originally developed for the annotation cra abstracts according to the scientific evidence they contain. we modified it so that it could be used to annotate abstracts according to our schemes. it works as a firefox plug-in. using this tool, experts can open each medline abstract assigned to them, assign a scheme category to each sentence by highlighting it and selecting the appropriate category from a menu with a single mouse click. highlighted sentences are displayed using colors which correspond to the different scheme categories as defined in the annotation guidelines. a screen-shot illustrating the annotation tool is provided in figure  <dig>  the figure shows an example abstract annotated according to each of the three schemes.

description of annotation
using the guidelines and the tool, the cra corpus was annotated according to each of the schemes. previous related annotation efforts have varied in terms of the expertise required from annotators. for example, mizuta et al.  <cit>  used a single annotator to annotate full biology articles according to s <dig>  this person was a phd level linguist with no training in biology. in contrast, liakata et al.  <cit>  used domain experts  to annotate chemistry papers according to s <dig>  teufel et al.  <cit> , in turn, used a mixed group of three annotators to annotate chemistry papers according to their recent refined az scheme: a phd level computational linguist, a chemist, and a computational linguist with some experience in chemistry.

we used a single annotator  to annotate the whole cra corpus. however, following teufel et al., we measured inter-annotator agreement between annotators who have different expertises: the computational linguist, one domain expert  and one phd level linguist with no training in biomedicine. the inter-annotator agreement was measured on a subset of the corpus as described later in results section.

the annotation proceeded scheme by scheme, independently, so that annotations of one scheme were not based on any of the other two. the annotation started from the coarse-grained s <dig>  then proceeding to s <dig> and finally to the finest-grained s <dig>  the inter-annotator agreement was measured using cohen's kappa  <cit> , which is the portion of agreement ) corrected for chance ): .

comparison of annotations
the three schemes we investigate were developed independently and have separate guidelines. thus, even though they seem to have some categories in common  this does not necessarily guarantee that the categories cover the same information across all three schemes. we therefore investigated the relation between the schemes and the degree of overlap or complementarity between them.

we created contingency tables and calculated the chi-squared pearson statistic, the chi-squared likelihood ratio, the contingency coefficient and cramer's v for pairwise comparison of schemes. however, since none of these measures give an indication of the differential association between schemes  we also calculated the goodman-kruskal lambda l statistic  <cit> . this gives us the reduction in error for predicting the categories of one annotation scheme, if we know the categories assigned according to the other.

in addition we examined the correspondence between the actual categories of the three schemes using the paradigm of kang et al.  <cit> . kang et al. discuss a framework for subsumption checking between classes in different ontologies. they argue that if for the set of mutual instances between two classes, instances of one consistently belong to the other, we can assume that a subsumption relation holds. they suggest setting a fault tolerance threshold to cater for erroneous annotations:  

where tk is the threshold, w, w are the weights for instances and x, y are the classes. we set w = w =  <dig>  and set tk =  <dig>  as a reasonable threshold, allowing at most 10% of instances to be allocated to other categories for the subsumption to hold.

automatic identification of information structure
use of information structure in real-life biomedical applications requires a method capable of automatically assigning sentences in documents to appropriate scheme categories. to find out whether our schemes are machine learnable in the cra abstract corpus, we conducted a series of classification experiments. these experiments involved extracting a range of linguistic features from each sentence in our corpus and given these features and the scheme labels in the annotated corpus, using supervised machine learning to automatically assign each sentence to the most likely category  of the scheme in question. previous works in this research area have used standard text classification features  and various well-known classifiers such as naive bayes  <cit> , support vector machines  <cit> , maximum entropy  <cit> , hidden markov models  <cit>  and conditional random fields  <cit> . we used for our experiment mainly features and classifiers which have proved successful in previous works. these will be described in detail in the subsequent sections.

features
the first step in automatic classification is to select features for classification. we chose a number of general purpose features suitable for all the three schemes. with the exception of our novel verb class feature, these features are similar to those employed in related works, e.g.  <cit> :

â€¢ history. there are typical patterns in the information structure so that certain categories tend to appear before others. for example, res tends to be followed by con rather than by bkg. therefore, we used the category assigned to the previous sentence as a feature.

â€¢ location. categories tend to appear in typical positions in a document, e.g. bkg occurs often in the beginning and con at the end of the abstract. we divided each abstract into ten equal parts , measured by the number of words, and defined the location  feature by the parts where the sentence begins and ends.

â€¢ word. like many text classification tasks, we employed all the words in the corpus as features.

â€¢ bi-gram. we considered each bi-gram  as a feature.

â€¢ verb. verbs are central to the meaning of sentences, and can vary from one category to another. for example, experiment is frequent in meth and conclude in con. previous works have used the matrix verb of each sentence as a feature. because the matrix verb is not the only meaningful verb, we used all the verbs instead.

â€¢ verb class. because individual verbs can result in sparse data problems, we also experimented with a novel feature: a lexical-semantic verb class . we obtained  <dig> classes by clustering verbs appearing in full cancer risk assessment articles using the approach of sun and korhonen  <cit> .

â€¢ part-of-speech - pos. tense tends to vary from one category to another, e.g. past is common in res and past participle in con. we used the part-of-speech  tag of each verb assigned by the c&c tagger  <cit>  as a feature.

â€¢ grammatical relation - gr. structural information about heads and dependents has proved useful in text classification. we used grammatical relations  returned by a parser as features. they consist of a named relation, a head and a dependent, and possibly extra parameters depending on the relation involved, e.g. . we created features for each subject , direct object , indirect object  and second object  relation in the corpus.

â€¢ subj and obj. as some gr features may suffer from data sparsity, we collected all the subjects and objects  from grs and used them as features. the value of such a subject  feature equals  <dig> if it occurs in a particular sentence .

â€¢ voice. there may be a correspondence between the active and passive voice and categories . we therefore used voice as a feature.

pre-processing and feature extraction
we developed a tokenizer to detect the boundaries of sentences and to perform basic tokenization, such as separating punctuation from adjacent words e.g. in tricky biomedical terms such as 2-amino- <dig> -diethylimidazoquinoxaline. we used the c&c tools  <cit>  adapted to biomedical literature for pos tagging, lemmatization and parsing. the lemma output was used for extracting word, bi-gram and verb features. the parser produced grs for each sentence from which we extracted the gr, subj, obj and voice features. we only considered the grs relating to verbs. the "obj" marker in a subject relation indicates a verb in passive voice ). to control the number of features we removed the words and grs with fewer than  <dig> occurrences and bi-grams with fewer than  <dig> occurrences, and lemmatized the lexical items for all the features.

classifiers
we used naive bayes , support vector machines , and conditional random fields  for classification. these methods have been used to discover information structure in previous related works, e.g.  <cit> . nb is a simple and fast method, and svm and crf have been used successfully in a wide range of text classification tasks.

nb applies bayes' rule and maximum likelihood estimation with strong independence assumptions. it aims to select the class c with maximum probability given the feature set f:

arg maxc p  

svm constructs hyperplanes in a multidimensional space that separates data points of different classes. good separation is achieved by the hyperplane that has the largest distance from the nearest data points of any class. the hyperplane has the form w Â· x - b =  <dig>  where w is the normal vector to the hyperplane. we want to maximize the distance from the hyperplane to the data points, or the distance between two parallel hyperplanes each of which separates the data. the parallel hyperplanes can be written as: w Â· x - b =  <dig> and w Â· x - b =  <dig>  and the distance between the two is . the problem reduces to:

minimize |w|

subject to w Â· xi - b â‰¥  <dig> for xi of one class,

and w Â· xi - b â‰¤ - <dig> for xi of the other.

crf is an undirected graphical model which defines a distribution over the hidden states  given the observations. the probability of a label sequence y given an observation sequence x can be written as:  

where fj is a real-valued feature function of the states, observations, and the position in the sequence; Î»j is the weight of fj, and z is a normalization factor. the Î» parameters can be learned using the lbfgs algorithm, and arg maxy p can be inferred using the viterbi algorithm.

we used weka  <cit>   and crf++  <cit>  for the classification.

evaluation methods
the results were measured in terms of accuracy , precision , recall , and f-measure :  

we used 10-fold cross validation to avoid the possible bias introduced by relying on any one particular split of the data. the data were randomly divided into ten parts of approximately the same size. each individual part was retained as test data and the remaining nine parts were used as training data. the process was repeated ten times with each part used once as the test data. the resulting ten estimates were then combined to give a final score. we compare our classifiers against a baseline method based on random sampling of category labels from training data and their assignment to sentences on the basis of their observed distribution.

user test in the context of cancer risk assessment
we developed a user test so that we could evaluate and compare the practical usefulness of information structure schemes for cra.

two schemes were selected for this test: the coarse-grained s <dig> and the fine-grained s <dig>  s <dig> was excluded because it proved fairly similar to s <dig> in terms of its performance in machine learning experiments .

the user test was designed independently from the schemes. the idea was to ask cancer risk assessors to look for the information they typically look for in biomedical abstracts during an early stage of their work . the test was designed to compare the time it takes for risk assessors to find relevant information in  unannotated abstracts and  abstracts annotated according to the schemes. longer reading times have been shown to indicate greater cognitive load during language comprehension  <cit> . minimizing the reading time is desirable as it can help to reduce the high cost of manual cra. intuitively, when risk assessors look for information about e.g. the methods used in a study, they should find this information faster when pointed to those sentences which discuss methods according to our schemes. however, whether this really helps to a significant degree , was an open question - along with which scheme  might be more useful for the task.

as a starting point, cancer risk assessors working in karolinska institutet  provided us with a list of questions they consider when studying abstracts for cra. as the questions were of varying style and granularity and focused on various parts of abstracts, they seemed ideal for the evaluation of the schemes. the majority were adopted for the user test; however, some of the open-ended questions requiring text-based inference and more elaborate answers  were simplified to merely test whether and how fast the information in question could be found . this yielded a more controlled experiment which was better suited for comparing the performance of different users. we ended up with the following questionnaire containing seven questions where each question has either a verbal, 'yes' or 'no', or multiple choice answer:

q <dig> what was the aim of the study?

verbal answer

q <dig> what was the main type of the study?

four possible answers from which users have to select one: an animal study, human study, in vitro study or a combined study. depending on the answer selected, three follow-up questions apply which each require a 'yes' or 'no' answer. for example, the following questions apply to a human study:

q3a is exposure length mentioned?

q3b is group size mentioned?

q3c are endpoints mentioned?

q <dig> positive results?

three possible answers: 'yes', 'no' or 'unclear'

q <dig> author's conclusions?

verbal answer

we designed an on-line form which shows an abstract  on the top of the page and each question on the bottom of the page. the questions are displayed to experts one at a time, in the sequential order shown above. the idea of the test is to record the time it takes for an expert to answer each question. this is done by asking them to press 'start', 'next' and 'complete' buttons during different phases of the test, as appropriate.

two screen-shots illustrating the test are shown in figure  <dig>  they show the same abstract annotated according to s <dig> for questions  <dig> and  <dig>  respectively. as illustrated in the screen-shorts, although the whole abstract is shown to experts with each question, only 1- <dig> scheme categories are highlighted  per question, as to attract experts' attention. those are the scheme categories which are most likely to contain an answer to the particular question. we settled for this option after conducting a pilot study which showed that users found abstracts annotated according to all the  scheme categories confusing rather than helpful.

highlighting only the 1- <dig> most relevant categories required creating a mapping between the questions and the potentially relevant scheme categories and investigating which of the categories are really the most important ones for answering each question. we asked an expert  to examine  <dig> abstracts which had been manually annotated for s <dig> and s <dig> and to indicate, for each question, all the possible categories where an answer to the question could be found. this pilot study showed that although it was often possible to find an answer in several categories, there were 1- <dig> dominant categories which nearly always included the answer. for s <dig>  a single dominant category could be identified for each of the questions. for s <dig>  a single category was found for five of the questions, and two questions had two equally dominating categories which were both included because they were usually mutually exclusive. table  <dig> shows all the possible categories per question, and the 1- <dig> dominant ones per scheme which we used in our test.

 shows all the possible categories and  shows the dominant categories. the latter were used in the user test.

three experts participated in our test: two professor level experts with a long experience in cra  - a and b - and one more junior expert: c who has a phd in toxicology and over  <dig> years of experience in cra. we selected  <dig> abstracts from the cra corpus for this test, in random but subject to the constraint that they were similar in length and focused on one of the four chemicals: butadiene, diethylnitrosamine, diethylstilbestrol, and phenobarbital. each expert was presented with the same set of  <dig> abstracts. the abstracts were divided into  <dig> groups  so that each expert was presented with:

s0: unannotated abstracts,

s1: abstracts annotated manually according to s <dig> 

s3: abstracts annotated manually according to s <dig> 

s1': abstracts automatically annotated according to s <dig> using the svm classifier, and

s3': abstracts automatically annotated according to s <dig> using the svm classifier.

the results were measured in terms of the  total time it took for the experts to examine each abstract in the five groups above, and  the percentage of time each expert saved when examining scheme annotated abstracts vs. unannotated ones. we also measured the statistical significance of the differences using the mann-whitney u test  <cit> . the results were measured in p-value, and the chosen significance level was  <dig> . finally, we examined whether automatic annotations affected the quality of the expert answers. we did this by comparing the agreement in expert answers between s <dig>  s1' and s3' annotated abstracts.

RESULTS
the annotated corpus and inter-annotator agreement
the corpus annotation work took  <dig>   <dig> and  <dig> hours in total for s <dig>  s <dig> and s <dig>  respectively. table  <dig> shows the distribution of sentences per scheme category in the resulting corpus. we see that for s <dig>  all the four categories appear in abstracts with sufficient frequency, with res being the most frequent category . for s <dig>  res is also the most frequent category . four other s <dig> categories appear in the corpus data with reasonable frequency: bkg, obj, meth and con, which cover 8-18% of the corpus each. two categories are very low in frequency, only covering 1% of the corpus each: rel and fut. also for s <dig>  res is the most frequent category . for s <dig>  six other categories cover 6-14% of the corpus each , while four categories cover 1-4% . all the scheme categories we set to explore thus did appear in abstracts, but some categories belonging to the schemes that have been developed for full papers are rare. however, some of these categories have proven infrequent also in full papers  <cit> .

we measured the inter-annotator agreement on  <dig> abstracts  using three annotators: one linguist, one expert in cra, and the computational linguist who annotated all the corpus. we calculated cohen's kappa  <cit>  between each pair of annotators and averaged the results. the inter-annotator agreement was Îº =  <dig> , Îº =  <dig> , and Îº =  <dig>  for s <dig>  s <dig>  and s <dig>  respectively. according to  <cit> , the agreement  <dig> - <dig>  is perfect and  <dig> - <dig>  is moderate. s <dig> and s <dig> are thus the easiest schemes for the annotators and s <dig> the most challenging. this is not surprising as s <dig> is the scheme with the finest granularity. its reliable identification may require a longer period of training and possibly improved guidelines. moreover, previous annotation efforts using s <dig> have used domain experts for annotation  <cit> . for s <dig> the best agreement was between the domain expert and the linguist . for s <dig> and s <dig> the best agreement was between the linguist and the computational linguist .

comparison of the schemes in terms of annotations
we used the resulting annotations to compare the degree of overlap between the schemes. table  <dig> shows the results of our pairwise comparison. the chi-squared pearson statistic, the chi-squared likelihood ratio, the contingency coefficient and cramer's v each show a definite correlation between the rows and columns for the three schemes. when calculating the goodman-kruskal lambda l statistic  <cit> , using the categories of s <dig> as the independent variables, we obtained a lambda of over  <dig>  which suggests a 72% reduction in error in predicting s <dig> categories and 47% reduction in error in predicting s <dig> categories. with s <dig> categories being the independent variables, we obtain a reduction in error of 88% when predicting s <dig> and 55% when predicting s <dig> categories. the lower lambdas for predicting s <dig> are hardly surprising as s <dig> has  <dig> categories as opposed to  <dig> and  <dig> for s <dig> and s <dig> respectively. s <dig> on the other hand has strong predictive power in predicting the categories of s <dig> and s <dig> with lambdas of  <dig>  and  <dig>  respectively. in terms of association, s <dig> and s <dig> seem to be thus more strongly associated, followed by s <dig> and s <dig> and then s <dig> and s <dig> 

the correspondence between the actual categories of the three schemes is visualized in figure  <dig>  take s <dig> bkg and s <dig> obj for example. the former maps to the latter for 96% of cases, whereas the latter maps to a number of categories in s <dig>  namely 49% bkg, 19% goal, 11% mot and 19% objt. it therefore would seem that s <dig> bkg is subsumed by s <dig> obj but not the other way round. according to the subsumption checking approach of  <cit> , if we take x to be s <dig> bkg and y to be s <dig> obj, we get  <dig>  <  <dig> ; therefore the subsumption relation s <dig> bkg âŠ† s <dig> obj holds.

take, for another example, s <dig> bkg and s <dig> bkg. the former maps to the latter in 97% of cases, whereas the latter maps to 78% bkg,  <dig> % hyp, 11% mot, 9% meth in s <dig>  the subsumption relation is one-way: s <dig> bkg âŠ† s <dig> obj . similarly, s <dig> bkg maps to s <dig> obj in 97% of cases, whereas s <dig> obj maps to  <dig> % bkg, 30% obj and 9% meth in s <dig>  the subsumption relation s <dig> bkg âŠ† s <dig> obj holds . therefore, we have a subsumption relation of the type: s <dig> bkg âŠ† s <dig> bkg âŠ† s <dig> obj.

we follow the same procedure for the rest of the categories. the subsumption relations between scheme categories are summarized below:

s <dig> hyp âŠ†  s <dig> obj

s <dig> mot âŠ†  âŠ† s <dig> obj

s <dig> bkg âŠ† s <dig> bkg âŠ† s <dig> obj

s <dig> goal âŠ† s <dig> obj s <dig> âŠ† obj

s <dig> objt âŠ†  âŠ† 

s <dig> exp âŠ† s <dig> meth âŠ† s <dig> meth

s <dig> mod âŠ†  âŠ† 

s <dig> meth âŠ†  âŠ† 

s <dig> obs âŠ† s <dig> res â‰¡ s <dig> res

s <dig> res âŠ† s <dig> res â‰¡ s <dig> res

s <dig> con âŠ†  âŠ† 

based on the above analysis, it is clear that all categories in s <dig> are subsumed by categories in s <dig> which are in turn subsumed or equivalent to categories in s <dig>  it is therefore reasonable to assume a subsumption relation between the three schemes of the type s <dig> âŠ† s <dig> âŠ† s <dig>  this also agrees with the values of the kruskall-lambda statistic above, according to which if we know s <dig> categories the likelihood of predicting s <dig> and s <dig> categories is high  and decreases if we try to predict s <dig> when knowing s <dig>  or s <dig> .

this subsumption relation is an interesting outcome given that the three different schemes have such different origins.

automatic classification
a-k: history, location, word, bi-gram, verb, verb class, pos, gr, subj, obj, voice

a-k: history, location, word, bi-gram, verb, verb class, pos, gr, subj, obj, voice

we have  <dig>  for fut in s <dig> probably because the size of the training data is just right, and the model doesn't over-fit the data. we make this assumption because we have  <dig>  for almost all the categories on the training data, but only for fut on the test data.

looking at individual features alone, word, bi-gram and verb perform the best for all the schemes, and history and voice perform the worst. in fact history performs very well on the training data, but on the test data we can only use estimates rather than the actual labels; an uncertain estimate of the feature at the beginning of the abstract will introduce further uncertainty later on, leading to poor overall results. the voice feature works only for res and meth for s <dig> and s <dig>  and for obs for s <dig>  this feature is probably only meaningful for some of the categories. when using all but one of the features, s <dig> and s <dig> suffer the most from the absence of location, while s <dig> from the absence of word/pos. verb class on its own performs worse than verb, however when combined with other features it performs better: leave-verb-out outperforms leave-verb class-out.

for s <dig>  svm finds all the four scheme categories with the accuracy of 89%. f-measure is  <dig> for obj, res and con and  <dig> for meth. for s <dig>  the classifier finds six of the seven categories, with the accuracy of 90% and the average f-measure of  <dig> for the six categories. as with s <dig>  meth has the lowest performance ; the one missing category  appears in our abstract data with very low frequency .

for s <dig>  svm uncovers as many as nine of the eleven categories with the accuracy of 81%. six categories perform well, with f-measure higher than  <dig>  exp, bkg and goal have f-measure of  <dig>   <dig> and  <dig>  respectively. like the missing categories hyp and mod, goal is very low in frequency. the lower performance of the higher frequency exp and bkg is probably due to low precision in distinguishing between exp and meth, and bkg and other categories, respectively.

user test
the results of the user test are presented in table  <dig> and  <dig>  table  <dig> shows the total time it took for the experts  to do the user test for abstracts belonging to groups s <dig>  s <dig>  s <dig>  s1' and s3', respectively , along with the percentage of time the experts saved when examining scheme annotated abstracts vs. unannotated ones. columns 2- <dig> show the results for each individual question and column  <dig> shows the overall performance. time stands for the sample mean , and save for the percentage of the time savings. table  <dig> shows, for the three experts, the statistical significance of the differences between all the eight scheme pairs . the statistical significance is indicated using p-values of the mann-whitney u test .

the table shows the time it takes for the cra experts  to answer the questions in the questionnaire  and the percentage of time they save using scheme annotations.

looking at the overall performance figures, the average time spent with unannotated abstracts  was  <dig>  seconds for a,  <dig>  for b, and  <dig>  for c. all the experts spent significantly less time with scheme annotated abstracts  than with unannotated ones : the percentage of time saved ranges between 11% and 46%. even a, who was the fastest expert with unannotated abstracts, saved 16%, 22%, 11% and 17% time with s <dig>  s <dig>  s1' and s3', respectively. for other users the savings in time were bigger.

as expected, the more accurate manually annotated abstracts  help save more time  than automatically annotated ones  . for instance, in the case of c, s <dig> and s <dig> saved 36% and 38% of time, respectively, whereas s1' and s3' saved 17%. however, automatic annotations still clearly helped experts conduct their task faster.

looking at individual questions, for all the users, no significant difference  was found in results between s1' and s <dig> for q3a, q <dig>  and q <dig>  and between s3' and s <dig> for q3c and q <dig>  these are the questions which map to the frequent scheme categories with high f-measures in machine learning experiments: res, con for s1', and obs for s3', as shown in table  <dig>  we can therefore expect future improvements in the automatic detection of lower frequency scheme categories lead to improved performance also in user tests.

comparing the two schemes, for each user, s <dig>  saved more time than s <dig>  for the majority of questions: q <dig>  q3a, q3b, q <dig>  and q <dig>  similarly, s3' saved more time than s1' for q <dig>  q <dig>  q3b, q3c, and q <dig> for the majority of users. these include both broader questions requiring verbal answers like q <dig> and q <dig>  and more specific questions requiring 'yes' vs. 'no' answers like q3a-c. although the majority of these differences between the two schemes are not statistically significant , the small benefit of s <dig>  is still a clear trend in the data, and shows also in the total results.

we finally examined whether using automatically annotated abstracts had an impact on the experts' accuracy. we took the abstracts annotated according to s1' and s3', respectively, and compared the results b and c obtained when using these abstracts against the results a obtained when using s <dig> annotations of the abstracts. interestingly, when using s1' annotations, 83-85% of the answers produced by b and c agreed with the answers produced by a. when using s3' annotations, the agreement of b and c with a was 93%. this demonstrates that the use of automatic annotations does not result in a significant drop in experts' accuracy, in particular when a fine-grained scheme such as s <dig> is used.

discussion and 
CONCLUSIONS
the results from our corpus annotation  show that for the coarse-grained s <dig>  all the four categories appear frequently in biomedical abstracts. this is not surprising because s <dig> was actually developed for abstracts. the inter-annotator agreement on this scheme was good and all the categories were also identified by machine learning  yielding high overall accuracy .

for s <dig>  all the seven categories appeared in the cra corpus and six were found by the svm classifier. also this scheme had good inter-annotator agreement and obtained very similar accuracy in machine learning experiments than s1: 90%.

for s <dig>  all the eleven categories appeared in the corpus and nine of them  were identified using the svm classifier with the overall accuracy of 81%. this accuracy is surprisingly good given the high number of categories, many of which were low in frequency in the cra corpus, and considering the low inter-annotator agreement on this scheme .

these results show that all the three schemes are applicable to abstracts and can be identified in them automatically with relatively high accuracy. interestingly, our analysis in section 'comparison of the schemes in terms of annotations' demonstrates that there is a subsumption relation between the categories of the three schemes. this is surprising since the three schemes were developed based on different principles: s <dig> on section names, s <dig> on following the knowledge claims made by authors, and s <dig> on tracking the structure of a scientific investigation at the level of scientific concepts. our comparison shows that the main practical difference between the schemes is that s <dig> and s <dig> provide finer-grained information about the information structure of abstracts than s <dig> .

ultimately, an optimal scheme will depend on the level of detail required by the application at hand. similarly, the level of accuracy required in machine learning performance may be application-dependent. to shed light on these issues, we conducted evaluation in the context of the real-life task of cra. in this evaluation we focused on the most general s <dig> and the most detailed s <dig> schemes only.

the user test was designed independently of the two schemes. three cancer risk assessors were presented with a questionnaire which involved looking for information  relevant for cra in different parts of biomedical abstracts. we evaluated the time it took for the experts to answer the questions when presented with plain unannotated abstracts and those annotated manually and automatically according to s <dig> and s <dig> 

the results show that all the experts saved significant amounts of time when examining abstracts highlighted for the most relevant scheme categories per question. although manually annotated abstracts are more useful , automatically annotated ones lead to significant savings in time as well  in comparison with unannotated abstracts. although no statistically significant differences could be observed between s <dig> and s <dig>  all the experts performed faster with the majority of questions when presented with s3-labeled abstracts. interestingly, this tendency could be observed with both manual and automatically annotated abstracts.

it is obvious, looking at the 1- <dig> dominant scheme categories  and comparing them to the full set of possible categories in table  <dig> that our cra questionnaire would not realize the full potential of s <dig>  however, the fact that s <dig> proved at least as helpful for users as s <dig> despite the lower machine learning performance is promising. on the other hand, it is encouraging that a scheme as simple as s <dig> can be used to aid a real-world task with a significant saving in users' time.

our user test - which is, to our knowledge, the first attempt to evaluate information structure schemes directly in the context of real-life biomedical tasks - focused on one step of cra. this step involves looking for relevant information in abstracts, mainly to determine the usefulness of abstracts for the task. other steps of cra, in particular those which focus on more detailed information in full biomedical journal articles, are likely to benefit from the schemes of information structure to a greater degree. we intend to explore this avenue of work in our future experiments.

for real-life tasks involving abstracts, it would be useful to further improve machine learning performance. previous works have not evaluated s <dig> or s <dig> on biomedical abstracts. however, hirohata et al.  <cit>  have evaluated s <dig>  they showed that the amount of training data used can have a big impact on the task. they used c.  <dig>  medline abstracts annotated  as training data for s <dig>  when using a small set of standard text classification features and crf for classification, they obtained  <dig> % per-sentence accuracy on  <dig> abstracts. however, when only  <dig> abstracts were used for training the accuracy was considerably worse; their reported per-abstract accuracy dropped from  <dig> % to less than 50%. this contrasts with our crf accuracy of 85% on  <dig> abstracts. although it would be difficult to obtain similarly huge training data for s <dig> and s <dig>  this result suggests that one key to improved performance is larger training data, and this is what we plan to explore especially for s <dig> 

in addition we plan to improve our method. we showed that our schemes are partly overlapping and that similar features and methods tend to perform the best/worst for each of the schemes. it is therefore unlikely that considerable scheme specific tuning will be necessary. however, we plan to develop our features further and to make better use of the sequential nature of information structure. although crf proved disappointing in our experiment, it may be worth investigating it further  and also comparing it  against methods such as maximum entropy which have proved successful in recent related works  <cit> . the resulting models will be evaluated both directly and in the context of cra to provide an indication of their practical usefulness for real-world tasks.

availability and requirements
â€¢ project name: tool for the user test

â€¢ project home page: http://www.cl.cam.ac.uk/~yg244/10crab.html

â€¢ operating system: platform independent 

â€¢ programming language: xul and javascript, perl and java for classification

â€¢ other requirements: firefox  <dig>  

â€¢ license: the application will be freely accessible for all users.

â€¢ any restrictions to use by non-academics: none

authors' contributions
all the authors participated actively in the work reported in this paper. ak took the main responsibility of the write-up of the paper, and together with ml and us designed, supervised and coordinated the project. yg conducted the corpus annotation work with the assistance of is, and conducted the inter-annotator agreement tests. ml did the comparison of the schemes using the annotated corpus. yg implemented and evaluated the automatic classification approach and set up and evaluated the results of the user test. the user test was carried out by us, jh, and is. all authors have read and accepted the final manuscript.

