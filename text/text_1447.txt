BACKGROUND
recently, technological developments have led to a situation where data analysts in different domains face data that are more and more complex. a special case of complex data are coupled data that consist of different data matrices for the same set of variables or experimental units. in systems biology, an example of matrices sharing the same set of variables is the study of the expression profile of a certain organism  on the basis of on the one hand different microarray compendia that can be downloaded from public repositories, and on the basis of, on the other hand, chip-chip or motif data  <cit> . an example of data matrices with shared experimental units are metabolomics data  gathered from different fermentations using mass spectrometry  with different ms data sets being available from different separation methods . in the first example each of the data matrices provides information on the same transcriptome and in the second example on the same set of metabolites, with some parts of the information being common for the different data matrices and some parts being specific: for example, gas chromatography mass spectrometry  and liquid chromatography ms  in general measure both a few classes of common compounds and many classes of compounds that are measured by one of the two methods only  <cit> .

a major challenge for researchers dealing with such coupled data, is to represent them in such a way that both shared and specific information as contained in the different data matrices is captured . for example, in the case of coupled gene expression and chip-chip data one may wish to retrieve modules of genes that have the same transcription factors and that are co-regulated under the same conditions, which is common information as contained in the transcriptome and chip-chip data matrices; in the metabolomics example, a coupled data analysis of gas and liquid chromatography ms data should allow to highlight the classes of compounds that are measured by both separation methods, as well as those that are measured by only one of them.

several tools are available that can be used for the analysis of coupled data. here we will focus on methods that simultaneously extract components from all data blocks. examples of such methods include sum-pca  <cit> , unrestricted pcovr , sca-p  <cit> , multiple factor analysis  <cit> , and statis  <cit> . whereas all these methods are based on the idea of a simultaneous component extraction they have been developed independently in different disciplines  and rely on different terminologies and mathematical frameworks. as a consequence, comparing them is not straightforward. the primary objective of this paper is to provide a structured overview in which all the methods fit, and to highlight their common core and particularities.

the paper starts by introducing some terminology to delineate the types of data to which the methods are applicable; then, a general framework is introduced that encompasses all the different simultaneous component methods and that frames them mathematically into a principal components setting. then, each of the methods is discussed with respect to this framework. an application is presented on simultaneous components analyses of gas and liquid chromatography ms data; in this application we compare the results obtained by applying the different methods and discuss how to interpret the results obtained by one of the methods .

methods
some terminology
in this paper, we are interested in multiblock  data consisting of at least two two-way two-mode data blocks that have one mode in common. two-way two-mode data denote rectangular data matrices where the term mode is used to indicate one of the sets of units that underlie the data, namely the set of row elements or the set of column elements. for example, a condition and a gene mode may underlie gene expression data while a condition and a metabolite mode may underlie mass spectrometry data. often the mode containing the experimental units , is called the object mode while the mode containing the variables  is called the variable mode. usually the data are organized such that the rows of the data matrix correspond to the different objects and the columns to the different variables. here, we will consider collections of coupled data matrices for which the shared, or linked, mode is the same for all couples of data matrices. figure  <dig> gives a graphical representation of two cases of such data: in the left panel, three data matrices that share the row mode are represented and in the right panel two data matrices that are linked by the column mode. it might seem a trivial issue to distinguish between these two cases as by transposing the data matrices one may move from one case to the other, but in the next section it will become clear that the results obtained from a method for data with a coupled object  mode may be different from those obtained with a method for coupled variable  data.

a structural overview of simultaneous component methods
in this section, first a general framework is presented that encompasses all simultaneous component methods; then each of the methods is discussed with respect to the general framework.

the general framework
the aspects that constitute the general framework are 1) the data to which the methods apply, 2) pre-processing steps included in the methods, 3) a general mathematical model for the data, 4) the objective function that is needed to estimate the model parameters, 5) identification constraints to obtain a unique solution for the parameters, and 6) the algorithmic strategy used to derive the model parameters .

we will follow the notation introduced in  <cit> , denoting vectors and matrices by bold upper and lower case letters respectively and using indexes for which the cardinality is indicated by the capital of the letter used to run the index. for example, the different data matrices will be indexed by k, with k running from  <dig> to k.

the data
as discussed in section  <dig>  simultaneous component methods are applicable to data consisting of at least two two-way two-mode data matrices that have one mode in common. the common mode can be either the object or the variable mode.

pre-processing
the data might be pre-processed in order to correct for irrelevant differences between variables or matrices. for example, when different measurement units are used to measure the variables the resulting differences in offset and scale might be accounted for by centering each variable  and by scaling to sum of squares or to range one  <cit>  ; see also  <cit> . coupled matrices may also differ in size such that the results of a particular statistical analysis may be dominated by the largest matrices; to correct for this effect, a possible strategy is to scale each data matrix to sum of squares one.

the model
assume k data matrices xk containing the scores of ik objects on jk variables. modeling each of these by a principal components structure with r components gives,

   

with wk  denoting a prespecified block weight, tk an ik × r matrix of component scores, pk the jk × r matrix of loadings, and ek the ik × jk matrix of residuals. in the general model, the fact that the k matrices are coupled is taken explicitly into account by subjecting  to the constraint that the model matrix that relates to the common mode is the same for all data matrices. in case of a common row mode  this introduces the constraint t <dig> = ... = tk = t such that the matrices are modeled by

   

with t denoting the common component scores and pk the matrix-specific loadings. similarly, for a common column mode , the constraint is imposed that p <dig> = ... = pk = p, so that the model becomes

   

with p <dig> = ... = pk = p the common loadings. this gives the general model

   

except for the equality constraint on the common component scores in  or on the loadings in , identification constraints  might be applicable but the latter do not affect the scores reproduced by the model, tk.

objective function
to estimate the model parameters tk and pk, the following optimization criterion is introduced:

   

under the restriction that either t <dig> = ... = tk = t  or p <dig> = ... = pk = p .

identification constraints
in general it holds for any nonsingular matrix b that if tk and pk are solutions of , tkb and pkb- <dig> are solutions too . examples of identification constraints often encountered in practice are that the component scores and loadings are expressed with respect to orthogonal axes in the direction of the highest variance  and that the component scores or loadings are orthonormal.

model estimation
optimal parameters that minimize  can be obtained either by a singular value decomposition  of the concatenated weighted data matrices or by a two-step approach in which first the common structure is found by an eigendecomposition of the sum of cross-product matrices and second the k companion matrices are found by means of a suitable regression analysis. both strategies can be used for data with a coupled object  mode as well as for data with a coupled variable  mode and they result in the same solution. we refer to the additional file  for details on both estimation procedures and a proof of their equivalence.

specific simultaneous component methods
in the previous section we have set up a general framework for a simultaneous component representation of data that consist of at least two matrices sharing a common mode. here, we will show how five published methods are specializations of the general framework. these are sum-pca  <cit> , unrestricted principal covariates regression , multiple factor analysis , statis  <cit> , and sca-p  <cit> ).

the general framework includes six aspects:  the data,  pre-processing,  the model,  the objective function,  the identification constraints, and  the algorithmic strategy. we will discuss the published methods with respect to the specific choices they make for the first, second, third, and fifth aspect. the fourth aspect  is not discussed because it is the same for all methods; the sixth aspect  is not discussed either, because the choice made with respect to this aspect does not affect the obtained solution.

specific simultaneous component methods
in the previous section we have set up a general framework for a simultaneous component representation of data that consist of at least two matrices sharing a common mode. here, we will show how five published methods are specializations of the general framework. these are sum-pca  <cit> , unrestricted principal covariates regression , multiple factor analysis , statis  <cit> , and sca-p  <cit> ).

the general framework includes six aspects:  the data,  pre-processing,  the model,  the objective function,  the identification constraints, and  the algorithmic strategy. we will discuss the published methods with respect to the specific choices they make for the first, second, third, and fifth aspect. the fourth aspect  is not discussed because it is the same for all methods; the sixth aspect  is not discussed either, because the choice made with respect to this aspect does not affect the obtained solution.

sum-pca
a first published method that fits within our general framework is sum-pca. a confusing element though, is that the name sumpca has been used for two different methods: this paragraph bears on sum-pca  proposed in the chemometric literature  <cit>  and not on sumpca proposed in the psychometric literature  <cit> ; we will come back to the latter method when discussing sca-p.

• data

sum-pca  <cit>  was developed for data that are linked in the row mode.

• pre-processing

the pre-processing steps consist of first autoscaling the data per variable and secondly, scaling each data block to sum of squares equal to one  <cit> .

• model

sum-pca models the k two-way data blocks by the following mathematical structure,

   

which implies pre-specified weights wk =  <dig>  note, however, that the pre-processing step of a scaling of each block to sum of squares one applied to autoscaled data, is equivalent to using weights on the autoscaled data; this means blocks with more variables are downweighted more than blocks with fewer variables in order to avoid that larger blocks dominate the solution.

• identification constraints

the sum-pca model  is estimated under the identification constraints of a principal axes orientation and orthonormality of the common component scores: ttt = i. the latter constraint is unusual in chemometrics where it is common practice to have orthonormal loadings. however, it implies that the method finds exactly the same global scores as cpca-w, a method that extracts the components in a sequential way .

unrestricted pcovr
• data

principal covariates regression  was proposed for the analysis of data consisting of a matrix of dependent variables x <dig> and a matrix of independent variables x <dig> for the same set of objects  <cit> .

• pre-processing

in  <cit> , different options are mentioned for pre-processing on the level of the variables, including centering, scaling and autoscaling.

• model

principal covariates regression represents the data by means of the following model,

   

with the common component scores t <dig> being restricted to belong to the column space of x <dig> and with  <dig> ≤ β ≤  <dig>  such a restriction does not fit within the general framework as outlined above; yet, in the more general context of multiway covariates regression, also an unrestricted model has been proposed in which this restriction was dropped ; clearly, the unrestricted pcovr model fits within the general framework. the pre-specified weight is determined by a strategy that minimizes the cross-validation error for the prediction of x2; this introduces some asymmetry in the treatment of x <dig> and x <dig>  both simulation studies  and empirical results  <cit>  yielded unsatisfying results for the proposed cross-validation approach because it results in β-values close to zero or one, implying that all weight is placed on a single matrix.

• identification constraints

in  <cit> , it is suggested to constrain the common component scores to be column-wise orthonormal: ttt = i.

mfa
• data

multiple factor analysis or mfa  <cit> , also known as analyse factorielle multiple or afm, was proposed in the french literature as a method for the analysis of data consisting of several sets of variables for the same group of subjects . recently, the method was applied to integrate distinct omics data  <cit> .

• pre-processing

the variables are supposed to be autoscaled.

• model

multiple factor analysis is based on the following model,

   

with σk <dig> being the largest singular value of xk. the choice of the inverted σk1's as the matrix-specific weights is motivated by the fact that in this way two corrections take place at once: one for differences in the number of variables and one for the redundancy of the information contained by the data matrices. this can be understood by observing the following properties of the eigenvalues:  the size of the matrix can be measured by  ;  redundancy can be measured by the proportion of vaf by the first component, . so, matrix xk can be corrected for size and redundancy by

   

which is the correction used by mfa. note that scaling the matrices  is of no influence on the mfa results as the singular value of f times xk equals fσk and the first step of mfa is to divide each block by this singular value.

• identification constraints

mfa estimates the parameters under a principal axes constraint and orthonormality of the component scores: ttt = i.

statis
• data

statis was proposed in the french literature  <cit>  . in both publications, three-way data are used but the authors note that the method is also applicable to coupled two-way data matrices with a common object mode.

• pre-processing

in  <cit> , nothing is mentioned on pre-processing. centering and scaling to unit variance are mentioned as options of the statis  method in  <cit> ; a weighting to account for differences in size of the matrices is described in  <cit> .

• model

statis is based on the following model,

   

with ak being the weight associated to the kth data matrix. these weights are obtained from the first component of the pca of a matrix which is derived from the data in the following way: 1) derive the cross-product matrices sk =  . 2) construct the matrix f of size n <dig> × k by inserting the vectorized matrix sk, formally written as vec, in the kth column. the weights ak are the loadings on the first principal component of f and can be obtained from the first right singular vector of f or from an eigendecomposition of the matrix s with values  t vec so s = ftf). larger weights can be expected for:  data matrices with larger values ),  larger matrices,  data matrices with more covariation between the vectors of object scores, and  matrices with more similar cross product matrices to other matrices . the latter property is the motivation for the proposed weighting strategy: statis wants to find a compromise of the different cross-product matrices sk. this idea of a compromise or a consensus is prominent in the analysis of three-way data and underlies generalized procrustes analysis  and generalized canonical correlation analysis  <cit> . see  <cit>  for a comparison of statis and gpa.

• identification constraints

statis looks for simultaneous components with a principal axes orientation and under the restriction of orthonormal loadings .

sca-p
• data

in the psychometric literature, sca-p was proposed as a method for the analysis of multiple data matrices obtained by measuring the same set of variables in different groups; the variable mode is therefore considered to be the common mode  <cit> . the acronym sca-p stands for simultaneous component analysis with a common pattern matrix, where "pattern matrix" is a psychometric term for "loading matrix".

• pre-processing

in the original applications of sca-p, where the main interest is to account for variation within the matrices xk, it is recommended to autoscale each variable per data block  <cit> . another proposed strategy is to center each variable per matrix and to scale the variables to sum of squares ∑kik for the concatenated data, this is over matrices  <cit> . the latter strategy preserves the variability within the matrices xk. in the remainder of the paper we will refer to sca-p with a variable-wise autoscaling per block.

• model

the following model is used to structure the data,

   

with p a common loading matrix. the different blocks are not explicitly weighted but larger blocks will not contribute more to the common components given that the variable mode is common and that each variable has been autoscaled: this can be understood by observing that p can be found by the eigendecomposition of  with rk the matrix of correlations between the variables in data matrix k. indeed, this is the method denoted as sumpca  <cit> , hence sca-p is equivalent to sumpca and also to levin's method  <cit>  that minimizes

   

• identification constraints

sca-p looks for simultaneous components that have a prinicipal axes orientation and orthonormal component scores, tconc= i, with tconc representing the matrix of concatenated block specific loadings).

reflection on the general framework
we presented a general framework for simultaneous component methods and showed how several published methods fit within the framework. in this section we will reflect on the comparability of the simultaneous component results when making a specific choice for different aspects of the general framework. in table  <dig> a summary overview of the published methods discussed in the methods section is given in function of the specific choices they make with respect to the four aspects as discussed above: 1) the mode that is considered common , 2) the pre-processing steps, 3) the pre-specified matrix-specific weights, and 4) the identification constraints. to arrive at a deeper understanding of the relations between the different published methods below we will first consider each of these aspects and the different choices made by the published methods with regard to them; second, we will discuss more in detail those aspects for which the choices are consequential for the obtained results.

structured overview of the methods
a first aspect concerns the labeling of the common mode as the object or as the variable mode. ignoring for a moment what happens on the level of pre-processing and weighting but taking into account whether the orthonormality restriction is imposed on the common mode or on the other mode, changing the label of the common mode from object to variable or vice versa makes that statis turns into sca-p or vice versa. second, with respect to pre-processing, all methods except statis autoscale each variable. furthermore, sum-pca additionally also scales each matrix to unit sum of squares . third, except for sca-p, all methods use a distinct matrix-specific weighting strategy . fourth, all methods use a principal axis identification constraint but differ in their choice of putting an orthonormality constraint either on the common structure  or on the concatenated structure ; as discussed previously, however, this affects the scaling of the components only, and yields the same reconstructed data.

consequential differences between the methods
next, one may wonder for which of the aspects as discussed above the choices may be consequential for the obtained results. a first such aspect is the type of pre-processing, with all methods except statis relying on a variable-wise autoscaling. this means that the solutions obtained with statis can be dominated by the variables with the largest sums of squares; for that reason, these solutions can be very different from the results as obtained with the other methods. a second possibly consequential aspect is the matrix-specific weighting strategy. this can strongly influence the obtained results, with as an extreme all weight being put on a particular matrix which results in a structure of the common mode  that equals the structure obtained from the separate component analysis of that matrix. note that in case the r dominant directions of the different data matrices span approximately the same subspace, a matrix-specific weighting will have little impact on the obtained results . third, although the labeling of the common mode has no direct consequential effect, it may have important indirect consequences through what happens on the level of pre-processing. indeed, methods that perform autoscaling do this in the direction of the variables: labeling the common mode as 'variables' results in autoscaling in the direction of the common mode while labeling it as 'objects' results in autoscaling in the other direction.

furthermore, matrix-specific weighting operates differently for autoscaled data with a common object mode than for autoscaled data with a common variable mode. to understand the latter, let the row elements  be common, such that the common structure  can be derived from  when the row mode is the variable mode xk is a correlation matrix and when the row mode is the object mode it will be a cross-product matrix. now, as the size of a correlation does not depend on the sample size, in case of a common variable mode larger matrices will not necessarily have a larger contribution; on the other hand entries of cross-product matrices may take larger values in case of larger matrices. as a consequence, in case of a common object mode, larger matrices may contribute more to the sum  hence in such a case, unlike in a case with a common variable mode, a downweighting of larger matrices may be desired. note that in the description of the methods used here, we are strict in the choices made on the level of pre-processing. the methods, however, could also be defined on the basis of the matrix-specific weighting strategy apart from any pre-processing strategy. then, the labelling of the common mode is not consequential .

reflection on pre-processing and weighting
the proposed pre-processing and weighting strategies reflect data-analytic concerns to correct for possible disturbing factors on the level of the variables and on the level of the matrices. first, on the level of the variables a disturbing factor may stem from different variables being measured by different measurement techniques or with different measurement scales. often such differences may be irrelevant from a substantive perspective but they are essential and may distort the results, which may necessitate a suitable type of preprocessing. now statis as original published  <cit> , unlike all other published methods does not autoscale variables; therefore, in case of irrelevant differences between variables in measurement scale, statis may not be used or only in combination with a preliminary centering, scaling, or autoscaling of the data as proposed in later publications on the method  <cit> .

second, on the level of the different matrices a major concern may lead to combine them in a fair way. different possible principles of fairness can be considered; an overview is given in table  <dig>  a first  principle may be to give equal weight to all matrices . more sophisticated principles imply the use of matrix-specific weights to correct for possible unwanted dominance of some matrices. a first such principle is to attach more weight to smaller matrices in order to avoid that large matrices dominate the solution. second, more weight may be given to matrices that contain more heterogeneous information in order to avoid that the solution is dominated by redundancies. third, one may wish to give more weight to matrices containing more stable predictive information, in order to avoid that the solution is steered by particularities that cannot be replicated. fourth, one may wish to give more weight to matrices that have more in common with other matrices to avoid that idiosyncracies dominate the solution. the different published methods aim at specific principles .

it can be useful to apply these principles of fairness in a flexible way. for example, different principles may be combined when there are different reasons for unwanted dominance of a matrix. this is the case for the weighting strategy underlying mfa; another example is a modified weighting strategy for statis that corrects for differences in sizes between the matrices and also gives more weight to matrices that have more in common with other matrices  <cit> . also, different principles can be used for different data matrices. for example, suppose that a particular matrix is very noisy, that all matrices have equal sum of squares, and that interest is in avoiding redundancies. in the noisy  block all singular values will be almost equal such that applying the principle of more weight for the less redundant matrices will give most weight to the noisy data block. in such a case one may consider to use the principle of more weight for the matrices with more stable predictive information in order to give less weight to this particular block, and to additionally use the principle of avoiding redundancies on the other blocks.

RESULTS
in this section we will apply each of the existing methods to an example biological dataset. first the data are described, then each of the methods is applied and the solution obtained with mfa is interpreted.

description of the data
the phenylalanine production in the escherichia coli nst  <dig> and a wild type strain was studied with the aim to identify bottlenecks in the production of this compound  <cit> . metabolomes were screened at different fermentation times and obtained under various environmental conditions using both gas chromatography  and liquid chromatography  in combination with mass spectrometry . in general gc/ms and lc/ms methods are known to detect different classes of chemical compounds, although some classes are detected by both methods  <cit> . the data consist of two coupled data matrices: a gc/ms matrix with the measurements of  <dig> metabolites  and a lc/ms matrix with the measurements of  <dig> metabolites, both for the same  <dig> samples of e. coli ; no metabolite was measured on both platforms, hence, there is no trivial overlap. in the data considered here, only those metabolites that were detected in at least  <dig> percent of the experiments were used; furthermore, the data were manually curated and normalized as described by  <cit> . measurement values below the detection threshold were set equal to one half of the smallest detected value  <cit> :  <dig>  of the gc measurements and  <dig>  of the lc measurements were below the limit of detection. due to a lot of values below the detection limit for a few metabolites in the gc data, some extreme outliers were observed. to deal with skewness and asymmetry, all values were log-transformed. no influential outliers were found among the log-transformed values.

application of different simultaneous component methods
the different published simultaneous component methods were applied to the coupled gc/ms – lc/ms data. due to the asymmetry associated to the cross-validation procedure in pcovr, we report two unrestricted pcovr analyses denoted as pcovr gc and pcovr lc with the former denoting a leave-one-out crossvalidation based on using the observed scores in the lc data to reproduce the scores of the gc data and the latter a crossvalidation that uses the observed gc data to reproduce the lc data. we also report the analysis based on the unweighted but variable-wise autoscaled data. this can be considered to be sca-p applied to the transposed versions of the data matrices.

an overview of the relative matrix-specific weights, calculated as the matrix-specific sum of squares divided by the sum of squares of the concatenated data, obtained with the different simultaneous component methods is given in table 3: for ease of comparison, these are expressed as the sum of squares of the specific weighted matrix divided by the total sum of squares over all weighted matrices. for the first method reported  all weight is put on the gc matrix; such extreme weights could be expected on the basis of what has been reported in the literature . also statis uses extreme weights with almost all weight put on the gc matrix; this can be mainly attributed to the fact that the gc matrix is much larger than the lc matrix. the relative weights obtained for sca-p purely reflect the size difference, with the gc matrix being approximately three times as large as the lc matrix. mfa also puts more weight on the gc matrix but less than sca-p because the weights used by mfa  correct both for size and redundancy: this reflects the larger heterogeneity in the gc matrix . sum-pca gives equal weight to both matrices as it weights all specific matrices to unit sum of squares. finally, the pcovr case where the cross-validation approach relies on reproducing the lc scores based on the observed gc scores, puts all weight on the lc data.

1: matrix-specific weights obtained by leave one out crossvalidation using the observed scores in the lc data to reproduce the scores of the gc data  or the other way around .

these weights have been calculated as the matrix-specific sum of squares divided by the sum of squares of the concatenated data.

to explore how dissimilar the common structure can be for the different methods , we calculated tucker's coefficient of congruence ϕ for pairs of matrices t, according to the definition below. for r components, tucker's coefficient of congruence between two matrices of component scores x and y is calculated as follows  <cit> :

   

with tr denoting the trace. . ϕ can be interpreted as an uncentered correlation and takes values between minus one and one . it is invariant under scaling but not under rotation of the matrices x and y. to account for the fact that pca is invariant under reflection and rotation, but ϕ not, we will calculate  after applying a procrustean similarity transformation without the translation step  <cit> . except for statis, all methods are applied to the matrices obtained after autoscaling per metabolite; therefore we included the separate component analyses of the thus preprocessed data as a reference. the modified rv coefficient  <cit>  calculated between preprocessed gc and lc matrices equals . <dig>  which is a low value such that the different weighting strategies can be expected to yield different results. table  <dig> gives an overview of tucker's congruence between the common scores obtained with the different simultaneous component methods and with the separate component analyses of the gc matrix and of the lc matrix . a low congruence can be observed between statis and the other methods due to the fact that it relies on a different way of pre-processing. the two pcovr methods, which put all weight on either gc or lc, are perfectly congruent with gc and lc respectively and by consequence also have the same congruence with the other methods. for the methods applied to the data that are autoscaled per metabolite, it holds that methods that put more weight on gc/lc have higher congruence with the separate analysis of gc/lc respectively. when interest is in finding a simultaneous solution that is congruent with both separate analyses, sca-p, mfa or sum-pca seem the methods of choice .

1: ordinary principal component analysis of gc data only

2: ordinary principal component analysis of lc data only

3+4: matrix-specific weights obtained by leave one out crossvalidation using the observed scores in the lc data to reproduce the scores of the gc data  or the other way around .

interpretation of the multiple factor analysis solution
in this section we will interpret the solution obtained with multiple factor analysis . our motivation to choose this specific method is  that we want to attach equal importance to the biological processes behind the metabolites found by both types of separation methods so we need to correct for the difference in size between the matrices, and  that we do not want processes related to the many nucleotides in the lc data to dominate the solution. from figure  <dig> it can be derived that five components are needed because the first five components account clearly for more variance than the next five in at least one of the data matrices.

for the interpretation of the components, we will first take a look at the scores of the samples on the components. we made use of the rotational freedom to rotate the scores to a simple structure using the varimax criterion : this is a rotation that targets a structure with high scores on only one component and low  scores on the other components. table  <dig> displays the resulting scores and, on the last three lines, the variance they account for overall  and in each of the data blocks . a description of the experimental design can be found in  <cit> ; here it is summarized by the first column that labels the experiments in relation to the reference condition  and by the second column that reports after how many hours the samples were taken from the bioreactor. taking a look at the two last lines of table  <dig>  we see that the first two components are involved in both types of separation methods while the third and fifth component seem to be specific for lc and the fourth for gc. furthermore, the information captured in the different simultaneous components  showed a clear link with the environmental conditions under which these samples were generated. for instance, based on this analysis sc <dig> seems to comprise metabolic processes related to oxygen limitation and to the early stationary growth phase  which might be the result of oxygen stress and sc <dig> captures metabolic processes related to succinate catabolism . this analysis also suggests that the metabolic processes related to mid-logarithmic  to early stationary when grown at a higher ph or at a lower phosphate concentration than under the reference condition are similar: the samples taken under these conditions have a high component score on both sc <dig> and sc <dig>  moreover, the lc specific components are dominated by the samples taken from the fermentation performed with the wild type strain and that were performed at an elevated ph compared to the reference fermentation condition.

in the part with component scores, the first column describes the experiments in relation to the reference condition and the number of hours the samples were in the bioreactor. component scores ≥ . <dig>  are in boldface.

discussion
in this paper we proposed a general framework for simultaneous component data integration methods based on six aspects:  the data,  pre-processing,  the model,  the objective function,  identification constraints, and  the algorithmic strategy. for two of these aspects there are no choices to be made that yield different results . for the four remaining aspects, we discussed for each of the published simultaneous component methods  the specific choices they make. we further discussed which choices imply significant consequences for the obtained results:

 <dig>  labeling the common mode as object versus variable is consequential for the obtained results only when considered in combination with pre-processing .

 <dig>  pre-processing can have a strong influence on the obtained results. this is the case when the variables are measured on different scales or when the block-specific sum of squares differ substantially. in the former case a single/few variables can dominate the solution, in the latter a single/few matrices. applying a simultaneous component analysis to data corrected for such differences may yield very different results from an analysis on the uncorrected data.

 <dig>  in general, different matrix-specific weighting strategies lead to different results. the different simultaneous component methods each proposed specific weighting strategies based on different principles of fairness. as illustrated, these strategies may yield very different weights ranging from one extreme  to the other .

 <dig>  the identification constraint to impose orthonormality on the common versus on the concatenated model structure affects the scale of the component scores and loadings but it is not consequential and yields the same reproduced scores.

CONCLUSIONS
summarizing, of the four aspects that differentiate the simultaneous component methods, the choices made with respect to pre-processing and weighting are consequential; this was also illustrated by applying the different simultaneous component methods to two coupled metabolomics data matrices. especially, the type of weighting of the different matrices is essential for simultaneous component analysis. these types were shown to be linked to different specifications of the idea of a fair integration of the different coupled matrices. a summary overview of these principles  may be of help to the data analyst in choosing an appropriate weighting scheme for the analysis of a data set at hand. as discussed, such a weighting scheme may be based on a flexible integration of different principles of fairness.

abbreviations
gc: gas chromatography; gpa: generalized procrustes analysis; lc: liquid chromatography; mfa: multiple factor analysis; ms: mass spectrometry; pcovr: principal covariates regression; pls: partial least squares; sca-p: simultaneous component analysis with a fixed pattern matrix; statis: structuration des tableaux à trois indices de la statistique; svd: singular value decomposition.

authors' contributions
kvd carried out the literature study, performed the statistical analyses and drafted the manuscript. aks coordinated the statistical analysis of the metabolomics data. mjvdw provided these data and performed the biological interpretation. aks, halk, ivm, and kvd conceived of the study, and participated in its design and coordination. aks, halk, mjvdw, and especially ivm helped to draft the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
estimation of the simultaneous component scores and loadings. estimation.pdf describes how optimal parameters can be obtained either by a singular value decomposition of the concatenated weighted data matrices or by a two-step approach. it also contains a proof of their equivalence.

click here for file

 acknowledgements
this work was supported by the research fund of katholieke universiteit leuven . we wish to thank the anonymous reviewers for helpful comments.
