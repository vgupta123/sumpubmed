BACKGROUND
sequencing projects and expressed sequence tags  are essential for gene discovery, mapping, functional genomics and for future efforts in genome annotations, which include identification of novel genes, gene location, polymorphisms and even intron-exon boundaries. the availability of high-throughput automated sequencing has enabled an exponential growth rate of sequence data, although not always with the desired quality. this exponential growth is enhanced by the so called "next-generation sequencing", and efforts have to be made in order to increase the quality and reliability of sequences incorporated into databases: up to  <dig> % of sequences in nucleotide databases contain contaminant sequences  <cit> . the situation is even worse in the est databases, where vector contamination rate reach  <dig> % of sequences  <cit> . hence, improved and user friendly bioinformatic tools are required to produce more reliable high-throughput pre-processing methods.

pre-processing includes filtering of low-quality sequences, identification of specific features , removal of contaminant sequences  and trimming the undesired segments. there are some bioinformatic tools that can accomplish individual pre-processing aspects , and other programs that cope with the complete pre-processing pipeline such as pregap <dig>  <cit>  or the broadly used tools lucy  <cit>  and seqclean  <cit> . most of these require installation, are difficult to configure, environment-specific, or focused on specific needs , or require a change in implementation and design of either the program or the protocols within the laboratory itself. moreover, it is not always possible to connect them easily with further processing tools for annotation or assembling. there are web implementations  that start with pre-processing and end with assembling and/or annotating ests, but no web page is devoted exclusively to pre-processing. further, these implementations are focused more on annotation than on a correct pre-processing, and tend to disregard the fact that poorly pre-processed sequences will produce effectively useless annotations.

this paper describes seqtrim, a software tool containing a flexible pipeline that successfully deals with pre-processing of any sequence read. its performance is compared with other broadly used applications, and when using high-throughput datasets.

implementation
seqtrim has been programmed in perl  <dig>  using bioperl libraries, can be executed as a command line tool or as a web tool http://www.scbi.uma.es/seqtrim, and tasks are queued to a hp-superdome computer. the command line version is more suitable for automatic batch processing, workflows, or high-throughput analyses, while the web interface is more appropriate for user interactivity. it makes use of the external programs phred  <cit>  for obtaining sequence and quality values, blast to compare sequences, and repeatmasker to mask repetitions and low complexity regions. this will work in any unix/linux release, including osx.

provided that the dependencies are installed, uncompressing seqtrim in /usr/local  is sufficient to make it operable. configuration parameters in the seqtrim directory are customisable by the user, transiently or permanently: working parameters can be permanently modified by editing the 'seqtrim.conf', or changed for a single run via command-line options or the web interface. the seqtrim directory also contains the necessary databases, an editable file called 're_sites.txt' that contains the usable restriction sites, and another editable file named 'adaptorseqs.txt' which contains a list of default adaptor sequences. database modification is achieved simply by adding or removing sequences in fasta format in the seqtrim/db directory. before each execution, seqtrim verifies if something has been added to databases for incorporation of new sequences thereafter.

the pipeline underlying seqtrim runs through four independent and interchangeable processes  plus two optional ending steps  . one of the main strengths of seqtrim is that, even if a default pipeline order is provided, users can change the flow completely, or can skip one or more steps.

execution time for a single sequence will depend on the complexity of the given sequence. for example, seqtrim  takes  <dig>  s/read in a  <dig>  ghz intel core  <dig> duo processor and  <dig>  s/read in a  <dig>  ghz itanium  <dig> processor when analysing  <dig> est reads  with an average length of  <dig> nt. when it has been tested  with  <dig> gs-flx pyrosequencing reads with an average length of  <dig> nt, execution takes  <dig>  s/read and  <dig>  s/read, respectively, in the processors mentioned above. a true high-throughput use of seqtrim must consider use of the command line version, since the web interface facet becomes unsatisfactory when showing jobs with more than  <dig>  reads; the browser taking  <dig> s to reply to a single click. however, the seqtrim web server is able to process up to  <dig>  reads without "hanging", safari always reacting faster than firefox.

algorithm
the recommended seqtrim pipeline starts with vector detection using vector libraries and the removal of special features. the next step involves to trimming low quality 5'- and 3'-tails. finally, any sequence coming from a contaminating source is removed. two optional end steps are focused on removing any other experimental artefacts arising from molecular modifications and the masking of low complexity regions.

input and output
seqtrim accepts usual sequence formats: fasta file with or without the corresponding quality value file, phd format file from phred, and chromatograms, all of them with any number of sequences and optionally compressed in zip format. text files are directly processed, but when input sequences are chromatograms, the external program phred is employed to obtain the quality value file. it must be understood that phred's low quality trimming option is disabled for such a conversion. since the first word in the description line of every input sequence is considered its identifier, checks for sequence name duplications, as well as consistency between the sequence file and the quality value file , are performed.

several output formats  can be obtained either from the web interface or the command line. nucleotides whose quality value  is not greater than  <dig>  are changed to lowercase. a coloured output of each sequence can also be seen on the screen, either using the command line or the web interface , which is intended to help users in the evaluation of pre-processing results. results will be stored for at least one month in the web server using an e-mail address as identifier since no account is needed for seqtrim usage.

vector cleaning and removal of specialised features
the recommended first pipeline step starts by detection of cloning vector by means of ncbi's univec and the embl's emvec vector/adaptor libraries using blast with relaxed parameters  to account for higher error rates at the beginning and end of reads. users do not need then to specify the cloning vector. blast alignment is parsed to identify regions that correspond to vector sequences, even if these regions are spliced into smaller dna fragments that match in opposite orientations. seqtrim is designed to locate cloning restriction sites only when cloning vector was not identified. cloning restriction sites must be entered in the parameters panel of seqtrim.

next is location of special features  appearing in many sequences. special features must be removed since they  provide false sequences,  mislead assembling or clustering algorithms that can be further used with these sequences, and  mislead researchers who use these contaminated sequences. adaptors are located with blast2seq, customising its parameters for short sequences . poly-a and poly-t tails are detected by the function findpolyats  developed by the authors, which includes the removal of one or more a's at the 3' end of a sequence. the algorithm has been set to produce maximal sensitivity while maintaining a very strict rule for false positives: sequences that are not compliant with all necessary criteria are always rejected. in the case of ests, insert orientation is detected by the presence of poly-a or t tails, and chimeric inserts are determined by the concomitant presence of two of these tails in the same sequence. poly-a or poly-t detection is skipped if input sequence does not come from a cdna, in order to gain cpu time. in the parameters panel, users can specify adaptors, dna source  and orientation  of inserts.

quality trimming
a sequence read can contain bases of very low quality, which can mislead further processing. therefore, the base-calling quality assessment for each nucleotide is taken into account to trim the original sequence, in order to obtain the longest sequence with the highest quality. in cases where the input sequences are in a text fasta format and low quality nucleotides are expressed by n's , seqtrim can extract the largest subsequence containing less than 18% of indetermination. values for assessing sequence quality can be changed in the parameter panel. since not all sequences include n's, quality trimming is split into two independent steps  in order to enable users to skip the useless function. trimming by qv is automatically skipped when input sequences are not chromatograms or a quality value file is not entered. stringency of qv trimming can also be changed by means of the parameter panel.

removal of contaminant sequences
during the experimental process, cloned sequences can result from contamination sources such as dna from the escherichia coli genome, cloning vector, cell plasmids, organelles, viruses, yeast, or humans . screening of contaminant sequences is based on blast comparisons with trimmed sequences against a database of likely contaminants using default parameters and an expected cutoff fixed to 1e- <dig> and a minimal score of  <dig>  the blast result is analysed by the in-house algorithm findcontaminantseqs  to establish when hits are real contaminations. users can vary the stringency of this analysis by changing the minimal length of sequence considered as a significant contaminant in the parameter panel. seqtrim is distributed with the genomes of e. coli, saccharomyces cerevisiae, lambda phage and several mitochondria. more databases can be added  by the user. length, position and identity of the contaminant sequence are returned for user information. the vector database is re-screened again at this moment, as well as adaptors, which serves to identify putative chimeras.

removal of other artefacts
this optional step is intended to be performed at the end of pre-processing, since it is focused on removing any experimental artefacts introduced in the apparently cleaned insert that are owing to molecular modifications. for example, extensions introduced by the terminal transferase enzyme, the n's and/or x's at both ends, and the t's from the 5'-end and/or a's from the 3'-end are discarded here. these removals are accomplished by means of the in-house function look_for_artefacts .

masking low complexity regions and repeats
this is the last step of the seqtrim pipeline, in which the unwanted sequence is not removed but masked, since low complexity regions and repeats are part of the real sequence, even if they can mislead further computer analysis. low complexity regions due to simple nucleotide repeats are masked by the in-house function lowcomplexitymasking . repeats in nucleotide sequences are masked by employing repeatmasker using species-specific repeat libraries obtained from the repbase  <cit> . the searching algorithm for repeatmasker has been fixed to wu-blast in order to reduce the time spent in one sequence analysis.

rejection criteria
a pre-processed sequence is finally rejected if it complies with one the following criteria:  there is no insert between identified cloning vector boundaries;  the usable sequence is not long enough ;  there are possibly two inserts ;  the whole insert was masked. in contrast to other pre-processing algorithms, absence of cloning vector is not a reason for rejection since sometimes useful sequence starts beyond vector boundaries, or there can exist vector rearrangements that make it unrecognisable although the insert is preserved .

RESULTS
developer versions of seqtrim have been used for some years at málaga university using actual data from various previous studies such as ests from xylem tissue of pinus pinaster  <cit> , ests from photosynthetic tissues of pinus sylvestris , ssh gene libraries from pine , or assembling bac sequences . now, a collection of comparative analyses are able to demonstrate that seqtrim outperforms other pre-processing software and that it is able to handle huge amounts of sequence.

comparison with other algorithms
seqtrim performance has been compared to other widely used pre-processors such as seqclean , lucy <dig>  <cit>   and estprep  <cit>  . although cross-match is a restricted smith-waterman algorithm and has been incorporated into some est processing packages, it has been discarded because it does not remove but masks vector-like regions, takes too much time to execute, and is not better than seqclean or lucy  <cit> . since ests are the only kind of sequence that can be used in all programs, a collection of  <dig> est chromatograms obtained in our laboratory  <cit>  was used as the testing sequence set. these reads resulted in  <dig>  nucleotides, of which  <dig> % were considered insert by estprep,  <dig> % by seqclean,  <dig> % by lucy and  <dig> % by seqtrim. the sequence reads had an average length of  <dig> nucleotides but, once pre-processed, the average insert size was  <dig> for estprep,  <dig> for seqclean,  <dig> for lucy and  <dig> for seqtrim. both kinds of data clearly show that seqtrim renders the shortest sequences; in fact, seqtrim provides the shortest final sequence in  <dig> cases, the second-shortest in  <dig> cases, the third-shortest in only two cases, and never provides the longest sequence.

even with shorter sequences, seqtrim is able to retain more information about trimmed sequences than the other programs . with the default pipeline, seqtrim is able to detect  the presence of cloning vector at the 5'-end and the existence of a poly-t segment, which indicates that this sequence was cloned in reverse. hence, seqtrim can return a reverse complement of such a sequence in order to acquire it in the same orientation as the others. if quality trimming had been performed in the first instance, the poly-t would have been removed and the researcher would have recovered a trimmed sequence in the correct orientation. as can be seen in the details of fig.  <dig>  the user is informed that this sequence was reversed.

with regard to the number of passed/rejected sequences , estprep validates  <dig>  lucy  <dig>  seqclean  <dig> and seqtrim  <dig>  lucy is therefore the most restrictive, with seqtrim nearly as restrictive, and seqclean and estprep each returning a similar result and being the most permissive. equivalent outcomes were also derived when assessing the number of sequences instead of the number of nucleotides. concordance among the algorithms was tested by assessing the number of sequences accepted and rejected. the four software programs agree in  <dig> sequences , which correspond to  <dig> % of sequences. if agreement is relaxed to three coincidences, the concordance rate increases to  <dig> %. seqtrim is primarily consistent with estprep and seqclean , with estprep and seqclean showing only slightly less consistency between them . seqclean ist not as consistent with lucy , an lucy disagrees similarly with estprep and seqclean .

agreement between the most coincident softwares  was cross-verified. when sequences trimmed by seqtrim were entered into seqclean, no changes were observed, indicating that both items of software remove the same features. on the contrary, when sequences trimmed by seqclean were entered into seqtrim, most of them were slightly shorter mainly due to adaptor removal that seqclean did not detect, although sometimes differences were related to low quality sequences, or chimeric sequences, that were not removed by seqclean .

trimming accuracy
the fact that seqtrim apparently provides the shortest sequences could be explained by over-trimming. testing against some "gold-standards" was therefore conducted to determine trimming accuracy. a set of  <dig> artificially obtained sequences of  <dig> nt long from p. pinaster genomic dna were enlarged with known vector and/or adaptor sequence at the 5'-end of the insert, and nothing and/or poly-a and/or adaptor and/or vector sequence at the 3'-end . overall, those datasets of artificial sequences simulate  <dig> dna cloning events into bamhi-hindiii cloning sites of pbluescript-fl with or without adaptors, and having or not reached the 3' cloning vector in the sequencing process. they simulate different cloning events handled by seqtrim with a precise knowledge of insert start and end points. hence, accuracy of seqtrim trimming can be examined, except trimming based on quality values .

vu,  <dig> nucleotides preceding the bamhi restriction site of pbluescript-fl. vd,  <dig> nucleotides following the hindiii restriction site of pbluescript-fl. i, a fragment of  <dig> nucleotides from pinus pinaster genomic dna. pa, poly-a tail of  <dig> a's. au, upstream 5'-adaptor, containing the sequence gatccgttgctgtcgtcg. ad, downstream 3'-adaptor, containing the sequence cggccgcgtcgacaagct. 'expected' corresponds to theoretical mean values for each set of artificial sequences. 'obs ' are the mean values obtained using seqtrim with the cloning restriction sites specified. 'obs ' are the mean values obtained using seqtrim with no cloning restriction site specified.

launching seqtrim with the corresponding parameters and restriction sites provided results shown in table  <dig>  columns 'obs '. rejection and incorrect processing of several sequences was not unexpected since all of these corresponded to inserts containing one of the cloning restriction sites within it. note that a real experiment would always result in inserts without cloning sites within it, except when partially digested dna or chimeric inserts were cloned, and in actuality, finding a cloning site within an insert is a reason to discard such an insert. as restriction site use is limited to cases where the cloning vector was not identified , the same sequences were analysed without specification of restriction enzymes ' in table 1) showing that the observed mean positions and lengths were almost identical to what was expected, and that no sequence rejection occurred. a manual inspection of results revealed that the small differences found  corresponded to chance instances in which the first nucleotide of the insert is the same as that following the bamhi site or preceding the hindiii site of the cloning vector. in the cases were the last insert nucleotide is an a, manual inspection revealed that it was removed with the poly-a tail. in conclusion, precision of seqtrim clips is virtually guaranteed and no over-trimming is found.

performance with high-throughput reads
sequencing projects have become a true high-throughput process with the advent of next-generation sequencing. it is of interest to test if very large-scale sequencing approaches can be processed using seqtrim with the same previously described precision. thus,  <dig>  random reads from various different organisms from the ncbi trace archive were selected , although no information was found regarding the pre-processor used to obtain the final clips. the first remarkable finding is that seqtrim rejected  <dig> reads in the worm caenorhabditis elegans and  <dig> in the plant arabidopsis thaliana  mainly due to the presence of two inserts in the sequence. the surprisingly high number of rejected reads in the worm is explained by the low quality sequences considered in the dataset. since human sequences were annotated as 're-sequencing', they were treated as genomic dna and only  <dig> reads were rejected; since no restriction site was defined in seqtrim parameters because the repository does not mention it, no read will be rejected by reason of having two inserts. another significant aspect is that some sequence reads were assessed as being too short to be useful. only a few reads  contained contaminant sequences . it is clear that the c. elegans reads do not have the same standard of quality as the others. it can be also inferred that seqtrim is able to detect cloning or sequencing artefacts that would not provide useful sequence for researchers in any kind of sequencing approach.

concerning trimmed nucleotides, fig. 4b shows the distribution of those removed due to low qv or because they were n's, cloning vector, or a specialised feature. the reads from the three species contain a high number of nucleotides discarded for having a low qv . these high values can be explained by the fact that the seqtrim qv cutoff is more stringent than that of other pre-processors, although stringency can be changed in the parameters panel . additionally, human and c. elegans reads contain very little vector sequence  while those from a. thaliana contain  <dig> %. the specialised features are present at significant levels in a. thaliana ests, but not in the other two species. is it important to note that, although being at low numbers, only the a. thaliana reads lack n's. as a consequence of trimming,  <dig> % of nucleotides correspond to insert in humans, while  <dig> % are insert in c. elegans and  <dig> % in a. thaliana, in contrast to figures of  <dig> %,  <dig> % and  <dig> %, respectively, that were obtained from the provided metadata.

a comparison of seqtrim results of these sequences with the final clips reported at the ncbi was then carried out. this comparison was focused on variations of the start position and insert length since it is the only affordable comparison regarding ncbi metadata . the final insert in  <dig> reads of a. thaliana and in  <dig> reads of human start at almost the same position as described in the database , but none in the worm. the reason is that most c. elegans reads contain long stretches of cloning vector that were not removed or reported by researchers. on the other hand,  <dig> reads in a. thaliana,  <dig> in c. elegans and  <dig> in human started in quite different positions . finally,  <dig> reads in a. thaliana,  <dig> in c. elegans and  <dig> in human differ slightly from database reports. in any case, seqtrim always reported shorter inserts than the database . visual inspection of the most divergent results showed that the major differences are due to more accurate localisation of the cloning vector, more stringent qv cutoff, and removal of n's. in conclusion, seqtrim is able to analyse high-throughput sanger sequences providing a final set of inserts of high quality and unlikely to have contaminant sequences. moreover, it seems to outperform the software used for pre-processing the ncbi-published sequences.

discussion
even if there are many dna pre-processing algorithms in the bioinformatics literature, getting them to work correctly may be very difficult, and getting them to process high-throughput data requires an extra programming effort, to enable consideration of unlikely special cases that appear when handling large amounts of sequences or when data quality are very low  <cit> . moreover, the arrival of next-generation sequencing with new experimental approaches and slightly different output format also reinforces the requirement for new software for pre-processing in a reasonable time period. collaboration between computer scientists and biologists for seqtrim development has permitted successful implementation of a theoretical design for a bioinformatic solution for these types of problems, and anticipated future problems.

the use of sequence pre-processors is expected to be in a pipeline with other programs  <cit> . sometimes, constructing a pipeline is not easy, mainly due to input/output compatibility or other program peculiarities  <cit> . this has been considered in seqtrim, since its flexibility regarding input and output formats contrasts with other sequence pre-processors that admit only one single type of sequence file: seqclean, estpass or estprep accept fasta sequences while lucy and pregap <dig> accept chromatograms. none accepts fasta sequences plus qualities as seqtrim does. concerning the output compatibility, saving final sequences as trimmed or masked sequences enables the possibility if including seqtrim in other known workflows such as phred/crossmatch/repeatmasker/phrap, or est2uni  <cit> . moreover, most sequence pre-processors must be used only as command line programs , only as web pages  or as command line and a gui interface . no web site is devoted exclusively to pre-processing, since they were included in more general pipelines . however, seqtrim usage was designed to be affordable by any type of user, by means of its web interface or as a standalone application. accordingly, the command line version is able to cope with high-throughput sequencing data while the web interface is limited to a few thousand sequences, due to browser capabilities and the number of simultaneous connection. nevertheless, taking into account that most seqtrim users would be laboratory scientists who wish to pre-process their own data in order to determine how accurately their experiment were carried out, the coloured output for differentiation of trimmed regions in each sequence  will facilitate interpretation of results as well as a comparison between cleaned and original sequences. in contrast to lucy, original and cleaned sequences are on the same string, instead of two synchronised scrolling panels, to enable a first-look analysis.

unlike other equivalent software, installation of seqtrim does not require special skills, because there is nothing to compile, and only requires installation of freely available ncbi-blast, bioperl libraries, phred  and repeatmasker . configuration files and databases provided with seqtrim can be customised, although most parameters  will never require change. in fact, seqtrim offers more customisation possibilities than seqclean or estprep, and nearly as many as pregap <dig>  but does not present more than the overwhelming forty parameters that can be modified in lucy  <cit> . concerning the final performance, this study clearly demonstrated that usage of seqtrim has significantly reduced the time and complexity involved in a number of gene discovery projects while increasing reliability . results with artificial and high-throughput sequences  suggested that seqtrim can handle any type of sequence read in huge numbers  and provide very accurate results. moreover, applying seqtrim to previously published reads demonstrates that these reads should be fewer and shorter than reported .

seqtrim differential behaviour can be explained as follows:  unlike other pre-processors, seqtrim is designed to remove adaptor sequences as is; note that seqclean and estprep are able to remove adaptors provided that they are included in the contamination database, which is not an easy task for unskilled users.  low quality sequence removal is more restrictive to improve the subsequent assembling procedures ; although this makes seqtrim able to render the shortest sequences and a high rate of rejection, trimmed sequences produce less error-prone contigs when assembled . however, parameters that affect this behaviour are configurable by users.  seqtrim is able to remove chimeric inserts --it should be noted that the only pipeline that proclaims to be able to remove double inserts is estpass  <cit>  but, in our hands, it marks as chimeric est sequences that are not.  inserts are located by the concurrence of vector, adaptor and restriction site sequences instead of only one criterion, usually cloning sites  <cit> , that can have experimental or base-calling errors and are too short to provide certainty. accordingly, table  <dig> shows that the presence of vector and adaptors produces slightly more accurate results than vector alone.  in contrast to most pre-processing pipelines, low quality sequences are not removed in early pre-processing since location of vector, adaptor, restriction site, and poly-a/t are more reliable using crude sequences . furthermore, starting with low quality removal may obliterate key information like sequence orientation or presence of poly-a or poly-t tails. in contrast, the number of sequences rejected due to two inserts can increase, as long a or t tails can be found in low quality reads. these facts can be interpreted as concomitant poly-a and poly-t tails in the same insert, and therefore the sequence is rejected before analysing its qv, which would be the true reason for rejection.  since localisation of vector, adaptors and contaminant sequences are not dependent on perfect matches, any slippage occurring at those regions will be successfully treated, while slippage in inserts is not detected.  seqtrim is ready for handling next-generation sequencing artefacts where results are provided as sequences and qualities in independent files. since  <dig>  est sequences will take more than one hour and since a single run of a  <dig> machine can generate  <dig> million sequences , future efforts for seqtrim improvements will be focused on parallelisation and function optimisation in order to reduce execution times, as well as providing robustness for these huge numbers of sequence.

CONCLUSIONS
seqtrim is the product of years of collaboration between computer scientists and biologists at the university of málaga and is under continuous development. it scales up for pre-processing huge sets of sequences  using large-scale parallel computers , providing a time- and cost-effective solution, and its web interface is user-friendly. although most parameters will never require change, seqtrim offers sufficient customisation and can cope easily with adaptors and chimeras, as well as next-generation sequencing artefacts. input/output features provide more flexibility than other pre-processors for integration in workflows with other programs or in existing ones. the coloured output facilitates differentiation of trimmed regions in each sequence and paves the way for result interpretation as well as comparison between cleaned and original sequences, since they are on the same string. accurateness and reliability of the final sequence clip obtained by seqtrim have been clearly demonstrated.

availability and requirements
project name seqtrim. no license or account needed.

availability http://www.scbi.uma.es/seqtrim. source code is directly available from the web page. it does not include the third party software required. it also includes the vector and contaminant databases used in this work.

operating systems platform-independent .

programming languages perl for algorithm; javascript and html for web interface.

other requirements web browser supporting javascript . for the command line version, the computer should have installed bioperl http://www.bioperl.org/wiki/getting bioperl, ncbi-blast http://blast.ncbi.nlm.nih.gov/blast.cgi?cmd=web&page_type=blastdocs&doc_type=download, phred http://www.phrap.org/phredphrapconsed.html only in case of using seqtrim with chromatograms, and optionally repeatmasker http://www.repeatmasker.org/. execution of repeatmasker requires further installation of wu-blast http://blast.advbiocomp.com/licensing/ or cross_match http://www.phrap.org/phredphrapconsed.html.

authors' contributions
jf coded in perl the software as a command line. ajl designed and tested the web interface. nfp tested the command line and web interface with experimentally- and artificially-derived sequences. frc obtained the chromatograms and next-generation sequences for testing seqtrim, and verified by hand the output reliability. gpt connected the web interface with super-computation capabilities at málaga university. mgc designed the software, took into account its compatibility, and helped frc in the manual inspection of trimmed sequences. all authors read and approved the final manuscript.

appendix
pseudocode of functions and algorithms developed specifically for seqtrim
   function findpolyats

      if found  aort

            

            

      {

         expand the region with { aort

            

            }

            at both sides up to  <dig> bases from each each end

         ns flanking the region are removed

         wasthereasecondpolytora = look for a second polyaort after this one

      }

      return 

   function findcontaminantseqs

      run blastn against local contaminant database

      ignore shorter than a minimal contamination length # defined by a user parameter if user gives a genus

         compare the contaminants found with the genus as it was not contaminant

      build up a list with the names of all the real contaminants

      if the contaminant region is longer than admitted

         the sequence is rejected.

      # look for adaptors as a key for chimeric inserts

      run blast2seq against adaptors 

      if found but the distance is longer than the own adaptor length

         the seq is rejected

   function look_for_artefacts

      if "gcgggg" or "ccccgc" found at 5' or 3' end

         delete it

      clean up from extreme ns and xs

      if is cdna, and forward read, remove first ts and last as 

      if is cdna, and backward read, remove first as and last ts 

   function lowcomplexitymasking

      # masking repeats

      run repeatmasker using wu-blast

      analyse the result file getting 

      mask with xs all regions found in each id

      # masking dust

      look for all but xs repeated at least  <dig> times

      mask them with xs

