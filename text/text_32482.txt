BACKGROUND
a descendant of dna microarray technology, the tiling microarray allows unbiased, high resolution interrogation of genome function  <cit> . generally, such investigations consist of querying labeled nucleic acid samples with tethered dna probes that target regularly spaced tiles from a known genomic sequence  <cit> . these tiles' average spacing can be decreased and/or their genomic coverage can be increased with improvements in microarray feature density, which certainly appears to be occurring . effective feature density can be further increased by synthesizing multiple probes within a single feature in so-called 'double-tiled' array designs  <cit> . such high feature densities allow large-scale functional genomics capabilities that have made tiling microarrays one of the encode consortium's major enabling technologies  <cit> .

given their utility and increasing resolution, tiling microarray data processing algorithms that scale well with advances in feature density and are available in usable software are generally desirable. a hurdle to the efficiency goal is that, like their gene-centric counterparts, tiling microarrays generate data that are most successfully analyzed with statistically robust techniques that aggregate signal measured from multiple different features targeting the same nucleic acid  <cit> . this adds a level of complexity to the efficient design of analysis algorithms since statistically robust procedures are typically more complicated than their less-robust counterparts.

the goal of most said analysis algorithms is to identify those regions of the tiled genomic sequence that yield higher than expected signal intensities. a variety of approaches have been suggested for this task, including simple intensity thresholding  <cit> , correlation of genomic neighbor features' array signals  <cit> , proximity-based heuristics  <cit> , dynamic programming  <cit> , and hidden markov models  <cit> . factor graphs have been successfully applied to analyzing exon tiling array data  <cit>  and there appears to be no restraints on their application to genomic tiling arrays. the different strategies listed here each have their own advantages. intensity thresholding and the various proximity-based heuristics are conceptually simple – the brightest spots on the array should correspond to active regions, and stretches of such features along a chromosome provide additional evidence. correlation based methods, which include factor graph approaches, have the advantage of using information from many hybridizations and avoid some of the common pitfalls of array analysis like background hybridization  <cit> . factor graphs have the additional advantage of easily being able to incorporate other forms of genomic information to aid in the segmentation. finally, factor graphs along with hidden markov models and dynamic programming are attractive in that they have their footing in rigorous mathematics.

for several of the approaches listed above , pre-segmentation smoothing can be beneficial. in fact, this processing step is the most widely applied technique in analyzing tiling microarray data and involves replacing every feature's measured signal with a robust, smoothed value thereof. the value typically employed is the pseudomedian of signals reported by features within a fixed genomic distance from the feature  being smoothed  <cit> . the pseudomedian is often defined as the median of all n/ <dig> pairwise averages among a list of n observations and is a robust estimator of central tendency  <cit> . an equivalent definition of the pseudomedian is the value μ that satisfies

  <dig> = ∑ rank  × sign  

where xi represents the ith value in a collection of observations. when interpreted this way, μ is a root of the one-sample wilcoxon test statistic  <cit> , or put more simply, a value from which the observations do not significantly deviate.

if one chooses to calculate the pseudomedian directly from its definition as the median of all observations' pairwise averages, this forces a o calculation. to arrive at this time complexity, we note that there are o pairwise averages that need to be sorted to find their median and that this sort dominates the computation. this gives us o) = o) = o. currently, this is the method implied in the tiling microarray literature for sliding window pseudomedian smoothing  <cit> . as feature densities increase , more features are typically contained within the sliding window. this of course leads to super-quadratically longer computation within each window and can be a burden to large-scale projects employing the tiling microarray technology. in many tiling microarray analyses, the computation of pseudomedians is the only analysis module that scales super-linearly with feature counts per window and therefore dominates overall runtimes, in practice.

to reduce the burden of pseudomedian computations, especially as feature densities increase, we first deployed a o solution for computing the pseudomedian. this improvement greatly reduced the theoretical runtime of this calculation and we found that, as to be expected, this solution significantly improved runtimes of tiling microarray smoothing. we next observed that this solution's theoretical properties carry with it a sizable constant factor. we therefore investigated an adjustment to the solution utilizing skip lists that greatly reduces this constant factor and, in practice, cut run times of the pseudomedian filter by a further forty-three percent for a representative test case. both algorithms are significant improvements over the definition-derived implementation of the sliding window pseudomedian filter commonly used in tiling microarray analysis.

RESULTS
algorithm
implementation of a pseudomedian filter from its definition
inputs to any pseudomedian filtering algorithm consist of a vector of ordered genomic coordinates and corresponding feature intensities acquired from a tiling microarray hybridization. the goal is to replace the observed intensity of feature f with the pseudomedian of those features' intensities that lie within a span of b nucleotides from f's genomic position .

if there are n probes within b nucleotides of f, then computing the pseudomedian estimate requires first computing all n/ <dig> pairwise averages among the n features, and then computing the median of these values. this algorithm is computed for each feature in a sliding window across genomic coordinates. a common practice is to combine replicate microarrays' data into this calculation. that is, if there are m replicate arrays, each window will contain nm features. this modification results in a dramatic increase in runtime in that this implementation will require the computation of o2) pairwise means within each window. therefore, so as to avoid any confusion, when we refer to n features within a window we will be assuming that this n includes any replicate data counts.

a more efficient pseudomedian algorithm
to reduce the time complexity of computing pseudomedians, we have implemented the algorithm of monahan  <cit> . prior to describing this algorithm, we will first describe another o algorithm for computing a pseudomedian. in doing so, we will set the foundations for monahan's algorithm.

another o option
this algorithm takes as its input a list of values x <dig>  ... xn and first computes their n/ <dig> pairwise averages,

 s <dig> = {/ <dig>   <dig> ≤ i ≤ j ≤ n}. 

these averages are then processed iteratively by  choosing a partition element am during the mth iteration and splitting s <dig> into two sets: one containing those elements of s <dig> less than am and a second containing those elements greater than or equal to am. the value am can also be thought of as a best guess at the set's pseudomedian which we will iteratively refine. sm+ <dig> is then defined to be the intersection of sm and the larger of the two partitions. since we always intersect sm with the larger of the two groups, the intersection must always contain the median.  this iterative partitioning is continued until s <dig> is evenly split . at this point, the pseudomedian is simply the maximum of those values within s <dig> which are less than the final am. in pseudocode, this algorithm can be written as in figure  <dig> 

if implemented, the algorithm described above would require o computations to compute just a single iteration of the partitioning procedure. we can expect that, on average, this procedure is iterated o) = o times because sm can be expected to be half as big after each iteration, given randomly chosen values of am. this results in an overall runtime complexity o.

an o algorithm
an improvement in runtime can be had if the input list x is sorted to begin with. specifically, the partitioning step of the previous algorithm can be found in linear time. we will continue to assume that all pairwise averages are computed upfront, requiring o computation. with these averages, one can construct an upper-triangular matrix m consisting of the elements in s <dig> where matrix entry mi,j is the average of xi and xj, such that i ≤ j and xi ≤ xj. then, for each row, we have mi,j ≤ mi,j+ <dig>  and for each column we have mi,j ≤ mi+ <dig> j. the partitioning of s <dig> can be found by traversing m top-to-bottom and right-to-left until the diagonal is reached as illustrated in figure  <dig>  by constructing m in this way, the o elements can be partitioned in the time it takes to reach the diagonal, which is o. we still require o partitioning steps, so the pseudomedian can be found in o time, assuming that the o averages have been computed. the initial sorting of x requires o time so the overall complexity is dominated by computing the pairwise averages. therefore, the runtime of this algorithm is expected to be o.

monahan's o algorithm
the important aspect of the previous matrix partitioning is that it can be done without actually computing all elements of the matrix upfront. rather, pairwise averages can be computed only when required of the algorithm. this is also illustrated in figure  <dig>  since all pairwise averages need not be computed upfront, the runtime analysis follows that of the previously described algorithm, except that the dominating term is now o as opposed to o previously. we compared the in-practice runtimes of computing the pseudomedian with this algorithm with computing the pseudomedian from its definition in figure  <dig> to test the theory and found that the runtimes do scale as expected.

as is the case with all 'divide and conquer' styled algorithms, choosing good partitioning elements is crucial for achieving good in-practice running times. since we are computing pseudomedians in a sliding window for our application, we set the initial partitioning element , a <dig>  to be the previous window's pseudomedian. this will approximately split the current window's set of pairwise averages in half. for subsequent values of am, we pick a row median from m at random. since each row is already 'sorted', this can be done in constant time at each iteration.

maintaining the sliding window
one of the most expensive steps of this pseudomedian algorithm is the initial sorting of elements in x. since from window to window, we expect a relatively small number of elements to be removed and added, we can replace the costly sorting routine with fast insertions and deletions into a sorted list. the sorted list is, of course, just the list used in the previous window's pseudomedian calculation. there are a number of alternative data structures that can maintain a sorted list of numbers. in this work, we chose to implement our sorted list as a skip list  <cit> . the skip list is easy to implement and insertions and deletions can be performed in logn expected time, where n is the number of elements in the list.

briefly, a skip list is simply a type of linked list data structure. the main difference between skip lists and linked lists is that each node of the skip list has an associated 'level' attribute. a node's level is determined probabilistically when it is created such that a node of level one is the most likely to occur in the list, level two is the second most likely, and so on. every node in a skip list contains a datum field analogous to the linked list, but each node has as many forward links as its determined level . for example, a node of level two will have two forward pointers. one of these pointers will point to the next node having level of at least one. the second pointer will point to the next node having level of at least two, and so on.

utilizing this data structure allows us to omit the sorting step of the modified pseudomedian algorithm and replace it with a few insert and/or delete operations within each window. while big-o time complexity of our pseudomedian filter is not improved upon by this modification, we expect at least some benefit in practice.

testing
to test the theory, we first implemented the algorithm of monahan and integrated it with our sliding window code for tiling microarray analysis. we first generated a synthetic dataset of one million features, giving each feature a random intensity and assigning chromosomal positions such that adjacent features were  <dig> nt apart. this allowed us to easily assess how observed run times scale with the number of features within each sliding window. the definition-derived algorithm's runtimes were compared with the modified algorithm's runtimes for a number of window sizes in figure  <dig>  moving beyond synthesized data, we also ran the two different implementations of the pseudomedian filter on encode chip-chip data, using the same window span  used in the original study. we found that the simpler algorithm ran in  <dig>  seconds while the routine using monahan's pseudomedian algorithm ran in just  <dig>  seconds.

next, we hypothesized that the sorting routine required by the monahan algorithm comprised a large fraction of its overall runtime within each window. to test this hypothesis, we ran our modified algorithm on the same one million data point set with various window spans, recording the amount of computation required for  sorting, for  the remainder of the pseudomedian routine  and for  overhead in maintaining sliding windows. the results of this analysis are plotted in figure  <dig>  we subsequently calculated that the average percent of runtime consumed by sorting was approximately 45%.

to free a bulk of this runtime, we designed our algorithm to maintain a sorted list of intensities from window to window, removing the need to perform a sort within each window. we implemented the sorted list as a skip list  <cit>  and integrated this approach into our code. a comparison of pseudomedian smoothing runtimes between performing sorts in each window and maintaining the sorted list is illustrated in figure  <dig>  by maintaining the sorted list between windows, runtimes were reduced by approximately 43%, on average in our synthetic dataset and the runtime for our chip-chip analysis is reduced to just  <dig>  seconds.

implementation
the main result of our work is the implementation of these improvements in a downloadable piece of software available at . the program is designed to run at the command-line and we provide precompiled versions for the linux and microsoft windows operating systems. source code, written in ansi-c, is available as well for those wishing to run this software on an alternate operating system.

CONCLUSIONS
post-genome technologies generate vast amounts of data. to arrive at biological inferences, this data must be processed in such a manner that is  accurate and  efficient. for tiling microarrays, accuracy is best obtained with robust statistical procedures  <cit> . however, the technique most often used to achieve this robustness is seemingly inefficient in its computation. to relieve this burden, we have suggested the use of monahan's algorithm for the computation of the pseudomedian. this alteration yields great improvements in runtimes for pseudomedian smoothing. upon examining this algorithm it became clear that a numeric sort in each window was responsible for much of its clock time. we therefore sought to remove this costly sort and instead decided to maintain a sorted list from window to window since neighboring windows largely consist of the same elements. we implemented a skip list to this end and found this modification increased efficiency by some 43%. there are many data structures that could be used to maintain the sorted list. various kinds of balanced binary search trees  can fill this role, for example. we have implemented the skip list purely out of ease of implementation. the sacrifice we make for this simplicity is that binary search trees can guarantee o worst case run times for inserts and deletes while the skip list can offer only o expected runtimes. furthermore, inserts and deletes can take quadratic time in the worst case scenario with skip lists. however, our application requires upwards of several million inserts and deletes to/from our skip list, so expected case considerations are realized.

in demonstrating the improved efficiency of our algorithms, we used a synthetic dataset which consisted of one million features, spaced at a constant interval of  <dig> nt. doing so simplified the analysis and presentation of runtimes since we could refer to the number of features within a window as opposed to an average genomic spacing as is the most common strategy of tiling microarray construction. assessing the algorithm in terms of feature counts within windows also allows for the general analysis of increased feature count due to higher tiling density and due to the inclusion of replicate tiling microarrays. to demonstrate practical gains, we have used the algorithms described here to smooth chip-chip data generated as part of the encode project. we found that the original pseudomedian filter's runtimes can be improved by 93%.

it is worth noting here that once prohibitive multi-pass analyses become much more feasible with the algorithms we have described. an example multi-pass analysis might aim to place error bars on the estimated pseudomedians. this could be achieved by repeatedly sampling independent hybridizations  and performing pseudomedian smoothing on these bootstrap samples. confidence intervals could then be readily estimated from the samples. other multi-pass analyses might include finding the optimal span to use in the pseudomedian filter, or in selecting a set of hybridizations that, when smoothed, yield the most satisfying results.

we have not assessed the sensitivity and/or sensitivity gained by using pseudomedian filters in tiling array analyses as these are beyond our scope of computational efficiency. nonetheless, given the large number of segmentation methods available, the field is probably ripe for such an analysis and we hope to see a comprehensive review soon, building on the recent work of  <cit> . hopefully our new algorithms will play a role in such an analysis.

it is our belief that tiling microarray datasets will continue to grow in size and in tiling density. we also expect that their popularity will continue to increase as they are adopted for a growing number of genomic applications. these advances make efficient algorithms such as those presented here and software that implements them of timely need.

