BACKGROUND
the advance of next generation sequencing  has greatly promoted the research on genomics analysis, hereditary disease diagnosis, food security, etc. the exponential growth of big ngs data outpaces the increase of storage capacity and network bandwidth, posing great challenges to data storage and transmission  <cit> . efficient compression methods of ngs data are needed to alleviate the problems  <cit> .

general-purpose compression methods such as gzip  and bzip <dig>  do not take into account the biological characteristics of dna sequencing data like small alphabet size, long repeat fragments and palindromes. they fail to obtain satisfying compression performance on ngs data. accordingly, many specific compression methods have been proposed, the majority of which were designed to process raw ngs data in fastq format  and/or alignment data in sam/bam format  . depending on whether external reference sequences are used, these specifically-designed methods are widely categorized into two groups namely reference-free and reference-based methods  <cit> .

reference-free methods, more applicable to fastq data, directly store the target sequencing reads with specific compressive encoding scheme based on the inherent statistical and biological nature of the data. for instance, scalce  <cit>  clusters the input reads of fastq data into groups sharing common ‘core’ substrings using locally consistent parsing algorithm  <cit> , and then compresses each group with gzip or lz77-like methods. fqzcomp  <cit>  predicts the nucleotide sequences in fastq format via an order-k context model and encodes the prediction results with arithmetic coder. dsrc algorithm  <cit>  divides input reads in an fastq file into blocks and compresses them independently with lz <dig> and huffman encoding schemes. dsrc  <dig>  <cit>  improves dsrc mainly in terms of processing time by introducing threaded parallelism and more efficient encoding schemes. seqdb  <cit>  compresses fastq data of fixed-length reads using block storage like dsrc/dsrc <dig> together with a data-parallel byte-packing method, which interleaves sequence data and quality scores.

reference-based methods, by contrast, do not encode the original read data but instead the alignment results, e.g., aligned positions and mismatches, of the reads against some external reference sequences. they are mainly targeted at sam/bam data where the alignment results of short reads are readily available. for example, cram  <cit> , taking bam-based input, encodes the differences between reads and a reference genome using huffman coding. cram also includes the idea of using a de bruijn graph  <cit>  based assembly approach to build an additional reference for the compression of unmapped reads. ngc  <cit>  traverses each column of read alignment in sam format to exploit the common features of reads mapped to the same genomic positions and stores the mapped reads with per-column run-length encoding scheme. samcomp  <cit>  makes full use of the sam flag, mapping position and cigar string in a sam file to anchor each base to a reference coordinate and then uses per-coordinate model to encode the bases. hugo  <cit>  introduces a specific compression scheme to store the aligned reads in sam format. the inexact mapped or unmapped reads are split into shorter sub-sequences and realigned against different reference genomes until they are all mapped. quip  <cit>  implements a standard reference-based compression of sam/bam data. it also is equipped with a de bruijn graph based de novo assembly component to generate references from the target data itself instead of relying on external references, which enables the reference-based compression of fastq data.

reference-based methods tend to obtain superior compression ratios compared to the reference-free ones  <cit> . nevertheless, reference-based compression relies on read mapping tools like bowtie  <cit> , bwa  <cit>  and soap  <cit>  to obtain the alignment results against the reference sequence. it is noted that these mapping tools are originally designed for other downstream analyses but not for data compression. they tend to involve unnecessary processes and generate undesirable information for the storage of the data. to solve this problem, we propose a novel light-weight mapping model based on k-mer indexing strategy to allow a fast alignment of short reads against reference genome and efficiently produce the very essential alignment results for storage.

based on the light-weight mapping model, a new lossless reference-based compression method namely lw-fqzip is put forward for fastq data. unlike most of the existing reference-based methods that aim at sam/bam format, lw-fqzip processes raw ngs data in fastq format, as such no extra information brought by sam/bam generators is involved in the archive and the maximal compatibility is ensured to the downstream applications. particularly, lw-fqzip first splits the input fastq data into three streams of metadata, short reads and quality scores, and then eliminates their redundancy independently according to their own characteristics. the metadata and quality scores are compacted with incremental and run-length-limited encoding schemes, respectively. the short reads are stored using a reference-based compression scheme based on the light-weight mapping model. afterward, the three processed data streams are packed together with general purpose compression algorithms like lzma . lw-fqzip was evaluated using eight real-world ngs data sets and the experimental results show that lw-fqzip obtains comparable or superior compression ratios to other state-of-the-art lossless fastq data compression algorithms.

the remainder of this paper is organized as follows: section ii describes the framework and implementation details of lw-fqzip. section iii presents the experimental results of lw-fqzip on eight raw ngs data sets. finally, section iv concludes this study.

methods
the general framework of lw-fqzip
fastq is the most widely used text-based format for storing raw ngs data. an fastq file normally contains millions of ngs records each of which consists of four lines. the first line is the metadata containing a sequence identifier and other optional descriptions. the second line is the raw nucleotide sequence, i.e., short read. the third line is typically unused in the downstream analysis and hence can be omitted for compression. the fourth line, of equal length to the second line, is the quality score string with each score reflecting the probability of the corresponding base been correctly determined during sequencing.

lw-fqzip follows the framework shown in fig.  <dig> to compress an input fastq data. it first splits the data into metadata, short reads and quality scores, respectively and then processes them independently with different schemes: the metadata goes through incremental encoding to identify and eliminate repeat segments across different records; the short reads are fed to a light-weight mapping model where they are aligned against an external reference genome and the alignment results are then recorded using a specific encoding scheme; and the quality scores undergo a run-length-limited encoding scheme  <cit>  to abridge consecutive repeats. finally, a general purpose compression algorithm lzma is utilized to pack the outputs of the three streams. the details of the compression of the three streams are provided as follows.fig.  <dig> the general framework of lw-fqzip



incremental encoding of metadata
the metadata in each ngs record begins with a symbol ‘@’ followed by a sequence identifier and other optional information like instrument name, flow cell identifier, tile coordinates, and read length. the major part of the metadata is identical for each record, so only one plain copy is needed if the variances in each record can be reserved. in lw-fqzip, the metadata is parsed into different fields and an incremental encoding scheme is used to record the data.

for example, the first four metadata lines of a fastq data srr <dig>  a ngs data of homo sapiens generated by roche/ <dig> gs flx platform, are shown in table  <dig>  the first field of a metadata line, i.e., ‘@srr <dig> x’, represents the sequence identifier that is common for all records, so only one copy is needed in storage. the third field, i.e., ‘length = xxx’ indicates the read length and can be omitted in the compression, as the length can be easily acquired from the corresponding short read. the most informative part lies in the second field, e.g., ‘e96djwm01d47cs’ in the first line, where an incremental encoding scheme is used to record the variances of one metadata to its previous neighbour. particularly, the first metadata is plainly encoded as ‘srr <dig>  e96djwm01d47cs’. the second one is then compared to the first one in the second field to find out nine consistent characters ‘e96djwm01’ followed by five variances, i.e., ‘co1kr’, so the second metadata can be encoded with ‘ <dig> co1kr’, where the length of consistency and the content of variances are delimited with a white space. similarly, the third and fourth metadata lines are incrementally encoded with respect to the second and third ones respectively.table  <dig> examples of metadata encoding



reference-based compression of short reads based on a light-weight mapping model
it is well known that homogenous genomic sequences share high similarity. when a large portion of the target genomic sequence has been captured by an existing homogenous sequence, i.e., the reference, it is of great benefit to store the target one by recording the differences of it to the given reference  <cit> . motivated by this, lw-fqzip aligns the short reads in fastq data against an external reference sequence, normally a genomic sequence from homologous species, with an efficient light-weight mapping model and then the alignment results are recorded instead of the original reads.

as shown in fig.  <dig>  the light-weight mapping model implements fast alignment by indexing the kmers within the reference sequence. firstly, given a reference sequence denoted as r, an index table ir is established to store the positions of the kmers prefixed by ‘cg’ in r. ir is a hash table of numeric keys, which are calculated with a hash function hashfunc taking input kmers. particularly, hashfunc converts a kmer say ‘riri+ <dig> …, ri+k’ to a binary number with ‘a’ =  <dig>  ‘c’ =  <dig>  ‘g’ =  <dig>  and ‘t’ =  <dig>  for example, hashfunc =  <dig>  the value associated with each key, denoted as ir < key>, is a set of occurrence positions of the corresponding kmer in r. to construct ir, the program sequentially scans r and at each time a kmer ki = ‘riri+ <dig> …,ri+k’ is detected, a key ki♯ is obtained with ki♯ = hashfunc and ir < ki♯ > is updated with ir < ki♯ > =ir < ki♯> ∪i. the kmers serve as seeds for mapping short reads to r. note that only the most commonly occurring dimer ‘cg’ is considered as the prefix for the sake of speed and memory consumption.fig.  <dig> flowchart of the light-weight mapping model



secondly, the mapping of a short read x proceeds by identifying in x any kmer ki that is present in ir. if there is no kmer found in x, the original x is output as mismatch values with mapped position pos =  <dig>  otherwise, the set of occurring positions of a kmer ki in r, i.e., ir < ki♯>, can be retrieved from ir. the assumed mapped positions of x on r using ki as seed are then represented as ui = ir < ki♯ > −i, where i is the position of ki in x. the most frequently occurring position in ui is the most likely mapped position of x on r  and a local alignment is thereby performed base by base to find out the exact mapping results. in case there are multiple positions of the same maximal occurrences, local alignments are performed at all positions and pos is set to the one with best matching. once a valid alignment is found, i.e., the match length is larger than a predefined threshold l and the mismatch rate is below e, the model outputs the mapping results of x. if the match length is shorter than l, the model outputs the mapping results of the aligned part and let the palindrome of the remaining unaligned part go through the mapping model again.

the output mapping results of a read include the mapped position, palindrome flag, match length, match type and mismatch values. we format the results as ‘ < pal >  <mtype > <misvalues>’. the mapped position pos and match length mlength are mandatory whereas the other fields are optional. flag pal is set to  <dig> if palindrome is used otherwise it is omitted. mlength denotes the number of bases matched/mismatched in the alignment. whether or not it is a match is indicated in the following field mtype that takes one value of m , i , d , and s . if mismatches are identified, the mismatch values, i.e., one or multiple bases in {‘a’,‘c’,‘g’,‘t’}, should be recorded in the last field misvalues. to boost the matching rate, the mapping model allows a small of number of approximate matches in each alignment. hence, the read must be reconstructed from the mapping results and compared with the original input to identify the variations. in this way, a lossless compression is guaranteed. if a variation is detected, the position ‘’ and mismatch values ‘’ are recorded with delta encoding scheme. the descriptions of the output fields are summarized in table  <dig> table  <dig> the descriptions of the output mapping result fields



examples of short read encoding based on mapping results are shown in fig.  <dig>  the first short read read <dig> is exactly mapped to the reference at the first base, so pos is set to  <dig> with  <dig> matches ‘10 m’. read <dig> is mapped to position  <dig> with four matches ‘4 m’, followed by two insertions ‘2i’ of ‘aa’, four matches ‘4 m’, one deletion ‘1d’, and the other two matches ‘2 m’. read <dig> is mapped to position  <dig> of reference for  <dig> bases but the remainders are unmatched. in this case, the aligned segment is first output as ‘ <dig> 7 m’, and the palindrome of the unaligned part is realigned against the reference to find out  <dig> matches from position  <dig>  so ‘ <dig> 08 m’ with a palindrome flag inserted before ‘8 m’ is recorded. read <dig> is encoded as’ <dig> 7m2d5m’ where two approximate matches in positions  <dig> and  <dig> are detected, so an additional delta code ‘16a7c’ is appended at the end.fig.  <dig> examples of short read encoding



compression of quality scores based on run-length-limited encoding
a quality score q is defined as a property logarithmically related to the base-calling error probabilities p, e.g., q = −10log10p. normally, q is quantized and represented with an 8-bit ascii printable code in fastq format. the much larger alphabet size and pseudorandom distribution of quality scores make them harder to compress than short reads  <cit> . both lossless  <cit>  and lossy  <cit>  strategies have been explored in the compression of quality scores. in lw-fqzip, only lossless compression is considered to guarantee the fidelity of the data.

the quality scores in fastq data usually contain a large number of consecutive occurrences  of the same character making them suit for run-length encoding. particularly, a run-length-limited  encoding scheme  <cit>  is used in lw-fqzip to record any run of length n >  <dig>  for example, a quality score string like ‘cccgfffffffhh’ can be encoded as ‘c3αgf7αhh’, where the runs of ‘c’ and ‘f’ are represented with ‘c3α’ and ‘f7α’, respectively, and ‘g’ and ‘hh’ are plainly recorded. the symbol ‘c’  denotes the repeated character, i.e., ‘c’ , by flipping the 8-th bit from the original  <dig> to  <dig> so that it is distinguishable from the plainly encoded characters. the length of a run, e.g., ‘3α’ and ‘7α’, is also represented in an 8-bit ascii code  to avoid confusion in decoding. as a result, the maximum length of a run is limited to 28 =  <dig> and any run of length n >  <dig> must be treated as multiple runs.

if consecutive runs appear to be alternate repeats of two characters like ‘ddddcccdddddcccc’, the corresponding rll code ‘d4αc3αd5αc4α’ can be further simplified to ‘d4α3α5αc4α’, where the solitary ‘3α’ indicates the insertion length of the second character recorded in the end of the code. in this way, the coding is shortened from 8* <dig> bits to 6* <dig> bits.

as shown in fig.  <dig>  the separately encoded metadata, short read alignment results, and quality scores are finally packed together and compressed with lzma.

RESULTS
eight real-world fastq data sets ranging from  <dig>  gb to  <dig>  gb are used to test the performance of lw-fqzip. the data sets were all downloaded from the sequence read archive of the national centre for biotechnology information   <cit> . the descriptions of the data sets are summarized in table  <dig> table  <dig> real-world fastq data sets used for performance evaluation

e. coli
p.syringae
s. cerevisiae
l. pneumophila
pseudomonas
e. coli
p. aeruginosa
s. cerevisiae


the other four state-of-the-art lossless fastq data compression algorithms namely quip  <cit> , dsrc  <cit> , dsrc <dig>  <cit>  and fqzcomp  <cit>  are selected for comparison study. quip  <cit>  is run in two different modes, i.e., the pure statistical compression  and the assembly-based compression . the general purpose compression algorithm bzip <dig> is also involved in the comparison as the baseline. all compared methods are tested on a cluster running 64-bit red hat linux operating system with 32-core  <dig> ghz intel xeon cpu. the parameters of lw-fqzip are empirically set to k =  <dig>  e =  <dig> , and l =  <dig> .

the compression ratios of all compared methods on the eight data sets are summarized in table  <dig>  a compression ratio is obtained by dividing the compressed file size by the original file size. it is observed that all specially-designed fastq data compression methods outperform the general purpose method bzip <dig>  lw-fqzip manages to obtain the smallest compression ratios on six out of the eight data sets. especially, on data err <dig> of variable-length short reads, the superiority of lw-fqzip is much more obvious. ‘quip –a’ wins on two data sets namely srr <dig> and err <dig>  it is found that these two data sets contain fewer consecutive quality score repeats than other data sets, so lw-fqzip with run-length encoding fails to obtain comparable compression ratios of quality scores to ‘quip –a’ that uses a more efficient arithmetic encoding scheme. on average, lw-fqzip obtains the best compression ratio of  <dig>  over all datasets, resulting in  <dig>  % reduction of the storage space.table  <dig> compression ratios of the compared methods on the eight fastq data sets

 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 


the compression ratios of lw-fqzip on the three components of fastq data are reported in table  <dig>  the metadata is best compressed with the smallest average compression ratio  <dig> . the compression ratios of quality scores are greater than the other two components due to the inherent larger alphabet size and quasi-random distribution. the compression of quality scores remains the major challenge and opportunity to achieve substantial reduction on storage space of ngs data. lossy compression could be considered if the loss of accuracy in downstream analyses is controllable.table  <dig> the compression ratios of lw-fqzip on the three components of fastq files



lw-fqzip is characterized with the light-weight mapping model. the framework can also work with other short read mapping tools like bwa as has been experimented in our previous work  <cit> . to evaluate the efficiency of the proposed light-weight mapping model, the mapping results, mapping time, and compression ratios using bwa and the light-weight mapping model are tabulated and compared in tables  <dig> and  <dig>  with comparable compression ratios, the proposed light-weight mapping model significantly outperforms bwa in terms of mapping time. the light-weight mapping model is much more efficient to obtain the essential alignment results for the purpose of storage. more complete running-time results of lw-fqzip on the test data sets are reported at http://csse.szu.edu.cn/staff/zhuzx/lwfqzip/#experiment.table  <dig> the mapping result of the proposed light-weight mapping model against that of bwa

n*: the number of unmapped segments that can be realigned to the same reference genome



CONCLUSIONS
the advance of ngs technologies produces unprecedented data volume and poses big challenges for data storage and transmission. to mitigate the problems, we develop lw-fqzip as a strictly lossless reference-based compression algorithm for raw ngs data in fastq format. lw-fqzip compresses the metadata, sequence short reads, and quality scores in fastq files based on incremental encoding, reference-based compression, and run-length-limited encoding schemes, respectively. it is characterized with an efficient light-weight mapping model to enable fast and accurate alignment of short reads to a given reference sequence. the experimental results on real-world ngs data sets demonstrate the superiority of lw-fqzip in terms of compression ratio to the other state-of-the-art fastq data compression methods including quip, dsrc, dsrc <dig> and fqzcomp. lw-fqzip is mainly designed to optimize the compression ratio, yet parallelism and more efficient coding schemes can be further introduced to improve its time and space efficiency.

yongpeng zhang and linsen li contributed equally to this work.

competing interests

the authors declare that they have no competing interests.

authors' contributions

yz, ll, and zz conceived the study, performed the experiments, and wrote the paper. yy, xy, and sh reviewed and revised the manuscript. all authors read and approved the manuscript.

