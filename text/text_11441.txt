BACKGROUND
structural genomics  <cit>  is an important field, the objective of which is the determination of the three-dimensional structures of all the proteins coded by a genome. recent advances in this field increase our understanding of protein function  <cit>  but have also an impact on the pharmaceutical industry  <cit> . the structural genomics centers provide hundreds of protein structures of unknown function  <cit> .

development of automated protein function predictor from the structure is imperative and is nowadays an active research field in bioinformatics. one of the various approaches for assigning a function to a protein is to study the network of its physical interactions  <cit> . in this context, identifying a potential binding site between proteins is of primary interest. the localization of such binding site can also reduce the search space required by protein docking algorithms to predict the best match between two proteins. this localization has also an importance in studies about interactions between proteins and small molecules, such as ligands or substrates.

bartlett et al.  <cit>  presented an analysis of the properties of the catalytic residues. they showed that catalytic residues share common properties including the propensity of residue types, the conservation and the solvent accessibility. petrova and wu  <cit>  compared  <dig> machine learning classifiers to specifically identify the catalytic residues within the whole enzyme. gutteridge et al.  <cit>  trained a neural network classifier to score the residues of a protein by their likelihood to be catalytic. the location of the active site were determined by searching for clusters of high-ranking residues.

keskin et al., in their algorithm prism  <cit> , used spatial similarity to predict protein-protein binding sites locations. shulman-peleg et al. proposed a method, called siteengine  <cit> , comparing the properties at different regions of the proteins surface, such as conformational properties, to find proteins with similar functions in databases. the same kind of approach has been done by shatsky et al.  <cit>  to recognize binding patterns common to a set of protein structures. jones and thornton  <cit>  presented a method to predict protein-protein interaction sites. the prediction was based on the calculation of relative combined scores for patches constructed on the surface of protein structures. zhou and shan  <cit>  published a method based on a neural network for predicting individual residues in protein-protein interfaces. since then, a lot of methods predicting individual residues or patches that overlap with interface have been proposed  <cit> .

bradford et al.  <cit>  proposed a protein binding site predictor based on a support vector machine  discriminating the patches constructed by segmentation of the surface. after the segmentation and the computation of some relevant properties for each patch of each protein, the svm was trained and became usable to distinguish binding site from non-binding site surface parts. bradford and westhead  <cit>  used a bayesian network in combination with a surface patch analysis to design a protein-protein binding site predictor. petsalaki et al.  <cit>  presented a different approach to predict peptide binding sites on protein surface. this approach is based on the construction of spatial position specific scoring matrices for each of the  <dig> standard amino acid. several groups performed the comparison of different tools usable for the prediction of protein-protein binding sites  <cit> .

in the literature, patches based binding site localization methods generally use supervised classifier to find the relationship between the patches properties and their overlap with the true binding site. machine learning or statistical techniques are used to discriminate patches totally superposed with the real binding site from patches lacking joined surface with it. in these cases, the response variable is categorical because it only takes two distinct values. so, supervised binary classifiers are used to predict the response variable from the predictor variables. however, when predicting the location of the binding site of a protein, a lot of patches constructed are partially superposed with the real binding site. using regression tools in place of classification tools allows to include partially superposed patches during the training process what should improve the predictive performance of the model.

the first contribution of this paper is the description of a patches based protein-protein binding site localization method including either regression or classification tools. the predictive performance of several regression and classification tools are compared using leave-one-out cross-validation. our method is also compared with three methods usable through web servers. the first method, named sharp <dig>  <cit> , is a patches based binding site localization method using a combined scoring function. the second method, named pinup  <cit> , is based on an empirical scoring function including a side-chain energy term, a solvent accessibility term and a conservation term. the third method named cons-ppisp  <cit>  is based on a neural network taking psi-blast sequence profile and solvent accessibility as input. another contribution is to consider the travel depth as one of the surface properties. the travel depth is a notion introduced by coleman and sharp  <cit>  to quantify the depth of pockets within a molecular surface. the third contribution of our method is the construction of the predicted binding site. in the literature, for patches based methods, the predicted binding site in chosen among the original patches according to the ranking of their probability to be the real binding site. our method takes into account the output of the statistical model on the whole surface to construct a new patch, which forms the predicted binding site. the size of this predicted binding site can be adjusted according to the application. whereas a too small predicted binding site increases the risk of false negative results, a too large counterpart increases the risk of false positive results. limiting false negative results avoids to miss a binding site even if the predicted zone is too large, whereas limiting false positive results ensures that predicted zones is a part of the real binding site.

methods
the aim of this work is to develop a method enabling to predict the location of potential binding sites in a protein according to its 3d structure. the development of this method is composed of two successive parts: the training and the application.

input data of the training part are the pdb files of several protein complexes. external surfaces of the proteins are constructed by using the 3d structures encoded in the pdb files. some characteristics are extracted and mapped on these surfaces. then, the surfaces are segmented into patches. the mean values of the properties and an index quantifying the overlap with the real binding site are computed for each patch. finally, a statistical model is used to find a relationship between the patches properties and their overlap indices.

input data of the application part are one or more pdb files. patches are constructed in the same way as in the training part except that the overlap index with the real binding site cannot be computed. the location of the real binding site is indeed unknown. the statistical model previously trained is then used to allocate a score to each patch. finally, a predicted binding site is located by using the score mapped on the protein surface.

to validate the method, the application part can be tested on a protein of known binding site location. in this case, the location is considered as unknown during the prediction of the binding site location. comparing the predicted location of the binding site with the real one, is used to validate the performance of our method.

a scheme of the method pipeline is depicted in figure  <dig> 

pdb
the input data of the algorithm in both the training and application parts, are a set of protein 3d conformations. for the tests performed in this work, they are downloaded from the protein data bank  <cit> . two datasets are used in this work and both are composed of known protein complexes. the first one, used to train the models and to compare the statistical tools, is the one used by bradford et al  <cit>  . this dataset is composed of proteins in their bounded conformation and had already been filtered at 20% sequence identity. to compare our method with existing methods, the dataset used consists of  <dig> proteins in the enzyme/inhibitor category of docking benchmark  <dig>   <cit> , after filtering at 35% identity . the genuine binding sites were identified using the bounded complexes but the tests were performed on the unbounded structures.

surface
there are different ways to represent a molecular surface  among which the van der waals surface, the solvent accessible surface  and the solvent excluded surface   <cit> .

with the van der waals surface model, the electron clouds around atoms are approximated by rigid spheres of different radii, which are called the van der waals radii of the atoms. with the sas model, the inner surface of the volume is filled by the possible positions of the center of a ball representing a molecule of solvent . the ses model is comparable but considering the exterior surface of the ball. in this work, we used the ses model, which is approximated by a 3d mesh. a mesh is a collection of points, edges and faces defining surfaces in a 3d environment. as no conformation changes are taken into account in our method, proteins are considered to be rigid objects. it allows to work with surface points without considering what occurs inside the protein.

binding site
a residue is considered to lie in the binding site if more than  <dig> Ã… of its ses area is hidden after a complex formation  <cit> . as the proteins of the training data set originate from pdb files containing proteins complexes, the locations of their binding sites are already known.

properties
properties of different types are affected to mesh points. these properties can be classified into three groups: geometric properties, composition and conservation score. in order to make the method more uniform, all properties are considered to be surface points properties. so, each point of the ses is related to the closest atom center and inherits its properties.

geometric properties
two geometric properties are considered: the local curvature and the travel depth.

the mean curvature is negative for hollows, positive for bumps and zero for saddle points and planes. it is approximated as follows: first, the mesh is smoothed by a laplacian operator and the displacement vector between each point position and the new position is evaluated. then, the dot product between the point normal vector and the displacement vector is calculated. the curvature estimation is the normalized value of this product .

the travel depth of a surface point is defined as the shortest distance that a solvent molecule should do to reach this point from the convex hull of the ses. it is more global than the curvature computation while keeping a good surface resolution. it is computed with a surface-based algorithm using octrees  <cit> .

composition
the composition is the proportion of each surface amino acid composing a surface patch. for instance, if a fifth of the points in a patch corresponds to a leucine and the rest to a proline, the composition of the patch is leu:  <dig> , pro:  <dig> , other amino acids:  <dig>  other physico-chemical properties are redundant with the composition and are not significant for the statistical model.

conservation score
the conservation score is a property in relation with the structure conservation across the evolution process. key amino acid positions are often under strong evolutionary constraints. they are important for maintaining the 3d structure of a protein and/or its functions. thus, the biological importance of a residue is often correlated with its level of evolutionary conservation within the protein family. the web-based tool consurf  was used to calculate the evolutionary conservation score of each residue  <cit> .

patches
once the mesh is created and properties are affected to each point, patches are constructed. the construction is very simple. patches are geodesic circles centered on each surface amino acid. the area of these circles is constant and proportional to the area of the whole ses. they are wider than the amino acid ses areas so that an important overlap exists between patches. the size of the patches is set empirically to one tenth of the whole ses and in this case, each surface point belongs to about fifteen patches. the mean of the properties are computed for each patches. every patch in the training pipeline is also associated with an overlap index, the positive predicting value , reflecting its similarity with the true binding site:

  

where s is the area of the ses, p is the analyzed patch and i is the true binding site. to give an intuitive vision of this measure, ppv is equal to one when the patch is completely included in the binding site. in the training part, the true binding site is also considered as an additional patch.

statistical tools
the objective of the statistical model is to assign a score to patches of a protein of unknown binding site location.

during the training part, the input data of the statistical model are the patches associated to their ppv and the properties extracted in the previous steps. the objective of the training is to find a relationship between the extracted patches properties , and the patches ppv . the relationship is used afterwards in the application part to allocate a score to the patches resulting from the segmentation of a target protein.

in this section, different statistical tools are presented. they can be separated into two categories: the classifiers, which are more commonly used to treat this kind of problems, and the regression tools. the output of a classifier is the probability of each patch to represent the real binding site. the output of a regression model is a predicted ppv for each patch. the statistical model output, whether it be a probability or a predicted ppv, is denominated the score of the patch. the score has to be highly correlated with the genuine ppv to ensure the success of the method.

multiple linear regression 
multiple linear regression  <cit>  is a statistical technique that fits the relationship between the response variable and the predictor variables, usually, by minimizing the sum of the squared deviations. the best multiple linear model can be found by using a stepwise selection of the predictor variables.

partial least square regression 
partial least square regression  <cit>  is an extension of the multiple linear regression. using multiple linear regression with a high number of predictor factors leads to over-fitting: the model fits the sampled data closely but fails to correctly predict new data responses. in a partial least square regression, a few latent factors are extracted and replace the original predictor factors for the response fitting. partial least square is useful to find a good predictive model without necessarily understanding the existing relationship between variables.

principal component regression 
principal component regression  <cit>  uses principal component analysis to fit the relationship between the predictor variables and the response variable. the first step is to compute the principal components of the predictor variables. the second step is the regression on a subset of the principal components.

multilayer perceptron 
multilayer perceptron  <cit>  is a machine learning technique using multiple layers of neurons. a neuron is a simple processing element, connected to the neurons of the previous and of the following layers. the connections between the neurons are characterized by weights which are adjusted during the training step. multilayer perceptrons can be used for regression as well as for supervised classification task. in this work, it was used as a regression tool.

random forest for regression  and classification 
random forest  <cit>  is a machine learning tool used for classification and regression tasks. in both cases, a random forest is a set of trees created by bootstrapping samples of the training data set. random forests for regression use a set of regression trees. in this case, the prediction is made by averaging the predictions of the different trees. random forests for classification use a set of classification trees. in this case, the prediction is made by a majority vote on the predictions of the different trees.

regression trees and classification trees are both decision trees. a decision tree is a technique used to predict a response variable using a set of predictor variables. regression trees are used for continuous response variables, whereas classification trees are used for discrete response variables. in both cases, it uses a series of "if then else" conditions based on values of the predictors to predict the response. during the training phase, the decision tree is built through an iterative process of data splitting. this iterative process continues until each node reaches a fixed minimum size. for instance, if there are three predictor variables, the data can be separated from the root according to the value of the first variable. at each new node, the data can then be separated according to the value of other variables, and then, at children nodes, they can be separated according to the first variable again, ... during the prediction phase, a new data starts from the root and follows the branches corresponding to the value of its predictor variables. the leaves determine the value of the response variable.

naive bayes classifier 
naive bayes classifier  <cit>  is a machine learning tool using the bayes theorem to compute the probability of a case to belong to a category. the nbc is based on a conditional independence assumption. given the value of the response variable, the predictor variables have to be independent. despite the fact that this assumption is rarely true in reality, naive bayes classifier often performs better than expected.

support vector machine 
support vector machine  <cit>  is a machine learning tool used for classification and regression. when used for binary classification, the objective of the svm is to map the training data into a property space by the aid of a kernel function, and to constructs an n-dimensional hyperplane that optimally separates the case according to the category of their response variable.

predicted binding site construction
after statistical analyses, the score assigned to each patch is used to construct a score map on the protein ses. each point of the surface is associated to the geodesically closest center of patch and get the score affected to this patch. next, the points of the mesh representing the ses are added successively to the predicted binding site in descending order of score. the growth of the predicted binding site stops when either a predetermined size or a score threshold is reached.

RESULTS
the two major consecutive estimation steps of our method are the statistical prediction and the predicted binding site construction. the predicted binding site construction is divided in two steps: the score mapping on the ses and the score map thresholding. results comparing, on one hand, the different statistical tools, and, on the other, our method to other existing methods are presented in this section. the different statistical tools were compared after the statistical prediction by calculating the correlation coefficient between predictions and real ppvs. next, they were compared after the mapping step by analyzing the score distribution inside and outside the genuine binding site. finally, overlapping indices between the predicted and the real binding site were computed. these indices were used to compare the results obtained with different statistical models, as well as to compare results of our method with results of other existing methods.

statistical prediction
the performance of our method strongly depends on the ability of the statistical model to correctly classify a patch or to predict the corresponding ppv. for both classification and regression tools, the output is a number between  <dig> and  <dig> called the score of the patch.

each classification and regression tool was evaluated through leave-one-out cross-validation, which is an iterated process. at each step, the data set, comprising the patches and their properties, was divided into two sub-sets: a test data set, which contained the patches of one protein, and a training data set, which contained the patches of all the other proteins. the statistical model was trained on the training data set, and used to affect a score to the patches of the test data set. the scores were finally compared to the real ppv through the computation of the pearson correlation coefficient. the protein in the test data set was different at each iteration until prediction and correlation coefficient were obtained for the patches of each protein in the complete data set.

after the leave-one-out cross-validation, each statistical tool was associated to  <dig> correlation coefficients, each of them corresponding to one protein. distributions of these correlation coefficients for regressions and classifications are compared in the box plot shown in figure  <dig>  using regression to fit the relationship between the properties and the ppv frequently led to better score correlations than using classification. as the correlation coefficients vary considerably from one protein to another for the same statistical tool, the box plots are stretched and overlap a lot. building a relative box plot allows to better visualize which statistical tool generally gives the best results. for this purpose, relative correlation coefficients were computed for each protein separately, relatively to the mean of the scores for all the statistical tools. a positive relative coefficient means that the statistical tool better works than the average. the distribution of these relative coefficients is depicted in a box plot in figure  <dig>  the box plot of the relative correlation coefficients shows that, for one particular protein, regressions generally gave better prediction than the average, whereas classification generally gave worse prediction than the average.

binding site localization
after statistical prediction, a predicted binding site was constructed in two steps, namely the score map construction and the thresholding on the score map. the score map construction was validated by comparing the score distribution inside and outside the genuine binding site, using a histogram. the thresholding on the score map was validated by analyzing the ppv and the sensitivity of the resulting predicted binding site. the validation results for the different statistical tools are compared in this section.

score map histograms
for each statistical tool, the score distribution of the points within the real binding site was compared with the score distribution of the points within the rest of the protein surface. the score was weighted by the area of the targeted zones. ten bins histograms are presented in figure  <dig> 

a histogram is given when the exact ppvs are taken as the score, another one represents the expected values for a random method, and each other one corresponds to a specific statistical tool. red hatched bars represent the proportion of score for surface points belonging to the binding site and green plain bars for points belonging to the rest of the surface. an ideal histogram would show 100% of the observation in the first bin for the non-site parts and 100% of the observations in the last bin for the site parts. in the histograms corresponding to classifications , a lot of binding site parts have a small score, what leads to a lot of false negative results. on the other hand, in the histograms corresponding to regressions , as for the "exact" histogram, the repartition is more distinct between site and non-site zones, what makes the prediction more accurate.

binding site prediction
the location of potential binding site is predicted by applying a threshold on the score map. then, the results vary with the chosen threshold and can be adapted to limit the false positive or false negative rates. the predicted binding site was compared with the real one using two indices: the sensitivity  and the positive predicted value . they are defined as follow:

  

  

where s is the area of the ses, p is the analyzed patch and i is the true binding site. our whole method was tested using different statistical tools. a precision-recall graph comparing results for all these tools appears in figure  <dig>  curves corresponding to regression tools generally are above the one corresponding to classification tools, what is consistent with the previous observations.

in this experiment, the size of the predicted binding site was arbitrarily set to 1/ <dig> of the whole surface of the protein, i.e., points with the highest score were added to the predicted binding site until it attained 1/ <dig> of the total surface. results were compared to the expected indices obtained via random selection of the predicted binding site. as in this application, the area of the binding site and the area of the predicted binding site are unchanged for one protein, the expected values of both indices are estimated as follows:

  

  

where st is the whole surface area. s and s are considered to be constant. so, in these formulas, the only variable is the overlap between p and i. the expected proportion of p inside i is the same as anywhere else on the surface. then, e ) is this proportion weighted by s. the reasoning is the same for the proportion of i inside p. to quantify the success rate of our method with each statistical tool, the percentage of proteins of the data set with a higher index than expected  was calculated . as for other tests, the results are better for regressions than for classifications. in 84% of the cases, the mlp gave a better result than the expected value, whereas the best classification tool  gave a result of 78%.

percentage of proteins in the whole data set resulting on a ppv higher than the expected value for each tool.

the tool giving the best results was the mlp with a mean sensitivity and ppv of  <dig>  and  <dig> , respectively. a molecule with a predicted binding site representing approximately these mean values is depicted in figure  <dig> 

comparison with other methods
finally, our method was compared to other methods for which applications are available on the web: cons-ppisp  <cit> , pinup  <cit>  and sharp <dig>  <cit> . these three methods return a score for each residue. these scores were mapped on the protein surfaces and the binding site localizations were predicted as it was done for the scores resulting from the different statistical models. the tests were performed on the second dataset . a precision-recall graph comparing results for all these methods appears in figure  <dig>  performances of our method using mlp are higher than those of sharp <dig> method and comparable to those of the pinup and cons-ppisp methods. the percentage of proteins of the data set with a higher index than expected  was also calculated . for both our method using mlp and the pinup method, the result was better than expected in 71% of the cases. these results are a bit worse than for the other dataset, probably because the training dataset was made of proteins from bounded structures.

percentage of proteins in the whole data set resulting on a ppv higher than the expected value for each method.

CONCLUSIONS
in this paper, we present a patch based binding site prediction method based on either classification or regression tools. this was motivated by the fact that patches based method presented in the literature generally use classification tools. in this case a binary classifier is trained to discriminate patches totally superposed with the real binding site from patches lacking joined surface with it. using regression instead of classification allows to include patches partially overlapping the genuine binding site during the training step. the variable to be estimated by the regression is the overlap between the real binding site and the patches constructed on the protein surface.

using leave-one-out cross-validation, we showed that regression tools have better predictive performance than classification ones. as the patches constructed during the application step partially overlap the genuine binding site, the predictions for these patches generally are more correlated with their ppv when regression is used. among regression tools, the multilayer perceptron is the most efficient. in 84% of cases, with dataset  <dig> , the method using an mlp for regression, allowed a better prediction than the expected value via random selection. our method used with mlp was also compared with three methods usable through a web server. our method performed better than sharp <dig>  which is also a patches based method, and performed equivalently to the two other methods.

to sum up, regression tools appeared to be more efficient than classification tools for a new patches based method comparable with existing binding site prediction methods. when possible, using regression instead of classification for other predictors will probably improve the results, not only when patches are used, but every time the output is a continuous variable.

the method presented in this paper is also flexible. indeed, the final predicted binding site is obtained by applying a threshold on the surface score map derived form the prediction of the regression or the classification tools. so, the size of the final predicted binding site is adaptable to the approximate false positive and negative rates required for the final application. therefore, the output of our method is a single predicted binding site instead of a list of top ranked patches  <cit>  or residues  <cit> .

in future works, our method will be applied on real cases and combined with other bioinformatics tools. for instance, the combination of our method with dynamic molecular simulation will be used to study the impact of residue mutation on the location of the binding site.

list of abbreviations
pdb: protein data bank; ses: solvent excluded surface; sas: solvent accessible surface; ppv: positive predictive value; lin.reg.: multiple linear regression; svm: support vector machine; plsr: partial least square regression; pcr: principal component regression; mlp: multilayer perceptron; rfr: random forest regression; rfc: random forest classifier; nbc: naive bayes classifier.

authors' contributions
jg developed the features extraction part, the patch construction, the predicted binding site localization. he participated at the whole writing process. ja applied and tested the statistical tools. he participated at the whole writing process. jlg supervised the biological part of the work. he helped to finalize the manuscript. bm supervised the whole work. he helped to finalize the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
dataset <dig>  dataset <dig> pdf contains the list of the  <dig> proteins pdb codes and names of the first dataset. it is the dataset used by bradford et al.  <cit> . in this work, it was used for the statistical models training and for the comparison between the different statistical tools. this dataset is composed of proteins in the bounded conformation and had already been filtered at 20% sequence identity.

click here for file

 additional file 2
dataset <dig>  dataset <dig> pdf describes the second dataset. it consists of  <dig> proteins in the enzyme/inhibitor category of docking benchmark  <dig>   <cit> , after filtering at 35% identity. the genuine binding site was identified using the bounded complex but the tests were performed on the unbounded structures. the first column of the file contains the pdb codes of the bounded complexed structures. the second and the third column contain the pdb codes and the names of the unbounded structures of the enzymes and inhibitors respectively.

click here for file

 acknowledgements
jg is funded by nanotic/tsarine, a project of the rÃ©gion wallonne of belgium . ja is funded by nanotic/dediccas, a project of the rÃ©gion wallonne of belgium .
