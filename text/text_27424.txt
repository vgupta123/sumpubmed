BACKGROUND
the rapid development of dna sequencing technologies has led to a huge number of dna sequences. it is possible now to obtain large amounts of individual genomes sequenced in one week with less than us$ <dig>   <cit> , using the high-throughput sequencing technologies, such as single-molecule sequencing  <cit>  and next-generation sequencing   <cit> . consequently, dna sequence analysis faces serious computational challenges due to the huge amounts of data.

similarity evaluation between sequences is a crucial starting point for analyzing genomic sequences and has a wide range of applications. one important application is to discover the evolutionary relationship between species. this is based on the assumption that two species having similar sequences are close in evolutionary relationship. another popular application is to search similar sequences in databases. the databases may be huge in size, hence an effective and efficient method for defining and computing the similarity between sequences is badly in need. in addition, there are many reference-based tools and algorithms for sequence compression, such as green  <cit> , rlz  <cit>  and so on. for these algorithms, the choice of reference sequence has a significant impact on both compression ratio and compression time. therefore, a preprocessing step that assesses the similarity between sequences and then selects the most similar one with the others as the reference is critical to compression performance.

due to the importance of sequence similarity analysis, a dozen of algorithms have already been developed. these algorithms can be roughly classed into two categories. the first category is based on sequence alignment, which has been reviewed in  <cit> . sequence alignment is powerful for comparison between two related genomes; blast  <cit> , fasta  <cit>  and mega  <cit>  are typical sequence alignment tools. however, sequence alignment depends on the orderings of the nucleotides and may be computationally prohibitive. and for comparing long sequences, alignment-based methods take too much time. for example, cudalign  <dig>  <cit> , one of the state-of-the-art parallel alignment methods, spends more than  <dig>  seconds when comparing two sequences of size ~ <dig> mb using  <dig> gpus. fortunately, alignment is not must for similarity analysis.

the second category is alignment-free methods, some of which are based on word  frequency. the frequencies of words within a dna sequence are calculated and compared between dna sequences using statistical distances. a review on these algorithms can refer to  <cit> . blaisdell  <cit>  introduced the first word frequency based method and used the euclidean distance for assessing sequence similarity. wu et al.  <cit>  proposed methods based on kullback-leibler discrepancy between frequencies of words, mahalanobis distances and standardized euclidean distances under markov chain models of base composition.

some other alignment-free methods use geometric representations for dna sequences. with these methods, sequences are transformed to 2d  <cit> , 3d  <cit> , 4d  <cit>  and 5d  <cit>  spaces and so on. such graphical representation techniques provide a way to visually measure similarity and dissimilarity between dna sequences. however, they are time-consuming when comparing long sequences. recently, there are also methods based on entropy. li et al.  <cit>  introduced the weighted pseudo-entropy, which is used for constructing representative vectors of dna sequences. and zhang et al.  <cit>  converted dna sequences into time sequences and then used the approximate entropy  <cit> .

in this paper, we develop a novel method based on frequent sequential patterns and entropy  to represent dna sequences. concretely, each sequence is first divided into blocks of the same length. then, a modified prefixspan  <cit>  algorithm is used to discover the maximal frequent patterns in each block. finally, with the probabilities of these patterns, the entropy of each block is calculated. the resulting entropies of the blocks constitute the components of the vector of the sequence. note that sequences are usually different in size, hence their vectors may have different dimensions.

we conduct extensive experiments to evaluate the proposed method on the β-globin genes of  <dig> species. by evaluating the correlation coefficient between the calculated similarities and the results from mega  <cit> , the proposed method fpe achieves higher correlation coefficients than two recently-developed alignment-free methods  <cit>  and blastn  <cit> . further comparison analysis also shows that fpe is more accurate than the two recently-developed methods, and our results agree well with the evolutionary fact.

methods
the pipeline of our method fpe is shown in figure  <dig>  which consists of five steps, each of which is implemented by a module. first, each sequence is preprocessed by the preprocessing module. second, each sequence is divided into blocks of the same size by the sequence blocking module. then, a modified prefixspan  <cit>  algorithm is used to discover the maximal frequent patterns in each block, which is done by the modified prefixspan module. the entropy of each block is calculated with the probabilities of discovered patterns in the block, which is finished by the entropy calculation module. for each sequence, the entropy values of all blocks form the dimensional components of its final representation vector. finally, the similarity calculation module computes the similarity between any two sequences using their vectors obtained above. we describe each step in detail in the following subsections.

preprocessing
for each sequence or genome to be processed, before partitioning it into blocks, a preprocessing is performed, which includes: 1) converting all characters to uppercases; 2) discarding all non-base characters; 3) ignoring all line-breaks in each sequence file.

sequence blocking
in our approach, each sequence is first split into several blocks, each of which consists of the same number of consecutive bases. the blocks are independent of each other and the block size can be changed in practice. note that if the size of the last block is smaller than the specified block size, this block will be discarded. to be clearer, following is an example. in this example, there are two sequences. assuming that the block size is  <dig>  these two sequences are divided into  <dig> and  <dig> blocks, respectively. and the last blocks of size  <dig> are discarded.

example  <dig> consider the two sequences in figure  <dig>  the block size is  <dig> and each block is underlined or double underlined. and the last blocks with strikethrough lines are discarded.

sequence blocking is an important step that brings two major advantages. first, the blocking strategy can capture fine-granularity information of sequences, including location and ordering information. second, even for the long sequences, blocking can reduce memory and time consumption for sequence processing.

mining maximal frequent patterns from sequences
most of the previously proposed methods based on word frequency select some fixed-length words and then calculate their frequencies, such as  <cit>  and  <cit> . as dna sequences are strings generated from the alphabet {a, g, c, t}, there are totally 4k words of length k, which are also called k-mers in the literature. to describe a sequence well, the parameter k should be carefully selected, which possibly depends on the application domain. to avoid information loss, as many as possible patterns  should be considered. however, this will unavoidably introduce many low-information patterns . keeping this in mind, our method in this paper tries to avoid manually determining the value of parameter k while taking all important patterns into account.

in addition, considering that subsequence rearrangements are normal during the biological evolution process, if there are too many rearrangements in the sequences, the results of alignment based methods may be unreliable.

therefore, in this paper we adopt a modified prefixspan  <cit>  algorithm to discover the frequent patterns in dna sequences and consider only the maximal frequent patterns. this makes our method be considerably tolerant of subsequence rearrangement and noise. in the experiments and results section, we will present the experimental results that show the proposed method's tolerance of noise and subsequence rearrangements.

prefixspan  <cit>  is an efficient algorithm for sequential pattern mining. however, our problem is a little different from traditional sequential pattern mining. instead of mining a sequence database, we process a single sequence. and there is no gap between the items  in each pattern. we give a formal definition of our problem as follows:

definition  <dig> . given a dna sequence s that is a sequence of bases denoted by s = <s1s <dig> ...sn> where n = |s| is the length of sequence and si is a character from the charset Ω = {a, t, c, g}, and a predefined minimum support threshold smin, the support  of a subsequence of s is the occurrences of the subsequence in s. a subsequence  < sk sk+ <dig> ..sm > is a frequent pattern if its support sup is no less than smin. a maximal frequent pattern is the one that none of its super-sequences are frequent. our problem is to find all the maximal frequent patterns in the dna sequence.

in frequent pattern mining, a close concept to maximal frequent pattern is closed frequent pattern. a closed frequent pattern is the one that none of its proper super-sequences have the same support as itself. so maximal frequent patterns must be closed frequent patterns. actually, mining maximal frequent patterns is done by mining closed frequent patterns.

example  <dig> considering sequence  <dig> in example  <dig>  let smin =  <dig> and check the sub-sequence 〈ct ga〉 in the first block of sequence  <dig>  its sup is  <dig>  so it is a frequent pattern in the first block of sequence  <dig>  furthermore, there is no any super-sequence of 〈ct ga〉 has a sup that is ≥  <dig>  so 〈ct ga〉 is a maximal frequent pattern in the first block of sequence  <dig> 

before describing the modified prefixspan algorithm, we give the definitions of prefix, suffix and projected database as definition  <dig> and definition  <dig>  these definitions are a little different from those in  <cit> . note that here we also perform pseudo-projection, instead of physical projection. that is, the projected database contains only the indexes of the suffixes, not the real suffixes. this technique is widely used in the area of frequent pattern mining. in the following algorithms and examples, we just use pseudo-projection for the α-projected database, s|α.

definition  <dig>  given a sequence s = <s1s <dig> ...sn>, we say sequence δ = <s1s <dig> ...sm> with m < n is a prefix of s, and sequence γ = <sm+1sm+ <dig> ..sn> is the suffix of s with regard to prefix δ, which is denoted as γ = s/δ.

definition  <dig>  let α be a sequential pattern in a sequence s, the α-projected database, denoted as s|α, is the collection of suffixes of s with regard to prefix α.

algorithm  <dig> outlines the mining process. assuming that the current pattern is frequent, the algorithm extends it by appending one base at a time , and constructs the corresponding projected database . if the extended pattern is frequent and closed , then the algorithm recursively calls itself with the extended pattern . therefore, the current pattern is always closed. if the current pattern cannot extend to any frequent pattern , it is maximal according to definition  <dig>  and if the pattern is long enough, it will be saved with its projected database .

to check whether a frequent pattern is closed, we adopt the method proposed in  <cit> , which is outlined in algorithm  <dig>  first, we calculate the start positions of the pattern's occurrences in the input sequence , using its pseudo-projected database. that is, we subtract the length of the pattern from each value in the pseudo-projected database . then, we check whether the set of these positions is a subset of any single-item-projected database . here, a single item means any base in {a, g, c, t}. if the answer is "yes", the pattern is impossible to be a closed frequent pattern.

algorithm 1: freqpattens -- the modified prefixspan algorithm for mining maximal frequent patterns from dna sequence.

input : Ω - the charset of bases, namely, {a, t, c, g};

smin - the minimum support threshold;

lmin - the minimum pattern length;

α - the current pattern;

s|α - the α-projected database;

output: r - the set containing all the maximal frequent patterns and their corresponding projected databases;

 <dig> ismaximal = true;

 <dig> foreach s ∈ Ω do

 <dig>     append s to α to form a new pattern β;

 <dig>     construct the β-projected database s|β ;

 <dig>     if |s|β | ≥ smin then    /* β is frequent */

 <dig>         ismaximal = false;

 <dig>         if isclosed then

 <dig>             call freqpattens;

 <dig> if ismaximal = true and |α| ≥ lmin then    /* α is maximal */

 <dig>     r = r ∪ {α, s|α};

 <dig> return r;

to have a better understanding of the mining process, we give an example as follows:

example  <dig> the mining process of the first block of sequence  <dig> is presented in table  <dig>  assume that both the minimum support threshold and the minimum pattern length to be  <dig>  therefore, 〈a〉, 〈g〉, 〈c〉 and 〈t 〉 are dropped for not reaching the required length. and the patterns whose supports are smaller than  <dig> are given up, such as 〈ca〉 and 〈cc〉 etc. patterns that are not maximal, such as 〈ct 〉 and 〈ct g〉 etc. are discarded. note that 〈ga〉 and 〈t ga〉 are still dropped because they are not closed frequent patterns. take 〈ga〉 for example. first, we compute the start locations of the pattern's occurrences in the sequence block, and obtain the set { <dig> -  <dig>   <dig> - 2} = { <dig>  18}. obviously, this set is the subset of 〈t 〉 -projected database, so 〈ga〉 is not a closed frequent pattern. then, we obtain three maximal frequent patterns: 〈at 〉, 〈ct ga〉 and 〈t c〉.

each row represents one recursive step. the numbers after each pattern represent the starting locations of the suffixes, which are the so-called pseudo-projections. patterns in bold are maximal.

algorithm 2: isclosed -- determining whether a pattern is closed.

input : α - the pattern;

s|α - the α-projected database;

output: true - if the pattern is closed;

false - if the pattern is not closed;

 <dig> p = ∅;    /* initialize the position set */

 <dig> foreach c ∈ s|
α 
do

 <dig>    p = p ∪ {c − |α|};

 <dig> if p ⊆s|a or p ⊆s|c or p ⊆s|g or p ⊆s|t then

 <dig>    return false;

 <dig> else

 <dig>    return true;

entropy calculation
so far, we have obtained all the maximal frequent patterns for each sequence block. before evaluating the entropy of each block, the probability of each pattern in the block is computed as follows:

  ppat=spatlblock-lpat+ <dig> 

where spat is the support of the pattern, lblock is the length of the block, and lpat is the length of the pattern. it is obvious that the probability of each pattern is positively correlated with its support and its length. when the length of pattern increases to the length of the block, its support will become  <dig> so that the probability equals  <dig>  and the probability equals  <dig> when the support drops to  <dig>  as we consider only maximal frequent patterns, and smin is usually > <dig>  those two extreme cases will not happen.

then, the entropy of a block is defined as below:

  h=-1∑pat∈rspat∑pat∈rspatppatln 

where r is the set of maximal frequent patterns mined from the block. finally, the entropies of all blocks constitute the dimensional components of the final vector of the sequence.

example  <dig> from the 1st block of sequence  <dig> in example  <dig>  we have obtained three maximal frequent patterns and their probabilities are shown in table  <dig>  then, it is easy to get its entropy  <dig>  via eq. .

spat
lpat
ppat
similarity calculation
in the above subsection, we have obtained a representative vector for each sequence.

however, as the vectors may differ in size, there should be a special way to measure

the similarity of two vectors of different sizes.

let v1={v <dig> v <dig> ⋯,v1m} and v2={v <dig> v <dig> ⋯,v2n} be the vectors of two different sequences, and assume that  <dig> ≤ m ≤ n. first, we search a start location k in v <dig> -- the longer vector, such that the following equation holds:

  |v2k-v11|=min1≤i≤n-m+1|v2i-v11|,1≤k≤n-m+ <dig>  

then, we calculate the distance between v <dig> and v <dig> as follows:

  dist=nm∑i=1mv2k+i-1-v1i <dig>  

when n equals m, dist is degenerated to euclidean distance.

example  <dig> considering the two sequences in example  <dig>  we obtain v <dig> = { <dig> ,  <dig> } and v <dig> = { <dig> ,  <dig> ,  <dig> }. then, we get k =  <dig> and the distance between the two sequences is evaluated as follows:

 dist=322+02= <dig> . 

experiments and 
RESULTS
data used in this work
we choose the β-globin genes of  <dig> species, which are widely used for evaluating the performance of sequence similarity analysis methods. the details of these genes are given in table  <dig>  because most existing methods do not provide executable tools or source codes, we do not compare with them on other datasets. all the data are available in the genbank repository  <cit> .

the location column gives the start/end location of the sequence of each gene.

 <cit> http://www.ncbi.nlm.nih.gov/genbank

experimental setting
we compare our method  with blastn  <cit>  and two recently-developed alignment-free methods  <cit> . the results of the mega  <cit>  software are used as the ground truth. the experiments are done on a pc with intel xeon e <dig>  <dig> ghz cpu and  <dig> gb memory. the operating system is ubuntu  <dig> . by default, we run our method with the minimum support threshold being  <dig> and the minimum pattern length being  <dig> 

experimental results and analysis
preservation of fine-granularity information
our method can capture fine-granularity information of sequences via blocking. to show this, we examine how the distance between human sequence and gorilla sequence changes with block size. the results are presented in figure  <dig>  it can be seen that as block size decreases, the distance becomes larger, because smaller block size means finer information exploited in the representative vectors. this is a proof that our method can capture fine-granularity information.

tolerance of sequence rearrangement
tolerance of noise
to test noise tolerance of our method, we randomly insert some letters selected from the charset {a, t, c, g} into the human sequence. we define the noise ratio as the proportion of the number of inserted letters over the length of the sequence. given a noise ratio, we first generate  <dig> contaminated sequences, then calculate the distance between the original sequence and each of the  <dig> contaminated sequences, and finally we evaluate the average distance over the  <dig> tests. figure  <dig> shows the averaged distance between the original sequence and the contaminated sequences when noise ratio increases from  <dig>  to  <dig> . here, we set the block size to  <dig>  which is much larger than the length of human sequence and the contaminated ones, so there is actually no blocking over the sequences. we can see that the distance increases as more noise is added. however, when noise ratio is less than  <dig> , the distance is smaller than the distance between the human sequence and the gorilla sequence, which is  <dig>  under this setting. this demonstrates that our method is substantially tolerant to noise.

similarity analysis
comparison with existing methods
we compare our method with three existing methods, including two recently-developed alignment-free methods and blastn  <dig> .29+  <cit> . one of the two compared alignment-free methods was developed by li et al. in  <dig>  <cit> , the other was developed by yu and huang in  <dig>  <cit> . as in  <cit> , the results obtained by the mega  <cit>  software -- a famous alignment-based tool, are used as the baseline. we also calculate the pearson correlation coefficient between the results of mega and those of our method and the three compared methods. table  <dig> shows the distances between human and other species via the five different methods . here, the last column lists the pearson correlation coefficients between the results of mega  <dig>  and those of our method and the three compared methods. for blastn, we use the maximum scores and normalize them to the range between  <dig> and  <dig> as the similarity, then the distance is  <dig> minus the similarity. as we can see, our method achieves the highest correlation with mega. this shows that our method can calculate the similarity between dna sequences more accurately.

note that the results from  <cit>  are based on the sequence of the first exon in each β-globin gene.

to make it clearer, we also present the distances between human and the other tested species calculated by our method and the three compared methods as well as mega in figure  <dig>  here, the distance is normalized to the range between  <dig> and  <dig>  we can see that the distance between opossum and human evaluated by the method proposed in  <cit>  is not large enough, which violates the fact that opossum is the most remote species among the mammals; while the distance between lemur and human obtained by the method proposed in  <cit>  is too large, which also disagrees with the evolutionary fact. however, for our method, the curve is more similar to that calculated by the mega method, which again shows that our method achieves the highest correlation with the ground truth.

discussion
existing methods for measuring similarity/dissimilarity between dna sequences roughly fall into two types: alignment-based methods and alignment-free methods. as alignment-based methods may be computationally expensive and not scalable to huge datasets, alignment-free methods have been extensively investigated recently.

for the alignment-free methods, on the one hand, algorithms based on word frequency are sensitive to the length of words used, and may include noise if taking all the words into account. on the other hand, algorithms based on geometric representation provide visual comparison among dna sequences locally and globally. however, for long sequences, these methods may require too much computation overhead and memory. recently, entropy-based methods provide simple representations for the sequences, but they are prone to lost some important information, for example, location and ordering information. as they treat all symbols or patterns equally, they may also include noise.

our method is based on frequent patterns and entropy. as we consider only the maximal frequent patterns in a sequence, our method can considerably tolerate noise and sequence rearrangements. furthermore, by using the blocking strategy, fine-granularity information of the sequences can be captured.

however, for more accurately evaluating the similarity between two sequences, some parameters have to be tuned. in our algorithm, the block size, the minimum support threshold and the minimum pattern length can be changed for different sequences. note that to compare two sequences, the block size should be the same. and the larger the block size, the more information will be lost. here, we present the following rule of thumb for parameter tuning:

1) by experiments, we found that it is better to set the block size less than  <dig>  which can be also observed from figure  <dig> 

2) once the block size is determined, there may be an optimum pair of the minimum support threshold and the minimum pattern length to be determined.

3) we observed a negative correlation between the minimum support threshold and the minimum pattern length. this means that if the minimum support threshold is large, the minimum pattern length should be set to relatively small. otherwise, we may get no maximal frequent patterns in some blocks, and thus lose too much information.

finally, we want to point out that different methods have their own application scenarios. for example, methods based on geometric representation are very powerful tools for visually and intuitively analyzing the sequences. as for our method, we provide an effective way to represent a dna sequence to a vector, which is suitable for searching similar sequences in databases or acting as a preprocessing step for other applications. considering that our method has been shown to provide more accurate distances, so it is also suitable for discovering evolutionary relationships.

CONCLUSIONS
this paper presents a novel method based on frequent patterns and entropy to represent the dna sequences and evaluate their similarities. by using blocking technique, our method can capture fine-granularity information of sequences. our method can also tolerate noise and sequence rearrangements because we take only the maximal frequent patterns into account. experiments over the β-globin genes of  <dig> species show that our method achieves more accurate distances than two recently-developed alignment-free methods and the blastn tool.

competing interests
the authors declare that they have no competing interests.

authors' contributions
shuigeng zhou and jihong guan conceived the study, and revised the manuscript. xiaojing xie designed the algorithm, performed all experiments and data analysis, and drafted the manuscript.

