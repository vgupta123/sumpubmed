BACKGROUND
with the development of electronic records, analysis of clinical narratives becomes increasingly important since such narratives often contain vast quantity of useful information about patients and health  <cit> .

in recent years, there has been a lot of work in information extraction from clinical texts. earlier studies mainly focused on rule- or dictionary-based methods. as examples, medlee  <cit>  used a vocabulary to recognize and classify words into semantic categories and then matched the sequences of semantic categories to structures defined in the grammar. metamap  <cit> , which adopted a knowledge intensive approach, mapped biomedical texts to the umls  <cit>  metathesaurus. mork et al.  <cit>  expanded a large number of term lists of drug phrases based on umls and used the lists to validate drug and indication relationships.

on the other hand, there have been various machine learning methods proposed recently for clinical text information extraction. roberts et al.  <cit>  treated a clinical relation extraction task which aims to extract relations between clinical entities such as a drug entity and a condition entity as a classification problem and applied support vector machine  model to accomplish it. lu et al.  <cit>  considered chemical compound and drug recognition as a sequence labeling problem and developed a high-performance named entity recognition system by integrating condition random field  with word clustering. he et al.  <cit>  combined dictionary look-up and crf method to recognize drug names. zhu et al.  <cit>  used svm to separate biological terms frombiological non-biological terms, before they used crf to determine the types of terms, which made full use of the power of svm as a binary-class classifier and the data-labeling capacity of crf.

in this paper, we present an approach to recognize disorder mentions from clinical narratives, which can be very complicated in some circumstances. in fig.  <dig>  sentence 1)–4) give some disorder mention examples.
fig.  <dig> examples of disorder mentions




in sentence 1), there is a disorder mention dyspnea on exertion, which is a contiguous disorder. in sentence 2), there is a disorder mention spleen enlarged, which is a discontiguous one. in sentence 3), there are two disorder mentions: abdomen nontender and abdomen nondistended. the two disorder mentions share the left boundary word abdomen. in sentence 4), there are also two disorder mentions hip abrasion and erythematous. the span of the first disorder mention hip abrasion covers the second one erythematous. the disorder mentions in sentence 3) and 4) are overlapping ones.

traditional structured svm  model  <cit>  can recognize contiguous and discontiguous disorder mentions, but it has trouble recognizing overlapping disorder mentions. to accomplish this disorder mention recognition task, we describe a multi-label scheme, which can record the information of different disorder mentions in the overlapping cases at the same time. combined with the multi-label scheme, our ssvm model performs well in the experiment.

related work
multi-label classification
there are two main strategies for multi-label classification: a) problem transformation methods and b) algorithm adaptation methods  <cit> . on the one hand, problem transformation methods transform multi-label problems into one or more single-label problems. boutell et al.  <cit>  solved the problem of semantic scene classification, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels. they considered a multiple class label as a new single label. on the other hand, some classification algorithms can handle multi-label data directly, such as adaboost.mh and adaboost.mr  <cit> , c <dig>  algorithm  <cit>  and ml-knn  <cit> .

complicated entity recognition
it is difficult to model entities that consist of discontiguous words or entities that share the same words. based on bio encoding scheme  <cit> , tang et al.  <cit>  proposed the biohd multi-label method. in this method, “h” denotes head entities which are consecutive sequences of tokens shared by multiple disjoint concepts in a sentence while “d” denotes non-head entities which are consecutive sequences of tokens in a disjoint concept not shared by other disjoint concepts in a sentence. later, tang et al.  <cit>  came up with a variant scheme biohd <dig>  where “1”, “2”, “3” and “4” indicate that a non-head entity is combined with the nearest head entity at left, the nearest non-head entity at left, the nearest head entity at right and the nearest non-head entity at right respectively. to recognize nested biomedical named entities, lee et al.  <cit>  came up with a two-phase method based on svms, which consists of a named entity boundary identification phase and a semantic classification phase.

overview of conference and labs of the evaluation forum 2013
the data set used in our work is from the task  <dig> of conference and labs of the evaluation forum   <dig> . to the best of our knowledge, the best f1-score for this data set is  <dig>  so far, achieved by tang et al.  <cit>  in  <dig> 

methods
overall approach
we take the disorder mention recognition task as a sequence labeling problem. ssvm model performs well in classification tasks with complex outputs, such as trees, sequences, or sets  <cit> , and we adopt the ssvm model to fulfill this task along with our multi-label scheme.

the detailed algorithm flow is represented in fig.  <dig>  in the multi-label scheme, for every disorder mention that a token belongs to, there is a sub-label to record the disorder mention. all the sub-labels of each token would be integrated into just one bitwise multi-label, which is called a final label. then we convert the final labels into decimal labels and feed the training data with decimal labels to our ssvm model. in the prediction phrase, what the ssvm model predicts are decimal labels as well, which will be converted into final labels. finally, all the sub-labels will be extracted from final labels and the corresponding disoder mentions are obtained.
fig.  <dig> the flow of our model




since labeled data are always scarce while unlabeled data are abundant, we generate clustering-based word representations as features to reduce the dependence on the labeled data and further improve the model  <cit> .

design of the multi-labels
as shown in table  <dig>  there are six types of multi-labels, b, i, l, o, u and c, all of which are in the form of 24-bit binary numbers. labels in classes b, i, l, o and u stand for the beginning, inside, last and outside tokens of multi-token disorder mentions as well as unit-length disorder mentions , respectively, like the bilou encoding scheme  <cit> . labels in class c denote tokens that play different roles in several disorder mentions simultaneously. for example, a token, which is the beginning of a disorder and the last of another disorder at the same time, is represented by labels in class c.
class u
6
a
5
a
4
a
3
a
2
a
1
class b
6
b
5
b
4
b
3
b
2
b
class l
6
c
5
c
4
c
3
c
2
c
class i
d
6
d
5
d
4
d
3
d
2
d
class c
d
6
d
5
d
4
d
3
d
2
d
 <dig> c
6
c
5
c
4
c
3
c
2
c
 <dig> b
6
b
5
b
4
b
3
b
2
b
 <dig> a
6
a
5
a
4
a
3
a
2
a
1
class o
to make the 24-bit label easier to understand, extra commas are used to split the label




in table  <dig>  a
i,b
i,c
i and d
i  represent binary  <dig> or 0; there are  <dig> variable bits  and  <dig> constant bits in labels in class u, b, l and i. the variable region lies in the rightmost  <dig> bits in labels in class u, the 7th to 12th bits from the right in labels in class b, the 13th to 18th bits from the right in labels in class l and the leftmost  <dig> bits in labels in class i. meanwhile, the rest  <dig> constant bits of the above  <dig> types of multi-labels are filled with binary  <dig>  unlike the above  <dig> types of labels, labels in class c consist of  <dig> variable bits and labels in class o are made up of  <dig> bits of  <dig>  labels in class c can be divided into four variable regions, each of which has the same position with the variable region in labels in class u, b, l and i, respectively. furthermore, some constraints need to be fulfilled in this multi-label scheme. there must be at least one bit  <dig> in labels in class u, b, l and i. and in labels in class c, there must be two or more variable regions where there is at least one bit  <dig>  as for the sub-labels, except that there are no sub-labels in class c, the types of the sub-labels are the same as the multi-labels introduced above. additionally, the sub-labels are made up of  <dig> bit binary  <dig> and  <dig> bits binary  <dig> 

the reasons why we choose binary numbers as the multi-labels are as follows: 1) each bit stands for information of a disorder mention, so that a binary number, namely a multi-label, can record information of many disorder mentions. 2) bitwise operations make it convenient to integrate sub-labels into a final label and extract sub-labels from a final label.

application of the multi-labels
although our multi-label scheme is based on bilou scheme, the way we use it is different from traditional ways. in sentence 5), there is a disorder mention tricuspid
leaflets
thickened. in the traditional bilou scheme, this sentence would be labeled as the/o tricuspid/b valve/o leaflets/b are/o mildly/o thickened/b./o. this method would run into trouble when there are multiple disorder mentions in a sentence. while in our method, this sentence would be labeled as the/o tricuspid/b valve/o leaflets/i are/o mildly/o thickened/l./o. there is therefore no confusion between multiple mentions and a single discontiguous mention using our method . 5) the 
tricuspid
 valve 
leaflets
 are mildly 
thickened.

6) abdomen
 is soft, 
nontender, nondistended
, negative 
bruits.




the first step of this multi-label method is to assign each token sub-labels. take sentence 6) as an example. there are three disorder mentions: abdomen bruits, abdomen nontender, and nondistended. when we implement our model, the disorder mentions are also encoded in this order, but actually the order of disorder mentions does not matter. the sub-labels of the three disorder mentions are shown in fig.  <dig>  in the beginning, we obtain the sub-labels of tokens through assigning  <dig> to the bits a
 <dig> b
 <dig> c
 <dig> d
 <dig>  and  <dig> to other bits according to the token’s ordering in that disorder, just like sub-labels of the first disorder mention abdomen bruits. we assign abdomen a sub-label in class b with its bit b
 <dig> set to  <dig> and assign bruits a sub-label in class l with its bit c
 <dig> set to  <dig> since abdomen is the beginning of the first disorder mention and bruits is the last. then, if the next disorder mention overlaps with the former one, the sub-labels of the next disorder mention are acquired through assigning  <dig> to the bits a
 <dig> b
 <dig> c
 <dig> d
 <dig> and  <dig> to other bits, just like the sub-labels of the second disorder mention abdomen nontender shown in fig.  <dig>  in the same way, when there are more disorder mentions overlapping with former ones, we obtain the sub-labels by assigning  <dig> to a
i,b
i,c
i,d
i, in which the subscript i increases one by one. thus when the third disorder mention nondistended which overlaps with the former two comes, a sub-label in class u with its bit a
 <dig> set to  <dig> is assigned to the third mention nondistended since it is a unit-length disorder mention, as shown in fig.  <dig>  when there comes a disorder mention which does not overlap with any former disorder mentions within the sentence, the bits to be assigned  <dig> come back to a
 <dig> b
 <dig> c
 <dig> and d
 <dig>  after that, we continue acquiring all the sub-labels by repeating the above process.
fig.  <dig> examples of the sub-labels




in view of the limited bits of our multi-labels, there can be up to six disorder mentions overlapping with each other. if needed, we can raise the limit by expanding the scope of the binary numbers that represent our multi-labels.

after all the sub-labels of every token are obtained, we need to integrate them into a final label by doing bitwise or operation, as the algorithm  <dig> shows. take the token abdomen in sentence 6) as an example, as shown in fig.  <dig>  its sub-label “  <dig> , <dig> ” and “  <dig> , <dig> ” are integrated into a final label “  <dig> , <dig> ”. every binary  <dig> in the final label indicates the information of a disorder mention. for example, the final label “  <dig> , <dig> ” means its corresponding token is the beginning of two different disorder mentions; another example of the final label “  <dig> , <dig> ” means this token is not only an inside token of a disorder mention but also the last token of another mention. the final labels of tokens of sentence 6) are listed in table  <dig> 
fig.  <dig> example of algorithm 1









when prediction is finished and the predicted decimal labels have been converted into bitwise final labels, the next step is to extract sub-labels from final labels using algorithm  <dig>  the and operator denotes the bitwise and operation. algorithm  <dig> scans all the  <dig> bits in a final label and it will output a sub-label for each bit  <dig> in the final label. take the token abdomen in sentence 6) as an example again, as shown in fig.  <dig>  there are two bits  <dig> in its final label “  <dig> , <dig> ”, which lie in the 7th and 8th bits from the right. correspondingly, algorithm  <dig> will output two sub-labels “  <dig> , <dig> ” and “  <dig> , <dig> ”, whose 7th and 8th bit from the right are assigned bit  <dig> respectively. after all the sub-labels are extracted, we need to gather sub-labels which have the same ranking position of binary  <dig> in their variable region and then extract the disorder mentions as bilou encoding scheme does. for instance, the sub-label sequence  can be regarded as a label sequence  in the bilou scheme.
fig.  <dig> example of algorithm 2








combining the ssvm model with this multi-label scheme, we can not only deal with the contiguous and discontiguous disorder mentions, but also the overlapping ones.

feature generation
we exploit several types of features:


 general linguistic features. these include the classic features for named entity recognition tasks, such as bag of words  and part of speeches . tokenization and pos tagging are conducted by stanford corenlp toolkit  <cit> .


 capitalization features. the reason why we use capitalization features is that various spelling habits of different people lead to different spellings of the same word. for instance, some doctors tend to write three times a day as “t.i.d.” while others may write “t.i.d.” instead. moreover, the grammar rule that the first word of a sentence should begin with a capital letter while the same word in other position should not is also a reason for that.


 case pattern features. case pattern features are helpful since mentions of the same semantic type often have similar capitalization patterns, such as c-polyp  and e-polyp .


 word representation features. previous studies showed that the unsupervised word representation features are beneficial to clinical named entity recognition tasks  <cit> . a common approach to induce word representation is to use clustering  <cit> . the unlabeled texts used for word clustering are the discharge records and various medical examination reports of  <dig>  patients derived from mimic  ii databases . word clustering is conducted by word2vec  <cit> , which provides an efficient implementation of the continuous bag-of-words and architectures for computing vector representations of word.


 contextual features. for each token, we combine above features of the contextual tokens together as the contextual features.

the detailed feature descriptions are presented in table  <dig> 



experiments
data set
the data set we used comes from task  <dig> of clef  <dig>  there are  <dig> clinical reports in the training set and  <dig> clinical reports in the test set. the clinical reports include discharge records, electrocardiogram, echocardiogram and radiology reports. table  <dig> gives the statistics for the three types of disorder mentions: contiguous and non-overlapping , discontiguous and non-overlapping , and overlapping. when multiple discontiguous disorder mentions overlap with each other, these mentions are categorized as overlapping.


table  <dig> statistics for discontiguous disorder mentions



table  <dig> statistics for overlapping disorder mentions



table  <dig> disorder mentions with different span lengths




among all the disorder mentions in the testing data set, the percentage of new disorder mentions, namely mentions that do not appear in the training data set, is about  <dig> %.

evaluation metrics
we use the precision, recall and f1-score in - to evaluate the performance  <cit> . 
  <dig> precision=tptp+fp 



  <dig> recall=tptp+fn 



  <dig> f1-score=2∗precision∗recallprecision+recall 


two evaluation modes are adopted. the strict mode requires that the predicted spans should be exactly the same as the answer. relaxed mode includes left match and right match mode. left match means the prediction is judged as correct as long as the left boundary matches correctly and right match is judged by the right boundary  <cit> . all the results presented below are evaluated in strict mode, unless explicitly specified.

experimental setup
we designed the following experiments to evaluate our model. first, in order to show the effect of the features we described above separately, a series of controlled experiments were set up. in these experiments, we added the features to the feature set one by one. second, crf model is widely used in sequence labeling tasks, therefore we take crf model as a baseline to compare with our ssvm model. the features and the multi-labels employed in the crf model are exactly the same as those in our ssvm model. last, in order to show the performance of our multi-label scheme, ssvm model with the biohd and biohd <dig> scheme, with which tang et al.  <cit>  achieved the best f1-score so far, are adopted as a baseline. the features employed are exactly the same as those in our ssvm model. we trained ssvm models with svm-hmm  <cit>  and crf model with crf++  <cit> . the parameters of our ssvm model and baseline models were optimized by 10-fold cross-validation on the training data set.

RESULTS
overall performancehypertension. addison’s disease. hypothyroidism. melanoma. bph.



8) upon arrival to  in preparation for cath, patient noted to be 
thrombocytopenic
 to  <dig> 





in sentence 7), there are five disorder mentions, hypertension, addison’s disease, hypothyroidism, melanoma and bph. our model could not recognize melanoma and bph until we added the word representation features. likewise, word representation features enable our model to recognize the disorder thrombocytopenic in sentence 8).

results in different evaluation modestable  <dig> results for different evaluation modes




there are many mistakes in recognizing left boundary of contiguous disorder mentions. in some cases, adjectives and nouns before disorder mentions are misjudged as the beginning of the mention. for example, there is a disorder mention fluid collects in sentence 9), while the prediction of the model is abdominal wall fluid collects. in some other cases, adjectives and nouns located at the left boundary of contiguous disorder mentions are often omitted. in sentence 10), there is a disorder mention multiple renal cysts, while the prediction is renal cysts. 9) reason: please drain abdominal wall 
fluid collects
  with ultras.


10) multiple renal cysts.




as we can see from table  <dig>  contiguous disorder mentions account for  <dig> % of all the mentions. furthermore,  <dig> % of contiguous disorder mentions appear after adjectives or nouns and the first tokens of  <dig> % of contiguous disorder mentions are adjectives or nouns so that this type of mistake makes a great difference.

performance for different types of disorder mentionstable  <dig> results for different types of disorder mentions




 contiguous disorder mentions
our model obtains the highest performance in recognizing contiguous disorder mentions among these three types of mentions. it is clear from the data that contiguous mentions are easier to recognize than the other two types of mentions. additionally, about  <dig> % of contiguous disorder mentions in our testing data are unit-length mentions, which are in the simplest form of disorder mentions.

 discontiguous disorder mentions
the results show that the recall of discontiguous disorder mentions is not good enough. the reason for that are: a) the samples of discontiguous disorder mentionss are too few, which only account for  <dig> % of all the disorder mentions. b) as table  <dig> shows, in some cases, the span lengths of many discontiguous disorder mentions are too large so that our model cannot capture their features.

 overlapping disorder mentions
the weakness of recognizing overlapping disorder mentions lies in the recall as well. there are mainly two reasons.

a) the samples of overlapping disorder mentions only account for  <dig> %. what’s more, table  <dig> indicates that the more disorder mentions overlap with each other at the same time, the sparser the multi-label of them will be. thus the performance in predicting tokens whose label contains many bits  <dig>  namely the token belongs to many disorder mentions, is poor. but from another perspective, tokens which belong to many disorder mentions simultaneously are rare so that they will not affect the final result too much. when the percentage of overlapping disorder mentions rises, the result would be better.

b) a disadvantage of our multi-label scheme is that the multi-labels of the same disorder mention may be different in some situations. sentence 11) and 12) are two examples . 11) abdomen: nontender.

12) abdomen: nontender, nondistended.




in sentence 11), there is only one disorder mention abdomen nontender. according to the multi-label scheme, the bit b
 <dig> of the multi-label of abdomen and the bit c
 <dig> of the multi-label of nontender would be assigned  <dig> because abdomen is the first and nontender is the last token of disorder mention abdomen nontender. thus, the label of abdomen is “  <dig> , <dig> ” and the label of nontender is “  <dig> , <dig> ”. but in sentence 12), there are two disorder mentions abdomen nondistended and abdomen nontender. the bits b
 <dig> and b
 <dig> of the label of abdomen would be assigned  <dig> because it is the beginning of both the two disorder mentions; the bit c
 <dig> of the label of nondistended would be assigned  <dig> because it is the last token of the first disorder mention abdomen nondistended; the bit c
 <dig> of the label of nontender would be assigned  <dig> as well because it is the last token of the second disorder mention abdomen nontender. thus, the label of abdomen, nontender and nondistended are “  <dig> , <dig> ”, “  <dig> , <dig> ” and “ <dig> ,  <dig> ”, respectively. therefore, the same disorder abdomen nontender may have different multi-labels in different situations so that our model may be confused. since the situation of sentence 11) would occurs much more frequent than sentence 12), our model would tend to predict the label of the disorder mention abdomen nontender as in sentence 11). to some extent, this characteristic weakens the performance of our model.

comparison with baselines
baseline 1: crf model with our multi-label scheme
as shown in fig.  <dig>  in strict mode, the best f 1-score of crf model is  <dig>  while the best f 1-score of our ssvm model is  <dig> ; in left match mode, the best f 1-score of crf model is  <dig>  while the best f 1-score of our ssvm model is  <dig> ; in right match mode, the best f 1-score of crf model is  <dig>  while the best f 1-score of our ssvm model is  <dig> . therefore, we can see ssvm model outperforms crf model in this task.
fig.  <dig> comparison between ssvm and crf model




baseline 2: ssvm model with biohd and biohd <dig> scheme
biohd and biohd <dig> multi-label scheme can deal with discontiguous and overlapping disorder mentions. but they also have some limitations. as tang said in  <cit> , neither biohd nor biohd <dig> scheme can represent sentences which contains two or more head entities, such as sentence 13) where there are two disorder mentions blood third ventricles and blood four ventricles. there are other complicated situations that neither biohd nor biohd <dig> can deal with, such as sentence 14) where there are two disorder mentions atrial pacemaker artifact and pacemaker capture. however, our multi-label scheme can handle all these complicated situations. figure  <dig> shows the results of biohd, biohd <dig> and our multi-label scheme. the performance for contiguous, discontiguous and overlapping disorder mentions are showed respectively. 13) there is a small amount of 
blood
 seen within the 
third
 and 
fourth ventricles.
fig.  <dig> comparison among biohd, biohd <dig> and our multi-label scheme




14) there is intermittent appearance of apparent 
atrial pacemaker artifact
 without 
capture.




for all the three types of disorder mentions, the f 1-scores of our multi-label scheme are higher than biohd and biohd <dig>  in particular, for overlapping disorder mentions, the f 1-score of our multi-label scheme is  <dig>  while the score of biohd and biohd <dig> are only  <dig>  and  <dig>  respectively. because there are few complicated sentences like sentence 13) and 14) in the data set, the advantage of our multi-label scheme is not fully reflected. in addition, when the percentage of overlapping disorder mentions rises, the performance of our model in recognizing overlapping disorder mentions would be better.

the experiments demonstrate that our multi-label scheme is better than tang’s biohd and biohd <dig> in this task. to figure out why our total f 1-score does not catch up with tang’s best f 1-score  <dig> , we performed a baseline in which we removed all discontiguous and overlapping disorder mentions, then we trained an ssvm model with bio scheme and our features, so that we can compare it with the results got under the same conditions except the feature sets in  <cit> . the results showed in table  <dig> indicate that features used by tang et al.  <cit>  are more effective than ours. this might be the reason why tang’s f 1-score is higher than ours. we would like to try more features to boost the performance of our model in our future work.



error analysis
with further analysis, the main errors of our model are categorized as follows.

 new disorder mention prediction error
the new disorder mentions account for  <dig> % in average among all the mentions in testing data set. although we added word representation features to increase the model’s ability to recognize new disorder mentions so that the recall of those mentions increased from  <dig>  to  <dig> , there are still  <dig> % of new mentions not recognized. the possible reasons are: a) some of the new disorder mentions have a very long span length . b) some of new disorder mentions have a complex structure so that there are few disorder mentions have similar features with them in the training data. consider the disorder mention elevated ce’s in sentence 15) as an example, few disorder mentions have the similar case pattern feature aaaaaaaa aa’a, pos feature jj nn pos and capitalization feature elevated ce’s in the context. 15) he was noted to have 
st segment elevations
 in inferolateral leads, 
elevated ce’s





 boundary error
errors often occur in the boundary of a disorder. a) adjectives and nouns before contiguous disorder mentions are sometimes misjudged as the beginning of the disorder. the examples in sentence 9) demonstrate these situations. b) adjectives and nouns located at the left boundary of contiguous disorder mentions are often omitted. the examples in sentence 10) demonstrate these situations.

 long distance dependency
as table  <dig> shows, the span length of some disorder mentions in the data are pretty long. for instance, the span length of the disorder abdomen tenderness in sentence 16) reaches  <dig>  long span length increases the difficulty of disorder recognition, especially when the span length exceeds the feature window sizes. 16) abdomen
: soft, 
nt/nd
, normoactive bs, no 
masses
, no 
rebound
 or 
tenderness.



fig.  <dig> results for disorder mentions with different span lengths




summarization for the multi-label scheme
to summarize, the major advantage of our multi-label scheme is that it can handle complicated situations in entity recognition tasks. the major limitations of our scheme are: 1) because the multi-labels of the same disorder mention may be different in some situations, the training instances of complicated disorder mentions would be sparse. 2) the number of possible disorder mentions is limited by the bits used in the multi-label scheme. however, the situations where there are more than six entities in a sentence are rare. moreover, we can use more bits to raise the limit if needed.

CONCLUSIONS
aiming at the disorder recognition task,we integrate a multi-label version of the bilou scheme with an ssvm model to create a novel multi-label ssvm model. using binary digits to record the disorder mention details, the multi-label scheme enables us to recognize complicated disorder mentions, e.g., those overlapping with each other. the best f 1-score of our model is  <dig> . in addition, for overlapping disorder mentions, the f 1-score of our multi-label scheme is  <dig>  higher than the baseline “biohd1234” scheme. this shows the perspective of the multi-label scheme in dealing with recognition of complicated named entities in biomedical text mining.

in the future, we would like to generate more features such as semantic group features. we also intend to address the problems described in the section error analysis. furthermore, we would like to try some other models such as neural network to recognize disorder mentions from clinical texts.

abbreviations
bowbag of words

clefconference and labs of the evaluation forum

crfcondition random field

mimicmultiparameter intelligent monitoring in intensive care

pospart of speeches

ssvmstructured support vector machine

svmsupport vector machine

