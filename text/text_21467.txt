BACKGROUND
ultra-high-throughput sequencing is having a growing impact on biological research by providing a fast and high resolution access to genome-scale information. the versatile technique can be used for unbiased genotyping  <cit> , transcriptome analysis  <cit> , protein-dna interactions <cit> , de-novo sequencing <cit> . while the sample processing is relatively streamlined, innovations in data management and information processing are necessary to exploit the full potential of the technology. a standard solexa/illumina genome analyzer "classic" run produces  <dig> gb of image files and  <dig> gb of processed data files over  <dig>  days totaling nearly  <dig>  image files and  <dig>  processed files. the latest gaii upgrade further increases this volume of data, mostly by acquiring larger images  and with the ability to perform paired-end sequencing . the computing infrastructure required for managing daily sequencing runs is extremely costly to set up and maintain. developing new algorithms to extract more information from available images and reduce the number of sequencing runs per project will therefore prove extremely valuable. finally, well-designed quality metrics and diagnostic tools will allow a rapid assessment of the quality of the sequencing runs and decide the applicable data retention policy.

the solexa/illumina genome analyzer performs sequencing-by-synthesis of a random array of clonal dna colonies attached to the surface of a flow cell. there are about  <dig> million such colonies on each of the  <dig> lanes of the cell. at each cycle of synthesis all four nucleotides, labelled with four different fluorescent dyes and blocked at the 3'-ends, are introduced in the flow cell. up to  <dig> such cycles of synthesis are performed.

the data acquisition on the genome analyzer "classic" proceeds as follows: each lane of the cell is divided into roughly  <dig> tiles that are individually photographed through four different filters. the image analysis software localizes each colony on each picture and quantifies the corresponding four fluorescence intensities. the output consists of one file per tile with one row per colony made of four coordinates and up to  <dig> real numbers for  <dig> intensity quadruples. the base calling starts downstream of this quantification and reconstructs the dna sequence that likely generated each colony. the solexa data analysis pipeline outputs two important files for each tile in each lane: a sequence file with the sequence determined from each intensity row and a fast-q file with a quality score for each base called. this fast-q score measures the most likely base intensity relative to the three other intensities on a logarithmic scale from - <dig> to  <dig> . here we propose an alternative probabilistic base calling method based on the fluorescence intensity quantifications that uses the extended iupac alphabet to code ambiguous bases. an information criterion is used to control the length of trustable reads. we show that this methodology increases the specific mapping of the tags onto reference genomes by about 15%  on raw sequences and an increase of up to 70% after quality filtering. the method is implemented in a freely distributed software called rolexa.

similar approaches have recently been published. closest to ours in their use of gaussian mixtures is the method introduced by cokus et al. in their analysis of arabidopsis methylation patterns <cit> . the alta-cyclic base caller  <cit>  uses a support vector machine that needs to be trained on a known dataset. our approach is computationally light and modular in that it offers a set of complementary functionalities that attempt to address the various biases observed in solexa sequence  <cit>  based on simple models of the biochemistry involved.

RESULTS
statistical properties of the fluorescent emissions
several sources of noise perturb the acquisition step: signal over noise ratio in the images depends on the position of the colony within the imaging field , colonies can be hard to segment on the pictures, fluorophore emission spectra partially overlap as emissions "leak" into adjacent channels. moreover synthesis efficiency is limited and therefore, within each colony, some dna strands incorporate a non-complementary base or are de-synchronized because they failed to incorporate a nucleotide at a previous step. both effects lead to the emission of a different fluorophore than the majority of the colony. these effects are possibly dependent on the base composition of the sequence <cit>  and are obviously deteriorating with each additional chemistry cycle.

we use the sequencing of the phix <dig>  to analyze the signal in the four color channels as the sequencing progresses. we first observe that the distribution of intensities in the individual channels shows a good separation between background noise and signal, although the shape of the histograms strongly depends on the dye used . for example, g has a tighter dynamical range than t and the range generally decreases with the cycle number. the largest range spans 4– <dig> logs. as the sequencing progresses, dynamic range decreases, signal over noise ratios worsen and the separation between background noise and signal becomes increasingly blurred . next, we observe that the a and c channels, as well as the t and g channels, are highly correlated .

reducing positional bias, dephasing and cross-talk
as observed above, there are three main sources of systematic bias at the level of intensity data. the first is the cross-talk between color channels: for example the a and c channels are not independent. thus we transformed the raw intensities by a linear mapping to the basis with axes at angles ϕ and θ with respect to the original axes . we optimize the two angles so as to minimize the overall correlation between the transformed coordinates. we repeat this operation at each cycle of sequencing as well as with the other two, g and t channels .

the second important bias is the colony dephasing: the amount of fluorescence emitted in a particular channel at cycle n depends on the number of corresponding bases present in the sequence at positions  <dig>  ..., n- <dig> because incorporation failures accumulated from previous cycles will be partly compensated at cycle n thereby increasing the signal in all channels. this cross-cycle dependence can be modelled by a binomial distribution with parameter q which is the probability of not elongating the complementary strand at each cycle of synthesis. we assume that this rate is equal for all nucleotides and all cycles. we determine the value of q by minimizing the average correlation between intensities at cycle n and n+ <dig> 

the last major source of systematic variation is due to an optical effect: on each tile, the colonies near the center of the image appear brighter than the ones near the edges . we correct this by fitting a two-dimensional lowess  <cit>  model to the intensities for each tile and subtracting the difference between the fit and the median intensity.

the three corrections are applied sequentially  to the raw intensities before applying the model-based clustering algorithm described next.

model-based clustering and information-theoretic base calling
we used a model-based clustering algorithm <cit>  to classify the intensity quadruples into four groups. clearly, four well-delineated clusters corresponding to the four bases emerge . specifically, we model the intensities measured in each channel by a mixture of four 4-dimensional gaussian random variables and we use the intensity quadruples from all colonies in one or few combined tiles to fit the model parameters. the fitted model provides four probability distributions on the space of intensity quadruples, namely the probability pa = p, ..., i4) that the kth base to call is an a knowing the measured intensities in all four channels at cycle k, and similarly for pc, pg and pt. we can measure the level of uncertainty in our base calling by the entropy h=−∑α∈{acgt}pαlog2pα which measures the uncertainty  in the determination of the correct kth base <cit> . knowing h and the four probabilities we then use cutoffs in the probability simplex to decide which iupac code to call . as the sequencing progresses, we also compute the cumulative entropy of each colony, h=∑k= <dig> …,nh, which estimates the log <dig> of the number of actual sequences compatible with the codes called up to position n. this total entropy is used to rank tags from least to most ambiguous. figure 3a shows that this ambiguity score correlates with, but is markedly different from the solexa fast-q quality score. the ambiguity metric is useful for genome assembly or polymorphism identification by allowing down-weighting the low quality tags when deriving statistics from multiple alignments of tags. as shown below, this metric can also be used to optimize tag lengths and increase the chance of identifying a match on the reference genome.

genome coverage statistics
to assess the quality of our base calling and to compare it with the sequences obtained via solexa's analysis pipeline, we compute the mapping efficiency #{reads mapping exactly to the genome}/#{total number of reads}. we used the fetchgwi tool  <cit>  to search for unique exact matches of each sequenced tag encoded in the iupac code on the  <dig> nt reference phix <dig> genome sequence . we thus discard every tag that matches at more than one position or does not match exactly anywhere on the reference sequence. one lane  of the solexa flow cell produced  <dig> m tags,  <dig> m unique tags and  <dig>  mappable tags, which amounts to a throughput of  <dig> million immediately usable bases per run. sorting tags by decreasing quality we see  that low-entropy tags are easily identified by both the solexa and rolexa pipelines, but that the coverage achieved by rolexa-called tags increases significantly among the low-quality sequences and results in an increased total coverage of up to 10–25% . we also see that ranking by quality  is a judicious prioritization strategy since the coverage increase is sharp in the top part of the list and subsequently plateaus off.

to estimate error rates of sequencing, we used align <dig>  <cit>  to search for an optimal match between each tag and the phix genome, and then computed the number of mismatches between tag and reference. figure 5a shows how the error rates increases as a function of the sequencing cycles for solexa tags. rolexa tags called with the most probable acgt base showed a slower increase, and introducing iupac codes significantly decreased both the intercept and slope of the error rate as a function of the sequencing cycle.

base distribution statistics
a surprising property of solexa sequences is the imbalance between complementary a and t base counts as well as between g and c <cit> . as shown in figure 5b, there is progressive deterioration in the proportions as the sequencing progresses, which is likely related to the varying noise levels across fluorescent dyes for complementary base pairs as well as dye-specific chemical effects . in consequence an intensity close to the background is more likely to be called t than a, or c than g. applying our corrections at the level of intensities stabilizes the proportions of bases, which is particularly pronounced for the t's. for reasons we do not currently understand the a/t ratio is not exactly one but stabilizes around  <dig>  .

to ascertain whether our increased coverage is not simply the consequence of the more degenerate alphabet, we verified that introducing ambiguities at random positions does not similarly improve the mapping. we thus selected the tags that did not match on the genome based on solexa base calling, but did match after rolexa introduced one to five ambiguous bases. then we introduced ambiguities in these tags, with the same frequency as rolexa, but at random positions. figure 5d shows that only about 2% of those randomized mutations found a match on the genome, indicating that the entropy is a specific predictor of ambiguous positions.

optimizing tag length
while solexa's quality score tends to decrease along the sequence, its distribution mostly spreads, rather than shifts, downwards . computing a global length cutoff based on the average quality will therefore discard a lot of high-quality bases and not necessarily ensure a uniform quality. thus we expect to increase the number of tags that can be mapped to a reference sequence by cutting them to a shorter length  <cit> . however this procedure has a downside since it will reduce the coverage length per tag and increase the probability of finding multiple matches. similarly, standard solexa procedures suggest selecting tags with high average fast-q. yet, a low average can be the result of just a few uncertain bases near the end of an otherwise useful tag.

we tested the different selections by applying the following quality filters. for the solexa method we cut the tags at length  <dig>   <dig>   <dig>   <dig>   <dig>  and then filtered all sequences with average fast-q score bellow  <dig>   <dig>  or  <dig>  in comparison, we used the following filtering procedure for rolexa tags: we chose  <dig> different length-dependent entropy cutoffs it  and searched within each read for the longest k-mer with total entropy less than it. we then extended this subsequence in both directions up to the next ambiguous base and eventually removed all tags shorter than  <dig> bases. the coverage statistics for the different filters are summarized in figure  <dig>  we performed a similar analysis of the  <dig> tiles of the sequencing of targeted human genomic regions and found an average of 50% increase in nucleotide coverage . we see that the efficiency of rolexa is superior in all datasets as measured by the ratio of actual coverage to expected coverage as well as by the ratio of tags having a unique match on the genome. the latter criterion is important since in many application of high-throughput sequencing , the extent of the coverage is less important than the number of hits on the genome. similarly, in genotyping and targeted re-sequencing, where inexact matches are expected, the ability to reliably filter out low-quality tags before doing the matching to the reference sequence is of the highest importance, since actual polymorphisms must be distinguished from sequencing errors.

discussion
several points in the analysis of solexa high throughput sequencing technology can likely benefit from further improvements. first the disequilibrium between complementary bases should be reduced. although the phix <dig> is a single-stranded dna virus, the library was prepared from the double-stranded covalently closed circular form of the genome. as shown, the output of the sequencing shows an increasing deterioration of the equilibrium between complementary bases as the sequencing cycles proceed . our approach improves on this but does not solve the issue completely.

similar approaches have recently been dohm et al. <cit>  have observed similar bias to the ones described here, but only proposed to correct them at the level of the sequence alignment, not at the level of the base calling. cokus et al. <cit>  use solexa's pre-treated data  and apply a very similar em procedure to fit a gaussian mixture model for probabilistic base calling. they do not use information based metrics to reduce the probabilities to iupac codes, but rather construct position-weight matrices with which they scan the reference genome, which is computationally expensive and not directly applicable for de-novo sequencing. erlich et al. <cit>  train a support vector machine optimized on a reference sequence which is computationally highly expensive. rolexa only needs a  multi-core computer and runs a complete analysis of one lane in  <dig> hours over  <dig> cores. moreover it is based on modeling the bio-chemical properties of the system.

we have not considered here the potentially important benefits of fine-tuning the image analysis algorithms. looking at images generated by the microscopic device shows that when the density of colonies is high in some region of the images, bleeding-over occurs and assigning the correct fluorescence intensity to each colony is clearly a delicate problem .

due to the large file size and format of the solexa output data, concurrently  accessing  <dig>  text files puts a heavy strain on any standard file system, not to mention backup devices. rolexa works with compressed inputs and outputs, which already reduces file size considerably. still, a better suited file format could help both the storage and the processing, e.g. using suffix tables and trees <cit> . the latest gaii upgrade to the solexa/illumina sequencer generates even more data, through larger acquisition area, longer reads, and paired-end sequencing. generating longer reads require efficient and reliable algorithms for base calling with reasonable levels of accuracy up to the end of the read. furthermore, this increased throughput requires these algorithms to be fast and be based on direct and simple methods that are re-usable without tuning from one run to the next.

CONCLUSIONS
solexa/illumina high-throughput sequencing has already and will increasingly produce vast amounts of systems scale genomics and functional genomics data. as with other high-throughput techniques, improvements in signal processing and statistical assessment of the data will prove to be a key step in the maturation of the technology and the progress towards reliable applications and new discoveries <cit> .

