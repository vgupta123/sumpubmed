BACKGROUND
rna secondary structure prediction is the process of predicting the position of hydrogen bonds in an rna molecule based only on its nucleotide sequence. these predictions can be used to better understand the functioning of cells, characteristics of gene expression and the mechanisms involved in protein production  <cit> . early attempts at systematic prediction include  <cit> ; who simply evaluated all possible structures with respect to free energy functions. later, thermodynamic principles were used to advance free energy methods in algorithms such as unafold  <cit>  and rnafold  <cit> . . whilst energy minimisation models have proved popular, scfg based methods also have their merits.

stochastic context free grammars
a context‚Äìfree grammar g  is a 4‚Äìtuple  consisting of a finite set n of non‚Äìterminal variables; a finite set v  of terminal variables that is disjoint from n; a finite set p of production rules; and a distinguished symbol s ‚àà n that is the start symbol. each production rule replaces one non‚Äìterminal variable with a string of non‚Äìterminals and terminals.

one possible grammar, generating strings which may be interpreted as addition/multiplication expressions using only the number  <dig>  may be represented thus: 

  s‚Üíf+s|ff‚Üí1||f‚àóf. 

note that each instance of s  generates a sum of n ‚â•  <dig> terms , and each f  generates a  <dig>  a product of terms , or a whole expression within parentheses. it should be clear that many different expressions might be generated: 1+1+ <dig> +1+, and so forth. formally, the grammar has non‚Äìterminal variables s, f, terminal variables ,+,‚àó, <dig>  production rules s‚Üíf+s,s‚Üíf,f‚Üí <dig> f‚Üí and start symbol s. the order in which the production rules are used forms the derivation of a string. one valid derivation would be s‚áíf‚áí‚áí‚áí, generating the string ‚Äò‚Äô and using the sequence of production rules s‚Üíf,f‚Üí,s‚Üíf,f‚Üí <dig> 

a scfg is a grammar with an associated probability distribution over the production rules which start from each t‚ààn. beginning with the start symbol and following production rules sampled from the relevant distribution, a string of terminal variables can be produced . choosing nucleotide symbols or the three characters used in dot‚Äìparenthesis notation as terminal variables, scfgs can be constructed which produce strings corresponding to nucleotide sequences or secondary structures.

application of scfgs to rna secondary structure prediction
the use of scfgs in rna secondary structure prediction was based on the success of hidden markov models  in protein and gene modelling  <cit> . any attempt to apply hmms to rna secondary structure was prevented by the long‚Äìrange interactions in rna  <cit> . scfgs, being generalisations of hmms, offer a solution. this was first exploited by  <cit>  and then developed by others .

the pfold algorithm  <cit>  is one of the most successful approaches using scfgs. it is designed to produce an evolutionary tree and secondary structure from an aligned set of rna sequences. pfold uses a scfg designed specifically for rna secondary structure prediction . therefore, when only considering a single sequence, pfold is simply a scfg prediction method. there are other approaches which predict secondary structures from aligned rna sequences, such as rnaalifold  <cit>  and turbofold  <cit> . however, we are concerned with the single‚Äìsequence prediction problem, so these are not used here.

while kh <dig> was effective, it seems to have been chosen relatively arbitrarily, in that there is little discussion about the motivation behind the choice of production rules. this problem was addressed by  <cit> , in which nine different lightweight scfgs were evaluated on a benchmark set of rna secondary structures. the set of grammars they tested, however, was by no means exhaustive. all of these grammars were constructed by hand and there was little motivation for their production rules . this suggested that a computational search of a large space of grammars might find stronger grammars, which is what we have attempted in this paper.

evolutionary approaches have already been implemented for hmms. indeed,  <cit>  used one to find the best hmm for protein secondary structure prediction. a mild improvement was found compared to hmms which had been constructed by hand. it is hard to tell quite how conclusive the results were since the limited size of the data set forced training and testing to be done on the same data. given the size of the data set, overtraining may have caused unreasonably high quality predictions. clearly though, the method is potentially very powerful.

methods
in this paper, only structure generating grammars are considered , . }). strings generated by these grammars uniquely define secondary structures, with a dot ‚Äò.‚Äô representing an unpaired nucleotide and an opening parenthesis indicating pairing with its corresponding closing parenthesis. once a structure is generated, a generative model for sequences is to stochastically allocate nucleotides to each site according to the frequency of occurrence  in some set of trusted sequences and structures.

normal forms
to develop algorithms for analysing sequences under grammatical models, it is convenient to restrict the grammar to a normal form, with only a few possible types of productions. the normal form most commonly used is chomsky normal form , as every context‚Äìfree grammar is equivalent to one in cnf. however, a grammar in cnf cannot introduce the corresponding parentheses of paired nucleotides in a single production, and therefore does not capture structure in a straightforward manner. thus it was necessary to create a new double emission normal form  which was able to capture the fundamental features of rna secondary structure: branching, unpaired bases, and paired bases. for any combination of non‚Äìterminals  we allow only rules of the following form: 

  t‚Üíuvt‚Üí.t‚Üí 

this normal form allows the development of the structural motifs commonly found in rna. for example  base‚Äìpair stacking can be generated by rules of the type v1‚Üí, hairpins by v1‚Üí,v2‚Üív2v3|v3v <dig> and v3‚Üí., and bulges by v1‚Üí, v2‚Üív3v <dig> and v3‚Üí.|v3v <dig> 

furthermore, with the exception of the ability to generate empty strings, this normal form allows the expression of dependencies of any context‚Äìfree grammar producing valid structures. it was also designed to avoid cyclical productions; that is, combinations of production rules which result in the same string that they started from. these are undesirable as they permit a countably infinite number of derivations for some strings. for this reason, rules of the form t‚Üíùúñ and t‚Üíu were not considered. there is, however, nothing intrinsically wrong with these rules; it is quite possible to create strong grammars with these rules present .

as a result of eliminating these rules, many grammars already established in rna secondary structure prediction cannot be exactly replicated, since they are not initially in the above normal form. for example, the kh <dig> contains the rule s‚Üíl. as this normal form is an extension of cnf, any context‚Äìfree grammar can be converted to this normal form, maintaining paired terminal symbol emissions. for example if s‚Üíl and l‚Üí.|, the forbidden rule s‚Üíl would be replaced by s‚Üí.|. this produces the same strings, and a given probability distribution for stochastic grammars can even be conserved. however, the transformation will often change the set of parameters in the model, which may result in different predictions when the production rule probabilities are inferred.

secondary structure prediction
secondary structure can be predicted by two methods, both of which are employed here. firstly, one can find the maximum likelihood derivation of a sequence, during which a structure is generated. the cocke‚Äìyounger‚Äìkasami  algorithm  <cit>  determines, by dynamic programming, the probability of the most likely derivation, and so backtracking can be used to find the most likely structure. it is designed for grammars in cnf, though there are established methods of dealing with grammars in a different normal form  <cit> .

secondly, one can employ a posterior decoding method using base‚Äìpairing probability matrices. the base‚Äìpair probability matrix for a scfg are obtained via the inside and outside algorithms  <cit> . the secondary structure with the maximum expected number of correct positions can then be calculated via dynamic programming. our decoding algorithm follows  <cit> , including a Œ≥ parameter specifying the trade‚Äìoff between correct base pairs relative to correct unpaired positions. for assessing the fitness of a grammar in the genetic algorithm, a value of Œ≥= <dig> was used, so as to maximise the expected number of positions correctly predicted.

both methods were used in the search, as this additionally gave a chance to compare the two prediction methods.

ambiguity and completeness
a grammar is said to be ambiguous if it produces more than one derivation for a given structure  <cit> . if structure a has one derivation with probability  <dig> , and structure b two derivations, each with probability  <dig> , the cyk algorithm will choose structure a, while structure b is more probable. this may reduce the quality of predictions using the cyk algorithm. the posterior decoding, though, sums over all derivations in prediction, so might be affected less by grammar ambiguity.

nine grammars are tested in  <cit> , of which two are ambiguous. they find that the cyk algorithm does choose suboptimal structures, and that the ambiguous grammars perform poorly relative to the unambiguous grammars. consequently, efforts have been made to avoid ambiguity  <cit> . the conclusion that ambiguous grammars are undesirable is not necessarily justified. the ambiguous grammars in  <cit>  are small, with at most two non‚Äìterminal variables, and one might expect them to be ineffective regardless. their poor predictive quality may be due to deficiency in design rather than ambiguity.

we define a grammar to be complete if it has a derivation for all possible structures which have no hairpins shorter than length two. clearly the ability to generate all structures is desirable for a grammar. however, one should be careful not to overemphasise this desirability, even for a complete grammar, once parameters have been inferred, converting it to a scfg, it is unlikely that all structures have a probability significantly different from zero for any sequence. similarly, the posterior decoding is not prone to grammar incompletness in the same way that cyk is since, in theory, after obtaining the probabilities of unpaired and paired positions, structures which cannot be derived with the grammar can still be predicted. one can perform a heuristic test for completeness by testing on a sample of strings. ambiguity can also be checked for heuristically  <cit> , but determining grammar ambiguity and completeness is undecidable  <cit> .

practically, it is very difficult to ensure both unambiguity and completeness. a complete, unambiguous grammar cannot be simply modified without compromising one of the properties. adding any production rules  will create ambiguity by providing additional derivations. equally, removing production rules will create incompleteness , as the original grammar is assumed unambiguous. because of this, an automated grammar design based on simple‚Äìstep modification is practically impossible without creating ambiguous and incomplete grammars. moreover, grammars that are unambiguous and complete are vastly outnumbered by grammars that are not. therefore, grammars not possessing these desirable qualities must be considered and as a result our grammar search serves as a test of the capabilities of ambiguous or incomplete grammars.

parameter inference
training data, consisting of strings of nucleotides and trusted secondary structures, is used to obtain the probabilities associated with each production rule, as well as paired and unpaired nucleotide probabilities. if derivations are known for the training sequences, then there are simple multinomial maximum likelihood estimators for the probabilities. usually, though, the derivation is unknown. again, one can estimate probabilities by finding derivations for the training set using cyk, or by the inside and outside algorithms.

for the cyk algorithm, in the case of ambiguous grammars, one cannot know which derivation produced the known structure, so probabilities cannot be obtained. consequently, we train these grammars using the same approach as  <cit>  to ensure comparability. that is, we randomly select one derivation. for unambiguous grammars, such as kh <dig>  this has no effect on the training. as with the prediction, inside‚Äìoutside training works for unambiguous and ambiguous grammars alike.

again, both cyk and inside‚Äìoutside were used for parameter inference in the search and evaluation.

evolutionary algorithm
with the double emission normal form, for m non‚Äìterminal variables there are 2m3+m2+m grammars  and m of type t‚Üí.). an evolutionary algorithm would allow for efficient exploration of the space of grammars in the above normal form. the way that the algorithm searches the space is determined by the design of the initial population, mutation, breeding and selection procedure. to find effective grammars, these must be well designed.

initial population
when forming the initial population, the size of the space of grammars quickly becomes problematic. the space is clearly large, even for small m, so the population size cannot approach that usually afforded in evolutionary algorithms  <cit> . we start with an initial population of small grammars, and use mutation and breeding rules which grow the number of non‚Äìterminal variables and production rules. our initial population comprised sixteen grammars, of the form: 

  s‚Üíss‚àísb‚àíbs‚àíbb‚àí.b‚Üí. 

where between zero and four of the s‚Üíuv rules were excluded. we also tried initial populations containing the scfgs from  <cit>  to consider examining scfgs close to these.

mutation
mutations constitute the majority of movement through the search space, so are particularly important. they give the grammar new characteristics, allow it more structural freedom, and add production rules which may be used immediately or may lie dormant. for non‚Äìterminals vi‚ààn, and corresponding production rules pvi, the allowed stochastic mutations were: 

¬∑ the start variable  change,

¬∑ a production rule is added or deleted,

¬∑ a new non‚Äìterminal variable v‚Ä≤ is added along with two new rules that ensure that v‚Ä≤ is reachable and that pv‚Ä≤ is not empty,

¬∑ a non‚Äìterminal variable is created with identical rules to a pre‚Äìexisting one,

¬∑ a production rule of the form vi‚Üívjvk is changed to vi‚Üívjvl, vi‚Üívlvk or vi‚Üívlvp, or production rule of the form vi‚Üí is changed to vi‚Üí.

this form of mutation is very basic, but allows many structural features to develop over generations. the rate of mutation determines movement speed through the search space and development of these structural features. adding rules too slowly prevents grammars from developing structure, while too many results in a lot of ambiguity and thus creates ineffective grammars. deleting rules almost always results in a worse grammar. to aid the grammar design, especially in consideration of facets of the normal form, the rule b‚Üí. was kept constant in the evolutionary process.

more complex mutation is clearly possible. the derivation could be used to find the rules used more often and make mutations of those rules more or less likely. a model for simultaneous mutations could be developed, which might be able to make use of expert understanding of rna structure, in combination with an evolutionary search. we have found the above model to give sufficient mobility in the search space, and therefore did not investigate other extensions.

breeding
the breeding model forms a grammar which can produce all derivations of its parent grammars. the grammar g formed from breeding g1and g2has start symbol s, non‚Äìterminals v  <dig>  v  <dig>  ‚Ä¶, v 
n
and w  <dig>  w  <dig>  ‚Ä¶, w 
m
, b, terminals ., and production rules 

¬∑ ps=ps1‚à™ps <dig> 

¬∑ for vi: pvi where all occurrences of s1are replaced with s,

¬∑ for wi: pwi where all occurrences of s2are replaced with s.

this breeding model was chosen to keep the size of the grammar relatively small, whilst allowing expression of both bred grammars to be present in derivations.

selection
we grow the population in each generation by introducing a number of newly mutated or bred grammars, then we pare it back to a fixed population size by stochastic elimination. we determine the probability of elimination of a grammar by the inverse of some fitness measure. fitness functions we use include mountain metric distances  <cit>  between the predicted structure and the trusted structure, sensitivity ) and positive predictive value  ). we follow the definition of accuracy in  <cit> , where a base pair constitutes a true positive if it is present in both the predicted and true structures, false positive if present in only the predicted structure, and false negative if present in only the trusted structure. we also tried these accuracy measures in combination with other factors in the fitness function, including largest correctly predicted length, inability to predict structures, and cost for grammar complexity.

brute force search
in addition to the evolutionary algorithm, we have run a brute force search to evaluate small grammars which might be effective. one of the main points of emphasis of  <cit>  was to look for lightweight grammars. in searching exhaustively for a small grammar, computational problems are quickly encountered. the search is feasible for grammars with only two non‚Äìterminals in addition to the rule b‚Üí.  but not for  <dig> non‚Äìterminals . whilst the search in three non‚Äìterminals may be worthwhile, especially since kh <dig> contains only  <dig> non‚Äìterminals, it was far from computationally feasible for the purposes of this paper.

data
we took data from rnastrand  <cit> , a collection of other databases  <cit> . we filtered the data set so that the sequences and structures could ensure reliability of predictions. we removed identical sequences and disregarded synthetic data and sequences with ambiguous base pairs. we further cleaned the data to filter out any sequences with greater than 80% base pair similarity with another structure . furthermore, we removed all sequences with pseudoknots as it is well established that scfgs cannot predict pseudoknots  <cit> .

the spectrum of sequence length, is of particular significance in selecting data. the cyk and training algorithms are of cubic order in the length of the string, so we decided to use large training and test sets with small strings. longer strings require longer derivations, thus they have a larger weight in the parameter training, which might lead to overtraining. equally, if one omits longer strings, poorer predictions may result from overtraining on the shorter strings. we found the trained parameters highly sensitive to the choice of training data set, and struggled to balance this with computational efficiency.

the sensitivities, ppvs, and f‚Äìscores of grammars gg1‚Äìgg <dig> and kh99‚Ä≤ on the evaluation set and on the on the  <cit>  data, as well as the benchmarked sensitivities and ppvs of unafoldv <dig>  and rnafoldv <dig>  on the  <cit>  data. results for kh <dig> and pfold are taken from  <cit> .

we used a final data set from a variety of families, consisting of  <dig> sequences with corresponding structures. there was a total of  <dig>  nucleotides with  <dig>  base pairs. as with  <cit>  and  <cit> , sensitivity and ppv were used as the measures of structure fit, as well as f‚Äìscore ), a more balanced measure of sensitivity and ppv. we investigated the practice of dividing data into training and test sets.  <cit>  randomly generate training sets of  <dig> and  <dig> strings for parameter training.  <cit>  use a training set of  <dig> structures and test set of  <dig> total structures. meanwhile,  <cit>  used a considerably larger training set, with  <dig> structures, and a much smaller test set of just four sequences. our training and test sets used in the evolutionary algorithm were disjoint sets of size  <dig>  chosen at random from those sequences in our data of length less than  <dig> nucleotides. the remaining data form our evaluation set, on which grammars‚Äô sensitivities and ppvs are reported.

as well as measuring performance on our own data, we have used results obtained with the  <cit>  data. this helped us to verify the performances of the new grammars, to check our results against earlier work, and to verify that kh99‚Ä≤ is a good representation of kh <dig> 

RESULTS
across all our experiments, over  <dig>  grammars were searched. a number of strong grammars were found using both cyk and io training and testing, denoted gg1‚Äìgg <dig>  kh99‚Ä≤ is kh <dig> in the double emission normal form. results on the sensitivity, ppv, and f‚Äìscore of each grammar can be found in table  <dig>  in addition to the benchmark with the  <cit>  data, and results on different training and testing methods can be found in table  <dig>  table  <dig> also gives the scores of the combined best prediction, calculated by selecting, for each structure, the prediction with the highest f‚Äìscore, and then recording the sensitivity, ppv, and f‚Äìscore for that prediction. 

  a‚Üíba|.|kh99‚Ä≤b‚Üí.|c‚Üíba|a‚Üíaa|ba|.||gg1b‚Üí.|c‚Üíba|a‚Üíaa|ab|ba|bb|cb|bc|.||gg2b‚Üí.c‚Üíaa|ab|ba|bb|bc|ca|cb|.|||a‚Üíab|ba|bb|aa|dd||||gg3b‚Üí.c‚Üíaa|.|d‚Üícd|bd||a‚Üícc|cb|bc|ec||b‚Üí.c‚Üícb|bb|gg4d‚Üígc|e‚Üíab|cd|.f‚Üíabg‚Üífb 

  a‚Üída|cc|.|b‚Üí.c‚Üíaa|hf|gg5d‚Üí.|e‚Üíf‚Üífb|bf|aa|.||g‚Üíh‚Üíbga‚Üíde|ab|ba|ah|.||b‚Üí.c‚Üígg6d‚Üíbb|ace‚Üí.|f‚Üífb|cf|.g‚Üígh||h‚Üífa|af|hh|| 

the sensitivities, ppvs, and f‚Äìscores of grammars gg1‚Äìgg <dig> and kh99‚Ä≤ on the evaluation set, using different methods of training and testing. ‚Äòcyk‚Äô indicates that the cyk algorithm was used, and ‚Äòio‚Äô that the inside and outside algorithms were used. the column ‚Äòbest‚Äô was calculated by selecting, for each structure, the prediction with the highest f‚Äìscore, and then recording the sensitivity, ppv, and f‚Äìscore for that prediction. it is perhaps not surprising that the ‚Äòbest‚Äô predictions for cyk are better than the ‚Äòbest‚Äô predictions for io, as io is in some sense averaging over all predictions. one might expect the predictions to be more similar than those from cyk, as seen by comparing io values for gg <dig> and ‚Äòbest‚Äô, giving less increase when considering those with best f‚Äìscore.

this shows grammars with very different structures perform well on the same  data set. kh99‚Ä≤ is still a strong performer, but we have shown that there exist many others which perform similarly .

gg <dig> is kh99‚Ä≤ with two rules added, a‚Üíaa and a‚Üí. these rules were used infrequently . the mild improvement in functionality allows for an additional fraction of base pairs to be correctly predicted.

gg <dig> and gg <dig> were found using the posterior decoding version of the evolutionary algorithm. they have a high density of rules, that is many rules for each non‚Äìterminal variable. particularly, gg <dig> has almost all of the rules it is possible for it to have, given b‚Üí. is kept constant through the evolutionary algorithm. given this, it is not surprising that they performed poorly using the cyk training and testing. however, with posterior decoding, the base‚Äìpair probabilities are still effective for good predictions.

gg <dig> has only two variables  used almost exclusively in producing base pairs. it then uses various exit sequences to generate different sets of unpaired nucleotides and returns to producing base pairs. finally, gg <dig> and gg <dig> are typical of larger grammars we have found with complex structure. it is not obvious to us how their structure relates to their success in secondary structure prediction. gg <dig>  gg <dig>  and gg <dig> were all found using the cyk version of the evolutionary algorithm, and perhaps their complex structure can be accredited to this. gg <dig> is a strong performer throughout, particularly when considering f‚Äìscore.

most grammars achieved lower predictive power on the dowell and eddy dataset. the difference in performance between kh <dig> and kh99‚Ä≤ is small and confirms that the representation of kh <dig> as kh99‚Ä≤ is a good one. particularly noteworthy is the performance of gg <dig> and gg <dig>  gg <dig> has had a considerable increase in ppv, likely due to the posterior decoding prediction method. given many of the structures in the dowell and eddy dataset contain pseudoknots, other grammars score poorly trying to predict pairs where there are not, in contrast to gg <dig>  by predicting fewer base pairs, gg <dig> gains higher ppv as more of them are correct, but lower sensitivity. gg <dig> is a grammar which was unremarkable in its results on the original data set, however, it has outperformed the rest of the grammars on the benchmark set and is the only grammar with improved sensitivity when compared to the rnastrand dataset.

overall, the grammars found in the evolutionary search still perform well because they are not overadapted to deal with the original data. determining which is best depends on the measure of strength of prediction, whether the size of the grammar is a concern, ability to approximate structures with pseudoknots effectively, and so on. however, it is clear that a selection of effective grammars has been found. results shown by unafold and rnafold continue to be superior to those produced by scfg methods.

we also checked that the grammars obtained from the evolutionary algorithm do not merely produce similar structures to kh99‚Ä≤ by using different derivations. to do this we define the relative sensitivity of method a with respect to method b as the sensitivity of method a as a predictor of the structures produced by method b. the relative ppv is defined in a similar manner. we then compared the predictions of the grammars by building a heat map of the relative sensitivities and ppvs , using the evaluation set. as expected, kh99‚Ä≤ and gg <dig> predict almost identical structures, as they are highly similar. similarly, it is perhaps not surprising that gg <dig> and gg <dig> have very similar predictions given they produce structure through posterior decoding. the rest of the methods have sensitivity and ppv relative to other prediction methods of approximately  <dig>  ‚Äì  <dig> . as they are designed to predict rna secondary structure using the same training set, one would expect some similarity in the predictions, although not as much as with kh99‚Ä≤ and gg <dig>  this is confirmed by our results, suggesting that the new grammars produce different kinds of structures which are good representations of rna secondary structure.

further analysis of 
to test the local features of the space, we evaluated variations of kh99‚Ä≤ against the full data set. where a single rule was deleted, only one grammar had prediction accuracy of the order of kh99‚Ä≤. this is the grammar without the rule a‚Üí . however, it is clear that deleting rules has a strong negative effect on the predictive power of kh99‚Ä≤, given that no others have sensitivity greater than  <dig> . of course, this might be expected given that this scfg has been constructed manually, and it is therefore unlikely to have unnecessary production rules.

with addition of rules, the number of grammars to check quickly becomes large. with one production rule added,  <dig> grammars must be evaluated, with two added this increases to  <dig>  a similar local search for larger grammars would be impractical, since there are many more grammars with one or two altered production rules . ambiguity of tested grammars had little or no effect. results of this local search can be seen in figure  <dig> 

brute force search
the brute force search illustrated how, with this normal form, larger grammars are needed to provide effective prediction. most small grammars will only be able to produce one type of string. also, it suggested that the existing grammars are close to locally optimal and that the space around them is quite flat, demonstrating the need for intelligent searching methods. figure  <dig> illustrates the distribution of sensitivity across the space of grammars with at most  <dig> non‚Äìterminal variables. no grammar has sensitivity higher than  <dig>  and approximately one quarter of grammars have sensitivity  <dig> .

ambiguity and completeness
one of the results of the search which we find most interesting is the ambiguity and completeness of gg1‚Äìgg <dig>  shown in table  <dig>  all grammars found in the search were ambiguous, and still predicted structure effectively. in particular, ambiguous grammar gg <dig> performed better than kh99‚Ä≤, being a slight modification of it. particularly, it is clear that gg <dig> and gg <dig> have many different derivations for each structure, and their strong performance relies on this ambiguity, as they perform poorly when tested with cyk. gg <dig> demonstrates further that ambiguous grammars can even be effective at approximating structures with pseudoknots. the effectiveness of some ambiguous grammars is likely due to the prediction algorithm picking structures that, whilst perhaps suboptimal, are close to what the best prediction would be. clearly there is room for a further investigation into quite why some grammars cope better with ambiguity than others.

ambiguity and completeness of kh99‚Ä≤ and gg1‚Äìgg <dig> grammars. all grammars found in the search were ambiguous. some of the grammars found , are incomplete but heuristically it seems that the structures that cannot be generated have little biological relevance.

similarly, it might be surprising that some of the grammars found , are incomplete. however, heuristically it seems that the structures that cannot be generated have little biological relevance ‚Äù). in some sense therefore, the incompleteness is permissible, as the grammar is still able to generate any relevant structure.

CONCLUSIONS
our brute force search and search around kh <dig> demonstrate that intelligent searching methods are necessary, and overall, the method of evolving scfgs for rna secondary structure prediction proved effective. we found many grammars with strong predictive accuracy, as good or better than those designed manually. furthermore, several of the best grammars found were both ambiguous and incomplete, demonstrating that in grammar design such grammars should not be disregarded. one of the strengths of the method is the ease of application and effectiveness for rna structure problems. in particular, grammatical models are used in phylogenetic models of rna evolution  <cit>  which make use of manually constructed grammars, and so the accuracy might be improved with automated grammar design. overall though, whilst many grammars have been found with good predictive power, the space of grammars grows rapidly with the number of non‚Äìterminals, so we cannot conclude that no better grammars exist. the effectiveness of the search heuristic is supported by the fact that we consistently find grammars on par with the best manually created grammars. the equally consistent inability to achieve any significant improvement on this level of performance, and the relative limited prediction correlation between the many good grammars found points to the inherent challenge of grammar design, or indeed to the limitations of scfg based methods as a whole. it appears that the number of grammars able to achieve this level of performance is large, and may depend little on the overall grammar structure, and at the same time it appears that a performance improvement may be difficult or impossible to achieve with a grammatical approach.

competing interests
the authors declare that they have no competing interests.

author‚Äôs contributions
jwja conceived the idea in discussion with rl and jh. jwja then developed the methodology with pt and js, with help from rl. pt and js then designed and wrote the code, and results were analysed and written up by jwja, with help from pt and js. all authors were involved in critical redrafting of the manuscript. all authors read and approved the final manuscript.

