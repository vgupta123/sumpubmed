BACKGROUND
over the last three decades, ancient dna  research has demonstrated that genetic material can survive for up to several hundreds of thousand years in fossil materials
 <cit> , such as bones, teeth, soft tissues, hair, coprolites and soil sediments
 <cit> . one successful application of adna has been to chart the diversity of paleo-communities through time, revealing how major environmental crises have transformed the composition of past ecosystems
 <cit>  and have impacted the past demographic dynamics of several megafauna species until their extinction
 <cit> . ancient dna has also been used to reveal the evolutionary origin of human populations
 <cit>  and to investigate our relationships with archaic hominins
 <cit> . the sequencing of nucleic acids from ancient pathogens, including the  <dig> influenza virus responsible for 20– <dig> millions of human deaths worldwide
 <cit> , has provided insights into the molecular mechanisms involved in virulence with potential applications in medicine and epidemiological surveys.

the recent advent of next-generation sequencing  technologies heralded a revolution in adna research, by providing access to the complete genomic sequences of ancient individuals and extinct species
 <cit> . most of the genetic information available at the genome-wide level has been generated using second-generation platforms, first on  <dig> roche genome sequencers
 <cit> , then on illumina platforms
 <cit> . common to all second-generation platforms is the need for building and amplifying dna libraries before sequencing, leading to a skewed representation of the underlying sample. recently, the first successful application of library-free third-generation sequencing of adna templates has been reported from a pleistocene horse bone using the helicos true single molecule dna sequencing platform 
 <cit> . of note, the different sequencing platforms available exhibit specific error profiles, with tsms showing higher error rates than illumina sequencing overall and a prevalence of insertions/deletions over nucleotide misincorporations
 <cit> . in addition, specific sources of bias have been observed with tsms, mostly as a result of the poly-a tailing and blocking reactions that are performed before sequencing and that skew the base composition of sequencing reads
 <cit> . adding to the dna damage induced nucleotide misincorporations that are typical of ancient templates, these specificities could limit our ability to map sequencing reads against modern reference genomes and therefore limit our ability to identify genuine endogenous tsms reads, reducing the efficiency of shotgun sequencing.

in this study, we explore different strategies for improving the mapping of sequencing reads against modern reference genomes, in order to improve the identification of endogenous adna fragments. we take advantage of two sets of sequence data generated from two pleistocene horse bone dna extracts on illumina gaiix and helicos genome sequencer platforms
 <cit> . semi-global aligners, such as the burrows wheeler aligner 
 <cit>  have been developed for mapping undamaged sequencing reads, generated from modern extracts, using platforms with low rates of indels and sequencing errors. to see if performance could be improved for adna, we have modified the default alignment parameters to improve mapping sensitivity at acceptable run-times. taking advantage of the fact that nucleotide misincorporations resulting from post-mortem damage preferentially cluster at sequencing termini
 <cit> , we further test if systematic trimming of likely damaged positions at read ends could improve overall read mapping quality and thereby lead to the recovery of additional genuine adna fragments. we examine different methods designed to identify human contamination and their respective consequences in estimating endogenous sequence contents. we show that combining the different approaches can increase the number of high-quality endogenous hits recovered by up to  <dig> % when mapping tsms reads against modern reference genomes. the fraction of endogenous illumina reads identified is less sensitive to the mapping strategy. overall, we present a series of approaches that could substantially improve the ability to identify the true fraction of endogenous reads when shotgun sequencing adna with illumina and helicos next-generation sequencing platforms.

RESULTS
overall strategy
to optimize the performance of alignment using bwa, the use of a seed region , the prohibition of indels close to read ends , the maximum number of gap opens  and gap penalties , and the overall number of differences tole-rated in the alignment  were examined. illumina and helicos sequencing reads were mapped against both the horse  and the human reference genomes and high-quality endogenous hits were defined as hits found to map to a unique location in the horse genome with mapping qualities equal or higher than  <dig> while showing no high-quality hit to the human reference genome. the latter was introduced as a conservative approach to limit the fraction of spurious hits that could result from chance alignments of reads of exogenous origin to the horse genome. additionally, the level of spurious hits recovered with the different sets of mapping parameters was monitored by the fraction of high-quality hits recovered when performing alignments against the non-mammalian chicken genome .

as endogenous adna reads have been shown to exhibit specific miscoding lesions that could be used as molecular signatures of post-mortem dna damage, nucleotide misincorporation patterns
 <cit>  were used to assess the quality of the extra hits recovered with the different mapping strategies explored. these patterns mainly result from post-mortem deamination of cytosine residues into uracil residues, and for illumina reads consist of an increase of c → t misincorporations at the 5’-ends of sequencing reads paralleled by an increase in g → a misincorporations at 3’-ends
 <cit> ; for helicos tsms reads, the deamination of cytosine residues results in an increase in g → a mismatches at 5’-ends of sequencing reads
 <cit> . finally, we validated the optimal set of mapping parameters for the helicos tsms platform using a simulation approach, showing that the suggested parameters preferentially lead to alignments at the correct location, while not significantly inflating the rate of spurious alignments.

seed region
by default, bwa uses the first  <dig> nucleotides of query sequences as a seed region, allowing at most two mismatches in this region in order to reduce the overall runtime of the alignment . unless specific enzymatic treatments are used , adna reads exhibit high rates of mismatches at 5’-termini due to the prevalence of post-mortem cytosine deamination at overhanging ends. the prevalence of such damage-related mismatches in the seed region could limit the ability of seed-based mapping strategies to identify genuine but highly damaged hits for both illumina and helicos platforms. in addition, helicos tsms reads starting with a minimum of two thymine residues are trimmed post-sequencing, as these could have resulted from the sequencing of the poly-a tail added early in the template preparation procedure. this has been shown to increase mismatch rates from thymine residues to any other nucleotide base at the 5' read termini
 <cit> , which again would increase the number of mismatches present at 5’ termini and could further limit the performance of seed-based mapping strategies. therefore, we examined whether significant yields of endogenous hits could be recovered by disabling the seed region.

disabling the seed region yielded a  <dig> % increase in the total number of high-quality hits for illumina sequences, while doubling the runtime . a higher relative gain of  <dig> % was observed for helicos sequences, at similar runtime costs . interestingly, the new population of hits identified for both platforms shows an excess of cytosine deamination related mismatches at 5’-ends , suggesting that these most likely consist of dna templates with 5'-overhangs, that were lost due to the use of a seed region. importantly, nucleotide misincorporations related to cytosine deamination were found to dominate mismatch patterns over the full length of sequencing reads, confirming that the new hits identified consisted of reads of ancient origin, as cytosine deamination has been shown to be the most prevalent form of post-mortem damage
 <cit> . this is further attested by the read size distributions of the new hits identified, which are enriched in reads of at least  <dig> nucleotides  or  <dig> nucleotides , which are very unlikely to be the result of chance alignments. this is confirmed by a very low number of matches when we map against the chicken reference genome . 

indels at read ends
by default, bwa prohibits indels within the first and last five positions in alignments. this behavior appears suitable for platforms that produce low levels of spurious insertions and deletions during sequencing, such as the illumina platforms. however, for tsms and more generally third-generation sequencing, the most common type of sequencing errors are indels, particularly deletions
 <cit> . reads with indels near termini would consequently have an increased number of mismatches when aligned to a reference sequence, which will most likely result in a significant loss of positive hits. as expected, disabling the seed and allowing for indels near read termini improved the amount of high-quality hits recovered on the helicos tsms platform  while only slightly increasing the runtime cost . a similar trend was found using our simulation framework, where a higher fraction of simulated horse tsms reads could be mapped at the right location of the genome, albeit with lower gains in extra-hits . the extra hits identified from tsms sequences were found to exhibit the nucleotide misincorporation pattern characteristic of ancient templates, with an excess of gaps opened at alignment termini . in contrast to what is observed for helicos data, no major improvement in the fraction of hits identified was detected for illumina reads . on the contrary, disabling the seed and allowing for indels at read termini resulted in spurious nucleotide indel patterns, especially at 3’-ends, most likely resulting from local misalignments due to the addition of indels instead of expected substitutions.

adjusting gap openings and penalties
by default, bwa uses a high penalty for opening gaps  and a significantly lower penalty for extending gaps , though still higher than the penalty used for a mismatch . in addition, only one indel can be opened per alignment during the alignment phase. this appears to be an optimization towards sequencing platforms that produce few indels, such as the illumina platform. however, indels, and particularly deletions, represent the most frequent type of sequencing error on helicos platforms, with overall rates per base estimated at around 5%
 <cit> . for the size range relevant in this case study , helicos tsms reads would therefore be expected to show a minimum of 1– <dig> indels on average, suggesting that standard parameters might lead to suboptimal performance. we therefore tested if extra high quality hits could be recovered for helicos tsms sequences, by increasing the maximum number of indels allowed to two. this was tested together with disabling seed and allowing indels at read termini, in agreement with the findings presented above.

allowing more gap openings was found to have a dramatic influence on mapping results at relatively low runtime costs, with extra-gains of high-quality reads spanning  <dig> % to  <dig> %  and a relative increase  in the class of reads longer than or equal to  <dig> nucleotides . with standard edit distances , allowing two or three gaps in the alignment resulted in recovering hits with lower deamination levels, with g → a misincorporation levels of  <dig> – <dig> % at sequencing starts compared to  <dig> % with one gap . not surprisingly, indel rates increased, even to levels higher than the manufacturer’s specifications, suggesting that the latter could be slightly under-estimated or that misalignments were more often solved by introducing gaps than by mismatches, despite higher alignment penalty scores. interestingly, a substantially higher fraction of simulated reads could be successfully mapped against the right location of the horse genome when allowing for a maximum number of two gap opens , suggesting the overall adequacy of this approach for mapping helicos reads.

we further evaluated how much different gap open and extension penalties could influence mapping outcomes for helicos tsms reads, while still allowing a maximum of two gap opens. the performance and runtime of bwa were therefore examined for combinations of penalties ranging from four to eleven . decreasing gap open penalties was detrimental while gap extension penalties were found to have almost no influence on the performance of bwa mapping. in addition, all combinations of parameters explored were found to result in similar runtime cost. consequently, default penalty values were found to perform better than most of the other combinations tested and were kept for further experiments.

sequencing reads recovered from the sample showing infinite radiocarbon date were aligned using different combinations of penalties for opening  and extending  gaps using the bwa aligner with seed disabled  and allowing a maximum number of two indels  possibly located at read ends . shifts in the percentage of high-quality hits recovered and total runtime are reported relative to the values observed using bwa default parameters, with the result for default gap penalties shown in green. reads were considered of high-quality when mapping uniquely to the equcab <dig> genome but not against the human genome  and showing mapping qualities of at least  <dig> 

increasing tolerance for higher edit distances
for standard bwa settings, the maximum edit distance allowed in a given alignment is controlled dynamically according to read length. in the size range relevant in our case study , three mismatches are accepted at maximum for reads shorter than  <dig> nucleotides, four for reads shorter than  <dig> nucleotides, and five otherwise. these thresholds may be manipulated using the -n option, with a default value of  <dig> . from default mapping parameters, the cumulative frequency of gc → at misincorporations located in the five nucleotides at read termini could be estimated at approximately  <dig> % and  <dig> % using helicos and illumina reads respectively. this rate is even higher if determined over all positions . this suggests that post-mortem deaminations at cytosine residues contribute massively to the edit distance of alignments, limiting our ability to identify the fraction of endogenous reads that are most divergent. increasing our tolerance for higher edit distances might help with recovering this fraction, at the risk of accepting a larger proportion of reads of exogenous origin.

in order to evaluate the consequence of relaxing the threshold, bwa mapping was performed with the set of parameters validated above, and relaxing the –n option to  <dig>  and  <dig>  . in the size range relevant to our case study, the former corresponds to a maximum of three mismatches for reads of  <dig> nucleotides or shorter, four mismatches for reads of  <dig> nucleotides or shorter, and five mismatches otherwise; for the latter, three mismatches are accepted for reads of  <dig> nucleotides or shorter, four for reads of  <dig> nucleotides or shorter, and five mismatches otherwise. interestingly, these parameters show only moderate extra gains in high-quality hits for illumina reads . this population of new hits exhibited nucleotide mismatch patterns characteristic of adna reads , suggesting most of those reads were genuine horse sequences. their identification required, however, extensive runtime costs; consequently, the default parameter value was used for further mapping tests performed with illumina sequencing reads.

for helicos tsms reads, the fraction of reads identified as high-quality hits was found to increase when allowing a higher edit distance, with extra-gains ranging from  <dig> % to  <dig> %, at a significant increase in runtime cost . in agreement with what observed on illumina reads, the new population of reads identified exhibits a nucleotide misincorporation pattern indicative of cytosine deamination at 5’-overhanging ends, and a predominance of g → a miscoding lesions over the whole sequence length , suggesting that those could be of endogenous origin.

in order to further evaluate the overall quality of these reads, helicos reads were mapped against the chicken reference genome . the most permissive edit distance  was found to yield more hits on the chicken genome, especially for read sizes of  <dig> nucleotides which corresponds to the smallest length tolerating three mismatches. this behavior was not observed for the intermediate edit distance , as >95% of helicos reads that could be successfully aligned to the chicken genome consisted of reads shorter than  <dig> nucleotides . in contrast, the fraction of new horse hits identified with this set of parameters consisted of reads of size longer than  <dig> nucleotides , suggesting that the option –n  <dig>  could be used to improve our ability to identify endogenous reads without inflating false-positive rates .

final recommendations for mapping adna reads
for illumina reads, the amount of high-quality hits was found to be rather insensitive to the set of mapping parameters explored, with marginal gains ranging from  <dig> % to  <dig> % . importantly, disabling the seed resulted in an extra  <dig> % of high-quality hits and identified a population of reads showing nucleotide misincorporation patterns characteristic of post-mortem damage and no reduction in read size, as would have been observed for spurious alignments. given that this only doubled runtime cost, we therefore recommend that adna reads generated with the illumina sequencing technology should be systematically mapped following this procedure. other sets of parameters explored have been found to improve our ability to identify endogenous sequence reads, however at extensive runtime costs, and therefore should be not be recommended on a systematic basis.

bwa parameters were found critical for mapping helicos tsms reads, with large increases in both the fraction of reads identified as high-quality and runtime costs . of all settings tested, the best compromise was obtained by disabling the seed and tolerating up to two gap opens, even if located at read termini. furthermore, the alignment edit distance could be relaxed slightly  with no apparent increase in spurious hits. however, tolerance for larger edit distances was found detrimental for the quality of the results and should be avoided. the same was observed for helicos reads generated from a younger sample associated with a finite radio-carbon date, admittedly with more limited gains in high quality hits . however, the suggested procedure was found to be computationally intensive, resulting in  <dig> – <dig>  fold increase in run time . the following experiments were performed according to the specific set of mapping parameters recommended for each sequencing platform.

trimming likely-damaged positions
since nucleotide misincorporations are clustered at read termini, trimming mismatches resulting from post-mortem dna damage at 5’-ends might further improve the chance of identifying endogenous dna sequences. on the other hand, short reads are more likely to cause spurious alignments. in order to determine the read size that would minimize the occurrence of spurious alignments after trimming, helicos reads, which are on average shorter than illumina reads, were aligned against a genome distant to the horse, specifically the chicken. even in absence of trimming and even though dna was extracted from a horse fossil, reads shorter than  <dig> nucleotides were found to yield substantial amounts of high quality hits against the chicken genome .

taken at face value, the relatively high frequency of very short hits mapping against the chicken genome could suggest that a substantial fraction of the shortest horse hits are spurious. these hits could alternatively correspond to genomic regions that are ultra-conserved across vertebrates. importantly, the large majority of high-quality alignments against the chicken genome  did not show any match at all to the horse genome. a closer look at the fraction of reads mapping positively against both the chicken and horse genome revealed an increase in misincorporations related to cytosine post-mortem deamination , suggesting that at least a fraction of those consisted of genuine ancient horse reads mapping to genomic regions showing low sequence divergence between the chicken and the horse . of note, for this fraction of hits, sequencing starts exhited  <dig> % g → a misincorporations for helicos and  <dig> % for illumina. this is lower than the rates observed when considering all horse high-quality hits . this reduction is not unexpected as for short sizes, undamaged horse reads have higher chances of being identified than damaged horse reads and, hence, exhibit lower mismatch rates at 5’ ends. this phenomenon has been previously observed by green and colleagues in the context of detecting derived alleles in short neandertal dna fragments
 <cit> . interestingly, we found an overall positive correlation between the levels of g → a mismatches and read length, both for helicos reads  and illumina reads , emphasizing that mapping short reads is more likely to succeed for undamaged reads than for damaged reads. 

that a substantial number of high-quality hits were identified against the chicken genome for reads shorter than  <dig> nucleotides suggests that putative spurious hits would mostly consist of reads of similar or shorter sizes. consequently, reads that did not previously align against the horse genome were filtered for sizes greater than or equal to  <dig> before being trimmed of one or two bases from the 5'-end and aligned against the horse and human genomes. resulting hits were further binned based on whether or not the trimmed base would have caused c → t  or g → a  mismatches against the reference, and nucleotide misincorporation patterns were plotted . as a control, the procedure was carried out while selecting for non-damage-derived mismatches .

for illumina reads, trimming of a single c → t mismatch at 5’-ends yielded a  <dig> % gain in high-quality-hits mapping against the horse reference ; additionally, the pattern of nucleotide misincorporation for the new hits is compatible with an ancient origin, showing the characteristic elevation of c → t mismatches at 5’-termini, up to levels that even exceeds the rate observed for the overall population of illumina hits , paralleled by an increase in g → a mismatches at 3’-ends . likewise, the trimming of two consecutive c → t mismatches resulted in similar findings, with an additional  <dig> % extra reads of horse origin .

in contrast, trimming of any other class of mismatch resulted in negligible gains ranging from  <dig> – <dig> %. those reads could represent a fraction of the ancient horse genome that exhibits substantial polymorphisms to the ancient horse genome. however, the mismatch patterns observed after trimming most other types of mismatches was not found to show the expected signature of post-mortem cytosine deamination , suggesting that the background increase in high-quality horse reads identified after trimming mismatches other than c → t mostly consists of modern dna and/or spurious alignments.

a similar procedure was performed on helicos tsms reads, with more ambiguous results. similar to illumina reads, gains  in high-quality horse reads were obtained after trimming one or two g → a mismatches at the very 5’ end, suggesting that the presence of post-mortem cytosine deamination prevented successful mapping before trimming . of note, the new population of hits identified exhibits substantial levels of cytosine deamination, as suggested by high overall levels of g → a mismatches at the 5’-ends . however, the trimming of other classes of mismatches also yielded substantial gains in high-quality hits  that exhibit g → a mismatch patterns similar to those observed without trimming . this was particularly true when trimming a → c, a → g, a → t, g → t and c → t mismatches at the first position. this may be partly explained by the early incorporation of virtual terminator nucleotides, during the fill-in / blocking step prior to sequencing
 <cit> . this would lead to the resulting sequence being prefixed with a thymine, as a base from the poly-adenine tail would be sequenced as part of the template strand
 <cit> .

while the above demonstrates that some additional adna fragments may be recovered by selectively trimming potentially damage-derived nucleotides, the gains were rather low , in contrast to previous estimates based on simulated helicos reads, which suggested a 5% loss of sequences due to post-mortem damage at the first position
 <cit> . we believe that additional hits obtained through the manipulation of bwa parameters were drawn from the pool of sequences that could not be aligned due to the presence of post-mortem damage, reducing the effect of this operation. given the relatively small gains and the additional complexity required for setting-up the analytical pipeline, we would generally refrain from recommending such a trimming approach.

filtering human contamination
as presented above, hits against the e. caballus genome were aligned against the human genome, and high-quality hits were filtered as contamination. this accounts for  <dig> % and  <dig> % of the horse hits, for helicos and illumina data respectively. as human contamination appears to represent only a small subset of the total data
 <cit> , a high rate of false positives is expected, especially for ultra-conserved genomic regions. this is further attested by the mismatch patterns of hits mapping to both genomes, which show typical signatures of post-mortem cytosine deamination both for illumina and helicos reads, suggesting that reads filtered according to this procedure largely consist of ancient horse dna . consequently, additional authentic adna horse fragments might be recovered by improving the specificity of the filtering of putative human contamination. 

we examined the benefit of a commonly used strategy that has already been applied for filtering environmental contamination from neandertal ngs reads
 <cit>  but also in many other contexts 
 <cit> . this method is based on the edit-distance of potential contamination to different target genomes . following a best hit criterion, reads of a likely human origin were identified as those with a higher edit-distance to the horse genome than to the human genome. using simulation, we could show that a substantially higher fraction of horse high-quality hits  could be identified .

when applied to the real sequencing data, the modified filtering criteria led to the recovery of  <dig> % of hits previously filtered as human for helicos, and  <dig> % for illumina . the smaller fraction of previously filtered hits recovered for the helicos platform may be explained as a consequence of the shorter read lengths, which increases the chance alignment of a horse read to a homologous region in the human genome. to examine the adequacy of this new filtering approach, nucleotide misincorporation patterns were plotted for reads mapping only to the horse genome, and for reads mapping to both the horse and human genomes . in the latter case, nucleotide misincorporation patterns were further examined for reads showing a lower edit distance to the horse, or equal or lower edit distances to the human genome . interestingly, reads mapping against both the horse and human genome showed nucleotide misincorporation patterns indicative of post-mortem cytosine deamination, regardless of the edit distance to the horse genome . as reads mapping uniquely against the human genome did not exhibit any trace of post-mortem cytosine deamination , this suggests that a substantial fraction of the reads filtered as putatively originating from a human source actually consisted of genuine horse reads, that are still filtered from our analytical pipeline. interestingly, g → a  and c → t  misincorporation rates at 5’-ends were found substantially higher for the fraction of reads showing a lower edit distance to the horse genome. this most likely results from the edit-distance, as we favor those hits that show fewer mismatches against the horse genome, including hits with less damage. while our filtering approach discarded a substantial fraction of potentially endogenous hits , we suggest that the above method is still more valuable than the first approach where all hits to the horse genome that also mapped to the human genome were eliminated. this approach can be used to reduce the fraction of endogenous dna sequences that would have been filtered otherwise due to high proximity to human genomic sequences.

divergence estimates
the set of alignment parameters has shown a significant impact on the population of sequencing reads identified as endogenous by bwa. reads with extremely high nucleotide misincorporation rates at the 5’-ends are disregarded with default parameters but are recovered when mapping is performed with no seed  or after the first  nucleotides are trimmed . additionally, we have shown that excluding reads based on their mapping onto the human genome could result in a substantial loss of potentially endogenous hits . we further investigated how these different approaches could affect divergence estimates between the ancient sample and the modern reference. in addition to true biological mutations, a number of other phenomena contribute to the total number of differences observed between sequencing reads and the reference: sequencing errors, post-mortem damage  and local misalignments. base qualities and gc → at misincorporations could be used as proxies for sequencing errors and post-mortem damage, respectively. however, not all gc → at substitutions originate from post-mortem cytosine deamination and a number equivalent to the reciprocal class of substitution  would be expected as a result of the process of lineage divergence. hence, a simple measure of divergence could be estimated by summing over sites showing high base quality scores all classes of transversions, indels and by doubling the number of at → gc transitions. this estimate is reported with a black line on figure 
 <dig> as a function of base quality scores  for illumina reads . not unexpectedly, divergence estimates increased drastically with low bq as a result of sequencing errors. for bq ≤  <dig>  sequencing errors even exceeded nucleotide misincorporations resulting from post-mortem cytosine deamination . high base quality scores , however, provide divergence estimates ranging  <dig> %– <dig> %. of note, neither read trimming nor changing the mapping procedure to the new set of modified parameters seemed to affect divergence, with estimates ranging from  <dig> % to  <dig> % for bq ≥  <dig> . read trimming, however, reduced  gc → at misincorporation rates in agreement to the higher cytosine deamination rates observed at overhanging ends in ancient dna fragments
 <cit> . 

the reference used for mapping could also represent another potential source of bias for divergence estimates as reads showing low, if any, divergence to the reference exhibit higher mapability success. yet, the different sets of alignment parameters investigated here do not have similar tolerance for highly divergent reads. therefore, we investigated whether standard and the suggested modified alignment parameters will provide similar divergence estimates between the ancient and modern horse. in absence of a complete genome from an ancient horse and from a non-caballine equid, we decided to restrict the analysis to the mitochondrial genome that is fully sequenced in donkeys . interestingly, although more hits were recovered using our modified parameters, no significant difference in the relative numbers of reads mapping the horse or the donkey mitochondrial genome was detected when using the standard or the different sets of parameters recommended here . thus, regardless of which alignment parameter was used, similar divergence estimates were recovered between the ancient sample and the horse mitochondrial genome reference  when using the horse mitochondrial as a reference for mapping . lower estimates were found when using the donkey mitogenome for mapping and realigning the reads identified to the horse . this illustrates that mapping against distant genome references leads to a significant under-estimation of divergence estimates, likely due to the fact that most divergent reads are missed. it follows that it is unlikely that the nuclear divergence estimate provided above was underestimated, given that the mitochondrial divergence was found to be 2- to 3-fold higher, suggesting the ability of our mapping parameters to identify most divergent reads. therefore, as it provides unbiased estimates of sample divergence while increasing sequence coverage, we believe that the approach recommended here would improve the sensitivity of the mapping procedure in other situations where ancient reads show similar levels of post-mortem damage and could accommodate divergence estimates up to at least  <dig> %.

CONCLUSIONS
in this study, we have demonstrated significant improvements in the identification of adna reads deriving from shotgun sequencing data, by optimizing the computational procedure used to identify endogenous dna. we have shown that illumina and helicos sequencing data recovered from adna extracts could not be aligned to modern reference genome sequences with the same efficiency unless mapping parameters are optimized both for the specific types of errors generated by these platforms and for post-mortem dna damage. for helicos tsms reads, we have shown that simple modifications to the behavior of the semi-global aligner can yield a  <dig> %– <dig> % increase in the amount of endogenous sequences identified. similar modifications yield a smaller increase for illumina sequencing data, most likely due to the longer reads and higher information content present in the sequences generated on that platform. additionally, we have shown that selective trimming of nucleotide misincorporations that result from post-mortem damage at overhanging ends of template strands could only slightly improve our abi-lity to identify endogenous sequences for helicos tsms sequencing data on samples showing high levels of cytosine deamination. this procedure was found to have only moderate effects for illumina reads. we have shown that additional gains could be achieved by filtering putative human contamination according to the edit distance of the alignment.

the fact that the specific nature of adna together with the specificities of sequencing platforms must be taken into account when attempting to identify endogenous adna fragments has important implications for future adna research, as our findings make it possible to recover a greater amount of endogenous dna sequences with no need for extra sequencing runs and further destruction of fossil samples. together with recent developments in molecular methods for more specific extraction, capturing and/or sequencing of adna molecules
 <cit> , the series of computational approaches presented in this study offer an additional technique for maximizing the recovery of genome-wide sequence information from ancient individuals and extinct species in a more efficient way. our findings pleads for the development of mapping software dedicated to the specificities of adna reads, combining high rates of particular classes of nucleotide misincorporations at read termini together with short read sizes. for now, this development is still in its infancy, with programs such as mia
 <cit> , sesam
 <cit>  and anfo
 <cit>  using nucleotide misincorporation patterns resulting from post-mortem damage in their scoring schemes, but most algorithms have limited use due to excessive runtime costs. we are convinced that such developments will contribute tremendously to the success of large-scale paleogenomic projects as it will improve our ability to identify the genuine fraction of reads originating from ancient samples in which contamination is inevitable.

