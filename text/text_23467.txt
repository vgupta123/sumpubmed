BACKGROUND
protein-protein interactions are constitutive of almost every biological process. the ability to identify the residues that form the interaction sites of these complexes is necessary to understand them. in particular, it is the basis for new therapeutic approaches to treat diseases  <cit> .

a great deal of work has been done on developing in-silico prediction methods. as already observed by zhou et al.
 <cit> , these methods can be subdivided with respect to the kind of mathematical foundation invoked and with respect to the features or characteristics of the protein used.

residue-wise score-based prediction methods
let xr be the data relevant for a residue r in a given protein chain. these methods then employ a function f, where λ are some coefficients which have been learned through the training. the value of f then determines, whether r is rated as an interface or not. the linear regression method  <cit> , the scoring function method , the neural network method , and the support vector machine method  are of this kind.

probabilistic methods
let x be the data relevant for a protein chain, where these data are assumed to stem from a random source thus obeying a random distribution. x, which alternatively is called the observation, typically includes the structure. the label sequence of the residues y that classifies each individual residue either as interface or as non-interface is assumed to be random, too. typically, probabilistic methods use the conditional probability distribution   to determine a classification y* of the residues of maximal posterior probability  . naive bayesian methods  <cit> , bayesian network methods  <cit> , hidden markov models   <cit> , and linear-chain conditional random fields taking the backbone as underlying graphical structure  <cit>  fall in this category. using posterior decoding on the basis of the forward-backward algorithm, both hmms and crfs are residue-wise score-based prediction methods, where the binary decision is made by thresholding the posterior probabilities of classifying the residues as interface.

notations
we use latin uppercase letters when referring to random aspects of the objects denoted by them. in contrast, lowercase letters denote arbitrarily chosen but fixed objects. in this context boldface letters indicate vectors, the corresponding non-boldface letters their coefficients.

the vast majority of methods use the 3d structure of the target protein chain in form of a pdb file as input . however, a few methods are not requiring a 3d structure and rather use sequences only  <cit> . we here consider the problem with a given 3d structure of the target protein chain. sequence-based input may include a multiple sequence alignment of related proteins from which, for example, sequence conservation can be inferred. when the 3d structure of an unbound binding partner is also available, protein-protein docking methods can be applied. this has also been exploited to provide feedback from docking to the more specific problem of interface prediction  <cit> . we here consider the case where the binding partner’s 3d structure is not given. nor requires the presented method the sequence of the binding partner. albeit, we tested on homodimers only as we here rather focus on our new method rather than on features or types of proteins. the protein features used for interface prediction in the literature are reviewed in the methods section as far as we make use of them in this article.

most of the current studies for predicting interaction sites of proteins that use a probabilistic method are restricted by treating the residues of the proteins as independent vertices. li et al. have taken the backbone neighborhood into account thus modeling the protein as a sequence  <cit>  using what can be called a line crf or linear-chain crf. the features they define on the label pair of two backbone neighbors have the effect of smoothing the predicted labels along the protein sequence. decisive is, however, that they were the first who used conditional random fields  for interface prediction. crfs in turn have come into use for solving sequence labeling problems due to lafferty et al.
 <cit> . see  <cit>  for an overview. from the mathematical point of view they take advantage of the fact that they model the conditional probability   rather than the joint probability  . recently there has been an explosion of interest in conditional random fields  with successful applications. it has been shown that crfs have the abilities for solving sequence labeling problems like part-of-speech tagging   <cit>  and natural language processing  <cit> . furthermore in the web extraction problem, in which the web-sites are modeled as two dimensional grid graphs, crfs perform well  <cit> . one of their outstanding benefits over many other statistical models is that a crf can easily describe the dependencies of observations.

as proteins are folded into three dimensional structures, spatial relationships create dependencies between residues. for example, we find on the test data described below that the correlation coefficient between spatial neighbors that are not also sequence neighbors  is  <dig> . this is only slightly lower than the correlation coefficient between residues that are sequence neighbors . as there are more than three times as many spatial pairs of neighbors than sequence neighbors at this threshold it is reasonable from a modeling standpoint to use a model that respects all dependencies induced by spatial proximity, not only the dependencies induced by proximity along the backbone.

there are many papers using spatial neighborhood information of residues to predict-protein interaction sites . however, the spatial information of proteins was only integrated into the feature functions, but not represented in the model. for probabilistic models, the difference between the two ways to integrate spacial information is that in previous models the label of the i-th residue yi is conditional independent from the labels of other residues given data x and – in the case of linear crfs or hmms – given the labels of yi− <dig> and yi+ <dig>  even when neighborhood information is only used for spatial smoothing of the labels, the intuitive advantage over, say, an svm classifier that uses spatial neighborhood in the features but classifies each residue independently, is that not-patch-like candidate labelings are explicitly punished. in contrast, such an independent classifier-approach may have a tendency to predict individual interface residues ‘sprinkled’ around the protein surface  <cit> .

for this reason, a general crf seems to be more suitable for the task. however, inference for general crfs is intractable. in this paper, pairwise conditional random fields  are utilized. specializing general crfs, only node cliques and edge cliques are taken into consideration in pcrfs. a pcrf retains most spatial information of proteins, can be specified with the same number of parameter as a line crf and approximate inference remains feasible with the generalization of the viterbi algorithm introduced here. taking pattern from piecewise training methods  <cit> , we disentangled the labels of nodes and edges to train the model.

in order to take advantage of a residue-wise score-based predictor, we model the protein surface by means of a pcrf, where the observation is solely a sequence of surface residue scores between  <dig> and  <dig> output by the predictor. we then utilize a generalized viterbi algorithm and piecewise training. the resulting tool tries to enhance the predictor chosen on the surface of the protein under study. it is the aim of this paper to demonstrate effectiveness of this approach provided that the interface residue scores and the non-interface residue scores are appropriately distributed.

methods
we address the problem of improving residue-wise score-based predictors for protein interface residues as a node labeling problem for undirected graphs using the model class of conditional random fields . lafferty et al.
 <cit>  were the first who applied crfs to the problem of labeling sequence data. li et al.
 <cit>  used line crfs to address the interaction site prediction. they have the advantage that the viterbi algorithm well-known from decoding hmms can be used to efficiently infer the most likely labeling sequence. very useful and illustrative presentations on crfs are given in  <cit> . above crf-based models make the assumption that the label of one residue is conditionally independent of the labels of all other residues given the labels of the two adjacent residues in the protein sequence. to the best of our knowledge, we are the first to employ a graphical model that takes the spatial neighborhood of residues located on the protein surface into account.

this section is subdivided into three parts. we first explain how we model protein surfaces by pairwise crfs. then we introduce our new inference method. finally, we elucidate our training method.

using conditional random fields to model protein surfaces
for every protein under study that has n surface residues, a pair of random vectors  is considered. the vector x is the observation that represents the knowledge about this protein that is utilized in the prediction, e.g. the 3d structure of the target protein and a multiple sequence alignment together with homologs.

the vector y is a random sequence of length n over the alphabet {i,n} that labels the index set { <dig> ,…,n}, which in turn is called the set of positions . the label i represents interface residues, whereas the label n represents non-interface residues. {i,n}n is the set of all label sequences of length n over {i,n}. we will also call them assignments as the term ‘label sequence’ may lead to confusion when applied below to subsets of { <dig> ,…,n} that are not contiguous sequences.

let  be the neighborhood graph, where  is the set of positions,  is the set of edges that typically results from an atom-distance-based neighborhood definition for positions. we assume for convenience in notation that  has no isolated nodes. cases with isolated nodes could trivially be reduced to cases without isolated nodes. let  be the set of ’s cliques, which we refer to as node cliques. for a node clique  and an assignment y we denote by yc the restriction of y to the positions belonging to the node clique c. for c={i} and c={i,j} we write yi and  rather than y{i} and y{i,j}.

the preceding notation is also used in the slightly more general case of partial label assignments to arbitrarily chosen subsets  of the set of positions . formally, let  denote . given two partial assignments  and  are identical on , the union  is well-defined.

the conditional distribution function of our pcrf  with respect to the neighborhood graph  is defined as follows:
  <dig>  

where x and y are arbitrarily chosen instances of the random observation x and the random label sequence y, respectively, Φc∈∙ () is the feature of the crf located at the node clique c , and z is the observation-specific normalization factor defined by
  <dig>  

let us call  the score of the label sequence y given the observation x.

a crf is called a pairwise crf  if Φc≡ <dig>  for all node cliques c larger than two. the remaining features Φi and Φi,j are referred to as node features and edge features, respectively. thus, every position  and every edge  is represented by the pair ,Φi) and by the quadruplet ,Φ{i,j},Φ{i,j},Φ{i,j}).

following  <cit> , we assume moreover that each node feature and each edge feature is a sum of weighted base features. more precisely, for every position  and every edge  we assume representations
  

where y∈{i,n}n and x is an observation. the two real vectors
  <dig>  

need to be calculated in a training phase.

in the most general sense, protein characteristics are real-valued evaluations of positions and pairs of adjacent positions , respectively, that are correlated with our position labeling problem. we use a standard step function technique to obtain base features from protein characteristics, rather than taking the raw values of the characteristics. to make our paper self-contained, let us describe this technique for short.

a protein characteristic depends on the observation and either a node or an edge. each protein characteristic, such as e.g. the relative solvent-accessible surface area of a residue, is transformed into several binary features by binning, i.e. we distinguish only a few different cases rather than the whole range of the characteristic. assuming the common case of real-valued characteristics, the bins are a partition of the reals into intervals. the use of this discretization allows to approximate any shape of dependency of the labels on the characteristics, rather than assuming a fixed shape such as linear or logarithmic.

from protein characteristics for positions to node features. we subdivide the range of the characteristics c into say γ intervals, where γ is at least two. let s1<s2<…<sγ− <dig> be the corresponding interval boundaries. it is reasonable to take sι as the ι/γ-quantile of the empirical distribution of c for non-interface residues, where c∈ y and any subset  of , let  denote the partial assignment of y with respect to .  introduced earlier in this study).

if  are pairwise disjoint position sets, the assignment for  canonically resulting from assignments  is denoted by . for , the score  is defined by
  

then the problem of determining a most probable label sequence y∗ given an observation x can be reformulated as
  

this is the case, because it suffices to consider the score.

to put this into practice, we devised an algorithm we call generalized viterbi. on the one hand, it is analogous to the classical viterbi algorithm. on the other hand, there is a major difference. in our case there is no canonical order in which the positions of  are traversed. having explained our algorithm for any order, we show how to calculate a fairly effective one. in what follows, we assume that the positions not yet touched are held in a dynamic queue. those positions having already left the queue form the history set.

assume that the subgraph of  induced by  has connected components , …, . for μ= <dig> ,…,m, let  be the so-called boundary component associated with  defined by . the complement  is the interior of the μ-th history component. see figure  <dig> for an example.figure  <dig> 
example history set

having boundary

.




for assignments  of the boundary components , the viterbi variables  are defined as
  <dig>  

the viterbi variables can be represented as a set of tables, one table of size  for each boundary component . in the case where a boundary component is empty the table reduces to a single number.

at any stage, the algorithm stores the connected components , …,  of the current history set , the corresponding boundary components , …, , and viterbi variable values  where  range over all possible assignments of corresponding boundary component. we store for every assignment on the boundary, a maximizing interior assignment. this assignment is the argmax of  but is determined with the dynamic programming recursions defined below. let us call these data the current state of the algorithm. it mainly consists of record sets indexed by the boundary labelings.

at the very beginning the queue contains all positions, the history set  and the corresponding boundary component  are empty. as long as the position queue is not empty, the top element v is extracted and the state is updated as follows.

adjoining v to the history set , there are two cases to distinguish. either position v is not adjacent to any other position of any old boundary component  or adjoining position v to  results in adding it to some connected component of the old history set or even merging together two or more of them .figure  <dig> 
computing the connected components of the new history set - case one.

computing the connected components of the new history set - case two.




in the first case we simply have to take over all the old connected components, boundary sets and viterbi variables. moreover, we perform the instructions
  

in the second case position v is adjacent to some boundary components, say . then the old history components  and the current position v are merged together:
  

the other history set components and corresponding viterbi variables are not affected.

for μ=m′,m′+ <dig> …,m, let  be the set of all positions out of  that are no longer boundary nodes after having adjoined v to the history set. the nodes in  are removed from the boundary  after the iteration. let  be the complement of  in . by inspecting the edges incident to the current position v, all these sets can be computed in linear time.

the new boundary set  is then either  or , where it can be checked in linear time whether or not v is a new boundary position.

we are now in a position to calculate the new viterbi variables , where  ranges over all assignments of the new boundary set .

if  then
  

here, any assignment of a node set is assumed to implicitly define assignments for any subset thereof. figure  <dig> illustrates this case of the recursion step. if, however, , then
  figure  <dig> 
viterbi recursion step in case

. after adding node v to the history set,  and  will be replaced by . in this example, for every assignment of the new boundary  the score of  is maximized by varying over the assignments of v and  and using the viterbi variables of  and .



finally, the interior labeling is stored, where the maximum is attained. the algorithm terminates after the last node v from  has been processed. in the typical case, where the graph is connected, at termination .

the running time of the algorithm is , where b is the size of the largest boundary set and n is the number of surface residues. we call this algorithm generalized viterbi algorithm as for the case of a graph that is a linear chain 1−2−3−⋯−n of nodes using the node order  <dig> ,…,n the viterbi variables we define are the same as in the standard viterbi algorithm for hmms. in the case of a graph that is a tree, this algorithm specializes to the fitch algorithm or an argmax-version of felsenstein’s pruning algorithm when a leaf-to-root node order is chosen after rooting the tree at an arbitrary node. in both special cases the boundary sets always have size at most  <dig>  the tree example also motivate the use of several history sets at the same time: using a single history set only, one would not be able to achieve a linear running time on trees.

a heuristic based on the generalized viterbi algorithm
first, it is vital for our generalized viterbi algorithm to keep the size of the boundary sets small. a good position order is here of great importance. the algorithm starts by choosing a vertex of minimal degree. when determining the next position to be dequeued, the algorithm selects a boundary node such that the number of incident edges leading to nodes not belonging to any current history set is minimal. in an arbitrarily chosen order these nodes are dequeued next.

second, the space demand is reduced by restricting the number of boundary labelings admitted. starting from the available labelings of the current history set, the percentage of the reachable boundary labelings of the successor history that will be discarded is calculated. then the corresponding percentile is estimated. to this end, a sufficiently large sample of possible labelings of the new boundary set is drawn, the viterbi variables are computed, and the corresponding sample percentile is taken. finally, only those boundary labelings of the new history set are retained whose viterbi variables exceed this percentile.

that way we compute near-optimal solutions good enough for our purposes within feasible computation time.

piecewise training for pcrfs
let
  

be the independent identically distributed training sample. for every μ= <dig> ,…,m, let  and  be the set of positions and edges in the neighborhood graph associated with xμ, let  be the number of positions of the μ-th training example and let  be the set of all possible label sequences of this graph.

this data set is unbalanced as there are many more non-interface positions as interface positions. as customary for other machine learning approaches such as support vector machines and artificial neural networks  <cit> , we here manipulated the ratio of positive and negative example positions for training in order to obtain reasonable results.

we have amplified the influence of the positive examples, rather than selecting various sets of training data by deleting negative ones as done in  <cit> .

let νi, νn, νii and νnn be the number of interface positions, the number of non-interface positions, the number of interface-interface edges, and the number of non-interface-non-interface edges in , respectively. then we define the following two amplifier functions for all positions i and for all edges {i,j} of the m neighborhood graphs resulting from the training data .
  

to uniformly govern the influence of the amplifiers, we introduce an amplifier control parameterη3∈ .

we set up our two log-likelihood objective function by
  

where ideally for each μ= <dig> ,…,m  

is the training-instance-specific normalization factor.

unfortunately, maximizing this objective function in general is algorithmically intractable. taking pattern from sutton et al.
 <cit>  who introduced what they called piecewise training, we deal with this problem by disentangling the labels of nodes and edges. for μ= <dig> ,…,m, a non-coherent labeling of the neighborhood graph  is any mapping that assigns to every position  and every edge  a label yv∈{i,n} and a pair of labels ye∈{i,n} <dig>  respectively.

we then replace z,η3) by
  

as normalization factor. this makes the optimization problem computationally feasible.

the l-bfgs method  <cit>  is used to solve it. that way we obtain the coefficient vectors α and β , which depend on the amplifier control parameter η3∈ .

to mitigate the negative consequences of disentanglement, we use a correction factorδ≥ <dig>  for any characteristics d and ι= <dig> ,…,γ− <dig>  the weights of the bases edge features  and   are all multiplied by δ. thus a change in classification along an edge is additionally penalized. the correction factor δ is set best between  <dig>  and  <dig> .

for our implementation of the training, we used the java crf package from sunita sarawagi at http://crf.sourceforge.net/.

RESULTS
in this section we demonstrate effectiveness of our pcrf-based protein surface model to enhance residue-wise score-based predictions of protein-protein interfaces. for the sake of ensuring reliability of the methods we used three data sets. the first one is planedimers due to zellner et al.
 <cit> , the second one is the list of  <dig> two-chain-proteins published by keskin et al.
 <cit> , which was used by liet al.
 <cit>  to test their linear-chain crf. third, we used a non-redundant data set containing  <dig> unique interface structures very recently compiled by cukuroglu et al. and published in  <cit> .

the data set planedimers is less known than the data due to keskin et al.. it consists of redundancy-free homodimers with flat protein-protein interfaces. zellner et al.
 <cit>  developed an svm, called prescont, that assigns to each residue on the protein surface a score between  <dig> and  <dig>  which we refer to as prescont score in the sequel. the larger the score, the more likely the residue belongs to the interface. zellner et al. made the prediction by thresholding the score. the prescont server and the data list planedimers are publicly available .

in the first subsection we describe two sets of experiments performed with synthetic data, one on planedimers
 <cit> , the other one on the list published by keskin et al.
 <cit> . in both cases we independently assign to each surface position a random score drawn according to two different parametrized sequences of β-distributions betaβi) and betaβn), one for the interface sites determined by the reference labeling, the other one for the non-interface positions. the parametrized values αi, αn, βi and βn determining the two sequences of distributions are chosen such that the following conditions are satisfied. the mean values ei>en are the average prescont scores on interface sites and non-interface sites of all chains from planedimers. the variances  and  are equal to  and , where  and  are the corresponding variances of the prescont score, and ς∈{ <dig> , <dig> , <dig> , <dig> , <dig> } models the precision of the synthetic score. the deciding feature of all these distributions is that they are unimodal. the result of the subsection is that enhancement works for unimodal score distributions.

the second subsection is about a synthetic data experiment on a new data set due to cukuroglu  <cit> . here we follow the line of the first subsection except for the fact that we restrict ourselves to signal precision ς= <dig> .

in the third subsection we study the prescont scores for two-chain protein complexes from the data set planedimers. according to figure  <dig>  the prescont score for non-interface residues is far from being unimodal. however, if one restrict oneself to the part above a threshold in the neighborhood of  <dig>  and larger, one may ask whether enhancement restricted to that domain will works. the subsection answers this question in the affirmative. having chosen a threshold as described above, one can improve the classification with respect to this threshold as follows. take over the prediction for scores below the threshold and reclassify the residues the scores of which are above by means of the pcrf-based enhancer.figure  <dig> 
the distribution of the
prescont
score for complexes from the data set
planedimers
.




in general, observations x could encompass a pdb file, which in particular determines the 3d-structure of the protein, together with an msa that models evolutionary aspects. in our case an observation solely consists of the prescont score sequence or of the sequence of synthetic scores for the surface residues. formally, every observation x is equal to a vector ∈ n.

there are several neighborhood notions for residues, surface/core definitions and interface determinations in the literature. when studying the data set planedimers, we follow  <cit> . in the case of the list due to keskin et al.
 <cit> , the definitions according to  <cit>  are used. finally, when studying complexes taken from the data set published in  <cit> , we take the following definitions. the rasa value of a surface residue is at least 15% . two residues are defined as contacting if the distance between any two of their atoms is less than the sum of the corresponding van der waals radii plus  <dig>  Å .

anyway, according to keskin et al.
 <cit>  we define the distance of two residues on one and the same chain as the distance of their major carbon atoms. we then say that one residue is nearby another residue, if they are at distance below  <dig> Å. . this definition in turn is the basis of the neighborhood graph  underlying the pcrf. two surface positions are joined together by an undirected edge if and only if the corresponding residues are nearby ones.

our pcrf-based enhancer utilizes one position characteristic and two edge characteristics on the basis of the standard step function method explained in the methods section. if x=∈ n is the observation associated with the protein under study, and if  is the neighborhood graph, then for every position  and every edge  we set
  

to enhance predictions obtained by thresholding, solely information coming from the residue neighborhood relations on the surface is additionally used.

in order to be able to calculate the performance measure of area under the roc curve  for our pcrf-based enhancer on synthetic scores, we proceed as follows. for each edge , we replace the local feature value Φi,j by κΦi,j, where κ∈.

we enhance residue-wise score-based predictors only on the protein surface. in our synthetic data experiments there is no predictor available for core residues. for proteins taken from the data list published by keskin et al.
 <cit>  it happens that interface sites belong to the core. that is why we use what we call surface auc ratioΓ of the enhancer as our performance measure for our synthetic data experiments.
  

if Γ is greater than  <dig>  the enhancement was successful. the larger Γ, the greater success.

to estimate performance measures, we applied 5-fold cross-validation experiments.

a fully built-out pcrf-based tool box for modeling protein surfaces needs to comprise all the standard algorithms as e.g. forward-backward techniques, marginalization and posterior decoding known for hmms and linear-chain crfs. to begin with, in the fourth subsection we explain how to put a variant form of the forward algorithm and posterior decoding for pcrfs into practice.

simulating unimodal scores of various precisions
we estimated means ei and en and variances  and  of the prescont score on interface sites and noninterface positions of planedimers, respectively, as follows.
  <dig>  

we randomly chose  <dig> instances under the uniform distribution from the data set published by keskin et al.
 <cit>  to perform our experiments. let us refer to this set as kl-subset in the sequel. .

zellner et al.
 <cit>  used the following determinations. a residue is defined to be part of the protein surface, if its relative solvent-accessible surface area is at least 5%
 <cit> . a surface residue is said to constitute an inter-facial contact, if there exists at least one atom of this residue which has a van-der-waals-sphere at a distance of at most  <dig>  Å from the van-der-waals sphere to any atom from a partner chain residue  <cit> .

based on  <cit> , li et al.
 <cit>  assume an inter-facial contact of a residue on a chain is assumed to be there, if any heavy atom of this residue is at distance of at most  <dig> Å from any heavy atom from a partner chain. the relative solvent-accessible surface area of surface residue is at least 15%.

we independently assigned to each interface surface residue of the two data sets a random score between zero and one according to the β-distribution betaβi), and to every non-interface surface residue a score according to betaβn), where the score precision ς satisfies
  <dig>  

and the parameters αi,βi,αn,βn were chosen such that
  <dig>    <dig>  

the surface auc ratios of the enhancer compared with the threshold predictor on planedimers and the kl-subset are displayed in table  <dig>  there is an improvement of  <dig> %− <dig> % on planedimers and of  <dig> %− <dig> % on the kl-subset.table  <dig> 
classification results on
planedimers
and the kl-subset, where the
β
-distributions according to which the synthetic scores were drawn are defined by equations  <dig>   <dig>   <dig> and 10


planedimers
surface auc ratio Γ

kl-subset
surface auc ratio Γ
depending on the variances determined by ς, the enhancer increases the auc referred to the protein surface by  <dig> %- <dig> %. on planedimers, and by  <dig> %- <dig> % on the kl-subset.



moreover, we compared individual classification results obtained by thresholding the scores with pcrf-based enhanced predictions. because of the fact that the specificity of the threshold predictor can be easily changed by manipulating the threshold, we proceeded as follows. for every score precision, the pcrf-based enhancer has a well-defined specificity referred to the surface residues. we then chose the threshold such that the specificity of the threshold predictor is close to that of the enhancer. the results are shown in table  <dig>  the sensitivity is increased by 53%−67% on the data set planedimers and by 14%−22% on the kl-subset.table  <dig> 
comparing the enhancer with the threshold classifier of approximately equal specificity on synthetic scores assigned to surface residues of protein complexes taken from the data set
planedimers
and the kl-subset



planedimer

kl-subset


utilizing the new data set due to cukuroglu  <cit> 

as in the case of the kl-subset, we randomly chose  <dig> dimers. we refer to the resulting list as cgnk-subset. having assigned synthetic scores according to equations  <dig>   <dig> and  <dig>  where ς= <dig> , we compared individual classification results obtained by thresholding the scores with pcrf-based enhanced predictions in exactly the same way as we did for the kl-subset. the results are shown in table  <dig>  the sensitivity is increased by 22%.table  <dig> 
comparing the enhancer with the threshold classifier of approximately equal specificity on synthetic scores assigned to surface residues of protein complexes taken from the cgnk-subset




a main finding of cukuroglu  <cit>  relevant to protein-protein interface prediction is, that the average interface rasa value is greater than 40%. since our method is designed to improve performance of a given residue-wise predictor, using this result is not in the scope of this paper. however, a crf-based predictor integrating features for cliques of size greater than  <dig> is not beyond the range of current algorithmic capabilities. in such a model a feature set that discretizes the mean rasa value of cliques is promising.

enhancing the prescontserver prediction on planedimers
for the sake of completeness, we shortly review the residue characteristics used by prescont.

relative solvent-accessible surface area
for any residue a, the solvent-accessible surface areaasa can be computed by e.g. the software library ball  <cit> . most of the classifiers known from the literature utilize this characteristic . for prescont the relative solvent-accessible surface area according to
  <dig>  

is taken into operation, where asamax is the maximally possible accessible surface area of residue a
 <cit> .

hydrophobicity
many interfaces possess a hydrophobic core surrounded by a ring of polar residues  <cit> . in order to reduce noise, in  <cit>  the contribution of hydrophobic patches rather then the influence of individual residues is utilized.

residue conservation
measures of this type utilized in  <cit>  are the shannon entropy and the relative shannon entropy of empirical residue distributions in msa columns. as an alternative, empirical expectations of blosum-based similarities are taken for them.

scores of local neighborhoods
they are evaluated by means of log-odd ratios of neighboring residue pair frequencies in interfaces as opposed to residue pair frequencies on complementary protein surface areas. the resulting scores are averaged both over the neighborhood of the positions under study and the rows of the msa associated with the protein.

on the basis of figure  <dig> we enhanced prescont for thresholds θ∈ . the decisive factor for this choice is that the prescont score distributions for interface sites as well as non-interface positions above θ are “sufficiently close to” unimodal distributions. for every such θ, we set all scores less than or equal to θ to zero and then left the classification of all surface residues to the pcrf modified as follows. the residues of score zero are not taken into account when it comes to discretizing the protein characteristics . let us call this enhancing aboveθ.

to evaluate improvements we proceeded as when compiling table  <dig>  for every threshold θ under consideration another threshold θ′ was chosen such that thresholding at θ′ has the same specificity as enhancing above θ. the results are displayed in table  <dig> and visualized for an individual protein by figure  <dig>  according to table  <dig> the increase in sensitivity ranges from 4% to 7%. the true-positive predictions on the surface of the protein with pdb-entry 1qm <dig> are compared in figure  <dig>  where again the specificity of the two classifiers is the same.table  <dig> 
enhancing above various thresholds on
planedimers
, where
prescont
’s threshold was chosen such that the specificity approximately equals that of enhancing



prescont

prescont

prescont

prescont

prescont

prescont
the sensitivity increased that way by 4%-7%. for every pair of experiments, the number of true negatives , false negatives , false positives  and true positives  are displayed.
comparison of enhancer and
prescont
service of same specificity on the protein with pdb-entry 1qm <dig> 
 green spheres on the left show the interface surface residues correctly predicted by both tools.  red spheres on the right indicate additional true positives of the enhancer.



discussing posterior decoding
as in the case of linear-chain crfs, the generalized viterbi algorithm can be transformed into a variant form of the forward algorithm. it might be the case that the following additional problem arises.

let v <dig> v <dig> …,vn be the ordering in which the positions of  are traversed by the algorithm, and let  denote the set of position indices i<n such that vi is not an element of the boundary  of the history set  at stage i. if  is not empty, we encounter an obstacle when it comes to sampling label sequences. for , position vi is not labeled in the course of the sampling procedure. that is why we augment the neighborhood graph  so that those positions no longer exist, all predictions remain unchanged, and the order of magnitude of the running time is not increased. to this end, we complement the ordering v <dig> v <dig> …,vn as follows. for every , we insert a new node  between vi and vi+ <dig>  having extended the neighborhood graph by these nodes not being associated with residue positions of the protein under study and by new edges , where for  and  the above mentioned obstacle is eliminated without any influence on the prediction and the order of magnitude of the running time.

proceeding now in a way analogous to the classical case, in every formula that is a building block of the generalized viterbi algorithm the following two steps of replacement need to be performed.

first, for every position , every edge , every label y0∈{i,n}, and every label pair ∈{i,n} <dig>  we replace Φi with exp), and Φ{i,j} with exp).

second, we replace sums with products and then maxima with sums.

thus we obtain as analogues of the viterbi variables  defined by equation  <dig> what we call component forward variables .

if  and  are the connected components of the history set  and the corresponding boundary set  at stage i∈{ <dig> ,…,n}, respectively, then the forward variable at stage i with respect to a boundary assignment  is defined as
  

for any assignment  , the forward variable  is a nontrivial linear combination of forward variables , where  ranges over some assignments of the boundary set  at stage i− <dig>  analogous to the linear-chain case, a random backward walk through a state graph, with all possible assignments   being the set of nodes, results in a random labeling of the positions, where each labeling is drawn with its posterior probability.

this sampling technique allows the efficient calculation of posterior probabilities at nodes and edges in a straightforward manner.

CONCLUSIONS
residue-wise score-based threshold predictors of protein-protein interaction sites assign to each residue of the protein under study a score. the classification is then made by thresholding the score. in case of using probabilistic data models, the parameters of the threshold predictor have been learned on a training data set in advance.

we have demonstrated that such threshold predictors can be improved by pcrf-based enhancers given the shape of the interface surface score distribution and the non-interface surface score distribution with respect to the training set resemble the shape of unimodal distributions. besides the surface residue scores, only the spatial neighborhood structure between the surface residues of the protein under study is taken into account. thus, the improvement can be attributed to our model. in addition to the precision of the scores, the amount of improvement depends on the 3d-complexity of the interfaces to be predicted. to this end, three sets of experiments with synthetic surface residue scores for protein complexes randomly chosen from the data set planedimers compiled by zellner et al.
 <cit>  and from the lists published by keskin et al.
 <cit>  and cukuroglu et al.
 <cit> .

the enhancement is structurally based on the following model property of pcrfs in contrast to residue-wise predictors. though the scores of near-by residues may be correlated, labeling a position as interface or non-interface by thresholding the score does not influence the classification of its neighbors. when using pcrfs, this is the case.

the pcrf-based enhancer is also applicable, if the score distributions are only unimodal over a certain sub-domain. the improvement is then restricted to that domain. thus we were able to improve the prediction of the prescont server devised by zellner et al. on planedimers
 <cit> .

the prediction is made on grounds of a generalized viterbi inference heuristic. as for training, we developed a piecewise training procedure for pcrfs, where the enhancer needs to be trained on data originating from the same source as the training data of the threshold predictor to be improved.

a prototypical implementation of our pcrf-based method is accessible at http://ppicrf.informatik.uni-goettingen.de/index.html.

competing interests

the authors declare that they have no competing interests.

authors’ contributions

the model was designed by zd and kw, who also adapted piecewise training to pcrfs and performed the analysis of the first submission. the tool was designed and implemented by zd and kw, who were supported by mw and tw. the algorithms were devised by mg, ms, mw and sw. the generalized viterbi algorithm, however, was conceived by ms. the web server was set up by mg. the analysis necessary for the revised version was performed by tkld, who was supported by mw. the manuscript was drafted by zd and kw, written by ms and sw and revised by tw. sw conceived the study. all authors read and approved the final manuscript.

