BACKGROUND
overview of the zuker algorithm
the zuker algorithm predicts the most stable secondary structure for a single rna sequence by computing its minimal free energy . it uses a "nearest neighbor" model and empirical estimates of thermodynamic parameters for neighboring interactions and loop entropies to score all possible structures  <cit> . the main idea is that the secondary structure of an rna sequence consists of four fundamental substructures: stack, hairpin, internal loop, and multi-branched loop. these fundamental substructures are independent of one another, and the energy of a secondary structure is assumed to be the sum of the substructure energies. with a single rna sequence as input, the algorithm is executed in two steps. first, it calculates the minimal free energy of the input rna sequence on a group of recurrence relations, as shown in formula  to . second, it performs a trace-back to recover the secondary structure with the base pairs. experiments show that the first step consumes more than 99% of the total execution time. thus, computing energy matrices as quickly as possible is critical to improve the performance.

  w= min{w,min} 

  v=minehes+vvbivmpairisallowed∞pairisnotallowed 

  vbi= mini<k<l<j{el+v} 

  vm= mini<k<j{wm+wm} 

  wm= min{vm,min,v} 

suppose r1r <dig> ..ri...rj...rn represents an rna sequence where i and j are the location of the nucleotides in the sequence, and n is the sequence length. formula  to  describe the method for computing free energy. here, w is the energy of an optimal structure for the subsequence r1r <dig> ...... rj; v is the energy of the optimal structure of the subsequence riri+ <dig> ..rj; vbi is the energy of the subsequence ri through rj, where rirj closes a bulge or an internal loop; vm is the energy of the subsequence ri through rj, where rirj closes a multi-branched loop; and es, eh, and el are free energy functions used to compute the energy of stacked pair, hairpin loop, and internal loop respectively. given any subsequence ri...rj, the zuker algorithm calculates free energies of the four possible substructures if pair  is allowed. the results correspond to the four items in formula : eh, es + v, vbi, and vm. the zuker algorithm then selects the minimum value v among the four results. the subsequence grows from r <dig>  r1r <dig>  ..., r1r <dig> ...rj- <dig> to r1r <dig> ..ri...rj...rn. the lowest conformational free energy is stored in vector w. the corresponding energy of r <dig> is stored in w, and r1r <dig> is stored in w, and so on for longer fragments, such as w for r1r2r <dig> ...... rj- <dig>  once the longest fragment  is considered, the lowest conformational free energy of whole rna sequence is calculated, and the energy of the most energetically stable structure is contained in w. the corresponding secondary structure is then obtained by a trace-back procedure from the vector w, and matrices v and wm.

overview of cpu architecture
in recent years, the number of processing cores available on a modern processor chip has increased steadily. quad-core cpus are now the norm, and more core systems have become economically available. figure  <dig> shows a typical quad-core cpu architecture. each core hosts one thread at a time, with a set of registers containing thread state and a large functional unit devoted to computation and management.

multi-core cpus make rethinking the development of application software necessary. application programmers should explicitly use concurrency to approximate the peak performance of modern processors. to utilize all available processing power of these processors, computationally intensive tasks should be split up into subtasks for execution on different cores. a number of different approaches are available for parallel programming on multi-core cpus, ranging from low-level multi-tasking or multi-threading such as posix thread  library  <cit> , over high-level libraries, such as intel threading building blocks  <cit> , which provide certain abstractions and features attempting to simplify concurrent software development, to programming languages or language extensions developed specifically for concurrency, such as openmp  <cit> . apart from multi-threading parallelism on the multi-core platform, data parallelism can be explored by simd vector processing instructions. for example, intel has an simd instruction set called streaming simd extensions   <cit> . sse contains  <dig> new instructions and  <dig> new 128-bit registers. sse <dig> adds new arithmetic types, including maximum and minimum operations. each 128-bit register can be partitioned to perform four 32-bit integers, or single-precision floating points, or eight 16-bit short integers, or sixteen 8-bit bytes operations in parallel.

overview of gpu architecture
a hierarchy of gpu memory architecture is available for programmers to utilize. the fastest memories are the shared memory and registers with severely limited sizes. registers are allocated by a compiler, whereas shared memory is allocated by a programmer. the constant, texture, and global memory are all located on the off-chip dram. the texture and constant memory are read-only and are cached. the global memory is the slowest memory, and its access may take hundreds of clock cycles.

most research in gpu programming involves finding the optimal way to solve a problem on data-parallel architecture while best using optimizations specific to gpu architectures. of the many gpgpu apis available  <cit> , the nvidia cuda stands out as the most developed and advanced. gpgpu api only operates on nvidia gpus. our development of gpgpu applications uses cuda api limited to nvidia graphics cards.

cpu-gpu hybrid computing system
system architecture
a cpu-gpu hybrid zuker accelerating system is generally composed of several cpus and gpus. figure  <dig> depicts the hybrid system architecture with two cpus and two gpus. the cpus and gpus communicate via i/o hub chipset, and are connected to i/o hub by a quick path interconnect link  and pcie. both cpus and gpus have their own storage. each cpu has four cores. each core owns an l <dig> cache. two cores share one l <dig> cache. the gpu has several sms each with several sps. the cpu is responsible for program control, including initiating the zuker application, allocating tasks between cpu and gpu, initiating the gpu computation, reading the result from the gpu, and backtracking of energy matrices. the gpu is responsible for filling energy matrices for multiple rna sequences. as the gpu performs computation, the cpu simultaneously performs computing tasks, instead of waiting for energy results from the gpu.

the hybrid accelerating system predicts secondary structures of all rna sequences input. firstly, the cpu reads all rna sequences, and performs some preprocess operations such as memory allocation for energy matrices. after the task allocation, cpus and gpus fill energy matrices in parallel. the cpu receives results from the gpu device memory after energy filling is through. finally, the cpu executes backtrack operations, and displays all the rna secondary structure information to the user.

task-allocation and execution scheme
the proposed hybrid accelerating system fully exploits the performance potentials of both cpu and gpu to obtain higher system performance for the same computing task. the key factor in the hybrid accelerating system is the task allocation between the cpu and the gpu. given several rna sequences numbered  <dig> ,...,n, the task allocation obtains a boundary sequence number b. the sequences with number below b are to be computed on the cpu. the rest of sequences are computed on the gpu. to achieve load balancing in the task allocation, the processing capability of the cpu and the gpu for rna sequence with different length is estimated beforehand.

calculation of the boundary sequence number b is demonstrated below. the average execution time of a single sequence on cpu and gpu are first assumed as t1cpu and t1gpu, respectively. the entire energy filling time will be minimal when the cpu and gpu executions are overlapped nearly completely, satisfying equation t1cpu · b = t1gpu · . the boundary value b then can be computed by equation b=1k+1⋅n, where k denotes the speedup of gpu vs. cpu and equals t1cput1gpu. the expression 1k+ <dig> is called the allocation ratio.

performance tuning schemes
multi-core cpu implementation
performance tuning on multi-core cpu mainly consists of compiler optimization, sse, and multi-thread processing. first, compiler optimization is used by setting the -o <dig> option to improve the performance of original software. sse instruction is then utilized to accelerate computation of the vm matrix. the element vm depends on the row wm and the column wm. computation on vm is divided into two steps. in the first step, elements on the row wm are added to corresponding elements on the column wm. the minimal value of the sums is then chosen to be vm. using sse instructions, the four elements from row i and column j of matrix wm are read and stored in a 128-bit register. two 128-bit registers are added, such that four simultaneous additions are performed to compute vm. the third approach for boosting performance of the cpu is by openmp libraries that explore parallel processing of multiple sequences. energy matrices computations of different sequences are mapped onto different threads for processing by an intel compiler for openmp libraries. each thread is responsible for the matrices computations of a single sequence allocated onto it. threads are working independently from each other to access their own memory spaces, achieving high coarse-grain parallel performance.

gpu implementation
several parallel schemes are used to improve the zuker algorithm performance on gpu. basically, hierarchy parallelism is adopted. the first layer utilizes kernel-level parallelism for execution of multiple concurrent kernels. in figure  <dig>  the gpu device consists of two kernels, ♯ <dig> and ♯ <dig>  executing concurrently for w vector and v matrix computations. the second layer employs sequence-level parallelism, where sequences are processed by different blocks in the same kernel. one block can hold multiple sequences when the length of the sequence is sufficiently short. in figure  <dig>  the ♯ <dig> kernel contains several blocks, with each block responsible for v matrix computation of one or more sequences. the last layer uses parallel execution of diagonal elements in a single sequence. in figure  <dig>  elements in the diagonal of v matrix are mapped onto different threads in a block for parallel execution.

next, memory optimizations are adopted to fully use limited device memory. data type transformation, immediate assignment, and redundant data cutting schemes are used to reduce parameters storage requirement in the device memory. to improve the memory access performance, column data of the matrix are stored in a consecutive memory address array. sequence data and energy matrices are stored in shared memory and global memory respectively. a data reuse scheme is implemented by storing partly energy matrices in shared memory. mediating data in the computation is partly stored in shared memory and registers to improve memory access and memory bandwidth.

computation of the energy elements in the diagonal is mapped to the threads in a warp to explore fine-grain parallelism. element computation in the v matrix is divided in two situations based on whether or not the ith and the jth nucleotide become a pair. hence, execution path of the threads in the same warp may differ. in the proposed hybrid system, the matrix elements with the same execution path are mapped to the threads in a warp with consecutive thread id to effectively improve the parallel efficiency. 

finally, tiling method is used to accelerate computation of the vm matrix of each sequence. figure  <dig> illustrates the computation of the vm matrix tiled into small blocks. the vm matrix elements in the diagonal are parallel computed in different threads. the computing direction is moving from the principal diagonal to the top-right. the shadow area represents the already known vm and wm matrix elements . the vm in a tile block, which depends on the elements in row wm and column wm, is divided into three parts with different colors. from the already known wm matrix data , the blue portion of the vm computation is calculated ahead of time and stored into the shared memory for fast access. all blue portions of vm elements in the tile blocks  are calculated ahead of time. when the computing diagonal moves from t <dig> to t <dig>  the pre-calculated blue portion results are read directly from the shared memory to accelerate the computation of the elements in the diagonal of t <dig> 

experiments and 
RESULTS
experimental environment
in general, the hybrid accelerating system is composed of multiple cpus and gpus. for ease, the proposed prototype system for performance evaluation only consists of a host pc and a gpu card. the host in the testbed is equipped with an intel xeon e <dig> quad  <dig>  ghz cpu,  <dig> gb memory, and asus z8pe-d <dig> motherboard . geforce gtx <dig> with cuda toolkit  <dig>  is utilized as the gpu experimental platform. the zuker algorithm program rnafold is derived from the software package viennarna- <dig> . <dig>  <cit> .

cpu performance
four groups  of randomly generated rna sequences are chosen. each group consists of  <dig> sequences of same length. the average execution time per sequence is measured by calculating energy matrices of all sequences in each group when performance-tuning method is adopted gradually from o <dig> compiler optimization, sse, to multi-thread processing. the results are shown in table  <dig> 

an average  <dig> × speedup over the naive implementation on single xeon e <dig> core can be achieved in the o <dig> optimization. the sse <dig> method is awkward for the first three groups of sequences, the control penalty induced is over the original vm decomposition operations for short sequence processes. after the multi-thread processing, the highest performance has been obtained for each group.

gpu performance
after thread scheduling and tiling, the optimal average execution time for each sequence of the corresponding group is available. the average execution time for the sequence with length  <dig> on a single gtx  <dig> card is inferred as  <dig>  ms  <cit> ; however, it is  <dig>  ms on the proposed method. thus, a  <dig> × speedup can be achieved over single gtx  <dig> card implementation.

hybrid system performance
four groups of randomly generated rna sequences are tested on our hybrid system. each group contains  <dig> sequences of similar length. sequence lengths in the four groups are  <dig>   <dig>   <dig>  and  <dig> respectively. the average estimated execution time for single sequence of each group is demonstrated in tables  <dig> and  <dig>  for each group, gpu vs. cpu speedup is calculated from the average execution time on cpu and gpu. the allocation ratio is further calculated by gpu vs. cpu speedup. the results are shown in table  <dig> 

allocation scheme evaluation
hybrid system execution time measurements for different allocation ratios ranging from 2% to 30% with 2% increasing step are shown in figure  <dig>  the optimal allocation ratios for minimal execution time in the groups a, b, c, and d are approximately 4%, 4%, 6%, and 14%, respectively. the estimated allocation ratio of the groups a, b, and c are slightly far from the real optimal value, mainly because the average cpu and gpu execution time is not estimated accurately. the error is acceptable because for short sequences, the execution time is very close to that of the optimal allocation ratio. for the group d with longer sequence, the optimal allocation ratio is very close to the estimated one. it can be inferred that our allocation method based on the average cpu or gpu execution time is reasonable. in group d, over 14% of computation task is allocated for cpu processing, which utilizes the cpu processing ability a lot.

hybrid system speedup
hybrid system execution time is measured for each group. to compare with cpu-only and gpu-only implementations, execution time on the cpu and gpu platforms where all computation tasks are loaded onto these devices is also obtained. the execution time and speedup are listed in table  <dig>  for the sequences in the group of b, speedup of  <dig> × and  <dig> × over the same optimized quad-core cpu implementation is achieved for the gpu and the hybrid system respectively. speedup of over 50× over single xeon e <dig> core can also be achieved for this group. for group d, speedup of  <dig> × and  <dig> × over cpu-only implementation is achieved for both systems, showing the hybrid system to be  <dig> × faster than that of gpu-only implementation. it means that our hybrid system can have 16% performance advantage over gpu-only implementation for zuker algorithm applications for this group of sequences.

discussion
according to the results of table  <dig>  the speedup over cpu-only implementation achieved for the hybrid system for the group b is larger than the rest of the groups. it can be inferred that the the efficiency of the hybrid system can be exploited well for sequences in this group. for longer sequences, the speedup is decreasing. it is mainly because that different portions of the zuker algorithm have different computational complexity and gpu efficiency  <cit> . for longer sequences in group d, the gpu efficiency is becoming lower. the speedup of gpu over cpu in group d is  <dig> ×, lower than that of the rest of groups. we can infer that although the main idea of the proposed hybrid system is to exploit cpu computing ability, the performance of the hybrid system is still limited to that of gpu. in group d, a majority of sequences  are still processed on gpu, which is the primary computing platform in the hybrid system. the key point of the hybrid system is the task allocation between the cpu and gpu. the proposed task allocation scheme may only be used to multiple sequences with same or similar length. for these sequences, we measure the estimated average execution time of each sequence on both cpu and gpu platform to calculate the speedup of gpu over cpu. the speedup then can be used to estimated the boundary value b for allocation of tasks between cpu and gpu. according to the boundary value b, the input numbered sequences are divided into two parts which are sent to cpu and gpu for parallel processing respectively. although we only evaluated the hybrid accelerating method in the testbed with one cpu and one gpu, the proposed method can be easily employed to hardware platforms with multiple cpus and gpus. in these systems, the performance of multiple cpus and gpus must be estimated to instruct the task allocation, which will become a little complicated. take the hardware system with two cpus and gpus for example, there may be three boundary values to be calculated to determine how many sequences will be sent to multiple cpus and gpus respectively.

CONCLUSIONS
the zuker algorithm is widely used for rna secondary structure prediction. based on careful investigation of the cpu and the gpu architecture, a novel cpu-gpu hybrid accelerating system for zuker algorithm applications is proposed in the current study. performance differences of cpu and gpu in the task allocation scheme is considered to obtain the workload balance. to improve the hybrid system performance, implementations of the zuker algorithm on both the cpu and gpu platforms are optimized. the experimental results show that the hybrid accelerating system achieves a speedup factor of  <dig> × and 16% performance advantages over optimized multi-core cpu and gpu implementations respectively. moreover, more than 14% computation task is executed on cpu. the method combining cpu and gpu to accelerate the zuker algorithm is proven to be promising and can also be employed to other bioinformatics applications.

competing interests
the authors declare that they have no competing interests.

authors' contributions
guoqing lei carried out the hybrid zuker computing system, participated in the characteristics analysis of the zuker algorithm and drafted the manuscript. yong dou conceived of the study, and participated in its design and helped to draft the manuscript. wen wan participated in the analysis of original viennarna- <dig> . <dig> software package, gpu implementation and performance measurement. fei xia helped to analyze the zuker algorithm and instructed the parallel implementation. rongchun li participated in the implementation of sse instructions and correctness verification. meng ma, dan zou participated in the hybrid system implementation. all authors read and approved the final manuscript.

