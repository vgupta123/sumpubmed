BACKGROUND
nowadays, an overwhelming amount of experimental studies on gene and protein interactions are being conducted. the results of these experiments are most often described as scientific reports or articles and published in public knowledge repositories, such as medline http://www.ncbi.nlm.nih.gov/. this literature database grows at a rate of  <dig> publications per week, which makes it impossible for a human to track every new experiment performed in the field.

therefore, the need for automated information extraction methods in biomedicine becomes critical, and a lot of efforts are invested in creating such methods. recently proposed approaches for interaction extraction are based not only on explicit textual information that is contained in publications, but also on a comprehensive language analysis that includes part-of-speech  and deep syntactic structure detection. to achieve state-of-the-art performance, researchers employ lexical information  along with shallow syntactic information  and/or deep syntactic features  .

as a consequence, extraction methods tend to become more complex, use more features and require more and more memory and computational efforts. however, little attention has been devoted to studying the individual impact of different feature types. we believe that this question is of great importance, because  when two types of features have a substitute rather than a complementary effect, one of them can be dropped to obtain a computationally more efficient method, and  dropping one type of features might make the mining algorithm more robust. the latter reason is especially relevant for lexical features since lexicons tend to be subdomain-specific. this problem can be alleviated by combining different biological phenomena in one corpus; however in practice corpora are often built for a particular organism or a particular set of proteins. despite this fact, it is common practice in the field to train and evaluate systems on the same data set with an n-fold cross-validation technique, thus partially avoiding this lexicon-dissimilarity problem which is inherent to real-life problems.

in this work, we study the impact of different feature types on the performance of a relation extraction system that uses a support vector machine  classifier with kernels as its core, since at present this is the most popular choice in the relation extraction field. in particular, we use the approach suggested by kim et al.  <cit> , which relies on lexical, shallow and deep syntactic features represented as parts of a dependency tree, and consequently apply occam's razor principle by cutting off the former two to get rid of all lexical and shallow syntactic information. in other words, we would like to exploit different aspects of the dependency tree and compare the net advantage that is obtained by these feature types.

to the best of our knowledge, besides us, only  <cit>  have looked into the impact of syntactic in addition to lexical features for the protein interaction extraction task . shallow syntactic features such as pos added to a lexical feature set are reported not to increase the performance of the classifier in  <cit> , while the deep+shallow syntactic- and lexical-feature based classifier in  <cit>  showed a poor performance when the set of lexical features is limited. neither of these has however studied how much performance can be obtained by using only deep syntactic features. the closest to our work is  <cit>  which compares the performance of an interaction extraction system using only lexical features versus using syntactic  features. we highlight the difference with our work at the end of section 'related work'.

the contribution of this article is twofold. first, we perform an extensive evaluation of a recently published svm-based approach  <cit> , which was evaluated only on the lll data set before, on  <dig> data sets  using cross-validation as well as  <dig> cross-data set experiments. secondly, we compare this approach with stripped down versions which take into account different feature subsets, and we demonstrate that omitting lexical and part of the syntactic features does not significantly change the performance of the relation extraction task.

in the remainder of this paper, we first formalize the protein interaction extraction problem as a classification task  in which sentences containing protein pairs are represented by dependency trees . in section 'building a classifier', we present the various classifiers that we use in this paper, all of them modifications of  <cit> , and in section 'related work' we clarify the relationship with related methodologies. we continue with a description of our experimental setup and present the results on the different data sets in section 'results and discussion'. our final conclusions are presented in section 'conclusion'.

methods
problem statement
whereas the general interaction extraction task is concerned with finding all interactions among proteins in a given text, several assumptions are usually made to simplify it. the first assumption is that the extraction task is restricted to binary interactions, i.e., exactly two proteins are involved in the interaction. secondly, the interaction is assumed to be fully expressed in one sentence, i.e., interactions which are described across several sentences are not considered. finally, the interaction extraction task is evaluated separately from the protein name recognition task. named entity recognition  is another area of text mining, which is usually performed and evaluated separately, thus it is generally assumed that interaction extraction is performed on a text with annotated protein names .

let us consider the following sentence containing  <dig> protein names.

example 1: "in the shaa <dig> mutant, sigma2-dependent expression of spo0a <dig> and spovg <dig> at an early stage of sporulation was sensitive to external nacl."

this sentence contains  <dig> protein pairs, namely shaa-sigma, shaa-spo0a, shaa-spovg, sigma-spo0a, sigma-spovg, and spo0a-spovg. a protein pair is a positive instance if the original sentence expresses an interaction between members of this pair, and a negative instance if they just co-occur in the sentence. in the example above, there are two positive instances, namely sigma-spo0a and sigma-spovg while the other  <dig> instances are negative. as such, the task of protein interaction extraction can be treated as a classification problem, to be solved by learning a suitable decision function that can separate the positive from the negative instances.

in particular, we need to choose a formal protein pair representation and a machine learning algorithm. the protein pair representation should include information from the sentence that can be used to distinguish between positive and negative instances. different approaches use different types of information , depending on the machine learning methods used, the available tools and the researcher's strategy. although feature selection for interaction extraction has received little attention  <cit> , several researchers  <cit>  report that applying feature selection techniques significantly speeds up the processing and in some cases increases the performance of the classifier. the difference between the feature selection problem and the current approach is explained in detail in section 'related work'.

interaction representation
a dependency tree represents the syntactic structure of a sentence. the nodes of the tree are the words of the sentence, and the edges represent the dependencies between words. in a typed dependency tree, edges are labeled with syntactic roles. the dependency tree for the following sentence is depicted in figure  <dig> 

example 2: "sigma f <dig> activity regulates the processing of sigma e <dig> within the mother cell compartment."

the most relevant part of the dependency tree to collect information about the relation between the two proteins is the subtree corresponding to the shortest path between these proteins, which is shown in figure 2a. both protein names are replaced with dummy strings ne <dig> and ne <dig> in order to generalize the interaction pattern. moreover, we introduce a pos dependency tree, where nodes represent part-of-speech information instead of the corresponding words. the shortest path between the two proteins in the pos dependency tree for example  <dig> is represented in 2b. note that a dependency tree contains lexical as well as deep syntactic information, while a pos dependency tree contains shallow and deep syntactic information. we can obtain a syntactic shortest path by only retaining the syntactic roles in either the shortest path or the pos shortest path, as shown in figure 2c. figure  <dig> depicts similar information for the sentence from example  <dig> 

in the remainder we keep referring to these paths as  dependency trees. such a dependency tree can be either a lexical dependency tree , a pos dependency tree  or a syntactic dependency tree . we will use t =  to denote a dependency tree t consisting of a set of nodes n, a set of edges e, and a function l that maps nodes and edges to their labels. if there is an edge from node n <dig> to node n <dig>  we denote this edge by e.

building a classifier
our feature impact study makes use of a support vector machine  approach. svm's are a classification method that views input data as vectors in a high-dimensional space and attempts to induce a maximum margin hyperplane between training data points that belong to different classes. the crucial point is that the hyperplane can be represented as a linear combination of a subset of training instances   <cit> . moreover, the selection of the appropriate support vectors can be done by using only the inner product between training examples. hence, if the inner product can be computed efficiently, svm's can induce a classifier even in a very rich feature space. in order to deal with various kinds of input data, different strategies to compute inner products  have been proposed .

in order to build a syntactic kernel, the dependency tree space has to be kernelized. each data point of our data set is a dependency tree corresponding to a part of the sentence in which the protein pair occurs. such a tree can be represented as a vector in the m-dimensional space made up by all subtrees in the data set  <cit> . more in particular, assuming that all unique subtrees in the data set are enumerated from  <dig> to m, the function hs, s ∈ { <dig> ...,m}, is defined as the number of occurrences of subtree s in tree t. then, each tree t can be represented by a vector ϕ  = {h <dig>  h <dig> ...,hm}. a kernel function measuring the similarity between trees t <dig> and t <dig> based on whether they contain the same subtrees, is defined as the inner product   

the underlying vector representation is very rich since the number of subtrees of a tree grows exponentially with the tree size, which makes the computation of the inner product intractable. however, the right hand side of  can be interpreted as the number of common subtrees of t <dig> and t <dig>  and can be computed efficiently following a procedure proposed in  <cit> .

kim et al.  <cit>  follow this procedure in developing an svm classifier based on a kernel kfull that is a combination of a kernel klex comparing lexical dependency trees, with a kernel kpos comparing pos dependency trees. note that, because of their construction, klex relies on lexical and deep syntactic information, while kpos is based on shallow and deep syntactic features. we propose a way in which the kernel kfull can be stripped down to a kernel ks, that uses only deep syntactic features. we compare the performance of all these kernels in section 'results and discussion'. for ease of explanation, in this section we follow a bottom-up approach by first defining ks, and then extending it to the full system from  <cit> .

the trees from figures 2a and 3a have no subtrees in common, while when we switch to a shallow or pure syntactic representation in figures 2b,c and 3b,c, we have one common fragment, namely the subtree consisting only of the edge prep_of and its adjacent nodes. in general, we use a recursive formula to compute the number of common subtrees between dependency trees t <dig> and t <dig>  this formula relies on the notion of common child pairs of node n <dig> in t <dig> and node n <dig> in t <dig>  i.e. the set of pairs of nodes that have parents n <dig> and n <dig> respectively, and that are connected to these parents by the same type of edge. when traversing down the trees in search of common subtrees, these are the nodes at which we want to continue our exploration.

definition  <dig>  let t <dig> =  and t <dig> =  be dependency trees. for n <dig> ∈ n <dig> and n <dig> ∈ n <dig>  the set of common child pairs is defined as com = {|  ∈ n <dig> × n <dig>  e ∈ e <dig>  e ∈ e <dig>  l1) = l2)}.

definition  <dig>  let t <dig> =  and t <dig> =  be dependency trees. for n <dig> ∈ n <dig> and n <dig> ∈ n <dig>  the number of common subtrees rooted at n <dig> and n <dig> is defined as  

the recursive formula reflects the fact that a new common subtree rooted at n <dig> and n <dig> can be found either by picking  <dig> of the cm subtrees or by adding the x/y nodes, or just by staying as is .  <dig> is subtracted from the whole result to exclude the combination with the tree consisting of the n1/n <dig> node only.

example  <dig>  let t <dig> and t <dig> be the dependency trees from figure 2a and 3a respectively. com is the empty set for all node pairs with exception of com = {}. hence cm =  + 2) - <dig> =  <dig> while cm =  <dig> for all other node pairs. this means that there is only one common subtree between t <dig> and t <dig>  rooted at the processing, expression nodes and ending at ne <dig> 

note that the calculation above of the number of common subtrees disregards node labels, i.e., it treats dependency trees as they are shown in figure 2c and figure 3c. using definition  <dig> we are now able to define a kernel ks that looks only at deep syntactic information. it computes the similarity between syntactic dependency trees as the number of grammatical structures that they have in common.

definition  <dig>  the kernel function ks is defined as   

for syntactic dependency trees t <dig> and t <dig> 

example  <dig>  let t <dig> and t <dig> be the syntactic dependency trees from figure 2c and figure 3c respectively. since |n1| =  <dig> and |n2| =  <dig>  the summation in the right hand side of  consists of  <dig> terms. in example  <dig> we already established that all of these terms are  <dig> with the exception of one term that equals  <dig>  hence ks =  <dig> 

to arrive at kernels that take into account additional lexical and/or shallow syntactic information, we need an extended version of definition  <dig> that also looks at the labels of nodes.

definition  <dig>  let t <dig> =  and t <dig> =  be dependency trees. for n <dig> ∈ n <dig> and n <dig> ∈ n <dig>  the set of common child pairs, taking into account the labels of the nodes, is defined as comlab = {| ∈ com, l <dig> = l <dig>  l <dig> = l2}.

the superscript "lab" refers to the fact that the labels of the nodes are taken into account. the appearance of com in the definition of comlab illustrates that the latter builds on the former. furthermore, it holds that  

indicating that using syntactic trees leads to a more general approach .

the number cmlab of common subtrees rooted at n <dig> and n <dig>  can now be defined in a recursive manner entirely analogous to definition  <dig>  however relying on comlab instead of on com. since they have different labels at the nodes, the value of cmlab might be different depending on whether a lexical dependency tree or a pos dependency tree is used. in both cases, it holds however that   

example  <dig>  let t <dig> and t <dig> be the lexical dependency trees from figure 2a and figure 3a respectively. for all node pairs it holds that comlab = ∅, and cmlab =  <dig> 

example  <dig>  let t <dig> and t <dig> be the pos dependency trees from figure 2b and figure 3b respectively. it holds that comlab = {} and cmlab =  <dig>  while for all other node pairs comlab = ∅ and cmlab =  <dig> 

the potentially different behavior of cmlab on lexical dependency trees and pos dependency trees gives rise to the definitions of the kernel functions klex and kpos respectively. both of them still consider the tree structure when computing the similarity between trees, i.e. they both rely on deep syntactic information. in addition, klex takes the actual words of the sentence into account  while kpos considers pos .

definition  <dig>   <cit>  the kernel function klex is defined as   

for lexical dependency trees t <dig> and t <dig>  in our case function l maps words in the tree nodes to corresponding lemmas eliminating the differences arising from different word forms.

definition  <dig>   <cit>  the kernel function kpos is defined as  

for pos dependency trees t <dig> and t <dig> 

finally, kim et al.  <cit>  combine klex and kpos into a kernel kfull that takes into account lexical, shallow and deep syntactic information.

definition  <dig>   <cit>  the kernel kfull is defined as   

for dependency trees t <dig> and t <dig> and their corresponding pos dependency trees  and .

notice that klex is a refinement of ks in the sense that all the information used by ks is also used in the same way by klex. as a consequence, kfull is also a refinement of ks, enriching the deep syntactic information of ks by lexical information  as well as shallow syntactic information .

example  <dig>  let t <dig> and t <dig> be the lexical dependency trees from figure 2a and figure 3a respectively, and  and  their corresponding pos dependency trees from figure 2b and figure 3b respectively. one can verify that  

and  

hence kfull =  <dig>  notice that although the trees do not show any resemblance on the lexical level, their similarity at the more general syntactic level is picked up by kpos. in example  <dig> we found that their syntactic similarity is also already reflected by ks.

a short summary of the kernels described above is provided in table  <dig> 

related work
in table  <dig>  an overview of recent approaches to interaction extraction is presented along with the characteristics that are relevant in the context of our work. below we describe these approaches in more detail.

many approaches exploit the idea of using explicit feature vectors to represent a possible interaction. in particular, approaches based on various combinations of lexical features are very popular in the relation extraction community.

bunescu et al.  <cit>  propose to use the sentence context, obtained by splitting a sentence into three parts, i.e. before the first protein, between the two proteins, and after the second protein, and they combine them in predefined ways to obtain  <dig> types of patterns. using this information, the authors propose a kernel that naturally emerges from the subsequence kernel described in  <cit>  and obtain good results on the aimed corpus. giuliano et al.  <cit>  start from the same pattern types, but treat them as bags-of-words, and define a global context kernel. moreover, they define a local context kernel by taking a window of predefined size around the candidate proteins and adding more shallow linguistic information, such as the lemma of the word and some orthographic features. the resulting kernel function in this case is a linear combination of the global context kernel and the local context kernel. their method obtains state-of-the-art results on the aimed and lll data sets.

some researchers focus on sentence structure, i.e., on the parse and dependency tree, to construct a feature vector. xiao et al.  <cit>  study the impact of features, starting with simple words up to parse and dependency trees on the iepa corpus, and they obtain a remarkable  <dig> % f-score using a maximum entropy model with lexical and shallow syntactic features. yakushiji et al.  <cit>  suggest that full parsing information could be very useful in the biology domain because the distance between entities in a sentence can be much longer than in general-purpose domains. therefore, they propose a method that builds complex predicate-argument structures , and apply an svm with an rbf kernel to these patterns to obtain a classifier model. they evaluate this model on the aimed data set and obtain a  <dig> % f-score. in  <cit> , the authors also focus on sentence structure and use dependency trees to extract the local contexts of the protein names, the root verbs of the sentence, and the parent of the protein nodes in the dependency tree. classification is further done by bayesnet and ensemble classifiers. another approach is proposed in  <cit> , where a manually constructed set of rules uses information from the dependency trees and a predefined vocabulary to classify possible interaction instances. this approach is evaluated on the hprd <dig> and lll data sets, as well as on a large-scale data set consisting of  <dig> million medline abstracts. the extracted set of interactions contained 40% of the hprd interaction database.

in our own previous work  <cit> , we proposed to abstract from lexical features and use only syntactic information to obtain a more general classifier that would be suitable for different data sets without retraining. we used features extracted from dependency and parse trees to build decision trees and bayesnet classifiers, and obtained promising results using aimed as test data and lll as training data. another group of approaches does not rely on an explicit feature vector but rather makes use of structured data as input information for the classifier. this means that structured features, such as dependency trees, can be used as an input to the classifier without any additional transformations, thus reducing the risk of losing useful information. one particular way to use structured features that we adhere to in the current paper, is to exploit structured kernels.

the approaches  <cit>  are closest to our work as they also use structured kernels. structured or convolution kernels were introduced in  <cit>  by haussler who proposed how to compute a kernel for structured objects. this work gave rise to many tree kernel methods in the text mining domain. although this idea is quite popular in general text mining, it has not been widely explored in the interaction extraction literature.

saetre et al.  <cit>  apply a structured kernel to the protein-protein interaction domain. in this approach, a mix of at and structured features is used to calculate the similarity of two protein pairs. the flat part of the feature vector contains lexical features, while the structured part is a shortest path dependency tree, referred to as a partial tree. this definition was introduced by moschitti  <cit>  who studied different tree partitioning strategies and their impact on tree kernels for dependency and parse trees. using these fatures, saetre et al. obtain promising results on the aimed data set, especially in combination with a rich lexical feature set.

in another very recent approach, described in  <cit> , the authors propose to use the whole dependency tree to build a classifier. they use a graph kernel that takes into account all paths in dependency trees, and exploit it with an rls  machine learning method. the experimental evaluation is performed on  <dig> data sets: aimed, bioinfer, hprd <dig>  iepa and lll, and for all of them, the method shows remarkably good results.

collins and duffy  <cit>  developed a tree kernel that counts the number of common subtrees, but used it for parsing and not for interaction extraction. kim et al.  <cit>  apply this kernel to a dependency tree and to a modified dependency tree with pos instead of words, and propose a combined kernel, which is a sum of these two. this approach obtains state-of-the-art performance on the lll data set. in the same paper they describe a flat feature vector-based approach that also utilizes dependency trees to extract graph walks as features. in  <cit> , the authors study the relative feature importance for the latter approach by using the gain ratio feature selection technique. moreover, they study the impact of different feature types as well by comparing the performance of methods that use syntactic features versus methods that use lexical features. our approach is also based on kim's work, however it is different from  <cit>  in several aspects. first of all, our approach is different from the feature selection task, because we focus on the type of the information  rather than on separate features. in other words, we do not use feature selection techniques to discriminate useful individual features, but fit an existing relation extraction method to consider only a subset of features of a certain type, and study the impact of this feature class. secondly, when we study the impact of different feature types we do not rely on a flat vector, but on a structured representation. moreover, we use an additional data set and define more extensive experimental setups in order to perform a complete study of different use cases.

RESULTS
data sets
to the best of our knowledge the only publicly available data sets that contain protein interaction annotations are: aimed  <cit> , bioinfer  <cit> , hprd <dig>  <cit> , lll  <cit>  and iepa  <cit> . these data sets have been frequently used in recent work of for example  <cit> ; therefore we use them in our current work. table  <dig> gives an overview of the number of positive and negative instances in the different data sets.

the aimed data set consists of  <dig> abstracts extracted from the database of interaction proteins ,  <dig> of which contain annotated human gene and protein interactions. another  <dig> abstracts contain protein names but do not describe any interactions. we have used only the former set of abstracts for our evaluation purposes.

the bioinfer data set is the largest data set among these 5; it contains  <dig> sentences describing protein-protein interactions. beside the interaction annotations, bioinfer contains additional information about biological interaction type, protein roles in interaction, syntactic dependencies between words, etc. moreover, there is a knowledge base behind the corpus, which allows to analyse it in more detail . hprd <dig> contains sentences that were extracted from a subset of  <dig> abstracts, referenced by the human protein reference database  and annotated with protein names and interactions between them. the lll data set consists of  <dig> sentences describing interactions concerning bacillus subtilis transcription. protein roles for interactions are annotated along with the interactions themselves. additionally, the data set contains annotations for lemmas and syntactic dependencies between the words in the sentences. finally, the iepa data set was built by querying medline with  <dig> diverse queries, reflecting  <dig> different biological topics.  <dig> abstracts were retrieved, and a data set was constructed with sentences extracted from these abstracts. the data set annotation includes an interacting verb along with the protein names and interactions.

the bioinfer and lll data sets provide syntactic dependencies for every sentence in their own formats, while the other data sets do not provide this information. we discarded this information to unify the setup and to make the experiment more realistic. besides being non-standard, some of the syntactic information in bioinfer and lll was obtained manually which violates the requirements of automatic processing. to obtain pos and dependency trees for all data sets we used the stanford parser  <cit>  trained on general purpose corpora. we choose this parser because of its peculiar annotation scheme that stresses the semantic connections between words rather than operating on the purely syntactic level. for example, prepositions are collapsed as can be seen in figure 2a, where the noun processing is being connected directly to a protein name. as we use a dependency tree representation to obtain all three types of features, we do not use an external pos tagger to obtain pos tags separately; instead they are assigned as part of the dependency tree building process inside the stanford parser. as the stanford parser does not provide lemmatized versions of words, we used the porter stemmer algorithm  <cit>  to compute klex and kfull. all data sets use different annotation schemes that emphasize different interaction properties. for example, in aimed homodimeric proteins are being annotated, i.e. proteins that interact with themselves, while the current mining approach is not able to detect such cases. moreover, in bioinfer some proteins have gaps in annotations, i.e. there is a gap between two parts of one protein name. we handle these cases separately, but they can potentially decrease the performance of the classifier as well. the quality of the annotation itself  may affect the quality of classifier. if an annotator misses an interaction between two proteins, the data point related to this protein pair would be treated as a negative instance, although containing an interaction pattern, which is harmful to the overall performance. to unify an experimental setup, we need to cast all corpora to a common ground format. pyysalo et al.  <cit>  designed custom software that converts all  <dig> data sets to a single xml format that contains only minimal protein and interaction annotations, which is sufficient for our evaluation purposes. however, not all annotation differences can be eliminated in this way. table  <dig> shows that different data sets have very different positive/negative ratios. this can be partially explained by different annotation strategies, e.g. for lll only proteins that are involved in interactions are annotated, while for other data sets all protein names are annotated. since we consider every possible protein pair within a sentence to be an instance, this leads to an exponential growth of the total number of instances, while in fact the number of positive instances remains the same. taking into account this information, we should choose our evaluation metrics carefully in order to provide a fair comparison of the performance on all data sets.

performance metrics
in this work, we use two evaluation metrics, namely recall-precision and roc  curves, to provide a more comprehensive analysis of the achieved results. let us first recall their definitions. let tp denote the number of true positives, i.e., the number of positive instances that are classified as such, let fp denote the number of false positives, i.e., the number of negative instances that are incorrectly classified as positive, and analogously, let tn and fn stand for the number of true negatives and false negatives respectively. the following metrics can then be defined:  

recall stands for the fraction of correctly classified instances  among all positive instances  in a data set, while precision denotes the fraction of correctly classified instances  among all instances that are classified as positive . recall is sometimes called true positive rate, while false positive rate counts how many of the negative instances were wrongly classified as positive. a combined measure, that takes into account both recall and precision is called f-score and defined as:  

often, a classifier's output can be ordered, i.e. the classifier also provides a degree of confidence for each prediction it makes. in this case, we can trade precision for a higher recall by lowering the confidence threshold to capture more positive instances. in this way we can build a recall-precision curve that shows the relationship between the quality of extracted relations  and the amount of extracted relations . the closer to the top-right corner a curve is, the less precision is lost with recall growth and the better the performance of the classifier is.

precision, recall and f-score are de-facto standards for the interaction extraction evaluation. however, these metrics are very sensitive to data set skewedness, i.e., the large difference between the number of positive and negative instances. as was shown in table  <dig>  this difference varies greatly for different corpora. on the other hand, roc curves are being used in the machine learning community to evaluate classifier performance and they do not depend on data set skewedness.

the false positive rate together with the true positive rate correspond to a point in roc space. by varying the trade-off between these two metrics we obtain a curve in roc space. the auc-score is the area under this roc-curve. it can be interpreted as the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance.

however, this metric should be used carefully for the same reason, i.e. it is suitable to evaluate the relative quality of a classifier , but it gives no information about precision, and thus makes the evaluation of a classifier difficult.

for example, if we increase the number of negatives  <dig> times, then the number of fp on average increases  <dig> times as well. this will lead to a significant drop in precision and consequently in f-score, but it does not influence the false positive rate.  

based on this observation, we can outline an application area for both evaluation metrics. the roc curve and the corresponding auc value should be used to compare the performance of a classifier on different corpora, since they show the relative number of extracted positive instances. recall-precision and f-score can be used to compare the quality of several classifiers on the same data set, since they indicate how 'clean' the classification is without regarding the proportion of negative instances.

experimental setup
for the experiments, we used the libsvm library  <cit>  to build the  <dig> svm classifiers that use the kfull kernel, kpos kernel, ks kernel and klex kernel. furthermore, we organized  <dig> experimental setups. the first setup uses 10-fold cross-validation , where each data set is split into  <dig> parts, of which  <dig> are used for training and one for testing. despite the fact that this is the most common way of evaluation, it should be used carefully. since we work on instance level, it can be the case that two nearly identical instances from the same sentence fall into a train and a test fold at the same time. this 'leak' can cause a performance boost as it was shown in  <cit> .

in the second setup  we join  <dig> data sets to form a training set, and use the remaining one as a test set. compared to cv, this alternative experimental setup is closer to a real world situation where information for processing is obtained from different sources and the lexicon is not as uniform as in one precompiled data set.

in the last setup  we use  <dig> data set as training set and the remaining  <dig> as test sets, thus making another step to the real world. typically, biologists have a very limited amount of annotated data compared to the size of available unlabeled information. we try to model this situation here by making the training set much smaller than the test set.

for each experimental setup we run all classifiers with all data set combinations. an analysis of the results obtained is provided in the following section.

discussion
precision plays a particularly important role in the interaction extraction task, because if the extracted information is processed by a biologist, she would not like the system if it produces too much rubbish. therefore, we are particularly interested in the left side of the recall-precision chart, where precision is typically high, although recall may be quite low.

the cross-validation setup reveals no clear leader for all data sets. for the lll data set, the syntactic kernel shows the best performance . that can be explained by the fact that the lll data set is very small and contains relatively short hand-picked sentences with a simple syntactic structure. however, experiments with other data sets show that the lexical kernel gives the best results for the hprd <dig> and bioinfer data sets . in the case of bioinfer, this can be explained by the fact that the training set size is large enough to discriminate useful lexical features. for iepa, the full kernel, i.e. lexical+shallow, performs best, while the lexical kernel shows the worst result , and for the aimed data set the syntactic kernel shows better results for small recall values . the predictive power of deep syntactic features by themselves is very interesting, given that the lexical and lexical+shallow methods in theory can take additional advantage of the lexicon similarity within the same data set that is caused by the nature of the cross-validation set up.

when we train on  <dig> data sets and test on  <dig>  the lexical+shallow kernel is among the best for all but the bioinfer data set. figure 4d shows that the lexical kernel outperforms the others on bioinfer for small recall values. a significantly better performance of the lexical kernel for small recall values can be interpreted as a sign of overfitting, i.e. a classifier with a lexical kernel produces too specific patterns, which causes a successful classification of several instances, but is followed by a significant precision drop due to the unability to generalize over less frequent cases. on the other hand, other classifiers avoid overfitting and a steep precision drop, but at the cost of missing some very reliable patterns. moreover, the lexical kernel shows a performance similar to the lexical+shallow kernel for hprd <dig> and aimed , but fails on iepa . on the other hand, the syntactic kernel performs good on the iepa and lll data sets , but is not that good on others.

although with the 1- <dig> experimental setup there is no best kernel either, we can still observe some interesting patterns. the lexical kernel shows a significantly better performance on the aimed and hprd <dig> data set for small recall values , while the syntactic kernel performs best for the lll and iepa data sets on the whole curve . as it is shown on figure 4e, training the classifier on lll causes extreme curve shapes caused by the significant difference in size between the training set and the test data set. the first instances for the lexical+shallow and the shallow kernels were classified correctly, but further precision drops dramatically. after  <dig>  recall value, the lexical kernel basically neglects all positive instances, and the curve shows simply the percentage of positive instances in the data set. other kernels perform slightly better and the syntactic kernel is able to consistently outperform others. this can be explained by the fact that  <dig> sentences  is definitely not enough to train a classifier. moreover, it shows that in the case of training information shortage the syntactic kernel can offer a better solution than others.

the last two experiments illustrate the case when the vocabulary of train and test data sets differ, which is often the case in the real world. in the former case the training set is large enough to successfully train the lexical+shallow kernel, making the difference in the vocabularies not so crucial. however, in the latter case, when the training set is much smaller than the test set  we can clearly see the influence of this fact on the performance difference between syntactic and lexical methods.

from the experiments above we can observe the following trends:

• lexical and combined methods are able to build better generalizations  and thus perform better with large  training sets

• syntactic methods are able to achieve better results than lexical ones when the training set is small in comparison with the test set

moreover, there seems to be a correlation between better performing kernels and data sets. for example, the syntactic kernel always obtains good results on the lll and iepa data sets, while the lexical+shallow kernel performs well for the bioinfer data set. moreover, the lexical kernel is always on top for the hprd <dig> data set. these observations show that the data set origin and properties  have a strong influence on classifier performance.

compiling roc curves for one method on one chart allows us to analyze the robustness of this method on different data sets. in figure  <dig> each chart displays roc curves for one method for all experimental setups. the less spread the curves are in the roc space, the more predictable the performance of the method is.

in most cases, the lll cross-validation setup is out of the trend, because of its small size and density. otherwise, the shallow  and syntactic  kernels exhibit more or less coherent behavior for all setups for the given data sets. the lexical+shallow kernel  shows some spread, but again mostly due to the lll data set's based setups, and the lexical kernel  proves to be the most unpredictable.

CONCLUSIONS
in this paper we examined different structured kernels with svm's to study the impact of different features on the relation extraction process. we took four kernels that reflect different degrees of using syntactic and lexical information and performed three types of experiments to study the behaviour of these methods under different conditions. we performed our experiments on five benchmark data sets, being aimed, bioinfer, iepa, hprd <dig> and lll.

the most important observation is that by using only grammatical relations  we can obtain a similar performance as with an extended feature set . this indicates the relative importance of grammatical information for the interaction extraction task. another finding is the correlation between training/test set sizes and the method choice. we observed that when the training set is much smaller than the test set, then the syntactic kernel performs better. this might be explained by the fact that there are too few instances to induce useful lexical features, whereas syntactic features require less instances to produce better results.

when the training set grows, the performance of the full kernel becomes better, and when the training data set is larger than the test set , the full kernel outperforms all other kernels. from the stability point of view , we can conclude that the syntactic kernel provides the best results, whereas the lexical kernel provides the worst results. the question of how different features within one feature type affect the quality of classification still remains open and represents an interesting direction for future work.

we believe that these findings can be helpful in building faster and less complicated classifiers, as well as for choosing a proper kernel according to the data set at hand.

list of abbreviations
abbreviations occured in figures: nsubj: nominal subject; dobj: direct object; nn: noun phrase; prep_*: preposition ; det: determiner; amod: adjectival modifier; vbz: verb, 3rd person singular present; nn: noun, singular or mass; ne1: 1st protein name; ne2: 2nd protein name.

authors' contributions
the results reported on in this paper are part of the phd research of the main author, tf. mdc, cc and vh contributed as his phd supervisors. all authors read and approved the document.

