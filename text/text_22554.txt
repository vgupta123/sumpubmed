BACKGROUND
genetic studies are challenged with identifying and characterizing the underlying genetic etiology of common, complex human diseases. recently, genomewide-association studies  have contributed an abundance of well-replicated findings that have identified regions of the genome likely to harbor disease genes . the current limitation is the ability to move from these initial association signals to identification of the underlying critical variants. analytical approaches that consider haplotypes will be useful to guide the mapping of underlying variants, in particular rare variants  <cit> . furthermore, multi-center collaborative efforts and use of resources enriched for genetic disease will be helpful in the effort to identify underlying variants. potentially powerful family-based resources already exist for many diseases, such as those previously ascertained for linkage studies. the ability to utilize these family-based resources for haplotype association studies and combine family-based and singleton resources for joint analyses would be extremely valuable. such analyses, however, present complex statistical challenges, such as haplotype inference and accounting for phase uncertainty in family data and the identification of appropriate statistics. here we present a monte-carlo method, hapmc, designed to perform valid haplotype association analyses in mixed resources.

a significant issue for use of haplotypes in association analyses is the estimation of phase conditional on the observed genotype data. in population-based data of independent individuals, haplotype frequencies can be estimated using bayesian methods  <cit> ) or expectation-maximization  approaches  <cit> . such methods are well established and are scalable to thousands of markers with thousands of subjects. for family-based data, extensive work has been done to properly estimate haplotypes , but there are still considerable limitations with regard to missing data and large pedigrees. linkage analysis software, such as genehunter  <cit> , merlin  <cit>  and simwalk <dig>  <cit> , can phase snp data in pedigrees, however, these software either require that markers are in linkage equilibrium or cluster the markers for analysis, conditions not suited to the situation we are interested in here. thomas   <cit>  developed a markov chain monte carlo  linkage method that considers markers in linkage disequilibrium  in general pedigrees. however, the method remains impractical for large pedigrees due to mixing problems and high computational burden. other efforts have focused on phasing tightly-linked markers in small nuclear families and moderate sized general pedigrees  <cit> . by focusing on markers within minimum- or zero-recombinant regions these methods reduce the complexity of the haplotype reconstruction problem. additional reductions in the haplotype configuration space are made by minimizing haplotype ambiguities and missing data with rules based on mendelian inheritance  <cit>  and genotype elimination  <cit> . concentration on regions with minimal recombination is reasonable to address specific regions, such as candidate genes or follow-up regions identified from gwas. however, the attention to only small to moderate pedigrees remains restrictive. thus far, no method has integrated haplotyping strategies for snps in ld for larger pedigree structures with missing data.

given any chosen method for estimating haplotypes, the uncertainty from this estimation must be accounted for at the analysis stage. for independent individuals in a classical case-control design, a likelihood approach is the usual solution, which allows consideration of all possible haplotype pairs for each individual, each weighted by the appropriate probability. for small families and transmission-disequilibrium statistics this also has been dealt with in a variety of valid ways  <cit> . three published approaches have attempted to extend haplotype association analyses to large pedigrees and allow for combination of pedigrees and singleton data. the first approach uses a weighting scheme to account for correlation between related cases. it is limited in its requirement for independent controls when using pedigree cases and only conducts a global haplotype likelihood ratio test  <cit> . the second approach is an extension of a full likelihood approach for combining nuclear family and singleton data  <cit> . the extension to general pedigrees is by splitting these into nuclear family components and treating these components as-if independent, which can lead to invalid tests. the third is a monte carlo  approach, proposed for single marker analyses, but with some restrictive opportunities for haplotype analyses  <cit> . in this mc method, to perform haplotype analyses population haplotype frequencies and phase-known observed data must be provided by the user. haplotype inference programs can be used to provide population haplotype frequencies, because, even with related individuals, point estimates of the haplotype frequencies are unbiased for zero-recombinant regions  <cit> . phase-known observed data, however, is not a realistic condition, and treating estimated haplotypes as-if phase-known is not valid  <cit> . certainly, mc methods can be valid for association testing provided the mc procedure is performed appropriately  <cit> , however, a more sophisticated approach is required than for single marker analyses.

beyond haplotype inference and uncertainty, to perform association in pedigrees attention must be made to the controls utilized in the family data. in particular, the parents of affected offspring are intuitively not ideal for explicit use as controls because they must share exactly one allele with the affected individual. previously, "pseudocontrols" have been suggested for family data where parents are available  <cit> . pseudocontrols for an affected offspring can be generated from the parental alleles or haplotypes not transmitted to the affected offspring. methods have been developed to generate up to three pseudocontrols per case to perform a matched case/control analysis and provide statistics robust to population stratification  <cit> . the use of pseudocontrols may offer more power for classical association tests in family-based resources. data for pseudocontrols can be used in the usual association statistics, thus also providing an easy way to combine association evidence across family and case-control data - an important consideration for joint analyses in mixed resource structures.

here we introduce an mc approach for haplotype association analyses, hapmc, which allows for valid analyses in large pedigrees and resources of mixed structure. our method incorporates a general em phasing method that estimates phase considering pedigree structure for a set of tightly linked markers in a non-recombinant region. our phasing algorithm builds upon previous methods by providing a pedigree-splitting preprocessing step, a set of simplified rules optimized for snp markers  <cit> , and an integrated genotype elimination procedure in haplotype configuration construction. valid haplotype association testing is achieved using an appropriate mc procedure, and includes full length and sub-haplotypes analyses, allowing for imputation of missing data based on the complete marker set. this new approach also allows for the use of either explicit or pseudocontrols in family data. hapmc is implemented in a java software package, which is incorporated as a module in the freely available genie software suite .

RESULTS
phasing comparison
the haplotype phasing accuracy and timing results using our pedigree-informed algorithm, the pedigree-informed algorithm haplore  <cit>  and gchap  are shown in table  <dig>  phasing accuracy was determined by the percentage of correct mle haplotypes across all individuals. as expected, for the independent case-control data, all three algorithms produced reasonably similar accuracy results. both pedigree-informed algorithms were marginally better  than gchap for longer haplotypes  due to their partition-ligation procedures; however, these marginal increases in accuracy come at the expense of increased computing time. as expected for an algorithm that is pedigree-naive, the accuracy of gchap remained similar across all data sets, independent of the changing pedigree structures.

*new pedigree-informed phasing algorithm

‡haplore 

† gchap 

cc :  <dig> cases,  <dig> controls =  <dig> individuals 

trio :  <dig> trios =  <dig> total individuals 

asp :  <dig> asps =  <dig> total individuals 

lp <dig>  =  <dig> generational pedigree ~ <dig> total individuals 

- program failed.

for the data sets that included pedigree structure , the pedigree-informed algorithms achieved significantly greater accuracy than gchap for all loci lengths and missing rates. the accuracy of both pedigree-informed algorithms continued to be similar in all situations where both algorithms completed the phasing, with our new algorithm consistently, if only marginally, the better of the two. our new algorithm was also able to phase all data sets and scenarios generated. however, for certain scenarios with  <dig> loci  haplore was unable to completely phase the data due to a configuration error, which was most likely due to the inappropriate removal of a critical haplotype from a partition. haplore could also not be performed for the lp <dig> data set because it was unable to process these large pedigree data sets in a tractable amount of time. for longer haplotypes and high missing rates the improvements made by the pedigree-informed algorithms were substantial . the increased accuracy of pedigree-informed algorithms with family-based data is perhaps expected given the nature of the two approaches. yet, the large differences in accuracies between the two types of algorithms highlights the importance of accounting for the family structure information, particularly for analyses of larger number of loci and higher rates of missing data.

phasing time for all algorithms increased with the number of loci considered and increased missing rates, as expected. for gchap, the phasing time increased with the number of subjects, but this increase was independent of pedigree structure. the phasing times for the cc and asp data sets  were similar and phasing times for trio and lp <dig> data sets  were also similar. for the pedigree-informed algorithms, the number of subjects and the pedigree structure influenced the phasing time. for the cc and asp data sets, the number of subjects is the same , however the pedigree structure in the  <dig> asps significantly reduced the haplotype configuration space, hence, phasing time is significantly reduced in the asp data set. for example, for  <dig> loci, 10% missing, cc time is  <dig>  and  <dig>  seconds whereas asp time is  <dig>  and  <dig>  seconds for our algorithm and haplore, respectively. the trio and lp <dig> data sets both contained  <dig>  individuals, however, the relationship between structure and phasing time is less straightforward for this comparison. the lp <dig> data set has more overall structure between a larger number genotyped individuals , but the pattern of the structure is more complex. conversely the trio data set has less structure between total subjects , but a uniform structure across smaller units. for 0% and 5% missing data, the computing time for both trio and lp <dig> data sets were relatively similar. however, for 10% and 15% missing data , the larger amount of structural information in the lp <dig> data set appeared to shorten the phasing time compared to the trio data.

comparisons of run times between haplore and our new algorithm for asp and trio data sets show that haplore was faster for  <dig> loci, but our new algorithm was faster for  <dig> and  <dig> loci, and substantially faster for many situations with  <dig> loci. both pedigree-informed algorithms scaled poorly compared to gchap when considering data sets with no or low pedigree structure , especially with larger numbers of markers and missing data. haplore was markedly faster than our algorithm with  <dig> loci and high missing rates for cc. for example, the new algorithm was one and two orders of magnitude slower than haplore and gchap, respectively, for  <dig> loci and 15% missing genotype rate.

power and validity
power and validity results for analyses using the simulated family and independent case-control data sets, as well as the mixed designs are shown in tables  <dig> and  <dig>  all analyses were haplotype specific tests for the known risk haplotype and were tested at the α-level of  <dig> . table  <dig> shows the power and type i error rates for each data set, including results from explicit controls  and pseudocontrols  in a standard cochran-armitage test for trend. in addition, the tdt was performed for the trio and asp data sets. table  <dig> shows the type i error rates and power for mixed resources including mixtures of two data sets.

p-values between  for  <dig> replicates are consistent with a valid  <dig>  type  <dig> error rate.

ec = explicit controls, pc = pseudocontrols, tdt = transmission disequilibrium test

* =  <dig> cases,  <dig> controls

# =  <dig> cases,  <dig>  controls

† power is shown for the hapmc with the pedigree-informed mle estimation

na = pedigree informed phasing not applicable to case-control.

results from most powerful statistic for each single resource analyses and mixed resource analyses. all controls within lp <dig> and lp <dig> are familial controls.

* =  <dig> cases,  <dig> controls

† =  <dig> cases,  <dig>  controls

† power is shown for the hapmc with the pedigree-informed mle estimation

na = pedigree informed phasing not applicable to case-control null.

based on  <dig>  replicates, all type i error rates were found to be not significantly different than  <dig>  , indicating validity of all tests within the mc framework both for hapmc using the pedigree-naïve and pedigree-informed mles. primarily, these results demonstrate the versatility and potential for hapmc to perform valid analyses on mixed structure study designs.

for the trio and asp data sets, we performed trend tests using ec and pc designs and also a tdt analysis. in general, the asp data set exhibited more power than the trio data set even though it had a smaller overall sample size , presumably due to the enrichment of disease alleles in the asp set. the exception was at the low genotypic relative risk of  <dig> , where the increased sample size of the trio design appears to have out-weighed the minor genetic enrichment of the asps at this small risk size. within both data sets power was observed to be quite similar across the three analysis approaches for all alternative models. however, both pc and tdt statistics showed consistently higher power than the ec, although these gains were extremely marginal . formal testing of the differences between the pc and ec approaches using a wilcoxon signed-rank  test provided evidence for significant differences  indicating consistent marginal power gains when using pc compared to ec in these designs. the power from the pc and tdt statistics differed by no more than 1% in the trio data set and by less than  <dig> % in the asp data set and were not statistically different .

for the lp data sets we also compared the ec and pc approaches. in contrast to the trio and asp data sets, the pc approach in large pedigrees involves a mixture of pseudo and explicit controls according to the pedigree structure. if both parents of an affected case are genotyped and are unaffected, then a pseudocontrol is generated from their data and used in place of their explicit data, otherwise controls are considered explicitly. in the lp <dig> data set, power differences between the ec and pc approaches were marginal across all models and neither approach was consistently better than the other. the largest difference between the two was  <dig> % for the disease model with a risk haplotype of  <dig>  and a grr of  <dig> . formal testing indicated that there was no evidence that one approach was consistently superior to the other . however, in the lp <dig> data set, the ec approach most often gave marginally more power  with an average increase of  <dig> %. this consistent marginal increase was significant .

examining power across all resource designs shows that the lp <dig> data set, matched in sample size to the trio data set, was consistently more powerful than the trio data set for all risk haplotype frequencies and grrs . the maximum difference in power of  <dig> % between the two data sets is seen with the  <dig>  risk haplotype frequency with  <dig>  grr. furthermore, even though the other three designs  had smaller sample sizes, these designs also out-performed the tdt design for the majority  of models. the lp <dig>  matched in sample size to the cc and asp data sets, performed comparably to the cc or asp data sets . an increase in power of the lp <dig> compared to lp <dig> was evident  consistent with the increase sample size of lp <dig> which contains twice as many controls.

for the mixed nuclear family and case-control designs that include trios and asps , the superiority of the pc approach reflected the observations for the single data set results . in the mixed large pedigree and case-control data sets, the pc approach outperformed the ec approach in one set , but not in the other . given the superiority of pc to ec in all but lp2cc and the marginal nature of the individual differences, only the pc results are detailed in table  <dig> 

as expected, power was always increased in the joint, two-data-set mixed resources compared to either single data set. furthermore, and previously shown by others  <cit> , the power of the joint analysis in the mixed resource was always superior to the power of analyzing both resources separately ×}, where p <dig> is the power for the first data set and p <dig> is the power of the second).

discussion
here we have described a mc, mle-based haplotype association method and software  designed to analyze a set of tightly-linked snps in general pedigree and/or independent case-control based studies. hapmc allows for the haplotype mle to be estimated by either a pedigree-naïve or pedigree-informed algorithm. a novel aspect of our method comes from the implementation of the pedigree-informed general phasing algorithm that appropriately handles related and unrelated individuals into the haplotype phasing. a variety of pedigree-informed phasing algorithms currently exist , but none have established practical measures for dealing with large amounts of missing data in extended pedigrees and directly integrated these for haplotype association testing. our algorithm includes a preprocessing step to optimally split large pedigrees into substructures, which enables it to consider important pedigree structure that surrounds dense genotype data and maintain tractability. while this step may appear trivial, it is a necessary step to analyze large pedigrees with missing data that current phasing programs cannot handle. our new approach includes both incorporation of pedigree structure and a partition-ligation in the haplotype estimation procedure.

we found that the accuracy of haplotypes estimated from our pedigree-informed algorithm was always equal or superior to that estimated without these algorithm improvements. even in the situations where there was little or no pedigree structure , the new algorithm performs substantially better due to the partition-ligation alone. however, a notable issue from our investigations is that while the new pedigree-informed algorithm always results in greater accuracy than a pedigree-naïve approach, the phasing times are orders of magnitude longer for data sets with little or no pedigree structure, large number of loci  and high missing rates . hence, for incorporation in a monte carlo analysis approach where the procedure must be repeated thousands of times, the new phasing algorithm is impractical for unrelated individuals with high missing rates. we therefore recommend that for a resource of unrelated individuals  the standard full likelihood approach  is the best alternative. however, for data sets that include family structure we find that substantial haplotype accuracy is lost by ignoring pedigree structure, and the use of an algorithm that considers that structure  is a more prudent choice. in some situations, this may require a more stringent quality control protocol with higher minimum genotyping thresholds to retain practical application of the more sophisticated algorithm.

comparisons between our new phasing algorithm and a previously proposed pedigree-based algorithm, haplore  <cit> , show equivalent or slightly better mle haplotype accuracy for the new algorithm for all situations and data sets considered. in terms of phasing times, our algorithm ran substantially faster in most situations considered for comparison, particularly when using family-based data with higher number of markers and missing rates. furthermore, haplore was unable to be used for in nuclear families with larger numbers of loci and high missing rates, or in the large pedigrees.

as an empirical approach, the space and time requirements for hapmc can be considered limitations. the time required to phase haplotypes and calculate the observed association statistics must be scalable to be able to practically generate the necessary mc simulations. as has been mentioned, the haplotype phasing aspect of hapmc can be computationally intense for large data sets and high missing rates. for example, to analyze ten markers in one mixed large pedigree and case control resource with ~ <dig> genotyped individuals the hapmc algorithm required ~ <dig> minutes using a  <dig>  ghz processor and  <dig> gb of memory. these requirements increase with increased missing data and markers and decreased pedigree structure. in our current application, the total number of loci hapmc can practically handle is approximately  <dig>  although the precise limitation is dependent upon the data set characteristics . however, because the method is designed for tsnps across a non-recombinant follow-up gwas region or candidate gene, marker sets of fewer than  <dig> markers is not unreasonably small. the marker limitation is also consistent with other programs with similar approaches.

it is known that the use of mle haplotypes in association analyses  can lead to invalid association tests and may result in biased estimates of effect size and other parameters  <cit> . we emphasize here that all tests in hapmc are under the null hypothesis of no association of any haplotype, and that the key to the mc procedure in producing valid association statistics using the mle haplotypes is to generate properly matched null data from which to generate the null distribution  <cit> . our method uses a mc procedure that matches the entire phasing process and the use of mles in the observed data and in all null data sets used for the null distribution. hapmc therefore produces accurate significance levels for both tests for independence and effect size, as we have shown. however, the point estimates for effect size statistics  estimated from our method may be upwardly biased. such biased effect sizes are possible when using pedigree data that have been ascertained for disease and analyzing related controls explicitly. while the bias may be removed by using a matched case/pseudocontrol analysis within families  <cit> , the point estimates  should be interpreted with caution both due to the use of mle and the pedigree-based data. it is worth noting that in joint analyses of multiple resources, if the disease maf and/or disease effect size is anticipated to differ across the component studies that a formal meta procedure should be followed. hapmc has been incorporated in to the genie framework, and hence formal meta procedures can be implemented in the approach  <cit> .

not addressed here, is that while haplotype association testing is considered a reasonable approach to explore, it is often burdened with the task of determining which haplotypes or sub-haplotypes that should be tested. it may be of interest to note, that the mc, mle haplotype association approach outlined in this paper has also been incorporated into a peripheral genie software package called hapconstructor  <cit>  . hapconstructor is a data mining software aimed at identifying the most significant haplotypes from a data set. however, due to computing time constraints of a data-mining approach, currently hapconstructor is limited to using the phase-naïve em algorithm for haplotype estimation. mixed resource structures and formal meta analyses are supported within hapconstructor and, as we have shown here, even though it may not be ideal, our mc approach with the pedigree-naïve mles remains valid.

we have illustrated hapmc using multiple single data sets of varying design, as well as several joint resources based on a combination of one traditional case-control data set and one family-based data set. however, the mc approach extends more generally to multiple constituent groups where each can be from any study design. furthermore, the family structures it can analyze are not limited in size or structure. this feature was demonstrated here by the lp data sets that were five-generation pedigrees with substantial missing data. we re-emphasize that large pedigrees with missing data may necessitate pedigree splitting at the phasing step, but that the full structures are maintained when generating the null configurations to fully account for the familial relatedness in the association analyses. to our knowledge, hapmc is the only method and software currently available that can provide valid haplotype analyses in resources of mixed study designs that include general pedigrees. as previously shown by others  <cit> , the importance of joint analyses is the increased power such analyses offer over simply combining the statistical evidence from two separate analyses.

beyond demonstrating the validity of the method, our power results provide some insight into the relative strengths of different study designs and statistical approaches. in our single data set analyses, for trio and asp data sets we found that both the trend test with a pc approach and the tdt were superior to the ec approach. there was a lack of significant difference in power between the pc approach and the tdt analysis. from this we would conclude that the tdt statistic remains the preferred statistic for analyzing nuclear family designs due to its additional robustness to population stratification. in the large pedigree data sets, the relative superiority of the two approaches  was not clear. in the single data sets , the ec and pc approaches were similar in lp <dig> and the ec approach appeared superior in lp <dig>  in the joint data sets , the pc approach was superior in lp1cc, but no significant difference was found in lp2cc. the lack of impact of the pc approach in lp <dig> may be due to the reduced control size in that data set , however, our observations highlight the difficulty in defining optimal approaches for general pedigrees where the specific structure may influence the relative powers of different approaches. to investigate this we repeated our lp analyses, but with oversampling of parents of affected cases and less sampling of other individual as controls, thus increasing the number of occurrences that the pseudocontrol could be used in the analysis. we found that the pc approach improved in power . in summary, the pc approach was found to have significant superiority over the ec approach in the trio, asp, triocc, aspcc, and lp1cc data sets. only one design indicated superiority of the ec approach , and the remainder indicated no significant difference . our results therefore suggest that the pc approach is likely to be the better approach for mixed nuclear family and case-control designs.

we also explored a stricter definition to select familial controls. we found that if close relatives are simply not considered in the analyses  that the power was adversely affected by the reduced control sample size . this indicates that the close relatives contribute positively to the power of the analysis.

comparing different single study designs, the trio design consistently performed worse than all other designs. of the remaining designs, it was interesting to note that for matched sample sizes the large high-risk pedigree design  was comparable in power to asp and cc designs. large pedigrees arguably contain the most redundancy in the familial cases , but are also enriched for disease alleles . familial controls have previously been shown to increase power  <cit> . the comparability in power between lp <dig> and cc suggests that the positive effect of the disease allele enrichment in lp <dig> may have balanced the decrease in effective sample size due to the redundancy in information from related subjects. we also found that lp <dig> and asp were not significantly different for power and both designs are enriched for disease. however, on average, large pedigree controls are less related to the cases than controls are in asps, hence the effective increase in control population in lp <dig> may balance the effect that the effective sample size of the cases is reduced. of course, it must be noted that our results may be specific to our simulated data sets, and, for other large pedigree structures, these findings may not hold. nonetheless, our results indicate substantial potential for large pedigree resources and using pedigree-based controls for haplotype association analyses.

CONCLUSIONS
in conclusion, we have developed a method and software to perform valid haplotype analyses in resources of mixed pedigree structure. to our knowledge this is the only method currently available that can perform such analyses. similarly to that found by others  <cit> , our findings illustrate the power advantage of joint analyses and, furthermore, suggest family-based resources can play a valuable role in haplotype association studies.

