BACKGROUND
gene expression microarrays are becoming increasingly promising for clinical decision support in the form of diagnosis and prediction of clinical outcomes of cancer and other complex diseases. in order to maximize benefits of this technology, researchers are continuously seeking to develop and apply the most accurate classification algorithms for the creation of gene expression patient profiles. prior research suggests that among well-established and popular techniques for multicategory classification of microarray gene expression data, support vector machines  have a predominant role, significantly outperforming k-nearest neighbours, backpropagation neural networks, probabilistic neural networks, weighted voting methods, and decision trees  <cit> .

in the last few years substantial interest has developed within the bioinformatics community in the random forest algorithm  <cit>  for classification of microarray and other high-dimensional molecular data  <cit> . the random forest algorithm possesses a number of appealing properties making it well-suited for classification of microarray data:  it is applicable when there are more predictors than observations,  it performs embedded gene selection and it is relatively insensitive to the large number of irrelevant genes,  it incorporates interactions between predictors,  it is based on the theory of ensemble learning that allows the algorithm to learn accurately both simple and complex classification functions,  it is applicable for both binary and multicategory classification tasks, and  according to its inventors it does not require much fine-tuning of parameters and the default parameterization often leads to excellent performance  <cit> . recent work  <cit>  reported an empirical evaluation of random forests in the cancer microarray gene expression domain and concluded that random forest classifiers have predictive performance comparable to that of the best performing alternatives  for classification of microarray gene expression data. in fact, the data in table  <dig> of  <cit>  suggests that random forests on average across  <dig> datasets slightly outperform svms as well as other methods. if true, this finding could be of great significance to the field, because combined with prior results about svm performance , this suggests that random forests offer classification accuracy advantages over "best of class" classifier algorithms for this type of data.

however, closer inspection of this prior comparison  <cit>  reveals several important data analytic biases that may have affected its conclusions: first, while the random forests were applied to datasets prior to gene selection, svms were applied with a subset of only  <dig> genes . given that the number of optimal genes varies from dataset to dataset, and that svms are known to be fairly insensitive to a very large number of irrelevant genes, such application of svms likely biases down their performance. second, a one-versus-one svm algorithm was applied for the multicategory classification tasks, while it is has been shown that in microarray gene expression domain this method is inferior to other multicategory svm methods, such as one-versus-rest  <cit> . third, the evaluation of  <cit>  was limited only to linear svms without optimizing any algorithm parameters such as the penalty parameter c that balances data fit with insensitivity to outliers. fourth, the performance metric used in  <cit> , proportion of correct classifications, is sensitive to unbalanced distribution of classes and has lower power to discriminate among classification algorithms compared to existing alternatives such as area under the roc curve and relative classifier information  <cit> . fifth, no statistical comparison among classifiers has been performed. finally, the prior comparison uses a .632+ bootstrap error estimator  <cit>  which is not the most appropriate error estimator for microarray data where powerful classifiers such as svms and rfs typically achieve  <dig> training error and the .632+ bootstrap becomes equivalent to repeated hold-out estimation that may suffer from the training-set-size bias as discussed in  <cit> . furthermore, .632+ bootstrap is currently not developed for performance metrics other than proportion of correct classifications.

we hypothesize that these apparent methodological biases of prior work have compromised its conclusions and the question of whether random forests indeed outperform svms for classification of microarray gene expression data is not convincingly answered. in the present work we undertake a more methodologically rigorous comparison of the two algorithms to determine the relative errors when applied to a wide variety of datasets. we examine the algorithms both in the settings when no gene selection is performed and when several popular gene selection methods are used. to make our evaluation more relevant to practitioners, we focus not only on diagnostic datasets that are in general known to have strong predictive signals, but also include several outcome prediction datasets where the signals are weaker and larger gene sets are often required for optimal prediction.

RESULTS
using full set of genes
the performance results of classification prior to gene selection are shown in figure  <dig> and table  <dig>  in total, svms nominally  outperform rfs in  <dig> datasets, rfs nominally outperform svms in  <dig> datasets, and in  <dig> datasets algorithms perform the same. the application of permutation-based statistical comparison test with significance level α =  <dig>  reveals that svms significantly outperform rfs in  <dig> datasets, while rfs do not significantly outperform svms in any dataset. the permutation test applied to all  <dig> datasets shows that svms statistically significantly outperform rfs on average over all datasets at the  <dig>  α level . it is also worthwhile to compare both methods in terms of the average performance across datasets. the average performance of svms is  <dig>  auc and  <dig>  rci in binary and multicategory classification tasks, respectively. the average performance of rfs in the same tasks is  <dig>  auc and  <dig>  rci.

the performance is estimated using area under roc curve  for binary classification tasks and relative classifier information  for multicategory tasks. see subsection "statistical comparison among classifiers" for the description of statistical test employed to compute reported p-values. p-values shown with boldface denote statistically significant differences between classification methods at the  <dig>  α level.

using gene selection
six classification performance estimates have been produced for each classifier and dataset . in figure  <dig> and table  <dig> we present a comparison based on the best performing gene selection method for each algorithm and dataset combination under the operating assumption that practitioners will optimize choice of the gene selection method for each dataset separately . the results in figure  <dig> and table  <dig> thus better mirror the actual practice of data analysis.

the performance is estimated using area under roc curve  for binary classification tasks and relative classifier information  for multicategory tasks. see subsection "statistical comparison among classifiers" for the description of statistical test employed to compute reported p-values. p-values shown with boldface denote statistically significant differences between classification methods at the  <dig>  α level.

according to the results in figure  <dig> and table  <dig>  in  <dig> datasets svms nominally outperform rfs, in  <dig> datasets rfs nominally outperform svms, and in  <dig> datasets algorithms perform the same. furthermore, svms outperform rfs statistically significantly  in  <dig> dataset. there is no dataset where rfs outperform svms with statistically significant difference. the permutation test applied to all  <dig> datasets shows that svms statistically significantly outperform rfs on average over all datasets at the  <dig>  α level . a comparison of the average performance across datasets also suggests superiority of svms: the average performance of svms is  <dig>  auc and  <dig>  rci in binary and multicategory classification tasks, respectively; while the average performance of rfs in the same tasks is  <dig>  auc and  <dig>  rci.

the number of genes selected on average across  <dig> cross-validation training sets is provided in table  <dig>  we note that in the present comparison we focus exclusively on classification performance and do not incorporate number of selected genes in the comparison metrics because there is no well-defined trade-off between number of selected genes and classification performance in the datasets studied. nevertheless, the detailed classification results for all gene selection methods, classifiers, and datasets are provided in the additional file  <dig> 

average number of genes selected over  <dig> cross-validation training sets.

discussion
the results presented in this paper illustrate that svms offer classification performance advantages compared to rfs in diagnostic and prognostic classification tasks based on microarray gene expression data. we emphasize that when it comes to clinical applications of such models, because the size of the patient populations is typically very large, even very modest differences in performance  can result in very substantial differences in total clinical outcomes   <cit> .

the reasons for superior classification performance of one universal approximator classifier over the other in a domain where the generative functions are unknown are not trivial to decipher  <cit> . we provide here as a starting point two plausible explanations supported by theory and a simulation experiment . we note that prior research has established that linear decision functions capture very well the underlying distributions in microarray classification tasks  <cit> . in the following two paragraphs we first demonstrate that for such functions svms may be less sensitive to the choice of input parameters than rfs and then explain why svms model linear decision functions more naturally than rfs.

the simulation experiment described in additional file  <dig> demonstrates high degree of sensitivity of rfs to the values of input parameters mtry  and ntree  even in the case of linear decision function when complicated decision surface modelling is not required. the experiment shows that the choice of rf parameters creates large variation in the classifier performance whereas the choice of the main svm parameter has only minor effects on the error. in practical analysis of microarrays this means that finding the rfs with optimal error for the dataset may involve extensive model selection which in turn opens up the possibility for overfitting given the small sample sizes in validation datasets.

a second plausible explanation is that decision trees used as base learners in the rf algorithm cannot learn exactly many linear decision functions in the finite case. specifically, if the generative linear decision function is not orthogonal to the coordinate axes, then a decision tree of infinite size is required to represent this function without error  <cit> . the voted decision function in rfs approximates linear functions based on rectangular partitioning of the input space, and this "staircase" approximation can capture a linear function exactly when the number of decision trees can grow without bound . svms on the other hand use linear classifiers and thus can model such functions naturally, using a small number of free parameters .

we note that regardless of the specific reasons why rfs may have larger error on average in this domain, it is still important to be aware of the empirical performance differences when considering which classifier to use for building molecular signatures. it may take several years before the precise reasons of differences in empirical error are thoroughly understood, and in the meantime the empirical advantages and disadvantages of methods should be noted first by practitioners.

data analysts should also be aware of a limitation of rfs imposed by its embedded random gene selection. in order for a rf classification model to overcome the trap of large variance, one has to use a large number of trees and build trees based on a large number of genes. the exact values of these parameters depend on both the complexity of the classification function and the number of genes in a microarray dataset. therefore, in general, it is advisable to optimize these parameters by nested cross-validation that accounts for the variability of the random forest model .

finally, it is worthwhile to mention the work by segal  <cit>  who questioned breiman's empirical demonstration of the claim that random forests do not overfit as the number of trees grows  <cit> . in short, segal showed that there exist some data distributions where maximal unpruned trees used in the random forests do not achieve as good performance as the trees with smaller number of splits and/or smaller node size. thus, application of random forests in general requires careful tuning of the relevant classifier parameters. these observations may suggest future improvements of rf-related analysis protocols.

CONCLUSIONS
the primary contribution of the present work is that we conducted the most comprehensive comparative benchmarking of random forests and support vector machines to date, using  <dig> diagnostic and outcome prediction datasets. our hypothesis that in previously reported work, research design limitations may have biased the comparison of classifiers in favour of random forests, was verified. after removing these benchmarking limitations, we found that, both on average and in the majority of microarray datasets, random forests exhibit larger classification error than support vector machines both in the settings when no gene selection is performed and when several gene selection methods are used.

the quest for high performance classifiers with microarray gene expression and other "omics" data is ongoing. random forests have appealing theoretical and practical characteristics, however our experiments show that currently they do not exhibit "best of class" performance. our data also points to methodological limitations of prior evaluations and thus emphasizes the importance of careful design of bioinformatics algorithm evaluation studies.

