BACKGROUND
high throughput bottom-up proteomics using lc-ms  <cit>  has become one of the major proteomics approaches today. in this technique tandem ms  spectra are usually matched by search or identification algorithms to peptide sequences in protein databases. the databases used contain protein sequences with varying quality: only a minor part of the sequences are experimentally validated, some are predicted, e.g. by homology to other species, while a considerable part of the sequences are only based on predicted open reading frames. protein prediction algorithms are very advanced, but still have weaknesses for the prediction of small proteins, introns and translation start sites. for most exotic species not commonly used in the lab, there are no well curated protein databases at all.

as bacterial genomes are comparatively short and thus cheap to sequence, it is feasible to create protein databases by translating all six reading frames of the genome. we call the proteins originating from this direct translation "pseudo proteins" in this work, whereas annotated proteins are referred to as "known proteins". such a database containing pseudo and known proteins can be used to identify ms/ms spectra, which cannot be identified in conventional databases or deriving from species without protein databases. this approach is called proteogenomics  <cit>  and allows enhancing the annotation of the genome of the analyzed species as well as the improvement of existing protein databases. these enhancements may include the correction of predicted reading frame boundaries as well as the discovery of new proteins or peptides.

there are already several approaches for proteogenomic tools: some try to tackle the very large number of pseudo proteins generated from eukaryotic genomes  <cit> , others developed new, specialized search engines for this task, as shown in  <cit>  and  <cit> . almost all tools, including e.g. the genosuite  <cit> , allow only a small set of search algorithms for peptide identification. to the best of our knowledge, there is no standalone tool which allows the visualization and comparison of pseudo peptides found in different experimental conditions and which imports identifications from mztab  <cit>  format and thus supports any peptide identification, combination of identification algorithms or post-processing algorithm. for further inspection of the results and all intermediate information, all protein and peptide information can be exported to the generic feature format  <dig> , which is widely supported by common genome browsers.

implementation
the bacterial proteogenomic pipeline consists of several java classes which allow a complete proteogenomics approach using ms/ms data, except for the peptide identification step, which is done by search engines. all parts of the pipeline can be run on any current desktop system compatible with java. the source code is available under a three-clause bsd license and thus open source for everyone. besides the command line execution, we provide a gui which will guide the user in six steps through the analysis. the steps will be further explained in the following paragraphs. figure  <dig> shows the gui at the last analysis step .

step 1: parse protein information
in this first step, the protein information of the already annotated known proteins respectively their genes is parsed either from a separated values file  or a protein fasta file and saved into a gff <dig> file. for each gene or protein the accession, the genomic start and end positions and the strand information  must be included in the file and will be parsed. additionally a protein/gene description and the originating chromosome or plasmid name may be obtained. for a tsv file, only the column for each parsed variable must be defined. for a fasta file, regular expressions of how to get the information from the gene or protein header are used. for the pipeline to be able to gather all information correctly, the fasta file, which contains the known proteins, should have the same accessions as the accessions parsed in this step.

step 2: compare and combine
this optional step allows adding further protein information from a reference fasta file, additionally to the one containing the known proteins' information generated in step  <dig>  this is for example interesting, if the fasta file for the known proteins originates from a species specialized database and the accessions and sequence information from e.g. the uniprot kb should be added to the known proteins. also the proteins of a host species  or a contaminant database can thus be merged to the list of known proteins.

there are two ways to find related entries in the protein list parsed in step  <dig> and an additional reference fasta: either a given mapping file between the accessions of the lists may be used or, if for an entry no mapping is found, the amino acid sequences are compared. in the latter case a relation between the proteins is assumed only if the difference between the lengths of the sequences is not bigger than  <dig> amino acids. three kinds of relations are identified and added to the description of the protein: "equal to x" if the protein sequences are identical, "elongation by x" if the reference protein has a longer amino acid sequence  and "elongation of x" if the reference protein's sequence is shorter and contained in the target protein's sequence . if an "elongation" relation is detected, the longer sequence is stored. for any protein, which cannot be related or mapped to a known protein, the information from the reference file is copied. the combination finally creates new fasta and gff <dig> files for the subsequent steps.

step 3: genome parser
the genome parser creates the naïve six frame translated protein fasta database of a given genome. the translation starts on the first position of the genome and reads nucleotide triplets until the first stop codon is reached. immediately after the stop codon is reached, a new pseudo protein is started instead of waiting for the next start codon to appear. if at least one start codon exists  in the pseudo protein, additionally the longest orf will be translated and written to the fasta file . it is necessary to also have these orf pseudo proteins starting with a methionine translated from the start codon to allow the search engine to correctly match possible ms/ms spectra against the respective n-terminal peptides. unfortunately, this approach creates a set of overlapping proteins for each start codon which does not immediately follow a stop codon and thus increase the time needed for the spectrum identification. the bacterial proteogenomic pipeline uses the codons atg, ttg, ctg, att, atc, ata and gtg as start codons, which in the case of a start codon are all translated into methionine. if the positions of the known proteins are given, proteins translating from exactly the same genome site will not be added to the pseudo proteins to avoid redundancy. pseudo proteins overlapping one or more annotated proteins are tagged appropriately in their description with "elongation of", similar as described in step  <dig> 

step 4: create decoy database
this step is optional and assists the user in building a decoy database containing shuffled decoy entries of the target entries to perform target-decoy searches  <cit> . either a concatenation of target and decoy entries or a single database with decoy entries only can be created.

after the search database respectively databases  containing both known and pseudo proteins are created, the peptide identification of the ms/ms data can be performed by any search algorithm, e.g. sequest  <cit> , ms-gf+ <cit> , mascot  <cit>  or x!tandem  <cit> . this must be performed by the user manually and thus also gives free choice of any validation and filtering using certain fdr or other criteria. after the identification and validation/filtering, the identified peptide spectrum matches passing the criterions must be exported into mztab files, one for each ms/ms run. for the export e.g. openms  <cit>  or pia  can be used, which are both open software.

step 5: combine identifications
in this step the results of the peptide identifications can be grouped into sets representing any kind of experimental condition, like e.g. different growth conditions of the samples. the identifications are parsed from mztab files, combined and can be saved into a sqlite database for subsequent analysis. additionally, the combined data can directly be written into two gff <dig> files, one containing only the peptides of pseudo proteins, the other all remaining peptides. a peptide is defined by the amino acid sequence only, neglecting any modifications or charge states. for each peptide in the gff <dig> file there will be one feature for each condition group with the score set to the respective number of identified spectra and one feature for the overall number of identifications.

step 6: analysis
the final step, which is only available in the gui and depicted in figure  <dig>  is for a manual review and analysis of the results. for each peptide, the corresponding proteins are shown and whether they are originating from the genome or plasmids. furthermore it is stated whether the peptide was found in pseudo proteins only and whether these proteins are an elongation of any known protein or are standalone pseudo proteins, i.e. proteins without any overlap to a known protein. the number of identified different spectra for each peptide, also called spectral counts, is given as sum of all imported files and additionally for each assigned group. for the assigned groups, the counts can also be shown normalized. this normalization makes the assumption that the total amount of identifiable protein is equal per sample and is performed by the following operation

 c′i=maxtnft×cinfi, 

where ci is the raw count for peptide  i and nfi is the total number of counts in the respective identification file. to obtain human readable values, the quotient is multiplied by the largest number of counts of all individual files . for a better perception, the distribution of counts per group is also visualized in a bar chart. if the full sequence of a protein is known, it is visualized with the sequences of the identified peptides highlighted to help in assessing the relevance of identifications. the analysis allows several filters to show e.g. identifications of pseudo proteins only or only peptides, which exceed a given number of identifications. the bacterial proteogenomic pipeline also allows adding of further identification files in this step to enhance an analysis and the export into gff <dig> files as discussed in the previous paragraph or a simple tab separated format.

RESULTS
the bacterial proteogenomics pipeline was tested on two datasets, one publicly available containing data from bradyrhizobium japonicum  samples grown in cowpea nodules  and one containing synechocystis sp. pcc  <dig>  samples, which were cultivated under different environmental conditions.

for the b. japonicum dataset, the genome and known proteins were downloaded from the ncbi using the reference sequence nc_ <dig>   and processed by the bacterial proteogenomic pipeline. the proteins of vigna unguiculata , which acted as host, were downloaded from the uniprot  and added to the list of known proteins, as well as "the common repository of adventitious proteins, crap" , resulting in a total of  <dig> known proteins. the genome parser created  <dig> pseudo proteins. from these databases , a target decoy database was created and searched by x!tandem and ms-gf+, using fixed carbamidomethylation of cysteine and variable oxidation of methionine as modifications. the results were combined with pia  and only psms with combined fdr scores  <cit>  below  <dig>  were exported to mztab files. the three resulting mztab files were further processed by our software and for the analysis the minimal number of identifications per peptide was set to  <dig>  with these rather strict settings, we detected all together  <dig> new peptides, of which  <dig> represent protein elongations respectively gene boundary changes and  <dig> completely new proteins, all peptides are proteotypic  given the databases used. most but not all of these new identifications were also found by kumar et al. in  <cit> , the list of peptides is shown in table  <dig>  all necessary steps except for the spectrum identification were carried out using the gui on a laptop computer  in a few minutes . the time needed for the spectrum identification depends on the used search engine and data and therefore cannot be estimated accurately in general, but for this test sample and the prior stated search parameters took about two hours.

this table shows the peptides of pseudo proteins found in a proteogenomic analysis of b. japonicum. ms/ms spectra were identified with ms-gf+ and x!tandem, the combined search results were filtered on a combined fdr score level of  <dig> . only peptides, which had at least  <dig> distinct peptide spectrum matches are reported, peptides from the same orf respectively pseudo protein are visually grouped by the alternating bold and recursive orf positions.

the analysed synechocystis cultures were grown under four different environmental conditions: normal  and high light  each combined with normal  and high co <dig>  levels. the genome and protein information was downloaded from the cyanobase  together with sequences for the plasmids psysa, psysg, psysm, psysx, sequences for pca <dig> , pcb <dig>  and pcc <dig>  were downloaded from the ncbi sites. this information was enriched by protein information from the uniprot by the "compare and combine" module . eight samples of each condition were measured and the resulting ms/ms spectra matched against a target-decoy database of the known and pseudo proteins with mascot, ms-gf+ and x!tandem. the results were combined and filtered as described in the previous paragraph. a thorough analysis of the  identified pseudo proteins is pending. the bacterial proteogenomic pipeline detected  <dig> peptides found in pseudo proteins with at least  <dig> distinctive identified spectra, of which  <dig> elongate known proteins and  <dig> belong to new standalone proteins,  <dig> of these peptides are not proteotypic, but could be associated to more than one pseudo protein.

besides the further analysis of the synechocystis dataset, further improvements of the bacterial proteogenomic pipeline may include the visualization of annotated spectra and the direct import of more standard formats like mzidentml and filtering of used identifications inside the pipeline.

CONCLUSIONS
we presented the bacterial proteogenomic pipeline, a set of tools for proteogenomics analyses with emphasize on the visualization of results, which runs on current desktop computers and allows an operating system independent execution. the usage of a standard format for the spectrum identifications import allows the user to run virtually any peptide identification and post processing algorithm. the results of a processed analysis can be browsed via the provided gui or can be exported into gff <dig> files and imported into any common genome browser.

availability and requirements
project name: bacterial proteogenomic pipeline

project homepage: https://github.com/mpc-bioinformatics/bacterial-proteogenomic-pipeline

operating system: platform independent 

programming language: java

other requirements: java  <dig> 

license: three-clause bsd license

any restrictions to use by non-academics: none

competing interests
the authors declare that they have no competing interests.

authors' contributions
ju did the implementation of the gui and drafted the manuscript, ju and me programmed together all other implementations. me and sr had the initial idea for the bacterial proteogenomic pipeline. sr additionally provided the samples for the synechocystis, which were cultivated, processed and measured on the mass spectrometers by np. km provided the computational infrastructure and contributed to the background and discussion parts.

list of abbreviations
fdr: false detection rate

gff3: generic feature format 3

orf: open reading frame

