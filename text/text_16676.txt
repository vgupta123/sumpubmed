BACKGROUND
secondary structure can provide important information about three-dimensional protein structure. therefore, its prediction has been an area of intense research over the past three decades. to predict secondary structure many methods have been implemented, including different machine learning techniques, such as artificial neural networks   <cit>  and support vector machines   <cit> , and different input schemes, such as position specific scoring matrices   <cit>  and hidden markov models  <cit> . notably, the predictive accuracy reached 80% for three-state prediction, where residues are divided into helix, strand and coil. helices and strands are repetitive, regular structures, while the remaining residues, which can be tight turns, loops, bulges or random coil, are all classified as coil; they are non-repetitive, irregular secondary structures  <cit> . although the helix and strand classes are structurally well-defined, the third class, coil, does not provide any detailed structural information. hence, further analysis of the local structure is necessary, such as prediction of backbone dihedral angles  <cit>  and prediction of tight turns  <cit> .

tight turns play an important role in protein folding and stability. they are partly responsible for the compact, globular shape of proteins, because they provide directional change to the polypeptide chain  <cit> . depending on the number of constituent residues, tight turns can be classified as α-turns, β-turns, γ-turns, δ-turns or π-turns. a β-turn is formed by four adjacent residues, which are not in an α-helix, where the distance between ca and ca is less than  <dig> Å  <cit> . the β-turns are the most common tight turns. on average, about a quarter of all residues are in a β-turn  <cit> . moreover, β-turns are crucial components of β-hairpins, the fundamental elements of anti-parallel β-sheets, whose prediction has recently attracted interest  <cit> . furthermore, β-turn formation is an important step in protein folding  <cit> , while improved β-turn sequences can improve protein stability  <cit> . additionally, their occurrence on the surface of proteins suggests their involvement in molecular recognition processes and their interactions between peptide substrates and receptors  <cit> . recently, mimicking β-turns for the synthesis of medicines  <cit>  or for nucleating β-sheet folding  <cit>  has also attracted interest. thus, the prediction of β-turns can facilitate three-dimensional structure prediction and can provide important information about the protein folding. hutchinson and thornton  <cit>  classified the β-turns into nine types based on the dihedral angles of residues i +  <dig> and i +  <dig> in the turn . this is the most established classification of β-turns.

β-turns are divided into nine classes based on the dihedral angles of the central residues. type iv is a miscellaneous category that contains all conformations outside the other eight classes and the dihedral angles shown here are the average values and, therefore, depend on the dataset.

prediction of β-turns has attracted interest in the past. the approaches can be divided into statistical methods and machine learning techniques. the former include early methods which used amino acid propensities  <cit>  as well as more recent methods, like coudes  <cit> , which used probabilities with multiple sequence alignments. over the past few years, machine learning techniques have been applied successfully to predict β-turns. since their first use  <cit> , anns have been frequently used for β-turn prediction  <cit> . over the past decade, several studies used svms to predict β-turns  <cit>  and other techniques, such as nearest neighbour, have been applied recently  <cit> . through the use of evolutionary information and more sophisticated machine learning techniques, the correlation coefficient for turn/non-turn prediction is now as high as  <dig>   <cit> . other methods predict the type of β-turn, rather than the location of the turn in the chain, with significant success, even though this problem is challenging, due to the lack of examples for many β-turn types. btpred  <cit> , betaturns  <cit> , molebrnn  <cit>  and the method of asgary and colleagues  <cit>  are ann-based, whereas coudes  <cit>  uses amino acid propensities with multiple sequence alignments. in spite of its successful use for the prediction of β-turn location  <cit> , the svm method has not been employed widely for β-turn type prediction.

despite the success so far, there is a need for more accurate predictions of both β-turn location and β-type, which could be realised through the use of additional information. evolutionary information from multiple alignments  <cit>  as well as predicted secondary structures  <cit>  can improve β-turn predictions dramatically. in this work, we show that the backbone dihedral angles can provide crucial information for turn/non-turn prediction and can also noticeably improve the prediction of β-turn types, since the types are defined by the dihedral angles of the central residues. predicted dihedral angles have been used successfully for secondary structure prediction  <cit> . the method presented here, called debt , uses predicted secondary structures and predicted dihedral angles from disspred  <cit>  and achieves the highest correlation coefficient reported to date for turn/non-turn prediction, while the prediction of β-turn types is, in most cases, more accurate than other contemporary methods. the method predicts β-turn type i, ii, iv, viii as defined by hutchinson and thornton  <cit> , while all remaining types are classified as ns . moreover, we show that using a small local window of predicted secondary structures and dihedral angles, rather than using the predictions of one individual residue, is beneficial.

methods
datasets
debt was trained and tested on four non-redundant datasets, which contain chains with at least one β-turn and have x-ray crystallographic resolution better than  <dig>  Å. all protein chains have less than 25% sequence similarity, to ensure that there is very little homology in the training set. the first dataset, denoted as gr <dig> in this paper, consists of  <dig> protein chains  <cit>  and was used to study the impact of various training schemes and to tune the kernel parameters. gr <dig> has been used by the majority of recent β-turn prediction methods and, therefore, we can use it to make direct comparisons. in  <dig>  the gr <dig> dataset was used for the evaluation of β-turn prediction methods  <cit>  and was partitioned into seven subsets in order to perform seven-fold cross-validation. in this work, we utilised the same partition for the cross-validation. after finding the optimal input scheme and tuning the kernel parameters, we used two additional datasets, constructed for training and testing coudes  <cit> , to validate the performance of our method. the datasets consist of  <dig> and  <dig> protein chains and are denoted as fa <dig> and fa <dig>  respectively. finally, debt's web-server was trained using pdb-select <dig>   <cit> , a set of  <dig> chains from the pdb with less than 25% sequence similarity. from these chains, we have selected those that contain at least one β-turn and have an x-ray crystallographic resolution below  <dig>  Å. this gave a dataset of  <dig> protein chains, denoted as pdb <dig> in this article, which is the largest training set used for a β-turn prediction server. the pdb codes and chain identifiers of the above datasets are listed at debt's website http://comp.chem.nottingham.ac.uk/debt/. the β-turns and their types were assigned using the promotif program  <cit> . in this work, we predict β-turn types i, ii, iv, viii, while all the remaining types are assigned to the miscellaneous class ns . table  <dig> shows the distributions of β-turns and their types in each dataset.

debt method utilises pssms, constructed by the psi-blast algorithm  <cit> , to predict β-turns and their types. pssms have n ×  <dig> elements, where the n rows correspond to the length of the amino acid sequence and the columns correspond to the  <dig> standard amino acids. pssms represent the log-likelihood of a particular residue substitution, usually based on a weighted average of blosum <dig>  <cit> . we generated the pssms using the blosum <dig> substitution matrix with an e-value of  <dig>  and three iterations against a non-reduntant  database, which was downloaded in february  <dig>  the data were filtered by pfilt  <cit>  to remove low complexity regions, transmembrane spans and coiled coil regions. the pssm values were linearly scaled simply by dividing them by ten. typically, pssm values are in the range , but some values outside this range may appear. linear scaling maintains the same distribution in the input data and helps avoid numerical difficulties during training.

support vector machines
debt employs svm  <cit> , a state-of-the-art supervised learning technique. the svm method has become an area of intense research, because it performs well with real-world problems, it is simple to understand and implement and, most importantly, it finds the global solution, while other methods, like anns, have several local solutions  <cit> . the svm can find non-linear boundaries between two classes by using a kernel function, which maps the data from the input space into a richer feature space, where linear boundaries can be implemented. furthermore, the svm effectively handles large feature spaces, since it does not suffer from the "curse of dimensionality", and, therefore, avoids overfitting, a common drawback of supervised learning techniques.

a detailed description of the svm algorithm can be found in various textbooks  <cit> . in brief, given input vectors xi ∈ rn and output values yi ∈ {- <dig>  1}, the fundamental goal of a binary svm classifier is to solve the following optimisation problem:   

where w is a vector perpendicular to the hyperplane, b is the offset from the origin and c is a penalty parameter for each misclassification. thus, it controls the trade-off between training error and the margin that separates the two classes. the kernel function used in our case is the radial basis function , shown in equation  <dig>  which was successfully used for complex problems, such as secondary structure prediction  <cit>  and dihedral prediction  <cit> .   

where xi and xj are the input vectors for instances i and j, respectively, and γ is a parameter that controls the width of the kernel.

libsvm  <cit> , a popular svm software package, was employed for the training and testing of the svm classifiers. in order to get the optimal predictive performance, we optimised three parameters: c , γ  and w. the latter controls the cost of misclassification for the minority class and, therefore, reduces the effect of the imbalance in the datasets. in other words, different penalty parameters costs are used for each class  <cit> . the optimised parameters for each classifier are shown in table  <dig>  seven-fold cross-validation was applied on datasets gr <dig>  fa <dig> and fa <dig>  for the former, we utilised the the same subsets used by kaur and raghava  <cit>  to evaluate different β-turn prediction methods, whereas the partition of the other two datasets was identical to the one used to train coudes  <cit> .

the parameters were optimised using the grid search approach.

debt architecture
filtering
because the prediction is based on individual residues, the svm outputs include some β-turns that are shorter than four residues, which is unrealistic. turn predictions longer than four adjacent residues are acceptable, since there are many β-turns in the dataset that are overlapping. in fact, about 58% are multiple turns  <cit> . to ensure that the predictions are at least four residue long, we applied some filtering rules similar to the "state-flipping" rule described by shepherd and colleagues  <cit> . the rules are applied with the following order:  flip isolated non-turn predictions to turn ,  flip isolated turn predictions to non-turn ,  flip isolated turn pairs of turn prediction to non-turn  and  flip the adjacent non-turn predictions to turn for isolated three consecutive turn predictions .

prediction accuracy assessment
six different scalar measures were used to assess debt's performance. all of them can be derived from two or more of the following quantities:  true positives, pi, is the number of correctly classified β-turns or β-turn type i,  true negatives, ni, is the number of correctly classified non-turns,  false positives, oi, is the number of non-turns incorrectly classified as β-turns or β-turn type i ,  false negatives, ui, is the number of β-turns or β-turn type i incorrectly classified as non-turn  and  total number of residues, t = pi + ni + oi + ui, where i = i, ii, iv, viii or ns. the first measure used is the predictive accuracy, the percentage of correctly classified residues.   

two measures, that are usually used together, are sensitivity  and specificity which give the percentage of observed β-turns or β-turn types that are predicted correctly and the percentage of observed non-turns that are predicted correctly, respectively. the optimal is to equalise the two measures.      

we report the commonly used matthews correlation coefficient   <cit> , which is the most robust measure for β-turn prediction. the reason is that, when the dataset is unbalanced, it is possible to achieve high predictive accuracy just by predicting all instances as non-turn. the mcc, defined by equation  <dig>  is a number between - <dig> and  <dig>  with perfect correlation giving a coefficient equal to  <dig>  therefore, a higher mcc corresponds to a better predictive performance.   

finally, we report qpred, the percentage of β-turn predictions that are correct:   

another important consideration is whether the classifiers perform better than random prediction. herein, we report a normalised percentage better than random , defined in equation  <dig>  which was introduced by shepherd and colleagues  <cit> . perfect predictions score si = 100%, whereas si = 0% shows that the prediction is no better than random.   

where r is the expected number of residues that would be predicted correctly by a random prediction and is defined as:   

apart from the scalar measures described above, we report the receive-operator characteristics  curves, which represent the sensitivity  against the false positive rate . roc curves have been widely used in bioinformatics  <cit>  for visualisation and assessment of machine learning classifiers. moreover, the area under the roc curve  is calculated to provide a scalar measure of the roc analysis and compare different methods. the trapezium rule is used to calculate the auc, as described by fawcett  <cit> .

RESULTS
the effect of the input scheme
before optimising the svm classifiers, we tried different input schemes, which showed that the combination of evolutionary information , predicted secondary structures and predicted dihedral angles gives the most accurate predictions. table  <dig> shows the results on the gr <dig> dataset from the experiments using various input schemes and different window sizes for the turn/non-turn classifier. firstly, we changed the size of the pssm window, l <dig>  by using lengths of seven, nine and eleven residues. the last two sizes give the highest mcc value. we selected a window size of nine residues, because the input vector is smaller and, therefore, the training time is shorter. subsequently, we augmented the pssm-only input vector with additional attributes only for the central residue  using predicted secondary structures, predicted dihedral angles or both. the results show that, when used together, predicted secondary structures and dihedral angles achieve the best performance. finally, we changed the size of the second window, l <dig>  using three, five or seven residues. the optimal window size is five residues. the same window sizes, l <dig> and l <dig>  were utilised for all classifiers.

window sizes, l <dig> and l <dig>  are the windows for pssm values and predicted secondary structures and/or dihedral angles, respectively. pssm-only: only scaled pssm values are used in the input vector; pssm + ss: the input vector is augmented with predicted secondary structures; pssm + dih: the input vector is augmented with predicted dihedral angles; pssm + ss + dih: the input vector is augmented with both predicted secondary structures and predicted dihedral angles.

turn/non-turn prediction
predicted dihedral angles and secondary structures improve the performance of the turn/non-turn classifier, as shown in table  <dig>  in fact, the mcc shows an improvement of over 10% and reaches values of  <dig> ,  <dig>  and  <dig>  for datasets gr <dig>  fa <dig> and fa <dig>  respectively. moreover, the overall accuracy is higher than 80% for datasets fa <dig> and fa <dig>  while it is  <dig> % for the gr <dig> dataset. finally, qpred, qobs  and the better-than-random score, s, also improved after using predicted dihedral angles and secondary structures.

in the parentheses are the predictions using pssm-only input. there is significant improvement on all measures when the input vector is augmented by predicted dihedral angles and secondary structures.

the methods are sorted by their reported mcc score. debt achieves the highest value on all datasets.

prediction of β-turn types
in the parentheses is the prediction using pssm-only input without predicted dihedral angles or secondary structure. notably, there is improvement in the predictive performance when the input vector is augmented by predicted dihedral angles and secondary structures.

debt is more accurate that other methods in the prediction of types i, iv, viii and ns. the results for btpred and the method by asgary and colleagues are obtained using different datasets.

roc analysis
debt web-server
our method is freely available online at http://comp.chem.nottingham.ac.uk/debt/. the web-server was trained using a large training set of  <dig> protein chains with at least one β-turn to improve the performance of the method. it is written in perl using a cgi interface. the user can either cut and paste the amino acid sequence or upload a fasta file. additionally, multiple fasta files can be uploaded in an archive. initially, debt firstly runs the psi-blast algorithm  <cit>  to construct the pssms and disspred  <cit>  to predict the secondary structures and the dihedral angles. subsequently, the results are merged to create the input file for six svm classifiers. the output file, shown in figure  <dig>  contains the number and the one-letter abbreviation of the amino acids with six binary prediction values: one for turn/non-turn prediction and five for the β-turn types. the prediction value can be "1" if the corresponding residues is predicted in a β-turn/β-turn type and "0" otherwise. moreover, the user can ask for disspred's results to be attached in the output file, which makes debt not only a β-turn prediction server, but also a three-state secondary structure prediction and a seven-state dihedral prediction interface. the output file, together with the log files, are sent to the user by e-mail, or can be downloaded, after the calculations are completed. the combination of disspred's iterative process with the training on a large dataset makes debt web-server slightly slower, but more accurate, than other β-turn prediction servers.

CONCLUSIONS
in this article, we presented a method that predicts the location of β-turns and their types in a protein chain. our method uses predicted dihedral angles from disspred  <cit>  to enhance the predictions. moreover, we improved the predictive performance by using a local window of predicted secondary structures and dihedral angles, rather than the predictions for one individual residue. the mcc of  <dig> , achieved for turn/non-turn prediction on a set of  <dig> non-redundant proteins, shows that debt is more accurate than other β-turn prediction methods. moreover, we report the highest mccs of  <dig>  and  <dig>  on two larger datasets of  <dig> and  <dig> non-redundant protein chains. additionally, the dihedrally enhanced prediction for β-turn types is more accurate than other methods. we report debt's prediction on three datasets with achieved mccs up to  <dig> ,  <dig> ,  <dig> ,  <dig>  and  <dig>  for β-turn types i, ii, iv, viii and ns, respectively. the prediction of β-turn types has limitations derived from the observation that identical tetrapeptides may form different β-turn types. in fact, around 15% of all tetrapeptides that form β-turns in datasets gr <dig> and fa <dig> appear in multiple β-turn types. this number is close to 18% in the fa <dig> dataset. a detailed analysis of the fundamental limitation of β-turn prediction is a challenging future focus. in spite of the limitations, the performance might be improved further by applying techniques introduced by other studies, such as feature selection techniques  <cit> , or by using predicted secondary structures and dihedral angles from multiple predictors. predicted β-turns can be used to improve secondary structure prediction  <cit>  and we are currently exploring this.

authors' contributions
pk carried out the experiments and wrote the manuscript. jdh conceived the study and assisted in writing the manuscript. both authors read and approved the final manuscript for publication.

supplementary material
additional file 1
roc curves for datasets fa <dig> and fa <dig>  roc curves for the predictions on datasets fa <dig> and fa <dig>  before and after using predicted dihedral angles and secondary structures. dashed curves correspond to the pssm-only prediction, while solid curves correspond to the prediction after aumenting the input vector with predicted dihedral angles and secondary structures.

click here for file

 acknowledgements
we thank the hpc facility at the university of nottingham and the university of nottingham for a phd studentship.
