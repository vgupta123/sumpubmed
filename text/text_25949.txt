BACKGROUND
high-throughput screening  is an automated technique and has been effectively used for rapidly testing the activity of large numbers of compounds  <cit> . advanced technologies and availability of large-scale chemical libraries allow for the examination of hundreds of thousands of compounds in a day via hts. although the extensive libraries containing several million compounds can be screened in a matter of days, only a small fraction of compounds can be selected for confirmatory screenings. further examination of verified hits from the secondary dose-response assay can be eventually winnowed to a few to proceed to the medicinal chemistry phase for lead optimization  <cit> . the very low success rate from the hits-to-lead development presents a great challenge in the earlier screening phase to select promising hits from the hts assay  <cit> . thus, the study of hts assay data and the development of a systematic knowledge-driven model is in demand and useful to facilitate the understanding of the relationship between a chemical structure and its biological activities.

in the past, hts data has been analyzed by various cheminformatics methods  <cit> , such as cluster analysis <cit> , selection of structural homologs <cit> , data partitioning  <cit>  etc. however, most of the available methods for hts data analysis are designed for the study of a small, relatively diverse set of compounds in order to derive a quantitative structure activity relationship  <cit>  model, which gives direction on how the original collection of compounds could be expanded for the subsequent screening. this "smart screening" works in an iterated way for hits selection, especially for selecting compounds with a specific structural scaffold  <cit> . with the advances in hts screening, activity data for hundreds of thousands' compound can be obtained in a single assay. altogether, the huge amount of information and significant erroneous data produced by hts screening bring a great challenge to computational analysis of such biological activity information. the capability and efficiency of analysis of this large volume of information might hinder many approaches that were primarily designed for analysis of sequential screening. thus, in dealing with large amounts of chemicals and their bioactivity information, it remains an open problem to interpret the drug-target interaction mechanism and to help the rapid and efficient discovery of drug leads, which is one of the central topics in computer-aided drug design  <cit> .

although the  structure activity relationship-sar has been successfully applied in the regression analysis of leads and their activities  <cit> , it is generally used in the analysis of hts results for compounds with certain structural commonalities. however, when dealing with hundreds of thousands of compounds in a hts screening, the constitution of sar equations can be both complicated and impractical to describe explicitly.

molecular docking is another widely used approach to study the relationship between targets and their inhibitors by simulating the interactions and binding activities of receptor-ligand systems or developing a relationship among their structural profiles and activities <cit> . however, as it takes the interactions between the compounds and the target into consideration, it has been widely used for virtual screening other than to extract knowledge from experimental activities.

decision tree  is a popular machine learning algorithm for data mining and pattern recognition. compared with many other machine learning approaches, such as neural networks, support vector machines and instance centric methods etc., dt is simple and produces readable and interpretable rules that provide insight into problematic domains. dt has been demonstrated to be useful for common medical clinical problems where uncertainties are unlikely  <cit> . it has been applied to some bioinformatics and cheminformatics problems, such as characterizations of leiomyomatous tumour <cit> , prediction of drug response <cit> , classification of antagonist of dopamine and serotonin receptors <cit> , virtual screening of natural products <cit> .

in this study, we propose a dt based model to generalize feature commonalities from active compounds tested in hts screening. we utilized dt as the basis to develop the model because it has been successfully applied in many biological problems, and it is able to generate a set of rules from the active compounds which can then be used for filtering the untested compounds that are likely to be active in the biological system of interest. moreover, it has the capability to handle the arbitrary degree of non-linear structurally diversified compounds.

many elegant algorithms for building decision tree models have been introduced and applied in real life problems, and c <dig>  <cit>  is one of the best known programs for constructing decision trees. in this work, the dt based model was developed on the basis of the decision tree c <dig>  algorithm <cit> . the representation of the molecular structures is described by the pubchem fingerprint system. the dt based model was further examined by four assays deposited in the pubchem bioassay database, the hts assay for 5-hydroxytryptamine receptor subtype 1a  antagonists, hts assay for 5ht1a agonists, and two other assays with pubchem aid  <dig> and  <dig> for screening the hiv- <dig> rt-rnase h inhibitors. the results of 10-fold cross validation  over these hts assays suggest the self-consistency of the dt models. since a model simply provides the rules based on the profiles of active compounds in a specific hts assay, the computationally generated models were further examined using two hts assays which tested the same hiv rnase target, but used different compound libraries and were performed independently by two individual research laboratories. our results suggest that these developed models could be used to validate hts assay data for noise reduction and to identify hits through virtual screening of additional and larger compound libraries.

RESULTS
development of dt model
in this study, four dt models were developed for activity data contained in pubchem bioassay aid  <dig>   <dig>   <dig> and  <dig> respectively, where compounds were screened for various activities against several protein targets .

model fitting accuracies are used to examine whether the proposed data model can handle the complex data and whether the chemical fingerprint descriptors are sufficient for model development. as shown in table  <dig>  the model fitting accuracies of the four dt models were in the range of  <dig> % to  <dig> %. this suggested that the dt based models are able to fit the majority of the hts data for the model generalization. sensitivity and specificity of each developed model, which indicate the ratio of the active and inactive compounds that can be successfully learned by the dt model, are also reported in table  <dig>  the sensitivities of the models built for aid  <dig>  aid  <dig>  aid  <dig> and aid  <dig> are  <dig> %,  <dig> %,  <dig> % and  <dig> % respectively, while the specificities are all greater than 98%. the model fitting accuracies for both active and inactive compounds suggest strong feature-activity relationship among compounds tested in the hts screenings. the small fraction of misrecognized compounds might result from the hts data noise, the discrepancy of bioactivities observed from the compounds with same or similar chemical structures, or the competition with the overwhelming inactive compounds

tp = true positives, the number of correctly recognized active compounds;

fn = false negative, the number of active compounds that the model is unable to recognize;

tn = true negative, the number of inactive compounds that successfully recognized by the model;

fp = false positive, the number of inactive compounds that the model is unable to recognize.

as shown by the comparison of the sensitivity and its corresponding specificity for each individual dt model, the sensitivity is usually lower than the specificity and contributes less to the overall accuracy. one possible explanation is the existence of the data imbalance issue. among the hts data analyzed in this study, the ratio between the number of active compounds and that of inactives are ranged from 1:51~1: <dig>  thus comparing to the active compounds, the rules could be easily generalized for inactive compounds when the chance of pattern reoccurrence is higher. data imbalance problem is common in the high throughput screening assay data. one hts assay could have tens of thousands of compounds tested and only yield few dozens of hits. due to this problem, the specificity becomes less objective in performance evaluation. therefore, we also use the matthews correlation coefficient   <cit>  as additional measure to evaluate the model's performance.

mcc took both sensitivity and specificity into account and it is generally used as a balanced measure in dealing with data imbalance situation. as shown in table  <dig>  mcc values fall in the range of  <dig> ~ <dig>  for the four hts assays, which again suggests the satisfactory performance of the model training and indicates that the self recognition of the model is not random.

model validation by self-consistency test
the validation of the dt based models and self-consistency test were performed by 10-fold cross validation  method, in which the compound dataset tested in one hts assay was randomly split into  <dig> folds. these models were set up using  <dig> randomly selected folds, and prediction was done on the remaining fold.

the 10-fold cv results are given in table  <dig>  the overall validation accuracies of all dt models ranged from  <dig> % to  <dig> %. while the sensitivities of the models built for aid  <dig>  aid  <dig>  aid  <dig> and aid  <dig> are  <dig> %,  <dig> %,  <dig> % and  <dig> % respectively, where the specificities were  <dig> %,  <dig> %,  <dig> % and  <dig> % respectively. the more than 96% overall accuracies of the four dt based models suggest overall good performance and the cv analysis validates the reliability of the dt based models.

tp = true positives, the number of correctly recognized active compounds;

fn = false negative, the number of active compounds that the model is unable to recognize;

tn = true negative, the number of inactive compounds that successfully recognized by the model;

fp = false positive, the number of inactive compounds that the model is unable to recognize.

the sensitivity and specificity values given here represent the classification accuracies for the active and inactive compounds respectively. the sensitivity is lower than specificity to a certain extent. for example, the dt model for the 4hta antagonist activity data demonstrates  <dig> % sensitivity but  <dig> % specificity. from the evidence given in the previous section, imbalanced data, data noise and data discrepancy could again account for the lower sensitivities. moreover, as about 90% percent of the data used for training during the cross validations, the generalization ability of the active compound dataset became easily affected due to the limited sample size as compared to that of the inactive compound dataset.

the learning capability of the dt model could also be affected by the way the model was trained, such as the minimum count of compound instances required for a decision node. however, it primarily depends on the datasets used for training. although the imbalanced active and inactive compound datasets have an effect on the performance of the 10-fold cv, our results still show that the models are self-consistent. in addition, compounds and their activity data in hts screens are able to converge toward a discrimination model with encouraging accuracies. in addition, the mcc values ranged from  <dig>  to  <dig> , again indicating the potential of the models to identify potential hits.

application of dt models to select potential active compounds
in this study, independent evaluation of the dt based models was attempted by using two hts assays, pubchem aid  <dig> and  <dig>  which were aimed at identifying hiv- <dig> rt rnase h inhibitors.

comparison of the compound libraries of these two hts assays were first performed, demonstrating the extent of similarity of the active compounds between the two assays. by using tanimoto coefficient <cit>  as a measurement for the compound similarities, there are only six active compounds that were found to be similar with tanimoto coefficient threshold of 95%. this suggested the overlap of the active compounds in these two assays was very limited. it maybe of interest to investigate whether the dt model built with one compound set can be used to filter out hits identified with another assay where a different compound library was screened. to this end, dt models of these two hts assays were first developed independently and then each model was applied to classify the compounds screened by the other assay. an enrichment factor, which simply describes the proportion of active compounds from any given collection compared with randomly picked compounds  <cit> , was calculated as assessment for the classification performance of each model.

assume n compounds are tested in a hts assay sample where a compounds have been experimentally verified as bioactive. by virtual screening, which is the activity classification using dt model in this study, ns compounds are predicted as active and among these na belongs to the group of known bioactive compounds. a randomly picked sample will on average contain ans/n active molecules. therefore, the formula for calculating the enrichment factor is nan/nsa.

the enrichment factors for cross dataset prediction of hiv rnase h inhibitors of aid  <dig> and  <dig> are  <dig>  and  <dig>  respectively. from the virtual screening point of view, which is focusing on selecting the true hits while excluding the false positives as much as possible, the results suggest that the model derived from these two bioassays have certain generalization abilities to increase the odds of selecting true hits.

on the other hand, the sensitivities of the dt models based on data sets aid  <dig> and  <dig> are  <dig> % and  <dig> %, and the specificities are  <dig> % and  <dig> % respectively, which yield corresponding mcc value of  <dig>  and  <dig> , apparently the sensitivities and mcc values in this experiment are "poor" comparing to the cross validation study. this is not surprising, and indeed is well expected as the dramatic chemical structural differences between the two data sets , and the models derived from the individual datasets may carry overwhelming localization features that might not be largely applicable to each other. this also leaves the gap to be filled in for a robust statistical model, better representation of physical chemical properties, enlarged and diversified dataset, and enhanced quality of the experimental accuracy in the future.

nevertheless, this preliminary test using dt model as virtual screening technique yields encouraging enrichment for selection of active compounds when applied to another hts activity dataset. it suggests that, despite of the very low similarities between the active compounds from the two hts assays, certain common profiles of the active compounds can be extracted using the dt model, which can ultimately be very useful for virtual screening tasks.

CONCLUSIONS
in this study, we use derived dt models based on structural fingerprints of compounds to select biologically interesting compounds from hts assay dataset. four hts assays were analyzed to determine to what extent the designed models can be applied to the compound libraries of an unknown domain. our results suggest that the dt based models can be successfully used to derive common characteristics from hts data, and the models can serve as filters to facilitate the selection of compounds against the same target. these dt models could also be used to eliminate hts hits arising from data noise or those lacking statistical significance.

the development of the model is a learning process. thus, the potential of the developed model is limited to the known active compounds and the properties used for training, and limited to the distribution of the compound collection to which the model is applied. with the growth in the number of compounds to be screened and the improvement over data quality produced with hts assay, a more robust model could be developed with increased ability for selecting biologically interesting small molecules from a diverse compound library.

