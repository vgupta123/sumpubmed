BACKGROUND
genomes of related organisms have been shown to share long tracts of homologous dna sequence  across species  <cit> . during the course of evolution, large-scale genome rearrangements of chromosomes shuffle the order of such homologous segments. some years ago, a controversy erupted in genome rearrangement studies over whether rearrangements are likely to recur in regions known as rearrangement hotspots  <cit> . pevzner and tesler  <cit>  inferred the existence of such “fragile sites”, from high values of their breakpoint reuse rate . an argument by sankoff and trinh  <cit>  intended to show that loss of breakpoint usage information occurs via synteny block removal, and leads to the inference of an artificially inflated breakpoint reuse rate. that argument was criticized by peng et al. <cit>  for its flawed synteny block generation method. subsequently, by devising a new approach for computing the brr, bergeron et al. <cit>  observed that rearrangement scenarios maximizing operations involving rearrangements of telomeres using only single cuts could result in much lower rates of reuse. this approach resulted in bounds for the human-mouse transformation which not only demonstrated that much lower rates are possible, but also that the brr is extremely sensitive to genome representation and rearrangement model.

in the current paper, we reconsider aspects of the original breakpoint reuse debate. we address a theme that david sankoff suggested in his  <dig> plos commentary  <cit>  was not yet fully confronted. in that commentary, sankoff argued that high values of inferred breakpoint reuse may result from noise introduced by imprecise synteny block construction rather than due to actual genome rearrangements in the course of evolution. imprecise synteny block construction or other processing might effectively randomize the information needed to reconstruct rearrangement history. while others use a variety of methods to explore the level of this "randomization", such as distance in xu et al. <cit> , the number of conserved adjacencies  <cit> , or the structure of common intervals  <cit> , we will use the cycle structure of the adjacency graph as an indicator of "randomization". we examine how the scale of synteny block resolution affects breakpoint reuse.

to this end, and to continue the discussion of whether elevated values of breakpoint reuse can be used to infer that genome rearrangements repeatedly strike the same “hot spots” during the course of mammalian evolution, we evaluated the traditional breakpoint reuse statistic using actual data, in the context of rearrangement model and resolution for two data sets: 3-way synteny blocks for human-mouse-rat constructed by bourque et al. <cit> , and a series of blocks generated by the mauve genome alignment system  <cit>  with resolutions ranging from fine-scale to coarse.

genome rearrangment transformations
the rearrangement transformation between two genomes with no insertions, deletions or duplications can be specified by the connections between corresponding syntenic segments in the two genomes. we call such segments genes even if they don't actually consist of single genes. the adjacency graph introduced by bergeron et al. <cit>  is an elegant representation of the genomic transformation. vertices of the adjacency graph are either the connections or adjacencies between two gene ends in the initial and target genomes, or solo telomeric gene ends at the ends of chromosomes. the complete set of adjacencies {{a <dig> a2}, {a <dig> a4}, …, {a2n- <dig> a2n}} specifies a genome of n genes. a breakpoint represents a disruption of their order, an adjacency of two gene ends associated in one genome but not the other.

a number of methods have been used to evaluate the minimum number of genome rearrangement events, known as rearrangement distance, of a genome transformation; we will focus on two: the hannenhalli and pevzner "hp" formulation  <cit>  which involves generalized inversions, , translocations, fissions and fusions), and the double cut and join   <cit>  which includes simple transpositions  and generalized transpositions  in addition to the operations considered by hp.

in the adjacency graph , adjacencies of one genome are connected to those of another by lines representing their common gene ends. followed continuously these lines resolve the adjacency graph into alternating closed paths called cycles and other continuously connected open paths. the ends of open paths belong to gene ends at the ends of chromosomes called telomeres. in our formulation of the dcj paradigm all open paths are closed by a capping procedure described more extensively in  <cit>  ; essentially, adjacencies containing telomeric gene ends of chromosomes are "capped" by artificial gene ends called "endcaps". paths that start and end on the same genome are closed by a double capped “null chromosome” on the opposite genome; paths starting and ending in different genomes are closed by connecting the two capped ends.

semantics, conventions, and cycle nomenclature
due to the addition of null chromosomes, when paths are closed into cycles, both genomes artificially contain the same number of chromosomes. some dcj operations involve "artificial cuts" between a telomeric gene end and an endcap. in other formulations  <cit> , paths are not closed hence such artificial operations do not occur. another outcome of the device of introducing caps and closing paths is the consequent increase in breakpoints. some formulations  <cit>  count only "internal breakpoints" which do not include "external breakpoints" such as capped telomeric gene ends, or null chromosomes. in our approach we include all breakpoints, internal and external; hence telomeric adjacencies are counted.

in a departure from the usual convention in the genome rearrangement literature, we call a cycle containing j adjacencies in only one genome a j-cycle as this corresponds better to the analogous cycles for unsigned permutations. usually it is called a 2j-cycle. in our formulation, 1-cycles are not counted in the dcj distance  as they are the identity transformation of a single adjacency transforming to itself; the cycle count "c" counts 2-cycles  or higher.

dcj operation, distance, and comparable hp distance
the double cut and join , is a universal operation that subsumes many biological rearrangement operations.to resolve the transformation, a dcj is performed by breaking connections in two adjacencies in the current genome and swapping gene ends so that at least one of the resulting adjacencies exists in the target, decreasing the distance by  <dig>  performing a single dcj decreases the distance by  <dig>  each cycle can be resolved independently by dcj. for the jth cycle containing bj breakpoints, this results in dj = bj –  <dig> steps. as all quantities are additive over individual cycles, the total dcj distance can thus be obtained from a cycle decomposition of the adjacency graph. that is, , , and , where the sums are over cycles. hence, the resulting dcj distance between any two genomes can be expressed as:

ddcj = b – c

where b is the number of breakpoints and c the number of cycles  <cit> . in the hannenhalli and pevzner formulation which is based solely on generalized inversions, the distance is:

dhp = b – c + h + f

where b and c  are as above, and h and f are respectively hurdles and fortresses, positive permutations which don't contain any inversions by dcj, or require "extra inversions" to resolve by hp, and are generally rare  <cit> . the difference in the distance between the two formulations, ddcj – ddcj = h + f, consists of these relatively rare obstructions. figures 1b and 1c illustrate how hurdles increase distance in the hp over the dcj paradigm.

definition of the classical inversion-based breakpoint reuse rate 
in considering an “undisturbed” stretch of genome, performing a reversal creates at most two breakpoints at each step. pevzner and tesler  <cit>  defined the inversion based breakpoint reuse rate  as r =2d/b, with d the genomic distance, and b the number of breakpoints. this "traditional" reuse statistic is  <dig> for reversals as long as no breakpoints are reused. the cycle distribution of such a transformation consists entirely of 2-cycles in our nomenclature  such as in figure 1a. if breakpoints are reused, the statistic can be as high as  <dig> 

brr for  transpositions is model dependent
to see the effect on the genome for these operations, figures 1b and 1c show adjacency graphs superimposed on their genome graphs for a simple transposition and a block interchange . both are 2-step dcj operations which exchange two segments in a genome by creating an intermediate circular. the simple transposition reuses the same cut in the circular whereas the bi does not. a simple transposition is a special case of a block interchange. as both operations involve the creation and reabsorption of circular intermediates  they are considered generalized transpositions

a breakpoint graph <cit>   is a representation dual to the adjacency graph such that vertices in the adjacency graph, are lines or arcs in the breakpoint graph. horizontal lines  are typically adjacencies in the current genome, while arcs are "desired" adjacencies in the target. vertices in the breakpoint graph correspond to gene ends, and are lines in the adjacency graph. particularly for block interchanges, the creation of ci, is not easily discerned from the adjacency graphs as they look like graphs for a pair of overlapping 2-cycles which seem deceptively like inversions. breakpoint graphs maybe more informative for identifying transpositions.

to see why this is so, we note that in the breakpoint graphs for the simple and generalized transpositions, the arcs are "unoriented" unlike arcs composing the breakpoint graph of the inversion in figure 1a, which are "oriented". oriented arcs have their black lines pointing in the same direction relative to the arc, whereas unoriented arcs both point away or towards the central arc. an arc's orientation is only defined for arcs rooted in the same chromosome, otherwise we consider them non-oriented. dcj performed on oriented arcs produce inversions, while dcj performed on unoriented arcs produce ci. in order for ci to be reabsorbed, the arc about which we perform the dcj must overlap others in the breakpoint or adjacency graph.

as shown in the legend for figures 1b and 1c, cycle structures resulting in generalized transpositions in the dcj paradigm do not result in the same distance in the hp paradigm, in which only generalized inversions can be performed and not the creation of a ci. as a result, not only do the distances differ, but also the inferred brr. we note that while figure 1b shows an unoriented 3-cycle, 3-cycles also exist which contain oriented arcs. such 3-cycles can be resolved by inversions. for oriented 3-cycles, dcj and hp distances agree as does breakpoint reuse.

random permutations, cycle structure, and brr for longer cycles
if we continue performing reversals indefinitely in the same stretch of genome, eventually we will start to reuse breakpoints . consequently, the cycle structure will change. each reuse of a breakpoint increases the length of the cycle in which that breakpoint appears. longer and longer cycles appear which continue growing in number and length. the resulting permutation will exhibit increased breakpoint reuse. we see this by computing the inversion based breakpoint reuse for a longer cycle having n breakpoints. the dcj distance for a cycle containing n breakpoints is n- <dig> and, as we have seen, is identical to the hp distance unless there are hurdles and fortresses. the inversion based brr statistic is therefore rlongcycle = 2d/b = 2/n, which approaches  <dig> as n gets large. since distance is additive over cycles, and so are breakpoints, for transformations containing n such cycles the total brr becomes, rlongcycle = 2n/nn = 2/n →  <dig> for large n.

as more and more random operations are performed between two genomes, they become increasingly divergent and randomized relative to one another. this affects the cycle structure. considering results from a paper on random signed gene order permutations by sankoff and haque  <cit> which constructed cycles of the breakpoint graph of random permutations, richard friedberg communicated results on cycle structure to us which we discuss in the next section. in particular, he shared a result for the expected number of j-cycles in the random edge graph for transformations by dcj for genomes containing only circular chromosomes. as gene number per chromosome gets large, we surmise results for the unrestricted circular case  coincide with those for the restricted linear case .

cycle structure of random unsigned permutations
before discussing the signed permutation case in the next section, we first consider the cycle structure of unsigned permutations . the results for the unsigned permutation case have already been reported  <cit>  . these formulas are analogous to those for signed permutations. in the unsigned case, the expected number of j-cycles is:   

to compare this later with the result for signed permutations, we write it as:  

we can get the expected number of cycles directly by summing eq  over j:   

unfortunately the analogous formula for bipairings  does not have the simplicity of eq , so we cannot accomplish this as easily for signed permutations. nor can we compute c by summing an approximate formula because its error becomes large when j is near n, no matter how large n is. fortunately, there is an indirect way of deriving eq  bypassing eq  so that c is found without finding the individual values of qj and the result agrees with eq . this indirect way has the merit that it also yields the variance of c, which cannot be inferred from eq . with a slight modification this method also works for bipairings. it yields:

<c> =  <dig> + 1/ <dig> + 1/ <dig> + … 

now these individual terms do not correspond to the formula for signed permutations, eq  below. for small j the jth term of  is larger than qj, but for j near n it is smaller. to solve for  one can write:

<c3> = <c> –  

true for both  problems. in the unsigned permutation problem this simplifies to

<c3> = 1/ <dig> + 1/ <dig> + 1/ <dig> + …

which has no analogue for bipairings. to evaluate  for bipairings one must use  for <c> and approximate formulas for the equivalent terms <q1>, <q2>, <q3>.

the cycle structure of random signed genome permutations
in the case of signed genome permutations, 2n gene ends can be paired into n adjacencies in !! different ways where k!! is a double factorial and is equal to k…k . the comparison of two such pairings permits a cycle structure to be constructed. for an adjacency graph with n adjacencies in each genome, let qj be the number of j-cycles and  the total number of cycles.

• then for such a random adjacency graph:  

for large n and small j the asymptotic limit of equation  is:  

which can be compared to the analogous result for unsigned permutations, eq .

thus for signed permutations:

• the expected number of cycles, by the indirect method mentioned previously is:  

where γ=euler’s γ=  <dig> . hence, for large n we have that <c> is given approximately by:  

• the expected number of cycles of length > j is:

<cj>=<c>– <q1> – <q1> –…–<qj>

which can be evaluated from  and  and approximately from  and .

we wish to know <c3>. thus, for large n,

<c3> = <c> –    

methods
summary of our approach
to explore whether randomization of genome transformations occurs beyond that due to actual evolutionary events recorded in the "signal in the genome" we have adopted the traditional breakpoint reuse statistic. we feel this statistic serves as an indicator of overall apparent breakpoint reuse and best reflects the approach followed by the pioneering paper of pevzner and tesler . towards this end, we consider the traditional inversion-based breakpoint reuse rate in three data-based computational experiments discussed in the next three sections. we calculated brr for the following :

1) a series of mauve synteny block sets at resolutions ranging from fine to coarse

2) the  <dig> kb bourque et al  h-m 3-way h-m-r blocks with block removal

3) the 141min lcb mauve blocks  before and after block removal

generating synteny blocks with mauvealigner
we applied the "mauvealigner" from the mauve genome alignment system  <cit>  to search for high-resolution 3-way locally collinear blocks  among the genomes of mouse, rat, and human. mauve uses a seed-and-extend local multiple alignment method to identify high-scoring local alignments, which it then clusters into locally collinear blocks—groups of matches in the same order and orientation among each genome. we assign each locally collinear block  a weight w equal to the sum of lengths of the constituent ungapped local alignments. the weight of a block w can be formally defined as , where a block b consists of one or more ungapped local alignments m, and length is the number of nucleotides covered by m. in the context of mauvealigner, the matches in a block must be in the same order and orientation in all genomes. that is, blocks are internally free from rearrangement.

because mauvealigner generates local alignments in unique regions of sequence only, it typically generates few lcbs containing only repetitive regions, and any lcbs generated in such regions tend to be small, having a low weight. mauvealigner filters out those spurious lcbs by iteratively removing low-weight lcbs in a process called greedy breakpoint elimination  <cit> . greedy breakpoint elimination is analogous to iterative block removal using w to give an ordering for blocks to be removed. at each step in the process, the lowest weight lcb is removed. if one or more rearrangement breakpoints have been eliminated, any surrounding lcbs may coalesce with each other. the weight of a coalesced lcb is equal to the sum of weights of lcbs that were joined.

in the present work, the repeatmasked assemblies of human , mouse , and rat  were searched for unique 3-way seed matches on the forward and reverse strands using the palindromic seed pattern: 11111*111*11*1*11*111* <dig>  <cit> . initial seed matches were maximally extended in each direction until the seed pattern no longer matches at any overlapping position. a total of  <dig>  ungapped 3-way local alignments containing unique sequence resulted. these initial matches have a minimum length of  <dig> . the initial set of 3-way matches gave rise to  <dig>  lcbs, to which we applied greedy breakpoint elimination to remove all lcbs up to a minimum weight of  <dig>  yielding a baseline set of  <dig>  3-way matches that compose  <dig> lcbs. the minimum sequence alignment length among these blocks is  <dig> nucleotides. we further applied greedy breakpoint elimination to the baseline set of  <dig>  lcbs, recording the observed genomic permutation at each successively higher lcb weight up to a minimum weight of  <dig> . at minimum weight  <dig>  , there are  <dig> 3-way lcbs among the mouse, rat, and human genomes. at weights larger than  <dig>  the lcb weight is approximately proportional to the overall chromosomal span of an lcb, with chromosomal span 100-1000x the lcb weight. the data set is included as additional files 2345678910

calculating genomic distance by grimm and dcj
the hp genomic distance for the mauve synteny block sets constructed at each successive minimum lcb weight was computed using the grimm  <cit>  server. we computed the dcj  <cit>  distance using a c++ program which inputs the genome as a signed permutation  as the grimm server does, and outputs the dcj distance, the number of breakpoints, the number of cycles of each kind and their total, and the number of null chromososmes.

RESULTS
onset of transpositions for human-mouse  rearrangements
we found that the hp and dcj distances agree identically at low resolution but at high resolution, the grimm distance exceeds the dcj distance. in figure  <dig>  we display these distances on a log scale so as to simultaneously display the difference between grimm  and dcj values. the maximum grimm  value is  <dig> while the maximum value of the dcj distance is  <dig>  the maximum difference between these two distances is therefore relatively small, only  <dig> or  <dig> % of the total  distance at highest resolutions.

we assume the differences we found between grimm  and dcj distances are due to hurdles, since dgrimm - ddcj = dhp - ddcj = h +f where fortresses, in breakpoint graphs containing an odd combination of hurdles, are extremely rare. the hurdles in our human-mouse  transformations may be due to simple transpositions and generalized transpositions  as in figures 1b and 1c, which differ in hp and dcj distances as shown. gt onset in figure  <dig> occurs at min lcb weights  <dig> for h-m corresponding to a dcj distance of  <dig> and grimm of 294

brr for human-mouse mauve blocks at different resolutions
the traditional inversion based breakpoint re-use rate, 2d/b, where d is the genomic distance and b the number of breakpoints  <cit>  was computed for both grimm  and dcj distance measures on each set of mauve synteny blocks. the two distances are identical for most of the min lcb weight range represented by mauve block sets , and the calculated breakpoint reuse rate would also be the same in that range if the number of breakpoints agreed for both grimm and dcj distances. however, to follow the  <dig> paper of pevzner and tesler in pnas  <cit> , we used "internal" breakpoints to calculate the grimm breakpoint reuse rate. for the dcj brr calculation we used "internal + external" breakpoints.

even though the dcj and hp distances are largely identical over most of the range, the graphed values of brr we present in  using the hp and dcj distances we evaluated are not the same. they run essentially parallel to each other. their difference is not due to the difference in distance for the most part , but rather is due to the manner of evaluating breakpoints.

our maximum values for the brr range from  <dig>   to  <dig>   depending on whether the number of breakpoints was evaluated using external breakpoints in addition to internal breakpoints  or only internal breakpoints . this variation is consistent with the range of values for brr in  <cit>  .

the lowest brr we observed is  <dig>  , occurring in mauve synteny blocks with min lcb weight  <dig>  these blocks have an average chromosomal length of  <dig>  mb, a grimm distance of  <dig>  and the total number of blocks is  <dig>  increasing the min lcb weight from here , we observe a dramatic rise in both breakpoint reuse curves, which reach their maximum at min lcb weight  <dig> . the hp brr then plateaus and stays in the same range until min lcb weight  <dig> . it remains over  <dig>  for min lcb weights up to  <dig>  which have average block size  <dig>  mb, for a total of  <dig> blocks, and a grimm distance of  <dig> 

dependence of brr and cycle structure on resolution
to understand the variation in the breakpoint reuse measures based on synteny block resolution, we calculated the cycle structure for the human mouse transformation on a series of progressively increasing resolution mauve synteny blocks. figure  <dig> shows the behavior of the breakpoint reuse measures along with the proportion of cycles for  <dig>   <dig> and > <dig> cycles for human-mouse as they vary with lcb weight.

to interpret how these fractional cycle curves affect the brr, we wish to express the breakpoint reuse rate brr r for the entire transformation as a weighted sum over the brr of individual cycles rj, that is,   

where we wish to determine the coefficient cj in such a way as to involve the fractional distributions over kinds of cycles. since the total breakpoint reuse is r = 2d/b, where both the total distance and the total number of breakpoints are summed over cycles, that is  and  hence the coefficient for equation  is:

cj = bj/b.

however, the sum in equation  is over individual cycles, and not kinds of cycles. to accomplish this we group cycles of the same kind together. if nk is the number of k-cycles the new sum over kinds of cycles, ie 2-cycles, 3-cycles, 4-cycles, etc is:  

where bk is the number of breakpoints in a k-cycle. expressing the coefficient in fractions of cycles,  with the fraction of k-cycles =fk=nk/Σnk. and the sum over nk, that is, Σnk=c, the total number of cycles, we arrive at a weighted sum over different kinds of cycles:  

where bk is the number of breakpoints in a k-cycle, fk the fraction of k-cycles and rk is the brr of a k-cycle, or rk = 2dk/bk = 2/k.

resolution based cycle structure for human-mouse
in figure  <dig> we represent the dependence of the cycle structure on min lcb weight. figure 4a shows the dependence of the cycle structure over the full range of min lcb weight. figure 4b, shows the high-resolution end. at high resolution, the dependence of brr bottoms at min lcb weight  <dig> approximately where the proportion of 2-cycles  is at a maximum and the proportion of >3-cycles  are at minimum. at the minimum of breakpoint reuse, 64% of the cycles are 2-cycles, 23% are 3-cycles and  <dig> % are cycles of length greater than  <dig>  at highest resolutions  a rise in the 3-cycle and >3-cycles fraction and corresponding drop in the 2-cycles produces a rise in brr.

in the plateau, nearly half  of the cycles are 2-cycles,  <dig>  to  <dig> % of the cycles are  <dig> cycles, and  <dig> to  <dig> % of the cycles are of length greater than  <dig>  the initial rise to the plateau from the minimum of breakpoint reuse corresponds to a rise in the percentage of 2-cycles, a decline in the percentage of 3-cycles and in the percentage of cycles of length greater than  <dig>  the decline in breakpoint reuse as the min lcb weight grows is due to a rise in the percentage of 2-cycles from  <dig> % at minimum lcb weight  <dig> to 67% at lcb weight  <dig> while the percentage of greater than 3-cycles declines from  <dig> % to  <dig> %.

the decline in breakpoint reuse as min lcb weight grows past  <dig> is due to finite chromosome length. entire chromosomes can be spanned by mauve synteny blocks at their lowest resolution. the average length of blocks with highest min lcb weight  is  <dig>  mb, almost the size of the smallest  chromosome,  <dig>  mb. at such resolutions, the brr diminishes, the percentage of 2-cycles rises and that of > <dig> cycles declines.

minimum of brr and cycle structure
the region around the minimum of brr is very interesting: near min lcb weight  <dig>  the fraction of 2-cycles peaks  and the fraction of > <dig> cycles is at a minimum at a slightly lower min lcb weight than the min of brr. it is in this overall region that the transformations are least complicated, and the "signal in the genome" is best preserved. there are mainly "simple generalized inversions" since we are simultaneously at the maximum of the 2-cycles fraction and at the min of the > <dig> cycles, hence at the smallest fraction of long cycles, although there is a caveat in that longer cycles have correspondingly higher weights. we next compare our computed cycle structures for human-mouse at different resolutions with the previously derived predictions of cycle structure for random permutations.

low resolution cycle structure approaches random permutation
we compared the total number of > <dig> cycles in the human-mouse transformation to the expected distribution for a random permutation having the same number of synteny blocks derived earlier. as resolution decreases, e.g. with increasing min lcb weight , the number of > <dig> cycles of the transformation approaches the theoretical distribution derived earlier for this number in a random permutation having the same number of synteny blocks.

systematic block removal for the  <dig> kb h-m bourque et al. blocks
bourque et al.  <cit> , constructed 3-way synteny blocks for human, mouse and rat, using grimm-syntenywhich can be accessed online  <cit> . grimm-synteny preserves information about microrearrangements within synteny blocks, dividing the blocks into “micro” and “macro” by a choice of parameters. choosing  <dig> kb as their cutoff, they arrived at  <dig> blocks for human-mouse.

we applied the following protocol of successive block removal on these  <dig> kb “bourque” blocks. blocks were removed in a stepwise fashion, starting with the smallest blocks. remaining blocks were concatenated if they appeared in the same orientation and order in both genomes. although our approach follows the spirit of the sankoff-trinh  <cit>  block removal procedure, in their approach, blocks were removed from simulated genomes. ours used a set of blocks that were derived from real data. theirs were generated so as to have no breakpoint reuse. after removing blocks, the sankoff-trinh procedure amalgamated the proximal blocks in a manner subsequently shown to be faulty  <cit> . since their data was simulated as a permutation file, it had no actual size. since we used real data, the blocks we used have size and orientation.

we calculated breakpoint reuse after each set of blocks was removed, and plotted the resulting curves versus the size of blocks removed . brr was calculated using grimm  <cit>  via the hannenhalli and pevzner algorithm  <cit>  and via dcj  <cit>  with breakpoints evaluated as previously discussed. as before, the differences in these values of breakpoint reuse are not significant, but are mainly due to the different methods of counting breakpoints . the change in breakpoint reuse with block removal  for the bourque et al. blocks is not significant. as the breakpoint reuse rates do not change significantly with block removal, we hypothesize that this situation is similar to the plateau region in figures  <dig> and 4a and 4b where the genomic permutation is "randomized". in the next sections we perform the same block removal procedure on the mauve blocks most closely corresponding to the  <dig> kb bourque et al. blocks.

corespondence between mauve blocks and "bourque-et al." blocks
no direct correspondence can be made between the mauve synteny blocks and those of bourque et al  <dig>  since mauve blocks and the bourque et al. 3-way synteny blocks for human mouse rat were generated from different genome assemblies. by comparing various parameters including grimm and dcj distance, breakpoints, and breakpoint reuse rate , we derived a rough correspondence ; the mauve blocks with min lcb wt  <dig> most closely correspond to both the  <dig> kb and  <dig> mb bourque et al. blocks. we call these the bourque et al.-like mauve blocks.



brr for the bourque et al.-like mauve blocks
we repeated our stepwise block removal protocol for the bourque et al.-like mauve blocks , since after block removal these mauve blocks most closely resemble the  <dig> kb bourque et al. blocks. in addition, blocks at min lcb weight  <dig> have the minimum observed value of brr . although both grimm and dcj brr curves initially rise with block removal for the min lcb weight  <dig> mauve blocks, after all blocks smaller than  <dig> kb are removed, the curves enter a plateau .

our prior experience with the sets of mauve blocks at different resolutions suggests that when enough smaller size blocks are removed, the remaining blocks approach a more randomized cycle structure. the brr curves in figure 6a for the mauve blocks after > <dig> kb blocks have been removed resemble those for the bourque et al. blocks  although the brr values do not match exactly. the existence of the plateau suggests the bourque et al. blocks have lost rearrangement information encoded by true rearrangement breakpoints and are approaching a random permutation.

> <dig> cycles for bourque et al.-like mauve blocks with block removal
in figure 6b we note that the divergence between the > <dig> cycle numbers for actual vs expected number of cycles for a random permutation is greatest when no blocks are removed and decreases with the proportion deleted. although the two curves in figure 6b never merge, the number of cycles greater than  <dig> approaches the curve depicting the expected number of > <dig> cycles lending credence to the notion that the permutation becomes increasingly randomized as blocks are removed.

discussion
alternative measure for breakpoint reuse
the work presented here uses the classical definition of breakpoint reuse rate. in a different approach, bergeron et al.  devised a new way of calculating the breakpoint reuse rate, brr, and showed that by this definition the brr is intimately connected to particular rearrangement scenario and model. they redefined the breakpoint reuse rate as:

r = c/b

where c is the total number of cuts made by the operations of the scenario, and b the number of b-vertices  in long cycles or paths. in methods that force an artificial closure of paths ending in telomeric adjacencies, and an equalizing of chromosome number resorting to use of null chromosomes , there is no biological basis for cuts performed between caps and gene ends or between caps and caps in null chromosomes. the traditional breakpoint reuse measure can double-count the number of actual cuts performed for each dcj in specific scenarios, leading to a severe overestimate of the brr. bergeron et al.  followed up on this insight by devising a number of ingenious manoeuvres to find scenarios that either maximize or minimize their statistic. for some long paths it is possible to decrease the number of cuts by a factor of two, thereby radically diminishing the effective breakpoint reuse. by the new definition, the value of the breakpoint reuse rate can become less than  <dig>  achieving the following bounds for human-mouse:

 <dig>  ≤ r ≤  <dig> 

although the variation we achieved is not quite as dramatic, as that obtained by bergeron et al. we did attain nearly half this variability just by changing the resolution of the synteny blocks.

in another work on this subject  <cit> , sinha and meller investigated the relationship between brr and synteny blocks using a simple approach to synteny block aggregation depending on two principle parameters: the maximum gap  between adjacent blocks to be merged, and the minimum length  of synteny blocks. they found that the classical breakpoint reuse rate was almost constant for different data sets and a wide range of parameters, which roughly corresponds to our results for brr in the plateau region . their work did not investigate synteny blocks of the high resolution presented here. although they do not report the actual chromosomal span of their smallest blocks, the block set they analyzed with the highest synteny block count is generated by grimm and, having  <dig> blocks, has much fewer than our highest resolution dataset which has about  <dig>  like us, they also found that brr was strongly correlated with hp distance, increasing with more divergent genomes. to compare our results for brr with theirs, we calculated the minimum, maximum, mean and standard deviation of the brr over the range in figure  <dig> as well as for regions corresponding to the rise in the graph , the plateau , and the fall . the results are summarized in table  <dig> 

our results largely agree with those of sinha and meller for the plateau region, however, sinha and meller did not arrive at values of breakpoint reuse varying as much as ours did overall. our breakpoint reuse rate varied nearly  <dig> % of the maximum value.

CONCLUSIONS
high values of breakpoint reuse have been used to justify the existence of fragile sites in genome rearrangement scenarios. although fragile sites may well exist in the course of mammalian genome evolution, we argue that computed high values of the "traditional" breakpoint reuse statistic do not yield conclusive evidence for the existence of such sites. rather, as we have shown, the cycle structure for such high brr transformations these genome transformations are more like random permutations. small synteny blocks may contain critical information about rearrangement history. as small synteny blocks are lost, either by diminished resolution or block removal, the numbers of > <dig> cycles increasingly approach the expected values for a random permutation. for the bourque et al.  <dig> kb blocks, block removal did not diminish brr. in an experiment in which we best matched the bourque blocks with mauve  we showed that the behavior of brr curves of the mauve blocks upon block removal also had flat levels once all blocks spanning less than  <dig> kb were removed. we posit that blocks spanning less than  <dig> kb, which are missing from the bourque et al. dataset, may encode vital information about the true rearrangement history and associated parameters. the distribution of the number of cycles > <dig> corroborates our suggestion that information is lost either with diminished resolution or with systematic block removal starting with the smallest blocks. finally, because the grimm and dcj distance was not significantly different for a great portion of the resolution range, the difference in rearrangement models did not come into play except at the highest resolutions. at these resolutions, evidence for transpositions exists but they comprise less than 2% of all rearrangements.

an implication of our study is that precise definition of synteny blocks both large and small is crucial for accurate inference of rearrangement history parameters. small blocks matter. although larger blocks can be predicted more reliably, homology can be confidently predicted even for small regions spanning less than  <dig> nucleotides using blast statistics. probabilistic methods for synteny block reconstruction  <cit>  can be used to assign a confidence value  to blocks large and small. future work might investigate the relationship between filtering blocks using such confidence estimates and rearrangement parameters such as breakpoint reuse, cycle count distributions, and others.

finally, even though we suggest that the breakpoint reuse rate may be lower than previous estimates, we note that our findings do not preclude the existence of chromosomal regions with an unusually large number of closely spaced rearrangement breakpoints or "fragile regions". even if breakpoint reuse is low, breakpoints might cluster near each other on the chromosome. indeed this could be a natural consequence of nadeau and taylor’s work: if breakpoints are selected uniformly at random along a genome, the inter-breakpoint distances will be geometrically or exponentially distributed , and clusters of nearby breakpoints may exist purely by chance. however, pevzner and tesler  <cit>  inferred an excess of short distances exist between breakpoints over the expected exponential distribution in the random model, concluding this implied the existence of fragile regions solely based on elevated values of their breakpoint reuse statistic. even though we contend this argument may not hold as we have shown the breakpoint reuse statistic depends significantly on the resolution scale used in the analysis, other investigators have shown the existence of breakpoint clusters  <cit> .

list of abbreviations
bi: block interchange; brr: breakpoint reuse rate; ci: circular intermediate; dcj: double cut and join; gt: generalized transpositions; h-m: human-mouse; h-m-r: human-mouse-rat; hp: hannenhalli-pevzner; kb: kilobase; lcb: locally collinear block; mb: megabase.

competing interests
aed served as co-editor for the supplement, but was not involved with the review of this paper. oa and sy declare that they have no competing interests.

authors' contributions
sy conceived the idea of using mauve and the dcj algorithm to investigate the behavior of brr based on resolution using real data. sy and aed designed the approach using mauve for this purpose. aed generated the 3-way locally collinear synteny blocks for human-mouse-rat genomes using mauve. sy and oa designed and implemented the dcj program. oa wrote perl and shell scripts, and r programs to process and analyze the data. sy, oa and aed analyzed the results. sy, oa and aed wrote and edited the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
appendix on capping examples for the dcj this appendix provides illustrative examples on how capping is performed in our version of the dcj paradigm.

click here for file

 additional file 2
permutation file these files contain the permutations for the human-mouse-rat collinear synteny blocks which are output by mauve.

click here for file

 additional files 3
lcb files these files contain the lcbs for the human-mouse-rat collinear synteny blocks files which are output by mauve. in addition, there are three files giving the lengths of the chromosomes of each genome.

click here for file

 additional files 4
lcb files these files contain the lcbs for the human-mouse-rat collinear synteny blocks files which are output by mauve. in addition, there are three files giving the lengths of the chromosomes of each genome.

click here for file

 additional files 5
lcb files these files contain the lcbs for the human-mouse-rat collinear synteny blocks files which are output by mauve. in addition, there are three files giving the lengths of the chromosomes of each genome.

click here for file

 additional files 6
lcb files these files contain the lcbs for the human-mouse-rat collinear synteny blocks files which are output by mauve. in addition, there are three files giving the lengths of the chromosomes of each genome.

click here for file

 additional files 7
lcb files these files contain the lcbs for the human-mouse-rat collinear synteny blocks files which are output by mauve. in addition, there are three files giving the lengths of the chromosomes of each genome.

click here for file

 additional files 8
lcb files these files contain the lcbs for the human-mouse-rat collinear synteny blocks files which are output by mauve. in addition, there are three files giving the lengths of the chromosomes of each genome.

click here for file

 additional files 9
lcb files these files contain the lcbs for the human-mouse-rat collinear synteny blocks files which are output by mauve. in addition, there are three files giving the lengths of the chromosomes of each genome.

click here for file

 additional files 10
lcb files these files contain the lcbs for the human-mouse-rat collinear synteny blocks files which are output by mauve. in addition, there are three files giving the lengths of the chromosomes of each genome.

click here for file

 acknowledgements
we are grateful to richard friedberg for discussions illuminating breakpoint re-use and the cycle structure of genomes and for providing the theoretical predictions for cycle structure as well as the number of cycles >  <dig> for random permutations. we thank john sikorski for writing the perl scripts to run grimm and for help with data analysis. we thank glenn tesler for clarifying issues around internal vs external breakpoints as well as the differences in breakpoint reuse between the dcj and hp formulations. we also thank our reviewers for their constructive criticisms and suggestions. oa is supported by simon daefler; sy thanks nicholas chiorazzi, md for support.

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2011: proceedings of the ninth annual research in computational molecular biology  satellite workshop on comparative genomics. the full contents of the supplement are available online at http://www.biomedcentral.com/1471-2105/12?issue=s <dig> 
