BACKGROUND
the lack of a suitable means for formally describing the semantic aspects of omics investigations presents challenges to effective information exchange between biologists  <cit> . the inherent imprecision of free-text descriptions of experimental procedures hinders computational approaches to the interpretation of experimental results. controlled vocabularies and/or ontologies can be used as a means of adding an interpretative annotation layer to the textual information  <cit> . a controlled vocabulary  is a structured set of terms  and definitions agreed by an authority or a community. an ontology includes cv terms to refer to concepts at the linguistic level, but also utilises a richer semantic representation to characterise the ways in which these concepts are related  <cit> . many scientific communities, including those operating in the metabolomics domain  <cit> , have started developing ontologies for data annotation  <cit> . the metabolomics standards initiative   <cit>  ontology working group   <cit>  has been appointed to establish a common semantic framework  for metabolomics studies to be used to describe the experimental process consistently, and to ensure meaningful and unambiguous data exchange  <cit> . while providing a mechanism for coherent and rigorous structuring of domain-specific knowledge, it is necessary for ontologies and cvs in an expanding domain such as metabolomics to be easily extensible. the new knowledge, largely generated by high-throughput screening, is communicated through the biotechnology literature, which can be exploited by text mining  tools to facilitate the process of keeping ontologies and their cvs up to date  <cit> . in this article we describe a tm approach for rapidly expanding a set of cvs maintained by the msi owg with terms extracted from the scientific literature, following initial term acquisition from sources such as domain specialists, literature, databases, existing ontologies, etc.

the msi owg  <cit>  aims to develop a set of ontologies and cvs in metabolomics as a direct support to the activities of other msi wgs  <cit> , which are responsible for: biological context metadata, chemical analysis, data processing and exchange formats. the coverage of the domain has been divided in accordance with the typical structure of metabolomics investigations:

• general components , and

• technology-specific components .

the ongoing standardisation endeavours in other omics domains, such as the human proteome organization  proteomics standards initiatives   <cit> , the microarray gene expression data society   <cit>  and other ontology communities under the open biomedical ontologies  foundry  <cit>  umbrella can largely be re-used to describe the general aspects of metabolomics investigations. therefore, the msi owg has focused initially on the technology-specific components. further, development activities in this sub-domain have been prioritised according to the pervasiveness of the analytical platforms used.

a range of analytical technologies have been employed in metabolomics studies  <cit> . mass spectrometry  is the most widely used analytical technology in metabolomics, as it enables rapid, sensitive and selective qualitative and quantitative analyses with the ability to identify individual metabolites. in particular, the combined chromatography-ms technologies have proven to be highly effective in this respect. gas chromatography-mass spectrometry  uses gc to separate volatile and thermally stable compounds prior to detection via ms. similarly, liquid chromatography-mass spectrometry  provides the separation of compounds by lc, which is again followed by ms. on the other hand, nuclear magnetic resonance  spectroscopy does not require any separation of the compounds prior to analysis, thus providing a non-destructive, high-throughput detection method with minimal sample preparation, which has made it highly popular in metabolomics investigations despite being relatively insensitive in comparison to the ms-based methods.

for ms, the msi owg will leverage previous work by the psi ms standards wg  <cit> . for chromatography, which is used in both proteomics and metabolomics, the msi owg is closely collaborating with the psi sample processing ontology wg. consequently, the technologies the msi owg is currently focusing on are nmr and gc. these two technologies are used in this paper to illustrate the effectiveness of the proposed tm approach.

the msi owg efforts are divided into two key stages:  reaching a consensus on the cvs, and  developing the corresponding ontology as part of the ontology for biomedical investigations   <cit> . in this paper, we focus on the first stage. each cv is compiled in the following three steps:

 <dig>  compilation: an initial cv is created by re-using the existing terminologies from database models , glossaries, etc. and normalising the terms according to some common naming conventions  <cit> . the result of this phase is a draft cv encompassing terms of different types: methods, instruments, parameters that can be measured, etc.

 <dig>  expansion: in the highly dynamic metabolomics domain, experts often use non-standardised terms. therefore, in order to reduce the time and cost of compiling a cv and to strive for its completeness, we use a tm approach to automatically identify additional technology-related terms frequently occurring in the scientific literature.

 <dig>  curation: the cv is discussed within the msi owg and is passed on to the practitioners in the relevant metabolomics area for validation in order to ensure the quality and completeness of the proposed cv.

we expect the cvs to evolve in time by reflecting the changes in the domain and the availability of new literature, and therefore steps  <dig> and  <dig> should be iterated over in certain time intervals.

implementation
a set of relevant tasks regarding cv term acquisition has been identified, including information retrieval, term recognition and term filtering. figure  <dig> summarises the main steps taken in our tm approach to cv expansion. first, the information retrieval module is used to gather documents relevant for a given cv from the literature databases. once a domain-specific corpus of documents has been assembled, it is searched for potential terms unaccounted for in the initial cv. automatic term recognition is performed to extract terms as domain-specific lexical units, i.e. the ones that frequently occur in the corpus and bear special meaning in the domain. in order to reduce the number of terms not directly related to a given technology, and therefore not relevant for the given cv, we filter out typically co-occurring types of terms denoting substances, organisms, organs, diseases, etc. in contrast to the considered analytical techniques, these sub-domains have more established cvs, which can be exploited to recognise these terms using a dictionary-based approach  <cit> . each of the tm steps is described in more detail in the forthcoming sub-sections.

information retrieval
information retrieval  implements the representation, storage and organisation of textual data to enable a user to access relevant pieces of information  <cit> . biomedical experts regularly exploit ir to locate relevant information  on the internet. apart from general-purpose search engines such as google™  <cit> , many ir systems have been designed specifically to query databases of biomedical publications  such as medical literature analysis and retrieval system online   <cit>  and pubmed central   <cit>  , which provide peer-reviewed literature and make it freely accessible in a uniform format. medline distributes abstracts only, while pmc provides full-text articles. pubmed is accessible through entrez  <cit> , an integrated retrieval system that provides access to a family of related biomedical databases maintained by the national center for biotechnology information .

documents available in pubmed are indexed by medical subject headings   <cit>  terms . mesh is a cv consisting of hierarchically organised terms that serve as descriptors to index and annotate documents. this permits direct access to relevant documents at various levels of specificity, thus improving the performance of ir in terms of speed as well as precision and recall. entrez uses automatic term mapping to match terms against the mesh hierarchy and to expand a query with synonyms and subsumed terms. for example, all of the following terms are explicitly listed as terms matching magnetic resonance spectroscopy in mesh:

• in vivo nmr spectroscopy

• magnetic resonance

• mr spectroscopy

• nmr spectroscopy

• nmr spectroscopy, in vivo

• nuclear magnetic resonance

• spectroscopy, magnetic resonance

• spectroscopy, nmr

• spectroscopy, nuclear magnetic resonance

similarly, a query searching for information on gas chromatography can be expanded automatically to include gas chromatography-mass spectrometry as a more specific term .

while the use of the mesh for indexing and query expansion in entrez is undoubtedly useful, these benefits cannot be fully exploited for the particular problem of accessing articles describing research that utilizes some analytical technology. in particular, an analytical technique employed in metabolomics is unlikely to be the main focus of the reported studies. consequently, the corresponding documents may not necessarily be indexed with technology-related mesh terms. further, the abstracts of such articles are more likely to report the actual findings rather than the technology-specific experimental conditions applied. these parameters are usually described in the materials and methods section or as part of the supplementary material. hence, two points arise when retrieving documents containing information pertinent for analytical techniques deployed in metabolomics studies. first, it is important to search full-text articles as opposed to abstracts only. for this reason we used pmc, which provides access to full-text articles, in addition to medline, which offers only abstracts. second, it is necessary to go beyond mesh terms in query formulation. this problem is alleviated using the following assumption: terms denoting related concepts tend to co-occur within textual documents  <cit> . on this basis, terms from an initially compiled cv can be combined in a search query to retrieve additional documents that describe research that utilises a technology, i.e. the ones that do not necessarily deal with the technology per se and thus may not be indexed by technology-related mesh terms. to achieve this, we index the literature with the cv terms. each cv term is used to search the literature via entrez. as a result, each term is mapped to a set of documents it matches. this information is stored in a local database using the following structure described in sql:

create table index

 not null,

document varchar not null

);

a cut-off point  is set to remove the non-discriminatory terms, i.e. the ones that return too many documents. these are likely to be broad terms not limited to a specific analytical technique, and consequently introducing unwanted noise in the context of the domain-specific corpus. for example, in the case of the nmr cv, the mean number of abstracts returned was  <dig>  with the median being just  <dig>  which is due to the fact that the nmr cv was constructed using a considerable number of terms coming from database schemata. these terms are semi-formal in the sense that they do not necessarily reflect the terminology used in the literature, e.g. amix viewer & amix-tools and jeol nmr instrument. on the other extreme, terms returning the maximal number of abstracts  were: analysis, characteristic, concentration, delta, instrument, method, reference, software, states and tube. the following sql query can be used to identify such terms:

select term, count as matching_documents

from index

group by term

where matching_documents >= d;

where d is chosen a cut-off point. having removed such terms from further consideration from the ir point of view, a cut-off point  is set to remove the documents that do not contain a sufficient number of the cv terms. the following sql query can be used to identify such documents:

select document, count as matching_terms

from index

group by document

where matching_terms <= t;

where t is chosen a cut-off point. for example, some of the documents with the highest number of matching terms from the nmr cv were  <cit> .

the ir module based on the methods described above is encoded in java. the java application takes advantage of e-utilities  <cit> , a web service which enables the users to run entrez queries and download data using their own applications. the information gathered about terms, documents and their relations is stored in a local database  hosted on a postgresql  <cit>  system. by storing the mappings between terms and documents, the querying ability of the db management system can be combined with that of entrez. the local db is also accessible via java applications . hence, all our implemented ir modules can be incorporated into customised workflows  <cit> .

term recognition
in the literature dealing with terminology issues, a term is intuitively defined as a phrase :  frequently occurring in texts restricted to a specific domain, and  having a special meaning in the given domain  <cit> . bearing in mind the potentially unlimited number of different domains and the dynamic nature of newly emerging ones , the need for efficient term recognition becomes apparent. manual term recognition approaches are time-consuming, labour-intensive and prone to error due to subjective judgement. these shortcomings can be addressed by automatic term recognition , the process of annotating an electronic document with a set of terms extracted from the document  <cit> . here, we emphasise that atr refers to the computer-based extraction of terms from a domain-specific corpus as opposed to merely matching the corpus against a dictionary of terms  <cit> . it has been suggested that scientific corpora can be used as reliable sources for terminology construction exploiting  <cit> :

• the growing number of electronic corpora,

• efficient nlp tools ,

• linguistically and/or statistically based atr procedures, and

• the fact that domain experts often use terms that have not been standardised, and as such are not included into standardised dictionaries.

the lack of terminological standards is especially apparent in the rapidly expanding domain of metabolomics, where there is no exact consensus on what constitutes a metabolite name although naming conventions do exist for some entities, e.g. the chemical entities of biological interest  dictionary that is emerging for small molecules  <cit> . still, these are only guidelines and as such do not impose restrictions on domain experts.

manual term recognition is performed by relying on conceptual knowledge, i.e. humans identify terms by relating them to the corresponding concepts. it is currently not feasible to implement an atr approach following such a paradigm due to the lack of appropriate knowledge representation systems and the difficulty of automatically performing “intelligent” tasks. for these reasons, atr approaches resort to other types of knowledge that can provide clues about the terminological status of a given natural language clause  <cit> . generally, the knowledge used for atr may involve two types of information:

• internal: morphological, syntactic, semantic and/or statistical knowledge about terms and/or their constituents , and

• external: linguistic and/or statistical knowledge regarding the term context, together with the knowledge contained in external resources, such as electronic dictionaries, ontologies, corpora, etc.

atr methods typically combine two approaches: linguistic  and statistical   <cit> . linguistic approaches to atr usually involve pattern matching to recognise candidate terms by checking if their internal structure conforms to a predefined set of morpho-syntactic rules. statistical methods rely on at least one of the following hypotheses regarding the term usage  <cit> :

• specificity: terms are likely to be confined to a single or few domains,

• absolute frequency: terms tend to appear frequently in their domain, and

• relative frequency: terms tend to appear more frequently in their domain than in general.

statistical approaches are prone to extracting not only terms, but also other types of collocations   <cit> : functional, semantic, thematic and others, e.g. “…to play an important role in…”. this problem is typically remedied by employing linguistic filters to extract candidate terms from a corpus, which are then ranked using statistical methods.

in this work, we utilised the c-value method  <cit> , publicly accessible at  <cit>  to the tm community via a web service. it first applies syntactic pattern matching to select term candidates, e.g. noun phrases having the structure described by the following regular expression:

 +| *  *) n 

where adj, n and prep denote adjective, noun and preposition respectively. the c-value of each candidate term t is then calculated as:

 c−value={ln⁡|t|⋅f,if s=∅ln⁡|t|⋅−1|s|∑s∈sf),if s≠∅ 

where |t| is the length of t in words, f is t's frequency of occurrence and s is the set of other term candidates containing t as a sub-phrase. all candidates whose c-value exceeds a certain threshold are proposed as domain-specific terms by this method. the threshold chosen will affect the performance of atr in terms of precision and recall, which are calculated as p = a /  and r = a / , where a is the number of true positives , b is the number of false positives  and c is the number of false negatives . higher thresholds will typically result in higher precision and lower recall, and vice versa, lower thresholds will increase the recall at the expense of precision. in general, a threshold used should be corpus-specific , as the c-value of each term candidate also depends on the corpus.

by its definition, the c-value method favours longer and more frequent phrases that are not typically nested within a relatively small set of other phrases. obviously, the c-value method relies primarily on the frequency of term usage and their general syntactic properties rather than exploiting orthographic, morphological and lexical features of specific named entities. for example, while protein names may vary significantly between authors, some general characteristics still apply  <cit> :

• distinctive orthographic characteristics of protein names such as capital letters, digits, special characters ,

• keywords  describing the protein function in multi-word protein names , and

• morphological principles for naming proteins, such as highly abundant affixes -ase, -in, etc. .

opting for a similar named entity recognition approach would significantly increase the time and cost of developing cv term acquisition methods, as these would have to be re-implemented for specific domains. moreover, the type of terms sought may not necessarily exhibit sufficiently discriminatory textual properties  <cit> .

on the other hand, a generic atr approach  can be manipulated to extract terms that are more likely to be of the required type by targeting only relevant documents, and within them specific sections potentially dense with terms of the given type. this can be followed by additional filtering of terms, known to be of different and not directly relevant semantic types to the ones needed, by using lexical resources of these terms where such resources exist. this issue of atr targeting only relevant documents has been addressed by the ir module described in the previous section. a domain-specific corpora is produced as a result of ir by using either mesh or cv terms in the search queries over collections of either abstracts or full-text articles in pubmed.

further, it is particularly important to target only sections that are likely to contain terms relevant for an analytical technology as a preparation step for atr in order to increase its precision. therefore, when using full-text documents we reduce them to the materials and methods sections, which are recognised automatically utilising pmc's xml format in which articles are distributed. once a domain-specific corpus is obtained, the c-value terms are extracted and further inspected to see if they include any terms known to belong to other sub-domains not directly related to the analytical technology under investigation, in which case they can be safely filtered out.

term filtering
given the initially compiled cvs for nmr and gc, we automatically obtained terms loosely related to these two analytical techniques by applying ir to compile a technology-specific corpus, followed by atr to extract a list of terms from the corpus in a way described in the preceding sub-sections. manual inspection of the extracted terms revealed typical types of terms frequently co-occurring with the nmr- and gc-specific terms, namely those denoting substances, organisms, organs, conditions/diseases, etc., which are not of direct interest for the analytical technology per se. examples of such terms automatically extracted by the c-value method are: amino acid, linseed oil, pancreatic juice, blood glucose, cell wall, halophilic bacterium, streptomyces antibioticus, systemic hypertension, cervical dislocation, etc. unlike analytical techniques, many of which are relatively recent, some of these terminologies are relatively stable with respect to the number of new terms being introduced, e.g. linnaean taxonomy  <cit>  classifies living organisms in a systematic manner.

the unified medical language system  <cit>  is a multi-purpose resource merging information from over  <dig> biomedical source vocabularies developed for different purposes. by providing uniform access  to terms belonging to various sub-domains of interest, umls aims to facilitate the development of information systems for text processing in biomedicine via a semi-formal representation of domain-specific knowledge in order to process, retrieve, integrate, and aggregate biomedical data and information contained in the relevant literature  <cit> . it currently contains  <dig>  million concepts named by  <dig>  million terms, organised into a hierarchy of  <dig> semantic types and interconnected by  <dig> different relations.

the following semantic types in the umls proved relevant to our problem of detecting technique-specific terms in a subtractive approach: organism, anatomical structure, substance, biological function and injury or poisoning. given these semantic types as part of the input to the term filtering module , the subsumed terms are automatically selected from the latest version of the umls thesaurus. then, a simple pattern matching approach is applied to filter out these terms and their variations. for example, the filtering approach helped identify the following “outliers” amongst terms extracted by the c-value method: experimental rat, bovine heart muscle, maternal blood sera specimen, farmworker pesticide exposure, arterial carbon dioxide tension, etc., simply by matching the umls terms from the above mentioned classes .

output
we have described an integrative approach combining relatively generic software  and data resources  for the rapid development of a tm tool for automatic expansion of cvs as a practical alternative to tailor-made named entity recognition methods . an html report is generated as a result of the automated cv expansion . the report summarises the output of each module described earlier, i.e.:

• the number of documents collected by the ir module with a link to the list of their citation details  and cross-references to the actual documents in pubmed 

• the size of the final text corpus with a link to the corresponding ascii file , and

• the number of new terms extracted by atr with a link to the list of terms sorted by their c-values.

terms extracted from four different corpora are also amalgamated into a single, alphabetically ordered list . to aid the curation of automatically extracted terms and their incorporation into the cv, the context of a term can be obtained on-the-fly. the context should help the curator interpret the intended meaning of a term and provide clues useful for generating its textual definition. the context of a term rather than its definition may be more crucial for the association of a term with its correct meaning  <cit> . terms sharing the same context are likely to have similar  meaning  <cit> . conversely, different contexts of the same term may point to the problem of term ambiguity . less drastically, the context may “deviate” the meaning of a term by emphasising only certain aspects of a term . bearing in mind the importance of contextual information in determining the correct meaning of a term and hence its position in a cv, we deployed a practical solution: all new terms reported are linked to medevi  <cit> , a service providing local context  for query terms  <cit> . clicking on a term launches a query to medevi, which in turn returns the aligned concordance  lines together with some handy features such as lists of co-occurring keywords and terms .

RESULTS
we performed two case studies to evaluate the effectiveness of the proposed cv expansion approach using the two cvs for nmr and gc, which are currently under development as part of the msi owg activities. the initial cvs were compiled manually by the msi owg members, providing a total of  <dig> and  <dig> terms for nmr and gc respectively. in addition to these terms, we hand-picked the mesh terms  relevant for the techniques of interest by using the web-based mesh browser. we used the given mesh terms to retrieve documents from pubmed that have been manually annotated with these terms. a complementary ir approach was based on the search queries combining the cv terms: at least  <dig> and  <dig> matching terms for abstracts and full papers respectively.

tables  <dig> and  <dig> provide the ir and atr results. the top two rows refer to the ir approach used for collecting a corpus of relevant documents. the use of mesh and cv terms to conduct searches over abstracts and full-text documents results in a total of four corpora, whose numerical properties are described in separate columns. the size of each corpus is given as the number of documents retrieved and its size in kbs . although freely available for browsing, for most articles in pmc the publisher does not allow downloading of the text in xml format; neither does pmc allow bulk downloading in html format. hence, we were able to process only a small number of full-text documents . total numbers of c-value terms extracted from each corpus are given in the bottom two rows, one referring to the total number of terms recognised by the c-value method and the other referring to the number of these terms remaining after applying the filtering approach based on the available knowledge about their semantic types.

by amalgamating all filtered terms, a total of  <dig>  and  <dig>  new terms were acquired for nmr and gc respectively. the bottom rows in tables  <dig> and  <dig> show their distribution across the four corpora. note that the total number of new terms does not correspond to the sum of these numbers due to duplication of terms extracted from different corpora. given a type of search terms , we compared the atr results acquired from abstracts and those obtained from materials and methods sections of full-text articles. we determined that the overlap between the terms extracted from abstracts and those from the body of full-text articles was 2% on average. by further contrasting the results acquired from abstracts and full-text articles, we determined the average ratio between the number of acquired technology-specific terms and the corpus size was  <dig>  for full-text articles and only  <dig>  for abstracts. this comparison confirms that the materials and methods sections represent a significant source of technology-specific terms and also emphasises the benefits that can result from making full-text articles available to tm applications for the benefits of the overall biomedical community.

the preliminary results are available at  <cit> , where the potential cv terms are accessible to the metabolomics community for comments and curation. the official version of the nmr cv has been made publicly available at  <cit>  as part of the nmr ontology. we have to note that the integration of new terms into the msi cvs has only just started and a full evaluation can only be published later on the web pages. nevertheless, we performed a preliminary evaluation using the following setup. for each case study, we selected a test set of  <dig> terms chosen randomly from the resulting set of candidate cv terms. each test set was evaluated independently by two domain experts. each term from the test sets was scored from  <dig> to  <dig> reflecting an expert opinion about the degree to which the term in question is related to the technology described by the cv:  <dig> – no, definitely;  <dig> – no, probably;  <dig> – don't know / not sure;  <dig> – yes, probably;  <dig> – yes, definitely. the detailed evaluation results are given in additional file  <dig>  where a reader can find the score given to each term by each of the curators. we also provide a mean score for each evaluated term and we measure the agreement between the curators by giving the score difference for each of the terms. the mean and median values for all scores are summarised in tables  <dig> and  <dig>  in both cases, the mean value of the average score was around  <dig>  with the average difference in scores given by two curators not being greater than one. the distribution of the scores is shown in figures  <dig> and  <dig>  from these results we extract the fact that in the case of nmr  <dig> terms were deemed relevant ,  <dig> terms were undecided  and  <dig> terms were deemed irrelevant . similarly, in the case of gc we obtained  <dig> positive examples,  <dig> negative ones and  <dig> undecided. by projecting these numbers to the total of  <dig>  candidate nmr terms extracted, we estimate the numbers of relevant, undecided and irrelevant terms to be  <dig> ,  <dig> and  <dig> respectively. for the total of  <dig>  candidate gc terms, it is projected that  <dig>  will be relevant,  <dig> undecided and  <dig> irrelevant. by including ≈ <dig>  positive examples into the nmr cv  and ≈ <dig>  new terms into the gc cv , both cvs can be effectively expanded by more than ten times the original size simply by curating terms as opposed to the process of cv term collection using interviewing techniques and reading the relevant literature.

in addition to the preliminary quantitative evaluation, we also provide some qualitative remarks about our approach tm approach to cv expansion, which will be taken into account in order to improve the functionality of the tool. some of the extracted terms were “incomplete”. for example, the term comparative nmr as found in the result list lacks the headword to be of sufficient understandability and to get inserted into a cv, e.g. as its concordance () reveals this term should be comparative nmr analysis or comparative nmr study. this is due to the term variation phenomenon when the same concept is designated by more than one term. when such term candidates are processed separately, their c-values are distributed across different variants providing separate frequencies for individual variants instead of a single frequency unifying all of the variants. hence, in order to make the most of the statistical part of the c-value method, term candidates need to be normalised prior to statistical analysis  <cit> .

further, the cv expansion process can be helped by a different way of presenting the resulting terms. having the candidate terms clustered according to their head noun phrases  would facilitate term integration and hierarchical structuring of the cv.

CONCLUSIONS
we described an integrative approach combining relatively generic, public software and data resources for time- and cost-effective development of a tm tool to aid the expansion of cvs across various domains. this should serve as a practical alternative to both manual term collection and tailor-made named entity recognition methods. the software makes use of web services to access three key resources:

• entrez for ir,

• c-value for atr, and

• umls as a semantic network of biomedical terms.

it is disseminated under an open-source licence. originally developed to the specification of the msi owg, it is still generic enough to be applied for the expansion of other cvs in biomedicine simply by changing the input parameters:

• the initially compiled cv,

• the mesh terms that reflect the domain of the cv, and

• the umls semantic types of terms indirectly related to those covered by the cv.

the output terms are presented to the user in html format so they can be inspected through a web browser, in which the context of each term as used in the scientific literature can be explored through the hyperlinked medevi service  in an effort to aid the curation of the potential cv terms.

availability and requirements
project name: cvexpand

project home page: 

operating system: platform independent

programming language: java 

other requirements: access to sql database

license: academic free license v <dig> 

any restrictions to use by non-academics: none

list of abbreviations used
atr automatic term recognition

cv controlled vocabulary

db database

gc gas chromatography

gc-ms gas chromatography – mass spectrometry

hupo human proteome organization

html hypertext markup language

ir information retrieval

jdbc java database connectivity

medline medical literature analysis and retrieval system online

mesh medical subject headings

mged microarray gene expression data society

ms mass spectrometry

msi metabolomics standards initiative

nmr nuclear magnetic resonance

obi ontology for biomedical investigations

obo open biomedical ontologies

owg ontology working group

psi proteomics standards initiative

pmc pubmed central

sql structured query language

tm text mining

umls unified medical language system

xml extended markup language

competing interests
the authors declare that they have no competing interests.

authors' contributions
is designed and implemented the text mining application and drafted the manuscript. ds provided the initial data, evaluated the results and helped to draft the manuscript. sas conceived the overall study and participated in its design and coordination. drs participated in the design and coordination of the text mining aspects of the study. dbk provided his expertise in metabolomics to help evaluate the results. np supervised the bioinformatics integration aspects. msi owg members participated in provision of the data, discussions and evaluation. all authors read and approved the final manuscript.

supplementary material
additional file 1
evaluation results: each test set was evaluated independently by two domain experts. each term from the test sets was scored from  <dig> to  <dig> reflecting an expert opinion about the degree to which the term in question is related to the technology described by the cv:  <dig> – no, definitely;  <dig> – no, probably;  <dig> – don't know / not sure;  <dig> – yes, probably;  <dig> – yes, definitely.

click here for file

 acknowledgements
we kindly acknowledge other members of the msi ontology wg, the msi oversight committee, other msi wgs, national centre for text mining, the obi wg, the obo foundry leaders and the ontogenesis networks members for their contributions in fruitful discussions. we also owe thanks to our colleagues for their assistance in the evaluation of the results. their names are : warwick dunn, farid khan and denis v. rubtsov. we gratefully acknowledge the support of the bbsrc/epsrc via “the manchester centre for integrative systems biology” grant , the bbsrc e-science development fund  and the eu network of excellence semantic interoperability and data mining in biomedicine .

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2008: proceedings of the 10th bio-ontologies special interest group workshop  <dig>  ten years past and looking to the future. the full contents of the supplement are available online at
