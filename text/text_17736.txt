BACKGROUND
the recognition of genetic variations is an important topic in bioinformatics. single nucleotide polymorphisms  are the most common form of genetic variation in human dna. humans are diploid organisms. there are two copies of each chromosome , one from each parent. the sequence of snps in a given chromosome copy is referred to as a haplotype. haplotype information is useful in many applications, such as gene disease diagnoses  <cit> , drug design, etc. due to their essential importance in many biological analysis, haplotypes have been attracting great attention in recent years  <cit> . since experimental methods for direct sequencing of haplotypes are both expensive and time consuming, computational methods are usually much more promising.

currently, computational methods for computing haplotypes often fall into two categories: population haplotyping  <cit>  and haplotype assembly   <cit> . the former tries to compute haplotypes based on the genotype data from a sample of individuals in a population. many software packages have been published in this field, e.g., phase  <cit> . an obvious drawback of population haplotyping lies in its weakness in recognizing rare and novel snps  <cit> . contrary to population haplotyping, haplotype assembly is more efficient and has received more attention in recent years. the input to the haplotype assembly problem is a set of fragments sequenced from the two copies of a chromosome of a single individual, and their locations in the chromosome, which can be pre-determined by aligning the fragments to a reference dna sequence. the task here is to reconstruct two haplotypes from the input fragments. in this paper, we focus on the haplotype assembly problem.

the haplotype assembly problem was first introduced by lancia et al.  <cit> . in  <cit> , the authors proposed three optimization criteria for solving this problem, i.e. minimum fragment removal , minimum snp removal  and longest haplotype reconstruction . some polynomial time algorithms have been designed to solve some versions of such optimization problems  <cit> . lippert et al.  <cit>  summarized the models in  <cit>  and proposed some new models. among these models, the most difficult and realistic one is minimum error correction , where we want to minimize the total number of conflicts  between the fragments and the constructed haplotypes . the haplotype assembly problem with mec is np-hard  <cit>  even for gapless fragments.

levy et al.  <cit>  designed a greedy heuristic algorithm that concatenates the fragments with minimum conflicts. the greedy heuristic algorithm is very fast but not very accurate when the error rate of fragments is high. later, bansal and bafna  <cit>  developed a software package hapcut and the algorithm is based on the idea of building a graph from the sequenced fragments, in which each snp site corresponds to a node in the graph and two nodes are connected by an edge if there exists a fragment that covers both snp sites . it then tries to minimize the mec cost of the reconstructed haplotypes by iteratively finding max-cuts in the associated graph. bansal et al.  <cit>  designed a markov chain monte carlo  algorithm, hash. both hash and hapcut have better performance than the greedy heuristic algorithm proposed in  <cit> .

recently, he et al.  <cit>  gave a dynamic programming algorithm that can give the optimal solution to the haplotype assembly problem with mec. the time complexity of the algorithm is o, where m is the number of fragments, n is the number of snp sites, and k is the length of the longest fragments. this algorithm works well for k ≤  <dig>  however, it becomes impractical when k is large.

in this paper, we propose a heuristic algorithm for the haplotype assembly problem with mec. it is worth mentioning that in hapcut  <cit>  and the dynamic programming algorithm proposed in  <cit> , the authors assumed that the two constructed haplotypes are complementary with each other, i.e. there are only  <dig> choices at a snp site in the reconstructed haplotypes. we drop this assumption in our heuristic algorithm. as a result, there are  <dig> choices at a snp site in the reconstructed haplotypes. we have tested our algorithm on a set of benchmark datasets and compare it with several state-of-the-art algorithms. experiments show that our algorithm is highly accurate. it outperforms most of the existing programs when the error rate of input fragments is high.

preliminaries
the input to the haplotype assembly problem is a set of fragments sequenced from the two copies of a chromosome of a single individual. each fragment covers some snp sites. we assume that all the fragments have been pre-aligned to a reference dna sequence. as a result, we can organize the input fragments as an m × n matrix m , where m is the number of fragments and n is the number of snp sites. each row of m corresponds to a fragment that can be represented as a string on the alphabet ∑ = {a, c, g, t, -}, where '-' indicates a space when the snp site is not covered by the fragment or the snp value cannot be determined with enough confidence. the start  position of a fragment is defined as the first  position in the corresponding row that is not a '-'. in the middle of a fragment, '-'s are allowed due to data missing or paired-end fragments. throughout the remainder of this paper, we will use the two notations, i.e. fragments and rows of m interchangeably when there is no ambiguity. moreover, we will use columns and snp sites interchangeably when there is no ambiguity.

it is accepted that there are at most two distinct nucleotides at a snp site. we assume that a column with more than two distinct nucleotides in m must contain errors. in this case, we keep the two distinct nucleotides that appear the most at this column and replace the rest of them with a '-'. after removing errors, m can be converted into m′, in which each entry is encoded by a character from the alphabet ∑′={ <dig> -}. figure  <dig> gives an example of an original input matrix m containing errors in some columns, figure  <dig> is the matrix after error correction. m′ is given in figure  <dig> 

we say that row i covers column j in m if mi,j is not a '-' or there are two integers p and q with p < j < q such that mi,p ≠ - and mi,q = -. the number of rows covering column i in m is referred to as the coverage of column i.

two rows p and q in m are in conflict if there exists a column j such that mp,j ≠ mq,j, mp,j ≠ - and mq,j ≠ -. obviously, for error-free data, two rows from the same copy of a chromosome should not conflict with each other, and two rows which conflict with each other must come from different copies of a chromosome. the distance between two rows i and j, denoted by d, is defined as the generalized hamming distance as follows:

  d= ∑k=1nd 

where

  d=1ifmi,k≠mj,k,mi,k≠-andmj,k≠-;0otherwise. 

minimum error correction  is a commonly used model for the haplotype assembly problem. for the haplotype assembly problem with mec, the input is a fragment matrix m, the task is to partition the fragments in m into two groups and construct two haplotypes , one from each group, such that the total number of conflicts  between the fragments and the constructed haplotypes  is minimized.

methods
in this section, we will describe the algorithms used to solve the problem. we first design a dynamic programming algorithm that gives an exact solution and runs in o time, where n is the number of columns in m, and t is the maximum coverage of a column in m. the dynamic programming algorithm will be very slow when t is large. we then design a heuristic algorithm that first computes an initial pair of haplotypes by using the dynamic programming algorithm on only a subset of m. this initial pair of haplotypes can be viewed as an approximation to the optimal solution. to obtain a better solution, we further introduce some techniques to refine the initial solution.

a dynamic programming algorithm
recall that the goal of the haplotype assembly problem is to partition the rows of the input fragment matrix m into two groups, each of which determining a haplotype. to obtain an optimal partition, a naive approach is to enumerate all possible partitions on the rows of m, among which we then choose the one minimizing mec. for an instance with m rows, there are 2m total partitions, and thus the approach does not work in practice. here we introduce a dynamic programming algorithm for the haplotype assembly problem with mec that runs in o time, where n is the number of columns in m, and t is the maximum coverage of a column in m.

before we give the details of the dynamic programming algorithm, we first define some basic notations that will be used later:

•ri: the set of rows covering column i in m.

•pj: the j-th partition on ri.

•qj: the j-th partition on ri ∩ ri+ <dig> 

•pj|ri∩ri+1: the partition on ri ∩ ri+ <dig> obtained from pj by restriction on the rows in ri ∩ ri+ <dig> 

•qqj: the set of partitions pk such that pk|ri∩ri+ <dig> = qj.

•c): the minimum number of corrections to be made in column i of m when the partition on ri is indicated by pj.

•mec): the optimal cost for the first i columns in m such that column i has a partition pj.

in order to compute mec) efficiently, we define

  me)=minpk∈qqjmec). 

let pj be the j-th partition on ri+ <dig>  qk = pj|ri∩ri+ <dig>  the recursion formula of the dynamic programming algorithm is illustrated as follows:

  mec)=c)+me). 

based on pj, we can get qk in o time. furthermore, we know the majority value  at column  in each group. to compute c), we can simply count the number of minorities in each group ) separately, and then add them up. thus, it takes o time to compute c).

the optimal mec cost for partitioning all the rows of m is the smallest mec) over all possible pj, where n is the number of columns in m. a standard backtracking process can be used to obtain the optimal solution.

let us look at the time complexity of the dynamic programming algorithm. to compute each mec) in equation , it requires o time to compute c). thus, it takes o time to compute all c)s for all the n columns in m. now, let us look at the way to compute m e)s. for each partition pj on ri, we can get qk = pj|ri∩ri+ <dig> in o time. we then update me) if the current value of me) is greater than mec). there are at most 2t pjs on ri. thus, it takes o time to compute all me)s on ri. since there are n columns in m, the total time required for computing all me)s is o.

theorem  <dig> given a fragment matrix m, there is an o time algorithm to compute an optimal solution for the haplotype assembly problem with mec, where n is the number of columns in m, and t is the maximum coverage of a column in m.

obtaining an initial solution via randomized sampling
the dynamic programming algorithm works well when t is relatively small. however, it will be very slow when t is large. to solve the problem when t is large, we look at each column i at a time, randomly select a fixed number of rows, say, boundofcoverage, from the set of rows covering it and delete the characters in the remaining rows at all the columns after i -  <dig>  after that, the coverage of each column in the newly obtained submatrix is at most boundofcoverage. we then run the dynamic programming algorithm on the submatrix. the resulting pair of haplotypes, which is referred to as the initial solution, can be viewed as an approximation to the optimal solution.

the detailed procedure for obtaining a submatrix from m via the randomized sampling approach is as follows:

 <dig>  compute the coverage ci for each column i in m.

 <dig>  for i =  <dig> to n, perform the following steps.

 <dig>  if ci ≤ boundofcoverage, do nothing and goto the next column. otherwise, goto step  <dig> 

 <dig>  randomly choose boundofcoverage rows from the set of rows covering column i. let s ¯ be the set of rows covering column i but are not chosen during this process.

 <dig>  for each row r∈s ¯, cut r from column i such that it no longer covers any column larger than i . accordingly, we need to reduce cj by  <dig> for each i ≤ j ≤ k, where k is the end position of r before being cut.

by employing this randomized sampling strategy, we can always make sure that the maximum coverage is bounded by the threshold boundofcoverage in the selected submatrix. how to choose a proper value for boundofcoverage? actually, there is a tradeoff between the running time and the quality of the initial solution output by the dynamic programming algorithm. on one hand, reducing boundofcoverage can reduce the running time of the algorithm. however, on the other hand, increasing boundofcoverage can maintain more information from m. as a result, the initial solution output by the dynamic programming algorithm has a higher chance to be close to the optimal solution. in practice, boundofcoverage is generally no larger than  <dig>  which is feasible in terms of running time and is large enough to sample sufficient information from m. see section experiments for a detailed discussion on how the size of boundofcoverage affects the initial solution.

refining the initial solution with all fragments
in the newly obtained submatrix, it is possible that  some columns are not covered by any rows, thus leaving the haplotype values at these snp sites undetermined in the initial solution,  the haplotype values at some snp sites in the initial solution are wrongly determined due to the lack of sufficient information sampled from m during the randomized sampling process. in this subsection, we try to refine the initial solution with all input fragments, aiming to fill haplotype values that are left undetermined and correct those that are wrongly determined.

the refining procedure contains several iterations. in each iteration, we take two haplotypes as its input and output a new pair of haplotypes. initially, the two haplotypes in the initial solution are used as the input to the first iteration. the haplotypes output in an iteration are then used as the input to the subsequent iteration. in each iteration, we try to reassign the rows of m into two groups based on the two input haplotypes. more specifically, for each row r of m, we first compute the generalized hamming distance between r and the two haplotypes. then, we assign r to the group associated with the haplotype that has the smaller  distance with r. after reassigning all rows of m into two groups, we can compute a haplotype from each of the two groups by majority rule. at the same time, we can also obtain the corresponding mec cost.

the refining procedure stops when, at the end of some iteration, the obtained haplotypes no longer change, or when a certain number of iterations have been finished. the two haplotypes output in the last iteration are the output of the refining procedure.

voting procedure
to further reduce the effect of randomness caused by the randomized sampling process, we try to obtain several different submatrices from m by repeating the randomized sampling process several times. accordingly, we can obtain several initial solutions, one derived from each submatrix. furthermore, we can refine these initial solutions with all fragments. given a set of solutions, each of which containing a pair of haplotypes, the goal here is to compute a single pair of haplotypes by adopting a voting procedure.

in the voting procedure, the two haplotypes are computed separately. we next see how to compute one of the two haplotypes. the other case is similar. let s be the set of solutions used for voting. first, we find a set of haplotypes , one from each solution in s, such that the haplotypes in s <dig> all correspond to the same copy of a chromosome. with s <dig>  we can then compute a haplotype by majority rule. simply speaking, at each snp site, we count the number of 0s and 1s at the given snp site over the haplotypes in s <dig>  if we have more 0s, the resulting haplotype takes  <dig> at the snp site, otherwise, it takes  <dig> 

how to find s1? first, we need to clarify that the two haplotypes in each solution in s are unordered. that is, given a solution h = , we do not know which chromosome copy h <dig>  corresponds to. so, we should first find the correspondence between the haplotypes in different solutions. let h1=,...,hy= be the set of solutions in s. without loss of generality, assume that the mec cost associated with h <dig> is the smallest among all the y solutions. we use h <dig> as our reference and try to find the correspondence between haplotypes in h <dig> and other solutions. for each i , we first compute two generalized hamming distances d and d. if d<d, we claim that h1i corresponds to the same chromosome copy as h <dig>  otherwise, h2i corresponds to the same chromosome copy as h <dig>  as a result, the set of haplotypes in s that correspond to the same chromosome copy as h <dig> is the s <dig> we want to find.

assume that at the beginning of this procedure, we obtain x solutions by repeating the randomized sampling process along with the refining procedure x times. it is worth mentioning that in the voting procedure, we only use part of the solutions, say, the first y  solutions with the highest quality. given two solutions a and b, we say that a has higher quality than b if the mec cost associated with a is smaller than that of b. in this case, we assume that a is much closer to the optimal solution and contains less noises than b. to reduce the sideeffect of noises and improve the quality of the solution output by the voting procedure, it is helpful to use only solutions with high quality in the voting procedure.

summarization of the algorithm
generally speaking, given an input fragment matrix m, our heuristic algorithm can be summarized as the following four steps.

step 1: we first perform a preprocessing on m to detect possible errors in it. after removing errors from m, we further convert it into m′ in which each entry is encoded by a character from the alphabet ∑′={ <dig> ,-}. see section preliminaries for more details. m′ is used as the input to the following steps.

step 2: we compute an initial solution by running the dynamic programming algorithm on a subset of m′. the submatrix is computed by using the randomized sampling approach.

step 3: refine the initial solution with all the fragments in m′, instead of the submatrix that is used to generate the initial solution in step  <dig> 

step 4: to further reduce the effect of randomness caused by the randomized sampling process, we repeat step  <dig> and step  <dig> several times. each repeat ends with a solution, from which we then compute a single pair of haplotypes by adopting the voting procedure. the resulting pair of haplotypes is the output of our algorithm.

RESULTS
we have tested our algorithm on a set of benchmark datasets and compare its performance with several other algorithms. the main purpose here is to evaluate how accurately our algorithm can reconstruct haplotypes from input fragments. all the tests have been done on a windows-xp  desktop pc with  <dig>  ghz cpu and 4gb ram.

the benchmark we use was created by geraci in  <cit> . it was generated by using real human haplotype data from the hapmap project  <cit> . there are three parameters associated with the benchmark, i.e haplotype length, error rate and coverage rate, denoted by l, e, c, respectively. each parameter has several different values, l =  <dig>   <dig>  and  <dig>  e =  <dig> ,  <dig> ,  <dig>  and  <dig> , c =  <dig>   <dig>   <dig> and  <dig>  note that unlike the "coverage" defined in section preliminaries, the coverage rate c defined in this benchmark refers to the number of times each of the two haplotypes replicates when generating the dataset. in other words, given an instance in the benchmark, i.e. a fragment matrix, there are up to 2c rows which take non '-' value at each column in the matrix. for each combination of the three parameters, there are  <dig> instances in the benchmark. as for the details on how to generate the benchmark, the reader is referred to  <cit> .

throughout our experiments, we measure the performance of our algorithm by the reconstruction rate, a frequently used criterion in the haplotype assembly problem. given a problem instance in the benchmark, the reconstruction rate is defined as follows:

  rh^,h=1−min+d′,d′+d′)2n 

where h =  is the pair of correct haplotypes that is used to generate the problem instance, and is thus known a prior, Ĥ= is the pair of haplotypes output by the algorithm, n is the length of the haplotypes, and d′ is the hamming distance between two haplotypes. more specifically, d′ is defined as follows:

  d′= ∑k=1nd′ 

where

  d′=0ifhi=ĥj;1otherwise. 

intuitively speaking, the reconstruction rate measures the ability of an algorithm to reconstruct the correct haplotypes.

recall that in step  <dig> of our algorithm, we try to compute an initial solution by using only a subset of the input matrix. the initial solution forms the basis for the following steps of our algorithm and is closely related to the parameter boundofcoverage. briefly speaking, boundofcoverage is the maximum coverage of a column in the submatrix selected during this step. for a formal description of boundofcoverage, we refer you to section methods. in this experiment, we first evaluate how the size of boundofcoverage affects the initial solution. as aforementioned earlier, boundofcoverage is generally no larger than  <dig>  here we consider three different sizes of boundofcoverage, i.e.  <dig>   <dig> and  <dig>  given a problem instance, we can obtain three initial solutions by using the three different sizes of boundofcoverage, respectively. as an example, we choose the set of benchmark datasets with l =  <dig> and e =  <dig> . for each combination of the coverage rate c and boundofcoverage, there are  <dig> instances and we compute the average of the reconstruction rates over the  <dig> instances. the results are listed in table  <dig> 

there are  <dig> different sizes of boundofcoverage, i.e.  <dig>   <dig> and  <dig>  and  <dig> different coverage rates c, i.e.  <dig>   <dig>   <dig> and  <dig>  the number outside each bracket refers to the average reconstruction rate under the corresponding parameter setting, while the one enclosed in the bracket refers to the average running time in seconds.

from table  <dig>  we can see that for a fixed coverage rate c, when increasing the size of boundofcoverage, the reconstruction rate of the obtained initial solution gets higher, and the running time increases accordingly. since the reconstruction rate in the case where boundofcoverage =  <dig> is relatively high, and the running time is feasible, we will set boundofcoverage to be  <dig> in the following experiments.

next, to evaluate the performance of our algorithm, we have tested it on the set of benchmark datasets. the parameters we use are as follows: boundofcoverage =  <dig>  x =  <dig>  and y =  <dig>  where x is the number of initial solutions obtained in step  <dig>  i.e. the number of times we repeat step  <dig> and step  <dig>  and y is the number of solutions used for voting in step  <dig>  the results for l =  <dig>   <dig> and  <dig> are given in the last column in table  <dig> table  <dig> and table  <dig>  respectively. in  <cit> , geraci compared the reconstruction rates of seven state-of-the-art algorithms on the same benchmark datasets. these seven algorithms are speedhap  <cit> , fast hare  <cit> , 2d-mec  <cit> , hapcut  <cit> , mlf  <cit> , shr-three  <cit>  and dgs, the greedy heuristic proposed in  <cit> . for a full review of these seven algorithms, the reader is referred to  <cit> . for the sake of comparison, we list the reconstruction rates of the seven algorithms, see columns  <dig> -  <dig> in table  <dig>  table  <dig> and table  <dig>  note that the results for the seven algorithms are directly taken from  <cit> . each reconstruction rate shown in the three tables is the average over  <dig> instances under the same parameter setting.


e
c
the columns e and c refer to the error rate and coverage rate, respectively. columns 3- <dig> represent the reconstruction rate of the seven algorithms, i.e. speedhap, fast hare, 2d-mec, hapcut, mlf, shr-three and dgs. for each combination of e and c, the best among the seven algorithms is highlighted in bold. the last column lists the reconstruction rate of our algorithm.

the columns e and c refer to the error rate and coverage rate, respectively. columns 3- <dig> represent the reconstruction rate of the seven algorithms, i.e. speedhap, fast hare, 2d-mec, hapcut, mlf, shr-three and dgs. for each combination of e and c, the best among the seven algorithms is highlighted in bold. the last column lists the reconstruction rate of our algorithm.

the columns e and c refer to the error rate and coverage rate, respectively. columns 3- <dig> represent the reconstruction rate of the seven algorithms, i.e. speedhap, fast hare, 2d-mec, hapcut, mlf, shr-three and dgs. for each combination of e and c, the best among the seven algorithms is highlighted in bold. the last column lists the reconstruction rate of our algorithm.

take a close look at the three tables, we can see that  each of the seven algorithms studied in  <cit>  only works well in some cases, e.g., speedhap works well when the error rate is low , while mlf works well when the error rate is high ;  the reconstruction rates of all the seven algorithms are relatively low when the error rate of input fragments is high. for example, in the case where l =  <dig>  e =  <dig>  and c =  <dig>  the best reconstruction rate of the seven other algorithm is  <dig> . compared with its competitors, our algorithm can give solutions with high reconstruction rate. it outperforms its competitors in almost all cases, especially in cases in which the error rate of fragments is high . we also notice that when the error rate is  <dig>  our algorithm may introduce some errors in the output solution, e.g., in the case where l =  <dig>  c =  <dig>  e =  <dig> . however, even in this case, the reconstruction rate can still reach up to  <dig> .

discussion
in the first step of our algorithm, we perform a preprocessing on the input fragment matrix. this allows us to detect errors in the input. for example, for the benchmark datasets with l =  <dig> and e =  <dig> , step  <dig> of our algorithm can identify about 47%, 55%, 59% and 61% of the total errors for the cases c =  <dig>   <dig>   <dig> and  <dig>  respectively. thus, step  <dig> has significant importance to the following steps of our algorithm.

next, we further investigate how the voting procedure in step  <dig> affects the performance of our algorithm. in step  <dig>  we first obtain x solutions, from which we then choose the first y  solutions with the smallest mec cost. the y solutions are then used in the voting procedure to compute the final solution of our algorithm. to demonstrate the effect of the voting procedure, we compare the final version of our algorithm with the one without the voting procedure. for the version without the voting procedure, we simply outputs the solution with the smallest mec cost among all the x solutions in step  <dig>  as an example, we have tested both versions on the set of benchmark datasets with l =  <dig>  the parameters are as follows: boundofcoverage =  <dig>  x =  <dig> and y =  <dig>  figure  <dig> ) shows the results for the two versions in the case where e =  <dig>  . the results for e =  <dig>  and  <dig>  are similar as that of e =  <dig> , and we omit it here.

from figure  <dig>  we can see that the two versions of our algorithm have almost the same reconstruction rate. however, when e =  <dig> , the final version of our algorithm has higher reconstruction rate than the one without the voting procedure, see figure  <dig>  thus, the voting procedure is essential for our algorithm.

to see how the size of the parameter x affects the reconstruction rate of our algorithm, we have tested our algorithm with three different sizes of x, i.e.  <dig>   <dig>   <dig>  the values of boundofcoverage and y are fixed to be  <dig> and  <dig>  respectively. the tests are done on the set of benchmark datasets with l =  <dig>  the results for e =  <dig>  and  <dig>  are shown in figure  <dig> and  <dig>  respectively. for e =  <dig>  and  <dig> , the reconstruction rates are almost the same in all the three cases, and we do not list it here. as can be seen from figure  <dig>  the reconstruction rate increases with the increasing of x in the cases where c =  <dig> and  <dig>  for c =  <dig> and  <dig>  the cases where x =  <dig> and  <dig> have almost the same reconstruction rate which is higher than that in the case where x =  <dig>  as for figure  <dig>  it is much more obvious that the reconstruction rate increases as x gets larger.

CONCLUSIONS
in this paper, we propose a heuristic algorithm for the haplotype assembly problem. experiments show that our algorithm is highly accurate. it outperforms most of the existing programs when the error rate of input fragments is high.

competing interests
the authors declare that they have no competing interests.

authors' contributions
fd participated in the design of the study, performed the experiments and drafted the manuscript. wc participated in the design of the study and helped to draft the manuscript. lw conceived the study, participated in its design and helped to draft the manuscript. all authors read and approved the final manuscript.

declarations
the publication costs for this article were funded by the corresponding author's institution.

