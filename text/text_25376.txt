BACKGROUND
volumetric anatomical atlases have become more prevalent as desktop computers have grown more powerful and have been provided with greater storage capacity. digital atlases can be superior to print atlases in a number of ways: true 3d atlases are not restricted to just the limited number of sectional views typically presented in a print atlas but can be resliced in any arbitrary orientation  <cit> ; the user can create spatially coded annotations or segmentation regions to share with other researchers  <cit> ; they can be used as a canonical framework to support semi- or fully-automatic atlas-guided segmentation  <cit> . furthermore, experimental datasets can be aligned to the atlas and be synchronously navigated, showing equivalent cross-sectional planes across all datasets. lastly, digital atlases provide a universal reference frame for other biomedical image or otherwise spatially coded data sets, enabling correlated analysis of complementary data modalities within an equivalent geometric scaffold  <cit> . for example, the system described here is being employed to spatially index the mouse brain library  <cit>  and is being employed within the mouse birn integrated atlasing tool  associated with the biomedical informatics research network  <cit> .

a number of digital atlases are now available, for several species and imaging modalities. focussing specifically on the nervous system of the c57bl/6j inbred strain of mus musculus , a controlled genetic population critical for genetic and functional genomic research, there are several existing brain atlases: the neuroterrain <cit>  and the smart  atlas developed for use within birn <cit>  are nissl-based light-microscopic atlases most closely analogous to the classical print, nissl-stained histological mouse brain atlases in wide use <cit> ; ucla's laboratory for neuro imaging  mouse atlas project  includes magnetic resonance microscopy  image sets, as well as basic histochemical and immunohistochemical tissue sets <cit> ; ma et al also described an mrm mouse brain atlas <cit> . the edinburgh mouse atlas project   <cit>  includes classic histochemical microscopy, confocal laser scanning microscopy  and optical projection tomography  image sets captured at multiple spatial resolutions from various developmental stages, though providing only limited nervous-system specific morphological detail.

several large-scale, mouse brain-specific gene expression reference data sets are being constructed which are critical resources for atlas-based integrated analysis of neurodevelopment and neurodegenerative disease  <cit> . all of these data sets are anatomically-mapped, some by virtue of being assays performed directly in a tissue context both using standard in situ immunohistochemistry, either fluorescence  <cit> , alkaline-phosphatase  <cit> , digoxygenin-labeled antisense riboprobes  <cit> , or radioactivity   <cit> , as well as gfp-reporter constructs used in the gene expression nervous system atlas  bac transgenics project  <cit> . for others, the anatomical context is defined only to a relatively coarse level of granularity by microdissecting mesoscopic brain regions  <cit> .

associated with these brain atlas projects and reference data sets are software systems for browsing the images in a 3d context , though only a subset of these supports reslicing the 3d data sets to produce novel 2d views independent of the orientation in which the 2d images were originally acquired  <cit>  browser, the emap jatlasview <cit> , the mouse brain image visualizer  <cit>  used by ma et al.). though sharing many functional features common to 3d anatomical analysis systems, each of these systems provides many unique features. many of these atlas software systems are designed to run on local desktop computers; this, unfortunately, restricts their use to fairly powerful computers, since these multi-gigabytes atlas data sets require substantial computing power to be manipulated interactively.

this list of available 3d image set browsers currently used to manipulate mouse brain atlas data sets, provides links both to the developers of the software tool, as well as to a representative neuroimaging project for which that browser is currently being employed. this is a subset of the available 3d anatomical browsers used in all species, a list of which can be obtained from the internet analysis tools registry maintained dr. david kennedy at the mgh, boston ma  <cit> . by our count, there are currently  <dig> 3d volume data set viewers available for use.

the amount of data processing required also militates against most common cross-platform approaches to browser design, since many of these approaches depend on relatively slow byte-code interpreters. one can carefully design viewers to work around some of the memory and performance limitations of typical desktop computers to create hybrid cross-platform atlas browsers  <cit> , but our experience has shown a client-server implementation is typically easier to extend and keep inter-operative. this architecture reduces the processing and memory load on the local desktop, shifting it to a server shared among many clients. proper server architecture design can enable the system to scale its performance to synchronously serve multiple 3d atlas data sets to many simultaneous users. the use of a client-server architecture also simplifies version management, since server-side enhancements need only be added to just a few systems in known locations, and client upgrades can be provided easily via web distribution. finally through the use of a well-thought-out client design, data from many sources can be integrated to accommodate specific research needs, greatly magnifying the power of the system.

a number of the above-referenced atlas-based 3d anatomical analysis systems are network based, but they all have practical limitations. smart atlas, for example, is not built on a true 3d dataset providing only those views available in the franklin & paxinos published atlas  <cit> . shiva from the loni map group and mbiv used by ma et al. each allows the user to slice a 3d set but only on planes orthogonal to one of the  <dig> standard anatomical orientations , not off-axis at an arbitrary angle which is often required in the course of a study. jatlasview from emap does allow for arbitrary-angle slicing, but, as with shiva and mbiv, you must download the entire 3d data set you intend to visualize to your local machine and require special knowledge to properly install the sets. in addition, all of these implementations exist as islands in the internet, with no real connections to other data sources. only the psc volume browser  <cit>  used to view data from siddiqui, et al. is fully implemented as a client-server architecture. though the powerful psc volume browser architecture is modular, it was not designed to be integrated as an element within a larger analysis framework, neither as a graphical user interface  component nor as a middleware connectivity application programming interface . the jatlasviewer does promote use of it's various components for construction of more complex applications and, in particular, provide useful wrappers for encapsulating new compiled functionality; however, it was not specifically designed as a development kit, so a significant amount of learning and additional coding is required to properly use these modules. in this paper we will describe a full 3d slicing server implementation supporting arbitrary resampling both on and off the traditional axes and the nt-sdk with an open api for easy integration into other java-based biomedical image visualization/analysis environments.

implementation
the client-server architecture is a well known solution to the need for providing access by multiple users to a single, controlled data source. presented here is a basic server design, a client development kit, and the language they use to communicate. the server architecture is highly modular, allowing individual server-side components to be enhanced or replaced without affecting the other components or existing client implementations. the neuroterrain atlas server  design is multi-threaded to take advantage of multi-core multi-processor systems and provide enhanced scalability. it allows for remote administration using standardized xml-based communications. the netostat client  is implemented directly on top of the nt-sdk, a framework promoting integration into existing applications and providing access to the ntas with minimal coding. the nt-sdk java framework supports both end-user client functionality, displaying atlas data graphically, as well as middleware and backend uses, supporting integration of data sets hosted by the ntas and image file repository  into a data federation framework, for example <cit> . these two distinct features of the sdk result from it being designed as two separate libraries – nt-sdk-ntas-comm.jar and nt-sdk-netostat-gui.jar. the latter depends on use of the former, whereas middleware can utilize the former without need of the end-user interactive gui components.

main server modules
there are five main modules included in the server. these are the server manager , the data manager , the slicers, the processing queue manager  and the modules handling the two public network interfaces. the servers, slicers, and server manager each run in their own threads; the others either run in their client connection's threads or spawn additional threads as needed.

server manager
the server manager is responsible for the overall operation of the server application:

 <dig>  exposing a control socket for administration;

 <dig>  loading or unloading datasets;

 <dig>  associating datasets with ontologies;

 <dig>  managing user  access;

 <dig>  managing the properties of the servers; and,

 <dig>  starting/stopping multiple instances of the slicer threads as needed.

administrative clients communicate with this module via a standardized xml stream over a tcp socket. secuity can be handled using ssl, and/or control limited to a unix-domain socket, which restricts access to processes running on the local host.

servers
this ntas implementation actually consists of two linked servers, each with its own communication socket. one server implements a binary interface, used to retrieve sliced images of the dataset; in the other, xml data is used to return single-instance data items.

the binary server is designed to maintain a persistent tcp session connection, similar to the way telnet is implemented. once the client logs in, a data structure maintaining atlas data set state is created and linked to the user via the same tcp socket  used to connect to the server. once the dm identifies the client and approves a connection, a list of atlases/datasets available for browsing is returned. the client requests the desired atlas, and the server returns the dimensions and resolution of the chosen dataset. subsequent session communication between the client and the server include serialized requests for slice  data based on the client's specified plane orientation, and requests to show/hide any available segmentation, including presentation metadata.

the xml server uses a single-transaction oriented protocol much like http. the user logs in, is approved, and can then request information about the selected atlas, including 2d paths representing the intersection of a segmented volume and a specified slicing plane, etc. per-client state data is tracked via a unique session id which expires after a period of inactivity. this interface is an implementation of an inter-atlas interoperability standard developed within the mouse birn subproject of birn <cit>  which was designed to promote atlas data sharing amoungst the mouse birn neuroanatomical data management clients neuroterrain, smart atlas and shiva. figure  <dig> provides an architecture diagram to support the description given below.

slicers
the data slicer runs in an independent thread, and accepts slicing requests from a queue fed by the servers. there is no theoretical limit to the number of instantiated slicing threads, but there is generally no practical advantage to having more slicing threads than processor cores on the host computer. each slicer thread is able to handle both image pixel requests from the binary server and segmentation path requests from the xml server. results of the slicing operation are returned to the client by the slicer via a return queue.

the key to the real-time responsiveness of the slicer component is the use of the macrovoxel 3d data format which provides for an extremely fast nearest-neighbor-based assembly of any arbitrary place on or off the original imaging axis <cit> . both the pixel data sets as well as the associated segmented volumes of interest  are stored in an efficient format.

binary slicing requests are serialized. this enables client to provide live navigation by tracking the mouse pointer, sending a continuing stream of slice requests as the mouse moves. a slicer thread checks each incoming slice request serial number against the current value in the client's state data; stale requests are dropped. returned slices are also serialized; several slicing threads may be running, and two different threads may be processing requests from the same client. the client ignores stale slices returned. region of interest  path requests are not serialized, since the order in which they are returned is not critical to the way they are processed by the client and presented to the user. this enables the server to process roi path requests more efficiently.

data manager
the dm is responsible for loading, unloading and maintaining access to datasets, and associating ontologies with the datasets. it also manages user access permissions to these datasets.

process queues
the process queues are used to communicate between the server threads and the slicer threads. requests for slicing service are placed in a queue as a data structure including client id and state information, target information, and requested action. each slicer calls a pqm function that blocks waiting on a condition variable for queue state. once a request is received, the slicer examines the request type, and dispatches it to the appropriate handler – binary or xml. this handler satisfies the request, and then returns the resulting data to the pqm return queue, where it is then forwarded to the client.

this module, implementing queues which the slicing thread feeds off, readily allows for multiple slicing threads, and also allows for threads to be started or stopped on the fly, if so desired, greatly enhancing the scalability of the server.

major client modules
the nt-sdk framework includes two major branches. the first encapsulates the api to communicate directly with the server providing an interface other software developers may use to obtain specific 2d slices and associated meta data  directly from the ntas.

the other library, contains gui components, is end-user oriented to support a researcher interacting directly with the server. this later module uses the former to handle its communications with the server and is encapuslated in a pair java jpanels for easy inclusion in other applications. the first class encapsulates the virtual knife navigator , while the other provides a viewer for a server-hosted data set. the core client connection controller has been implement to support instantiating the sv jpanel as many times as necessary to open multiple connections to a single atlas data set  or connections to multiple data sets. vk action drives sumultaneous navigation through all open views. the view classes can be created as jpanels for tight integration with the parent application, or as free-floating jframes.

server interfaces
the ntas interactions are encapsulated in the nt-sdk-ntas-comm framework. within this library, the client communicates with the server via two discrete sub-interfaces. one, derived from the original macostat server, transfer all data in binary format – values are encoded as either  <dig> bit floats and integers or strings . integer codes transmit ntas request commands chosen to give recognizable display in a binary data stream. this interface provides efficient, dynamic image-oriented transactions: return of a sliced volume being the main example, but control of the vk is also included here. the binary commands of this socket protocol are implemented as a collection of java classes handling all the communication details, making server easily available to external programmers by merely creating a new object of a particular command class and sending it to the communication manager's write command. a communication controller embedded in the nt to mediate server transactions,

the nt-sdk-ntas-comm also provides callbacks as a java interface to monitor ntas responses to the binary commands – e.g., the latest slice image or associated metadata. by implementing these methods, an applications can dyanically react to new server responces as required.

the other request interface in the nt-sdk-ntas-comm library is based on xml-formatted text streams, and is used to poll atlas state: information such as the 2d planar location within a mounted data set, paths describing the intersection of vois with the vk, and so on.

detailed specifications for these protocols can be found at the web site  <cit> .

graphical user interface
the gui consists of:

• the vk jpanel used to set the current 3d atlas cross-section view ;

• a set of related controls specifying how dragging of the mouse over the vk translates into movement of the knife ;

• a slice viewer  jpanel for each atlas data connection ;

• a collection of menu commands providing additional functionality .

this interactive functionality is embedded in the nt-sdk-netostat-gui framework. by obtaining these jpanels from this library, an enclosing application incoporates the entire binary server api, as well as the gui controls listed above. detailed specifications for the nt-sdk api can be found at <cit> .

the knife is presented as an intersecting plane within a wireframe rectangular volume whose dimensional extent is the x, y, z bounds of the 3d atlas dataset under view . a series of tool buttons enables an user to specify how mouse dragging over the wireframe afects vk movement. the investigater may translate the knife along the slicing axis, or rotate it about any of the three cartesian axes. a series of buttons in the sv control panel enables the user to select the cutting axis to present slices along any one of the standard anatomical axes – coronal, horizontal, or sagittal . thus, a user can slice through the 3d data set in an arbitrary plane either on or off axis. each user intiated vk movement triggers a request to the server resulting in an updated atlas image in the sv . a client preference setting enables an investigator to adjust the scaling of mouse cursor movement to knife movement on the server .

the nt-sdk-netostat-gui also can provide the vk and sv interfaces as separate, floating frames and enable the researcher  to create multiple ntas connections. each of these distinct svs  comes with a collection of tool buttons supporting easy view specific actions: switch slice axes, select vois, pan & zoom, save knife locations, and save atlas grayscale views. a single vk provides coupled navigation across all views . this enables the researcher to simultaneously co-navigate multiple, aligned macrovoxel data sets. from the point-of-view of external programmers, each sv is provided as a separate object not only providing access to the jframe itself, but also the callbacks tied to each one . all navigational interoperability amoungst the distinct sv connections and the vk jframe is handled internal to the nt-sdk, though this activity can also be driven from the external program. the programmer can also request these views and the vk interface be passed as jpanels for tighter integration into the external program.

the multi-threaded nature of the server makes it possible to support these parallels data connections. multiple ntas instances can also be setup when user demand requires more slicing throughput.

a separate dialog allows the user to examine the list of volumes of interest or segmentated regions included in the atlas, select a subset for display and to specify a particular color for the volume . these colored rois are superimposed on the main display either as a semi-opaque mask or as an outline . a researcher can toggle individual rois on/off, or selectively dim certain ones to emphasize a selected region among a set of related regions.

a collection of hierarchical menus under the main neuroterrain atlas server menu supports the following, some of which are described in more detail in the following section on client-server interactions:

• login to and logout from and ping the server;

• select atlas data sets for viewing;

• test network throughput to a given ntas.

these features are all described in more detail below.

client-server transactions
in this section, the various interactions between client and server are described. there are five basic transactions: client login, selection of an atlas or dataset to browse/query, selection of an axis along which the vk is moved, knife positioning, and requests for 2d path cross-sections through segmented volumes of interest.

login
prior to performing any other action, the client must log in to the server. this allows the server to set up the necessary session structures, and registers the particular client implementation or user for any special actions desired. for example, at this point the user-agent name is transmitted by the client. on the server side, this can be used to select a particular default dataset, to specify that only a subset of metadata is returned to the client, or restrict the client to a subset of mounted data sets. the user agent is a configurable value in a java properties file within the nt-sdk, so clients can be distributed to a user base with unique and defined set of default conditions the server can enforce. using either the binary or xml api, the login message contains the user-agent id, and optionally a user id. once the login message has been processed by either server protocol, an acknowledgment is returned. in the case of the binary api, the return message contains a list of all available 3d datasets , associated with a reference number. in the case of the xml interface, the returned message contains the unique session id. a client logged in via the xml api can then send a query for a list of available datasets.

atlas selection
once successfully logged in via the binary api, the client further configures the session by choosing which available dataset to view. this results in the server returning a data block containing the dimensions and resolution of the requested dataset. when using the xml api, a client does not declare a specific atlas data set up front, as the desired atlas must be included in all subsequent requests.

axis selection
after selecting an atlas, a client using the binary api must specify the axis along which the vk will be moved. this value defaults to the z-axis, which provides the anatomically standard coronal tissue presentation the x-axis and y-axis similarly provide sections in the sagittal and horizontal orientations, respectively. a coronal view of the neuroterrain mouse brain histological atlas is presented in figure 2c.

knife position change
once atlas and axis selections have been completed, the system is ready to return sliced volume data. to initiate slice retrieval, the client sends a knife position request. this involves specifying a sequence number for serialization purposes, an offset along the slicing axis, and a triplet of angles describing the orientation of the vk to the slicing axis . the server queues this request for the slicer, along with client address information and the client sequence number. in turn, the slicer executes the slice requests, and returns pixel data as a series of linear pixel arrays or rasters, along with raster positioning information and the slicer's sequence number.

roi request
a client using the xml api is able to request paths describing the intersection of vois with the vk. to do this, the client provides identifiers for the vois to be sliced, and the orientation of the vk. presently, brain region names and abbreviations developed in our lab are hard-coded into a list and linked to the assocated voi data set. we are in the process of incorporating a more dynamic means of associating anatomical segmented region identifers from a public knowledge source, such as neuronames <cit>  in the case of brain region volumes. a complete set of the available vois for the current atlas dataset on view may be obtained from the server. voi slice requests are also queued for the slicer, and the 2d roi cross-section for the current knife position are returned in the form of an ordered set of cartesian coordinates. more specifically, individual rois are returned to the client as pointlist 3d objects where all points lie in a single plane.

other operations
several other operations are available. for example, the client can specify a magnification or zoom level for the returned slice images, currently spanning the range from  <dig> × to 10×. to support dynamic response to knife movement at higher zoom, the view is cropped and the user can pan the cropping window across the entire extent of the data set. server throughput and latency  operations provide a means for a researcher to assess the performance of their network connection from any point on the internet.

RESULTS
the system described here was designed with flexibility in terms of functionality, and ease of use or implemenation for the client in mind.

system flexibility
by settling on a client-server implementation, we are able to isolate the low-level implemenation details of the slicers, and image data structures from the high-level implemenation of the client. this means, for example, although the server currently sources image data from disk files, it is entirely possible to source all or part of the dataset from a database. this would allow individual users, for example, to specify their own segmentation to be served by this system. making this a back-end function means that this enhancement may be provided to the client with no change needed to the client software.

another change would be addition of, for example, the ability to serve datasets with higher resolution areas of interest, or to provide overlaid data from mutiple spectral channels.

migration of the server to larger, more capable hosts as demand dictates likewise is invisible to the client. the only change evident to the client is that the service seems faster.

it is possible applying a standard compression algorithm such as lempel-ziff-huffman to the image slice pixels would also help to boost throughput  given many pixel series consist of runs of minimal variation. however, given the local nature of these algorithms, using compression effectively with the rasterizing performed by the ntas return queue will require considerable experimentation and testing.

though the xml interface uses a ubiquitious w3c encoding standard to exchange information, we would also like to adapt this protocol to the web service description language , so as to make the server more interoperable in a service oriented architectural  environment. we have developed wsdl services as a part of the neuroterrain image repository framework  <cit>  and see the utility they can provide, especially for database controlled distribution of binary image data.

ease of implementation
the nt-sdk is designed to simplify the process of including a ntas client within other java applications. in fact, the stand alone client application netostat distributed via our web site for the mac os x, windows xp, and linux os platforms is merely the nt-sdk wrapped in a java source file with a main() function and just a few dozen lines of custom code to instantiate two helper objects  and implement atlas monitor and driver java interface methods. the nt-sdk also provides a simple means to simultaneously present multiple axis views of a single atlas data set , though we've not fully implemented this yet in the distributed netostat client. we have also created other custom applications specifically designed to use the ntas client to jointly browse and analyze other 3d mouse brain data sets aligned to our atlas  <cit> , as well as to examine mulitple, aligned atlas data sets in synchrony. in each case, the only code written to provide the ntas client functionality beyond ~ <dig> lines of api glue code is that needed to add new functionality beyond what the nt-sdk already implements.

performance and scalability
the use of a processing queue and multiple slicing threads allows the core module of the server to take full advantage of advanced multiprocessor hosts. we have implemented optimization logic on both the server and client side to streamline the image rendering to a series of client knife movement requests over the current internet. table  <dig> provides a brief summary of throughput testing we have done under varying network and operating system conditions. one should note, given the underlying collision-sensitive nature of ethernet transmission, the additional software layers present between the application and the os-level tcp/ip stack, and the complex logic associated with opening and controlling communicating sockets make such metrics prone to considerable moment-to-moment variablity; thus, one needs to take variability into account when assessing these metrics. even though ping latency was measured, the metrics taken are clearly approximate, especially without normalization using an independent measure of network throughput.

 <dig> identical knife slice requests were sent in rapid succession . requested cross-sections consisted of  <dig> ×  <dig> . were all  <dig> returned in their entirety to the client, they would consist of ~ <dig> separate  <dig> pixel rasters. those rasters returned by the server were not rendered in order to control for client display performance. metrics for both throughput and latency are the mean of  <dig> independent tests run ~ <dig> seconds apart. as indicated, not all requested images were returned in full due to the efficiency logic in the server and client. the metrics, based on those pixel rasters returned to the client, are: throughput, frame rate , application latency , transmission time , completed frames , partial frames , partial frame % , ping . for all of the tests where client and server were on separate machines, one of two ntas servers were used , each of which had the following configuration: dual  <dig>  ghz g <dig> ppc macintosh xserve/ <dig>  gb pc <dig> ecc sdram  running mac os x  <dig> . <dig>  the following clients configurations were tested: pbg <dig> ; 3gp4-mdklnx ; 3gp4-winxpsp <dig> . all clients machines used the jvm v <dig> . <dig> to run netostat – for linux and windows xp as distributed by sun microsystems; for mac os x as distributed by apple computer. all tests were performed at  <dig> am est, when both the internet and lan activity is at a minimum and is the most stable.

with those caveats in mind, the following observations can be made:  for all machines tested, a standard, university lan connection  provided excellent throughput ;  though considerably slower , when connected over the public internet , performance was acceptible for such an application – ~ <dig>  seconds to fully complete image transfer for a stream of knife requests typical of that generated via vk knife dragging action . this is reasonable given typical university-to-university available bandwidth . on an internet <dig> trunk, performance would be much closer to lan performance given the greatly decreased latency and increased bandwidth ;  throughput over a typical, home internet connection  whether to a proximal region on the internet or from east coast  to west coast  was ~ <dig> times slower than when using a typical university-to-university internet connection and a bit too sluggish for practical use;  application latency for windows and linux is much longer than on mac os x, essentially injecting a  <dig> second delay prior to image refresh. given the differences in jvm, os, and hardware, it will require additional low level testing beyond the scope of this study to determine the root cause of this delay;  the algorithmic efficiencies actually make performance fall off a little under very low latency conditions . though this is not a typical runtime scenario, it could be addressed in the future by modulating the communication logic using runtime latency and bandwidth data generated with the throughput/latency testing methods embedded in the nt-sdk. in summary, as relates to end-user, interactive experience using netostat, the total refresh time in response to a typical vk movement is  <dig>  and  <dig>  seconds for lan and university internet connectivity, respectively – quite acceptable for the intended use of the ntas-netostat architecture.

CONCLUSIONS
the overiding application driving development of this client-server 3d data set analysis environment has been the pressing need within neuroanatomics  to provide an easy-to-use platform promoting the synergistic use of:  brain atlasing systems based on full 3d representation of gray-scale data for both histological <cit>  and mri <cit>  brain atlases supporting rapid slicing through the volume in arbitrary planes;  existing public terminological respositories for neuroantomical information ;  ontological frameworks with knowledge both of terminology and basic spatial relations . the overall goal is to use such a system to support automatic and semi-automatic, meta-analysis of the accumulating large-scale, anatomically-mapped, mouse cns data sets  <cit> . this system is also critical to providing a means for researchers to examine and analyze data acquired on our cryoplane fluorscence microscope  <cit> .

the system described here provides a powerful means of displaying atlas and other 3d biomedical image data. it fully supports requirements  <dig> and  <dig> above, and we expect to integrate a means to provide for  <dig> in the future. by moving most of the heavy lifting from the client desktop to a dedicated server and using our high-performance macrovoxel format on the server side, the data presented may be made more accessible to a wider range of end users. the nt-sdk specifically provides a very simple means to combine data from many different neuroterrain server hosts, or inclusion of atlas data served by a ntas in a wide range of end-user applications, with little effort on the part of the developer. the value of being able to browse atlas-aligned data sets, in an intuitive way, such as the vast mouse brain library, can not be overstated. finally, we believe the implementation by nt and other mouse birn atlases of the birn atlas interoperability api provides an example of how to address the emerging need for such standards to support sharing of biologically relevant segmented geometries derived using a variety of tools from image data sets. this interoperability standard can serve as an interface around which a community atlas mediation service could be constructed.

availability and requirements
project name: neuroterrain atlas server & netostat neuroterrain client;

project home page: ;

operating system: platform independent;

programming language: nt atlas server ; netostat ;

other requirements: java  <dig> . <dig> or higher required for netostat

licence: gnu gpl

written in java, the nt-sdk has been tested on macos x , windows xp sp <dig>  and linux  using java  <dig> . <dig> and  <dig> . <dig>  it is available both as source code and compiled byte-code on an as-is basis. source code for the ntas is not currently available, but clients can obtain a server address from the same url listed above for use with the nt-sdk and/or the netostat client built on the nt-sdk. we plan in the future to create a 64-bit linux cross-compile build file for the server. at the present time we are not planning to distribute the ntas as source due to the complexity this engenders for further development and distribution of the nt-sdk both in source and compiled bytecode form.

list of abbreviations
api application programming interface

bgem brain gene expression map located at st. jude children's hospital in memphis, tn  <cit> 

birn biomedical informatics research network  <cit> 

cfm cryoplane fluorescence microscopy

cslm confocal laser scanning microscopy

dm data manager – the server module responsible for managing access by the rest of the server components to the image datasets provided by the server application.

gensat the gene expression nervous system atlas <cit>  sponsored by the nih national institute of neurological disorders and stroke  which currently includes the bac transgenic project located at the rockefeller university and bgem.

gui graphical user interface

jvm java virtual machine

mrm magnetic resonance microscopy

ntas neuroterrain atlas server

nt-sdk neuroterrain software development kit – the packaged java framework used by other java-based software seeking to act as a client to the neuroterrain atlas server – e.g., an applet, application, servlet, ejb-based application server, etc. it contains pre-built classes with data structures, convenience methods, and interface-based callbacks greatly facilitating the process of communicating requests to the nt atlas server and using the server responses effectively. it also includes an easy to implement jpanel containing both the nt atlas client controls and the viewing for atlas-derived data. the nt-sdk-ntas-comm portion of the sdk contains only the server communication and control classes. the additional nt-sdk-netostat-gui library provides all the interactive gui classes seen in the netostat application.

opt optical projection tomography

pqm process queue manager – the server module that provides communication between the server modules and the slicing modules.

soa service-oriented architecture

smart spatial markup and rendering tool – the interactive atlas interface developed for the mouse birn sub-project with birn

sv slice viewer

vk virtual knife

voi/roi volume of interest /region of interest 

wsdl web service description language

xml extemsible markup language – a standardized way of encoding a data stream such that the stream is mostly self-documenting and both machine and human readable.

authors' contributions
author cg developed the binary communication protocols, wrote the server and created an early prototype of the java-based atlas client.

wb wrote nt-sdk and assisted in debugging the communication protocols.

jn guided the project and provided overall direction and focus.

supplementary material
additional file 1
netostat for mac osx. this mac os x disk image file a build of the netostat client application best suited for use on the mac os x operating system.

click here for file

 additional file 2
netostat for windows xp and linux. this zip compressed archive file includes a build of the netostat client application suited for use on the linux or windows xp operating systems.

click here for file

 acknowledgements
the design and implementation of the neuroterrain atlas analysis environment was funded by the nih grant p <dig> mh <dig> and the nsf grant  <dig>  the bioinformatics research network is supported by nih grants rr08605-08s <dig>  and rr043050-s <dig> .
