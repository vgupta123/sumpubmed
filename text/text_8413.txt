BACKGROUND
immunological parameters are hard to measure. a well-known problem  <cit>  is the occurrence of values below the detection limit, the non-detects. in a project that we will use as an example in this paper, depending on the parameter, more than half of the data, concentrations of soluble biological markers in human blood, consists of non-detects.

non-detects  are a nuisance in statistical analysis. an ad-hoc solution is to fill in values for the nds, e.g. one half of the detection limit. this may be acceptable if only a few per cent of the observations are nds. if there are many of them, estimated values of means, standard errors and trend lines will be unreliable and conclusions may be wrong.

nds occur in many places in science and technology. they have received a lot of attention in the work of helsel  <cit> . although nds are extremely common in immunology, the literature about them is not very extensive. an exception is the paper by uh et al.  <cit>  that studies a number of approaches to analyse datasets with nds. in that paper quantile regression was not considered. we believe it to be a very useful tool, and like to share our experiences in this expository paper.

most statistical methods develop a model for the expected values of the observations. in an analysis of variance  these will be the mean values for different groups. in the case of the regression line y = ax + b, the parameters a and b allow us to compute the expected value of an observation y for every x, which might be age or time or another covariate, that we are interested in. in addition, we can compute prediction intervals, in which a new observation will lie with a specified probability. this type of model belongs to the standard toolbox that most applied scientists learn these days in their statistics lessons. modern statistical packages make it very easy to use them in practice.

regression and anova , use the so-called principle of least squares: parameters like a and b in the example above, are computed in such a way that the sum of the squares of the residuals is minimized. the residuals are the differences between expected values, according to the model, and the observations. if a part of the observations is wrong, because of many nds, the parameter estimates will be  wrong.

in this paper we propose to use quantile regression instead of the usual linear regression models. a simple example is provided by anova. instead of computing means per groups, one could compute the medians, also known as p <dig>  the 50th percentile. a familiar recipe for computing the median of a set of numbers is to sort them from low to high and pick the middle number in the sorted list. half of the data will be below this number and the other half will be above it. the key point is that the actual values of the lowest observations play no role: what matters is that they are lower than the median. so if we would have 30% nds and gave them small values, the computed median would still be the same.

if more than 50% of the observations are nds, but less than 75%, we are still able to compute the p <dig>  the number below which 75% of the data are found. in anova we can still compare p <dig> in the different groups and look for interesting differences.

for a regression line, the sorting recipe will not work. however, in the last two decades a very useful generalization of regression modelling has become available, quantile regression. with this method we can estimate regression lines, which allow us to compute for y a percentile of our choice for any value of x. the only condition is that all nds lie below the line. with many nds, as in our example data set, this means that it is not possible to compute a line for the median, but that the p <dig> is sufficient.

the outline of the paper is as follows. first, we introduce quantile regression. we have tried to limit the amount of technical material, keeping in mind the expected statistical level of our audience. we also show in this section how the required computations can be done relatively easily with the r system and the package quantreg <cit> . then we apply quantile regression to a real data set, with an extremely high number of nds. the paper ends with a short discussion.

methods
quantile regression
in this paper the words quantiles and percentiles will be used repeatedly. to avoid confusion we first give their precise meaning. the 90-th percentile is the number below which 90% of the data lie. it is also the  <dig>  quantile. so, when we use percentages we talk of percentiles, and when we use fractions we talk of quantiles.

in the introduction we described the familiar sorting algorithm for computing percentiles. it has a strong intuitive appeal, and it is easy to implement, or even to do by hand. however, it cannot be generalized to the case of a regression line or more complicated models. fortunately there exists another, more flexible approach, based on optimisation.

the mean of n observations, y <dig> to yn, is computed as y¯=∑iyi/n. averaging is such a familiar process that one usually does not give much thought to the fact that the sum of squares

  s2=∑i <dig> 

is minimized when μ = y¯, the mean of y. the sum of squares is stated explicitly in more complicated models like a linear regression line and it leads to explicit expressions for optimal values of the parameters in the model. this is an extremely powerful statistical tool.

for percentiles we can also introduce a function that has to be minimized, in such way that the desired percentile minimizes it. compared to the sum of squares, two changes are needed:

1) replace the squares by absolute values, and 2) give different weights to positive and negative residuals. the residuals are the differences between the observations and the percentile that is being computed. as a formula: minimize

  s1=∑iwi|yi−q| 

here q is the p-quantile  for a chosen value p  and wi is the weight of observation i, computed as

  wip=p if yi>qp 

  wip=1−p if yi≤qp 

in the case of the  <dig> -quantile, the positive residuals get a nine times larger weight  than the negative ones . it might not be directly obvious why this procedure leads to the desired quantile, but after some mathematical adjustments one finds indeed that 90% of the observations have to be below q <dig>  to minimize s <dig>  an intuitive explanation is that every observation above the quantile has to be balanced by nine below it.

now that we have an optimisation criterion, it is very easy to extend the quantile idea to more complicated models. in the case of a linear regression line, the function to be minimized is

  s1=∑iwi|yi−a−bxi| 

it will be clear that we can generalize this to more complicated models. notice that generally the values of a, the intercept, and b, the slope, change with p.

it is easy to state the function that has to be minimized, but computing the solution is harder than for classical models . fortunately, there is excellent open-source software available, free of charge. we did our computations using the quantreg package for the statistical software system r  <cit> . fitting a linear regression line for the 90th percentile is as simple as writing model = rq. the parameter tau  corresponds to p in our formulas.

with quantile regression it is not possible to get p-values for model coefficients like slope and intercept; instead the quantreg package delivers 95% confidence intervals .

although it is not an issue here, quantile regression is very robust against outliers, in contrast to the mean and least squares regression. also a normal distribution of errors is not assumed.

for those interested in statistical backgrounds of quantile regression, we can recommend a paper by koenker and portnoy  <cit>  and a book by koenker  <cit> . an interesting paper from an applied point of view  is the one by cade and noon  <cit> .

RESULTS
an implementation
to illustrate the use of quantile regression in immunology, we use data from the stardrop-study, a randomized placebo-controlled trial in  <dig> youngsters  with hay fever. a detailed description can be found in röder et al  <cit> . the main aim of the study was to determine the effect of sublingual immunotherapy  with grass pollen allergen on nose and eye symptoms . allergen-specific immunotherapy consists of the repeated administration of the allergen to which the patient is allergic, with the intention to modulate the response of the immune system to the allergen  <cit> . in the case of slit, the allergen is administered under the tongue by drops or tablets. in a sub-study, the effect of slit and other factors on the immune system was assessed by measuring the levels of soluble biological markers  in plasma during the trial. serum samples were collected at five time points during the two-year treatment period: baseline , after 6 months , after 12 months , after 18 months  and after 24 months . all samples were collected outside the grass pollen season. the samples were analysed for their il- <dig>  ifn-γ, tnf-α, il- <dig>  il- <dig>  sicam- <dig>  se-selectin and sil-2receptor content. the following factors were studied: treatment, age, gender, cohort , time points and co-sensitisation to birch pollen and house dust mite.

out of the  <dig> youngsters included in this sub-study,  <dig> subjects were observed all  <dig> times and  <dig> only once. the  <dig> remaining subjects were observed  <dig> to  <dig> times.

we start by presenting histograms of the measurements, emphasizing the need for a statistical method that can handle a large proportion of nds. then we compute trends with age using quantile regression.

distributions and non-detects
the samples were analysed in two parts, because an interim-analysis had to be presented to our sponsor. as a consequence, two different assays with different detection limits were used.

initially, for the time points t <dig>  t <dig> and t <dig>  the production of the sbms was detected with enzyme-linked immunosorbent assay . the sensitivity limits for quantitative determinations were  <dig>  pg/ml ,  <dig>  pg/ml ,  <dig>  pg/ml ,  <dig>  pg/ml ,  <dig>  pg/ml ,  <dig>  pg/ml ,  <dig>  ng/ml , and  <dig>  ng/ml . for the later time points t <dig> and t <dig>  the sbm production was measured with cytometric bead assay flex sets . the sensitivity limits for quantitative determinations were  <dig>  pg/ml ,  <dig>  pg/ml ,  <dig>  pg/ml ,  <dig>  pg/ml ,  <dig>  pg/ml ,  <dig>  pg/ml , 5 pg/ml , and  <dig>  ng/ml . the measurements above the detection limits were not affected by the change in assays.

we chose to apply the detection limits of one method to all data. the detection limits of the first method  were used because these limits were higher than those of the second method  for all sbms except il- <dig>  for il- <dig> the detection limits of both methods were used. all values below the detection limit were replaced with the value between  <dig> and the detection limit.

for the analysis and presentation of the data in this implementation we use the logarithms  of concentrations. the highest concentrations measured were around 5000 pg/ml, the lowest were always at the detection limit. because of the enormous range of the concentrations, the highest ones being more than a  <dig> times higher than the lowest, we work exclusively on the logarithmic scale.

summarizing, due to changes in detection limits and the presence of a substantial number of nds for some sbms, classical statistical analyses, like anova and regression, could not be applied to this dataset.

trends and quantile regression
one of the research questions was to determine whether concentrations of sbms change with age. we use quantile regression for p <dig>  the 75th percentile. in general any quantile level can be modelled and it would be more attractive to model p <dig>  the median. the percentage of non-detects for some sbms, however, was more than 50% and therefore we have to settle for a higher percentile. we chose p <dig>  and we are aware that this arbitrary. any percentile can be chosen, as long as it results in a quantile regression line that is above the values of non-detects everywhere.

before a quantile regression line on age only was calculated, the influence of the variable “time point” was explored. figure  <dig> shows an example, again for il- <dig>  the data points have been “jittered” by adding small  random numbers to the time point, shifting the dots in horizontal direction and thereby giving an impression of the distribution of the concentrations. for each time point the p <dig> midpoint with its 95% ci is presented. the line represents the p <dig> quantile regression line with only “time point” as an explanatory variable. because this line showed a slightly increasing trend, “time point” was incorporated as an additional factor  in the analysis on the effect of age. also visible in figure  <dig> is the change in the analytical procedure between t <dig> and t <dig>  leading to an increase in detection limits. as stated before, the measurements above the detection limits were not affected by the change in assays. figure  <dig> shows the results of the analysis on the influence of age on the il- <dig> levels. age was rounded to integer years and again the data points have been “jittered” in the horizontal direction for better visibility. two lines are presented in figure  <dig>  the full line is the result of quantile regression on age only. the broken line adds the factor “time point” as an explanatory variable. thus, age appears not to have an effect on this sbm.

discussion
immunological datasets often contain many non-detects. when a signal produced by the stimulant is too small for the instrumentation to discriminate the signal from the background noise, a value cannot be determined precisely. values below a given detection limit are called non-detects . the presence of nds will cause the data to be left-censored and special attention should be paid to selecting the appropriate statistical method to analyse such a censored dataset.

several statistical methods are being used to deal with nds. uh et al. evaluated the performance of several commonly used methods in immunology and more advanced methods used in other fields such as environmetrics and econometrics via simulation studies  <cit> . two often-used approaches, deletion or single value substitution followed by linear regression, did not perform well. because nds are not missing at random, bias can be expected when dropping nds. uh et al. showed that even with a nd proportion of 10%, the bias was unacceptable. substitution of nds with  <dig>  half of the detection limit or the detection limit itself, followed by linear regression, underestimated the variance. two more sophisticated methods, the tobit method and the multiple imputation technique, performed well but only when the proportion of nds was less than 30% and 50%, respectively. in our dataset, for some markers the percentage of nds was higher. furthermore, use of the tobit method requires that the normality assumption is met. like in our dataset, immunological measurements are often positively skewed and even after logarithmic transformation normality cannot always be achieved. therefore, we had to seek for a method that could handle large proportions of nds with no assumptions on the underlying distribution. we explored the use of quantile regression, a generalization of percentiles to regression models. like for the computation of simple percentiles, the only information that is being used is whether observations are below or above estimated model values. if the number of nds is not too large, one can estimate models for p <dig>  the median. in extreme cases, like for some immunological markers in the data set we used as an example, it is necessary to go to higher percentiles. in fact we chose p <dig> 

we illustrated quantile regression with data from a clinical trial in youngsters with hay fever, in which the effect of immunotherapy treatment and other factors on the immune system was evaluated by measuring levels of soluble biological markers . we showed that groups can be compared and that meaningful linear trends can be computed, even with very large fractions of nds. the slope of the regression line for a percentile is the same as that for the mean in the case of a linear relationship plus errors with a constant variance, the common default assumption in linear regression. that means that the estimated slope for the p <dig> is also a very good estimate for the usual regression slope that would be obtained if nds did not occur.

we have not discussed efficiency. it is true that quantile regression uses less information, that is, only the signs of residuals, disregarding their size, leading to wider confidence intervals and consequently loss of power. this means that if data are complete , estimated classical regression coefficients have more narrow confidence intervals than those obtained from quantile regression. but this knowledge does not help us much if we have many nds. when analysing our data set, we chose one percentile level, 75% for all variables. in principle it could vary with the fraction of nds, so that for some variables p <dig> could have been chosen. however, we felt that this would have made the interpretation more complicated.

the data have been analysed as  <dig> independent observations, which is a limitation, as  <dig> observations represent multiple observations on  <dig> participants. in the world of standard least squares statistical methods, one would use repeated measure anova or a mixed model for a proper analysis. unfortunately similar technology is not yet developed enough for quantile regression, although research is ongoing. nds can, however, generate unpleasant complications when using mixed models. it might happen that all or most measurements of some of the subjects are nds. consequently mixed models, which rely on fitting  individual coefficients to subjects, might be difficult to use. as far as we know, no statistical technology is yet available to handle mixed models with nds.

an alternative approach is reducing the problem to logistic regression, after setting proper thresholds . choosing the thresholds can be a matter of debate, which is avoided in quantile regression. in fact the quantile regression line acts as a “moving threshold” in such a way that on average  a quarter of the data lies above it. nevertheless, thresholding an logistic regression could be an interesting venue for longitudinal data modelling, because mixed model technology for binary responses is available.

in our application trends are so weak that there is no need for anything more complex than a straight line. but we remark that the quantreg package also allows computing more complex non-parametric trends.

CONCLUSIONS
quantile regression is a valuable addition to the statistical methods that can be used for the analysis of immunological datasets with non-detects.

abbreviations
anova, analysis of variance; cba, cytometric bead assay; elisa, enzyme-linked immunosorbent assay; nd, non-detect; sbm, soluble biological marker; slit, sublingual immunotherapy.

competing interests
the author declare that they have no competing interests.

authors’ contributions
pe performed the statistical analysis. hs was responsible for the soluble biological marker measurements. pe and er drafted the manuscript. all authors interpreted the data and critically reviewed, edited and approved the written manuscript.

