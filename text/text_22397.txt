BACKGROUND
next-generation dna sequencing  technologies have made huge impacts in many fields of biological research, but especially in evolutionary biology  <cit> . concurrent with the increased use of ngs technologies has been an improved understanding of the amount and type of data required to answer certain types of evolutionary and population genetics questions. for example, where mitochondrial dna  data are required, it is seen as increasingly necessary to obtain complete mitochondrial genomes. this is especially true in studies of humans, but for other animal species also  <cit> . the use of complete mtdna genomes can help mitigate the reduced phylogenetic resolution, homoplasy and ascertainment bias that is otherwise encountered when using markers for known single nucleotide polymorphisms  or shorter mtdna sequences   <cit> . for human populations that are poorly studied, complete mtdna genomes are even more important because there are often few known snps and, therefore, the relevant regions of the mitochondrial phylogenetic tree may be poorly resolved.

in addition to evolutionary applications, complete mtdna genomes are also being sequenced to identify markers associated with mitochondrial disease, and the advent of ngs has seen the significant expansion of research in this area  <cit> . for example, ngs of mtdna genomes is being used to clinically diagnose mitochondrial diseases in individuals with phenotypic evidence of mitochondrial oxidative phosphorylation disease  <cit> . the high sensitivity of ngs means it is also being used to discover diseases associated with low-level mitochondrial heteroplasmy that would be undetectable with conventional sanger sequencing  <cit> .

despite the increasing use of ngs technologies and a better appreciation of their importance, there remain significant obstacles to the successful implementation of ngs-based projects. these challenges often relate to assembling the constituent components of a ngs sequencing protocol into a single workflow to suit a given study. ngs workflows are often complex, and necessarily span everything from the generation of suitable starting template, to various molecular biological steps, to the generation of the raw sequence data, and finally to the bioinformatic steps required to convert those data into a suitable format for downstream phylogenetic or population genetic analyses. new problems can arise when scaling up a protocol for use on tens or hundreds of individuals; protocols need to be robust and remain time efficient. the bioinformatics steps offer their own challenges because although many of the individual components/programs are available  it is often difficult to get the outputs from one component into a format where they can be used as inputs for other components. taken together, the challenges in assembling a complete ngs protocol represent a major source of inertia for researchers wanting to undertake ngs studies for the first time.

here we present an ‘a to z’ protocol for obtaining complete human mitochondrial  genomes – from dna extraction to consensus sequence. although designed for use on humans, this protocol could also – with minor modifications – be used to sequence small, organellar genomes from other species, and also nuclear loci. an overview of the a to z method is presented in figure  <dig> 

all steps in this protocol are designed to be straightforward to implement; although the particular combination of steps is novel, the reagents and recommended equipment are widely available, and the bioinformatics is easily performed by non-experts . the molecular steps are scalable to large numbers  of individuals and all steps post-dna extraction can be carried out in 96-well plate format . also, the protocol has been assembled so that individual ‘modules’ can be swapped out to suit different scientific questions, facilities, skill sets and budgets. some of these alternatives are described in the protocol.

it should be noted that the protocol here is largely kit-based, with speed, efficiency and throughput the main priorities. labs with limited consumables budgets may wish to investigate non-kit-based alternatives for some of the more expensive steps. non-kit-based methods may, however, decrease reliability and increase labour costs through additional time spent preparing reagents, etc. the decision of whether to use a kit or a non-kit alternative may also depend on access to the equipment required in either case, and equipment requirements should be thoroughly investigated beforehand.

both consumables and labour costs vary dramatically across countries, and therefore we have not included a cost analysis for this protocol because it would not be broadly applicable. instead, it is recommended that researchers create a budget before using this protocol, where the costs of consumables are balanced against labour costs, the technical expertise required for different methods, the number of samples to be processed and the deadlines for the project.

although initial sample preparation is still largely carried out by researchers ‘in-house’, it is increasingly common to take advantage of the significant cost savings associated with out-sourcing ngs to an external provider. as such, researchers using the early steps of the protocol may wish to investigate out-sourcing the wet lab stages from library prep onwards. the decision of whether or not to use an external sequencing provider should also form part of the project plan.

methods
sample collection and dna extraction
dna was collected from participants using a buccal swab. two swabs were obtained for each participant . all samples were obtained with informed consent , ref. 2008/203). dna was extracted from the cheek swabs using a phenol–chloroform method . briefly, dna samples were digested overnight with proteinase k and then extracted with phenol, chloroform and isoamyl alcohol. dna was precipitated with isopropanol, washed with ethanol and eluted in low te ,  <dig>  mm edta ). dna was visualised by running 5 μl aliquots of each extraction on a 1%  agarose gel, and successful dna extractions were confirmed by the presence of a band of high molecular weight dna. dna samples were quantified using a picogreen quantification assay.

long-range pcr amplification of complete mtdna genomes
long-range pcr  is an efficient method for generating template for sequencing, especially in well-characterized taxa where lr-pcr primers can be designed easily . in less well studied lineages where primer design can be problematic, rolling circle amplification  has shown to be an effective alternative to lr-pcr for generating template for ngs  <cit> . even when lr-pcr is possible, some modification of the protocol may be necessary, such as for species that have at-rich mtdna genomes  <cit> .

the complete human mt genome was amplified from each individual by generating two overlapping long-range pcr products of 8511 bp  and 8735 bp  . the lr-pcr primers were designed using primer3plus , and with no, or very weak, predicted secondary structures. the primer-binding sites were positioned to be conserved across the  <dig> complete mt genomes in the dataset of pierson et al.  <cit> .

lr-pcr products were amplified using the expand long range dntpack . individual reactions contained 1× expand long range buffer ,  <dig>  mm of each dntp,  <dig>  μm forward primer,  <dig>  μm reverse primer, 3%  dimethyl sulfoxide ,  <dig> u enzyme mix, and  <dig>  μl  genomic dna in a total volume of 30 μl. thermal cycling conditions were: initial denaturation at 92°c for 2 min; followed by  <dig> cycles of denaturation at 92°c for 10 s, annealing at 55°c for 15 s, and extension at 68°c for 8 min 30 s; followed by  <dig> cycles of denaturation at 92°c for 10 s, annealing at 55°c for 15 s, and extension at 68°c for 8 min 30 s, with the extension time increasing 20 s/cycle for each subsequent cycle; followed by final extension at 68°c for 7 min; followed by a hold at 10°c.

pcr products were visualised by electrophoresis of a 2 μl aliquot of the pcr on a 1%  agarose gel. successful pcrs were represented by a bright band  of the expected size.

purification and quantification of lr-pcr products
lr-pcr products were purified using the ampure xp kit  and solid-phase reversible immobilization  technology. purifications were carried out using  <dig>  volumes  of ampure xp and exactly as described in steps 5– <dig> of meyer et al.  <cit> . purified dna was eluted in 25 μl of 10 mm tris . the significant advantage of the spri technology is that, using a multi-channel pipette, an entire 96-well plate can be purified in less than  <dig> hours. smaller numbers of samples can be purified using individual columns ) but this is very time-consuming and expensive with large numbers of samples.

a picogreen quantification assay was used to accurately quantify the purified lr-pcr products prior to fragmentation. to ensure the concentration values of the samples fell within the linear section of the standard curve, it was necessary to dilute an aliquot of the purified samples 20-fold, although the exact dilution required will depend on the quantification setup.

fragmentation of pcr products using nebnext dsdna fragmentase
for each individual, the two lr-pcr products  were pooled in equimolar ratios  to yield a total of 1 μg dna for fragmentation. next, the pooled dna was fragmented using the nebnext dsdna fragmentase according to the manufacturer’s instructions and additional file  <dig>  briefly, dsdna fragmentase generates dsdna breaks in a time-dependent manner, producing 100–800 bp fragments, depending on incubation time. note that the optimum incubation time must be determined empirically as described in additional file  <dig>  although we found it to be between  <dig> and  <dig> minutes.

alternatively, sonication may be used instead of fragmentase. we have used the bioruptor® pico sonication system, which has provided successful fragmentation following the manufacturer’s instructions and a  <dig> s/ <dig> s on/off cycle time for 7– <dig> cycles.

purification and quantification of fragmentase reactions
fragmentase reactions were purified using the polyethylene glycol–bead  solution described in additional file  <dig>  briefly, the beads are isolated from ampure xp solution and resuspended in a 10–30% peg solution, with the percentage determining the size cut-off below which fragments are removed. purifications were carried out using  <dig>  volumes  of peg–bead solution and exactly as described in steps 5– <dig> of meyer et al.  <cit> . purified dna was eluted in 20 μl of 10 mm tris.

a subset of the purified fragmentase reactions was run on the bioanalyzer  <dig> using a dna  <dig> chip to ensure that fragments were within the desired size range . typical bioanalyzer fragment profiles are shown in figure 2a. a picogreen quantification assay was used to accurately quantify the purified fragmentase reaction products prior to barcoding.

barcoding for parallel tagged sequencing
barcoding and pooling was carried out exactly as described in meyer et al.  <cit> , except that the ampure xp kit was used in place of the ampure kit.

sample library construction and sequencing
sequencing using gs flx or gs junior  offers a complete system for preparing sequencing libraries and generating sequence data. in concert with the instrument, kits were used for constructing libraries, carrying out empcr and sequencing the samples. most of the components required to undertake these processes are supplied in these kits, the exceptions being a system to accurately quantify dna and reagents for performing essential quality control on sequencing libraries. below we briefly summarise the gs junior sequencing process.

fragmented and barcode-tagged samples must be accurately quantified before commencing library construction. we recommend using a fluorometric method, for example, with picogreen or the qubit system . fragmented, tagged samples from different individuals are mixed in equal amounts to form a single pool of dna molecules. this pooled sample is used to construct the sequencing library. adding equal amounts of dna from each individual ensures equal representation of these sequences in the final data output. in the example described here  the samples are of equal length. if samples of different length are pooled for library construction then the mass of dna used for each sample should be adjusted accordingly to ensure coverage levels are the same across all samples . we have successfully constructed libraries from pooled dna samples ranging from 80 ng to 750 ng in total.

a single rapid dna library was constructed from each pooled sample using the flx titanium rapid library kit . library construction results in the ligation of specific adaptors to the ends of the dna molecules. following library construction, dna fragments less than 500 bp in length, including unligated adaptors, were removed from the sample using the ampure xp kit . this was achieved by first isolating the dna-bead mixture on a spriplate and then discarding the derived buffer as per the manufacturer’s protocol. the ampure xp beads were then washed in size solution  to remove short dna fragments and any buffer components from previous reactions. two washes using 70%  ethanol were then carried out according to the ampure xp protocol. the bead pellet was air-dried completely and the size-selected sequencing library eluted by resuspending the pellet in 53 μl of te buffer , 1 mm edta ). the sequencing library was transferred to a clean tube by drawing down the beads with the magnet prior to transfer.

library quality was determined in two ways. first, one of the sequencing adaptors is supplied pre-labelled with fluoroscein isothiocyanate , allowing a fluorescent plate reader to be used to determine the concentration of the library in molecules/μl . standards are supplied with the gs flx titanium rapid library kit . in our experience, these libraries yielded between  <dig>  ×  <dig> and  <dig>  ×  <dig> molecules/μl. second, the size distribution of the sequencing library is determined from a 1 μl aliquot run on a high sensitivity dna chip  on the bioanalyzer instrument . sequencing library dna fragment size distribution should be between 350 bp and 2000 bp with a peak distribution around 700 bp. libraries with fragment sizes significantly outside of this range should not be used; this indicates that fragmentase reaction conditions should be reoptimised for dna preparation.

preparation of the library for sequencing starts with emulsion pcr   <cit> , which was carried out using the gs junior titanium empcr kit  . this process begins with binding a single dna molecule from the library to a single sequence capture bead. the amount of dna added is critically important: if too much dna is added, the beads will bind multiple dna molecules resulting in mixed sequence on each bead and as a result will be unreadable; if not enough dna is added, the empcr will not deliver enough sequence capture beads for efficient sequencing. for this project, a ratio of two dna molecules per bead was used. the volume of library to add to an aliquot of sequence capture beads is calculated using the equation :

  μloflibrary=moleculesperbead×10millionbeadslibraryconcentrationmolecules/μl 

the steps from emulsion formation to biotin–streptavidin-enrichment were carried out according to the manufacturer’s protocols. following the enrichment process, approximately  <dig>  sequence capture beads should remain. fewer than  <dig>  sequence capture beads will be insufficient for a sequencing run. more than  <dig>  million beads remaining after enrichment indicates that there are too many beads coated with multiple sequences. these beads should not be used, as the sequences they hold cannot be resolved and they will be discarded from the final data set.

sequencing was performed using the gs junior and a ptp kit and sequencing kit . this method of pyrophosphate-based sequencing is described elsewhere  <cit> . each run took  <dig> hours to complete. control beads seeded onto the picotiterplate  at the time of loading independently indicated both the chemical and instrument performance of each sequencing run. the output from the sequencing run is a computer file , containing quality scores and raw data for each sequence generated from the run. only those sequences that pass five quality filters are present in the final data set. this ensures only high quality sequence reads progress into the analysis phase of the project. the final output from the gs junior typically yielded between  <dig>  and  <dig>  quality sequence reads with an average length of between  <dig> and 450 bp.

computational raw data processing
in the next step the raw sequencing data are processed for use in downstream analysis . here we present an easy-to-use bash-script-based pipeline that allows the user to automatically process sequencing files for single or multiple individuals. the presented pipeline runs on all unix-based operating systems. the step-by-step protocol is provided in additional file  <dig> and the associated scripts in additional file  <dig>  additional file  <dig> contains two example .sff files that can be used to perform test runs of the scripts. the presented pipeline consists of freely available standard tools for read mapping and post-processing, such as bwa  <cit> , samtools  <cit>  and our own scripts, which complement these tools. all incorporated scripts run either on python or perl, which should be pre-installed on unix operating systems. it can be used to map and process sequencing reads from different data sources, such as evolutionary genetics, medical research or even short, damaged ancient dna reads .

the presented pipeline first sorts individual reads according to their barcodes , then removes pcr priming sites, performs reference-based mapping and finally genotype and variant calling. different data quality and quantity statistics are included. these steps are discussed in more detail below.

de-multiplexing, removal of barcodes and priming sites and reference-based read mapping
in the first data processing step the .sff file is converted into a fastq file using sff_extract_0_2_ <dig> . the fastq format is similar to the commonly used fasta format, but also stores data quality scores in addition to the sequence information. the reads are then separated into per-individual fastq files according to their barcode using nuntag.pl . nuntag is based on untag , but is coded in perl and should thus be much easier to run than untag, which requires a haskell compiler and various additional libraries to be installed. the source code for untag is included in additional file  <dig> and, once it has been correctly installed, switching the pipeline to use it is straightforward.

in the next step, priming sites have to be removed from the reads because the primer sequence can vary from the priming site and thus might lead to calling false sequence variation. the tagcleaner software  <cit>   was used to remove the long-range priming sites. this software tool looks for the specific sequences within a specified distance to the 5′ and the 3′ end of the reads. to account for mismatches and partial primer sites present, the last five nucleotides of the respective primers were used. the trim_within regions was specified as  <dig> for humlr_1f and humlr_1r and  <dig> for humlr_2f and humlr_2r, respectively. alternatively, the freely available software tool adapterremoval  <cit>  can be used.

in the next step the cleaned reads are aligned using reference-based mapping. to do so, the pipeline applies the burrows–wheeler alignment tool  with the bwasw algorithm, which uses heuristic smith–waterman-like alignment to find high-scoring local hits  <cit> . this approach is very powerful when applied to long read data with a high error rate, but can be slower and less accurate for short low-error mappings  <cit> . the revised cambridge reference sequence  was used as a reference for the mapping. alternatively, other sequences can be used. for comparing called snps across datasets, the same reference is required for each. in some cases, the rcrs might differ substantially from the consensus sequence of the processed reads. in this case a second mapping against a reference for the inferred haplotype might lead to more reads being mapped.

downstream variant and haplotype calling
the resulting sam file is then processed with the software samtools  <cit>  to call the consensus sequence and variants such as snps. it should be noted that samtools  <dig> . <dig> treats n’s in the reference as a’s when calling the consensus. furthermore, wherever a region of the reference is covered by a single or multiple gaps in the reads, the program will call the nucleotide of the reference instead of the gap. thus, it is recommended that suspicious snps or regions in the original mapping are checked.

in the following step, the filtered snps output from bcftools  are transformed into an input file for the haplogroup-assigning tool haplogrep  using a perl script . the haplotypes can then be called online  using haplogrep.

it should be noted that the current setup does not allow for calling of indels. indels are insertions or deletions of point mutations. in recent years indels in mitochondrial dna and mitochondrial dna analysis in general have gained wide interest in genetic medicine  <cit> . however, data produced on the  <dig> platform shows an increased rate of false-positive snps  <cit> , due to problems in calling the correct number of nucleotides in polynucleotide stretches because of signal-to-noise threshold issues. this limitation might be overcome by deeper sequencing . however, studies have shown that a higher coverage is not sufficient to overcome this effect if homopolymeric nucleotide stretches are longer than  <dig> nucleotides  <cit> . studies in which indels are particularly important, such as on human diseases  <cit> , might need to adapt the approach by deeper sequencing and allowing samtools to call indels  or by avoiding using  <dig> altogether. it is recommended that indels are called using technologies with low indel error rates, such as illumina.

heteroplasmy  is a common phenomenon in human mitochondrial dna. thus, by default the pipeline includes ambiguity codes in the consensus sequence. however, it should be noted that the downstream haplotype assignment using haplogrep does not support heteroplasmic sites. therefore, the major nucleotide has to be determined by eye prior to using haplogrep. in cases of heteroplasmic length polymorphisms a de novo approach might be more appropriate than reference-based mapping .

read quality
the data quality of the mapped reads can be checked using the freely available software tool fastqc . fastqc can be used to infer sequence quality scores, gc content, read lengths distribution and to identify overrepresented sequences. base-calling algorithms, such as pyrobayes  <cit>  for  <dig> data, produce per-base quality scores by analysis of incorporation signals, so-called phred scores  <cit> . a phred score of  <dig>  for example, means that there is a  <dig> in  <dig> chance that the read is incorrectly mapped  <cit> . the distribution of phred scores can easily be assessed using fastqc. the software is further able to assess quality values such as read length, sequence gc content, etc. if the read quality is low, reads can be trimmed e.g. with the freely available software tool trimmomatic  <cit> .

coverage plots
coverage plots showing the number of reads overlapping each position in the reference genome are useful for quickly assessing mapping quality . the presented pipeline  automatically produces a coverage plot for each sample, which shows coverage level versus reference position as a greyscale bar graph and the average coverage level as a dashed line. plots are broken across lines and pages as necessary; the default scale factor fits up to  <dig>  nucleotides per row and  <dig> rows per page, which is convenient for examining multiple mitochondrial genomes, but these settings can be adjusted by the user. the plots are in high-resolution  postscript format and can be viewed or printed using the freely available ghostview program . a healthy coverage plot shows near-average coverage across the genome.

all coverage plots we have generated show a peak of high coverage between  <dig> – <dig>  bp, which roughly corresponds to the location of atp <dig>  at the time of writing, we do not have an explanation for the high number of reads at this location, but we have ruled out low gc content  and it does not appear to be associated with the end of the long-range fragments, as no equivalent peak appears near the overlap of humlr_ <dig> and _ <dig> between positions  <dig> and  <dig>  we have also ruled out any artefacts associated with the bioinformatics pipeline. the consistent appearance of this peak across all individuals suggest that it is a product of the nucleotide order in this region . this high coverage region has no effects on the consensus sequence obtained.

important considerations for alignment and assembly
a critical decision is the choice of the appropriate strategy for alignment and assembly of the sequencing reads. in general, two approaches can be used to obtain a consensus sequence or an assembly to call variants: “reference-based mapping” and “de novo assembly”. if a high-quality reference sequence is available, as in the case of the human mitochondrial genome, the sequencing reads can be mapped against this reference. reference-based mapping has some advantages over de novo assembly. since the reads are mapped against a reference, reads can be assembled even if regions in between are poorly or not at all covered . this allows consensus sequences to be generated even in the presence of missing data. furthermore, contamination or sequencing artefacts are usually filtered because they are unlikely to align to the reference.

reference-based mapping is commonly used in human genetic studies that are based upon mitochondrial genomes. a multitude of freely available software tools for reference-based mapping are available, including the commonly used software tools bwa  <cit>  and bowtie  <dig>  <cit> . available mitochondrial genome sequences such as the revised cambridge reference sequence  can be used for the mapping. however, due to the mapping algorithms, problems can occur in cases such as duplication or deletion of genomic regions. for example, a commonly found motif in mitochondrial genomes obtained from the pacific region is the deletion of a 9 bp  repetitive sequence, located between the cytochrome oxidase ii  and lysine trna  genes. this motif is commonly present in two copies in europeans and only one in the pacific or asia  <cit> . reference-based mapping of reads from an individual having only one copy against a reference containing two copies  can lead to false consensus calling. this phenomenon is due to the possibility of aligning this motif either to the first or the second copy in the reference. if different reads are aligned to different copies, the consensus will call both copies . this problem can be overcome either by applying realignment tools  or de novo assembly. unfortunately gatk cannot handle  <dig> data and was thus not included in the pipeline. for known deletion or insertion of repeats, the excessive copy in the reference can be substituted by “-”, which allows for mapping of the same number of or fewer copies.

de novo assembly is a powerful approach to align reads if no high-quality reference sequence or sequence of a closely related taxon is present. different free software tools are available to perform de novo assembly, such as velvet  <cit> , mira  <dig>  <cit>  or newbler . for a detailed review on available methods see  <cit> . de novo assembly is based upon the redundancy of short-read sequencing and the resulting possibility to find overlapping sequencing reads. this approach strongly benefits from the availability of longer reads  or the sheer number of data reads provided by next-generation sequencing platforms . the advantages of de novo assembly are that it is independent of any reference sequence and that it can be used to detect variants on a population level . disadvantages include substantially higher computational requirements and problems resolving contigs  into the correct linear order. although not implemented in the presented protocol, software tools such as mira  <dig>  <cit>   or the standard  <dig> de novo assembler newbler  <dig>   can also be easily implemented in the pipeline if desired.

RESULTS
dna extraction
dna extractions typically yielded 1–50 μg of high molecular weight total genomic dna . this dna was suitable for routinely amplifying long-range pcr products . in addition, dna extracted using this method was found to be stable at 4°c for at least  <dig> months, avoiding the need to repeatedly freeze–thaw the dna samples.

long-range pcr amplification of complete mtdna genomes
the long-range primers described in table  <dig> have been used to amplify mt genomes from phylogenetically diverse individuals , but it is possible that mutations in the primer-binding sites for some haplogroups may interfere with amplification. in these cases, it might be necessary to redesign the primers for different primer-binding sites, or to include degenerate bases.

the lr-pcr proved to be highly reliable, with > 95% of individuals yielding both lr-pcr products on the first amplification attempt. when the pcr failed, it was usually for both products of an individual, suggesting a problem with the dna, rather than with the pcr itself. in these cases, if a second pcr attempt also failed then that individual was re-extracted using the back-up buccal swab.

the concentration of the undiluted purified lr-pcr products was typically 200–500 ng/μl.

fragmentation of pcr products
although it is necessary to optimise the dsdna fragmentase reactions for a given template, this proved to be an efficient method for fragmenting the long-range pcr products of a large number of individuals, and produced consistent results. although mechanical shearing or sonication methods  may be suitable for small numbers of samples, enzymatic fragmentation allows higher throughput.

bioinformatics
the presented pipeline is an easy-to-use unix shell script that runs a series of programs  to transform a set of raw-read input files into a variety of useful output files. in addition to applying existing freely available software tools, new scripts have also been developed, e.g. to produce coverage plots, which show the number of reads overlapping each position in the reference genome for easy quality assessment, and to convert samtools output files into haplogrep input files for convenient haplotype calling. the pipeline has been set up to work for reads from the  <dig> sequencing platform , but it can easily be adjusted to be used for different platforms such as illumina miseq, hiseq or iontorrent . due to its modular organization, it is straightforward to change different parts of the data processing. recently, wilm et al.  <cit>  presented lofreq, a freely available variant calling tool , that has similar precision to samtools  <cit>  but shows a higher sensitivity in calling rare variants. alternative tools, such as lofreq, can easily be incorporated into the processing. our pipeline can process reads for hundreds of sequences in a very short amount of time . the performance is strongly dependent on the different tools used in the processing. for detailed discussions on the performance for different steps such as variant calling please see the publications for the respective tools.

CONCLUSIONS
here we present a protocol for sequencing complete human mitochondrial genomes. this protocol could, however, be used to sequence mitochondrial genomes from other species, and also nuclear loci of a similar length. our aim is for this protocol to help researchers who are new to next-generation sequencing make full use of this technology. the benefits of this protocol include that it is straightforward to implement, and that the molecular steps are scalable to large numbers of individuals. the bioinformatics modules are designed to be reasonably easy to use for researchers new to command line-based inputs. conscious of the different questions, facilities, skill sets and budgets available across research groups, we have assembled the protocol so that individual ‘modules’ can be changed to suit a particular project.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
acc developed the molecular biology protocols, drafted the manuscript and oversaw the integration of protocol components into a single workflow. sp and wtjw developed the bioinformatics protocols and drafted the manuscript. jals carried out the library construction and sequencing, advised on the molecular biology protocols and drafted the manuscript. mek developed the dna extraction protocol. eams contributed to the project design, provided overall guidance to the project and revised the manuscript. members of the genographic consortium assisted with the project design and revised the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
phenol–chloroform dna extraction protocol. pdf file of the phenol–chloroform protocol for isolating human genomic dna from buccal  swabs.

click here for file

 additional file 2
fragmentase digestion protocol. pdf file of the protocol for fragmenting lr-pcr products with nebnext® dsdna fragmentase™.

click here for file

 additional file 3
size selection protocol. pdf file of the protocol for dna size selection using ampure xp-derived peg–bead solution.

click here for file

 additional file 4
bioinformatics pipeline. pdf file of the bioinformatics protocol.

click here for file

 additional file 5
bioinformatics pipeline. tar.gz file  of the bioinformatics scripts.

click here for file

 additional file 6
example .sff files. tar.gz file  of two example .sff files.

click here for file

 acknowledgements
for technical assistance we thank aaron jeffs, martin kircher, sebastian lippold, ross marshall-seeley and christy rand. the genographic project is funded by ibm, the waitt family foundation and the national geographic society.

genographic consortium members: syama adhikarla <dig>  christina j. adler <dig>  elena balanovska <dig>  oleg balanovsky <dig>  jaume bertranpetit <dig>  david comas <dig>  alan cooper <dig>  clio s. i. der sarkissian <dig>  matthew c. dulik <dig>  jill b. gaieski <dig>  arunkumar ganeshprasad <dig>  wolfgang haak <dig>  marc haber <dig> , li jin <dig>  shilin li <dig>  begoña martínez-cruz <dig>  nirav c. merchant <dig>  r. john mitchell <dig>  amanda c. owings <dig>  laxmi parida <dig>  ramasamy pitchappan <dig>  daniel e. platt <dig>  lluis quintana-murci <dig>  colin renfrew <dig>  daniela r. lacerda <dig>  ajay k. royyuru <dig>  fabrício r. santos <dig>  theodore g. schurr <dig>  himla soodyall <dig>  david f. soria hernanz <dig> , pandikumar swamikrishnan <dig>  chris tyler-smith <dig>  arun varatharajan santhakumari <dig>  pedro paulo vieira <dig>  miguel g. vilar <dig>  pierre a. zalloua <dig>  janet s. ziegle <dig>  and r. spencer wells <dig> 

addresses: 1madurai kamaraj university, madurai, tamil nadu, india. 2university of adelaide, south australia, australia. 3research centre for medical genetics, russian academy of medical sciences, moscow, russia. 4universitat pompeu fabra, barcelona, spain. 5university of pennsylvania, philadelphia, pennsylvania, united states. 6lebanese american university, chouran, beirut, lebanon. 7fudan university, shanghai, china. 8university of arizona, tucson, arizona, united states. 9la trobe university, melbourne, victoria, australia. 10ibm, yorktown heights, new york, united states. 11institut pasteur, paris, france. 12university of cambridge, cambridge, united kingdom. 13universidade federal de minas gerais, belo horizonte, minas gerais, brazil. 14national health laboratory service, johannesburg, south africa. 15national geographic society, washington, district of columbia, united states. 16ibm, somers, new york, united states. 17the wellcome trust sanger institute, hinxton, united kingdom. 18universidade federal do rio de janeiro, rio de janeiro, brazil. 19applied biosystems, foster city, california, united states.
