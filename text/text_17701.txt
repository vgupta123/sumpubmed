BACKGROUND
numerous strategies have been devised to come up with rigorous methods for detecting differentially expressed genes in sets of microarray data . the majority of microarray analytical methods require n =  <dig> in each condition in order to perform statistical measures. due to the expense of microarrays, experimental imperfections such as poor hybridization and limited quantities of available biological sample source, it is not always possible to obtain the required sample sizes.

on an affymetrix expression array, such as the hg-u133a genechip, each gene is represented on the array by a number of separate  <dig> mer probes that correspond to a part of the gene sequence. many popular statistical methods including mas <dig> <cit> , rma <cit>  and gcrma <cit>  summarize probes into a single value for the entire probeset before performing statistical inference. in contrast, probe-level modeling has been used by the software packages affyplm <cit>  for quality-control purposes. there are also a number of statistical models that directly utilize probe information in statistical inference including logit-t  <cit> , fisher's combined p-value  <cit> , gmos  <cit> , and multi-mgmos  <cit> . despite performing inference on probes, rather than on probesets, these methods still require multiple experiments  in each condition.

in this paper, we explore the idea that it should in principle be possible to use the high number of probes in each probe set to substitute for repeat experiments. that is, instead of using repeated chips to estimate the variance for statistical inference, can we exploit the existence of multiple probes per probe set to estimate the variance? previously, hein and richardson have used a bayesian hierarchical model to estimate expression levels from probe level data allowing for analysis with n =  <dig> in each condition <cit> . in their algorithm, called bgx <cit> , inference is performed at each stage of analysis  <cit> . the bgx algorithm outperformed <cit>  existing probe level metrics such as the wilcoxon test statistic and another bayesian algorithm ebarrays <cit> . because the bgx algorithm requiring a markov chain monte carlo  model for each stage of microarray analysis it is a very computationally demanding technique. as an alternative, we introduce an algorithm called pinc  based on the cyber-t algorithm first described by bali and long <cit>  and a method we recently described for generating accurate p-values <cit> . we show that pinc has attractive characteristics when compared to cyber-t, bgx and other methods of performing inference on affymetrix microarrays at low sample sizes.

RESULTS
the performance of test statistics in ranking genes on a control data set at n = 1
on the affymetrix latin square hg-133a dataset, there are  <dig> probes per probeset. given  <dig> independent measures in two samples, there are a variety of statistical tests available to evaluate the null hypothesis for each gene that the expression observed in each sample is identical. these include the standard-t test, a paired t test  and the wilcoxon test <cit>  . in addition to these canonical statistical tests, there are variants of the t-test specifically designed for microarrays. these include the paired and unpaired cyber-t tests  <cit>  in which the variance for each gene is an estimate based on an average of the canonical variance for that gene and a background variance of other genes with similar intensities on each array .

we applied these different statistical measures to the affymetrix latin square hg-133a dataset, which consists of  <dig> conditions of  <dig> replicates each. each condition has  <dig> known genes spiked in at different concentrations that are true positives while the remaining  <dig>  probe sets on the chip are true negatives. we examined the first replicate from each of the  <dig> experiments and compared experiments where there is a 2-fold change in spiked in concentration resulting in  <dig> separate comparisons . applying the test statistics to these datasets yields for each statistic a gene list ranked according to the calculated scores. for each of these  <dig> comparisons, we can generate roc curves of the number of true positives versus false positives at each possible cutoff for these gene lists with n =  <dig> in each condition. figure 1a shows the average of these  <dig> roc curves in which the x-axis displays all  <dig>  true negatives. at this scale, it is immediately obvious that the bgx and wilcoxon tests underperform the other statistics while the paired and unpaired cyber-t tests perform the best.

while the data in figure 1a give a broad overview of how the algorithms perform, the scale of the x-axis does not represent a biologically useful signal. for example, at a false positive rate of  <dig>  a gene list for the hg-133a microarray would have over  <dig>  false positives. clearly such a gene list is not that useful. to better explore a more biologically relevant cutoff, in which a gene list consists of mostly true positives, figure 1b shows the same data as in figure 1a, but with the x-axis scaled to show only gene lists that include a small number of false positives. figure 1c shows the number of true positives captured at a cutoff of n =  <dig> false positives  for all  <dig> comparisons. at this more stringent cutoff the paired and unpaired cyber-t tests clearly outperform the other statistics.

the performance of test statistics in providing accurate p-values for inference
roc curves rank all of the genes in an experiment but generating a gene list in a "real" experiment also requires choosing a cutoff point. that is, it is not enough to rank genes into an ordered list, one must know how many genes to consider significant from the list. each test statistic generates a score for each gene and we wish to determine the threshold score above which genes are considered to be significantly differentially expressed. this has proven to be a challenging problem. in the microarray literature it is generally accepted that family-wise error rates, such as bonferroni correction, are too conservative in an effort to prevent type-i errors thereby producing an abundance of type-ii error  <cit> . the use of false discovery rates  has become a popular alternative for controlling error rates .

in this study, we evaluated the performance of different test statistics using the benjamini and hochberg  <cit>  and benjamini and yekutieli  <cit>  fdr cutoff levels , as well as the holm's step down method, a more conservative family wise error rate correction algorithm . for the fdr algorithms, we set the cutoff level at 10%, i.e., we are willing to accept that 10% of the genes considered to be significant will be false positives. for the holm's step down fwer, we set a cutoff level of  <dig>  divided by n  for the highest scoring gene pair. then for each subsequent gene, the cutoff is recalculated as  <dig>  divided by the number of remaining genes.

we have previously shown that, when applied at the probeset level, p-values produced by canonical statistics and the unpaired cyber-t test are not very accurate on control affymetrix datasets  <cit> . we proposed as a simple alternative, a method that assumes that all the background values on a microarray form a single distribution . we here propose a new algorithm pinc , which is the paired cyber-t test performed at the probe level in which the p-values provided by the cyber-t test are replaced with p-values generated by this assumption of a single background distribution. applying the pinc algorithm yields a list in which the rank order is identical to the paired cyber-t test  but the p-values differ. in figure  <dig>  we see that p-values generated by pinc do a better job of controlling fdr under both bh and by fdr; the sensitivity of pinc in nearly as good as the sensitivity shown by cyber-t paired, but the specificity is much closer to the expected level of  <dig> . indeed, no matter which of the three cutoff schemes we used to determine the threshold p-value of significance, the pinc algorithm nicely balanced sensitivity and specificity picking up a substantial fraction of true positives with a minimal number of false positives . all other algorithms perform poorly on either sensitivity and specificity suggesting that p-values calculated with these algorithms are either inappropriately large or inappropriately small. we conclude that when compared to other algorithms, the p-values produced by the pinc algorithm lead to inference that is less susceptible to bias introduced by the method of determining the threshold cutoff. that is, we argue that the p-values produced by pinc are more robust than p-values produced by the cyber-t software or by canonical statistical tests.

consistency in technical and biological replicates
our results suggest that, at least on the technical replicates of the latin square experiment, the pinc statistic produces p-values that allow for correct inference in discriminating true and false positives. because the p-values generated in  <dig> vs.  <dig> comparisons do not involve biological replicates, they cannot be used to evaluate biological variability; that is, they do not indicate the reliability of the observed difference in gene expression relative to biological noise across individuals. rather, the p-values reflect the magnitude of the differences between the samples relative to technical variability that arise from hybridization noise, optical noise, differences in rna degradation between the samples, artifacts that arise from probe selection and so forth. for the tightly controlled datasets such as the latin square dataset, the performance of the pinc algorithm at assigning p-values reflecting these sources of noise at n =  <dig> is clearly acceptable . however, what happens when we examine biological datasets in which biological noise, by necessity absent from the technical replicates that make up control datasets, makes up a significant component of the measured signal?

to begin to examine this question we first ask, what are the consequences in the latin square experiment of increasing sample size? we applied the pinc algorithm to technical replicates in the latin square dataset by analyzing n =  <dig>  n =  <dig> and n =  <dig> . for n =  <dig> and n =  <dig>  we determined the average value for each probe and then applied pinc in a pairwise probe to probe comparison similar to when n =  <dig>  by contrast, in most microarray experiments an analysis is performed at the probeset level; that is, an algorithm such as rma is applied to produce for each probeset on each array a single value and a test statistic is then applied to these values <cit> . we therefore included a comparison of pinc to a probeset level analysis, in this case using cyber-t . condition  <dig> in figure  <dig> shows the results of using quantile quantile normalization and rma summation <cit>  to power an analysis with cyber-t an n =  <dig> 

we next applied pinc to a series of biological replicates with varying degrees of biological noise. we chose to analyze an affymetrix dataset from a cell line study  that explored changes in gene expression of sw <dig>  a primary colon cancer cell line  <cit>  and an experiment extracted from human tissue with multiple human donors  that explores the regulation of the ubiquitin cycle in bipolar disorder  <cit> . we reasoned that the biological noise in the human tissue dataset would be higher than the biological noise from the cell lines, while the cell lines would in turn have more noise than the technical replicates of the latin square experiment . the experiments we chose all met the following criteria; the sample size needed to be at least n =  <dig>  the datasets needed to be a control versus treatment type of design, the datasets needed to be based on the affymetrix hg-u133a platform and the cel files publicly available. within each dataset with n> <dig>  three microarrays for analysis were randomly chosen using a random number selection program .

for each of these datasets, we compared the results obtained when using a probeset analysis with n =  <dig> with the nine possible analyses comparing individually each of the three chips in each condition. for the probeset analysis, we used "scheme 4"  <cit>  as described previously, which compares datasets at the probeset level using cyber-t and then calculates p-values by assuming a single background distribution. a gene list of significant results was determined from "scheme 4" using bh-fdr at 10% fdr. we call these gene results the "scheme  <dig> n =  <dig> probeset results" . next, using the  <dig> arrays in each condition, we generated  <dig> different lists of differentiated genes by performing all  <dig> possible comparisons using pinc with a single array under 10% bh-fdr . we then compared these  <dig> results to the "scheme  <dig> n =  <dig> probeset results" to determine how consistent the gene selection process was. figure  <dig> depicts a venn diagram of how these results are interpreted.

box plots showing the results of these  <dig> analyses for each dataset are shown in figure  <dig>  in the latin square experiments, genes detected by the  <dig> different pinc comparisons are in good agreement with the n =  <dig> gene list . as we proceed to the more diverse biological datasets, gene list agreement decreases to 68% and 32% for the cell culture experiment and tissue experiment respectively . for the human tissue experiment, the gene lists generated from the  <dig> different  <dig> to  <dig> comparisons show the highest level of variability . this is consistent with other tissue microarray experiments we analyzed . while this is not a surprise, it does emphasize the danger of analyzing tissue samples via microarray when sample size is low. the extent of variability suggests that when designing a microarray experiment, selection of sample size should reflect the noise of the biological source. these results suggest that a "one-size-fits-all" rule of microarray experimental design  is not always the best use of experimental resources. when biological noise is very low, a single microarray may suffice; when biological noise is high, many microarrays may not capture all of the variability in the system under study.

CONCLUSIONS
experiments with few numbers of repeats are ineligible for analysis via most published microarray analytical methods. we have shown that when applying analysis at a probe level using pinc, we are able to generate reasonable results on control datasets at n =  <dig> in each condition. for paired single microarrays, pinc outperforms both canonical statistics and a recently published method <cit>  while offering conceptually simple statistics and fast run-times. because the p-values are derived from a distribution estimated from all of the genes on the array, pinc also avoids the large p-values usually associated with low sample size microarray experiments. this allows for the possibility of using a more conservative cut off criteria such as family wise error rate as an alternative to false discovery rate when selecting a p-value cutoff for selecting differentiated genes .

the success of the pinc algorithm in performing accurate inference on the latin square dataset at n =  <dig> suggests that there is little benefit to performing additional technical replicates. this is consistent with previous literature  <cit>  as is our observation that one gets largely similar results whether one uses n =  <dig>  n =  <dig> or n =  <dig> in ranking the 2× latin square experiments . the ability to analyze single affymetrix experiments in a statistically rigorous way opens up the possibility of interesting analyses even for experiments in which multiple biological samples were collected. for example, in a cancer study in which cancer tissue is compared against non-cancer tissue from the same patient, we could generate gene lists consisting of genes that are differentially expressed at a given cutoff threshold for every patient in the study. this may yield very different insights than the usual practice of averaging the samples together and performing a single analysis to generate a single gene list. we know that diseases like cancer are very diverse with many different molecular mechanisms presenting similar clinical diagnostics. the ability to evaluate each patient individually in a statistically rigorous way may improve our understanding of the diverse causes of diseases such as cancer and may allow for better use of microarrays in personalized medicine.

