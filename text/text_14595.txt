BACKGROUND
evolution can be measured and studied on a number of different scales, one of which is through the determination and comparison of genetic sequence information. current-day gene sequences in living organisms have arisen through modifications of an array of ancestral sequences. duplication with modification is the central paradigm of protein evolution, wherein new proteins and/or new biological functions are fashioned from earlier ones.  <cit> . to detect these functionally  related proteins based upon similarity between their primary nucleic acid or amino acid sequences, a variety of sequence comparison algorithms have been developed. when a new gene is cloned and sequenced, it is now standard practice to use these algorithms to search for similarities between the translated nucleic acid sequence and a protein sequence database such as the ncbi non-redundant protein database   <cit> . sequence similarity that implies similar structure and therefore similar protein or enzymatic function is not definitive proof of such function; however, the results of database sequence similarity searches provide a starting point for researchers attempting to ascertain the function of an unknown gene by supporting the intelligent design of subsequent laboratory experiments.

the basic local alignment search tool   <cit>  is by far the most widely used pairwise-based sequence similarity comparison tool. it completes searches more swiftly than other tools, including fasta, ssearch  <cit> , and scanps  <cit> . blast uses an efficient, rapid algorithm to look for short segments or words of sequence similarity between two sequences that meet some predefined scoring threshold. after initially locating at least two of these words within a short distance of one another on a common diagonal, the algorithm uses them as "seeds" from which to extend the alignment to encompass longer regions of similarity, resulting in high scoring pairs . the heuristic algorithm used by blast decreases search time dramatically in comparison to that of other search programs. however, the emergence of high-throughput dna sequencing techniques has increased the size of sequence databases tremendously; thus conventional large-scale blast searching against the most commonly used databases has become infeasible on a pc or even a dedicated unix server. for this reason, new search strategies are needed.

while blast provides a balance between search sensitivity and speed, in many cases a researcher would like to detect more distant sequence similarities by employing search strategies that maximize the sensitivity of the search. a profile-based comparison which, for example, compares a sequence to a hidden markov model  representing an empirically derived estimate of all possible evolutionary changes for a protein of a particular function, generally permits identification of a much higher proportion of distantly related sequences  <cit> . there are two major profile-based comparison tools. psi-blast  <cit>  compares sequences with a profile model constructed dynamically during the initial search phase of a traditional blast search, while hmmpfam in the hmmer package from sean eddy at washington university compares sequences with a well-curated database of hmm profiles as well as models constructed by users. well-curated profile databases such as pfam  <cit>  are being developed through the combined efforts of bioinformaticists and molecular biologists. profile-based comparison has become a reliable way to gather information for predicting structure and function of unknown genes, and tools from the hmmer package are becoming a key centerpiece in many bioinformatics pipelines. the tradeoff of using hmm-based searches for increased sensitivity is the intrinsically slow nature of the viterbi  <cit>  or forward algorithm used in the application.

taking advantage of larger databases and more sensitive searching methods necessitates the use of high performance computing  platforms. traditionally, hpc has been synonymous with high-priced vector or parallel supercomputers, but rapid advances in microprocessor and network bandwidth technologies are changing the definition  <cit> . clusters of connected workstations utilizing commodity microprocessor systems provide enormous benefits in terms of cost and performance. thus, cluster computing can meet the increased computational needs of resource- and data-intensive bioinformatics applications. hpc environments using workstations connected via high-speed networks are becoming more and more popular in the bioinformatics community  <cit> . here we describe the ss-wrapper package, which provides tools to adapt currently available similarity search applications onto hpc environments implemented through linux clusters.

implementation
the objective of this study was to implement a generic wrapper application that could deploy similarity searching applications on a linux cluster. design criteria for the wrapper included the ability to deploy applications without the need to alter the original application and the ability to increase the speed of the underlying application in a linear manner dependent on the number of cluster nodes available. to meet these objectives, tools in the ss-wrapper package were written in c/c++ using the message passing interface   <cit> . tools in the ss-wrapper should work with few if any changes on any linux cluster running mpi utilizing any hardware platform. in addition to the ss-wrapper executables , executables for the underlying similarity search applications  appropriate for the hardware platform underlying the cluster will be needed. these can be obtained from ncbi and washington university in st. louis. ss-wrapper is available without charge under the artistic license described in the open source initiative  <cit> . the source code can be downloaded via ftp  <cit> .

using multiple processors is practical only when the computational task is too large to complete on a single processor in a reasonable amount of time. in the case of database searching, the database to be searched may be too large or the set of query sequences used in the search may be too large to accomplish the search on a workstation with a single or even double or quad processor architecture. since the search must compare every query sequence to every database sequence, parallelizing the process can be accomplished by three different methods. the first method is to split the query sequence file into smaller subsets and apply each subset to one particular node of the cluster in a search against the entire database. the second method calls for the database to be split into a series of smaller files, one of which is distributed to each node of the cluster; then the entire file of query sequences is searched against each of the database segments. finally, it is possible to use a combination of query sequence splitting and database splitting to accomplish the search.

using the query splitting approach does not require any modification to the output generated by search programs such as blast. when utilizing the database splitting approach, however, a correction must be made to the reported e-value for any particular hit. most similarity comparison tools provide a statistical measure  that gives an indication of the statistical significance of a match between the query sequence and a particular database hit. this statistical measure is generally influenced by the size of the search space  and therefore the e-value needs to be recalculated when only a portion of the database is being searched. an added complication is that some search tools require that the database be intact. for example, psi-blast  <cit>  is a variant of blast that constructs a sequence profile model based on hits from an initial round of blast searching. this profile is then used and refined in subsequent rounds of searching to increase the sensitivity of the overall search. because psi-blast depends on the integrity of the database to guarantee that the resulting profiles are representative of the entire search space, it is best to perform parallelization using the query splitting approach.

in contrast to database splitting, query splitting offers greater flexibility in that the query file segments can more easily be adaptively distributed to the nodes of the cluster according to the load on each particular node during the search process. as any one node becomes available, another segment of the query file can be distributed to that node during the search process, which improves performance. it is difficult to predict the workload on any one node before the search begins, and the time required to complete a program running on a cluster depends on the processor that finishes last. adaptive distribution of the workload maximizes resource utilization. optimizing resource utilization is dependent on finding a balance between having larger numbers of smaller tasks versus increased startup and communication overhead due to distribution of the required query and database files to each node. for this reason, for our query splitting wrapper , we adopted a hybrid strategy wherein approximately 90% of the total workload is evenly divided and distributed to each node at the beginning of the search using a modified bucket algorithm . then the remaining 10% of the workload is divided and distributed to each node as it becomes available after completing its previous task. this strategy is accomplished using a master-slave model, where one node is set aside to act as the master, which is responsible for distributing the workload and supervising the slave nodes, which perform the computations.

in general, the query splitting approach seems to be superior to the database splitting approach due to higher performance and fewer post-search processing tasks. database splitting does provide a distinct advantage when the database is too large to fit into the physical memory of a single node  <cit> . ncbi blast uses memory mapped file i/o for database access. blast runs fastest when it can cache the database in memory. when the database size exceeds that of the available memory, however, the database splitting approach can reduce the possibility of swapping the database from physical memory to disk swap space, which could significantly slow the search process. therefore, we have also developed a wrapper to support database splitting . ds-blast is specific for blast because it is necessary to include code to recalculate e-values following the search.

as indicated above, a modified bucket algorithm is used to split up the query sequences for qs-search, and a similar algorithm is used to split up the database sequences for ds-blast. the modified bucket algorithm works as follows: first, the sequences are sorted according to length. in the first cycle, sequences are placed one at a time into each bucket in descending order of length. in the next two cycles, individual sequences continue to be placed into each bucket after first reversing the order of the buckets. bucket order is reversed every two cycles and the process continues until all sequences have been placed into a bucket. at the end of this process, each bucket contains nearly the same number of sequences, and the total length of all sequences in any one bucket is also approximately the same as the total sequence length of any other bucket. in the end, therefore, the file of query sequences or the file of database sequences is evenly divided in both length and number.

the blast e-value is a function of the size of the effective search space, which is dependent on three factors: the number of sequences in the database, the total combined length of all sequences in the database, and the length of the query sequence  <cit> . figure  <dig> shows that when splitting the database into n fragments of equal sequence number and length, the effective search space of each database fragment is approximately 1/n that of the intact entire database. therefore, the e-value calculated for any particular hit of a query sequence to a database sequence will approximate a linear function dependent on the value of n. for that reason, at the beginning of the search, ds-blast lowers the user-provided e-value cut-off to account for the number of nodes used in the search. following the search, ds-blast recalculates the effective search space and each resulting e-value by multiplying by the value of n.

RESULTS
usage
the qs-search executable  provides the same interface for all search tools. the command line is as follows:

qssearch -c <command> -q <query> -d <database> -o <output> -l <local scratch> -x <database files>

ds-blast uses two executables: dsblast and dsformatdb. dsformatdb is responsible for splitting the database into fragments according to the modified bucket algorithm and then formats these fragments using the ncbi formatdb executable.

the command line for dsformatdb is as follows:

dsformatdb -n <number> -c <command> -d <database> -p <path>

the command line for dsblast is as follows:

dsblast -o <output> -c <command> -l <local scratch> -d <database> -q <query>

the command-line variables are as follows:

• -c command: normal command line used for the underlying application including all desired options

• -q query: query filename in fasta format

• -d database: database filename

• -o output: output filename

• -l local scratch: temporary directory on each node

• -x database files: a space-delimited list of the database file names generated by the search program's formatting utility 

• -n number: desired number of database fragments

• -p path: directory to store database fragments.

benchmarking
all benchmark experiments were performed on a linux cluster in the department of engineering at the university of alabama at birmingham  <cit> . the cluster consists of one compile node and  <dig> compute nodes , as well as  <dig> storage servers . all machines have  <dig> ×  <dig>  ghz xeon processors,  <dig> gb of ram, an  <dig> gb scsi hard drive, and are connected via gigabit ethernet to a cisco  <dig> switch. the ncbi non-redundant protein database , downloaded from genbank  <cit>  in august,  <dig>  was used for testing both ds-blast and qs-search; it contained  <dig> , <dig> sequences composed of  <dig> , <dig> amino acids. release  <dig>  of the pfam  <cit>  database from washington university  was used in benchmarking hmmpfam under qs-search; it contained  <dig> profile models that, when combined, were  <dig> , <dig> residues long. the same set of query sequences was used for all experiments. the query sequences represented all open reading frames of more than  <dig> amino acids from the genome of monkeypox virus strain wrair 7– <dig> , and totaled  <dig> sequences comprising  <dig>  amino acids.

we also compared the performance of ds-blast to that of mpiblast  <cit>  version  <dig> . <dig>  we found that ds-blast was almost twice as fast as mpiblast when both utilized  <dig> processors and more than twice as fast when both utilized  <dig> processors . both mpiblast and ds-blast required a substantial part of the total run time to merge and format the final blast output.

CONCLUSIONS
to increase the speed and efficiency of sequence similarity search programs, we have developed the ss-wrapper package, a series of wrapper applications that supports the deployment of sequence similarity searches on high-performance computing clusters. qs-search implements a query sequence splitting approach for the deployment of ncbi blast and hmmpfam. it also will support other similarity search programs, including all variants of ncbi blast  as well as all options provided by the blastall executable. because this implementation does not alter the original program, program updates and new programs should be easily accommodated. the output from qs-search is effectively identical to that produced by the underlying program. qs-search is designed to provide optimal load balancing and maximize resource usage when using computer clusters. the performance gain approaches linearity in proportion to the number of processors employed.

when the database is too large to fit into the physical memory of a single node in the cluster, a database splitting approach should outperform the query splitting approach used by qs-search  <cit> . therefore as a complementary application, the ss-wrapper package also includes ds-blast, which implements a database splitting approach for blast searches and provides an effective solution to recalculate the e-value during the post-search phase of processing.

ss-wrapper provides a suite of tools that makes large sequence similarity searches feasible by deploying the search on a linux cluster. these tools permit the bioinformatics community to take advantage of the power of high-performance cluster computing. other tools such as disperse  <cit>  and turboblast  <cit>  are designed to deploy bioinformatics applications onto loosely connected machines. a more general approach to deployment uses grid computing as an increasingly popular alternative to cluster computing  <cit> . grid computing organizes widespread, diverse collections of cpu resources  into a virtual supercomputer, where these collections of hardware, software, and data resources are organized into a more uniform, manageable, visual whole. in contrast, the cpus in a linux cluster are more tightly coupled and specialized. grid computing has the advantage of utilizing large numbers of cpus as they become available to the grid. the disadvantage of a grid is in managing the complexity of the disparate architectures of the available cpus, minimizing overhead, and making maximum usage of network bandwidth. for maximal performance, tools in the ss-wrapper package have been developed under the assumption of a homogenous linux cluster in which every cpu is similar. we are currently exploring methods to extend our current work to take advantage of grid computing technologies. to accomplish this, the complexities involved will require significant modification and extension of the applications that are a part of the ss-wrapper package.

availability and requirements
the ss-wrapper package is freely available under the artistic license described in the open source initiative. the source code can be downloaded via ftp  <cit> . contact elliotl@uab.edu for information on obtaining the software. all tools have been tested on an ibm intel® processor-based linux cluster with lam/mpi  <cit>  and should be compatible with other implementations of mpi.

authors' contributions
cw was responsible for the conception, design, implementation, and testing of the ss-wrapper package. ejl contributed to its conception and testing and provided overall project coordination. both authors have read and approved the final manuscript.

