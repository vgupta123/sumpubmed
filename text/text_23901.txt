BACKGROUND
microarrays enable the simultaneous measurement of the expression levels of tens of thousands of genes and have found widespread application in biological and biomedical research. the use of microarrays to discover genes, which are differentially expressed between two or more groups of patients has many applications. these include the identification of disease biomarkers that may be important in the diagnoses of the different types and subtypes of diseases  <cit> . although increasing numbers of multi-class microarray studies are performed, the vast majority continue to be two class  studies, for example where a control and a treatment are examined. in this case, the object of the study, is to determine the genes that are differentially expressed between the two classes. the number of gene probes represented on microarray chips may exceed  <dig>  and the number of cases  in microarray studies is frequently limited. this presents a considerable dimensionality issue, which together with the noise inherent in microarray data is a significant challenge to any feature selection approach.

numerous feature selection methods have been applied to the detection of differentially expressed genes on microarrays. different methods produce gene lists that are strikingly different  <cit> , yet few studies have compared methods. this is largely due to the lack of benchmark datasets that contain sufficient numbers of known true positive and true negative expressed genes. studies have verified the expression of detected genes using experimental techniques like rt-pcr  <cit> . however, though rt-pcr verifies the prediction success of a subset of true positives, it provides no indication of the number of true positives or negatives falsely predicted.

several studies have examined feature selection by investigating the consistency between gene lists from small subsets of samples and those from the full dataset  <cit> , or using a bootstrap method to generate simulated datasets from real datasets  <cit> . this approach is limited in that it assumes that gene lists generated on the full dataset are correct. a number of studies have used simulated data where the truly regulated genes are known  <cit> . although simulated data sidesteps many problems, it is unclear whether these simulated datasets realistically reflect the noise inherent in real microarray data. to address these issues, choe et al   <cit>  generated binary  microarray dataset with artificial crna samples which contain known quantities of "spike in" targets, of which approximately one third were spiked-in differentially. the differentially spike-in targets provided genes with known differentially "expression" ranging from  <dig>  – 4-fold between the two classes  <cit> . these data provide a substantial resource, but contain only six samples. it would be difficult for these  <dig> cases to represent the complete biological and technical noise inherent in a typical microarray experiment. due to these limitations, in this study, we apply feature selection methods to  <dig> real binary  microarray datasets. these datasets include the well-known publicly available colon  <cit> , lymphoma  <cit>  and leukaemia datasets  <cit> . we applied  <dig> commonly used feature selection methods to these datasets.

the gene lists produced were evaluated using two criteria. the first was the similarity in content between gene lists derived using the different methods. the second was the effectiveness of each gene list to form a gene classifier which could predict the class of a test sample. in using classification to rank feature selection methods, we are assuming that a better gene list should discriminate classes in the data more effectively. a better gene list should provide better input information which will produce a more effective classifier. therefore it is possible to train a classification model using a particular set of genes, and test how well this model discriminates between classes when applied to a separate blind test dataset. the test dataset can not be used for feature selection or classifier training. the prediction strength of the model is a measure of the power of the input gene list. therefore it is possible to rank gene lists and assesses the performance feature selection methods.

we also examine the impact that a reduction in sample number has on the performance of feature selection methods. the problem of too few cases is a considerable practical obstacle faced in most microarray data analyses. typically, the number of samples in a microarray study is limited by cost and/or the availability of sufficient biological material. we make recommendations of feature selection approaches which are most suited to different data structures.

RESULTS
similarity of gene lists
we assessed the overlap between gene lists produced by different feature selection methods. the  <dig> feature selection methods were applied to the full dataset,  <dig> percent of samples in the dataset, and to subsets of size  <dig>   <dig> and  <dig> samples per class. to limit sampling bias sample subsets were randomly selected  <dig> times. ranked lists of differentially expressed genes were produced using each of the  <dig> feature selection approaches . we examined the top  <dig>   <dig> and  <dig> mostly highly ranked genes and recorded the proportion of genes that were different between gene lists. results were obtained for all  <dig> datasets . a comparison of the overlap between these ranked gene lists are shown as dendrograms in figure  <dig> 

the clusters of methods were consistent when gene lists of the top genes  <dig>   <dig>  or  <dig> were compared. figure  <dig> shows representative dendrograms comparing the overlap of the  <dig> most highly ranked genes averaged over all  <dig> datasets. the individual dendrograms followed by their corresponding percentage matices, for each of the datasets, can be found in additional files  <dig>   <dig>   <dig>   <dig>  interestingly, only  <dig> % of the top  <dig> genes are present in all  <dig> gene lists when the full datasets are examined . the set of randomly selected genes did not cluster with any of the  <dig> feature selection methods and was an outlier.

it can be seen from the topology of the dendrograms in figure  <dig> that there are two main clusters. the first cluster, consisting of fold change methods  had ~58% identical gene lists. the second cluster contained two subgroups. gene lists in first subgroup were obtained using the welch t-statistics methods  and sam, and were  <dig> % identical when produced from full datasets . the second subgroup consisted of anova, template matching, and the bayesian t-statistic. anova and template matching produced gene lists which were identical in content. gene lists produced using the bayesian t-statistic was very similar to these with  <dig> % overlap in gene content. although roc falls in neither subgroup, its gene lists shares  <dig> % of genes with anova, template matching and the bayesian t-statistic, and 69% identity with the welch t-statistic, maxt and sam subgroup. this topology was consistent when gene lists were produced using feature selection methods applied to 50% of the data .

however as the number of samples is reduced, the challenge of estimating gene variance is increased. when sample size is reduced further to  <dig> samples per class, the topology of second cluster changes dramatically . the distance between the welch t-statistic and maxt is reduced , as there is less information available when sample permutation is performed. there is greater difference in gene content between gene lists produced by the two modified t-statistics  and the other t-statistic methods .

when the sample size is reduced even further to  <dig> samples per class, we observed that the overlap in genes lists between all methods drops to only  <dig> % . the distinction between the modified t-statistics and the other methods is even more apparent . interestingly, the roc method is most affected by the reduction in samples size and appears as an outlier of the second group when the sample size falls below  <dig> samples per class . in contrast, the first cluster  was not affected to this same extent when sample size was reduced.

these analyses show that sample size clearly affects ranked gene lists produced by different feature selection methods, and that different methods are more robust to a reduction in sample size.

gene lists as classifiers
gene lists were assessed by comparing the success of each gene list as a classifier . all ranked gene lists of length between  <dig> and  <dig> were compared. the success of each feature selection approach is represented as an accumulated rci score . rci scores were accumulated over  <dig> different datasets using all  <dig> classification methods. it is clear that all methods easily out performed random feature selection. however random feature selection does perform better with increasing numbers of genes.

when the datasets were split so as to have the same number of samples per class in the training and test datasets ), we observed that the fold methods performed weakly. fold methods received lower accumulated rci values than the other methods, over the full range of gene lists lengths . classification performance of classifiers trained with genes lists produced by rank products were better than bga and fold change but poorer than the other methods. performance of gene lists from anova and template matching methods are nearly indistinguishable as shown in figure 3a. this is not surprising given that these produced highly overlapping gene lists .

although anova and template matching has almost identical gene lists, the most highly ranked genes were different when compared to anova. in particular, template matching had problems with the all. <dig> t dataset when the number of genes was below  <dig>  the effect of the variance structure of each of the  <dig> datasets assessed in figure 3a is shown in figure 3b. figure 3b shows the classification success  of gene lists from each datatset, when the top  <dig> genes are used to build the classifier. further figures are provided as additional files showing the classification success of the gene lists for each classifier, for each dataset, for the top  <dig>   <dig> and  <dig> genes . the corresponding classification accuracy for each classifier, for each dataset, for the top  <dig>   <dig> and  <dig> genes are provided in additional files  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> 

the feature selection approaches that perform best on the large sample size datasets were area under the roc curve and naïve bayes ). however the performance of naïve bayes was only marginally better then the other methods in this training and test cross validation.

the performance of many feature selection approaches was dependent on the variance structure of the dataset . it can be seen from figure 3b that the datasets that contribute most to the success of the roc method are the leukaemia, prostate and dlbcl datasets. the roc methods performance is as good as any other method in the remaining datasets, excluding colon and myeloma. these two datasets are the noisiest datasets with pooled variances of  <dig>  and  <dig>  respectively . the methods that performed the best on these two noisy datasets are the fold change methods. interestingly, the roc method performs well on the leukaemia dataset that has the third highest pooled variance of  <dig> , and the fold methods performed poorly.

the effect of reduced numbers of samples per class
in figure 3a, large numbers of samples were available in the training datasets. such large numbers of cases are rare in most microarray studies where replicates are frequently limited. to examine the effect of small sample size, we generated training datasets with fewer samples; only  <dig>   <dig> or  <dig> samples per class. the remainder of the data were used as a blind test set, and the class prediction strength of the training gene lists were assessed using the classification methods, support vector machines , bga  <cit> , naïve bayes classification  <cit> , and k-nearest neighbours . when we investigated the training with  <dig> cases per class , we found that the results were similar to figure 3a. that is, fold change methods were still the worst, followed by the t-statistic, but there was less of a difference in the performance between the methods in cluster  <dig> .

as the training set size is reduced further , 3a) to  <dig> or  <dig> samples per class, lower cumulative rci scores are observed when compared to figure 3a, indicating that classifier accuracy is affected by sample size. given fewer samples, there is less information to determine the usefulness of each gene and there is a greater chance of false positives in a feature selection. also there is a loss of classification power during the generation of the classification models. a classification model trained on a smaller training dataset is less likely to calculate realistic values for the significance of the genes.

the ranking of feature selection methods is different when the number of samples in the training dataset is reduced. feature selection methods, such as area under the roc curve and maxt that were suited to large numbers of samples ) have reduced performance with smaller class sizes , 3a). in fact, roc is very sensitive to low sample size and performs poorly compared to the other methods when the number of samples per class is  <dig> ). this is consistent with the observation that the content of gene lists produced by the roc method were dramatically affected by low sample size .

in contrast to the large sample size study where all t-statistic methods perform comparably ), the modified t-statistic methods  outperform the other t-statistic methods when the sample size is reduced ). maxt, anova and template matching lose power at lower numbers of samples. this maybe attributed to the reduction in information that can be used to calculate the variance obtained from the reduced number of samples. this is supported by the change in the rankings of the t-statistic methods as the number of samples change. when the results from each of the datasets are examined ), the empirical bayes method and sam perform comparably to other methods in most of the datasets. but in the prostate, colon, and all <dig> datasets, empirical bayes method does better then the other t-statistic methods, although in the latter two, empirical bayes method is beaten by the fold methods. when the two datasets with the greatest pooled variance  are looked at, we see that the fold change methods especially rank products do well. the fold methods are beaten by other methods in datasets with low variance .

when the number of samples is reduced further to  <dig> samples per class ), the gap between the modified t-statistics and the other t-statistic methods is increased. this is consistent with the separation of these two subgroups in figure 2d. the empiricial bayes statistic is now ranked second, below rank products. despite being ranked first the rank products method only gets the highest rci value in two of the datasets. this is because rank products, and to a lesser extent the empirical bayes statistic, was ranked consistently high across the datasets, while the rank of other methods varied.

overall the empirical bayes t-statistic was most robust. it performed comparably well with any number of cases, but it was outperformed by the roc method when the number of samples in the training dataset was large, and the rank products method when the number of samples was limiting or when the dataset has a high pooled variance.

discussion
different feature selection methods produce dissimilar gene lists, which can produce dramatically different discrimination performance when trained as gene classifiers. the gene lists produced by the feature selection methods can be grouped broadly according to the manner in which they treat gene variance.

the bga, fold change and rank products cluster consists of methods that do not model the variance when ranking genes. although fold change continues to be widely utilised in many studies, this early approach to ranking differentially expressed genes is not optimal. this is because fold change and bga do not control the variance and so are susceptible to outliers. this is different to rank products, which assumes constant variance across all samples. rank products compared the product of the ranks of genes in a class with the product of the ranks of genes in the second class. for each gene in the dataset, rank products sorts the genes according to the likelihood of observing their ranked positions on the lists of differentially expressed genes just by chance. our study has shown that this method performs well with limited numbers of samples and with noisy datasets which agrees with a recent study  <cit> .

in this study the t-statistic methods performed relatively poorly. given the high levels of noise in microarray data, together with the low samples sizes, computing a t-statistic can be problematic, because the variance estimate  can be skewed by the genes which have a low variance. due to the large numbers of genes studied in microarray datasets, there will always be some genes which have a low standard deviation by chance. thus, these genes will have a large t-statistic and will be falsely predicted to be differentially expressed.

classifiers built using gene lists from the roc method outperformed all other methods when applied to large datasets. high rci scores were observed even when only a few of the most highly ranked genes were examined. these high rci scores were maintained when the number of genes examined was increased. it is possible to obtain p-values using this method  <cit> . however our analysis showed that roc, like the t-statistic methods, loses power when the number of samples is reduced. roc ranks a gene based on its power to discriminate between the groups given a threshold false positive rate. this means that it ignores the level of expression of the gene in the two groups. therefore as the training size decreases, the likelihood of a gene with low variance and no biological meaning being a good discriminator by chance increases. our results suggest that roc is an unsuitable method when the sample size is below  <dig> . this agrees with a previous study which noted the drop in reproducibility of results when the sample size was reduced from n =  <dig> to n =  <dig>  <cit> .

when the number of replicates is small, variance estimation is much more challenging. we observed that gene rankings based on most statistics were poor. at low numbers of samples this study finds it difficult to report any differences between methods such as bga and fold which do not model the variance, and sam which attempts to model the variance. equally, in data sets with high variance, fold or non-parametric methods were more powerful than parametric methods. we observed that gene lists from fold change or bga produced formed comparable or better classifiers to those generated with gene lists from the welch t-statistic, anova, maxt or template matching. small noisy datasets are very common in practise, and in these cases rank products can be recommended.

several modified t-statistics have been proposed to address this problem, of which sam  <cit>  is arguably the most popular. in this study sam performed moderately well across most analyses, except when applied to data with low sample size, where it did not outperform the classic fold change. sam also performs poorly when applied to the noisy datasets. sam uses a moderated t-statistic, whereby a constant is added to the denominator of the t-statistic. the addition of this constant reduces the chance of detecting genes which have a low standard deviation by chance. the constant is estimated from the sum of the global standard error of the genes. it is reported that the sam algorithm favours using a large value denominator constant factor, which in turn means the t-statistic depends more on the fold change value  <cit> . therefore at low samples sizes it may provide a less reliable estimate of variance, which may explain why simple fold change or non-parametric methods outperform sam on these types of data. this has also been reported in a number of recent studies  <cit> .

although both sam and the empirical bayes method are moderated t-statistics, the empirical bayes method provides a more complex model of the gene variance. the gene standard error is estimated as a representative value of the variance of the genes at the same level of expression as the gene of interest  <cit> . we report that in training sets with a large number of cases, the empirical bayes method performed comparably with anova and template matching, although the genes selected by these methods varied slightly. importantly, unlike most other methods the empirical bayes t-statistic proved equally robust with low numbers of cases. we observed that when the number of cases was small, gene rankings based on the empirical bayes t-statistic proved to be much more reliable than other methods examined in this study. the bayesian statistic also provides p-values and, has the advantage that it can be expanded to deal with datasets that have more then two classes.

CONCLUSIONS
this study used an indirect method of testing the feature selection methods by using classification models. using this method we have demonstrated that the empirical bayes statistic, the area under the roc curve method and rank products are accurate ways to identify differentially regulated genes in a microarray dataset and that these can produce robust classifiers. the empirical bayes statistic was the most robust method across all sample sizes. when dealing with datasets that have a low pooled variance that contain  <dig> or more samples, the roc method is proved to be the most accurate. for datasets that have a high pooled variance or a low number of samples, the rank products method proved useful.

