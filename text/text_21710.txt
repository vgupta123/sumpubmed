BACKGROUND
quantitative variations in living organisms result from environmental factors and multiple segregating genes  <cit> . the search for genomic markers that are linked to quantitative traits is an important first step towards finding the gene variants responsible for the observed phenotype, and is consequential for commercial breeding purposes and for uncovering the mechanistic underpinnings of pathologies. linkage between genetic loci and morphological traits was first demonstrated almost a century ago  <cit>  but early efforts  <cit>  were difficult due to the sparsity of known genetic markers across the entire genome.

the mapping problem for quantitative trait loci  is, briefly stated, to find the genetic markers that correlate with measured quantitative traits. single marker regression  <cit>  was the traditional approach to mapping quantitative trait loci. this one-by-one analysis has well known drawbacks e.g. effect size is confounded with marker separation  <cit> . the availability of dense genetic linkage maps ushered in modern quantitative genetics  and single marker regression has been superseded by interval mapping  . im allows a more accurate determination of the location and effect size of a qtl as the likelihood of a qtl can be placed in the context of its genomic position. it still maps only a single locus at a time, contradicting the known polygenic character of quantitative traits.

this led to the formulation of multiple im methods and composite im with the introduction of markers used as covariates . the issue of the selection of the appropriate covariates remains an interesting challenge, and the genomic context of a trait is not as clear as with single im. the present paper is directly comparable only to standard single im.

the use of linkage maps, obtained using multi-point analysis of marker segregation data, is a major advantage of these im methods compared to single marker regression, but is considered as a separate preliminary step before im. in the present work, we report on a method, block network mapping , that incorporates linkage through an experimental data-driven linkage network found using similarity network fusion  combined with a new bayesian approach to locus selection. ref.  <cit>  did not suggest this novel application of snf.

to develop bnm, we used synthetically generated phenotypes paired with real genotypes obtained in a study of white blood cells  <cit>  in a specific strain of mice, diversity outbred  <cit>  mice. these were developed to overcome the low mapping resolution of conventional mouse crosses. as an example,  <cit>  demonstrated that behavioral traits could be mapped with high precision with even a modest number of animals.

we investigated the effect size and population size dependence of the false discovery rate , the power, and the receiver operating characteristic  obtained using our method, bnm, compared to the standard expectation maximization  implementation of im implemented in the r/qtl package  <cit> .

methods
the bnm algorithm can be divided into three major parts outlined in fig.  <dig>  for any finite sample of genotypes, there are correlations between genotype markers due to the finite amount recombination that could have occurred. our approach is to first find contiguous blocks of markers in a data-driven but phenotype-independent manner, which we term haplotype blocks. the idea of looking for such blocks is inspired by multiple im  though our approach to finding these blocks is based on ideas from  <cit> . they are defined by clustering the snps based on similarity matrices constructed from the information obtained from the genotypes of the n subjects being studied . second, we compute the likelihood of each block, i.e., the likelihood that there is at least one marker in this block that is contributing to the overall phenotypic effect . third, using the similarity matrices and likelihoods obtained in the first two parts, we calculate the empirical likelihood, of each marker via a bayesian approach , considering each marker as a possible ‘model’ of the phenotype data.
fig.  <dig> flowchart outlining the major parts of the bnm algorithm




genotype data
the real genotype data  <cit>  in a specific strain of mice, diversity outbred  <cit>  mice, underlying the simulated phenotypes that were used to develop and test our approach is available at http://churchill-lab.jax.org/website/gattidoqtl.

obtaining the haplotype blocks
we first represent each sequence of snp pairs as a sequence of four numbers:  <dig> ,  <dig> ,  <dig> , and  <dig> . we remove snps missing values on more than half of the mice and then use snf to cluster the remaining snps based on both their distance matrix d, and the mutual information matrix i.

distance matrix
we define a distance matrix dss′ between snps s and s
′ by using the genetic distance between the snps, measured in centimorgans: 
  <dig> dss′=|cm location of snps−cm location of snps′| 


we also tried using the actual base position index along a chromosome to define the distance matrix, but the final results were not much different.

mutual information matrix
we suppose that the phenotypes measured have been transformed into positive values by exponentiating. if w
m is the phenotype of mouse m, we define a normalized phenotype ρ
m by ρm=wm/∑m′wm′. by definition, ∑mρm= <dig>  with the possible values of any snp s taking values α= <dig> ,or  <dig>  the phenotype-weighted mutual information between two snps s and s
′ is defined as 
  <dig> iˇss′=∑αβpsαs′βlogpsαs′βpsαps′β 


where psαs′β=∑mρmσm,sασm,sβ′, psα=∑s′βpsαs′β=∑mρmσm,sα, and 
  <dig> σm,sα=1when snpsof mousemis in stateα,0otherwise. 


note that ∑αβpsαs′β=∑mρm∑αβσm,sασm,sβ′=∑mρm= <dig>  when the value of the snp is missing , it is randomly assigned a state  <dig>   <dig>  or  <dig> with a probability equal to the distribution of each state for this snp among the subjects with available data. we want a sample-driven measure of the mutual information between snps that is independent of the phenotype. we could, of course, simply take the phenotype to be unity for all m, but in order to avoid bias due to the empirical distribution of phenotype values, we permute the phenotypes to obtain a phenotype-independent mutual information by averaging iˇ over a large number of permutations over the phenotype values of the subjects, 
  <dig> iss′=∑permiˇss′perm/nperm 


where ρ
perm is a permutation of ρ,iˇss′perm is the same as iˇss′ but with ρ
perm replacing ρ, and n
perm is the total number of permutations. note that with every permutation, the missing values of snps are randomly assigned a state independent of the previous permutation. in this way, our empirical i is independent of the actual phenotypes, but may possibly depend on the distribution of phenotype values. in more detail, suppose that the samples are drawn with unknown bias. then, a uniformly weighted mutual information between snps would be a biased estimator of snp-snp mutual information, due to the unknown sampling bias in the observed samples. for example, if the phenotype values are skewed due to sampling bias, our empirical permutation formulation of i will maintain the skewed distribution, while reducing the effect of biased sampling on the estimated snp-snp mutual information. this may reduce the power to detect correlations, but it will not enhance correlations due to biased sampling, so this is a conservative approach. permutation tests are often used in similar settings in qtl analysis  <cit> .

similarity matrix
the matrices i and d defined in this manner are sample-dependent and sample-independent, respectively. moreover, the mutual information similarity is in no way constrained by contiguity on the chromosome, and indeed, the two similarity measures are defined in units that are not directly comparable. we want to find a principled approach to combining these similarity measures into a single unified similarity matrix. the similarity network fusion  approach  <cit>  is a recently published algorithm that solves exactly this problem, by translating each independent similarity measure separately into a network, and then fusing these networks into a combined single network. an important point emphasized in ref.  <cit>  is that snf is an algorithm for fusing network information obtained from many different data types characterizing a group of subjects into a combined similarity network, even for data types as different as methylation data and expression data. for example, the similarity metric suggested in ref.  <cit>   is chi-squared distance for discrete variables and agreement-based measure for binary variables, compared to euclidean distance for continuous variables. this versatility makes snf particularly well-suited to our application. thus, for each chromosome with snf, we obtain a similarity matrix of snps which is defined by a fusion of the distance matrix d and the mutual information matrix i. fusion using the snf algorithm requires specifying two parameters: the number of neighbors κ and hyperparameter η. we elaborate on the choice of these parameters in the next subsection.

hierarchical clustering
given the fused similarity matrix, we use it to find blocks of snps that are correlated in the available dataset independent of the phenotype and, due to the use of the genetic distance matrix d, contiguous on the chromosome. a clustering method must be chosen to carry out this block decomposition based on the fused similarity matrix. we implemented a version of hierarchical clustering, and as in most approaches to defining clusters with hierarchical clustering, we must specify how the tree is used to define the final clusters.

we separate the snps into different clusters based on their fused similarity matrix in the following manner. we perform hierarchical clustering where we iteratively divide a cluster into two clusters, or more if the binary split did not satisfy the conditions described below  where the end branches are the final clusters to be used. this turns out to be mostly a binary tree for the present dataset. at each iteration, the splitting process is repeated for every “open” branch. an open branch is one that did not meet the stopping conditions. if a branch meets all the stopping conditions then the branch will be considered “closed”. for each open branch/cluster k, we first check if the size of cluster k, s
k, is smaller than κ. if it is, then branch k ends, i.e. cluster k is now closed. otherwise, the splitting process starts: first, the similarity matrix for cluster k is constructed. then, starting with setting the number of clusters n to n= <dig>  cluster k is split into n new clusters via spectral clustering. if all the new clusters have contiguous members and none of them is smaller than a minimum size, s
min , then the new clusters are accepted and considered open, i.e., they move on to the next iteration. if these conditions are not met, we incrementally increase n and repeat the splitting test until either the new clusters are accepted or n reaches a maximum number n
max in which case the new clusters are rejected and branch k ends. this process is illustrated in fig.  <dig>  note that n
max depends on the size of the cluster being split, n
max= min where t
max is the maximum number of iterations allowed. the iterations stop when either all branches are closed or the maximum number of iterations is reached.
fig.  <dig> flowchart outlining the hierarchical clustering algorithm




we still need to decide the values of κ and η, as the final clustering results will differ depending on these values. therefore, we perform the above hierarchical clustering algorithm with different values of κ,η pairs . the optimal κ,η pair is the one that leads to the smallest cluster sizes, as we wanted to obtain higher resolution for the correct snp. note that in some examples where we selected for larger clusters, we found a tradeoff between snp localization and phenotype prediction accuracy. as we focus in this paper on the snp mapping problem, we chose smaller cluster sizes. in particular, we look at the biggest cluster at the end branches and choose the κ,η pair with the smallest biggest cluster . if the size of these smallest biggest clusters is the same then we compare the number of big versus small clusters.

obtaining the block likelihoods
the likelihood of a block is the likelihood that at least one snp in this block contributes to the overall phenotype value. if a block has an effect on the phenotype, then a regression model on this block should have a good predictive power relative to a null model . if we assume that only one block in each chromosome contributes to the overall phenotype value, then the likelihood of a block should be obtained by comparing the relative predictive power of all the blocks in the chromosome in which the block resides .

the relative predictive powers of the blocks
for each block k, we perform n
trial trials  of regression/ testing simulations. for each trial t we randomly divide the data points into two equal halves, a training and a testing set. a data point is composed of a subject’s phenotype value and its block k genotype, i.e., its sequence of snp states composing block k. then, we test two models, one to obtain the predictive power of block k for trial t and another to serve as the null model for block k and trial t. for both models, we use the sequence phenotype inference approach described in  <cit> . this approach allows the investigation of possible nonlinear dependence of the phenotype on allele frequency.

for the first model, we train its parameters on the data points in the training set and then use it to predict the phenotypes of the subjects in the test set. comparing our predicted phenotypes, wk,tpred, to the actual values of the phenotypes in the test set, wttest, we obtain the pearson correlation r
k,t between log and log for trial t, block k. the sign of the snp’s effect on the phenotype never appears in these calculations because we are always comparing the predicted phenotype values with the test set phenotype values. if the prediction is correct, whether the snp enhances or decreases a phenotype, the value of r
k,t will be positive.

for the second  model, we perform n
p permutation trials  <cit> . for each permutation trial p, we permute the phenotypes of the data points in the training set before training the model parameters. then, similar to the first model, we predict the phenotypes of the subjects in the test set to obtain the pearson correlation rk,tp between log and log, where wk,tpred,p is the set of predicted phenotypes using the training model for block k, from trial t and permutation trial p. the relative predictive power of block k for trial t can now be defined via the ratio 
  <dig> rk,t=exprk,t∑pexprk,tp. 


this algorithm is outlined in fig.  <dig>  notice that the exponentiation of the pearson correlations here implies that possible negative values of r
k,t or rk,tp lead to lower values, as is appropriate.
fig.  <dig> flowchart outlining the algorithm steps to obtain the ratio in eq. 5





the likelihoods of the blocks
we can finally define the likelihood l
k of block k as the fraction of trials where rk,t≥rk′,t for all k
′≠k.

obtaining the snp empirical pseudo-probabilities
in formulating our approach in a bayesian setting, we consider each snp as a possible model for the observed phenotype. in particular, we assume that each chromosome has only one possible ‘true’ snp. our prior probability is that every snp is equally likely to be causative. it remains then to define the likelihood of the ‘data given the model’ part of the bayes computation, which in our context corresponds to ‘the phenotypes observed given the causative snp s’, to find the probability of the snp s as the model given the phenotype data, as is standard in applications of bayes’ theorem.

to motivate our likelihood function for the data given the model, we first note that a higher likelihood value of a block k, l
k, suggests that at least one snp in block k is contributing to the quantitative phenotype. as discussed above, these blocks are chosen such that snps in the same block are more correlated to each other than to snps in a different block. however, there are still correlations between snps from different blocks, albeit not strong enough to be included in the same block. because of these inter-block correlations, we expect that even blocks that do not contain the causative snp could have a high likelihood as well simply due to correlations that exist in the finite set of sampled genotypes, and we can quantify this as follows. assuming that there is only one causative snp on each chromosome, if snp s is the one then the likelihood that a block b will show an effect should be proportional to the odds ratio, ls,b <dig> of the correlation between snp s and block b compared to its correlation to all other blocks in the same chromosome. we define 
  <dig> ls,b0=qs,b∑b′≠bqs,b′, 


where qs,b=maxs′∈b,s′≠sms,s′, and m is the similarity matrix for the corresponding chromosome obtained by fusing its distance and information matrices as described in section “hierarchical clustering” using the optimal κ,η pair values obtained while performing the hierarchical clustering.

note that ls,b <dig> is phenotype independent  and is simply a measure of the correlation between snp s and block b based on the finite amount of data available. we also tried using l
 <dig> defined in terms of the mean instead of the maximum of m, but this did not materially affect the results.

using the pearson correlation again but in a completely different context, we use our definition of ls,b <dig> to define the empirical likelihood of a snp s given the data as the pearson correlation, r
s, between the sequence of phenotype-independent likelihoods ls, <dig> ls, <dig> …ls,nc <dig> and the sequence of phenotype-dependent likelihoods  l <dig> l <dig> …lnc, where n
c is the number of blocks in chromosome c which contains the snp s. if this pearson correlation r
s is negative, we define r
s≡ <dig>  it should be emphasized here that r
s< <dig> does not correspond to a snp that has a negative correlation with the phenotype. what is being compared here is the correlation pattern between likelihoods of snp blocks, phenotype-independent  and phenotype-dependent . a negative correlation r
s< <dig> occurs when the genetic linkage of snp blocks is exactly the opposite of the linkage suggested by the phenotype weighting. for small sample sizes, such negative correlations can appear by chance, but just as in the definition of r
k,t in section “the relative predictive powers of the blocks”, they are not related to the sign of the effect of the snp block on the phenotype. we call this empirical likelihood, r
s, the pseudo-probability of s because it takes values between  <dig> and  <dig> as defined, with the ‘pseudo-’ prefix to emphasize that it is not, in fact, a probability. we will use r≡1−r
s in our calculations of power, false discovery rate and other measures of our methodology.

summary
as we have given several definitions in the preceding method subsections, we summarize the relevant information to make the “results” section clearer in conjunction with  <dig>  

l
k: the likelihood that a block k contains a snp which has an effect on the phenotype, calculated using training/testing splits of the data and null trials with permuted training phenotypes.


ls,k0: the phenotype-independent likelihood that a snp s is correlated with a block k, calculated from the snf fused similarity matrix.


r
s: the pearson correlation coefficient over all blocks k between l
k and ls,k <dig> 



r: for each snp s, r≡1− max.r is defined like this so that increasing fdr p-value thresholds correspond to increasing r-value thresholds.




RESULTS
we demonstrate our method by applying it to simulated data with  <dig> genotypic sequences of diversity outbred  mice and  <dig> phenotypes. we simulated phenotypes on the  <dig> autosomes and did not add a sex effect. we selected  <dig> genomic locations, one on each autosome, and generated  <dig> qtl effect sizes from an exponential distribution. using the genotypes at each location, we created the qtl effects and scaled the variance to be  <dig>  then we added n noise and the qtl effects together.

we compare our results to that of simulations using interval mapping  with expectation maximization from the r/qtl package. for the r/qtl simulations, we use scanone with the default settings and calc.genoprob with step =  <dig> and error.prob =  <dig>  for the p-values calculations we run scanone with  <dig> permutations . the simulated data are obtained by choosing only one snp that influences a particular phenotype on each of the  <dig> autosomes with varying effect sizes. these effect sizes range between  <dig> ×10− <dig> and  <dig>  . we compare the power  <cit>  of our method with that of im for different effect sizes. with  <dig> phenotypes,  <dig> autosomes, and  <dig> “true signal” on each autosome, we have  <dig>  effect size data points. we arrange them in order of increasing effect size and then divide them into  <dig> groups of  <dig> data points with  <dig> points offset. for example, the first group is composed of the first  <dig> data points with the lowest effect sizes, and then the second group is composed of data points  <dig> to  <dig>  and so on. then the power and false discovery rate  are calculated within each group separately . while the r/qtl scanone implementation of im assigns a p-value for each snp, bnm assigns a empirical likelihood r
s  and thus an r-value = 1−r
s. to compare the power of the two methods at matching thresholds, we choose a p-value for im and look for the bnm r-value with comparable average fdr over the  <dig> groups . we compare the power of our method with that of im at  <dig> different p-value thresholds  and their fdr-matching bnm r-values . in all cases bnm has a higher power . this is more prominent at higher effect sizes even though bnm has a monotonically decreasing fdr with increasing effect size .
fig.  <dig> power and fdr as a function of effect size. power and fdr of the bnm algorithm  and im from the r/qtl package  with increasing effect sizes. each point corresponds to the power  or fdr  within a group of  <dig> data points with an average effect size in the x-axis. we show the power and fdr at three p-value  and r-value  thresholds:  <dig>  and  <dig>  ,  <dig>  and  <dig>  , and  <dig>  and  <dig>  . these p-value, r-value pairs are matched so that they have the same fdr averaged over all points 




we also compare the roc curves in  <dig> different effect size ranges . in this case the data points are divided into  <dig> groups of  <dig> data points each with an offset of  <dig> points. bnm outperforms im at moderate and high effect sizes. at very low effect sizes, both im and bnm do not have much predictive power.
fig.  <dig> roc curves as a function of effect size. roc curves for im  and bnm  within  <dig> groups of  <dig> data points with average effect sizes  <dig> ± <dig> , <dig> ± <dig> , <dig> ± <dig> , <dig> ± <dig> , <dig> ± <dig>  and  <dig> ± <dig> 




in all of the above results, the power and fdr calculations are based on the precise location of the true snp. in other words, even if the method predicts a signal  in the neighborhood of the “true signal”, it is considered a false positive. this is more stringent than the usual expectation of qtl mapping  <cit>  so we repeated the above analysis after uniformly dividing each autosome into blocks of snps within d mb from each other. for example, if one or more signals are obtained in a particular block, this counts as  <dig> true positive  or  <dig> false positive . we did the analysis at three different block sizes, d= <dig> , d= <dig> , and d= <dig> . bnm still shows a higher power than im at all block sizes and  thresholds, despite the fact that adding this freedom improves the im r/qtl results much more than it does the results from bnm. even with the use of blocks of varying sizes, im shows a decreasing and then increasing fdr as effect sizes are increased while the bnm fdr continues to be a monotonically decreasing function of effect size .

next we examine how the power and false discovery rates change with the choice of different samples of phenotypes and with decreasing number of mice . to examine the variation with choice of phenotype sets, we use three samples of  <dig> phenotypes. in two samples we randomly select  <dig> of the  <dig> phenotypes and in the third we select the  <dig> phenotypes with the highest average effect sizer over the  <dig> autosomes. as above, we arrange the data points in order of increasing effect size and then divide them into  <dig> groups of  <dig> data points with  <dig> points offset. then the power and fdr are calculated within each group separately . at low threshold values we see high variation in the average fdr between the samples  which is due to the low number of predicted signals, making for larger statistical uncertainties. except for the lowest threshold, this variation decreases when the analysis is repeated after setting blocks of size  <dig> mb ,  <dig> mb , and  <dig> mb .
fig.  <dig> power and fdr as a function of sample size. power and fdr of the bnm algorithm  and im from the r/qtl package  with increasing effect sizes. each point corresponds to the power  or fdr  within a group of  <dig> data points with an average effect size in the x-axis. we show the power and fdr at p-value =  <dig>   and the matching bnm r-value such that im and bnm have the same fdr averaged over all points . in  we use all the mice  and three samples of  <dig> phenotypes from the  <dig> simulated phenotypes; the fdr matching r-value =  <dig>  . in  we use three samples of randomly selected  <dig> mice out of the  <dig> mice available; the fdr matching r-value =  <dig>  . in  we use three samples of randomly selected  <dig> mice out of the  <dig> mice available; the fdr matching r-value =  <dig>  . the plots are the means over the three samples in each case, and the errorbars are the standard deviations from the mean in each case


fig.  <dig> false discovery rate variability. the average false discovery rates  are the mean over the three samples of the average over the fdrs within each of the  <dig> effect size groups in fig.  <dig> at different p-value ) or r-value ) thresholds . in  we take the average fdr over the points in fig. 6
d for each of the three samples. the plot is mean over the three samples and the errorbars are the standard deviation from the mean. similarly we take the average over the points in fig. 6
e , and fig. 6
f . in  we replot all together the results for bnm ). in  we replot all together the results for im 




the effect of population size on qtl detection has been demonstrated  <cit> , so we investigated the performance of bnm with a change in the number of mice. to examine the change and variation in fdr as we decrease the number of mice, we randomly select three samples of  <dig> mice and three samples of  <dig> mice out of the total number of  <dig> mice. for all of the  <dig> samples we used the  <dig> phenotypes with the highest average effect size over the  <dig> autosomes. choosing the phenotypes in this manner slightly increases the fraction of high effect signals which will allow us to go to slightly higher average effect sizes in the  <dig> groups of data points.

as the number of mice decreases, we see more variation in the fdr between the three samples , particularly for im at low p-values . as is to be expected, the fdr increases as the number of mice decreases for both im  and bnm . for each of the  <dig> ,  <dig> and  <dig> mice samples, we match the im p-values to r-values of comparable average fdrs and compare the powers at p-value =  <dig>  . in all cases, bnm shows higher power and less variation in fdr than im. applying the block analysis with d= <dig> , <dig> improves the im fdr and removes the effect of the lower number of mice but this is not as much the case for the bnm fdr . in fact, for the block analysis, bnm’s fdr increases with smaller numbers of mice while im’s fdr is relatively insensitive, making the matching bnm r-value much lower at the chosen im p-value. now when we lower the number of mice to  <dig>  bnm shows less power . overall, however, bnm shows better roc curves in all cases with and without the block analysis .

finally, we use the same  <dig> do mice to map neutrophil counts in whole blood obtained from  <cit> . we set our r-value threshold to  <dig>  since this is the fdr-matching r-value to p-value =  <dig>  in our simulated data. we found signals on loci in chromosomes  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  and  <dig> . the loci we found on chromosome  <dig> are between  <dig>  mb and  <dig>  mb. this interval included cxcr <dig> which is involved in neutrophil trafficking  <cit> .
fig.  <dig> neutrophil snp signals. for each chromosome we show  <dig> - r-value of each snp . the x-axis shows the location of each snp in mb. the horizontal red dotted line denotes the threshold value above which signals are detected




discussion
we formulated an integrated data-driven approach, block network mapping , to linkage mapping and phenotype regression using similarity network fusion  <cit>  to define haplotype blocks of snps that were then used for phenotype regression. the importance of using snf or similar network methods is that multiple similarity measures with disparate underpinnings  can be combined using a common graph-theoretic framework which is noise-tolerant. we chose snf  <cit>  because it is very recent and has proven efficacy. we combined the results obtained from this block-by-block regression with the known inter-block correlations between snps using bayes’ theorem to obtain a final measure of the association of a snp with the phenotype. changing s
min to higher values, i.e., using bigger smallest possible clusters did not materially affect our results. however, uniformly segmenting the chromosome into clusters of equal size gave worse results.

qtl locations and effects are specific to populations, and can only be detected when the population is polymorphic at the relevant loci. in light of this, bnm uses no information beyond the genotypes and phenotypes measured in the sample, besides the genetic distance. we did not find much difference between using the genetic distance and the distance measured in base pairs between markers.

we found that the area under the receiver operating characteristic curve  exceeded that of im for all effect sizes, all allowed genome interval sizes  and all chosen numbers of mice . the power of our approach was considerably higher than that of im in all circumstances, except when we allowed for some genomic distance between the true and predicted snps and simultaneously decreased the number of mice to  <dig>  it is known  <cit>  that for qtls common to two populations, the im estimate of effect size was reduced in the larger population, supporting the notion that im overestimates the magnitude of qtl effects in small populations, which may also explain the increase in fdr for im as sample size is decreased. power graphs do not directly exhibit the fdr, but the roc curves show that the predictions made using bnm are more likely to be correct in all circumstances compared to im. we note that the false discovery rates are quite high for this simulated dataset, both for im and for bnm. as we are presenting our methodology here, it is the relative performance that is of interest.

as our approach works block-by-block, it is somewhat similar to composite im  but with a definite prescription for the selection of covariates in the form of snf clusters. as such, the genomic position of the trait locus is interpretable. note that accounting for inter-block correlations was crucial for suppressing spurious snp-phenotype correlations in our approach. however, we have compared our results to im alone in this paper because composite im also addresses the presence of multiple loci by partial regression with selected covariate snps. a standard approach to composite im is to add known qtl to the model iteratively, and we can carry this out iteratively as well in bnm, with the qtl uncovered in a first pass with simple im. however, we are investigating whether a more natural extension of bnm can be developed using the snf framework.

our work does not improve on simple im with respect to effects of opposing sign associated with linked snps. it is not clear that our method could be modified to overcome this limitation, though our approach can detect nonlinear dependence on allele dosage. in its present form, we also made the assumption that only one block in a chromosome contributes to the phenotype. this is, of course, an external assumption from the viewpoint of the underlying mathematics. it can be relaxed by using only contiguous parts of chromosomes in the analysis, but these parts will have smaller numbers of blocks, which in turn will lead to lower power. in other words, multiple effects on a chromosome could be detected with bnm albeit with a worse auroc. it would be more appropriate to investigate better approaches to solving the multiple locus problem  <cit>  within a network paradigm. we are working on extending bnm to account for multiple related phenotypes.

CONCLUSIONS
in this study, we have presented a network approach to qtl analysis that uses sample genotype data to define covariates in a systematic and interpretable manner. using the network of correlations between snps through snf for finding covariate blocks was a central feature of our approach, along with a bayesian approach to finding the likely snps within blocks using inter-block correlations. network approaches may be more noise-tolerant and may scale well to larger sets of measured markers.

additional file

additional file  <dig> block_network_mapping_supplementary figures. all supplementary figures referenced in the text, figures s1-s <dig>  




abbreviations
aurocarea under receiver operating characteristic

bnmblock network mapping

cimcomposite interval mapping

cmcentimorgan

dodiversity outbred

emexpectation maximization

fdrfalse discovery rate

iminterval mapping

mbmegabase

qtlquantitative trait locus

rocreceiver operating characteristic

snfsymmetric network fusion

snpsingle nucleotide polymorphism

tprtrue positive rate

