BACKGROUND
molecular technologies are used in routine clinical practice to identify microorganisms, and evaluate the presence of virulence factors, antibiotic resistance determinants and host-microbe interactions  <cit> . for instance, numerous nucleic acid assays have been developed  <cit>  using hybridization or dna extension techniques that include a wide range of technologies, such as polymerase chain reaction  methods  <cit> , gene and whole genome sequencing  <cit> , luminex  <cit>  and microarray analysis  <cit> .

there is a wide range of technologies that provide specific short base sequences of dna as probes — used to detect the complementary base sequence of interest—or as primers—that guide the dna amplification process—used for different purposes. primers and probes are the main components of nucleic acid-based detection systems and have been the subject of multiple studies. therefore, different software programs have been developed to design these specific sequences of primers and probes minimizing potential cross-hybridization to be spotted, for example, as oligonucleotides in cdna microarrays  <cit>  or sequences of primers to amplify a segment of a unique target gene using reverse-transcriptase -pcr, or to identify a wide spectrum of human pathogens  <cit> .

the biological literature is the main information source on probes and primers for infectious disease diagnosis and prescription. primer and probe sequence data reported in the biomedical literature are an aid for the laborious task of primer and probe design for microorganism identification, genotyping and gene expression studies. therefore, researchers need to search this information in the biomedical literature. unfortunately, there are only a few online databases established as repositories for empirically validated primer and probe sequences submitted by researchers. these repositories include the molecular probe data base  <cit> —available through the sequence retrieval system  <cit> —which contains information on synthetic oligonucleotides used as primers or probes, or primerbank  <cit> , created to retrieve primer information about humans and mice for gene expression analysis by pcr and quantitative pcr   <cit> . conversely, the ncbi probe database  <cit>  is a public registry of nucleic acid reagents designed for use in a wide range of biomedical research applications. on the other hand, rtprimerdb  <cit>  and probebase  <cit>  are freely accessible databases containing empirically validated primer and probe sequences. all these resources are manually updated using the primer/probe information submitted by different researchers rather than automatically acquiring the sequences from the available literature.

over the last few years, text mining, information extraction and knowledge engineering approaches have proven useful for automatically extracting, analyzing and visualizing biological information from the scientific literature for biomedical research  <cit> . although text mining applied to biological data is an active research field, these techniques have not yet been applied to create methods and tools aimed at automating probe and primer extraction from scientific papers. thus, to detect and identify target microorganisms and host-microbe interactions or design pcr and diagnostic microarrays, for instance, researchers normally have to manually review all the literature of interest to identify relevant primers and probes. this is a tough and time-consuming task.

in this paper we present an original method based on a combination of text mining, information extraction, and knowledge engineering techniques aimed at automatically detecting and extracting infectious disease-related primer and probe sequences from scientific papers. given an input of a set of manuscripts in adobe's portable document format , our method returns the extracted primer/probe sequences that appear in the article annotated with information regarding their respective microorganism and gene-specific regions, if available

the paper is organized as follows. in the next section we describe the proposed method. next, we present the results of the evaluation of the method using a test set of  <dig> papers containing  <dig> sequences. after that, we discuss the results of the evaluation. finally, we outline our conclusions in the last section.

methods
the proposed method for automatically extracting primer/probe sequences from the scientific literature is composed of four different activities. as shown in figure  <dig>  the method input is a set of manuscripts in pdf format, and its output is the set of extracted primer/probe sequences annotated with their respective organism/gene information, if available. experts suggested working with pdf files since currently the pdf format is a de facto standard for publishing, archiving and exchanging electronic articles. however, to broaden the scope of the implementation of the proposed method, the software tool optionally accepts two additional input formats: pubmed central xml and plain text.

the first activity of the method is aimed at converting the articles into a suitable format for applying text mining techniques. the objective of the second phase is to analyze the manuscript text and extract all the candidate sequences. they are filtered, refined and post-processed during the third activity. finally, during the last phase, we query different public online genomic databases to automatically annotate the extracted sequences with their associated organism/gene information. a detailed description of each phase follows.

phase 1: pre-processing of articles
the pre-processing phase is aimed at translating each paper into a section tree . the st is a data structure that represents the structure of a document—i.e. sections and subsections—and stores its textual contents hierarchically.

sts are automatically built from the pdf files using a software tool developed by the authors. to build the st, the software tool resorts to custom xml templates each describing the structure and layout of papers published in a particular journal or set of journals sharing a common layout—e.g. publications edited by biomed central.

to evaluate our method, we have focused on papers published in pdf format by several biomed central and plos journals since  our method requires full-text articles rather than abstracts to extract relevant information and bmc and plos papers can be freely accessed and downloaded,  different plos and bmc journals publish papers that are relevant for extracting information regarding infectious disease-related pcr primers and probes, and  most journals published by the same editorial group—either bmc or plos—share a common layout. nevertheless, papers published by other journals can be easily converted into sts by creating a custom xml template to be used by the software tool for st generation. both the document type definition  and the custom xml templates that we created for bmc and plos journals—which can be used as a guide to create custom templates for other journals—can be freely downloaded as part of the source code package at the project's homepage.

as primer and probe sequences are often presented in tables  they need to be handled differently from other sections of the papers. to extract and organize the text from tables we follow the approach described next. table cells are processed by the pdf extractor from left to right and top to bottom. the text belonging to the current cell is extracted and concatenated with the text corresponding to the previously explored cells using an artificial delimiter. the latter was specifically chosen to ensure that the recognizers used in phase  <dig> will not merge the contents of two consecutive cells.

as stated previously, the software tool implementing our method optionally accepts manuscripts in  pubmed central structured xml and  plain text format. building the st for manuscripts in pubmed central xml format is straightforward, since the xml file already provides the document structure—including both tables and figures—that can be easily converted to a st. conversely, if users choose to feed the method with plain text articles, then the sectioning of the document is skipped—i.e. the full text of the manuscript is stored into a single section  of the st—and users will be warned that this may decrease the accuracy of the method. in addition, as the source code of the application is freely available, users can write their own format converters  by creating instances of the document class  and filling in the different sections with the corresponding text.

once the st has been created, it is possible to fully reproduce the logical reading order of the original document by iterating the st in depth-first order. this data structure is the input to the second activity described next.

phase 2: preliminary recognition of candidate dna sequences
the next step of our method aims to recognize and extract dna sequences appearing in the text. to address this task, we analyzed  <dig> papers selected by domain experts that contained the most common representations for pcr primers and probes in scientific literature. we would like to remark that  the training and test sets were disjoint, and  we did not have access to the test set at this stage. as a result of this study, we found that primer and probe dna sequences are composed of individuals belonging to a 30-symbol alphabet—including  <dig> uppercase letters and their lowercase counterparts—that we will refer to as ∑ from now onwards. similarly, we will denote the set of all different strings composed of one or more symbols from ∑ as ∑+. the latter includes symbols for the four nucleotide bases adenine , cytosine , guanine , thymine , whereas the remaining individuals are wildcard characters that represent a single nucleotide chosen from two or more nucleotide symbols. see table  <dig> for further details on the wildcard-to-nucleotide mappings.

this table  shows the mappings between wildcard symbols used to represent dna sequences in scientific papers and their permissible nucleotide types.

regarding the representation of primer and probe sequences in scientific articles, we found that they may be enclosed by the strings 5' and 3' . other papers, by contrast, present the sequences immediately after the string 5' , but they do not terminate the sequence with the string 3' . in both cases, the occurrence of the "prime" symbol is optional, since there are several manuscripts where the sequences are enclosed by strings  <dig> and  <dig>  rather than 5' and 3' . similarly, other articles include sequences that do not appear enclosed between the strings 5' and 3' , but, instead, are sub-divided into n groups of exactly three nucleotides—except for the last group, which may include less than  <dig> bases. on the other hand, other manuscripts include sequences that do not match any particular pattern apart from being composed of symbols from ∑, blanks, dashes, colons and newline characters.

based on such findings, we created a set of three sequence detectors. the sequence detectors are applied to a textual input in a priority-based manner in decreasing order of specificity. if, given an input string, the current detector fails to match a sequence, then the input is forwarded to the next, more general recognizer. if a detector matches a sequence in an input string, confidence in the reliability of the matched sequence will be greater, the higher its priority is. in the following, we briefly describe the types of sequences detected by the different recognizers sorted by priority.  sequences of symbols belonging to ∑ either delimited by the pairs of strings <5', 3'>, <3', 5'>, < <dig>  3>, < <dig>  5> or beginning with the strings 5',  <dig>  3',  <dig>  these sequences may also contain blanks, dashes and can span multiple lines.  sequences organized into n- <dig> groups  of exactly three symbols from ∑ delimited by blanks plus an additional nth group that may contain three or less symbols. these sequences can also span multiple lines.  sequences of symbols from ∑ that can span several lines and that may also include blanks, colons and dashes. table  <dig> presents some examples of nucleotide sequences that can be recognized by each detector. as shown in figure  <dig>  we followed a theoretical computer science approach to build the different recognizers using finite state machines  <cit> .

ep1- f atg gtg ggc cag ctt gtc\n
this table shows some examples of sequences that can be recognized by each detector. the number under the column "detector" identifies the detector that recognized the "list of tokens" from the string "text string" that can be found in the manuscript whose pubmed identifier  is shown under the "pmid" column. the symbols /n and @ denote the newline and the table cell separator characters respectively.

to extract the candidate sequences occurring in a pdf paper, we feed the set of detectors with the text corresponding to the main sections of the manuscript—i.e. sections containing primer and probe sequences. these include sections whose type is one of the following: majorsection, submajorsection, subsubmajorsection, table and figure. for instance, sections such as "background", "methods", "results", "discussion" or "conclusions"—all of type majorsection—are provided as input to the detectors. conversely, sections such as the paper title , authors , "abstract" , "acknowledgements"  or "references"  are discarded. the text belonging to each section is obtained by iterating the st associated with the manuscript being processed in depth-first order. recognized sequences are then converted into a sequence of tokens composed of strings belonging to ∑+. separator tokens—i.e. blanks, dashes and newline characters—are not included in the sequence, since  blanks are already implicit in the structure of the list of tokens and  both dashes and newline characters are no longer required for further processing. conversely, colons are not discarded since they are required for the next stage. for instance, the string "5-cgt ccm\narr-gga-wac-tga" would be recognized by detector # <dig>  this would produce the list of tokens {"cgt", "ccm", "arr", "gga", "wac", "tga"}. table  <dig> shows a few examples of dna sequences recognized by each custom detector, and their corresponding lists of tokens as provided by the recognizers.

once the candidate sequences have been recognized and recorded, we move on to the next phase that we describe below.

phase 3: automated refinement and post-processing of the recognized dna sequences
this phase is aimed at automatically refining and post-processing the sequences—i.e. lists of tokens—extracted during the previous activity. this includes:  discarding false positives, i.e. list of tokens that despite including strings belonging to ∑+, do not represent any sequences occurring in the paper—e.g. {"standard", "assay"—;  refining noisy sequences that include residual prefix or postfix expressions belonging to ∑+—e.g. {"acgtacccgtacgat", "tamra", "t"—, and  splitting incorrectly merged sequences, which are composed of two or more different sequences linked by infix expressions belonging to ∑+—e.g. {"acgtacccgtacgat", "and", "cgtaccgtaccaggctac"}. besides, the refined sequences—still represented as lists of tokens—need to be converted into singletons whose only element belongs to ∑+, a format suitable for the sequences to be used for querying the blast tool.

to address these issues, we have adopted a knowledge engineering approach. we have created a rule-based expert system  <cit>  to automatically refine and post-process the extracted sequences. table  <dig> shows the complete knowledge base, composed of eight rules, whereas table  <dig> provides the description of functions, actions and symbols used by the different rules. each rule was specifically designed to address a different issue. r <dig> is aimed at discarding sequences whose size—in terms of the number of symbols—is smaller than a predefined threshold lmin. these sequences are unlikely to be true primer/probe sequences, and are thus discarded. we set the parameter lmin to a size of seven symbols. on the other hand, r <dig>  r <dig>  r <dig> and r <dig> are aimed at refining noisy sequences by removing residual suffixes  or prefixes  and even english words incorrectly appended by the detectors either at the beginning  or the end  of a sequence. the key difference among the pairs of rules <r <dig>  r3> and <r <dig>  r7> is that <r <dig>  r3> resorts to a list of problem affixes that can be composed of several tokens—e.g. {"tamra", "t")—whereas the <r <dig>  r7> removes single words belonging to a custom english dictionary created by the authors and composed of words belonging to ∑+. both the dictionary  and the list of problem affixes  are provided as additional material. by contrast, r <dig> aims to remove false positives—i.e. lists of tokens, all belonging to ∑+, that do not represent any real sequences. the rule resorts to the same dictionary used by rules r <dig> and r <dig> to discard all sequences—i.e. list of tokens—all of whose elements are in the dictionary. on the other hand, r <dig> is focused on splitting lists of tokens that contain two or more real sequences that were incorrectly merged by the detectors during the previous phase. this happens since two sequences may appear linked in the manuscript by words belonging to ∑+—e.g. the particle and—and thus the recognizers are unable to detect the end of the first sequence. to properly separate the sequences, the rule resorts to the list of affixes used by rules r <dig> and r <dig>  our approach can successfully separate incorrectly merged sequences composed of more than two real sequences by recursively applying rule r <dig>  the aim of the last rule is to convert a sequence defined by a list of tokens into a singleton whose only element is the concatenation of all elements in the list—i.e. this is a post-processing rule. rules are sorted in descending order of priority, r <dig> being the rule with highest priority.

s' = {s <dig> ...,si-1},
this table shows the complete knowledge base for refining dna sequences. r <dig> is designed to discard short sequences. r <dig>  r <dig>  r <dig> and r <dig> are designed to refine noisy sequences, whereas r <dig> deals with incorrectly merged sequences. r <dig>  by contrast, removes concatenations of dictionary words recognized by the detectors as valid sequences. finally, r <dig> converts a list of tokens containing two or more elements into a singleton whose only element represents the refined sequence. the symbol s denotes a list of tokens s = {s <dig>  s <dig> ..., sn} of size n. see table  <dig> for details on the functions, actions and symbols used by the different rules.

to automatically refine a set of sequences extracted from a manuscript we proceed as follows:  all the sequences—i.e. lists of tokens—detected by the recognizers are added to the facts base,  the inference engine attempts to match the antecedents of the rules to the elements in the facts base, and  if there is one or more matches, then the rule with highest priority is fired. the execution of the rule changes the state of the facts base, and then the whole process is repeated from step  <dig> until no more rules can be fired. at the end of this process, the facts base contains only singletons—i.e. lists of tokens containing just one element—each being a valid sequence. for further details on the structure and functioning of rule-based inference engines, see  <cit> .

{"gagggacgcttggtaacg"}, {"tcgcaagccaagcaaatac"},
this table shows the results of using the knowledge-based system to refine some sample lists of tokens produced by different recognizers in phase  <dig>  each row of the table presents the refinement process of a single list of tokens, including:  the initial contents of the facts base,  the execution trace and  the final state of the facts base. all singletons in the facts base at the end of the execution are considered as valid and refined sequences.

once the sequences provided by the detectors have been refined, we can proceed to the last activity described below.

phase 4: linking the recognized sequences to their corresponding organism/gene information
in this phase, we use local copies of the nucleotide databases associated to two publicly available online resources—namely blast  <cit>  and entrez nucleotide  <cit> —to connect the refined sequences to their corresponding organism and gene information, if available. to carry out this task, we created local instances of the blast-formatted database—downloaded from the ncbi website in fasta format—and of the entrez nucleotide database in relational format. the latter was created using the bioperl  <cit>  package. the rationale for using local copies instead of the actual online resources is that users and tools performing massive queries of these online resources without express authorization may be delayed—and even banned—by the ncbi. besides, if we query the nucleotide database using a concrete genbank identifier  via the provided web service, nucleotide returns all available information about the matched record—sometimes over  <dig> mb—, whereas we are only interested in the organism/gene name. instead, we retrieve just the required information from the local relational instance. the nucleotide database was populated with data on micro-organisms only, since this method is aimed at detecting pcr primers and probes related to infectious diseases. however, the method can be easily adapted for other species simply by updating or replacing the database contents with the required data.

to obtain the organism/gene information for a specific paper, we use the blast software tool to query the blast-formatted database with all the detected sequences for the current manuscript. for each sequence, we select the best  <dig> matches provided by blast, and then we record  their gis and  the relative positions of the query strings within the sequences associated with the matched gis.

once we have obtained the <gi, position> pair for each match, we query the relational instance of the nucleotide database using the gi to retrieve the associated organism name. on the other hand, the position is used to find the specific location within the gi entry that contains the gene name, if available. for each processed manuscript, the output of this activity is a set of tuples <paper, sequence, organism_name, gene_name>.

once the sequences have been labeled with the information obtained from the blast-formatted and nucleotide databases, then these results are checked against the text of the manuscript and automatically assigned a confidence score  ranging from  <dig> to  <dig> points. the cs is assigned as follows. for each <paper, sequence, organism_name, gene_name>tuple, we search the paper for  all occurrences of the string gene_name and  different automatically generated spelling variants of the string organism_name—e.g. for "brucella mellitensis 16m" we would generate the variants "brucella mellitensis", "brucella", "b. mellitensis 16m" and "b. mellitensis". gene names are assigned a cs of  <dig> points if the string gene_name appears in the text, plus  <dig> additional points when the string gene_name co-occurs with the detected sequence in the same section. it is assigned a null cs otherwise. conversely, the organism_name is assigned a score that depends on the size of the longest matched variant—hereinafter denoted l—when compared to the full length l of the organism name provided by the nucleotide database. the cs corresponding to a match—or matches—of length l ≤ l is calculated using the following function:  

RESULTS
the software implementing the four-phase method for sequence extraction and annotation was developed using the java programming language and the apache pdfbox open source library  <cit> . the software, including both the binaries and the source code, can be freely downloaded from the project website. see the availability and requirements section for further details.

to evaluate the performance of our method, a panel of experts composed of three senior molecular biologists from the institute of health carlos iii in madrid  created a test set composed of  <dig> papers published in  several bmc journals  and  different plos journals  containing  <dig> primer and probe sequences. the papers in the test set were manually collected by the experts. they did not follow any specific criteria other than availability, provided that the manuscripts contained actual primer and/or probe sequences.

experts were asked to manually analyze all the manuscripts in the test set to identify all valid dna sequences occurring in the papers. they were also asked to specify which of the identified dna sequences were actual primers and probes. according to the experts, less than the 2% of the dna sequences occurring in the manuscripts were regular dna sequences , thus suggesting that papers reporting primers and probes rarely contain regular dna sequences.

each manuscript from the test set was fed through the method's pipeline to output a list of detected sequences—together with the context in which they appear within the documents—annotated with organism/gene information. these results were manually analyzed and assessed by the panel of experts.

no. of sequences
on the other hand, table  <dig> summarizes the results of the evaluation of the annotation phase. as shown in table  <dig>  our method correctly annotated  <dig> % of the  <dig> detected primer/probe sequences. conversely,  <dig> % of the sequences were assigned incorrect organism names, whereas the nucleotide database did not return any results for the remaining  <dig> %. according to the experts, this happens because these are either human or chicken instead of microorganism sequences, and the local instance of the nucleotide database currently contains information regarding microorganisms alone. regarding the annotation with gene-related information of the  <dig> sequences previously tagged with correct organism names, the nucleotide database also returned  <dig> % with correct gene names. the nucleotide database did not return any results for the remaining  <dig> %. regarding performance issues, the annotation of all sequences appearing in a single manuscript took on average  <dig> minutes to complete.

 <dig> human
 <dig> chicken
discussion
to our knowledge, the most recent biomedical text mining and information extraction research using scientific papers as a source of information does not accept manuscripts in pdf format as input. instead these works resort to the plain-text or html versions of the documents, if available, or to tools, such as pdf to text/html converters or optical character recognition software, to extract the text to be processed from pdf files. however, these tools do not normally preserve the structure of the documents, and frequently fail to sort the sentences in the correct reading order. this hampers the information extraction and text mining activities. this problem is particularly serious when the target documents have a multiple column layout. to address these issues, our pdf-to-st converter automatically creates a data structure that preserves the original organization of the document, including sentence order. this facilitates the text processing tasks, since the text corresponding to any section of the document can be easily retrieved from the st data structure in the correct order. however, our approach requires a custom template to be created for each type of document to be processed—e.g. different journal layouts—to enable the pdf analyzer to properly build the st structure. at the time of writing this paper these templates are created by manually inspecting the document layout. however, we are currently developing a tool to help users to create custom templates. we believe that both the st data structure and the template creation tool are potentially valuable tools for biomedical informaticians working on text mining or information extraction and retrieval research.

regarding the recognition and refinement of the primer/probe sequences present in the papers, our method achieves high precision and recall rates, as stated in the results section. in addition, the knowledge engineering approach we followed for sequence refinement successfully handles the different issues caused by the large number of english dictionary words belonging to ∑+ that appear in the manuscripts. our approach is also flexible, since the knowledge base will not normally have to be modified to refine sequences not currently being properly recognized—e.g. noisy sequences that the knowledge-based system fails to adequately refine due to words from ∑+ that occur in the text but are missing from the dictionary or from the list of problem affixes—. instead, the expert system can be easily adjusted by adding the required elements to the list of affixes or to the dictionary. this can be done manually or even automatically following an adapted relevance feedback-based approach  <cit> . besides, the simplicity of the preliminary recognizers and the size of the knowledge base for sequence refinement—only eight rules—enables our method to achieve high throughput rates in sequence detection and refinement. our method automatically recognizes sequences—with large precision/recall rates—as primers or probes, provided the system is fed with papers known to contain primer and probe sequences alone. the expansion of this feature to recognize and discriminate the different types of dna sequences is a topic for future research. besides, most major approaches for dna/rna sequence recognition in biomedical text focus on efficiently detecting—or aligning—sequences built upon the symbols a, c, g and t only  <cit> . on the other hand, the kangaroo system  <cit>  is a web-based pattern matcher that reports back all genbank records that match a user query—i.e. a regular expression that may contain any of the symbols reported in table  <dig>  conversely, our approach addresses a different issue, i.e. recognizes dna sequences occurring in non-structured text.

regarding the automated annotation of the detected sequences with their corresponding organism/gene information, the method assigns a valid organism name for most sequences , as shown in table  <dig>  conversely, for some sequences , the method could not find any information in the database regarding the organism, since, according to the panel of experts, the sequences belonged to either humans or chickens. these results can be explained since the local copies of both the blast-formatted and nucleotide databases were populated with microorganism-related information only. we did not load the information corresponding to other organisms since the method was initially designed to detect primers and probes related to microorganism-caused infectious diseases. however, we plan to populate the database with information related to other organisms in the near future. regarding gene-related information, the system assigned correct gene names for  <dig> % of the sequences that were properly annotated with their corresponding organism name. for the remaining sequences, the database did not provide any results. according to the panel of experts, this deficient gene-related information is due to the fact that the database does not contain this information, or even, that it is currently unknown. regarding performance issues, annotating the sequences with their related information is computationally expensive, since multiple queries of different databases have to be launched. to address this issue, other distributed processing approaches may be helpful. this will enable the proposed pipeline to process a large number of documents in parallel, using multiple copies of the local databases.

CONCLUSIONS
in this paper we present an original method for automatically extracting primer and probe sequences from scientific papers and annotating them with their corresponding organism/gene information. our method can be used by biomedical researchers using molecular methods to diagnose and prescribe infectious diseases to facilitate tasks such as detecting the presence of a particular microorganism, or designing diagnostic pcr or microarrays. on the other hand, the extracted information can also be used to update the different existing primer/probe databases or to create a new data resource from the scratch. the proposed method can be extended to detect other types of biological sequences. in addition, the pdf-to-st converter is a potentially valuable tool for different kinds of bioinformatics research using pdf files as a source of information.

availability and requirements
• basic implementation of the proposed method: primerxtractor

• project website: http://www.gib.fi.upm.es/en/primerxtractor

• operating systems: platform independent, tested on windows vista and ubuntu linux  <dig>  .

• programming language: java.

• other requirements: java  <dig> , mysql community server  <dig>  activeperl  <dig> . <dig>  biosql and bioperl. see the online documentation at the project website for further details.

documentation, source code  and binaries of primerxtractor are available in the project website. other required software components are available at their corresponding sources.

authors' contributions
mgr participated in the creation of the method and in the design of the experiment, drafted the manuscript and supervised the work. ac participated in the creation of the method and in the design of the experiment, wrote the software tool implementing the presented method, and helped to draft the manuscript. vla and glc helped to draft the manuscript and led the evaluation process. gc, ddl, dpr, jc and fms participated in the design of the experiment and helped to draft the manuscript. vm helped to draft the manuscript and to coordinate the work. all authors read and approved the final manuscript.

supplementary material
additional file 1
dictionary of problem english words. list of english dictionary words belonging to ∑+ used by the expert system for sequence refinement.

click here for file

 additional file 2
list of problem affixes. list of problem affixes used by the expert system for sequence refinement.

click here for file

 acknowledgements
the present work has been funded, in part, by the european commission through the acgt integrated project  and the action-grid support action , the spanish ministry of science and innovation through the ontominebase project , the imgrasec project , fis/aes ps09/ <dig> and combiomed-retics, and the comunidad de madrid, spain. the authors would also like to thank carmen ramirez for collaborating in the evaluation process, rachel elliot for her editorial assistance and the anonymous reviewers for their valuable comments and suggestions.
