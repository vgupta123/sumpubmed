BACKGROUND
dna microarray technology enables the measurement of expression levels of thousands of genes simultaneously. this unique feature has a fundamental role in a wide range of current biological and medical research. one of the most common applications is to compare the gene expression levels in tissues extracted from different conditions, such as healthy versus diseased tissues, and thus to characterize the genetic profiles of these conditions. two main genetic profiling tasks have been investigated extensively in the past two decades: identification of biomarker genes, which are the genes differentially expressed under various experimental conditions; and the construction of classifiers based on these identified genes, to effectively recognize the experimental conditions. for example, recognizing the primary anatomical site of tumor origin is a fundamental requirement for optimal treatment for cancer patients.

early research on genetic profiling employed gene and sample class simultaneous clustering to recognize expression patterns associated with samples inside a class, as well as those genes whose expression levels characterize the class  <cit> . nevertheless, in general, among the thousands of genes examined, only a small number of them are significantly associated with the experimental conditions. these genes, or biomarkers, would have their expression levels increase or decrease under certain conditions, compared to normal expression levels. such discriminatory genes are very important in genetic disease studies as they often are set as targets for drug design  <cit> . however, identification of discriminatory genes is not an easy task, because there are usually only a small number of experiments  available for a typical application, largely due to the high experimental cost. the huge number of genes versus a tiny number of experiments is a familiar machine learning challenge, often labeled as "the curse of dimensionality".

there are many algorithms proposed in the past two decades for gene selection, most of which directly extend ideas in the more general problem of feature selection/extraction, which is a long-standing research topic in statistics and machine learning. nevertheless, one should keep in mind that gene selection in gene expression microarray data analysis differs from the general feature selection in many aspects, such as the dimensionality issue and the subsequent data overfitting; consequently, many general feature selection algorithms will not work satisfactorily on microarray data. on the other hand, gene selection is closely related to the other main task in genetic profiling: to build a good sample classifier. besides validating selected genes via other biological experiments, for example in which they are set as drug targets, another way to confirm that the selected genes are biologically correlated to various experimental conditions is to use them to build a sample classifier and to test its classification accuracy. clearly, high classification accuracy suggests the good quality of the selected genes. in fact, much research has followed this rule to compare their gene selection methods, in addition to considering the detailed biological annotations for the selected genes.

to name a few gene selection methods, golub et al.  <cit>  developed a measure of correlation that emphasizes the "signal-to-noise"  ratio in using the gene as a class membership  predictor, and selected a number of top ranked genes as discriminatory genes. the s2n ratio captures the basic rule of gene selection: a discriminatory gene must have close expression levels in samples within a class, but significantly different expression levels in samples across different classes. other approaches that follow the same rule, with certain modification and refinement, include t-test  <cit> , regularized t-test  <cit> , supervised gene shaving  <cit> , a method focusing on expression homogeneity in a certain class  <cit> , a method which uses both small within-class expression variance and large between-class expression variance  <cit> , as well as many other so-called single gene scoring or univariate methods.

to further exploit the correlations amongst genes, xiong et al.  <cit>  first ranked genes using their individual classification accuracy and then selected a subset of genes with an overall near maximal classification accuracy through sequential  forward selections . guyon et al.  <cit>  did not rank genes but suggested a gene selection method to use support vector machines  combined with recursive feature elimination  to select a subset of genes with an overall near maximal classification accuracy. li et al.  <cit>  combined a genetic algorithm  and a k-nearest neighbor  method to identify a fixed number of genes that can correctly classify binary samples, based on the occurrence frequency of the gene in many gene subsets. lee et al.  <cit>  and zhou et al.  <cit>  proposed to first constrain the number of genes to be selected and then use a markov chain monte carlo  based stochastic search algorithm to discover important genes. shevade and keerthi  <cit>  proposed a logistic regression for class membership estimation based on a linear combination of all the genes, then further reduced the density by setting up a sensitivity threshold to prune genes. the resulting regression model is thus sparse and the remaining genes are considered informative. díaz-uriarte and alvarez de andres  <cit>  defined a measure of gene importance with the random forest and iteratively removed the gene with the smallest importance until the smallest out-of-bag error rate is yielded.

jaeger et al.  <cit>  proposed a pre-filtering approach where fuzzy clustering was applied, using the gene expression levels across all samples, followed by selecting a varying number of representatives per cluster.

these representative genes were then ranked by the aforementioned t-test to select the top-ranked as feature genes. a similar approach was proposed by hanczar et al.  <cit>  to cluster genes using their expression levels across all samples and then to represent the clusters using their mean expression levels ; this was followed with single gene scoring methods on the prototype genes to select a pre-specified number of top-ranked genes. noticing that in general the gene clustering algorithms need the number of clusters as an input parameter , the hykgene proposed by wang et al.  <cit>  first applied single gene scoring methods to select a set of top-ranked genes, then performed a hierarchical clustering on them, and lastly selected one representative per cluster to form the final marker genes. the number of clusters in hykgene was determined using the leave-one-out cross validation classification accuracy.

it is worth pointing out that most of the proposed gene selection methods are classifier independent, or they can be combined with any classification methods, such as linear discrimination analysis  <cit> , nearest neighbor models  <cit> , support vector machines  <cit> , and logistic regression models  <cit> . these gene selection methods typically produce a small set of biomarker genes which can be used in classifier construction. however, there are some other gene selection methods which are bound with specific classifiers  <cit> .

it is also equally important to point out that most of these gene selection methods, and the associated classifiers, work only on two-class datasets, though through one-versus-all  they can be theoretically extended to multi-class datasets. the methods that have been explicitly tested on multi-class dataset include  <cit> . in addition, ramaswamy et al.  <cit> , su et al.  <cit> , and pomeroy et al.  <cit>  specifically dealt with classification of multiple classes of human tumors through identification of a set of tumor genes .

in the context of gene selection and the subsequent classification, the performance of one method is normally validated through a training stage that tunes the parameters in the method, followed by a testing stage which estimates the quality of the selected genes and the performance of the resultant classifier. for some of the above mentioned methods, such as the ones in  <cit>  and  <cit> , a given gene expression dataset is partitioned into two parts, one called training dataset and the other testing dataset. the class memberships of training samples are used in the training process while the class memberships of testing samples are blinded to the classifier for estimating its classification accuracy. other methods adopt cross validation schemes for performance evaluation. there are two popular cross validation schemes, one is ℓ-fold and the other is leave-one-out . in ℓ-fold cross validation  <cit> , the whole given dataset is  partitioned into ℓ equal parts, and  parts of them are used to form the training dataset while the other one to form the testing dataset; the process is done when every part has been used as a testing dataset. the classification accuracy is defined as the ratio between the number of correctly predicted samples and the total number of testing samples. most of the methods that adopt this cross validation scheme repeat the  partition several times and the average classification accuracy is reported. however, most of the methods, for example, ramaswamy et al.  <cit> , su et al.  <cit> , and wang et al.  <cit> , adopt the loocv scheme, in which only one sample is used as the testing sample while all the others are used to form the training dataset; the scheme goes over every sample in the given dataset. again, the ratio between the number of correctly predicted samples and the total number of samples is defined as the classification accuracy. in this work, on each real cancer microarray dataset, we adopt the same cross validation scheme to the methods we are comparing with. we also report the loocv results on all the datasets and some 5-fold cross validation results.

while binary classification has been extensively explored, multi-class classification remains challenging in microarray data analysis. in this work, we focus on gene selection for the multi-class classification and we demonstrate the strength of the proposed method by applying it on cancer subtype identification. our main goal is to select genes that, as a whole, have superior class discrimination strength. to this purpose, for each gene we define its class discrimination strength vector, and based on these vectors we measure the similarity between two genes. such a similarity measurement is adopted in the k-means algorithm to cluster genes. subsequently, a single gene scoring method is used to rank all the genes. our method then walks through this ordered gene list and picks up one gene per cluster until a pre-specified number is reached. these selected genes are then used to construct a classifier whose performance is evaluated and measured by the loocv  classification accuracy. through experiments on four real multi-class human cancer microarray datasets, we demonstrate that our method can achieve significantly higher classification accuracies than the best previously published. in this sense, our method can serve as a useful addition to existing methods for highly accurate human tumor classification.

 <dig> methods
assume in the given multi-class microarray dataset there are n samples on p genes, and these n samples belong to m classes. in the following, we define a novel vector representation for genes, which can differentiate their class recognition strength.

let gij denote the expression level of the i-th gene in the j-th sample. that is, in the gene expression matrix, each row represents a gene, gi = ⟨gi <dig>  gi <dig> ...,gin⟩, and each column represents a sample. for the i-th gene, its mean expression level in the k-th class is denoted as hik, for k =  <dig>   <dig> ...,m. the value |hik - hil| captures the difference between the mean expression levels of the i-th gene in the k-th class and in the l-th class. obviously, if this value is small, then the i-th gene would not be effective in discriminating samples from these two classes, but it could be effective otherwise. therefore, we define the class discrimination strength vector for the i-th gene as

 hi = ⟨|hi <dig> - hi2|, |hi <dig> - hi3|,...,|hi <dig> - him|, |hi <dig> - hi3|, |hi <dig> - hi4|,...,|hi <dig> - him|, |hi <dig> - hi4|,...,|hi,m- <dig> - him|⟩. 

let d <dig> and d <dig> denote the euclidean distances between the i1-th and the i2-th genes based on their g-vectors and h-vectors, respectively. note that there are n entries in the g-vectors and m/ <dig> entries in the h-vectors, respectively. we define

 d=d1n+2d2m
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgkbazcqggoaakcqwgpbqadawgaawcbagaegymaedabeaakiabcycasiabdmgapnaabaaaleaacqaiyagmaeqaaogaeiykakiaeyypa0zaasaaaeaacqwgkbazdawgaawcbagaegymaedabeaaaoqaaiabd6gaubaacqghrawkdawcaaqaaiabikdayiabdsgaknaabaaaleaacqaiyagmaeqaaagcbagaemyba0maeiikagiaemyba0maeyoei0iaegymaejaeiykakcaaaaa@4537@ 

to be the distance between the i1-th and the i2-th genes.

we can calculate the euclidean distance by eq.  between every pair of genes, and then call the k-means algorithm to cluster genes, where the default value for the number of gene clusters k is  <dig> . essentially, k-means is a centroid-based clustering algorithm that partitions the genes into k clusters based on their pairwise distances, to ensure that intra-cluster similarity is high and inter-cluster similarity is low.

at the same time, we run a gene ranking method to sort all the genes into a decreasing order of their class discrimination strength. such a method essentially assigns a score for each gene, which approximates its class discrimination strength. the gene scoring functions can be the classification accuracy of individual genes  <cit> , or can be designed to capture the basic rule that discriminatory genes are those having close expression levels in samples in a common class but significantly different expression levels in samples from different classes  <cit> . the latter category of gene ranking methods are also called single gene scoring methods, which do not consider the correlation between genes when assigning the scores. in our study, we adopt the f-test  <cit>  and the gs method  <cit>  as our base gene ranking methods.

walking through the gene order and using the gene cluster information obtained by the k-means algorithm, our method picks up a pre-specified number of genes under the constraint that at most t genes per cluster are included. for ease of presentation, we call our gene selection method, which targets at selecting genes having dissimilar class discrimination strength, the disc-based method, and use disc-f-test and disc-gs to denote the facts that the base gene ranking method is the f-test and the gs method, respectively. we remark that the disc-based method is generic, in that it can use any other single gene scoring method, such as the cho's gene ranking method, to create the disc-cho's gene selection method.

to computationally evaluate the quality of the selected genes as a whole, we use the classification accuracy of the classifier built on the selected genes, under the loocv scheme. in other words, for each gene selection method , the selected genes are used in the k nearest neighbor model or the support vector machine model to construct a classifier , and then use the classifier to predict the class memberships of the testing samples. the percentage of correctly predicted memberships is defined as the classification accuracy of the classifier. essentially, the knn-classifier  <cit>  predicts the class membership of a testing sample by a majority vote using its k nearest neighbors in the training dataset; the linear kernel svm-classifier  <cit>  finds decision planes to best separate the set of training samples having different class memberships, then uses these planes to predict the class memberships of the testing samples. for each class membership prediction, we have also calculated the associated confidence score as follows. su et al.  <cit>  adopt the dixon metric to assign confidence scores, where the distances between the testing sample and all the training samples are calculated, then the "class distance" between the testing sample and a class is derived to be the average distance over all the distances between the testing sample and all the training samples in that particular class. these class distances are then sorted in increasing order, say for example, d <dig> ≤ d <dig> ≤...≤ dm where m is the number of classes, to compute the value c = / which is the confidence value for assigning the closest class to the testing sample. a dixon threshold of  <dig>  is generally accepted as conservative boundary for high confidence prediction. associated with our knn-classifier for k =  <dig>  assuming the closest  <dig> neighbors for the testing sample are at increasing distances d <dig>  d <dig> ...,d <dig> and the k1-, k2-, k3-th neighbors form a majority vote, then /
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqggoaakdawcaaqaaiabigdaxaqaaiabdsgaknaabaaaleaacqwgrbwadawgaaadbagaegymaedabeaaasqabaaaaogaey4kasyaasaaaeaacqaixaqmaeaacqwgkbazdawgaawcbagaem4aas2aasbaawqaaiabikdayaqabaaaleqaaaaakiabgucarmaalaaabagaegymaedabagaemizaq2aasbaasqaaiabdugarnaabaaameaacqaizawmaeqaaawcbeaaaagccqggpaqkcqggvawlcqggoaakdawcaaqaaiabigdaxaqaaiabdsgaknaabaaaleaacqaixaqmaeqaaaaakiabgucarmaalaaabagaegymaedabagaemizaq2aasbaasqaaiabikdayaqabaaaaogaey4kasyaasaaaeaacqaixaqmaeaacqwgkbazdawgaawcbagaeg4mamdabeaaaagccqghrawkdawcaaqaaiabigdaxaqaaiabdsgaknaabaaaleaacqai0aanaeqaaaaakiabgucarmaalaaabagaegymaedabagaemizaq2aasbaasqaaiabiwda1aqabaaaaogaeiykakcaaa@56d9@ is assigned as the confidence value. the prediction is considered as highly confident if the value is greater than  <dig> . the svm-classifier  <cit>  we adopt implements a decision directed acyclic graph to combine several binary classifiers into a multiclass classifier and does not produce all the distances for confidence evaluation. we also calculated the covariance of the selected genes, and conducted permutation tests, to measure the extent of dissimilar class discrimination strength of the selected genes as a whole.

 <dig> computational 
RESULTS
 <dig>  the carcinomas dataset
the carcinomas dataset contains in total  <dig> samples  in  <dig> classes: prostate, bladder/ureter, breast, colorectal, gastroesophagus, kidney, liver, ovary, pancreas, lung adenocarcinomas, and lung squamous cell carcinoma, which have  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> samples, respectively  <cit> . each sample contains  <dig>  genes, whose maximum hybridization intensity is ≥  <dig> in at least one sample. all hybridization intensity values <  <dig> were raised to  <dig>  the data were subsequently log transformed. we obtained this dataset through the website provided by su et al.  <cit> .

on this dataset, su et al.  <cit>  applied a wilcoxon score to rank genes and selected the top ranked  <dig> genes for each of the  <dig> classes. using a subset of  <dig> samples, under the ova/loocv scheme, an svm-classifier was used to further select  <dig> best genes , among the  <dig> genes, which achieved the highest loocv classification accuracy . the classifier trained on these  <dig> samples was then tested independently on the other  <dig> samples , and achieved  <dig> correct predictions, amongst which  <dig> were confident.

our disc-f-test method was trained on the same  <dig> samples by setting  <dig> gene clusters to select  <dig> genes. the knn-classifier was able to make  <dig> correct predictions on the  <dig> testing samples, among which  <dig> were confident and among the confident predictions  <dig> were correct. it was pointed out that the samples not being correctly classified in the original paper are considered very difficult for computational classification  <cit> . therefore, our disc-f-test-knn classifier can be regarded as a significant improvement . alternatively, our disc-gs-knn classifier, with exactly the same settings, performed slightly worse than the disc-f-test-knn classifier, making only  <dig> correct predictions out of  <dig>  among which  <dig> are confident and  <dig> of them are accurate. nevertheless, the disc-gs-knn classifier also outperformed the method by su et al.

we have also performed a loocv on the f-test, gs, disc-f-test, and disc-gs methods, combined with the svm and knn  classifiers, on the whole dataset of  <dig> samples. to run the disc-based methods, we set the number of gene clusters in the k-means algorithm to be  <dig>  figure  <dig> plots their classification accuracies with a number of selected genes, ranging from  <dig> to  <dig>  the plot clearly shows that the disc-based methods significantly outperformed the single gene scoring methods , in terms of the classification accuracy, but there was a convergence tendency with an increasing number of selected genes. typically, when  <dig> genes were selected, the disc-f-test-knn classifier made  <dig> confidence predictions out of  <dig>  among which  <dig> are correct . among the other  <dig> non-confident predictions,  <dig> are correct. the overall classification accuracy is  <dig> %.

 <dig>  the embryonal dataset
the embryonal dataset  <cit>  contains a total of  <dig> patient samples analyzed with oligonucleotide microarrays containing probes for  <dig>  genes . these  <dig> samples include  <dig> medulloblastomas,  <dig> cns at/rts,  <dig> renal and extrarenal rhabdoid tumours, and  <dig> supratentorial pnets, as well as  <dig> non-embryonal brain tumours  and  <dig> normal human cerebella. note that there is a slight difference between this dataset and the one mentioned in the original paper  <cit> , which contains  <dig>  genes. note also that cns at/rts and rhabdoid tumours are together considered as a class, so there are  <dig> samples in  <dig> separate classes.

on this dataset, pomeroy et al.  <cit>  applied ova s2n statistics  to select a number of genes, and then built a weighted knn  to predict class memberships. this method obtained  <dig> correct predictions of the  <dig> under the loocv scheme.

our disc-gs-knn classifier made  <dig> correct predictions when the number of selected genes was  <dig> and the number of clusters was set to  <dig> . note that there is no confidence evaluation associated with the predictions reported in the original paper. for the disc-gs-knn classifier,  <dig> out of the  <dig> correct predictions were confident. the disc-f-test-knn classifier also made  <dig> correct predictions out of the  <dig> 

again we performed a complete loocv on the f-test, gs, disc-f-test, and disc-gs methods, combined with the svm classifier and the knn classifier, respectively. since the number of samples in each class is small, we set k =  <dig> in the knn classifier. to run the disc-based methods, we set the number of gene clusters in the k-means algorithm to be  <dig>  since the number of genes is relatively small compared to the other datasets. figure  <dig> plots their classification accuracies when a number of genes are selected, which ranges from  <dig> to  <dig>  the plot clearly shows that the disc-based methods significantly outperformed the single gene scoring methods , in terms of the classification accuracy.

 <dig>  the lung carcinomas dataset
the lung carcinomas dataset  <cit>  contains  <dig> samples  on  <dig>  genes. these  <dig> samples are distributed in  <dig> classes . by removing those genes with standard deviations smaller than  <dig> expression units, the resultant dataset contains  <dig>  genes.

hanczar et al.  <cit>  proposed a method that first uses the k-means algorithm to cluster genes and then defines a progene as the mean expression vector of the genes in a cluster. subsequently, a single gene scoring method is run on the progenes to select a subset of them to build an svm-classifier. under the 3-fold cross validation , a highest classification accuracy of  <dig> % was obtained in  <cit> , where  <dig> gene clusters were formed and  <dig> progenes were selected.

similarly, under the 3-fold cross validation in which  <dig> random partitions have been performed, our disc-gs-knn and disc-f-test-knn classifiers were able to achieve  <dig> % classification accuracy, when no more than  <dig> genes were selected.

again we did a loocv on the f-test, gs, disc-f-test, and disc-gs methods, combined with the svm classifier and the knn classifier , respectively, on the dataset. to run the disc-based methods, we set the number of gene clusters in the k-means algorithm to be  <dig>  figure  <dig> plots their classification accuracies with respect to the number of selected genes, ranging from  <dig> to  <dig>  their classification accuracies clearly show that the disc-based methods significantly outperformed the single gene scoring methods , in terms of the classification accuracy. for both disc-based methods, the knn classifiers made  <dig> confident predictions out of the  <dig>  among which  <dig> were correct .

 <dig>  the all tumor dataset
ramaswamy et al.  <cit>  targeted pure molecular classification of tumor samples and assembled the all tumor dataset, which contains  <dig> tumor and  <dig> normal tissue samples on  <dig>  genes   <cit> . using these samples, one training dataset consists of  <dig> tumor samples . the testing dataset consists of the other  <dig> tumor samples .

adopting ova scheme, the authors proposed to use a linear svm algorithm  to recursively eliminate the bottom 10% genes that show low importance. under the loocv scheme, on the training dataset, their method was able to make  <dig>  confident predictions of which  <dig>  were correct. only  <dig>  of the other  <dig>  predictions of low confidence were correct, which gave an overall training classification accuracy of  <dig> %. the classifier thus trained on the  <dig> samples was tested on the independent  <dig> samples, and achieved a classification accuracy of  <dig> % . the loocv classification accuracy of our disc-gs-knn classifier on the training dataset reached  <dig> % too , the same as that in  <cit> . nonetheless, with respect to confident prediction, our disc-gs-knn classifier performed much better on the testing dataset, where it made  <dig>  confident predictions, of which  <dig>  were correct.

we have also performed a loocv on the f-test, gs, disc-f-test, and disc-gs methods, combined with the svm classifier and the knn classifier , respectively. to run the disc-based methods, we set the number of gene clusters in the k-means algorithm to be  <dig>  figure  <dig> plots their classification accuracies with respect to the number of selected genes, ranging from  <dig> to  <dig>  the plot shows that the disc-based methods outperformed the single gene scoring methods , typically when combined with the knn classifier, in terms of the classification accuracy. for both disc-based methods, when selecting  <dig> genes, the knn classifiers made  <dig>  confident predictions out of the  <dig> predictions, among which  <dig>  were correct.

 <dig> discussion
 <dig>  the number of gene clusters and the maximum number of genes per cluster
based on the hypothesis that all strong gene clusters must be identified, in order to support reasoning about biologically plausible causation, we have adopted principal component analysis and various hierarchical clustering algorithms to find the most likely number of predictive gene clusters in a dataset. unfortunately, the determination was not uniformly successful, as on some datasets the returned number of gene clusters is unreasonably small . obviously, if too many gene clusters are created, the disc-based gene selection methods might still include too many redundant genes. on the other hand, if the number of gene clusters is too small, then the disc-based methods would miss some useful genes. however, the true number of gene clusters must remain dataset dependent, and the effective determination of that number is challenging. we chose to examine several likely values for the number of gene clusters, k, in the k-means algorithm: k =  <dig> –  <dig> in tens. for each k, we examined at most t genes per cluster, for t =  <dig> –  <dig>  for the carcinomas dataset, all the corresponding classification accuracies, with respect to the number of selected genes, are plotted in figures  <dig> and  <dig>  in these plots, we used a 5-fold cross validation scheme  and combined with the svm and the knn classifiers.  figures  <dig> and  <dig> show that the classification accuracy reached the highest when t =  <dig>  and the value of k had little impact on the experimental results. in fact, the classification accuracies were almost the same at all  <dig> different values for k. we decided on a default setting of k =  <dig> and t =  <dig> 

note that we randomly selected k genes as initial cluster centers every time we ran the k-means algorithm. the reported results in the last section are from the first run of the k-means algorithm. we in fact have repeated the whole process  <dig> times, but we found out that the detailed gene clustering results from the k-means algorithm do not affect the subsequent classification accuracies. for the loocv study on the whole carcinomas dataset of  <dig> samples, the standard deviations of the classification accuracies of the four disc-based classifiers are all very small across all the numbers of selected genes, and the reported classification accuracies of the four disc-based classifiers on the first run of the k-means algorithm all have p values around  <dig> , indicating that they are not particularly correlated to the k-means gene clustering results. for the disc-gs-knn classifier, the loocv classification accuracies on the first run of the k-means gene clustering algorithm are plotted in figure  <dig>  together with the average classification accuracies and the standard deviations over  <dig> runs of the k-means algorithm. for the other three disc-based classifiers, the results are similar and we omit them from figure  <dig> since plotting them all makes the figure difficult to read.

 <dig>  the impact of class discrimination strength vector
to increase our confidence on the independence of each identified gene cluster and improve accuracy in sample classification, we define the notion of a class discrimination strength vector for each gene, the h-vector, which contains the absolute differences between all pairs of class mean expression values. subsequently, the distance between two genes, d, is defined as a weighted linear combination of the euclidean distances, d <dig> and d <dig> between their g-vectors and their h-vectors , respectively. for comparison purpose, we have also experimented with the distance d <dig> in the k-means algorithm, and the results showed that using the h-vectors does improve the clustering quality, which in turn results in a higher classification accuracy. on the carcinomas dataset, applying the f-test method to rank all the genes, and then using the gene cluster information returned by both distance measures , d and d <dig>  to select one gene per cluster, we collected the 5-fold classification accuracies for both the knn- and svm-classifiers and plotted them in figure  <dig>  where "disc" and "expression" indicate the gene cluster information by the distance measures d and d <dig>  respectively.

we have also looked into the percentage of the overlapped genes selected using two different gene cluster information. to this purpose, we allowed minor rank difference to set t =  <dig>  that is, up to three genes per cluster were selected. in figure  <dig>  the x-axis labels the total number of selected genes and the y-axis labels the percentage of the overlapped genes between the gene sets selected using the distance measures d and d <dig>  respectively, where each of the f-test and the gs methods was applied to rank all the genes. for the first  <dig> genes, the two distance measures voted the same. though there are several local peaks, the overall tendency of the overlapping percentage was decreasing, which non-surprisingly indicates that the h-vector did contribute to the gene clustering quality that eventually improved the classification accuracy.

 <dig>  covariance of selected genes and their permutation tests
as we mentioned earlier, the underlying design idea in the disc-based gene selection methods is to avoid selecting too many similar genes for classifier construction. covariance can be used to measure the correlation among two or more sets of random variables, where a larger covariance value indicates a stronger relationship among the sets of variables. table  <dig> shows the average absolute covariance value among all the pairs of gene sets selected by different gene selection methods, on the lung carcinomas dataset. in this case, we ran the loocv to collect  <dig> sets of  <dig> selected genes each. next, we calculated the frequency of a gene occurring in these  <dig> sets. the  <dig> most frequent genes were identified and their covariance was calculated, based on their expression levels across all the  <dig> samples. the absolute covariance values are listed in table  <dig>  where one can see that the genes selected by the f-test and gs methods have larger absolute covariance values. this indicates that these two 80-gene sets have stronger relationships to each other, or they share closer expression patterns. the disc-f-test and disc-gs methods produced gene sets of smaller covariance values, indicating that the selected genes are more dissimilar to each other.

for each of these four 80-gene sets, we also examined its quality by doing the permutation test under the 5-fold cross validation scheme. each time, the sample class labels in the complete dataset were randomly permuted, then the dataset was randomly partitioned into  <dig> equal parts to build the knn- and svm-classifiers using these  <dig> genes, and lastly the average classification accuracy was collected. the random permutation was repeated for  <dig>  times and the  <dig>  classification accuracies were fitted into a normal distribution. on the embryonal dataset, the p-values of the achieved classification accuracies on the original dataset, by all eight classifiers, are listed in table  <dig>  where one can see that the p-values associated with the disc-based methods are much smaller, indicating the higher quality of the selected genes.

 <dig> 
CONCLUSIONS
in this paper, we aimed at solving the much more challenging multi-class classification problem in microarray data analysis. we have examined a novel method to incorporate gene cluster information to identify many biologically relevant discriminatory genes, and to use that information to construct classifiers for such a purpose. we define a distance measurement between two genes to approximate their difference in the class discrimination strength, based on a novel class discrimination strength vector representation. this disc-based gene selection method is generic, in that it can be combined with any gene ranking method to identify genes that have dissimilar strength in class discrimination, but together they would have superior class discrimination strength. our experiments on four real human cancer microarray gene expression datasets showed that the disc-based gene selection methods achieved significantly higher classification accuracies, compared to the corresponding non-disc-based gene selection methods.

 <dig> authors' contributions
zc, mrs, and gl participated in the method design. zc and gl performed all the experiments. zc, rg, and gl participated in the experimental result discussion. all authors participated the paper writing. rg and gl finalized the submission. all authors read and approved the final manuscript.

