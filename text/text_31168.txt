BACKGROUND
microscopic processes with few interacting components can have considerable effects at
the macroscopic scale  <cit> . stochasticity is a defining property of these processes, which can have so
few component particles that random fluctuations dominate their behaviour  <cit> . stochastic simulation methods take proper account of these fluctuations, as
opposed to deterministic methods that assume a system does not deviate from its mean
behaviour  <cit> ; although deterministic methods can often be useful for an approximate
description of the dynamics of a system, their results are not always representative  <cit> .

a common stochastic modelling approach is to consider the system as a continuous-time
markov jump process  <cit> . the stochastic simulation algorithm  of gillespie  <cit>  is a simple and exact method for generating markov paths. however, because it
keeps track of each reaction, it can be too computationally costly for more complex
systems or those with frequent reactions. many approximate methods have since been
developed, which use similar principles as the ssa but group many reactions into a
single calculation, reducing computational time .

the first of these is commonly called the euler or poisson τ-leap  <cit> ; it corresponds to the euler method for ordinary differential equations
, and samples a poisson random variable at each step. the original stepsize
selection procedure has since been modified to improve accuracy  <cit> . to deal with issues of negative populations, tian and burrage  <cit>  and chatterjee et al. <cit>  introduced the binomial τ-leap, which samples a binomial random
variable at each step. in addition, a newer binomial τ-leap  <cit>  and a multinomial τ-leap  <cit>  have since been proposed.

in his seminal paper, gillespie also proposed the midpoint τ-leap, a
higher-order method that allows for larger timesteps by reducing the inherent bias of
the τ-leap  <cit> . more recently, other higher-order methods have been developed, such as the
unbiased τ-leap  <cit> , random-corrected τ-leap  <cit> , θ-trapezoidal τ-leap  <cit> , and extrapolated τ-leap  <cit> . rather than focussing on adaptively optimising the timestep to reduce
processing time, these methods instead improve their order of accuracy so that they find
more accurate results for a given stepsize. because of this, they can use larger
timesteps for a desired error level, reducing processing time.

in this paper, we introduce a new adaptive-stepsize method for simulating discrete
markov paths, which we call the stochastic bulirsh-stoer  method. this is inspired
by the deterministic method of the same name, a very accurate method for solving odes,
based on richardson extrapolation. its high accuracy due to extrapolation, and its
ability to adaptively maximise the timestep make the bulirsch-stoer method one of the
most powerful ode solvers. possessing these same advantages, our sbs method is a very
efficient and accurate new discrete stochastic numerical method.

the sbs calculates several approximations for the expected number of reactions occurring
per timestep τ using stepsizes τ/ <dig>  τ/ <dig>  and
so on, and extrapolates these to arrive at a very accurate estimate; the state of the
system is then found by sampling a poisson distribution with this parameter. the sbs is
in some ways similar to the extrapolated τ-leap methods we proposed in a
previous paper  <cit> . these involve running simulations with a τ-leap method of
choice over the full time period of interest, and then taking moments and extrapolating
them. the sbs is also based on extrapolation, but the extrapolation is carried out
inside each timestep, rather than at the end of the simulation, allowing
τ to be optimised at each step.

overview of stochastic methods
we start with a chemical system of n species and m reactions,
interacting in a fixed volume Ω at constant temperature, that is both
well-stirred and homogeneous. thus we assume that individual molecules undergo both
reactive collisions and non-reactive ones, and the latter is more frequent than the
former, mixing the molecules thoroughly  <cit> . individual molecules are not tracked, rather it is their total numbers
that we are interested in. these are stored in an n× <dig> state vector,
x≡x≡
t
, that contains the integer number of each type of molecule at some time
t. reactions are represented by an n×m matrix
consisting of stoichiometric vectors 
ν

j
≡
t
,j= <dig> …,m, which dictate how each reaction changes the
system state, and an m× <dig> vector of propensity functions a
j
, where a
j
dt gives the probabilities of each reaction occurring in an infinitesimal
time interval dt. together, these three variables fully characterise the
chemical system as it evolves through time. in this paper, we adopt the following
notation: a bold font variable refers to an n× <dig> vector, e.g.
x, and unless otherwise specified the indices
i= <dig> …,n and j= <dig> …,m.

a conceptually simple way of simulating problems using this framework is the ssa of
gillespie  <cit> . it steps along reaction-by-reaction, at each step calculating the
 time until the next reaction τ, and the
reaction j′ that will occur. the state vector is evolved in time according to
the update equation 

  xn+1=xn+∑j=1mνjkj,kj={1ifj=j′,0otherwise, 

i.e. only one reaction occurs over [t,t+τ). both
τ and j′ are sampled randomly as required by the stochastic nature of the
process:   

the ssa is a statistically exact method for generating monte carlo paths.
that is, a histogram built up from an infinite number of simulations of the ssa will
be identical to the true histogram of the system. it is the stochastic method of
choice for many researchers, but it has one main limitation: as with other stochastic
methods, many realisations 
must be simulated to get a reasonable idea of the histogram shape, and ssa
simulations can be slow depending on the problem.

the τ-leap  <cit>  was introduced by gillespie as a faster alternative to the ssa. it
improves speed by evaluating many reactions in one step, which is typically much
larger than that of the ssa. this allows the τ-leap to be generally
very fast compared to the ssa, but also means that it is not exact. assuming
τ is sufficiently small so that the propensities do not change
significantly during each step , the number of
reactions occurring during [t,t+τ), k
j
, is a poisson random variable  <cit>  with parameter a
j
τ. the simplest τ-leap implementation is
the euler τ-leap with fixed stepsize.   

this method is effective but a little crude: unless simplicity is the most important
consideration or τ must be fixed, as is the case when used in the
context of richardson extrapolation  <cit> , it is advisable to use more advanced τ-leap methods.

adaptively changing τ at each step can give even greater gains in
speed, and this is easily introduced into algorithm  <dig>  a successful approach in
current implementations is to find τ such that the mean and variance of
the change in propensities over [t,t+τ) are
bounded by some fraction ε≪ <dig> of a
j
. the advantage of this is that τ is controlled to stick more
closely to the leap condition, ensuring better accuracy, while at the same time
maximimising τ for faster simulations. there have been several
successive improvements for best selecting τ <cit> , and these methods can achieve a very high efficiency.

methods
extrapolation
richardson extrapolation is a technique for increasing the order of accuracy of a
numerical method by eliminating the leading error term in its error expansion  <cit> . it involves numerically solving some deterministic function
y at a given time t=nτ using the same solver with different stepsizes, where we define ytτ as an approximation to y at time t
using stepsize τ. y can be written as 

 y=ytτ+εg, 

 where ε
g
 is the error of the approximate solution compared to the true
one. for a general numerical solver, ε
g
 can be written in terms of powers of the stepsize
τ: 

  εg=ek1τk1+ek2τk2+ek3τk3+…, 

where the e
k
 are constant vectors and depend only on the final time and k1<k2<k <dig> …. eq.  tells us that this method has order of accuracy
k <dig> 

essentially, richardson extrapolation employs polynomial extrapolation of
approximations ytτq,q= <dig> ,… and τ1>τ2>…, to estimate yt <dig>  i.e. the numerical solution in the limit of zero stepsize, which
corresponds to y . each
successive extrapolation removes the next leading error term, which is the largest
contribution to the error, thereby increasing the accuracy of the numerical solution
and allowing it to better estimate y.
stepsizes τ1=t,τ2=t <dig> τ3=t <dig> find estimates closer and closer to the true
solution y=yt <dig>  i.e. the numerical solution in the limit of zero
stepsize. they can be extrapolated to find an estimate very close to
yt <dig> 

to demonstrate this, assume a numerical method with stepsize τ has an
error expansion of 

 y−ytτ=e1τ+e2τ2+o 

 for instance, the well-known euler method for solving odes has such an error
expansion. now instead of τ, if we use a stepsize τ/ <dig> 
the error expansion is 

  y−ytτ/2=e1τ2+e2τ24+o 

we can take ytτ,τ/2=2ytτ/2−ytτ, giving 

  y−ytτ,τ/2=−e2τ22+o 

the leading error term has been removed, resulting in a higher-order approximation.
this can be repeated to obtain an even higher order of accuracy by using more initial
approximations ytτ <dig> …ytτq, where q can be any integer and τ1>τ2>… τ
q
. we define ytτ <dig> τq as the extrapolated solution using initial approximations ytτ <dig> …ytτq. the easiest way of visualising this is to build up a neville table
 from the initial approximations .
ytτ <dig> …,ytτq
with order k <dig>  and
extrapolated to find a solution of order
kq, that isytτ <dig> τq

the first column of the table contains the initial numerical approximations. these
are then extrapolated to find the next column, and so on. for instance, with three
initial solutions ytτ,ytτ/ <dig> ytτ/ <dig>  then ytτ,τ/4=43ytτ/ <dig> τ/4−13ytτ,τ/ <dig>  for ytτ/ <dig>  then one similar to eq.  for ytτ/ <dig> τ/ <dig>  and once more for ytτ,τ/4). at each subsequent column, the next leading error term is
cancelled, giving a yet higher-order solution. the correct coefficients to calculate
each new term of the neville table can be found from 

 ytτq−r,τq=pkqytτq−r+ <dig> τq−ytτq−r,τq−1pkq− <dig>  

 where p=τ
q−r
/τ
q−r+ <dig> and k
q
 is the order of the solution at column q ), and
r= <dig> …,q− <dig>  this can be generalised to any
order method with any appropriate error expansion. the only condition for
extrapolation is existence of an error expansion of the form in eq. .

bulirsch-stoer method
the bulirsch-stoer method is an accurate ode solver based on richardson extrapolation  <cit> . a neville table is built by repeated extrapolation of a set of initial
approximations with stepsizes that are different subintervals of a larger overall
step τ, and is then used to find a very accurate solution. this happens
inside each timestep, allowing τ to be varied between
steps. a modified midpoint method  is used to generate the initial
approximations in the first column of the table. this lends itself well to an
extrapolation framework, as the mmp subdivides each step τ into n^ substeps τ^=τ/n^. furthermore, crucially, the error expansion of the mmp contains
only even powers of τ^, resulting in fast convergence  <cit> .   

we give a brief overview of the deterministic bulirsch-stoer method here; ref.  <cit>  has an excellent description of the algorithm, as well as a guide to its
implementation. at each step, a column of the neville table, k, in which we
expect the approximate solutions to have converged, as well as a stepsize
τ are selected. the neville table is then built up by running
k mmps, with stepsizes τ^1=τ/ <dig> …,τ^q=τ/nq, where n
q
=2q,q= <dig> ,…,k and successively extrapolating
the appropriate numerical approximations. the convergence of the solutions is
evaluated based on the internal consistency of the neville table, that is, the
difference between the most accurate solution in column k and that in column
k−1: from table  <dig>  this is Δy=yττ^ <dig> τ^k−yττ^ <dig> τ^k. as successive initial approximations yττ^q are added to the first column, the extrapolated results in each new
column converge to the true solution and
Δy shrinks. the final
approximation at column k is acceptable if err
k
≤ <dig>  where err
k
 is a scaled version of Δy
. if
err
k
> <dig>  the step is rejected and redone with τ=τ <dig> 

in a practical implementation, the initial step tests over
q= <dig> …,k
m
a
x
, where k
m
a
x
 is usually set as eight, in order to establish the k necessary to
achieve the required accuracy and ensure the stepsize is reasonable; subsequent steps
then test for convergence only in columns k− <dig> k and
k+ <dig>  <cit> . because of its accuracy, the steps taken by the bulirsch-stoer method can
be relatively large compared to other numerical solvers. τ is changed
adaptively at each step, and is chosen to minimise the amount of work done  per unit stepsize. in this way the bulirsch-stoer
method adapts its order and stepsize to maximise both accuracy and computational
efficiency.

stochastic bulirsch-stoer method
the sbs method is based on its deterministic counterpart described in the previous
section. there are some key issues that must be addressed in order to successfully
adapt it into a stochastic method. the two most important ones are interlinked:
first, what quantity should be calculated at each step, and second, how can
stochasticity be introduced into the picture? the deterministic bulirsch-stoer method
calculates x
n+ <dig> from x
n
 using the mmp to find the intermediate stages over
[t,t+τ). however, stochasticity cannot simply be
added to this scheme either inside or outside the mmp, as this would interfere with
the extrapolation necessary for the neville table. in order to update the state
vector as in eq. , we must find the number of reactions per step.

looking at the update formula for the trajectory of a jump markov process  <cit> , 

  xn+1=xn+∑j=1mνjp∫tntn+τaj)dt, 

it is clear that the quantity we must calculate is ∫tntn+τa)dt, in order to then take a poisson sample for the update )τ). thus, rather than calculating x
n+ <dig> directly using the mmp, we need an accurate way to find the
integral of the propensity functions over each step. proceeding in a somewhat similar
way to algorithm  <dig>  we arrive at algorithm 4: the intermediate stages are found using
the mmp, and the propensities calculated at each stage. these intermediate
propensities are then fed into a composite trapezoidal method to give an accurate
estimate of the integral.

an important point is that the intermediate stages are solved using the reaction rate
equations , which give the expectation of the
stochastic trajectory over each step provided both are started in state x
n
 at time t
n
. thus we find the expected∫tntn+τa)dt using romberg integration, and use this to sample a poisson
distribution in order to increment x
n
. this method is both extremely accurate at finding the mean and fully
stochastic, that is each simulation gives a different stochastic realisation and the
full probability density can be found from a histogram of many simulations.   

we have now arrived at the implementation of the sbs. first, we calculate Δaτ^q, the expected integral of the propensities over [t
n
,t
n
+τ), using algorithm  <dig> with multiple stepsizes τ^ <dig> τ^ <dig> …. we then extrapolate these using the neville  table to
arrive at the extrapolated solutions Δa
e
x
t
r
; this is known as romberg integration. once these are
sufficiently accurate, we sample the number of reactions as 

  xn+1=xn+∑j=1mνjpΔajextr. 

this is our approximation to the underlying probability density function at each
step. combined with the extrapolation mechanism described previously and a way to
adapt the stepsize, we have the full sbs method.

the stepsize is chosen by calculating the quantity 

  τk=τs1s2errk12+ <dig>  

where τ is current timestep, τ
k
 is the hypothetical next timestep for romberg table column k and
s <dig> and s <dig> are safety parameters, introduced in the next paragraph. here err
k
 is the local error relative to a mixed tolerance, with order o+1) . its ideal
value is exactly one: if it is any smaller than this the step could have been made
bigger, and if it is any larger it means our error bound is exceeded and the step
must be redone using a smaller τ. at each step, a candidate next
timestep is selected for each neville table column k. as we know some
measure of the work done for each k , we can calculate the efficiency of each of the k candidate
timesteps as the work per unit τ. we then select the candidate
τ
k
 that gives the highest efficiency. for brevity, we have left the full
description and step-by-step implementation of the sbs  until the
appendix.

the sbs uses several different parameters , all of which have some
effect on the results. s <dig> and s <dig> are both safety factors that resize the next timestep by some amount:
the smaller they are, the smaller the timestep and the more accurate the solution. as
always, however, there is a compromise between stepsize and speed, so one must be
careful to optimise the parameters for maximum efficiency. the same is also true for
the vectors a
t
o
l
, the absolute error tolerance, and r
t
o
l
, the relative error tolerance. these are used to scale the error that is
calculated from the internal consistency of the romberg table. they are usually set
fairly low: around 10− <dig> is common. there is an additional
consideration with the sbs, namely that of the column of convergence, k.
even when the safety factors are set high , the sbs can
achieve very high accuracy by simply doing another extrapolation, and going to a
higher column. for this reason, the relationship between the safety factors and
accuracy is not a direct one, and it is advisable to check the timesteps and column
of convergence for each new set of parameters.

extension: sbs-da
there is an alternative scheme to eq.  for finding the stochastic update to the
state vector: this is the ‘degree of advancement’, or da approach, and we
call the resulting method the sbs-da. its focus is the m× <dig> random
process z
j
,j= <dig> …,m, the number of times that each
reaction occurs over [ <dig> t)  <cit> . z
j
 is related to the state vector x by 

 x=x+∑j=1mνjzj. 

 in fact, x is uniquely determined by 
z
, where 
z
 is an m× <dig> vector  <cit> . this allows us to use the da approach to calculate the number of
reactions per step, then return to the population approach to update the state
vector, using 

  x=x+∑j=1mνjzj), 

where we define z
j
) as the number of reactions occurring
over [t,t+τ). notice that eq.  has the same form
as eqs.  and . in fact, k
j
=z
j
): in the case of the ssa, the timestep
tends to be very small and only one reaction occurs, but for the τ-leap
and sbs it is much larger so more reactions can occur. similarly to eq. , we know
that  <cit> 

  kj=p∫tt+τaj)ds. 

in order to find the update of the state vector, we must solve for the mean  of k
j
, and sample according to eq. . the equations for the evolution of the mean
and variance of k
j
,μ
j
 and v
j
, respectively ), can be derived from its master equation,
and take the form  <cit>   

  dμjds=∑j′=1mfjj′μj′+aj,μj= <dig>  

  dvjds=2fjjvj+dμjds,vj= <dig>  

where s∈[t,t+τ), x
n
 is the value of the state vector at the start of the step and fjj′=∑i=1n∂aj∂xiνij′,j,j′= <dig> …,m are the elements of an m×m matrix . eqs.  and
 must be solved simultaneously with initial conditions μ
j
=v
j
= <dig> to find μ
j
 and v
j
. it should be noted that they are only exact for
systems with linear propensities. in the case of non-linear propensities, the moment
equations contain higher moments and we obtain eqs.  and  by a standard
closure argument: we taylor expand the propensities and truncate at first-order  <cit> . in fact, eq.  is only necessary because eq.  is not exact in the
general case. for larger timesteps this may lead to a sizeable error, so we must
approximate the true, poisson, distribution of k
j
 with a gaussian whose variance has been corrected. this leads to the update
scheme 

 kj,vjextr)=p)ifμjextr< <dig> nμjextr,vjextrifμjextr≥ <dig>  

which now replaces eq. . here ⌊ ⌉ denote rounding to the nearest
integer and the value ten has been chosen heuristically as above this value a poisson
sample can be well represented by a gaussian sample with the appropriate mean and
variance.

this approach is somewhat similar to the unbiased τ-leap  <cit> . the key difference is that ref. <cit>  uses this scheme in the context of a fixed-stepsize τ-leap
method, basing the entire method around this scheme. in contrast, the sbs-da is
grounded in the bulirsch-stoer method, which it uses for its stepsize selection and
combination of mmp and richardson extrapolation to find the poisson increment. the da
approach is only one part of the whole sbs-da, and is only used as an alternative to
eq. , in order to also find the variance of the number of reactions occurring over
each step.

the sbs and sbs-da methods both calculate the parameter of the poisson sample but
they take different approaches to this. the two key differences are that  the
sbs-da attempts to correct using the variance of the sampled distribution in order to
better approximate the true poisson parameter when the stepsize is large, but  it
sacrifices some of its performance because of the inherent inaccuracies of eqs. 
and  .

RESULTS
to illustrate their effectiveness, we apply the sbs and sbs-da methods to four example
problems of varying degrees of complexity. we compare them with the popular benchmark of
the euler τ-leap method  <cit> , and we also selected two newer methods that are intended to be
representative of the most current, fastest and most accurate methods. these are the
θ-trapezoidal τ-leap   <cit> , which has two stages and weak order two, and the unbiased
τ-leap   <cit> , which accurately estimates the mean and variance of the number of reactions
that occur during one step. although the authors of these methods have used fixed
stepsizes in their works, we have implemented their methods using the same
τ-adapting scheme as the euler τ-leap. this actually
makes them more advanced than originally described, but we believe this ensures a fairer
comparison with the sbs.

we use four example problems: a simple chain decay, the michaelis-menten reactions, the
schlögl system and the mutually inhibiting enzyme system. all the methods we tested
have parameters that can be varied: for the sbs methods these are r
t
o
l
,a
t
o
l
,s <dig> and s <dig>  and for the τ-leap methods it is ε . in the former case, we
chose to focus our attention on r
t
o
l
, as it plays a somewhat similar role to ε in the latter ones, i.e.
as a relative bound for the errors. for each system, we produced a plot of the
‘histogram error’  versus runtime for several values of
r
t
o
l
 and ε. we only varied these single parameters, listed in
table 2: the other parameters of the sbs were chosen to
maximise the overlap between the runtimes of the five methods, and kept constant. this
was solely to facilitate comparison between the different methods, and these values do
not necessarily fall in the normally useful ranges of those parameters. in order to
discriminate between the methods, the plots could be used to choose a cpu time and check
which method has the lowest error at that point, or to find which method takes less time
to run for a set error level.
order from fastest to slowest 

the same number of simulations were run with all methods. we plotted probability density
functions  for each species and compared them to ones obtained from a reference
set of  <dig> ssa simulations, all generated from histograms using identical
bins. we defined the histogram error as the l <dig> distance between the probabilities of each method and the ssa in each bin.
the runtime is the time taken to run a single simulation, obtained by dividing the total
runtime by the number of simulations.

we show the probability distributions of all the simulation methods, as well as plots of
histogram error versus  runtime. we refer to the latter as
‘efficiency’ plots, as they clearly indicate some measure of computational
efficiency. if a method is both fast and has low error, it is efficient: its points are
concentrated towards the origin. in contrast, points to the top right indicate low
efficiency .

chain decay system
we start with a simple test system that has linear propensity functions ∝x). the system has three species that are converted
into each other by the reactions 

 x1→c1x <dig> c1= <dig> x2→c2x <dig> c2= <dig>  

the simulations were started in initial state x= <cit> 
t
 and simulation time was t= <dig>  we ran 5×105
simulations. the sbs safety factors were s1= <dig> ,s2= <dig> , those for the sbs-da were s1= <dig> ,s2= <dig> , and a
t
o
l
=10− <dig> for both. probability distributions of the simulation
results are shown in figure 2a for x <dig>  for clarity, the figure shows only the results for the most and least
accurate parameter values. the ubtl and sbs methods’ pdfs both match the ssa
very closely; the other methods are less accurate. this is quantified in
figure 2b: the ubtl returns the lowest errors, followed
by the sbs-da and sbs. this is not surprising: for linear systems, the ubtl  are exact. for this system, taking into account all three chemical species,
the sbs methods and the ubtl are the most efficient . we have included the efficiency plots for all species in the
additional file, and we have defined a quantity to estimate the total measure of
efficiency across all species; these are described in the further comparisons
section.
from 5× <dig> simulations. only the pdfs of the most and least
accurate error parameters are shown.  histogram error of each method
as compared to the pdf of x <dig> simulated with the ssa.
parameters varied are listed in table  <dig> 
each test system 

histogram bin sizes for each species are listed in order.

michaelis-menten system
this is a well-known system in biochemistry and is often used to test computational
simulations. it is a model of an enzyme  catalysing the production of some molecule . it consists of four chemical species undergoing the reactions 

 x1+x2→c1x <dig> c1=10− <dig> x3→c2x1+x <dig> c2= <dig> ,x3→c3x2+x <dig> c3= <dig> . 

the initial state was x=  <cit> 
t
 and the simulation time was t= <dig>  we ran  <dig> simulations.
the sbs safety factors were set as s1=s2= <dig> , and those of the sbs-da as s1=s2= <dig> , with a
t
o
l
=10− <dig> for both. the pdfs and efficiency plot for x <dig> are shown in figure  <dig>  the sbs, sbs-da and
tttl all achieve high accuracy. the tttl becomes more accurate than the sbs methods
at longer runtimes, but the sbs methods have the advantage at shorter runtimes. thus
when it is important to minimise runtime, the sbs methods are preferable. overall,
the sbs-da has the highest efficiency, with the tttl second and sbs a close third
.
generated from  <dig> simulations, and  histogram error of
each method as compared to the pdf of x <dig> simulated with the
ssa. parameters varied are listed in table  <dig> 

schlögl system
the schlögl system is useful as a test system that is both bimodal and
non-linear, while at the same time being very simple. it is bimodal in species
x with a high and a low stable state, although this is only the case for
certain parameter combinations. it consists of the four reactions 

 a+2x→c13x,c1=3×10− <dig> x→c2a+2x,c2=10− <dig> b→c3x,c3=10− <dig> x→c4b,c4= <dig> , 

and species a and b are held constant at  <dig> and
2× <dig> units respectively. we used the initial condition
x= <dig>  which is an intermediate value between the two stable states. the
simulation time was t= <dig>  and we ran  <dig> simulations for each
method. the sbs safety factors were s1=s2= <dig> , those for the sbs-da as s1=s2= <dig> , and a
t
o
l
=10− <dig> for both.

the pdfs and efficiencies of each method are shown in figure  <dig> for x <dig>  for this system, the tl is surprisingly accurate compared to the other
methods. the sbs and tttl have approximately the same efficiency as the tl, with the
sbs-da being somewhat less efficient and the ubtl the least .
from  <dig> simulations, and  histogram error of each method
as compared to the pdf of x <dig> simulated with the ssa.
parameters varied are listed in table  <dig> 

mutually inhibiting enzymes system
this system has  <dig> chemical species and  <dig> reactions  <cit> . it represents the interactions of two enzymes, e
a
 and e
b
, and their products, a and b, respectively. each enzyme
reacts with some substrate  to create its
product. these products then go on to inhibit the other enzyme. thus, if initially
there are more e
a
 or a, this reduces the chances of b being produced, and vice
versa. this makes the system bistable in the products. this system is a good example
of the double-negative feedback mechanism that is very common in cell biology. here,
however, we use a parameter set that does not result in bistability. the reactions
are 

 ea→c1ea+a,c1= <dig> eb→c2eb+b,c2= <dig> ea+b⇌c4c3eab,c3=5×10− <dig> c4= <dig> eab+b⇌c6c5eab <dig> c5=10− <dig> c6= <dig> a→c7∅,c7= <dig> eb+a⇌c9c8eba,c8=5×10− <dig> c9= <dig> eba+a⇌c11c10eba <dig> c10=10− <dig> c11= <dig> b→c12∅,c12= <dig>  

the initial state was set to x = [ <dig>   <dig>   <dig>   <dig>   <dig>   <dig> 
 <dig>  500]
t
, where x=[a,b,e
a
,e
b
,e
a
b,e
a
b <dig> e
b
a,e
b
a2]
t
, and the system was simulated 2× <dig> times for time
t= <dig>  we used safety factors of s1=s2= <dig>  for the sbs and s1= <dig> ,s2= <dig>  for the sbs-da, with a
t
o
l
=10− <dig>  the pdfs and efficiencies for x <dig> are shown in figure 5; again the tl is
unexpectedly efficient, with only the sbs and sbs-da more efficient overall . at the longest runtimes, both the tttl and tl
are more accurate than the sbs-da and similar to the sbs. however, as runtime is
decreased, the sbs remains very accurate whilst the tttl and tl quickly lose
accuracy, and for shorter runtimes the sbs-da is also more accurate than them
. taking into consideration all eight
species, it is, in fact, the sbs-da that is most efficient, followed by the sbs
.
x <dig> generated from 2× <dig> simulations,
 histogram error of each method as compared to the pdf of
x <dig> simulated with the ssa. parameters varied are listed
in table  <dig> 

further comparisons
all of our test systems have more than one species, and so far we have only presented
results for x <dig>  this can often be unrepresentative of the full picture. the chain decay
system is a clear example of this. additional file 1:
figure a <dig> shows the efficiency plots for all three species. only looking at
x <dig> could lead one to think that the ubtl is the most efficient method for
simulating this system. but including the other two species reveals that the sbs-da
is, in fact, the most efficient overall. this is important, because it is clear that
factors such as linear/non-linear propensities, population size and stiffness all
affect each reaction and species in a different way. thus it is overall
performance we are interested in.

to overcome this problem, we use a way of quantifying the overall efficiency of each
method over all species. this follows directly on from our previous definition of
efficiency: low error and low runtime implies an efficient method, high error and
high runtime implies an inefficient method, and a combination of the two, for
instance high error but low runtime, clearly lies somewhere between the two. we
define ‘efficiency’ η as 

  η=)−1sum. 

this varies for each system, and is comparable only when the bins used to
calculate errors are identical. in other words, the values are a direct comparison of
the efficiency of each method for each test system, but should not be used across
different test systems. table  <dig> compares the efficiencies
of each simulation method for every test system.

additional file 1: figures a <dig> to a <dig> contain a full picture
of our computational results for all four example systems and all simulation methods.
the overarching trend was the following: the sbs was very accurate, returning the
lowest error in many cases. the tl was unexpectedly efficient for some systems. the
tttl also achieved good efficiency for longer runtimes, but the sbs had a flatter
efficiency curve than the tttl, with the tttl quickly losing accuracy at lower
runtimes even when it was more accurate than the sbs at higher runtimes. a clear
trend emerges: the sbs is a very accurate method. moreover, it is the most
efficient method we tested, maintaining its accuracy better at low
runtimes than the other methods.

sbs methods excel when we want a short runtime with high accuracy. in this case, we
set the safety factors high, allowing large steps and a corresponding increase in
extrapolations. this retains a high accuracy, whilst reducing runtime because of the
large timesteps. in contrast, when we allow a longer runtime, we set the safety
factors low, restricting the stepsize and removing the need for higher extrapolation.
in many of our test examples, we have seen the sbs only using one extrapolation
throughout the simulation. this is a waste of the extrapolation capability of the
sbs, and it is no surprise that in these cases it is not the most efficient method,
especially as the stepsize adaptation scheme adds some overhead to each step.

higher order of accuracy and robustness
a thorough study of the order properties of τ-leaping methods was first
given by rathinam et al. <cit> , who showed that for linear reactions the euler τ-leap
method is weak order one in the moments under the scaling τ→ <dig> 
this analysis was extended by li  <cit>  to non-linear propensity functions by considering sdes driven by poisson
random measures . li showed that the euler τ-leap method is precisely the
euler method applied to this sde and hence inherits the properties of strong order
half and weak order one. however, there are issues with using the scaling condition
τ→ <dig> as the τ-leap condition requires that ∑ajτ≫ <dig>  anderson et al. <cit>  overcame this scaling condition by considering order under a large volume
scaling Ω→∞. in this case, by letting x/Ω=o and with τ=Ω−β
,0<β< <dig>  global strong and weak order convergence can be
established. hu et al. <cit>  investigate these issues in greater detail through the use of rooted tree
expansions of the local truncation errors for the moments and covariance, thus
generalising the approach first applied to sdes by burrage and burrage  <cit> . this analysis shows that while some τ-leap methods may have
higher order moments , their covariance is invariably of unit order, unless
this is specially taken into consideration . as hu et al. <cit>  point out, these issues arise as a consequence of the differences between
the infinitesimal generators for deterministic odes and jump processes.

it is well-known that the bulirsch-stoer method has a high order of accuracy: this is
the reason it is able to use large steps whilst still finding very accurate
solutions. this is because of the richardson extrapolation that is used at each step
on the mmp solutions . in
contrast, rather than the mmp solutions for x, the sbs instead
extrapolates at each step the deterministic quantities Δaτ^q  and  in the case of the sbs-da), calculated using the
composite trapezoidal rule, which also has a known error expansion. thus the
extrapolation is performed on a deterministic variable: the mean of the poisson
update.

we investigated the behaviour of the sbs and sbs-da methods on two simple systems:
first, the linear system x→2x, x= <dig>  and
second, the non-linear system x+y→∅,
x=y= <dig>  as the sbs changes both stepsize and romberg
table column k  adaptively, we used a restricted
version, which had both a fixed stepsize and k. we ran simulations with both
sbs and sbs-da, for k= <dig> and k= <dig>  that is no extrapolation and one
extrapolation, respectively. in addition, we also used the tl and tttl methods for
comparison, as they have known weak orders of accuracy .
the gradients of the errors for the different methods are computed based on a linear
least-squares regression of the data points.

we find that both sbs and sbs-da can have high weak order in the mean . for the
linear system, both methods have weak order approximately two and four in the mean
for k= <dig> and k= <dig>  respectively . however, there is a difference in behaviour between the sbs and sbs-da
for the non-linear system . here, the sbs-da is
limited to at most weak order two in the mean, even when extrapolated. in contrast,
the sbs is limited to at most weak order four when extrapolated. this shows the
limitations of eqs.  and : for non-linear systems, they limit the order of
accuracy of the mean of the sbs-da to two. this is not the case for linear systems,
as here eq.  is exact, so the order of the sbs and sbs-da is identical.the clear
message we can take from figures  <dig> and  <dig> is that the sbs does behave as if it had higher weak order in the
moments, and this order increases as the romberg table column  is increased. however, we cannot tell whether this trend continues to
higher extrapolations as these are so accurate that monte carlo error interferes with
our ability to reveal the weak order. on the other hand, the sbs-da behaves as if it
has at most weak order two in the mean for non-linear systems, but this restriction
in weak order is compensated for by the use of an appropriate gaussian sample when
the poisson parameter is large, and generally it is similarly, or even more,
efficient than the sbs. however, in neither case does extrapolation increase the weak
order of the variance beyond one.thus one can legitimately ask whether our approach
offers any advantage over, for example, the tttl method, which has weak order two in
both the moments and the variance. this can be addressed by perusal of
figures  <dig>   <dig>   <dig> and  <dig>  where we compare the distances of the pdfs
of the numerical methods and the exact solution  as a
function of the runtime. of the methods tested the sbs appears to be the most robust
and efficient, even though the tttl has weak order two in the variance. it is the
criteria of efficiency and robustness that are the most important properties of any
good numerical method. we claim that these properties are intrinsic to the sbs along
with its ability to adaptively select the timestep and number of extrapolations to
carry out, thus maximising efficiency whilst keeping accuracy high.
 mean, and  variance of the linear system
x→2x, with c= <dig> , x= <dig> 
t= <dig>  the gradients of linear regression lines fitted to the points
are shown in the legend.
 mean, and  variance of the non-linear system
x+y→∅, with
c=10− <dig> 
x= <cit> t,t= <dig>  the
gradients of linear regression lines fitted to the points are shown in the
legend. the first three points of the sbs with k= <dig> are omitted from
the regression line, as they are clearly affected by monte carlo error.

now we discuss the order of accuracy behaviour of our methods. first of all, we are
not claiming that they have high weak order uniformly in the stepsize, and we have no
such proof apart from in the linear case when the higher order is inherited directly
from the underlying deterministic extrapolation methods. however, this statement
gives us a key insight into considering the behaviour of numerical methods when
applied to sdes with small noise of the form 

  dx=f)dt+εg)dw,x=x <dig>  

where ε> <dig> is a small-noise term. it is well-known that langevin sdes
represent an intermediate regime between discrete stochastic chemical kinetics and
the deterministic regime, arising as the number of molecules x in the system
increases. in particular, ε behaves as 1x <cit> . for such systems, milstein and tretyakov  <cit>  showed that the global weak order of numerical methods to solve the above
sde has the general form o, where q<p. when noise is ignored
, the sde becomes an ode and the weak order of its approximate
solution is just the deterministic order term o. milstein and tretyakov  <cit>  also performed an analysis in the strong sense, and again found the
general form of the global strong error to be o,q<p. in addition, buckwar et al. <cit>  also examined small-noise sdes in a strong sense for some well-known
classes of runge-kutta methods. the implication of the extra term in the stochastic
order is that although the underlying deterministic order of the method may be high,
the stochastic order is restricted by the noise term. however, when the noise is
small, this term will also become small, thus allowing the stochastic order to
increase, possibly even up to the deterministic order o. this is also the case if the stepsize is large. in fact, it occurs
over a range of values of τ and ε: it is trivial to see
that the condition for the deterministic term to dominate is τ≫εrp−q.

now, a standard way of mathematically investigating discrete stochastic methods is to
analyse sdes with jumps; this is the approach of li  <cit> . in particular, li  <cit>  shows that the euler discretisation of such an sde is the euler
τ-leap method. our hypothesis is that the analysis of milstein and
tretyakov  <cit>  is also applicable to sdes with jumps; this then tells us something about
the behaviour of discrete stochastic methods when noise levels are medium or small. a
small-noise analysis similar to that of milstein and tretyakov  <cit>  for the sbs is beyond the scope of this paper but we postulate that there
is a small-noise error expansion in both τ and ε for the
sbs method. the sbs is most useful when applied to systems with relatively larger
biochemical populations , as it is in these cases that the ssa is
prohibitively slow and an approximate method is necessary. combined with the fact
that the sbs often uses large stepsizes, this implies that in many systems of
interest, the global weak order of the sbs may, in fact, not be far from its
deterministic  order. this also explains the behaviour of the sbs in our
numerical tests, where the timesteps were large and the populations moderate
, thus making it likely that the condition for the
deterministic order term to dominate was met.

implementation issues
the speed of the sbs is due to the large steps it takes compared to other solvers. we
compare the stepsizes for all five methods we used in this paper on the
michaelis-menten system . clearly, the stepsizes
are influenced by our choice of error parameters. we controlled for this by using
parameters that gave similar  error levels, regardless of
runtime. figure  <dig> shows that the largest steps were taken
by the sbs methods and the ubtl, with the stepsizes being very similar, followed by
the other two methods. this is not very surprising: the ubtl is in some respects
similar to the sbs-da, in that it also finds very accurate solutions for the moments
of k
j
 at each step. however, the stepsize is controlled using a completely different
mechanism, so it is interesting to see that both employ a similar stepsize for a
similar error level in this case.
stochastic solvers we have compared in the case of the michaelis-menten system.
the largest steps are taken by the sbs, sbs-da and unbiased
τ-leap, then the θ-trapezoidal
τ-leap, and finally the euler τ-leap. parameters
used were:
s1=s2= <dig> ,atol=10− <dig> rtol=10−4
for the sbs methods, and ε= <dig> , <dig> , <dig> for the euler,
θ-trapezoidal and unbiased τ-leap methods,
respectively.

one peculiarity of the sbs is that it can settle into one of several different
‘regimes’: because it builds the romberg table adaptively, it can achieve
the same accuracy using a larger step and higher extrapolation  or smaller step and lower extrapolation. the regime into which the
particular simulation falls is strongly influenced by the initial stepsize
τ, but often changes mid-simulation. for instance, a smaller
τ is more likely to fall into the smaller- τ  regime, and vice versa. figure 9
shows how τ changes with time in chain decay system simulations using
several different τ. it is clear that there are two regimes, one
high- τ and one low- τ. when τ is very
small, τ settles down to the low regime, and only the second romberg
table column is used; as τ is increased, τ settles in
the high regime and uses the third column. when τ= <dig>  τ
initially enters an even higher- τ regime using the fourth column, but
eventually settles into the high regime with the third column. in practice, it is
advisable to bear this in mind, and choose τ accordingly: low-
τ, low-column simulations are more computationally expensive and if
the same accuracy can be achieved with a larger timestep then efficiency can be
improved even further.
stepsize using different initial stepsizes τ, and 
the column of the romberg table at which the solutions converge sufficiently
. sbs parameters are
s1=s2= <dig> ,atol=rtol=10− <dig> 
for clarity not every point has been given a marker.

there are two distinct approaches to determining τ: first, as
described previously, we could set τ to an arbitrary value and run
the initial step through as many columns as necessary  until it finds the required accuracy. should τ be so large
that it drives the populations negative, it would also be reduced here until it
reaches a more suitable size for the given problem. in addition, if
τ is still larger than its optimum value, it is reduced over the
next several steps until it has reached this optimum value . this is the standard approach for the deterministic bulirsch-stoer
method, and it is the one we have taken in our simulations. however, in the
stochastic regime there is another approach: we could set τ as some
multiple of 1/a <dig> , along with an initial guess of the romberg table column to aim for. as
1/a <dig> is very small, this is a more conservative approach, but
τ is increased to its optimum value over the first few steps. it
could be useful for systems that are very stiff, or that oscillate, whose timestep
must be very small at certain parts of the solution domain and larger timesteps could
result in large errors. there seems to be no substantial difference in accuracy
between the two approaches, and we believe both are equally valid.

CONCLUSIONS
our results have shown that the sbs is generally a very accurate method, at least
comparable to or, in most cases, better than its competitors. however, the real strength
of the sbs is this accuracy combined with the fact that its efficiency curve
has a relatively low gradient; in other words, it is an accurate method that loses
little of its accuracy as it is speeded up, allowing for fast, robust and accurate
simulations. this is because as runtime is shortened, the sbs uses more and more
extrapolations to maintain its accuracy. at the same time, the use of larger timesteps
means less overhead overall, allowing the sbs to be very efficient. it is in such
parameter regimes that the sbs can achieve its full potential. in addition to this, we
believe the sbs is also able to achieve high weak order  in the small-to-moderate noise regime, that is when the number of molecules
in the system is moderate to large, and also when the timesteps are large compared to
the noise level. its performance in this regime is accelerated as more and more
extrapolations are performed, giving it exceptional accuracy.

as the sbs is an explicit method, it is not necessarily suited for solving especially
stiff problems. in such cases, runge-kutta methods with larger regions of stability,
such as the stochastic runge-kutta method  <cit> , are more ideal, as well as implicit or multiscale methods  <cit> . the initial stepsize of the sbs should be chosen appropriately, as it may be
possible for the sbs to settle in a higher-stepsize regime, which could affect accuracy,
or a low-stepsize regime, which could affect runtime. in addition, τ
should be chosen such that it is within the stability region of the modified midpoint
method. running a few preliminary simulations can help choose τ.

in previous work, we have extended richardson extrapolation into the discrete stochastic
regime  <cit> . in this framework, full simulations with fixed stepsize are run over
t=, and their moments are extrapolated to find
accurate approximations to the moments at time t. in contrast, the sbs uses
extrapolation within each timestep and varies τ to optimise
efficiency. thus the sbs is a complementary approach to extrapolated
τ-leap methods that has two advantages: first, the stepsize can be adapted
to lower runtime and eliminate the need for finding a suitable range of fixed stepsizes;
second, the sbs returns an entire histogram, rather than just the moments. this can be
desirable in many cases, especially if the solutions do not follow a simple distribution
such as a gaussian or poisson, or have multiple stable states.

in this paper we have introduced a new efficient and robust simulation method, the
stochastic bulirsch-stoer method, which can also achieve higher weak order in the
moments for certain systems. this is inspired by the deterministic method of the same
name, and as such it also boasts the two main advantages of that method: its speed and
its high accuracy. we have shown using numerical simulations that for a range of example
problems, it is generally the most efficient and robust out of all recent
τ-leap methods that we tested, which are the current state-of-the-art
in fast stochastic simulation. thus the sbs is a promising new method to address the
need for fast and efficient discrete stochastic methods.

appendix: stochastic bulirsch-stoer full algorithm
here we explain in detail the stochastic bulirsch-stoer method. the aim of the stepsize
adapting mechanism of the sbs  is
to select the optimal column k of the romberg table  that will give an acceptably low error while requiring as little
computational work as possible. we define the error of each romberg column q as 

  errq=μττ^1τ^q−μττ^2τ^qatol+rtol×μττ^1τ^q, 

where μ
τ
≡μ
j
 is the mean of k
j
 as defined in eq. , which is equivalent to Δaτ^ from algorithm  <dig>  and |
v
| denotes the l <dig> norm of the vector 
v
. the most ideal situation is if the error of the k-th column, err
k
=1: if it is larger than one, accuracy has been lost because τ was
too large; if it is smaller than one, computational time has been lost because
τ was unnecessarily small. below, we follow refs.  <cit>  in our exposition. an idea of how τ can be adjusted to its
optimal value for the next step is given by 

 τq=τs1s2errq12+ <dig> q= <dig> …,k, 

 where τ
q
 is a set of hypothetical new stepsizes adjusted from the current stepsize
τ. s <dig> and s <dig> are safety factors 0<s <dig> s2< <dig>  that ensure τ is not set too large because of errors in
the mmp and composite trapezoidal rule approximations.

we want the column that minimises the work done per unit step. this is defined for
column q as 

 wq=aqτq,q= <dig> ,…, 

 where a
q
 is the work done in computing the q-th romberg table row and is assumed
to be the number of function evaluations inside the mmp. an mmp with stepsize τ^=τ/ <dig> needs three evaluations, i.e. a1= <dig> using our scheme; this can be generalised to 

 aq+1=aq+nq+ <dig>  

 where n
q
=2q. the optimal column k for the next timestep is given by the
lowest w
q
, and the optimal stepsize by the corresponding τ
q
. in reality, after the initial step only columns k− <dig>  k
and k+ <dig> are tested for convergence, as otherwise the convergence is likely to
be an artifact or the timestep is far off its optimal size. this helps reduce the
runtime but makes the implementation more complicated.

now that the reasoning behind the adaptive mechanism is clear, we set out a detailed
algorithm for a practical implementation of the stochastic bulirsch-stoer method. to
implement the sbs-da instead, algorithm  <dig> should be replaced with algorithm  <dig>  which
would calculate the mean and variance of k
j
 according to eqs.  and . in addition, there should be two neville tables,
one for the mean and one for the variance, which find the extrapolated solutions to
each.   

abbreviations
ssa: stochastic simulation algorithm; sbs: stochastic bulirsch-stoer; ode: ordinary
differential equation; mmp: modified midpoint method; da: degree of advancement; pdf:
probability density function; tl:  τ-leap; tttl:
θ-trapezoidal τ-leap; ubtl: unbiased
τ-leap.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
mb conceived the sbs idea and supervised the work. tsz and mb jointly developed the
method and analysed the results. tsz implemented the method, performed the simulations
and drafted the manuscript. kb and kcz helped with the analysis and provided theoretical
insight. all authors took part in revising the manuscript, and all authors have read and
approved the final manuscript.

supplementary material
additional file 1
supplementary information. these show full sets of simulation 
