BACKGROUND
dna microarray technology is now playing an increasingly important role in biomedical research. microarray technology gives one the opportunity to measure gene expression levels of thousands to tens of thousands of genes simultaneously, in order to study the differential gene expression pattern between different developmental stages, diseases states and samples treated with drugs or other compounds. before comparing data from different arrays to address these biological questions, however, a "much more mundane but indispensable " normalization step  <cit>  is currently used in most microarray analyses.

because of the slight difference in rna quantities, imaging settings and other variables, even in very controlled experiments the intensity levels from different arrays are of different scales and need to be normalized before they can be compared with each other. various normalization methods have been developed and some are widely used. the simplest method is total intensity based normalization  <cit> ; this approach scales intensity levels of every gene by a constant factor so that total intensities of all the arrays are the same. "spiked-in" based normalization methods scale intensity based on spiked-in standards  <cit> . nonlinear normalization methods use local regression to scale intensities to compensate for the intensity-dependent differences between arrays  <cit> .

for most current applications, these normalization methods seem to be adequate. however, the residual left by a less than perfect normalization procedure is another source of non-biological variation that is usually non-desirable, especially when the differences in expression levels are expected to be small  <cit> . in addition, if the goal is meta-analysis of multiple sets of microarray data  <cit>  systematic differences between experiments may result in a normalization artifact. we were therefore interested in developing an approach to analyse microarray data without first performing a normalization step. our approach was partly inspired by non-parametric statistical methods  <cit> . for example, nonparametric methods that use ranks  <cit>  to compare microarray results, in addition to being distribution free, have the additional advantage of being normalization free.

dna microarray technology has been used widely in biomedical studies. one interesting application is in the area of molecular classification; one popular use is in the comparison of tumor samples. since clinical and histopathological classification is sometimes difficult and labor-intensive, the use of genome wide expression patterns to classify tumor samples has recently become a very active research area  <cit> . although some tumors appear to be amenable to classification using microarray data  <cit> , general multiple tumor classification using microarray data has proved to be an interesting and challenging task for several reasons: the general difficulties inherent in multi-class classification problems, the small number of samples available, and the inherent biological variation between specimens, etc. we decided to use multi-class tumor classification as a test case to illustrate the power of our approach. we compared our results for a multi-class tumor classification problem with more conventional approaches published by ramaswamy et al.  <cit>  and yeang ch et al.  <cit> . these authors compared the accuracies of using k-nearest neighbors , weighted voting  and support vector machine  algorithms in a multi-class tumor classification problem and concluded that svm is a more powerful machine learning algorithm for this application.

RESULTS
normalization free approach to microarray data analysis
generally, measurements on single microarrays give a real-valued intensity level xi  for each gene i on the array, where n is the total number of genes on the array. without first doing some type of normalization, the intensity level of gene i from array a, xia, cannot be directly compared with the intensity level of gene i from array b, xib. in this study, we sought an alternative quantity or quantities that can be directly compared between different arrays without compromising important biological information. one obvious candidate is ri, the rank of intensity level of gene i on the array. however, we felt that rank is not an adequate measure because information about relative expression level is not represented explicitly. instead, we decided to use the following measures.

let

sij =  <dig> if

 <dig> otherwise     

, where 1<= i, j <= n. basically, sij is the sign of the intensity difference of gene i and j on a single microarray and therefore will remain unchanged under any monotonic transformation of x. therefore, instead of computing with the absolute expression level of a gene, its relative level to all the other genes on the microarray is used. for each gene i, instead of one real valued xi, the approach uses si = , a binary vector of size n. for ease of reference, we will simply refer to this value as the sed  of gene i; and the entire matrix  the sed of the array. given , sij is simply and uniquely defined but  does not uniquely determine xi so some information is lost by only using  instead of .

since ri = Σj =  <dig> ...,n sij is the rank of gene i in terms of intensity levels, rank information is preserved in . what is lost in the transformation from  ->  is just the intensity differences between the closest ranked genes, which in most cases are small, considering that microarray data are generally considered "very noisy". it was our major goal to demonstrate that  has indeed captured important components of the information from .

instead of directly using the intensity levels x, and its derivatives such as the mean μ, the standard deviation σ and the signal to noise ratio  between two sample groups a and b, , we will use  to compare gene expression differences between arrays. since we expect measurement variations within an array will be less than those between arrays and we take the signs of relative expression differences to get sed, we expect the sed will be less "noisy". however, the value of any single sij may still vary between technical and biological replicates. one would expect more sij would change values randomly if the technical replicates was done on arrays that were fabricated in a different run than arrays from same run, for example. biological variations are expected to be even more frequent. however, we hypothesize that we can perform statistical analysis on the sed, which contains tens of thousands of sij for a single gene, and minimize the impact of such noises.

we will also consider , a natural generalization of the sed concept. here, spij is the probability of xi > xj. in other words, imagining one can get a large number, n, of either technical or biological replicates of the sample of interest, then spij = m/n as n ->  <dig>  where m = Σk =  <dig> ...,n sijk and sk is for replicate k. we will call  =  the sed probabilities of gene i. note that in calculating both sed and sed probabilities, only intensity comparisons within arrays are involved and therefore forego the normalization step.

for example, if gene i is more highly expressed in sample a than b we would expect that more sija than sijb would be  <dig> instead of  <dig> and the overall  spija would be larger than spijb. since rank can be calculated from sed, any rank based method can be expanded to use sed. a gene i's sed can be viewed in two different perspectives. on one hand, it provides information about gene i's expression level relative to every other gene on the array, and therefore can be used to examine gene i's expression patterns between samples. on the other hand, it also provides information about the expression levels of all the other genes on the array, using the gene i as a control, in essence. therefore, sed can be used to study questions either at the gene level or at the array level. in this paper, we focus on solving a simpler problem at the array level where it is not necessary to decide whether the expression level of an individual gene is increased or decreased between array a and b and by how much. rather, it is focused on whether the overall expression patterns are different at all between array a and b.

multi-class classification of tumor samples
to test whether  and  extracts most of the information from , we used these values in a test case of a multi class classification problem described by ramaswamy et al. and yeang et al  <cit> . two algorithms were used to classify each of the  <dig> tumor samples into one of  <dig> tumor classes. one is the naive bayes  classifier  <cit>  using sed probabilities. the other is the nearest neighbor  classifier using sed. in the nb method, to classify a sample t, we first calculate  for each class c  using training samples, i.e. the  <dig> samples with t taken away . then sample t is classified according to:

score= Σi =  <dig> ...n Σj =  <dig> ...,n log,     

where pij = spijc if sijt = 1

 <dig> - spijc otherwise

t is simply classified to the class c that has the maximum score.

in the nn method, we compute instead, for each training sample t,

matches = Σi =  <dig> ...n Σj =  <dig> ...,n δ,     

where δ =  <dig> if x = y.

 <dig> otherwise

then t is classified to class c of the sample t that has the maximum matches.

if one is to give a statistical interpretation of these scores, one can simply view  as defining a multi-binomial probability model. in addition, one could consider each sij as a draw from a binomial distribution with probability pij = spij. then, the score is simply a logarithm of the probability p to get all the sij exactly the same as sijt under the probability model c where pij = spijc.  for difference class c, in theory, should not be directly compared. instead, a p value should be calculated from the probability model for pr)) and used to evaluate the closeness of sample t to each class c. for simplicity, we are not considering such issues here.)

when the nb algorithm was applied to the  <dig> samples, the accuracy obtained was about 63%; the nn algorithm performed slightly better and gave an accuracy of 70%.

feature selection
depending on the algorithm, a better classification result can sometimes be obtained by using a subset of genes  <cit> . we were interested to know whether feature selection helps to increase accuracy in our approach. within our framework, it is easier to treat  pairs as selection units. we therefore filtered out  pairs where the variance of spij across the  <dig> tumor classes was less than a pre-determined value and left the rest of the algorithm unchanged. we reasoned that the filtered-out part of the matrix has less discriminating power across the tumor spectrum and might add noise due to the small sample sizes used. using the nb algorithm, the best result achieved was 70% with a cutoff σ <dig> =  <dig> , while the nn method with σ <dig> =  <dig>  gave 77%. table  <dig> lists the accuracies achieved using different feature cutoffs.

"single gene's sed" based classifier
the above procedure utilized the entire sed matrix as a classifier. in other words, all the relations between genes were considered in the classification. to determine whether the inclusion of the whole matrix was actually required to achieve the current accuracy, we investigated the efficacy of using single gene's sed as classifiers.

in these cases, we define a classifier based on gene i and its relative expression to every other gene. therefore, the scorei = Σj =  <dig> ...,n log, where pij is as same as mentioned previously. similarly, matchesi = Σj =  <dig> ...,n δ defines a classifier based on gene i's sed. fig.  <dig> shows a display of the cumulative frequency of single gene sed based classifiers versus accuracy for all the  <dig> classifiers. in general, single gene-based classifiers performed worse than the whole genome-based classifiers, as expected. nevertheless, most of the classifiers performed reasonably well, compared with just using single gene expression levels. about 80% of the single gene based classifiers resulted in an accuracy between 40% and 60%, while about half of the classifiers had an accuracy greater than 50%. these results suggest that there is a lot of redundant information in the seds and sed probabilities and that our method should be reasonably robust. we then investigated the number of genes that are required to achieve the current accuracy. fig.  <dig> shows the combined results for classifiers using only a subset of genes. our results suggest that a subset of genes  is sufficient for predictions and that the prediction accuracy is stable after  <dig> genes.

different classification accuracy between tumor classes
from the analyses described above, we noticed that there was a significant difference in accuracy between different tumor classes. for  <dig> classes  we obtained either 100% or close to 100% accuracy . since these happen to correspond to the  <dig> classes with more than double the number of training samples than the other classes, we tested whether this high accuracy is due to the larger sample sizes by using only  <dig> training samples for every tumor classes. the results were essentially the same, indicating that sample size is not the issue. on the other hand, there are classes where we obtained very poor results; these often happen to be the same classes where svm in  <cit>  performed poorly as well .

we were interested in exploring the possible reasons for misclassification. in fig.  <dig>  a scatterplot of the sed match scores  between  <dig> ov samples and  <dig> cns samples is displayed. the ov and cns class were selected since one is "very hard" and the other is "very easy" to classify. without trying to be statistically correct, the plot does suggest that samples of the ov class are in general "farther away" from each other, compared with those from the cns class. as this may be one of the reasons that the ov class is harder to classify, algorithms that take this kind of information into account may perform better than the simple ones we have presented here.

discussion
although we used the multi-class tumor classification problem as our test case, our major goal was to illustrate the feasibility of the normalization free sed approach, and not in sample classification per se. therefore, we chose the algorithms nb and nn for their simplicity and not for their performance in solving this specific problem. the performance of a classifier depends, in this case, mainly on the power of its algorithm, and the data representation it used. from a machine learning perspective, one can simply view the intensity -> sed transformation as a change in data representation, a mapping from the gene's attribute, intensity x, to some features sed. it was our goal to demonstrate that the new features , in addition to being normalization free, still convey the essential information in the original attribute, the intensity x.

since the data representation is quite different between the intensity x  and sed , it is difficult to directly compare the two. no obvious yet non-trivial algorithms work with both representations; even if there were such an algorithm it is not clear that it would be the right one to use for comparison as it might well be the case that different data representation works best with different algorithms. here, we have limited the presentation to some empirical results with sed representation, which are comparable with results using several different algorithms that are based directly on raw intensity  <cit> . our classification results are close to those obtained with wv and knn methods, which are based on directly focusing on intensity levels. previous results using svm were significantly better, but we feel the differences are due more to the power of the algorithm  <cit>  than the way information is coded. in fact, slightly more accurate results are obtained with modification of algorithms that directly manipulate intensity levels  <cit> . we do not imply that the algorithms  we chose are better than other alternatives . instead, we fully expect more sophisticated algorithms would work better with the sed approach as well.

certainly, sed probability is more information rich than sed. we expect that an sed probability based analysis would perform better than the simple binary valued sed. in this paper, we mainly tested the sed. sed probability is only used for a group of samples, not for single samples. if one limits oneself to use only raw data, then for single arrays one can only get sed. however, if some assumptions about the patterns of gene expression levels can be made, one can certainly get an estimation of sed probability even for a single array. for example, as in some nonlinear normalization algorithms, if one assumes that the variation of expression levels are similar for genes with similar expression levels, then one can estimate sed probability from a probability model. also, the magnitude of the intensity difference can also be used to help such an estimation. alternatively, as more and more microarray data become available, one can use other similar samples to get an estimation of a prior sed probability, and then use a bayesian approach to estimate the sample's sed probability.

the obvious disadvantage of our sed based approach is that for each gene expression level, one is not dealing with a single real number but instead a vector of size n, where n is in the tens of thousands. this could significantly increase both computing time and memory requirement  on the other hand, it also has certain advantages: 1) it is free of normalization noise. since it is generally believed that biological variation is larger than technical variation and normalization noise is just another source of technical variation, the benefit here is only of a limited scope. however, it may be important when the expression level difference one is interested in is small. 2) in addition to being normalization free, sed and sed probability also have the advantage in being distribution free, and therefore could perform better if the intensity levels were non-normal. 3) sed and sed probability are easier to interpret. sed values can easily be checked against raw intensity levels according to eq. . while sed probability is one step further away from intensity levels, one could still have an intuitive sense of it and make comparisons between different experiments. it would be much harder to have a real grasp of the absolute gene expression level, except that it is "high" or "low" or somewhere "in between"; it is certainly harder to compare between experiments intuitively.

we have only tested the sed approach on datasets that are from the same chip format. data from different chip formats or complete different technology platforms, of course, would be harder to compare. but they are also challenge for normalization based method. it would certainly be interesting to compare sed and normalization based method under these more challenging conditions.

if this normalization-free approach  proves to retain the essential biological information in general, its application may be extended to meta-analysis where different datasets could be integrated and intervalidated. the method could also be used when the number of arrays is a limiting factor for experiments. for example, one could take advantage of the massive amount of public array data, obtain prior distribution of sed probabilities from datasets with similar conditions, and analyze new data within a bayesian framework. if the performance of the nearest neighbor method in general is anywhere close to what we demonstrated in the multi tumor classifications here  and fig.  <dig>  the nearest neighbor method, without feature selection at least, allows direct sample vs. sample comparison. note also that the samples in the multi tumor problems are from different biological specimens, therefore, large between-sample-variation is expected), it might be used as a microarray database query method, i.e., to find similar microarray results in the database that are "similar" to one's own, independent of array annotations.

it might also be worth noting that the sed approach could easily be applied to other kinds of comparative data analysis for samples with very large numbers of "noisy" attributes. the sed approach may also perform better when between-sample-variation is large, especially if such variation contains some rather uninteresting technical measurement errors that would not affect within-sample-variations.

CONCLUSIONS
we have proposed a new approach to analyze microarray data and tested the method on a set of publicly available datasets. the results were comparable to those obtained with some widely used normalization based algorithms. we hope that we have demonstrated that this normalization free method is feasible and promising. we think the sed based, normalization free approach could be used to complement the more popular normalization based approaches in microarray data analysis.

