BACKGROUND
protein structure prediction by comparative modeling and/or fold recognition consists of three largely independent steps:  postulating the structural similarity of the target protein sequence with a known template structure on the basis of a significant alignment score between the two protein sequences.  this or a different alignment serves as a basis for model construction. in this process residues in the target sequence that are aligned to residues in the template structure are mapped on the corresponding coordinates in the structure.  finally, unmapped regions are filled in, breaks in the backbone are mended, and the overall model is refined.

thus the quality of the alignment in the second step has an essential impact on the quality of the resulting model. the continual benchmarks in the biannual casp assessment of protein structure prediction methods witness that there is significant progress in identifying suitable templates  <cit> , due in part to the introduction of profile-profile alignment methods  <cit>  and the sophisticated construction of profiles  <cit> . while casp assessors found little improvement in the predicted models  <cit> , they found steady progress in alignment quality over the years  <cit> .

the optimal alignment resulting from an algorithm with a specific optimized parameter setting is not always the best choice for model creation. jaroszewski et al. have set up a computational experiment in which they sample a huge conformational space  of alternative alignments by combining an approach of varying parameters  with an iterative approach of penalizing previously visited regions of the sample space  <cit> . the study states that there exist alignments surpassing the original alignments in quality for about 50% of the protein pairs. contreras-moreira and coworkers  <cit>  as well as john and sali  <cit>  propose genetic algorithms for constructing a large number of alternative alignments by recombining an initial set of alignments. a common problem of these approaches is the selection of the alignment allowing for the construction of the final model.

recently, a lot of effort has gone into the development of model quality assessment programs   <cit> . mqaps are computer programs that receive as input a 3d model of a protein structure and produce as output a real number representing the quality of the model  <cit> . we will refer to this number as the model score. in contrast to model evaluation programs, like gdt  <cit> , maxsub  <cit> , or tm-score  <cit> , which assess the quality of the model by comparing it to the native structure, mqaps do not compare to the native structure. instead, they estimate the quality of a proposed model without knowledge of the native structure. unlike scoring functions in sequence-to-structure alignment and to physical energy functions, mqaps operate on an intermediate level – they are more flexible than a sequence-to-structure alignment function as the dynamic programming paradigm used in alignment computation imposes the requirement of prefix optimality which is not required in mqaps. mqaps aim at scoring the quality of predicted models. typically, mqaps use one or more different statistical potentials, representing information coded in protein structures  <cit> . different mqaps were recently tested in cafasp- <dig> as meta-selectors for pinpointing high quality models from the ensemble of models proposed by different automated servers  <cit>  proving that mqaps are highly effective selectors.

 <dig> 
RESULTS
 <dig>  overview of protocol and evaluation
in this manuscript we propose and validate a protocol for improving alignments in step  of comparative modeling or fold recognition. optimization is achieved by generating alternative alignment-based models for a target sequence and selecting the most promising model using an mqap.

ensembles of alternative alignments are generated with the state-of-the-art profile-profile alignment method arby  <cit>  by varying parameters. apart from the arby default, we suggest two different procedures for generating alternative alignments: pvs varies the parameters in the profile-profile alignment method slightly, whereas pvh varies the parameters heavily. each procedure reports an ensemble of distinct alignments. for each alignment a model is constructed .

the ensembles of alternative models typically contain models with higher quality as well as models with lower quality than the standard arby model. the frst  <cit>  mqap program is applied to scoring the quality of the models. by choosing the model with the best model score according to the frst potential, we can select a promising model for each target. these selected models are potentially improved with respect to the arby default model. additionally, we developed an svm-based selection mechanism. a support vector machine  is trained on the model scores and on the frst partial potentials for recognizing the models with increased quality.

the performance of the protocol is evaluated by comparing the chosen models to the previously withheld native structures. the comparison is performed with the model evaluation program tm-score  <cit> , its score reflecting the "real" quality of the models. the tm-score always lies in the interval . for each target t we computed the arby default model d and exercised the two model generation procedures pvs and pvh resulting in two ensembles of models epvs and epvh per target. summary statistics of the number of models per target are given in table  <dig> 

 <dig>  evaluation of model generation: quality of generated models
first, we analyze the quality of the model generation procedures. the key ideas are to count per target the number of models with increased quality, and to measure the average difference of model quality with respect to the default model in terms of tm-score.

 <dig> . <dig> analysis per target
for a target t, we denote the quality of a model ml by tm, where greater tm-score is better. the relative frequency of models per target with a quality measure above the arby default is defined as

fpte,>=1|e|∑ml∈e,
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgmbgzcqwgwbaccqwg0baddawgaawcbagaemyraukaeiilawiaeyopa4dabeaakiabcicaoiabdsha0jabcmcapiabg2da9maalaaabagaegymaedabagaeiifawnaemyraukaeiikagiaemidaqnaeiykakiaeiifawhaamaaqafabagaei4waslaemivaqlaemyta0kaeiikagiaemyba0maemibawmaeiykakiaeyopa4jaemivaqlaemyta0kaeiikagiaemizaqmaeiikagiaemidaqnaeiykakiaeiykakiaeiyxa0laeiilawcaleaacqwgtbqbcqwgsbabcqghiiizcqwgfbqrcqggoaakcqwg0badcqggpaqkaeqaniabgghildaaaa@5eb8@

where d is the default arby model, e is an ensemble of models for the target, and  is the iverson bracket defined for arbitrary propositions x as

={1if x is true0else.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqggbbwwcqwg4baecqggdbqxcqgh9aqpdagabaqaauaabmqaciaaaeaacqaixaqmaeaacqqgpbqacqqgmbgzcqqggaaicqwg4baecqqggaaicqqgpbqacqqgzbwccqqggaaicqqg0badcqqgybgccqqg1bqdcqqglbqzaeaacqaiwaamaeaacqqglbqzcqqgsbabcqqgzbwccqqglbqzaagaeiola4cacagl7baaaaa@49e3@

similarly, we consider the relative frequency fpte,< of models with a quality below that of the arby default models.

the average within an ensemble e of quality improvement of a model over the default arby model is

qire=1|e|∑ml∈e−tm)).
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgxbqccqwgpbqacqwgybgcdawgaawcbagaemyraueabeaakiabcicaoiabdsha0jabcmcapiabg2da9maalaaabagaegymaedabagaeiifawnaemyraukaeiikagiaemidaqnaeiykakiaeiifawhaamaaqafabagaeiikagiaemivaqlaemyta0kaeiikagiaemyba0maemibawmaeiykakiaeyoei0iaemivaqlaemyta0kaeiikagiaemizaqmaeiikagiaemidaqnaeiykakiaeiykakiaeiykakcaleaacqwgtbqbcqwgsbabcqghiiizcqwgfbqrcqggoaakcqwg0badcqggpaqkaeqaniabgghildgccqgguaglaaa@5bf9@

we define an indicator function whether a better model for a target t exists in the ensemble e.

fbe = 

and compute the quality improvement that is theoretically possible

qibe=max⁡ml∈etm−tm).
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgxbqccqwgpbqacqwgibgydawgaawcbagaemyraueabeaakiabcicaoiabdsha0jabcmcapiabg2da9uaabmqabiaaaeaadawfqaqaaigbc2gatjabcggahjabciha4bwcbagaemyba0maemibawmaeyici4saemyraukaeiikagiaemidaqnaeiykakcabeaaaoqaaiabdsfaujabd2eanjabcicaoiabd2gatjabdygasjabcmcapiabgkhitiabdsfaujabd2eanjabcicaoiabdsgakjabcicaoiabdsha0jabcmcapiabcmcapaaacqgguaglaaa@543c@

 <dig> . <dig> performance over all targets
the frequency fpt was defined per target and its average over all targets is fpt¯=1n∑fpt
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdagamjabdchawjabdsha0baacqgh9aqpdawcaaqaaiabigdaxaqaaiabd6gaubaadaaeabqaaiabdagamjabdchawjabdsha0bwcbeqab0gaeyyeiuoaaaa@3a9a@. while fpt describes the frequency of better models per target, fb¯=1n∑fb
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdagamjabdkgaibaacqgh9aqpdawcaaqaaiabigdaxaqaaiabd6gaubaadaaeabqaaiabdagamjabdkgaibwcbeqab0gaeyyeiuoaaaa@3780@ reflects the fraction of targets that have a model with a quality above the arby default within the ensemble of constructed models.

when selecting models randomly, an average quality improvement of qir¯=1n∑qir
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdghaxjabdmgapjabdkhaybaacqgh9aqpdawcaaqaaiabigdaxaqaaiabd6gaubaadaaeabqaaiabdghaxjabdmgapjabdkhaybwcbeqab0gaeyyeiuoaaaa@3aa2@ is obtained. when selecting models optimally, an average quality improvement of qib¯=1n∑qib
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdghaxjabdmgapjabdkgaibaacqgh9aqpdawcaaqaaiabigdaxaqaaiabd6gaubaadaaeabqaaiabdghaxjabdmgapjabdkgaibwcbeqab0gaeyyeiuoaaaa@3a62@ is obtained, imposing a theoretical upper bound to what is feasible with mqap selection on the alignments generated as proposed. for the two procedures pvs and pvh generating alignment-based models these numbers are listed in table  <dig> 

in order to visualize the distributions of model quality, in figure  <dig> for each target t the tm-score of the default arby model s plotted versus the tm-score improvements of the models constructed for that target. the scatter plots in figure  <dig> along with table  <dig> clearly indicate that better models are generated for a large fraction of the targets. summing up, the above-mentioned procedures generate models with a better quality than the arby default models, but identification of the improved models among the generated models is a hard task as analyzed in the next section.

 <dig>  evaluation of model selection
in the following, we analyze how well the model selection procedure works on the models generated with procedures pvs and pvh. the key ideas are to count for how many targets an improved model is selected, and to measure the quality improvement with respect to the default model in terms of tm-score. we perform the analysis for the selection based on the frst potential and then repeat it analogously for the svm based selection.

 <dig> . <dig> analysis per target
identification of the best model per target can be performed based on the frst mqap scores. for each target t, we select the model s with the lowest estimated frst energy

se,frst=arg⁡min⁡ml∈efrst,
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgzbwcdawgaawcbagaemyraukaeiilawiaemozaymaemocainaem4camnaemidaqhabeaakiabcicaoiabdsha0jabcmcapiabg2da9uaabmqabiaaaeaadawfqaqaaigbcggahjabckhayjabceganjgbc2gatjabcmgapjabc6gaubwcbagaemyba0maemibawmaeyici4saemyraukaeiikagiaemidaqnaeiykakcabeaaaoqaaiabdagamjabdkhayjabdohazjabdsha0jabcicaoiabd2gatjabdygasjabcmcapaaacqggsaalaaa@5607@

since lower frst is better. in the supplementary material  we analyze the frst partial potentials in more detail.

in order to count the number of occurrences in which this is an improvement of model quality measured in tm-score, we define the indicator functions fim as follows:

fime,frst,> = 

fime,frst,= = 

fime,frst,< = 

these functions indicate whether the model selected by the mqap is of higher, equal, or lower quality than the arby default.

while fim serves to count the number of targets which improve, we use the measure qim to quantify the improvement of model quality with respect to the default arby model:

qime,frst = tm) - tm).

 <dig> . <dig> performance over all targets
across all targets, fim¯>=1n∑fim>
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdagamjabdmgapjabd2gatbaadawgaawcbagaeyopa4dabeaakiabg2da9maalaaabagaegymaedabagaemoba4gaamaaqaeabagaemozaymaemyaakmaemyba02aasbaasqaaiabg6da+aqabaaabeqab0gaeyyeiuoaaaa@3cc9@ is the fraction of targets whose models improve when choosing models using the frst mqap. we measure the average improvement in model quality as qim¯=1n∑qim
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdghaxjabdmgapjabd2gatbaacqgh9aqpdawcaaqaaiabigdaxaqaaiabd6gaubaadaaeabqaaiabdghaxjabdmgapjabd2gatbwcbeqab0gaeyyeiuoaaaa@3a8e@.

a summary of the results when selecting models according to the frst potential is given in table  <dig> 

 <dig> . <dig> spotting candidate targets with estimated improvement
both model generation procedures pvs and pvh include the arby default model in the ensemble of generated models. therefore, for any target, model selection will only pick an alternative model, if a model with a score better than the arby default exists. an indicator for this is

nie,frst = .

the set of targets for which model selection proposes candidates with estimated improvement consists of n·ni¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabd6gaujabdmgapbaaaaa@2f7d@ = ∑ni targets. on this candidate set, we denote the average improvement in model quality as qim¯=1n⋅ni¯∑tqim
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaadaaqaaiabdghaxjabdmgapjabd2gatbaacqgh9aqpdawcaaqaaiabigdaxaqaaiabd6gaujabgwsixpaanaaabagaemoba4maemyaakgaaaaadaaeqaqaaiabdghaxjabdmgapjabd2gatjabcicaoiabdsha0jabcmcapawcbagaemidaqhabeqdcqghris5aaaa@440b@.

 <dig> . <dig> significance and coverage
the fim> and qim numbers exhibit a noticeable increase in model quality with respect to random selection of models .

more importantly, comparing models resulting from the selection process to the arby default by applying a paired wilcoxon signed rank test, we find for model generation procedure pvs that the models selected according to frst are significantly better than the arby default . for the model generation procedure pvh, the models selected with frst alone are neither significantly better nor worse than the default, demonstrating that it is hard to select better models when generating more low-quality models.

selection of models constructed with model generation procedure pvs results in an average quality improvement of qim¯epvs,frst
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaadaaqaaiabdghaxjabdmgapjabd2gatbaadawgaawcbagaemyrau0aasbaawqaaiabdcfaqjabdafawjabdofatjabcycasiabdagamjabdkhayjabdohazjabdsha0bqabaaaleqaaaaa@3c6b@ =  <dig>  and works better than selection of models constructed with model generation procedure pvh with an average quality improvement of qim¯epvh,frst
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaadaaqaaiabdghaxjabdmgapjabd2gatbaadawgaawcbagaemyrau0aasbaawqaaiabdcfaqjabdafawjabdieaijabcycasiabdagamjabdkhayjabdohazjabdsha0bqabaaaleqaaaaa@3c55@ =  <dig> .

for generating procedure pvs, the selection according to frst suggests an alternative model for 51% of the targets; 53% of these suggested targets are improved according to tm-score. for generating procedure pvh, alternative models are suggested for 70% of the targets; 48% of these suggested targets are improved according to tm-score.

 <dig> . <dig> selection of high quality models using an svm-based selection
based on the frst scores, an svm was trained to choose high quality models as described in the methods section. the values fimsvm, qimsvm, and nisvm are calculated analogously to the previously defined fimfrst, qimfrst, and nifrst, by replacing frst in these formulas with the negative svm decision values.

 <dig> . <dig> significance and coverage of the svm selection
the results produced with selecting models according to the svm decision values are summarized in table  <dig>  for pvs, an overview is given in figure  <dig> 

compared to the frst potentials, the ni¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabd6gaujabdmgapbaaaaa@2f7d@sum are smaller . the svm more effectively avoids changing models for the worse. this is visible in figure  <dig> and also reflected by noticeably smaller average numbers of models with decreased quality . the overall average improvement in tm-score model quality  increases.

applying a paired wilcoxon signed rank test, we find for both generation procedures that the models selected by the svm are significantly improved with respect to the arby default . the svm selected models are also significantly improved with respect to the frst selection process .

for generating procedure pvs, the svm-based selection suggests an alternative model for 40% of the targets; 64% of these suggested targets are improved according to tm-score. for generating procedure pvh, alternative models are suggested for 58% of the targets; 61% of these suggested targets are improved according to tm-score.

 <dig> discussion
jaroszewski et al.  <cit>  show their method to produce significantly better alignments in about half of test cases, for a benchmark set of  <dig> protein pairs and make no statement regarding the likelihood of selecting such a solution from the ensemble of alternative alignments generated. to this end, they generate an average of  <dig> alignments per target-template pair with improved solutions in 34% of the test cases . our method is able to generate improved alignments for 59% of the test cases  with only  <dig> alignments on average. the 55-fold decrease in the number of evaluated alignments compared to the method of jaroszewski et al., while maintaining at least comparable increments in alignment quality, implies that we are exploring regions in the space of alignments that are densely populated with high-quality solutions, making the method practical for improving fully automated fold recognition servers such as arby  <cit> . this is important when comparing our method to other approaches like robetta  <cit> , or the work of john and sali  <cit>  or contreras-moreira and coworkers  <cit> , where improved model generation requires several orders of magnitude more alignments to be evaluated.

improved alignments have to be selected from the ensemble of alternatives with an mqap program in order to be useful. neither jaroszewski  <cit>  nor chivian  <cit>  make quantitative statements about the selection of improved solutions. the results of john and sali or in silico recombination are not directly comparable, as they generate and select the solutions iteratively. our data show that the selection of improved alignments is a difficult task. a random selector would actually deteriorate the overall performance of the method. generating more models does not necessarily help the selection process. especially if more models below default quality are generated , avoiding to select worse models is more difficult. thus the error rate can increase and the overall performance can decrease if more low-quality models are generated.

here we show that the proposed protocol, including model generation and svm-based selection, significantly improves model quality . with the model generation procedure pvs, using the svm-based selection the proposed method achieves a close to optimal average tm-score improvement of  <dig>  and a maximal observed increase in tm-quality of max qim =  <dig>  this has to be related to typical fold recognition targets, where the tm-scores for large portions of the predictions lie in the range of  <dig>  to  <dig>  for hard targets and from  <dig>  to  <dig>  for easier targets.

to emphasize the relevance of the proposed method in practical use, we compare the quality improvement of the proposed protocol with the quality gain obtained by loop modeling alone. the average quality increase in tm-score incurred by using our protocol amounts to  <dig> , which is a factor of five above the quality gain obtained by loop modeling alone  <dig> . the quality increase for our protocol is computed as the difference between the tm-scores of the mqap selection from varied models with modeled loops minus the arby default with loops modeled. the model quality obtained by loop modeling alone is computed as the difference of the tm-score of the arby default with loops modeled minus that of the arby default without loops modeled .

 <dig> 
CONCLUSIONS
we have presented an approach for improving structure prediction models that goes in a different direction from the one recently proposed by pettitt et al.  <cit> . whereas they have evaluated the possibility to choose better templates with an mqap program, we show that it is possible to generate and select better alignments for a fixed template with an mqap program. the two approaches can be combined and will improve automated servers such as arby.

as this seems a promising approach in a competitive field, we will continue to work on the topic in two directions: first, generation of models with a high likelihood of improving the quality and second improving the selection process. for the latter, the numbers on the svm performance clearly indicate that the current linear combination of the partial potentials in frst can be improved.

 <dig> methods
 <dig>  protocol
 <dig> . <dig> alternative alignments and models
the 3d protein structure model that we construct for a target protein is based on an alignment with a template structure. the method described here is independent of the strategy for template identification. with a given target and template as input, we compute a default alignment using profile-profile alignment with log-average scoring and parameters as tuned for the arby server  <cit> . namely these parameters are: substitution matrix blosum <dig>  gap insertion  <dig> , gap extension  <dig> , and a relative weight of secondary structure to sequence information of  <dig> .

in addition to the arby default alignment, we propose two procedures for generating alternative alignments for a target in analogy to the parametric approach of jaroszewski et al.  <cit> . the alternatives are computed by a global profile-profile alignment method, using parameters multiplied with a factor varied inside the range from a lower to an upper bound. the parameters varied are gap-insertion, gap-extension, and the relative weights of amino-acid and secondary structure profiles. the two procedures differ with respect to the ranges of the factors. each procedure reports alignments that occur multiply for different parameter settings only one time, resulting in an ensemble of distinct alignments.

for each alignment a model is built as follows. loop modeling of insertions and deletions is performed, using the lobo program  <cit> . conserved  residues and their side chains are copied from the template structure. the non-conserved residues and their side chains are positioned and optimized by scwrl <dig>   <cit> .

 <dig> . <dig> model scores
the quality of the model is then estimated using the frst mqap program  <cit> , which computes four potentials, namely a residue-specific all-atom distance potential  <cit>  , a solvation potential , a hydrogen bonding potential , and a torsion angle potential . these four potentials are linearly combined into the frst energy score . this leaves us with the frst score as an estimate of the quality of each constructed model.

we can select the best alignment-based model for each target, by choosing the model with the lowest energy score according to the frst potential. these selected models are potentially improvements over the default model . in the supplement we additionally analyze selection according to the partial contributions rapdf, solv, hydb, and tors of the frst potential . the frst mqap program places a strong emphasis on the torsion angle component  <cit> . since each residue can either increase or decrease the overall score, there is no correlation between the number of gaps in a model and the overall score.

for 95% of the targets in the benchmark set of this paper, the frst mqap can distinguish the native structures from the arby default models. similarly, the performance of frst on selecting the native structure from the models generated with procedures pvs and pvh is 95% and 94%, respectively.

 <dig> . <dig> model quality evaluation
if, additionally, the native structure of the target is known, using the model evaluation programs gdt  <cit> , maxsub  <cit> , and tm-score  <cit> , we can compute scores , reflecting the "real" quality of the model in terms of structural similarity between model structure and target structure.

in general the quality measures gdt, ms, and tm correlate well: the correlation coefficients between quality measures for all models produced are corgdt,ms =  <dig> , corgdt,tm =  <dig> , and corms,tm =  <dig>  . overall the analysis yields similar results for all three quality measures. as the tm-score has the advantage of being independent of the size of the protein, we restrict our presentation to the analysis of the tm-score.

overall, a moderate negative correlation cortm,frst = - <dig>  of the quality measure tm-score with the frst score can be observed. it has to be pointed out that the correlation of the frst score across all targets is not as relevant as its selection capabilities per target.

 <dig> . <dig> combining mqap partial potentials using a support vector machine
we train a support vector machine  for selecting models with higher tm-score than the tm-score of the default model. the binary labels used for each model are tm-score-increase and tm-score-decrease with respect to the default arby model. as features we use the frst, rapdf, solv, hydb, tors values of each model and the corresponding default model as well as the differences of these scores between model and default. for each target, the best model is selected based on the svm decision value  <cit> . models with a negative svm decision value remain unchanged with respect to the arby default. as svm implementation, the r package e <dig>  <cit>  based on libsvm  <cit>  is employed. as parameter tuning showed only negligible changes in classification accuracy, standard parameters and a radial basis function kernel are used.

 <dig>  benchmarking
 <dig> . <dig> dataset of targets and templates
for the validation of our approach, the improvement of the proposed models over the default arby models was evaluated. target sequences were taken from a representative set of scop  <dig>  domains  <cit>  with at most 40% sequence identity as provided by the astral compendium  <cit> . as a basis for the alternative models, in this study, one template was chosen for each target: with log-average scoring and default parameters as listed in table  <dig> <cit> , the target was compared against the rest of the domains in the astral 40% set and the top ranking hit was chosen as template. our analysis was restricted to targets which have a template with at least 25% sequence identity, evaluating the proposed method for targets from the homologous fold recognition category. these criteria specify  <dig> targets, each with one template. for  <dig>  of these  <dig> targets, some of the necessary computations failed. we excluded those targets, which leaves us with n =  <dig> targets, for which we have all relevant scores available.

 <dig> . <dig> cross- validation of svm-based selection
the training and validation of the support vector machine is performed using five-fold cross-validation. in order to ensure that there are no models for the same target in the training as well as in the testing set, during the cross-validation successively models for one fifth of the targets  are removed from the training set and used for testing.

as the pairwise sequence identity between targets is below 40% according to selection criteria it is guaranteed that models in the test and training sets are sufficiently distinct.

in order to assess the effect of the choice of k in k-fold cross-validation, a ten-fold cross-validation was also performed, yielding results identical to one digit precision in table  <dig> .

abbreviations
a <dig> arby default model for targets t

e set of models constructed for targets t according to model generation procedure i ∈ { <dig> ,2}

tm model quality evaluation measure as computed with the tm-score program

gdt model quality evaluation measure as computed with the lga program

ms model quality evaluation measure as computed with the maxsub program

 iverson bracket

x¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdiha4baaaaa@2e36@ average of x over all targets

xaverage of x over targets suggested by the selection procedure

fpte,> relative fequency of models per target with tm-score above arby default

qire quality improvement when choosing randomly

fbe indicator whether better model exists per target

fb¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdagamjabdkgaibaaaaa@2f5f@e relative frequency of targets for which a better model exists

qibe quality improvement which is theoretically the best possible

se,frst selected model per target

fime,frst,> indicator whether selected model is improved in real quality

fim¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdagamjabdmgapjabd2gatbaaaaa@30d0@e,frst,> relative frequency of selected models with increased tm-score model quality

qime,frst measure of quality improvement with respect to default arby model

ni¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabd6gaujabdmgapbaaaaa@2f7d@ relative frequency of targets for which a selection procedure suggests improved models

the indicator functions are constructed to count the relative frequencies. they draw their names from the respective relative frequencies.

 <dig> authors' contributions
s.c.e.t, s.t., and i.s. conceived the experiment, i.s. and s.t. performed the experiment, i.s., o.s., and t.l. analyzed the results, and i.s., t.l., and s.c.e.t. wrote the final manuscript, which all authors have approved.

supplementary material
additional file 1
additional statistical analysis of partial potentials and of models with and without loop modeling. some additional material, analysising partial potentials and analyzing the behaviour of the protocol when using models with our without loop modeling performed.

click here for file

  <dig> acknowledgements
i.s. is funded by dfg . s.c.e.t. is funded by a "rientro dei cervelli" grant from the italian ministry for education, university and research . we thank giorgio valle and alessandro albiero for insightful discussions as well as lars kunert, francisco s. domingues, and jörg rahnenführer for valuable comments on the manuscript. this research was performed in the context of the biosapiens network of excellence .

figures and tables
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdagamjabdchawjabdsha0baaaaa@30ec@> are the relative frequencies of models per target with a tm-score below and above arby default, respectively, qir¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdghaxjabdmgapjabdkhaybaaaaa@30f0@ is the improvement in tm-score when choosing models randomly. fb¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdagamjabdkgaibaaaaa@2f5f@ is the relative frequency of targets for which a better model exists, qib¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdghaxjabdmgapjabdkgaibaaaaa@30d0@ is the best theoretically possible improvement for the given ensemble of models.

 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdagamjabdmgapjabd2gatbaaaaa@30d0@<, fim¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdagamjabdmgapjabd2gatbaaaaa@30d0@=, and fim¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdagamjabdmgapjabd2gatbaaaaa@30d0@>, are the relative frequencies of selected models with decreased, equal, or increased tm-score quality, respectively. min qim and max qim are the minimal and maximal quality improvements achieved per target. qim¯
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaqdaaqaaiabdghaxjabdmgapjabd2gatbaaaaa@30e6@ is the average quality improvement over all targets. qimis the average quality improvement for the targets that the selection procedure suggests improved models for.

                                 qim
                                 qim
