BACKGROUND
membrane proteins mediate a broad range of fundamental biological processes such as cell signaling, transport of molecules that cannot otherwise cross impermeable cell membranes, cell-cell communication, cell recognition and cell adhesion  <cit> . about  <dig>  of human genes encode for membrane proteins, the sequences of which are relatively easy to obtain. in contrast, of the  <dig>  protein structures deposited to-date in the protein data bank , only  <dig> or  <dig> % correspond to membrane proteins  <cit> . production of membrane protein crystals which is required for determining highly accurate detail of the 3d structure is very difficult or sometimes impossible due to the inherently hydrophobic nature of membrane proteins  <cit> . nmr methods do not apply readily to very large molecules such as transmembrane proteins. knowledge of the transmembrane  segment locations in a membrane protein can narrow down possible tertiary structure conformations for the given protein  <cit>  and aid in prediction of its function. therefore prediction of the structure by computational methods is useful.

labeling data  by experimental or other manual methods is often time consuming and expensive. to characterize data by computational methods, supervised machine learning approaches require a training set which is a representative sample of all the unlabeled data in order to achieve comparable performance on the latter. in practice, data selection for experimental or manual characterization is rarely ever carried out by taking into account the complete space spanned by the unlabeled data. it is useful and efficient to design algorithms that can not only learn from existing training data but can also direct the optimal selection of new data instances for manual labeling. active learning, a type of supervised learning, samples the unlabeled pool of data and selects instances whose labels would prove to be most informative additions to the training set. each time new labelled instances are added to the training set, the classification function is updated. as a consequence of this, the information valuable to the learning function is maximized.

common strategies employed for data selection in active learning  <cit>  are density based, where a set of data points from dense regions are selected for labelling  <cit> ; or uncertainty based, where data points with maximum confusion or uncertainty with current classifier are selected  <cit> ; or representative based, in which data points most representative of the data set are selected  <cit> ; or ensemble based in which multiple criteria are employed  <cit> . many of the active learning approaches combine density-based and uncertainty based strategies to achieve better performance. clustering is commonly applied to select the representative data points  <cit> .

the scarcity of labelled data for tm helix prediction makes it an excellent candidate for active learning. in the case of transmembrane helix prediction, unlabeled data refers to sequences of all the membrane proteins and labeling refers to determination of structural annotations by experimental means. technological improvements over the last decade lead to a rapid increase in biological data including gene sequences from several organisms. one of the major challenges of bioinformatics relates to this flood of raw data. moreover, in some cases such as transmembrane  helix prediction, manual annotation is very difficult or sometimes impossible  <cit> .

early tm helix prediction methods use two fundamental characteristics:  the length of the tm helix being at least  <dig> residues so that it is long enough to cross the  <dig> Å thick lipid bilayer  <cit> , and  the tm residues being hydrophobic for reasons of thermodynamic stability in the hydrophobic membrane environment  <cit> . explicit methods employ a numerical scale to code for the hydrophobic property of amino acids followed by computing average-hydrophobicity in moving windows, to locate long hydrophobic segments  <cit> . other methods treat the  <dig> amino acids as distinct entities, without explicit representation of their similarities, and statistically model their distribution in different topological locations of tm proteins  <cit> . these methods generally use statistical modelling and assume that membrane proteins conform to the commonly observed topology of cytoplasmic-tm-extracellular. drawbacks with these methods have been low accuracy with the windowing methods and over-fitting by statistical methods due to the lack of sufficient training data. recently, ganapathiraju et. al. developed tmpro, a computational method with a radically different method for feature computation that attains a good separation of the tm versus non-tm "windows" and thereby, a high accuracy in tm helix prediction  <cit> .

here, tmpro is employed for transmembrane helix prediction, in conjunction with active learning algorithms to select a minimal set of proteins to train the tmpro statistical models .

methods
datasets
prior to tmpro, tmhmm has been the most widely-used transmembrane helix prediction algorithm. a set of  <dig> proteins was used to train tmhmm  <cit> . in order to compare the performance of tmpro with tmhmm, in  <cit>  the same training dataset of  <dig> proteins has been used to train tmpro. keeping the training dataset the same, evaluations were presented on three datasets: benchmark data, pdb_tm and mptopo. to demonstrate the merits of active learning in comparison to non-active learning based methods, evaluations of tmpro-active are presented in comparison to tmpro without active learning. note that, when all the data in training, namely all of the  <dig> proteins are selected, tmpro-active is the same as tmpro.

the following three datasets of membrane proteins with high resolution structural annotations are used for evaluation:  high resolution set from results reported by the benchmark evaluation by chen et. al  <cit> ,  membrane proteins with high resolution information from the mptopo dataset consisting of  <dig> proteins and  <dig> tm segments in  <cit> ,  pdbtm dataset downloaded in april  <dig> ; it contains all transmembrane proteins with 3d structures from the pdb determined to that date  <cit> . pdbtm provides a non-redundant subset of alpha-helical tm proteins having sequence identity less than 40%. chains corresponding to this non-redundant list were extracted from the complete set, resulting in  <dig> proteins consisting of  <dig> tm segments  <cit> .  <dig> out of  <dig> proteins of pdbtm and  <dig> out of  <dig> proteins of mptopo are redundant with the training set. a 2-class labeling scheme has been employed for all three datasets where each residue is marked as "non-tm"  or "tm".

approach
the proposed approach, tmpro-active, explores the feature space to identify the data points, and thereby the proteins, which are most-representative of the feature space. proteins thus selected are used to train the neural network of the original tmpro algorithm. the steps involved in tmpro-active are as follows :

 to begin with, all proteins are considered to be unlabeled proteins.

 primary sequence of each protein is expanded into five different primary sequences, each coding for one of polarity, charge, aromaticity, size and electronic property.

 features are computed over moving windows of length  <dig> as done for tmpro.

 a self-organizing-map is constructed over the feature space spanned by the proteins.

 active learning algorithm is applied to determine the training set iteratively.

 a neural network  is trained with the selected training set, and the output of the neural network is interpreted as done for tmpro.

feature computation
this process is same as carried for tmpro, but is presented here for completeness of information.

data preprocessing and vector space representation
the primary sequence of each protein is decomposed into five different sequences by replacing each amino acid with its property according to charge, polarity, aromaticity, size and electronic property  <cit> . the protein sequence, of length l, is analyzed with a moving window of length l; the window is moved along the sequence one residue at a time, each position of the window yielding a feature vector. the feature vector at position i, represented by ri, is derived from the window beginning at the ith residue and extending l residues to its right. it is given as   

where cij is the count of property-value j in window i. the specific property-values counted by the cij are as follows:

ci1: count of "charge-positive"

ci2: count of "charge-negative"

ci3: count of "charge-neutral"

ci4: count of "polarity-polar"

ci5: count of "polarity-nonpolar"

ci6: count of "aromaticity-aromatic"

ci7: count of "aromaticity-aliphatic"

ci8: count of "aromaticity-neither"

ci9: count of "electronic property-strong acceptor"

ci10: count of "electronic property-strong donor"

ci11: count of "electronic property-acceptor"

ci12: count of "electronic property-donor"

ci13: count of "electronic property-neutral"

ci14: count of "size-medium"

ci15: count of "size-small"

ci16: count of "size-big"

when feature vectors ri are computed for every position of the window, moving to the right one residue at a time, the entire protein will have a matrix representation p ,   

whose columns are the transpose of feature vectors ri .

singular value decomposition
amino acid properties for feature representation  are mutually dependent. it is therefore desirable to transform these feature vectors into an orthogonal space prior to the use of this data for clustering and features for prediction. to achieve this, protein feature matrices of all the proteins are concatenated to form a large matrix a, and subjected to singular value decomposition    

where u and v are the right and left singular matrices and s is a diagonal matrix whose elements are the square roots of the eigen values of the matrix aat. only the top  <dig> dimensions have been used for feature presentation, since the top  <dig> dimensions of s of training data have been found to carry 85% of the energy   <cit> . the matrices u, s and v are dependent on the matrix a from which they are computed.

therefore, for each new protein, singular value decomposition should ideally be recomputed. however, this would also involve recomputation of all the statistical models built on the features derived through singular value decomposition. to avoid this, the feature vectors along the same principal components can be approximated by multiplication ri with ut similarly as given in equation  <dig>  <cit> .

data clustering
a self-organizing map  is computed as a way of clustering the unlabeled data. it arranges a grid of nodes topologically based on the values of the features of the training data, such that adjacent nodes are more similar to each other. the som grid can efficiently be used to visualize and investigate properties of the data  <cit> . in basic som, the neurons  are located on a low dimensional grid, usually one- or two-dimensional. the basic som construction follows an iterative procedure. each neuron receives the input signal vectors weighted by a vector indicating the degree of closeness of peripheral neurons. for each input vector, the node closest to it is determined via similarity between the input vector and the weights of each node. in the training process, the weights of the winning neuron and its adjacent neurons are updated in terms of distance of each of these neurons from the input vector.

an som has been created with  <dig> nodes arranged in  <dig> ×  <dig> hexagonal lattice grid  <cit> , and is trained on  <dig> random data points . the hexagonal lattice emphasizes the diagonal directions in addition to horizontal and vertical directions . euclidian distance is used to calculate distance between nodes to its neighbours. weights of nodes are used as cluster centroids. the som is then simulated for the entire set features - which amounts to clustering of the data around these  <dig> nodes.

active learning
as discussed earlier, active learning is an approach to minimize the number of labelled proteins that form the training set. the algorithm can actively choose a minimal training set. to explore design choices that affect this desirable behaviour, four different experiments were designed for choosing the training set. we have a pool of  <dig> proteins that are considered to be unlabeled by the algorithm, from which it can select training proteins.

random selection
this method is considered the baseline with which to compare active learning techniques. one protein is selected randomly per iteration and added to the training set.

selection by node-coverage
coverage of the feature space spanned by a protein is indicated by the nodes of the som to which the features are assigned. all proteins in the data set are ranked by the number of nodes they each cover; i.e., by the number of nodes into which their features fall. the protein that covers the largest number of the nodes of the som is selected and added to the training set.

selection by maximal entropy 
initially,  <dig> protein is selected randomly and is added to the training set. subsequent to that, in each iteration, proteins whose feature vectors fall in the nodes with maximum confusion between tm and non-tm labels, or rather nodes with higher confusion rate as determined by their entropy, are selected and added to the training set. the confusion  in labelling the data points can be measured as   

where p <dig> is the fraction of data points labelled tm and p <dig> is the fraction of data points labelled non-tm. by choosing proteins from highest confusion nodes, we assume that the most informative proteins are those that the classifier is most uncertain about.

selection by both node-coverage and confusion-rate
proteins whose labels are asked are selected according to node coverage and confusion rate in an alternating manner in each iteration.

neural networks for feature classification
in each iteration, the neural network classifier is retrained with the updated set of labelled proteins, and prediction performance on test data is evaluated. the neural network is modelled as described before  <cit> . during training, a class label is defined for each window based on the number of tm and non-tm residue labels in the window:

• completely-membrane : if all residues in the window are labeled tm

• completely-nonmembrane : if all residues in the window are labeled non-tm

• mixed : if some residues in the window are labeled tm and some non-tm

the number of input nodes of the nn is  <dig> and the number of output neurons is  <dig>  one hidden layer of  <dig> nodes is placed in between input and output layers. the model is fully connected in the forward direction. each of the hidden and output neurons is a tansig classifier  <cit> . back-propagation procedure  <cit>  is used to train the network by presenting it with feature vectors and their corresponding target output class labels. mixed label feature vectors are not presented for training, since they arise from both tm and non-tm residues and hence are expected to lie in the "confusable" region in the features space. the output neuron learns to fire - <dig> for non-tm features and + <dig> for tm features. a threshold of  <dig>  is chosen for automatic classification of the feature into its class. each input feature vector causes the output neuron to fire an analog value ranging from - <dig>  to + <dig> . a threshold of  <dig>  is used to label the residue at the first position in the window to be tm or non-tm. since the feature is derived over a window of length  <dig>  and threshold of  <dig>  is "more confident" towards the tm label, the  <dig> residues starting from the first position of the window are all set to be of tm type . the process is repeated for the next feature vector, and so on, and a tm label is assigned to  <dig> residues at a time every time the output neuron fires a value greater than the threshold. evaluation is carried out with the same metrics as designed by chen et al in  <cit> . the same metrics were also used in comparing tmpro with tmhmm in  <cit> .

implementation
singular value decomposition of the protein feature matrix is computed using the svds tool in matlab®. the som toolbox of matlab® is used to create soms. training and classification procedures for neural networks are implemented using the neural net toolbox of matlab®.

RESULTS
self organizing map
som is implemented to cluster the unlabeled data. only  <dig> random data points are used while training the som network. after  <dig> random data points, increasing the amount of unlabelled data did not improve clustering efficiency. moreover, by using a small amount of data points, som can be formed as quickly as possible. figure  <dig> shows the resulting som among the spread of a random sample of unlabeled data. not all unlabeled data is shown so as to keep the som nodes visible.

benchmark analysis of transmembrane segment prediction in membrane proteins
predictions are uploaded to the transmembrane helix  benchmark evaluation server  <cit> . transmembrane helix  benchmark server is an excellent resource to quantitatively compare new tm helix prediction methods with previous methods which include both simple hydrophobicity scale methods to more advanced algorithms that use hidden markov models, neural nets, etc. the benchmark server uses the following metrics for evaluations  <cit> : qok is the percentage of proteins whose membrane segments are all predicted correctly. segment-recall  is the percentage of experimentally determined  segments that are predicted correctly. segment-precision  is the percentage of predicted segments that are correct. the residue accuracy q <dig> refers to the percentage of residues that are predicted correctly. we also computed the f-score, which is the geometric mean of segment level recall and precision. since recall and precision can each be increased arbitrarily at the expense of the other, the two metrics when viewed independently do not reflect the strength of the algorithm. hence, the geometric mean of the two,  is used as the metric.

as described in methods section, random selects proteins to add to the training set based on random selection, node-coverage selects proteins that cover the largest number of nodes, confusion-rated selects proteins that cover nodes with maximum confusion and node-coverage & confusion-rated alternately selects proteins based on node coverage and confusion-rated. in each iteration, the neural network is reinitialized and then trained with the updated labelled  data. this process is performed independently for each method and their performance is evaluated on the test data. neural network initialization, training and evaluation are carried out ten times, and average performance over ten experiments is reported here. table  <dig> shows the average performance for each of the learning algorithms reported by the benchmark server  after first, second, fifth and tenth round of training. active learning methods  outperformed random selection . the results for the active learning method that performed best, node-coverage, reached 94% segment level f-score even for a single protein with balanced performance between segment-recall  and segment-precision . node-coverage & confusion-rated performed similarly to node-coverage after one protein, likely because this alternating method used node-coverage selection on the first round rather than confusion-rated. compare this to confusion-rated alone, which reached 91% segment level f-score only after a second iteration, and random which only achieved 63% f-score for comparable size of training set. active learning significantly reduced the cost , while reaching high prediction accuracy, from over  <dig> training proteins for other state of the art tm helix prediction algorithms to only one training protein. moreover, by using the smallest number of examples, a classifier can be trained as quickly as possible.

 it can be seen that tmpro achieves high segment accuracy  even if the classifier is trained with just one protein that is found by active learning algorithms. the columns from left to right show: method being evaluated; number of proteins in training-set; protein level accuracies: qok, which is the percentage of proteins in which all experimentally determined segments are predicted correctly, and no extra segments are predicted; that is, there is a one to one match between predicted and experimentally determined segments; segment f-score which is the geometric mean of recall and precision; recall ; and precision . q <dig> is the residue level accuracy when all residues in a protein are considered together, and the q <dig> value for the entire set of proteins is the average of that of individual proteins. see  <cit> for further details on these metrics.

tmpro-active shows similar performance over tmpro. however, lower qok is observed, which is likely due the smaller number of training proteins. when the training data includes larger number of proteins, tmpro-active would be the same as tmpro in algorithm .

performance on mptopo and pdbtm data sets
we additionally tested prediction performance of our active learning algorithms on two larger data sets, mptopo dataset of  <dig> proteins  and pdbtm dataset of  <dig> proteins . figure  <dig> shows comparison of f-scores for active learning. for both data sets, active learning methods outperformed random selection. the results for the active learning method that performed best for mptopo and pdbtm is node coverage. this method reaches on average 91% segment level f-score even for single protein with balanced segment recall  and precision  on mptopo data set. moreover, when trained even with a single protein, it achieved an f-score of 91%, segment recall of 93% and segment precision of 90% on a pdbtm dataset. although for second iteration the performance is reduced for both data sets, it recovered after tenth iteration. confusion-rated and node-coverage & confusion-rated perform approximately the same after second iteration. active learning significantly reduced the cost  while reaching high prediction accuracy. as shown in table  <dig> and table  <dig>  node-coverage continued to outperform both random and confusion-rated, achieving about 91% segment f-score with only five training protein, as opposed to random with 84% on mptopo and 76% on pdbtm and confusion-rated with 89% on mptopo and 85% on pdbtm. again, node-coverage & confusion-rated performed the same as just node-coverage on the first round, but on subsequent rounds the f-score decreased slightly below just node-coverage, perhaps showing that confusion-rated is known to typically exceed performance compared to density-based selection after acquiring some amount of initial training  <cit> . in this case however, the feature creation has been very effective and the learning rate is significantly high even with a single protein, leaving no room for confusion rated selection to overtake density based selection.

for description of columns, see caption of table  <dig>  qhtm%obs and qhtm%pred have been computed per-protein and averaged over all the proteins.

for description of columns, see caption of table  <dig>  qhtm,%obs and qhtm,%pred have been computed per-protein and averaged over all the proteins.

CONCLUSIONS
active learning is a promising method for bioinformatics applications such as membrane structure prediction and protein-protein interaction prediction which are marked by availability of small amounts of fully-characterized data and unwieldy procedures for experimental characterization. in this paper, active learning has been employed to tag the proteins that prove to be most informative in training a transmembrane-helix prediction algorithm, tmpro. results show that active learning can significantly reduce the labelling costs without degrading performance. it is seen that latent semantic analysis of protein sequences  <cit>  is highly effective for prediction of tm segments.

competing interests
the authors declare that they have no competing interests.

authors' contributions
mkg designed the algorithms. jaw developed the framework for feature analysis and clustering and huo developed the active learning components. manuscript has been prepared by huo and mkg. jgc provided advice on algorithm-design and reviewed the manuscript.

