BACKGROUND
study of the interactions and functions of genes is crucial to deciphering the mechanisms governing cell-fate differentiation and embryonic development. the dna microarray technique is commonly used to measure the expression levels of a large number of genes simultaneously. however, this technique primarily documents the average expression levels of genes, with information on spatial patterns often unavailable  <cit> . in contrast, the rna in situ hybridization uses gene-specific probes and illuminates the spatial patterns of gene expression precisely. recent advances in this high-throughput technique have generated spatiotemporal information for thousands of genes in organisms such as drosophila  <cit>  and mouse  <cit> . comparative analysis of the spatiotemporal patterns of gene expression can potentially provide novel insights into the functions and interactions of genes  <cit> .

the embryonic patterning of drosophila melanogaster along the anterior-posterior and dorsal-ventral axes represents one of the best understood examples of a complex cascade of transcriptional regulation during development. systematic understanding of the mechanisms underlying the patterning is facilitated by the comprehensive atlas of spatial patterns of gene expression during drosophila embryogenesis, which has been produced by the in situ hybridization technique and documented in the form of digital images  <cit> . to provide flexible tools for pattern searching, the images in the berkeley drosophila genome project  high-throughput study are annotated with anatomical and developmental ontology terms using a controlled vocabulary   <cit>  . these terms integrate the spatial and temporal dimensions of gene expression by describing a developmental "path" that documents the dynamic process of drosophila embryogenesis  <cit> . currently, the annotation is performed manually by human curators. however, the number of available images is now rapidly increasing  <cit> . it is therefore tempting to design computational methods to automate this task.

the particular nature of this problem determines that some challenging questions need to be addressed while designing the automated method. owing to the effects of stochastic processes during development, no two embryos develop identically. also, the quality of the obtained data is limited by current image processing techniques. hence, the shape and position of the same embryonic structure may vary from image to image. indeed, this has been considered as one of the major impediments to automate this task  <cit> . thus, invariance to local distortions in the images is an essential requirement for the automatic annotation system. furthermore, gene expression pattern images are annotated collectively in small groups using a variable number of terms in the original bdgp study. images in the same group may share certain anatomical and developmental structures, but all terms assigned to a group of images do not apply to every image in the group. this requires the development of approaches that can retain the original group membership information of images, because we need to test the accuracy of the new method using existing  annotation data. prior work on this task  <cit>  ignored such groups and assumed that all terms are associated with all images in a group, which may adversely impact their effectiveness for use on the bdgp data. finally, the drosophila embryos are 3d objects, and they are documented as 2d images taken from multiple views. since certain embryonic structures can only be seen in specific two-dimensional projections , it is beneficial to integrate images with different views to make the final annotation. in this article we present a computational method for annotating gene expression pattern images. this method is based on the bag-of-words approach in which invariant visual features are first extracted from local patches on the images, and they are then quantized to form the bag-of-words representation of the original images. this approach is known to be robust to distortions in the images  <cit> , and it has demonstrated impressive performance on object recognition problems in computer vision  <cit>  and on image classification problems in cell biology  <cit> . in our approach, invariant features are first extracted from local patches on each image in a group. these features are then quantized based on precomputed "visual codebooks", and images in the same group with the same view is represented as a bag-of-words. thus, our approach can take advantage of the group membership information of images as in the bdgp study. to integrate images with different views, we propose to construct a separate codebook for images with each view. then image groups containing images with multiple views can be represented as multiple bags, each containing words from the corresponding view. we show that multiple bags can be combined to annotate the image group collectively. after representing each image group as multiple bags of words, we employ a classification model  <cit>  developed recently to annotate the image groups. this model  <cit>  can exploit the correlations among different terms, leading to improved performance. experimental results on the gene expression pattern images obtained from the flyexpress database  show that the proposed approach outperforms other methods consistently. results also show that integration of images with multiple views improves annotation performance. the overall flowchart of the proposed method is depicted in figure  <dig> 

methods
the proposed method is based on the bag-of-words approach, which was originally used in text mining, and is now commonly employed in image and video analysis problems in computer vision  <cit> . in this approach, invariant features  <cit>  are first extracted from local regions on images or videos, and a visual codebook is constructed by applying a clustering algorithm on a subset of the features where the cluster centers are considered as "visual words" in the codebook. each feature in an image is then quantized to the closest word in the codebook, and an entire image is represented as a global histogram counting the number of occurrences of each word in the codebook. the size of the resulting histogram is equal to the number of words in the codebook and hence the number of clusters obtained from the clustering algorithm. the codebook is usually constructed by applying the flat k-means clustering algorithm or other hierarchical algorithms  <cit> . this approach is derived from the bag-of-words models in text document categorization, and is shown to be robust to distortions in images. one potential drawback of this approach is that the spatial information conveyed in the original images is not represented explicitly. this, however, can be partially compensated by sampling dense and redundant features from the images. the bag-of-words representation for images is shown to yield competitive performance on object recognition and retrieval problems after some postprocessing procedures such as normalization or thresholding  <cit> . the basic idea behind the bag-of-words approach is illustrated in figure  <dig> 

for our problem, the images are annotated collectively in small groups in the bdgp database. hence, we propose to extract invariant visual features from each image in a group and represent the images in the same group with the same view as a bag of visual words. the 3d nature of the embryos and the 2d layout of the images determine that certain body parts can only be captured by images taken from certain views. for example, the body part "ventral midline" can only be identified from images taken from the ventral view. hence, one of the challenges in automated gene expression pattern annotation is the integration of images with different views. we propose to construct a separate codebook for images with each view and quantize the image groups containing images with multiple views as multiple bags of visual words, one for each view. the bags for multiple views can then be concatenated to annotate the image groups collectively. after representing each image group as a bag-of-words, we propose to apply a multi-label classification method developed recently  <cit>  that can extract shared information among different terms, leading to improved annotation performance.

feature extraction
the images in the flyexpress database have been standardized semi-automatically, including alignment. three common methods for generating local patches on images are those based on affine region detectors  <cit> , random sampling  <cit> , and regular patches  <cit> . we extract dense features on regular patches on the images, since such features are commonly used for aligned images. the radius and spacing of the regular patches are set to  <dig> pixels in the experiments . owing to the limitations of image processing techniques, local variations may exist on the images. thus, we extract invariant features from each regular patch. in this article, we apply the sift descriptor  <cit>  to extract local visual features, since it has been applied successfully to other image-related applications  <cit> . in particular, each feature vector is computed as a set of orientation histograms on  <dig> ×  <dig> pixel neighborhoods, and each histogram contains  <dig> bins. this leads to a sift feature vector with  <dig>  dimensions on each patch. note that although the invariance to scale and orientation no longer exists since we do not apply the sift interest point detector, the sift descriptor is still robust against the variance of position, illumination, and viewpoint  <cit> .

codebook construction
in this article, we consider images taken from the lateral, dorsal, and ventral views, since the number of images from other intermediate views is small. for each stage range, we build a separate codebook for images with each view. since the visual words of the codebooks are expected to be used as representatives of the embryonic structures, the images used to build the codebooks should contain all the embryonic structures that the system is expected to annotate. hence, we extract codebook images in a way so that each embryonic structure appears in at least a certain number of images. this number is set to  <dig>   <dig>  and  <dig> for codebooks of lateral, dorsal, and ventral images, respectively, based on the total number of images with each view . the sift features computed from regular patches on the codebook images are then clustered using the k-means algorithm. since this algorithm depends on the initial centers, we repeat the algorithm with ten random initializations from which the one resulting in the smallest summed within-cluster distance is selected. we study the effect of the number of clusters  on the performance below and set this number to  <dig>   <dig>  and  <dig> for lateral, dorsal, and ventral images, respectively.

an image group is a group of gene expression pattern images of a particular gene at a particular stage range.

pattern representation
after the codebooks for all views are constructed, the images in each group are quantized separately for each view. in particular, features computed on regular patches on images with a certain view are compared with the visual words in the corresponding codebook, and the word closest to the feature in terms of euclidean distance is used to represent it. then the entire image group is represented as multiple bags of words, one for each view. since the order of the words in the bag is irrelevant as long as it is fixed, the bag can be represented as a vector counting the number of occurrences of each word in the image group. let c <dig> ⋯,cm ∈ ℝd be the m cluster centers  and let v <dig> ⋯,vn ∈ ℝd be the n features extracted from images in a group with the same view where d is the dimensionality of the local features . then the bag-of-words vector w is m-dimensional, and the k-th component wk of w is computed as

  

where δ =  <dig> if a = b, and  <dig> otherwise, and ∥·∥ denotes the vector 2-norm. note that , since each feature is assigned to exactly one word.

based on this design, the vector representation for each view can be concatenated so that the images in a group with different views are integrated . let wl, wd, and wv be the bag-of-words vector for images in a group with lateral, dorsal, and ventral views, respectively. then the bag-of-words vector w for the entire image group can be represented as

  

to account for the variability in the number of images in each group, we normalize the bag-of-words vector to unit length. note that since not all the image groups contain images from all views, the corresponding bag-of-words representation is a vector of zero if a specific view is absent.

pattern annotation
after representing each image group as a global histogram using the bag-of-words representation, the gene expression pattern image annotation problem is reduced to a multi-label classification problem, since each group of images can be annotated with multiple terms. . the multi-label problems have been studied extensively in the machine learning community, and one simple and popular approach for this problem is to construct a binary classifier for each label, resulting in a set of independent binary classification problems. however, this approach fails to capture the correlation information among different labels, which is critical for many applications such as the gene expression pattern image annotation problem where the semantics conveyed by different labels are correlated. to this end, various methods have been developed to exploit the correlation information among different labels so that the performance can be improved  <cit> . in  <cit> , a shared-subspace learning framework has been proposed to exploit the correlation information in multi-label problems. we apply this formulation to the gene expression pattern image annotation problem in this article.

we are given a set of n input data vectors  ∈ ℝd  which are the bag-of-words representations of n image groups. let the terms associated with each of the n image groups be encoded into the label indicator matrix y ∈ ℝn × m where m is the total number of terms, and yiℓ =  <dig> if the ith image group has the ℓth term and yiℓ = - <dig> otherwise. in the shared-subspace learning framework proposed in  <cit> , a binary classifier is constructed for each label to discriminate this label from the rest of them. however, unlike the approaches that build the binary classifiers independently, a low-dimensional subspace is assumed to be shared among multiple labels. the predictive functions in this framework consist of two parts: one part is contributed from the original data space, and the other part is derived from the shared subspace as follows:

   

where wℓ ∈ ℝd and vℓ ∈ ℝr are the weight vectors, Θ ∈ ℝr × d is the linear transformation used to parameterize the shared low-dimensional subspace, and r is the dimensionality of the shared subspace. the transformation Θ is common for all labels, and it has orthonormal rows, that is ΘΘt = i. in this formulation, the input data are projected onto a low-dimensional subspace by Θ, and this low-dimensional projection is combined with the original representation to produce the final prediction.

in  <cit>  the parameters  and Θ are estimated by minimizing the following regularized empirical risk:

   

subject to the constraint that ΘΘt = i, where l is some loss function,  = yiℓ, and α >  <dig> and β >  <dig> are the regularization parameters. it can be shown that when the least squares loss is used, the optimization problem in eq.  can be expressed as

   

where x = t ∈ ℝn × d is the data matrix, ∥·∥f denotes the frobenius norm of a matrix  <cit> , uℓ = wℓ + Θtvℓ, u = , and v = . the optimal Θ* can be obtained by solving a generalized eigenvalue problem, as summarized in the following theorem:

theorem  <dig> let x, y, and Θ be defined as above. then the optimal Θ that solves the optimization problem in eq.  can be obtained by solving the following trace maximization problem:

   

where s <dig> and s <dig> are defined as:

   

   

   

for high-dimensional problems where d is large, an efficient algorithm for computing the optimal Θ is also proposed in  <cit> . after the optimal Θ is obtained, the optimal values for  can be computed in a closed form.

RESULTS
we report and analyze the experimental results on gene expression pattern annotation in this section. we also demonstrate the performance improvements achieved by integrating images with multiple views and study the effect of the codebook size on the annotation performance. the performance for each individual term is also presented and analyzed.

data description
in our experiments, we use drosophila gene expression pattern images retrieved from the flyexpress database  <cit> , which contains standardized versions of images obtained from the bdgp high-throughput study  <cit> . the images are standardized semi-manually, and all images are scaled to  <dig> ×  <dig> pixels. the embryogenesis of drosophila has been divided into six discrete stage ranges  in the bdgp high-throughput study  <cit> . since most of the cv terms are stage range specific, we annotate the images in each stage range separately. the drosophila embryos are 3d objects, and the flyexpress database contains 2d images that are taken from several different views  of the 3d embryos. the size of the cv terms, the number of image groups, and the number of images with each view in each stage range are summarized in table  <dig>  we can observe that most of the images are taken from the lateral view. in stage range 13– <dig>  the number of dorsal images is also comparable to that of the lateral images. we study the performance improvement obtained by using images with different views, and results show that incorporating images with dorsal views can improve performance consistently, especially in stage range 13– <dig> where the number of dorsal images is large. in contrast, the integration of ventral images results in marginal performance improvement at the price of an increased computational cost, since the number of ventral images is small. hence, we only use the lateral and dorsal images in evaluating the relative performance of the compared methods.

evaluation of annotation performance
we apply the multi-label formulation proposed in  <cit>  to annotate the gene expression pattern images. to demonstrate the effectiveness of this formulation in exploiting the correlation information among different labels, we also report the annotation performance achieved by the one-against-rest linear support vector machines  in which each linear svm builds a decision boundary between image groups with and without one particular term. note that in this method the labels are modeled separately, and hence no correlation information is captured. to compare the proposed method with existing approaches for this task, we report the annotation performance of a prior method  <cit> , which used the pyramid match kernel  algorithm  <cit>  to construct the kernel between two sets of feature vectors extracted from two sets of images. we report the performance of kernels constructed from the sift descriptor and that of the composite kernels combined from multiple kernels as in  <cit> . in the case of composite kernels, we apply the three kernel combination schemes  and the best performance on each data set is reported. note that the method proposed in  <cit>  required that the training set contains embryos that are annotated individually, and it has been shown  <cit>  that such requirement leads to low performance when applied to bdgp data in which the images are annotated in small groups. hence, we do not report these results. in the following, the multi-label formulation proposed in  <cit>  is denoted as mlls, and the one-against-rest svm is denoted as svm. the pyramid match kernel approaches based on the sift and the composite features are denoted as pmksift and pmkcomp, respectively. all of the model parameters are tuned using 5-fold cross validation in the experiments.

from table  <dig> we can see that the first stage range  is annotated with only two terms, and we do not report the results in this stage range. in other five stage ranges, we remove terms when they appeared in less than  <dig> training image groups in a stage range, which yielded data sets in which  <dig> or fewer terms need to be considered in every case. the two primary reasons for this decision are  terms which appeared in too few image groups are statistically too weak to be learned effectively, and  we used 5-fold cross-validation to tune the model parameters, and each term should appear in each fold at least once. therefore, the maximum numbers of terms reported in table  <dig>  table  <dig>  table  <dig>  table  <dig>  and table  <dig> represent the "all terms" test.

"mlls" denotes the performance obtained by applying the shared subspace multi-label formulation to the proposed bag-of-words representations derived from lateral and dorsal images. "svm" denotes the performance of svm applied on the bag-of-words representations using the one-against- rest scheme. "pmk" denotes the method based on pyramid match kernels, and the subscripts "sift" and "comp" denote kernels based on the sift descriptor and the composite kernels, respectively. some terms appear in few image groups, and we eliminate them from the experiments. we randomly generate  <dig> different training/test partitions, and the average performance and standard deviation are reported.

see the caption of table  <dig> for detailed explanations.

see the caption of table  <dig> for detailed explanations.

see the caption of table  <dig> for detailed explanations.

see the caption of table  <dig> for detailed explanations.

the experiments are geared toward examining the change in the accuracy of our annotation method, as we used an increasingly larger set of vocabulary terms. in our experiment, we begin with the  <dig> terms that appear in the largest number of image groups. then we add additional terms in the order of their frequencies. by virtue of this design, experiments with  <dig> terms should show higher performance than those with  <dig> terms, because  <dig> most frequent terms will appear more often in image groups in the training data sets as compared to the case of  <dig> terms . the extracted data set is partitioned into training and test sets using the ratio 1: <dig> for each term, and the training data are used to construct the classification model.

the agreement between the predicted annotations and the expert data provided by human curators is measured using the area under the receiver operating characteristic  curve, called auc  <cit> , f <dig> measure  <cit> , sensitivity and specificity. for auc, the value for each term is computed and the averaged performance across multiple terms is reported. for f <dig> measure, there are two ways, called macro-averaged f <dig> and micro-averaged f <dig>  respectively, to average the performance across multiple terms and we report both results. for each data set, the training and test data sets are randomly generated  <dig> times, and the averaged performance and standard deviations are reported in table  <dig>  table  <dig>  table  <dig>  table  <dig>  and table  <dig>  to compare the performance of all methods across different values of sensitivity and specificity, we show the roc curves of  <dig> randomly selected terms on two data sets from stage ranges 11– <dig> and 13– <dig> in figures  <dig> and  <dig> 

we can observe from table  <dig>  table  <dig>  table  <dig>  table  <dig>  and table  <dig> and figures  <dig> and  <dig> that approaches based on the bag-of-words representation  consistently outperform the pmk-based approaches . note that since both the shared-subspace formulation and svm are based on the bag-of-words representation, the benefit of this representation should be elucidated by comparing the performance of both the shared-subspace formulation and svm to the two approaches based on pmk. in particular, mlls outperforms pmksift and pmkcomp on all of the  <dig> data sets in terms of all three performance measures . in all cases, the performance improvements tend to be larger for the two f <dig> measures than auc. it can also be observed from figures  <dig> and  <dig> that the roc curves for svm and the shared-subspace formulation are always above those based on the pyramid match algorithm. this indicates that both svm and the shared-subspace formulation outperform previous methods across all classification thresholds. a similar trend has been observed from other data sets, but their detailed results are omitted due to space constraint. this shows that the bag-of-words scheme is more effective in representing the image groups than the pmk-based approach. moreover, we can observe that mlls outperforms svm on most of the data sets for all three measures. this demonstrates that the shared-subspace multi-label formulation can improve performance by capturing the correlation information among different labels. for the pmk-based approaches, pmkcomp outperforms pmksift on all of the data sets. this is consistent with the prior results obtained in  <cit>  that the integration of multiple kernel matrices derived from different features improves performance.

performance of individual terms
to evaluate the relative performance of the individual terms used, we report the auc values achieved by the proposed formulation on  <dig> data sets in figures  <dig> and  <dig>  one major outcome of our analysis was that some terms were consistently assigned to wrong image groups. for example, the terms "hindgut proper primordium", "malpighian tubule primordium", "garland cell primordium", "salivary gland primordium", and "visceral muscle primordium" in stage range 11– <dig> achieve low auc on all three data sets. similarly, the terms "ring gland", "embryonic anal pad", "embryonic proventriculus", "gonad"", and "embryonic/larval garland cell" achieve low auc on all three data sets in stage range 13– <dig>  for most of these terms, the low performance is caused by the fact that they only appear in very few image groups. such low frequencies result in weak learning due to statistical reasons. therefore, the number of images available for training our method will need to be increased to improve performance.

integration of images with multiple views
to evaluate the effect of integrating images with multiple views, we report the annotation performance in the cases of using only lateral images, lateral and dorsal images, and lateral, dorsal, and ventral images. in particular, we extract six data sets from the stage range 13– <dig> with the number of terms ranged from  <dig> to  <dig> with a step size of  <dig>  the average performance in terms of auc, macro f <dig>  and micro f <dig> achieved by mlls over  <dig> random trials is shown in figure  <dig>  we observe that performance can be improved significantly by incorporating the dorsal view images. in contrast, the incorporation of ventral images results in slight performance improvement. in other stage ranges, the integration of images with multiple views can either improve or keep comparable performance. this may be due to the fact that the dorsal view images are mostly informative for annotating embryos in stage range 13– <dig>  as large morphological movements happen on dorsal side in this stage range. similar trends have been observed when the svm classifier is applied.

effect of codebook size
the size of the codebook is a tunable parameter, and we evaluate its effect on annotation performance using a subset of lateral images from stage range 13– <dig> with  <dig> terms in this experiment. in particular, the size of the codebook for this data set increases from  <dig> to  <dig> gradually with a step size of  <dig>  and the performance of mlls and svm is plotted in figure  <dig>  in most cases the performance can be improved with a larger codebook size, but it can also decrease in certain cases such as the performance of mlls when measured by macro f <dig>  in general, the performance does not change significantly with codebook size. hence, we set the codebook size to  <dig> for lateral images in previous experiments to maximize performance and minimize computational cost. an interesting observation from figure  <dig> is that the performance differences between mlls and svm tend to be larger for a small codebook size. this may reflect the fact that small codebook sizes cannot capture the complex patterns in image groups. this representation insufficiency can be compensated effectively by sharing information between image groups using the shared-subspace multi-label formulation. for a large codebook size, the performance of mlls and svm tend to be close.

CONCLUSIONS
in this article we present a computational method for automated annotation of drosophila gene expression pattern images. this method represents image groups using the bag-of-words approach and annotates the groups using a shared-subspace multi-label formulation. the proposed method annotates images in groups, and hence retains the image group membership information as in the original bdgp study. moreover, multiple sources of information conveyed by images with different views can be integrated naturally in the proposed method. results on images from the flyexpress database demonstrate the effectiveness of the proposed method.

in constructing the bag-of-words representation in this article, we only use the sift features. prior results on other image-related applications show that integration of multiple feature types may improve performance  <cit> . we plan to extend the proposed method for integrating multiple feature types in the future. in addition, the bag-of-words representation is obtained by the hard assignment approach in which a local feature vector is only assigned to the closest visual word. recent study  <cit>  shows that the soft assignment approach that assigns each feature vector to multiple visual words based on their distances usually results in improved performance. we will explore this in the future.

authors' contributions
all authors analyzed the results and wrote the paper. sj designed the methodology, implemented the programs, and drafted the manuscript. sk and jy supervised the project and guided the implementation. all authors have read and approved the final manuscript.

