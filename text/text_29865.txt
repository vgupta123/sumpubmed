BACKGROUND
advances in microscopy technologies
the recent advances in modern microscopy enable three-dimensional imaging of large biological specimens at high resolution. often, the sample is too large to fit into the field of view of the microscope, so that a combination of multiple overlapping recordings  is needed to enable the reconstruction  of the whole image from the individual image stacks. software 3d-stitching tools are needed to assemble tiles since in many cases reliable tile positions are not available. this is true also when motorized stages are used for fast and reproducible acquisition of multiple tiles, since their physical coordinates may not be precise enough to allow direct reconstruction.

current microscopy technology can easily require image reconstructions starting from tens of tiles, each with sizes ranging from tens of megavoxels  to hundreds of megavoxels . several stitching tools have been therefore recently developed to deal with such a challenging task, and successful stitching of 3d images of these orders of magnitude has been reported in the literature  <cit> .

however, the growing demand to acquire complete and large biological specimens at high resolution has lead to further advances in microscopy. in particular, the class of microscopes based on the technique commonly referred to as ultramicroscopy or light sheet-based microscopy   <cit>  enables fast three-dimensional high resolution imaging of large specimens exploiting their induced fluorescence. samples are illuminated by a thin sheet of light and the fluorescence emission is observed from the axis  perpendicular to the illumination plane. objects are then translated along the scanning axis, so obtaining a tile as a stack of 2d slices  <cit> . combining this technique with a special procedure to clear tissue makes it possible to acquire large specimens, such as whole mouse brains  <cit> . recently, at lens in florence, it has been developed an improved lsm technique, referred to in the following as confocal light sheet microscopy , which is capable of imaging very large volumes  with good enough contrast and resolution  to be used for neuroanatomical studies on the whole mouse brain  <cit> . since a typical field of view of the objective is ≈400×400μm <dig> and dimensions of each raw image may be 512× <dig> pixels, at least 25× <dig> tiles are needed to cover the whole volume, each composed by  <dig>  slices. actually, the number of tiles is substantially higher, since acquiring overlapped tiles is necessary to enable automatic and very precise stitching. this increases the dimension of the stitching problem of at least two orders of magnitude, leading to acquisitions of hundreds of large tiles and final image sizes ranging from hundreds of gigavoxels to one teravoxel and more.

characteristics of teravoxel-sized datasets
to make acquisitions of such large images viable, computer controlled micropositioning stages must be used to acquire tiles arranged according to a bidimensional grid. tiles physical coordinates can therefore be recorded during acquisition. due to the range of movements in each dimension , even very small mechanical drifts might make these coordinates not precise enough to enable direct reconstruction of the image. nevertheless, they can definitely allow a precise control over the overlap between adjacent tiles and the a posteriori determination of the region of interest  for each tile where the overlap occurs.

with respect to the image content there are two problems that are relevant to stitching. first, images may contain thin structures that are just larger than the resolution of the microscope, which makes mandatory to perform the stitching with high precision to avoid artifacts due to misalignments of a few pixels. second, since specimens can be selectively labeled with fluorescent markers, regions with very limited or none information content may occur in the acquired volume. since these regions do not contain useful alignment information, the stitching algorithm should be capable to deal with them.

current software solutions
different solutions and tools are available for automatic 3d stitching of large images  <cit> . their common strategy is the following:  performing a pairwise registration through a combination of phase correlation  and normalized cross-correlation , both of which provide an image similarity score, but with different computational requirements and performance, so that ncc is generally used to refine the pc results;  finding a globally optimal placement of tiles using similarity scores;  combining all tiles in a larger image, while correcting lighting differences in the overlapping regions.

existing 3d-stitching tools do not seem adequate to address the stitching of teravoxel-sized datasets because they were designed under the following major assumptions, which are related to the common characteristics of modern high-resolution imaging techniques such as confocal, bright field or electron microscopes. first, the specimen can be very large along the two radial directions only, whereas the extension along the scanning direction is much more limited. this implies that there are at most hundreds of slices per tile instead of thousands. second, the number of tiles is limited and their spatial placement can be either organized  or un-organized . note that in the latter case, step  must be performed for all possible pairs of tiles making the task computationally tractable only if the number of tiles is limited. finally, as a consequence of previous assumptions, the overall size of datasets can vary from hundreds of megavoxels up to a few gigavoxels.

for these reasons, the typical stitching tool today addresses some, but not all of the aspects of teravoxel-sized datasets.

software design considerations and requirements
moving from the considerations discussed so far we give the following general requirements for a stitiching tool capable to deal with teravoxels-sized images.

 stitching processing pipeline
similarly to other state-of-the-art tools, the stitching process has to perform the following main steps:  find the relative position between each pair of adjacent stacks ;  find a globally optimal placement of stacks in the final image space;  combine all stacks into a single 3d image and substitute overlapping regions with a blended version of them.

to effectively deal with the huge dimensions of this class of images one more step to be added is the saving of the resulting 3d image into a multiresolution representation suited for efficient image retrieval and processing.

 minimization of the overall computational workload
the computational complexity and memory requirements of the algorithms that process tiles should be minimized, while at the same time multiple i/o operations on the same data should be avoided. since dataset dimensions is expected to further grow as long as microscope techniques develop their potentials, all algorithms, including for what is concerned with i/o operations, should also be parallelizable to make the whole process scalable.

 identification of indices measuring the alignment quality
since there may be portions of the volume with scarce or none information content, some of the computed displacements at step  may be wrong. this produces a redundant set of alignment data of different quality, which should be associated to some quality index and then globally composed minimizing a measure of the overall error.

 manual intervention
when dealing with images with hundreds of tiles, a few wrong computed alignments are likely to occur since thresholds used by the algorithms to deal with alignment quality indices may fail in some cases. in these cases, even if only very few stacks were incorrectly stitched, the entire stitching pipeline should be repeated from the beginning with different settings or the introduced artifacts accepted. in order to avoid this situation, the user should be able to intervene manually by easily detecting and possibly correcting the abnormalities before the final image is produced. to make this possible, stitching steps should be separable thanks to a loading/saving feature for metadata describing their results, which the user should easily edit. a preview-like feature should also be provided to check the effectiveness of the manual intervention on a limited portion of the acquired volume, without processing again all data from scratch.

 independence of sophisticated content-based analysis of the images
the stitching process should not depend on sophisticated content-based analysis of the raw images, which is likely to generate too much computational workload on very large datasets. in practice, all algorithms should depend at most linearly on the dataset size. analyses requiring more than linear algorithms, if any, should be performed on limited regions of the acquired volume, and only after a nonredundant, easy to access representation of the whole 3d image has been generated.

 expandability
given the wide variety of images in microscopy and the concrete possibility that new techniques will be soon developed to acquire teravoxel-sized datasets, it may be expected that modified, customized or totally new algorithms will be needed in some of all steps of the stitching process. hence, it is essential that the software structure of the stitching tool provides means for incorporating new capabilities.

implementation
architecture
as reported in figure  <dig>  the terastitcher tool is structured in a stepwise fashion according to requirement . first, a preliminary step that imports data organization is necessary to access data properly and efficiently in subsequent steps. we assume raw data arranged as a tile matrix whose dimensions will be referred to as vertical  and horizontal . data are stored according to a two-level hierarchical structure where, at the first level, directories contain all tiles of a row or of a column and at the second level directories contain the tile corresponding to that row or column, stored as a sequence of slice images disposed along the depth  direction. this allows us to load just a small portion of data at the time, which is a necessary condition for requirement . note also that directory names and image files names convey the information about tile and slice positions as provided by the controlling software of the microscope, so that this information can be extracted in the import step and used whenever required. second, the pairwise stacks displacements computation step produces a redundant set of alignment data with associated reliability measures, according to requirement . both redundancy and reliability measures are exploited to select the most reliable displacement for each pair of adjacent stacks , to discard the unreliable displacements  and to find an optimal placement of tiles  before merging tiles into a multiresolution representation. the details of each step are discussed in section algorithms. third, most steps produce metadata describing their results and/or use these metadata to perform their tasks. at each step, metadata are saved in xml files which the user can edit to manually refine displacements, tile coordinates or other stitching metadata, according to requirement . the details of this approach are given in section manual intervention.

in order to satisfy the expandability requirement , we followed an object-oriented approach in designing the software architecture of our tool. it consists of four modules organized in three layers with growing abstraction levels . at the lowest abstraction level, the iomanager and xml modules contain input/output routines for data and metadata, respectively. the middle layer contains the volumemanager module, which is responsible to model data organization and to access both data and metadata through functionalities provided by the lower layer. at the top layer, the stitcher module implements the stitching pipeline.

let us have a look inside the stitcher module, which is critical when considering the expandability and generality requirements of our tool. the internal structure of stitcher is shown in figure  <dig>  stitcher methods implement the stitching pipeline of figure  <dig> and the corresponding strategies to use efficiently system resources. two design patterns, strategy and abstract factory <cit> , are used to create a layer of interfaces separating stitcher’s strategies from the actual algorithms and data structures, respectively, since these may depend on the specific characteristics of the acquired images. strategy lets the user determine at runtime which pairwisedisplalgo and tilesplacementalgo algorithms should be used, whereas abstract factory lets a pairwisedisplalgo produce its own displacement. a concrete subclass of displacement should provide not only spatial information , but also one or more displacement reliability measures tailored to the adopted alignment strategy, with values between  <dig>  and  <dig> . in addition, its project and threshold methods should use these reliability measures to implement the displacement projection and displacement thresholding steps respectively, as explained in more detail in section algorithms. so, new implementations of steps 3- <dig> of the stitching pipeline can easily be embedded within this framework by implementing at most three class interfaces.

algorithms
pairwise stacks displacement computation
to compute the relative position of a pair of tiles  we adopted a multi-mip-ncc approach . tiles are split along the d direction into substacks of nslices slices, where the parameter nslices can be set by the user with typical values in the range . for each couple of homologous substacks of two adjacent tiles, the overlapping regions are identified using tile positions provided by the instrument. the information contained in these regions is condensed in three 2d images computed as maximum intensity projections  along the three dimensions. then a 2d-ncc map is computed for each of the three pairs of mips in a search region of side 2δsearch +  <dig> pixels centered on the central pixel of the overlapping regions. in this way, two displacements corresponding to ncc peaks are obtained for each direction, for a total of  <dig> displacements. the most reliable displacement for each direction is then selected and memorized using a combination of two reliability measures extracted from the ncc maps. these measures are the ncc peak value  and the ncc shape width of the peak . the informations collected so far are stored in a mip-ncc-displ object . note that pairs of substacks are aligned separately, producing a redundant set of alignment data for each pair of adjacent tiles. such a redundancy is exploited in the displacement projection step, as explained below.

besides the relevant reduction in memory occupancy memory requirements reduction, which is discussed in section strategies to limit resource requirements, there are two reasons for dividing tiles into substacks. first, since multi-mip-ncc is based on mip projections, it works well if the information content of the overlapping regions in not too dense. hence, it could perform poorly if applied to whole stacks, whose mips along the d direction could accumulate too much information. second, working on a few relatively small 2d matrices at the time allows an efficient use of the memory hierarchy. the only minor drawback is a small increase in computational workload, since multiple ncc maps have to be computed along d.

displacement projection
once all substacks of one tile have been aligned with their adjacent homologous, the obtained displacements are combined to give the best displacement for each pair of adjacent tiles. such a combination is implemented in the project method of mip-ncc-displ class and it is done by selecting, for each direction separately, the most reliable single direction displacement. this is possible because each single direction displacement has its own reliability measures, thanks to the adopted mip-ncc approach. note that, even if independent reliability measures were not available for each direction, one could still select the most reliable displacement among the ones computed for every substack. in any case, multiple substacks alignments provide an advantage, since substacks difficult to stitch are likely to occur.

it is worth noting that having multiple substacks alignments could also be used to cope with situations where different reliable alignments in v and h directions are returned by the mip-ncc algorithm. this would correspond to acquisitions where sample movements in d direction where not perfectly parallel and tiles should be someway rearranged before merging. while on the one hand this would require tile resampling with a remarkable increase in computational costs, on the other hand we did not actually observe such shifts along the d direction in our acquisitions. for this reasons the tool uses just one displacement  to align adjacent tiles.

displacement thresholding
after the best displacements have been obtained, their reliability is thresholded: when it is below the given threshold, the default displacement provided by stage coordinates is reestablished and the corresponding reliability measure is set to  <dig> . furthermore, the associated pair of stacks is marked as a nonstitchable pair. if all the four pairs associated with a tile are nonstitchable, then the tile itself is marked as nonstitchable, i.e. there’s no way to stitch it to any other stacks, except of using the tile position estimation provided by the instrument. this information is taken into account in the next step.

we observed from different datasets that good candidate values for displacement threshold are in the range , as also suggested by the results reported in table  <dig>  however, we do not exclude that in the case of low-contrasted or high noisy data these values could be lower.

in this table we report a comparison of displacement errors  and quality measures between terastitcher and the state-of-the art stitching tools when stitching megavoxel-sized datasets.

apublicly available dataset downloaded from http://www.vaa3d.org.

bpublicly available dataset downloaded from http://www.xuvtools.org.

cclsm dataset  available at http://www.iconfoundation.net.

dwrong alignment.

optimal tiles placement
since displacements provided by previous steps are not independent, a globally optimal placement of tiles has to be found before producing the final representation of the whole 3d image. following  <cit> , we computed the minimum spanning tree of the undirected weighted graph constituted by the mesh of tiles connected by edges representing tiles displacements. the minimum spanning tree algorithm finds the edges  connecting all tiles without forming cycles and whose total weight is minimum. weights are computed as the inverse of the displacements reliability measures . in this way, displacements associated to nonstitchable stacks have a weight equal to + ∞ since their reliability was set to  <dig>  this prevents the tree connecting stitchable stacks from passing through nonstitchable ones, unless there is a group of isolated stitchable stacks, in which cases a path including them is provided anyway. if different reliability measures are available along the three directions, as in our case after using mip-ncc to compute misalignments, the algorithm is applied separately for each direction to find the alignments that are globally optimal for that direction.

merging and multiresolution generation
after all relative displacements among tiles have been computed, tile merging is performed.

final slices are produced simply copying nonoverlapping regions, while the overlapping ones are substituted by a blended version of them. blending is performed using two sinusoidal functions phase-shifted by Π for weighting the pixels of the two images .

finally merged slices are saved at several resolutions. at every lower resolution the downsampling is done by computing the mean of an 8-pixels cube of the higher resolution. the resulting image can be saved either as a single stack or in a multistack format with the dimensions of individual stacks decided by the user. the multistack format is provided since it allow a selective access to the higher resolution image, which dramatically improves memory efficiency of further processing.

all these operations are performed according to a strategy minimizing memory occupancy, which is described in detail in the next section.

strategies to limit resource requirements
in this section we discuss how the entire stitching procedure is organized to minimize both i/o data transfers and memory requirements which are critical problems in processing data of tb size. we will focus on the two stitching steps that process data, which are the pairwise stacks displacements computation and the merging and saving tiles into a multiresolution representation steps. as reported in figure  <dig>  data are read once for each of the two steps. to understand why, one should note that the final representation of the whole 3d image cannot be produced if the precise relative positions of tiles have not been determined. tile positioning in turn requires that all tile pairs have been properly aligned. since the entire dataset cannot be kept in memory, all data have to be read at least twice and written back to mass storage once, after the final image representation has been produced.

let us refer to nslices as the number of slices per tile and to nrowsand ncols as the number of rows and columns of the tile matrix, respectively. each tile which is not on the matrix border has four adjacent tiles, conventionally referred to as west, north, east and south, respectively.

as mentioned in section algorithms, in the pairwise stacks displacements computation step the whole volume is processed a layer at the time, each composed by nslicesslices at most. for each layer, tiles are read row-wise or column-wise depending on which dimension is smaller. supposing that stacks are read row-wise, a stack is maintained in memory until its displacements with its south and east stacks are computed, after which it is released . if border tiles are properly managed skipping alignments with missing adjacent tiles, only ncols +  <dig> tiles have to be maintained in memory to compute all pairwise displacements, and all data have to be read only once. the column-wise case is symmetric. hence, this step needs to keep in memory only ×nslicesslices at the time. such a quantity can be directly controlled by the user by choosing a small value for nslices.

in the merging step, only a small group of slices for each tile are read at the time, following again a row-wise order for tiles, and taking into account for each tile the right displacement along d computed after the first step. when one slice group of every tile in row i of the tile matrix have been read, adjacent slices in the groups are merged with the algorithm described in section algorithms. this produces a group of horizontal stripes of the slices of the final image representation. these stripes are then merged with the ones previously generated from the slices of the preceding rows, so producing larger stripes of the same group of final slices .

managing properly the first stripe and repeating this procedure for all rows of the tile grid, a group of complete slices of the final representation is produced and saved back to mass storage. in this way, the memory requirement is to keep in memory only a small number of final slices. the number is determined by the minimal resolution of the whole 3d image that has to be generated and by the downsampling method adopted. groups of a few tens of slices are enough in practice. since a resolution reduction of  <dig> times is enough for efficient access to data, in practice there are 25= <dig> slices in each group.

we conclude the section observing that in both steps tiles are divided in multiple groups of slices and each group is processed independently. parallelization of the whole process is therefore trivial and very efficient. indeed, if groups of slices of all tiles are distributed on multiple disks and multiple processing units are available, all operations can be parallelized with negligible overhead, including i/o operations.

manual intervention
in this section we discuss how the tool enables easy and fast manual intervention in order to refine stitching metadata such as pairwise displacements, tiles coordinates and stitchable/nonstitchable attributes of stacks. our solution is based on three principles:  using xml files to load/save stitching metadata,  enabling a preview-like feature to stitch selectable portions of data and  providing graphical models of metadata for a more comprehensive analysis. in the following we detail each of these principles.

xml files editing
stitching steps save their metadata into xml files and they can be launched individually by loading an existing xml containing the metadata used by that step. the benefit of this approach is threefold. first, the user can manually refine stitching metadata by simply editing xml files, which structure was designed to provide easy readability and understanding . second, steps become separable not only logically but also physically by saving/loading the state of the stitching process into/from the xml file. it is then easier to tune single-step parameters without re-executing the previous ones. third, steps dealing only with metadata can be performed on different machines. this enables manual intervention on any machine when the result does not need to be verified on data .

preview-like feature
each step dealing with data lets the user select the portion of volume to be processed in terms of an interval of stacks rows and/or columns of the tile matrix and/or in terms of an interval of slices along d axis. the latter is very useful to verify the quality of stitching before and after any manual intervention. thanks to the adopted slice-based merging strategy, the tool enables fast stitching of small subset of slices of the whole volume, so obtaining a preview of the final volume in just few seconds. after disabling any activated blending method, the user can then easily detect and locate abnormalities on this preview by visual inspection on stacks borders.

graphical models of metadata
we developed and provide matlab scripts that starting from xml files generate graphical models of metadata to be used for a more comprehensive analysis and faster manual intervention. an example is given in figure  <dig> for the displacement thresholding step, where we report a map of stitchable and non stitchable stacks together with their displacements values and reliability measures. from this map, one could observe if the connected region of stitchable stacks does correspond to the shape of the acquired specimen and if the alignment of some nonempty stacks turned out to be particularly difficult along a particular direction. in this way, the user can detect abnormalities directly on metadata and then edit the corresponding xml entry.

RESULTS
in this section we provide four sets of performance data characterizing our stitching tool from both qualitative and quantitative points of view. a comparison between the terastitcher tool and the ones presented in  <cit>  is also provided in both cases.

first, in table  <dig> we report maximum and average displacements errors as well as their corresponding quality measures obtained by stitching tiles from different datasets with our tool and the ones it has to be compared with. the aim of such a comparison is to demonstrate that terastitcher performs at least as well as the others from a qualitative point of view. to this purpose, several image stacks from  <dig> datasets differing in content, bitdepth, lighting conditions, contrast and snr have been used. displacements errors were measured taking into account an uncertainty of ± <dig> voxel of the groundtruth, which was then subtracted from each error. quality measures were normalized from  to  for xuvstitch. the results show that terastitcher can stitch both images acquired in clsm and images acquired with other microscopy techniques with single-pixel precision. conversely, both istitch and xuvstitch didn’t perform well with clsm images. in fact, the former provided more than one wrong displacement. however, differently from other tools considered here, this tool does not use stage coordinates, which probably made stitching more difficult. as to xuvstitch, it provided just wrong displacements in two instances. moreover, its quality measures were quite low in all the instances of the mouse cerebellum dataset, which would make them not suited to be used as reliability measures. finally, fiji performed very well in all the instances considered, but its quality measures seemed to be content-dependent and they were very low for  <dig> of the  <dig> considered datasets.

second, in table  <dig> we report a comparison about the execution times and memory occupancy of individual phases as well as of the whole stitching procedure on megavoxel-sized data extracted from the whole mouse cerebellum dataset acquired in clsm. measurements have been carried out on a laptop equipped with  <dig> gb of ram,  <dig> gb of disk space,  <dig> dual-core cpu at  <dig>  ghz. we used the following values for terastitcher parameters: nslices= <dig>  δsearch= <dig>  in order to have a fair comparison, the other tools parameters having the same meaning of δsearch were set at the same value. it is worth noting that although istitch implementation does not exploit the knowledge of the approximate relative position of the two stacks as the other tools considered do, it computes an initial relative position on a quite undersampled image . for this reason the contribution of this preliminary step to the overall execution time can be considered negligible. the results show that both execution times and memory occupancy of our tool are significantly lower than those of the others, except for xuvstitch, whose execution times were comparable with those of terastitcher. however, its memory peak becomes significantly higher than that of terastitcher when stitching large datasets, as it is shown in the third row of the table, where four stacks of 512×512× <dig> voxels have been stitched. another interesting result is that istitch performed better than the other tools when stage coordinates are not provided, probably thanks to the preliminary step on the undersampled image mentioned above, which confirms to be quite effective.

in this table we report a comparison of execution times  and memory peaks  between terastitcher and the state-of-the art stitching tools when stitching megavoxel-sized datasets.

atime spent in stitching, net of i/o.

third, in table  <dig> we report execution times and memory peaks of individual phases when stitching gigavoxel and teravoxel-sized data with terastitcher. we could not make a comparison with the other tools because they easily ran out of memory with such data. measurements have been carried out on a workstation equipped with  <dig> gb of ram,  <dig> tb of disk space,  <dig> quad-core cpus at  <dig>  ghz and using the following values for terastitcher parameters: nslices= <dig>  δsearch= <dig>  the first three rows refer to increasing subvolumes of a 3d image of the whole mouse cerebellum dataset. it is apparent that all times are linearly dependent on the image size. the complete image  has been processed in about  <dig> hours . the last row refers to the time needed to process a  <dig>  teravoxels image corresponding to about  <dig> days. times corresponding to individual substeps are not available in this case.

in this table are shown both execution times  and memory peaks  of the terastitcher tool when stitching gigavoxel and teravoxel-sized datasets.

anot available.

finally, we show in figure  <dig> the result of the stitching process on slices at the border of four overlapping tiles for different datasets. this example is representative of the very good qualitative performance attained by our alignment and blending algorithms.

CONCLUSIONS
forthcoming microscopy techniques like confocal light sheet microscopy are able to acquire tiled images of  <dig> teravoxel or more. these images pose novel requirements to stitching that are not adequately addressed by existing tools.

in this paper we have presented a tool designed for automatic 3d stitching of teravoxel-sized tiled images. the tool, initially developed for stitching images generated by the clsm microscope, is completely general, and suited to be adapted to other instruments capable to acquire images of these dimensions. the central idea is to accurately specify the requirements of the stitching problem when teravoxel-sized images are involved, and then use efficiently the system resources to perform the stitching of these images. to improve the generality of the proposed tool, we defined a software architecture that clearly separates the strategies in common among different datasets from the algorithms that may depend on specific characteristics of the acquired images.

an implementation of the tool that is capable to perform the stitching of real teravoxel-sized images on workstations with relatively limited memory resources in reasonable time has been presented. it uses specific algorithms well suited for a relatively wide class of images, that substantially reduce memory and computational requirements of some basic steps of the stitching procedure with respect to previous approaches. a comparison with state-of-the-art stitching tools confirms that the solutions adopted are best suited for stitching very large, automatically acquired microscopy images.

availability and requirements
we provide terastitcher both as standalone application and as plugin of the free software vaa3d  <cit> . sources and binaries, as well as some matlab scripts for the generation of graphical models for metadata, are freely available at the project home page. an online help is also provided which comprises the detailed description of command line parameters, stitching pipeline, supported data formats, supported stacks organization, and a faq section. 

● project name: terastitcher

● project home page: http://code.google.com/p/terastitcher

● operating system: platform independent

● programming language: c++

● other requirements: opencv library  <dig> . <dig> or higher is required for compiling both the standalone application and the plugin of vaa3d. on the contrary, our binary packages can be directly used since we provide opencv precompiled binaries within them.

● license: both source code and binaries are freely available at the project home page for non-commercial purposes only and we require that our work is cited in user’s related studies and publications, if any. a short license agreement which encloses this clause is provided in the header of every source file as well as in the binary packages and before downloading any material related to the project.

abbreviations
lsm: light sheet microscopy; lens: european laboratory for non-linear spectroscopy; clsm: confocal light sheet microscopy; roi: region of interest; pc: phase correlation; ncc: normalized cross-correlation; i/o: input/output; v: vertical; h: horizontal; d: depth; gb: gigabyte; tb: terabyte.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
both authors worked to the design, development and testing of the terastitcher tool. they also equally contributed to write the manuscript. both authors read and approved the final manuscript.

