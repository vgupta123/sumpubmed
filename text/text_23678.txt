BACKGROUND
introduction
in the era of high-throughput computational biology, discovering the biological functions of the genes/proteins within an organism is a central goal. many studies have applied machine learning to infer functional properties of proteins, or directly predict one or more functions for unknown proteins
 <cit> . the prediction of multiple biological functions with a single model, by using learning methods for multi-label prediction, has made considerable progress in recent years
 <cit> .

a major step forward is the learning of models which take into account the possible structural relationships among functional classes
 <cit> . this is motivated by the presence of ontologies and catalogs such as gene ontology 
 <cit>  and mips-fun 
 <cit> , which are organized hierarchically , where classes may have multiple parents), where general functions include other more specific functions ). in this context, the hierarchial constraint must be observed: a gene annotated with a function must be annotated with all the ancestor functions from the hierarchy. in order to tackle this problem, hierarchical multi-label classifiers, that are able to take the hierarchical organization of the classes into account during both the learning and the prediction phase, have been recently used
 <cit> .
 <cit> .  an example of input data: the fun class hierarchy of an example and corresponding class vector and attribute set.  an example of a predictive clustering tree for hmc. the internal nodes contain tests on attribute values and the leaves vectors of probabilities associated with the class values.

the topic of using protein-protein interaction  networks in the identification and prediction of protein functions has attracted increasing attention in recent years. the motivation for this stream of research is best summarized by the statement that "when two proteins are found to interact in a high throughput assay, we also tend to use this as evidence of functional linkage"
 <cit> . as a confirmation, numerous studies have demonstrated the guilt-by-association  principle, which states that proteins sharing similar functional annotations tend to interact more frequently than proteins which do not share them. interactions reflect the relation or dependence between proteins. in the context of networks of such interactions, gene functions show some form of autocorrelation <cit> .

while correlation denotes any statistical relationship between two different variables  of the same objects , autocorrelation denotes the statistical relationships between the same variable  on different but related  objects . although autocorrelation has never been investigated in the context of hierarchical multi-label classification , it is not a new phenomenon in protein studies. for example, it has been used for predicting protein properties using sequence-derived structural and physicochemical features of protein sequences
 <cit> . in this work, we introduce a definition of autocorrelation for the case of hmc and propose a method that leverages on it for improving the accuracy of gene function prediction.

motivation and contributions
the method developed in this work, named nhmc, addresses the task of hierarchical multi-label classification where, in addition to attributes describing the genes, such as microarray-derived expression values, phenotype and sequence data, the network autocorrelation of the class values  is also considered. the main goal is gene function prediction in the context of gene interaction networks, where network autocorrelation exists among the functional annotations of genes. each of the aspects of nhmc and network autocorrelation have been addressed individually in the framework of predictive clustering and in particular within the task of learning predictive clustering trees 
 <cit> . vens et al.
 <cit>  proposed clus-hmc, an approach for building pct for hmc. stojanova et al.
 <cit>  proposed nclus, an approach for building pct to perform regression on network data, taking into account the network autocorrelation of the real-valued  response variable. we bring both of these recent developments under the same roof and propose nhmc, an approach for building pct to perform hmc on network data, taking into account the  network autocorrelation of the hierarchical annotations .

the consideration of network autocorrelation itself raises several challenges. the existence of autocorrelation violates the assumption that instances  are independently and identically distributed , which underlines most machine learning algorithms. the violation of the i.i.d. assumption has been identified as one of the main reasons responsible for the poor performance of traditional methods in machine learning
 <cit> . moreover, most of the learning methods which model autocorrelation in networked data assume its stationarity
 <cit> . this means that possible significant variations of autocorrelation throughout the network due to a different underlying latent structure cannot be properly represented.

the consideration of hierarchical multi-label classification introduces additional complications. network autocorrelation in the context of different effects of autocorrelation can be expected for different class labels. furthermore, the classes at the lower levels of the hierarchy will have a higher fragmentation: for those classes, the autocorrelation phenomenon will likely be local . thus, in hmc tasks, we will need to consider autocorrelation by modeling  its non-stationarity.

while the simultaneous consideration of the relationships among class labels  and instances  introduces additional complexity to the learning process, it also has the potential to bring substantial benefits. the method nhmc that we propose will be able to consider gene function hierarchies in the form of dag structures, where a class may have multiple parents, and to consistently combine two sources of information . in this way, we will be able to obtain gene function predictions consistent with the network structure and improve the predictive capability of the learned models. we will also be able to capture the non-stationary effect of autocorrelation at different levels of the hierarchy and in different parts of the networks.

in this article, we first define the concept of autocorrelation in the hmc setting and introduce an appropriate autocorrelation measure. we then introduce the nhmc algorithm for hmc, which takes this kind of autocorrelation into account. like clus-hmc, nhmc exploits the hierarchical organization of class labels , which can have the form of a tree or a direct acyclic graph . like nclus, nhmc explicitly considers non-stationary autocorrelation when building pct for hmc from real world  network data: the models it builds adapt to local properties of the data, providing, at the same time, predictions that are smoothed to capture local network regularities. finally, we evaluate the performance of nhmc on many datasets along a number of dimensions: these include the gene descriptions, the functional annotation hierarchies and the ppi networks considered.

methods
in this section, we introduce the method nhmc , the major contribution of the paper. nhmc builds autocorrelation-aware models  for hmc. we shall start with a brief description of the algorithm clus-hmc, which builds trees for hmc and is the starting point for developing nhmc.

for the hmc task, the input is a dataset u consisting of instances  that have the form ui =  ∈ x × 2c, where x = x1 × x2…× xm is the space spanned by m attributes or features , while 2c is the power set of c = {c <dig> …,ck}, the set of all possible class labels. c is hierarchically organized with respect to a partial order ≼ which represents the superclass relationship. note that each yi satisfies the hierarchical constraint:

  c∈yi⇒∀c′≼c:c′∈yi. 

the method we propose  builds a generalized form of decision trees and is set in the predictive clustering  framework
 <cit> . the pc framework views a decision tree as a hierarchy of clusters: the top-node corresponds to one cluster containing all the data, that is recursively partitioned into smaller clusters when moving down the tree. such a tree is called a predictive clustering tree . pct combines elements from both prediction and clustering. as in clustering, clusters of data points that are similar to each other are identified, but, in addition, a predictive model is also associated to each cluster. this predictive model provides a prediction for the target property of new examples that are recognized to belong to the cluster. in addition, besides the clusters themselves, pc approaches also provide symbolic descriptions of the constructed  clusters.

the original pc framework is implemented in the clus system
 <cit>  , which can learn both pct and predictive clustering rules. the induction of pct is not very different than the induction of standard decision trees . the algorithm takes as input a set of training instances and searches for the best acceptable test to put in a node and split the data. if such a test can be found, then the algorithm creates a new internal node labeled with the test and calls itself recursively to construct a subtree for each subset  in the partition induced by the test on the training instances.

clus-hmc
the clus-hmc
 <cit>  algorithm builds hmc trees, pct for hierarchial multi-label classification  for an example of an hmc tree). these are very similar to classification trees, but each leaf predicts a hierarchy of class labels rather than a single label. clus-hmc builds the trees in a top-down fashion and the outline of the algorithm is very similar to that of top-down decision tree induction algorithms . the main differences are in the search heuristics and in the way predictions are made. for the sake of completeness both aspects are reported in the following. additional details on clus-hmc are given by vens et al.
 <cit> .

search heuristics
to select the best test in an internal node of the tree, the algorithm scores the possible tests according to the reduction in variance  induced on the set u of examples associated to the node. in clus-hmc, the variance of class labels across a set of examples u is defined as follows:

  var=1|u|·∑ui∈ud <dig>  

where li is the vector associated to the class labels of example ui ,
l¯ is the average of all li vectors corresponding to the class labels of examples in u and d is a distance function on such vectors. the basic idea behind the use of the variance reduction is to minimize intra-cluster variance.

in the hmc context, class labels at higher levels of the annotation hierarchy are more important than class labels at lower levels. this is reflected in the distance measure used in the above formula, which is a weighted euclidean distance:

  d=∑k=1kω· <dig> 

where li,k is the k-th component of the class vector li and the class weights ω associated with the labels decrease with the depth of the class in the hierarchy. more precisely, ω = ω0·avgj {ω)}, where pj denotes the j-th parent of class c and 0 < ω0 < 1). this definition of the weights allows us to take into account a hierarchy of classes, structured as a tree and dag .

for instance, consider the small hierarchya in figure
 <dig>  and two examples  and , where y1 = {all,b,b. <dig> c,d,d. <dig> d.3} and y <dig> = {all,a,d,d. <dig> d.3}. the class vectors for y <dig> and y <dig> are: l1 =  and l2 = . the distance between the two class vectors is then:

  d=3·w02+4·w <dig> 

at each node of the tree, the test that maximizes the variance reduction is selected. this is expected to maximize cluster homogeneity with respect to the target variable and improve the predictive performance of the tree. if no test can be found that significantly reduces variance , then the algorithm creates a leaf and labels it with a prediction, which can consist of multiple hierarchically organized labels.

predictions
a classification tree typically associates a leaf with the "majority class", i.e., the label most appearing in the training examples at the leaf. this label is later used for prediction purposes when a test case reaches that leaf. however, in the case of hmc, where an example may have multiple classes, the notion of "majority class" cannot be straightforwardly applied. in fact, clus-hmc associates the leaf with the mean
l¯ of the class vectors of the examples in the leaf. the value at the k-th component of
l¯ is interpreted as the membership score of class ck, i.e., the probability that an example arriving at the leaf will be labeled with a class ck.

for an example arriving at a leaf, binary predictions for each class label can be obtained by applying a user defined threshold τ on this probability: if the i-th component of
l¯ is above τ , then the leaf predicts the class ci. to ensure that the predictions satisfy the hierarchical constraint, i.e., whenever a class is predicted, its super-classes are also predicted, it suffices to choose τi ≤ τj whenever cj is ancestor of ci.

nhmc
we first discuss the network setting that we consider in this paper. we then propose a new network autocorrelation measure for hmc tasks. subsequently, we describe the clus-hmc algorithm for learning hmc trees and introduce its extension nhmc , which takes into account the network autocorrelation  when learning trees for hmc.

network setting for hmc
some uses of a ppi network in learning gene function prediction models include: treating the interactions between pairs of genes as descriptive attributes  and generating new features as combinations of ppi data with other descriptive attributes. both approaches require that data are pre-processed before applying a network oblivious learning method . however, the applicability of predictive models built in this way is strongly dependent on ppi network information being available for the testing data, i.e., for the proteins whose gene function we want to predict.

in order to learn general models, which can be used to make predictions for any test set, we use protein interactions as a form of background knowledge and exploit them only in the learning phase. more specifically, in the training phase, both gene properties and network structure are considered. in the testing phase, only gene properties are considered and the network structure is disregarded. this key feature of the proposed solution is especially attractive when function prediction concerns new genes, for which interactions with other genes are not known or are still to be confirmed.

following steinhaeuser et al.
 <cit> , we view a training set as a single network of labeled nodes. formally, the network is defined as an undirected edge-weighted graph g = , where v is the set of labeled nodes, and
e⊆{〈u,v,w〉|u,v∈v,w∈r+} is the set of edges. each edge u ↔ v is assigned with a non-negative real number w, called the weight of the edge. it can be represented by a symmetric adjacency matrix w, whose entries are positive  if there is an edge connecting i to j in g, and zero  otherwise. in ppi networks, edge weights can express the strength of the interactions between proteins. although the proposed method works with any non-negative weight values, in our experiments we mainly focus on binary  weights.

each node of the network is associated with an example pair ui =  ∈ x × 2c, where
yi=,q≤k, is subject to the hierarchical constraint. given a network g =  and a function η : v ↦  which associates each node with the corresponding example, we interpret the task of hierarchical multi-label classification as building a pct which represents a multi-dimensional predictive function f : x ↦ 2c that satisfies the hierarchical constraint, maximizes the autocorrelation of the observed classes yi for the network g, and minimizes the prediction error on yi for the training data η.

network autocorrelation for hmc
an illustration of the concept of network autocorrelation for hmc is a special case of network autocorrelation
 <cit> . it can be defined as the statistical relationship between the observations of a variable  on distinct but related  nodes in a network . in hmc, domain values of the variable form a hierarchy, such as the go hierarchy for protein functions. therefore, it is possible to define network autocorrelation for individual nodes and for various levels of the hierarchy.

in predictive modeling, network autocorrelation can be a problem, since the i.i.d. assumption is violated, but also an opportunity, if it is properly considered in the model. this is particularly true for the task of hierarchical multi-label classification considered in this work. indeed, due to non-stationary autocorrelation, ppi network data can provide useful  information for each single class at each level of the hierarchy. intuitively, genes belonging to classes at higher levels of the hierarchy tend to participate in very general types of interactions, while genes belonging to classes at lower levels of the hierarchy tend to participate in very specific and localized interactions. in any case, the effect of autocorrelation changes from level to level . for this reason, we explicitly measure autocorrelation and we build a model such that its value is maximized.

geary’s c for hmc
in order to measure the autocorrelation of the response variable y in the network setting for hmc, we propose a new statistic, named ay, whose definition draws inspiration from global geary’s c <cit> . global geary’s c is a measure of spatial autocorrelation for a continuous variable. its basic definition  is given in additional file
 <dig> 

let ui =  ∈ u ⊆ x × 2c be an example pair in a training set u of n examples. let k be the number of classes in c, possibly defining a hierarchy. we represent yi as a binary vector li of size k, such that li,k =  <dig> if ck ∈ yi and li,k =  <dig> otherwise, and each li satisfies the hierarchical constraint. let d be a distance measure defined for two binary vectors associated to two examples ui =  and uj = , which takes the class-label hierarchy into account.

the network autocorrelation measure ay, based on geary’s c, is defined as follows:

  ay=1-·∑i∑jwij·d24·∑i∑jwij·∑id <dig> 

where
l¯ is the vector representation of the mean vector computed on all binary vectors associated to example pairs in u. the constant  <dig> in the denominator is included for scaling purposes. the new autocorrelation measure ay takes values in the unit interval  <cit> , where  <dig>  means strong positive  autocorrelation and  <dig>  means no autocorrelation.

the algorithm
we can now proceed to describe the top-down induction algorithm for building network hmc trees. the main differece with respect to clus-hmc is that the heuristic is different. the network is considered as background knowledge and exploited only in the learning phase. below, we first give an outline of the algorithm, before giving details on the new search heuristics, which takes autocorrelation into account. we discuss how the new search heuristics can be computed efficiently.


outline of the algorithm the top-down induction algorithm for building pct for hmc from network data is given below . it takes as input the network g =  and the corresponding hmc dataset u, obtained by applying η : v ↦ x × 2c to the vertices of the network.

in practice, this means that for each gene ui ) there is a set of  attributes describing different aspects of the genes. for the experiments with the yeast genome, these include sequence statistics, phenotype, secondary structure, homology, and expression data  and a class vector, li i.e., functional annotations associated to it.

the algorithm recursively partitions u until a stopping criterion is satisfied . since the implementation of this algorithm is based on the implementation of the clus-hmc algorithm, we call this algorithm nhmc .  


search space as in clus-hmc, for each internal node of the tree, the best split is selected by considering all available attributes. let xi ∈ {x <dig> …,xm} be an attribute and
domxi its active domain. a split can partition the current sample space d according to a test of the form xi ∈ b, where
b⊆domxi. this means that d is partitioned into two sets, d <dig> and d <dig>  on the basis of the value of xi.

for continuous attributes, possible tests are of the form x ≤ β. for discrete attributes, they are of the form
x∈{ai <dig> ai <dig> …,aio} . in the former case, possible values of β are determined by sorting the distinct values in d, then considering the midpoints between pairs of consecutive values. for b distinct values, b- <dig> thresholds are considered. when selecting a subset of values for a discrete attribute, clus-hmc relies on the non-optimal greedy strategy proposed by mehta et al.
 <cit> .


heuristics the major difference between nhmc and clus-hmc is in the heuristics they use for the evaluation of each possible split. the variance reduction heuristics employed in clus-hmc  aims at finding accurate models, since it considers the homogeneity in the values of the target variables and reduces the error on the training data. however, it does not consider the dependencies of the target variables values between related examples and therefore neglects the possible presence of autocorrelation in the training data. to address this issue, we introduced network autocorrelation in the search heuristic and combined it with the variance reduction to obtain a new heuristics .

more formally, the nhmc heuristics is a linear combination of the average autocorrelation measure ay  and variance reduction var :

  h=α·|u1|·ay+|u2|·ay|u|+·var′-|u1|·var′+|u2|·var′|u| 

where var′ is the min-max normalization of var, required to keep the values of the linear combination in the unit interval  <cit> , that is:

  var′=var-δminδmax-δmin, 

with δmax and δmin being the maximum and the minimum values of var over all tests.

we point out that the heuristics in nhmc combines information on both the network structure, which affects ay, and the hierarchical structure of the class, which is embedded in the computation of the distance, d used in formula  and . we also note that the tree structure of the nhmc model makes it possible to consider different effects of the autocorrelation phenomenon at different levels of the tree model, as well as at different levels of the hierarchy . in fact, the effect of the class weights ω in equation  is that higher levels of the tree will likely capture the regularities at higher levels of the hierarchy.

however, the efficient computation of distances according to equation  <dig> is not straightforward. the difficulty comes from the need of computing a and aincrementally, i.e., from the statistics already computed for other partitions. indeed, the computation of a and a from scratch for each partition would increase the time complexity of the algorithm by an order of magnitude and would make the learning process too inefficient for large datasets.


efficient computation of the heuristics in our implementation, in order reduce the computational complexity, equation  is not computed from scratch for each test to be evaluated. instead, the first test to be evaluated is that which splits u in u2 ≠ ∅ and u1 ≠ ∅ such that |u2| is minimum  and u1 = u - u <dig>  only on this partition, equation  is computed from scratch. the subsequent tests to be evaluated progressively move examples from u <dig> to u <dig>  consequently, ay,ay,var and var are computed incrementally by removing/adding quantities to the same values computed in the evaluation of the previous test.

var can be computed according to classical methods for incremental computation of variance. as regards ay, its numerator ) only requires distances that can be computed in advance. therefore, the problem remains only for the denominator of equation . to compute it incrementally, we consider the following algebraic transformations:

 ∑ui∈ud2=∑ui∈u∑k=1kω2=∑k=1kω∑ui∈u2=∑k=1kω∑ui∈u′2+=∑ui∈u′d2+× 

where u = u′ ∪ {ut} and
lu¯  is the average class vector computed on u .

this allows us to significantly optimize the algorithm, as described in the following section.


time complexity in nhmc, the time complexity of selecting a split test represents the main cost of the algorithm. in the case of a continuous split, a threshold β has to be selected for the continuous variable. if n is the number of examples in the training set, the number of distinct thresholds can be n -  <dig> at worst. since the determination of candidate thresholds requires an ordering of the examples, its time complexity is o, where m is the number of descriptive variables.

for each variable, the system has to compute the heuristic h for all possible thresholds. in general, this computation has time-complexity o ·  · k), where n -  <dig> is the number of thresholds, s is the average number of edges for each node in the network, k is the number of classes, o is the complexity of the computation of the variance reduction and o is the complexity of the computation of autocorrelation.

however, according to the analysis reported before, it is not necessary to recompute autocorrelation values from scratch for each threshold. this optimization makes the complexity of the evaluation of the splits for each variable o. this means that the worst case complexity of creating a split on a continuous attribute is o · k).

in the case of a discrete split, the worst case complexity  is o ·  · k), where d is the maximum number of distinct values of a discrete variable . overall, the identification of the best split node  has a complexity of o · k) + o · k), that is o · k). this complexity is similar to that of clus-hmc, except for the s factor which equals n in the worst case, although such worst-case behavior is unlikely.


additional remarks the relative influence of the two parts of the linear combination in formula  is determined by a user-defined coefficient α that falls in the interval  <cit> . when α =  <dig>  nhmc uses only autocorrelation, when α =  <dig> , it weights equally variance reduction and autocorrelation, while when α =  <dig> it works as the original clus-hmc algorithm. if autocorrelation is present, examples with high autocorrelation will fall in the same cluster and will have similar values of the response variable . in this way, we are able to keep together connected examples without forcing splits on the network structure .

finally, note that the linear combination that we use in this article ) was selected as a result of our previous work on network autocorrelation for regression
 <cit> . the variance and autocorrelation can also be combined in some other way . investigating different ways of combining them is one of the directions for our future work.

RESULTS
in this section, we present the evaluation of the system nhmc on several datasets related to predicting gene function in yeast. before we proceed to presenting the empirical results, we provide a description of the datasets used and the experimental settings.

data sources
we use  <dig> datasets for gene function prediction in yeast  as considered by clare and king
 <cit> , but with the class labels used by vens et al.
 <cit> b.

the seq dataset records sequence statistics that depend on the amino acid sequence of the protein for which the gene codes. these include amino acid frequency ratios, sequence length, molecular weight and hydrophobicity.

the pheno dataset contains phenotype data, which represent the growth or lack of growth of knock-out mutants that are missing the gene in question. the gene is removed or disabled and the resulting organism is grown with a variety of media to determine what the modified organism might be sensitive or resistant to.

the struc dataset stores features computed from the secondary structure of the yeast proteins. the secondary structure is not known for all yeast genes; however, it can be predicted from the protein sequence with reasonable accuracy, using prof
 <cit> . due to the relational nature of secondary structure data, clare and king
 <cit>  performed a preprocessing step of relational frequent pattern mining; the struc dataset includes the constructed patterns as binary attributes.

the hom dataset includes, for each yeast gene, information from other, homologous genes. homology is usually determined by sequence similarity; here, psi-blast
 <cit>  was used to compare yeast genes both with other yeast genes and with all genes indexed in swissprot v <dig>  this provided for each yeast gene a list of homologous genes. for each of these, various properties were extracted . clare and king
 <cit>  preprocessed these data in a similar way as the secondary structure data to produce binary attributes.

the cellcycle, church, derisi, eisen, gasch <dig>  gasch <dig>  spo, exp datasets include microarray yeast data
 <cit> . attributes for these datasets are real valued. they represent fold changes in gene expression levels.

we construct two versions of each dataset. the values of the descriptive attributes are identical in both versions, but the classes are taken from two different classification schemes. in the first version, they are from fun , a scheme for classifying the functions of gene products, developed by mips
 <cit> . fun is a tree-structured class hierarchy; a small part is shown in figure
 <dig>  in the second version of the data sets, the genes are annotated with terms from the gene ontology 
 <cit>  , which forms a directed acyclic graph instead of a tree: each term can have multiple parents . only annotations from the first six levels were taken.

in addition, we use two protein-protein interaction networks  for yeast genes. in particular, the networks biogrid
 <cit>  and dip
 <cit>  are used, which contain  <dig> and  <dig> interactions among  <dig> and  <dig> proteins, respectively. biogrid stores physical and genetic interactions, dip  stores and organizes information on binary protein-protein interactions that are retrieved from individual research articles.

the basic properties of the datasets in terms of the number of examples, number of attributes  and number of  classes are given in table
 <dig>  for both networks, binary  weights are considered in nhmc. exceptions are explicitly mentioned.

we use  <dig> yeast  datasets  and two functional annotation  schemes.

experimental setup
in the experiments, we deal with several dimensions: different descriptions of the genes, different descriptions of gene functions, and different gene interaction networks. we have  <dig> different descriptions of the genes from the clare and king’ datasets
 <cit>  and  <dig> class hierarchies , resulting in  <dig> datasets with several hundreds of classes each. furthermore, we use biogrid and dip ppi networks for each of those. moreover, for each dataset, we extracted the subset containing only the genes that are most connected, i.e., have at least  <dig> interactions in the ppi network . we will focus on presenting the results for the datasets with go annotations, while the results for the fun versions of the datasets are given in the additional files
 <dig> and
 <dig> 

as suggested by vens et al.
 <cit> , we build models trained on 2/ <dig> of each data set and test on the remaining 1/ <dig>  the results reported in this paper are obtained using exactly the same splits as
 <cit> . the subset containing genes with more than  <dig> connections uses the same 2/3-1/ <dig> training-testing split. this is necessary in order to guarantee a direct comparison of our results with results obtained in previous work. however, in order to avoid problems due to randomization, we also performed experiments according to a 3-fold cross validation schema.

to prevent over-fitting, we used two pre-pruning methods: the minimal number of examples in a leaf  and f-test pruning. the latter uses the f-test to check whether the variance reduction achieved after adding a test is statistically significant at a given level . the algorithm takes as input a vector of significance levels/ p-values and by internal 3-fold cross-validation selects the one which leads to the smallest error.

following vens et al.
 <cit> , we evaluate the proposed algorithm by using as a performance metric the average area under the precision-recall curve , i.e., the  average of the areas under the individual  precision-recall  curves, where all weights are set to 1/|c|, with c the set of classes. the closer the
auprc¯ is to  <dig> , the better the model is. a pr curve plots the precision of a classifier as a function of its recall. the points in the pr space are obtained by varying the value for the threshold τ from  <dig> to  <dig> with a step of  <dig> . in the considered datasets, the positive examples for a given class are rare as compared to the negative ones. the evaluation by using pr curves , is the most suitable in this context, because we are more interested in correctly predicting the positive instances , rather than correctly predicting the negative ones.

in order to evaluate the performance of the proposed nhmc algorithm, we compare it to clus-hmc  which takes into account the attributes, as well as the hierarchical organization of classes, but does not consider network information. we also compare nhmc with the functionalflow 
 <cit>  and hopfield 
 <cit>  approaches, which exploit the network information, but consider neither the attributes nor the hierarchical organization of classes. we report the results of nhmc with α =  <dig>  when it uses only autocorrelation as a heuristic, and with α =  <dig> , when it equally weights variance reduction and autocorrelation within the heuristic.

results for go hierarchical multi-label classification
for each of the datasets, we report in table
 <dig> the
auprc¯ results obtained with nhmc , clus-hmc , ff and h . two variants of each dataset are considered, one with all genes and the other with the subset of highly connected genes . furthermore, results for dip and biogrid are presented. for all genes, we also report 3-fold cross-validation
auprc¯ results in table
 <dig> 

we use the 2/3-1/ <dig> training-testing evaluation schema. we report the
auprc¯ of the clus-hmc , nhmc , functionalflow , and hopfield  methods, when predicting gene function in yeast using go annotations. we use  <dig> yeast  datasets . we consider all genes. results for two ppi networks  are presented.

we use the 3-fold cross-validation evaluation schema. the averageauprc¯  of the clus-hmc , nhmc , functionalflow , and hopfield  methods, when predicting gene function in yeast using go annotations. we use  <dig> yeast  datasets. results for two ppi networks  are presented.

on the datasets with all genes, the best results are overall obtained by nhmc with α =  <dig>  for the dip network, there is no clear difference between nhmc with α =  <dig> and nhmc with α =  <dig> . note that in the dip network only a half of the genes have at least one connection to other genes.

on average, nhmc outperforms clus-hmc. the difference in performance is especially notable for some datasets, i.e., cellcycle when using the dip network and derisi, eisen and gasch <dig> when using the biogrid network. this indicates that while some form of autocorrelation on the go labels is present in both networks , they provide different information. exceptions are the struc and hom datasets. a possible explanation can be the high number of attributes, which may provide information redundant with respect to the information provided by the ppi networks. in this case, nhmc encounters the curse of dimensionality phenomenon
 <cit> .

the advantage of nhmc over clus-hmc comes from the simultaneous use of the hierarchy of classes and the ppi information in protein function prediction. it confirms the benefits coming from the consideration of autocorrelation during the learning phase. the tree structure of the learned models allows nhmc to consider different effects of autocorrelation at different levels of granularity. all these considerations are valid for both evaluation schemata we use, that is, the 2/3-1/ <dig> training-testing split and the 3-fold cross-validation; schema.

the results obtained by using two functional linkage network  based algorithms, i.e., functionalflow 
 <cit>  and hopfield 
 <cit> , are comparable between them, but are not comparable with those obtained by nhmc and clus-hmc. this is due to the different classification problem considered by ff and h, which does not take into account the attributes or the hierarchy of classes. thus, nhmc and clus-hmc obtain far better accuracies than ff and h.

as expected, when we use only highly connected genes for both training and testing, we obtain better performance. to investigate this effect in more detail, in figure
 <dig> we present the
auprc¯ results obtained by using clus-hmc and nhmc  for predicting go annotations in the gasch <dig> ) and cellcycle ) datasets. the graphs depict the performance of the two models learned from highly connected genes from each dataset, for different portions of the genes from the testing data, ordered by the minimum number of connections of the gene in the dip ppi network. both clus-hmc and nhmc perform better when tested on highly connected genes only  and
2) as compared to being tested on all genes . for both datasets, nhmc clearly performs better than clus-hmc for all subsets of the testing set. the difference in performance is less pronounced when all genes are considered , but becomes clearly visible as soon as the genes with no or few connections are excluded, and are most pronounced for the most connected genes . note that no network information is used by nhmc about the testing set. this means that network information from the training set is sufficient to obtain good predictive models.
auprc¯ learned by clus-hmc and nhmc  from the most connected subsets of genes from the  gasch <dig> and  cellcycle datasets annotated with labels from the go hierarchy. the horizontal axis gives the minimum relative number  of interactions a gene must have in the dip ppi network to be included in the testing data, whereas the vertical axis gives the model performance on the testing data in terms of the
auprc¯ values. at the far right , we have the performance on the most-highly connected genes from the test set. at the far left , we have the performance on all genes for the testing set.

after considering the accuracy on the subset of highly connected genes , we turn our attention to the accuracy on the subset of weakly connected genes. this subset contains all genes that are not highly connected, i.e., have less than  <dig> interactions in the ppi network. in table
 <dig>  we present the predictive performance of the models trained on the subset of highly connected genes and tested on the subset of remaining genes.

we report the
auprc¯ of the clus-hmc , nhmc , functionalflow , and hopfield  methods, when predicting gene function in yeast, using go annotations and the biogrid ppi network. the models are trained on the subset of highly connected genes and tested on the subset of weakly connected genes.

as expected, with a threshold of  <dig> connections  the predictive accuracy decreases for all methods/models, but the reduction is the smallest for nhmc  and the largest for the fln-based algorithms . nhmc does not use the network information in the testing phase and its predictions are not affected much by this scenario, whereas the fln-based algorithms are highly dependent on the network information and their accuracy decreases drastically. moreover, the results obtained by using nhmc are better than those of clus-hmc on the subset of weakly connected genes. this means that nhmc can build better models because it uses both the hierarchy of classes and the network information, as compared to the models built by only using the hierarchy of classes  or by only using the network information . in the case of a threshold of  <dig> connections, we have better results both for clus-hmc and nhmc. however, in this case, the advantage provided by the network information does not allow nhmc to outperform clus-hmc in all the datasets and the accuracies obtained by the two systems are very similar. this is mainly due to the noise present in the ppi networks  which works against the beneficial effect of the networkc.

a different perspective of the results is presented in table
 <dig>  where we compare the use of binary weights  with the use of a simple weighting that considers the number of times that an interaction is identified in the experiments. results show that non-binary weighting is, in general, not beneficial. this behavior is observed not only for nhmc but also for ff and h. a possible explanation can be found in the quality of the weights which, according to the way they are generated, may introduce noise.

binary vs. weighted connections:
auprc¯ of nhmc  with weighted distances in the biogrid ppi network.

a final remark concerns the sensitivity of nhmc to the presence of highly redundant features and how nhmc works in combination with a feature selection algorithm. with this goal in mind, we have used a spectral feature selection algorithm
 <cit>  with a normalized cut to select the 15%, 10%, 5% and 1% top ranked features for the two datasets characterized by a very large number of attributes, that is, struc and homo . the results obtained after learning models with reduced sets of features are reported in table
 <dig>  they essentially show that nhmc is not affected by the high number of features. this can be explained by the top-down tree induction approach, which, at each internal node of the tree, selects the best attribute to be considered in the test and ignores the others, thus implicitly implementing an embedded feature selection
 <cit>  algorithm.

auprc¯ of nhmc  with biogrid ppi network for struc and homo datasets, when working on all the features, top 15% of the features, top 10% of the features, top 5% of the features and top 1% of the features. the datasets struc and homo are chosen because of their very high number of features as compared to the other datasets.

results for fun hierarchical multi-label classification
in addition to the experiments with go annotations, we also perform experiments with fun annotations. the results are reported in additional file
 <dig> and summarized here. they agree with results obtained with go annotations, apart for the fact that when using all genes, nhmc performs comparably to clus-hmc. when working with highly connected genes, nhmc yields very good results, mainly better than clus-hmc, independently of the considered network.

we also compare the results of nhmc using fun annotations to the results of additional methods from other studies. in particular, we compare our results to the results of three recent bio-inspired strategies which work in the hmc setting, but do not consider network information. the three methods are artificial neural networks , ant colony optimization , and a genetic algorithm for hmc 
 <cit> . while the first algorithm is a 1-vs-all  method based on artificial neural networks trained with the back-propagation algorithm, the latter two are methods that discover hmc rules. the algorithms are evaluated on  <dig> yeast fun annotated datasets
 <cit> , using the same experimental setup we use for clus-hmc and nhmc, i.e., the setup proposed by vens et al.
 <cit> .

in additional file
 <dig>  we present the performance  obtained by using hmc-ga, hmc-lmlp, hmant-miner and nhmc  on  <dig>  fun annotated datasets. nhmc outperforms all other methods by a wide margin. an exception is only the church dataset, for which nhmc performs worse than hmant-miner. note that the
auprc¯ <cit>  measure used in this comparison is similar to
auprc¯, but uses weights that consider the number of examples in each class. we use
auprc¯ here to make our results easily comparable to the results obtained with the other three methods: the study by cerri et al.
 <cit>  only gives their results in terms of
auprc¯.

using different ppi networks within nhmc
although all ppi networks are frequently updated and maintained, many works have pointed out that the ppi networks are also very noisy . in the following, we argue that nhmc can be a valid tool for assessing the quality of network data in the context of exploiting information coming from ppi networks for gene function prediction. before we compare the results obtained by nhmc using different ppi networks, we discuss some functional and topological properties of the  <dig> considered yeast ppi networks: dip and biogrid.

table
 <dig> shows the percentage of proteins that are covered  by each of the ppi networks. while biogrid has almost complete coverage, within the other network  only half of the proteins interact with other proteins. next, table
 <dig> shows the percentage of function-relevant interactions. an interaction is considered to be function-relevant  if the two proteins involved in the interaction have at least one function  in common. in the  <dig> networks and across the  <dig> datasets, the degree of function relevance varies widely, i.e, 2%–33% of the observed interactions are function relevant. however, a closer look at the statistics reveals that the connections are much more function-relevant with respect to go annotations than with respect to fun annotations. this is largely due to the fact that go contains a much larger number of functions. in addition, table
 <dig> gives the average degree of a node, i.e., the average number of neighbors that a node has in the network.

the percentage of connected genes, the percentage of function-relevant interactions and the average degree of nodes for  <dig> different ppi networks , we see that dip leads to higher
auprc¯ results. the "% of function-relevant interactions" is on average higher for dip, even though the number of connected genes in biogrid is twice as high as the one in dip . this indicates that biogrid, although denser than dip, does not provide the same quantity/type of information for the gene function prediction task as dip. similar conclusions hold for predicting gene functions within the fun annotation scheme, as evident from additional file
 <dig> 

finally, comparing the results in tables
 <dig> and
 <dig> and the table reported in additional file
 <dig>  we see that, although for go we have a higher "% of function-relevant interactions" in both the dip and the biogrid networks, the learning task for go is more complex than that for fun. this is primarily due to the significantly higher number of classes in go. this explains the better
auprc¯ values for fun in comparison to those for go, when comparing the results of nhmc on the same dataset and using the same network.

related work
many machine learning approaches that tackle the problem of protein function prediction have been proposed recently . a review of the plethora of existing methods can be found in
 <cit> . in this section, we will only discuss related works which are close to ours along two dimensions: using hierarchical annotations
 <cit>  and using network information
 <cit> .

hmc for predicting gene function
our work builds on the foundations by vens et al.
 <cit> , where the hierarchical constraint is enforced by the algorithm clus-hmc that learns predictive clustering trees  for hmc. recently, cerri et al.
 <cit>  applied a genetic algorithm  to solve the hmc problem. in their method, the antecedents of decision rules evolve with a biased fitness function towards rules with high example coverage. the method also removes from the training set examples already covered by the generated rules. valentini
 <cit>  developed the true-path rule  ensemble learner for genome-wide gene function prediction, where positive  probabilistic predictions for a node transitively influence the ancestors  of the node. while
 <cit>  and
 <cit>  ignore information coming from relationships among examples,
 <cit>  exploits ppi networks. however, the considered information is limited to binary  attributes, which describe the interaction of a gene with specific other genes.

using ppi networks in predicting gene function
recent reviews of the latest techniques that use ppi data for protein function prediction
 <cit>  suggest that it is possible to distinguish two major approaches. the first one explores direct annotation schemes and infers the function of a protein based on its connections in the network. the second one, module-assisted, first identifies modules of related proteins and then annotates each module. the first approach is followed by letovsky and kasif
 <cit> , who apply a markov random field model. there, a node’s label probability is entirely a function of its neighbors’ states. in addition, karaoz et al.
 <cit>   presented a functional linkage network  based algorithm , inspired by discrete-state hopfield networks as used in physics, for predicting the functions of genes. the method constructs a graph, whose nodes are genes and edges connect genes that may share the same function, by integrating gene expression data, protein-protein interactions and protein-dna binding data. functionalflow
 <cit>  generalizes the guilt-by-association principle to groups of proteins that may interact with each other physically. the algorithm annotates nodes as an infinite reservoir of functional flow. initially, each node with known go functional annotation is a "source" for that function. in each round, "function" flows along the weighted edges of the graph. in nariai et al.
 <cit> , nodes in the graph are genes and edges represent evidence for functional similarity based on gene expression data, protein-protein interactions and protein-dna binding data. some form of autocorrelation, limited to directly connected nodes, is considered. however, the above approaches
 <cit>  do not consider the hierarchy of categories.

the second approach attempts to define the relationship between the ppi network topology and biological protein function. milenkovic and przulj
 <cit>  relate the ppi network structure to protein complexes. they group proteins by considering local topology of the ppi network and show that these protein groups belong to the same protein complexes, perform the same functions, are localized in the same compartments, and have the same tissue expressions. the work by borgwardt et al.
 <cit>  uses graph kernels that measure the similarity between graphs and learns a support vector machine classifier for protein function prediction. the graph model combines sequential, structural and chemical information about proteins. gillis and pavlidis
 <cit>  recommend to test the effect of critical edges  when assessing network quality using gba-like approaches. tao et al.
 <cit>  modified the traditional k-nn classification algorithm to consider the semantic similarity between functional classes when predicting the functions of genes based on go annotations. pandey et al.
 <cit>  used the same algorithm and the same similarity measure, but using a different definition that also includes the similarity between the sets of functional labels of two proteins. the difference between these two approaches
 <cit>  and our approach is that they use go to identify the distances, while we use ppi networks to identify relationships and classify genes according to go.


what distinguishes our work from related work as discussed above, many approaches exist for gene function prediction, some take into account ppi networks and some take into account the hierarchical organization of annotation schemes. most approaches in the first category do not consider the hierarchical constraint directly, but rather in a post-processing phase. most approaches in the second category do not explicitly consider the effect of network autocorrelation of gene function. moreover, none of them, takes into account simultaneously the hierarchical constraint and network autocorrelation in predicting gene function.

in contrast to related works, in general and the related work described above in particular, our approach considers both the network autocorrelation that arise in the ppi networks and the hierarchical organization of the annotation schemes. in general, our approach nhmc performs better than the approaches that consider only the network or only the hierarchy of classes. this was clearly demonstrated above through the empirical comparison of nhmc with functionalflow and hopfield algorithms, on one hand, and with clus-hmc, hmc-ga, hmc-lmlp and hmant-miner on the other hand.

moreover, the network setting that we use in this work is different from that used in other studies where the network is not considered at all  or is considered in tight connection to the data so that predictions can be made only for genes for which interactions with other genes are known. in our approach, the network structure is considered only during the training phase  and is disregarded during the testing phase. this key feature of the proposed approach is especially attractive from a practical perspective when the function we want to predict of new genes for which interactions with other genes may exist, but are not known or still need to be confirmed.

CONCLUSIONS
in this work, we address the problem of learning to predict gene/protein function by exploiting their individual properties as well as their interactions . in contrast to most existing approaches, which use only one of the two sources , we use both. moreover, we only use the network information in the training phase and can thus make predictions for genes/proteins whose interactions are yet to be investigated.

we view the problem of gene/protein function predictions as a problem of hierarchical multi-label classification, where instances may belong to multiple classes and the relationships between the classes are hierarchical. we also consider relations among the instances, i.e., interactions of the proteins within a ppi network: these relations introduce autocorrelation and violate the assumption that instances are independently and identically distributed , which underlines most machine learning algorithms. while the consideration of these relations introduces additional complexity into the learning process, it can also bring substantial benefits.

the major contributions of our paper are as follows. first, we formulate the problem of hierarchical multi-label classification in a network setting: to the best of our knowledge, the hmc task of structured-output prediction has not been considered in a network setting before. second, we define the notion of autocorrelation for such a setting and introduce an appropriate measure of network autocorrelation for hmc. third, we develop a machine learning method for solving the task of hmc in a network setting, which successfully exploits the network information  during the learning phase.

finally, we perform an extensive empirical evaluation of the proposed machine learning method for hmc in a network setting on the task of predicting yeast gene/protein function in a ppi network context. we use a variety of datasets describing genes in different ways, two functional annotation schemes  and two different ppi networks . in sum, the results of the evaluation show that our method, which uses both gene/protein properties and network information, yields better performance than the methods using each of these two sources separately: overall, this holds across the different gene descriptions, annotation schemes, and different networks. the benefits of using the network information are diminished  when the genes/proteins are described with a very large number of features, i.e., under the curse of dimensionality.

more specifically, the properties of the ppi network used have a strong influence on the overall performance. best results are achieved when the ppi network is reasonably dense and contains a large proportion of function-relevant interactions. the dip and biogrid networks rate best in this respect and lead to most notable improvements in performance when used within our method. it is worth noting here that networks  are nowadays often tuned for function-relevance with respect to the commonly used gene ontology  annotation scheme.

finally, note that the use of network information improves the accuracy of gene function prediction not only for highly connected genes, but also for genes with only a few connections . note also that we do not need information on the network around a gene when we want to predict the function for a new gene: this is important for genes that are not well known . in such a case, we can expect better predictions from the models learned by our approach, regardless of whether the gene is well connected or only weakly connected to other genes and regardless of the fact whether its connectivity is known or unknown.

we will explore several avenues for development and evaluation of our approach in further work. in terms of development, we plan to consider different ways for combining variance reduction and autocorrelation within the search heuristic used in our approach. in terms of evaluation, we plan to use additional datasets and networks. this will include new datasets for organisms other that yeast and networks based on sequence similarity  among genes, as well as more recent function labels for the presently considered datasets. in the context of the latter, we will consider additional networks with non-binary weights that reflect the strength of the connections within the network.

availability of supporting data
project name: nhmc

project home page:http://kt.ijs.si/daniela_stojanova/nhmc/

available resources: nhmc software and user manual, and the datasets.

endnotes
a note that the tree structure induced by clus-hmc does not directly reflect the hierarchy of classes. for this reason, in this paper, we will distinguish between the terms hierarchy  and tree .

b the function labels were downloaded in  <dig>  on one side, this facilitates comparison with previous work. on the other side, it is possible that results with new labels would be slightly different.

c obviously, the considered case is an extreme situation where the evaluation suffers from the non-random distribution of the examples.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
sd conceived of the study, which was primarily carried out by ds and mc. ds designed and implemented nhmc, while ds and mc carried out the experimental evaluation of nhmc and other methods on the different tasks of gene function prediction. ds, mc, sd and dm contributed to the manuscript drafting and finalization. mc, sd and dm participated in the design of the study. sd and dm participated in the biogrid: a general repository for interaction datasets  <dig> coordination of the study. all authors read and approved the final manuscript.

supplementary material
additional file 1
algorithm 2: pseudo-code of the clus-hmc algorithm for top-down induction of hmc trees.

click here for file

 additional file 2
geary’s c for spatial regression.

click here for file

 additional file 3
the performance of the models for predicting fun annotations.

click here for file

 additional file 4
the performance of nhmc and other methods in predicting fun annotations of yeast genes.

click here for file

 acknowledgements
ds and sd were supported by the following institutions and grants: the slovenian research agency , the european commission , and the operation no. op <dig> . <dig> . <dig>  financed by the european regional development fund  and the ministry of education, science, and sport of slovenia . sd, mc and dm were supported by the eu-funded project maestra . the authors thank hossein rahmani who provided some datasets and fran supek who provided useful feedback.
