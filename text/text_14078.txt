BACKGROUND
g protein-coupled receptors , also known as  <dig> α-helices transmembrane receptors due to their characteristic configuration of an anticlockwise bundle of  <dig> transmembrane α helices  <cit> , are one of the largest superfamily of membrane proteins and play an extremely important role in transducing extracellular signals across the cell membrane via guanine-binding proteins  with high specificity and sensitivity  <cit> . gpcrs regulate many basic physicochemical processes contained in a cellular signaling network, such as smell, taste, vision, secretion, neurotransmission, metabolism, cellular differentiation and growth, inflammatory and immune response  <cit> . for these reasons, gpcrs have been the most important and common targets for pharmacological intervention. at present, about 30% of drugs available on the market act through gpcrs. however, detailed information about the structure and function of gpcrs are deficient for structure-based drug design, because the determination of their structure and functional using experimental approach is both time-consuming and expensive.

as membrane proteins, gpcrs are very difficult to crystallize and most of them will not dissolve in normal solvents  <cit> . accordingly, the  <dig> d structure of only squid rhodopsin, β <dig>  β <dig> adrenergic receptor and the a2a adenosine receptor have been solved to data. in contrast, the amino acid sequences of more than  <dig> gpcrs are known with the rapid accumulation of data of new protein sequence produced by the high-throughput sequencing technology. in view of the extremely unbalanced state, it is vitally important to develop a computational method that can fast and accurately predict the structure and function of gpcrs from sequence information.

actually, many predictive methods have been developed, which in general, can be roughly divided into three categories. the first one is proteochemometric approach developed by lapinsh  <cit> . however, the methods need structural information of organic compounds. the second one is based on similarity searches using primary database search tools  and such database searches coupled with searches of pattern databases   <cit> . however, they do not seem to be sufficiently successful for comprehensive functional identification of gpcrs, since gpcrs make up a highly divergent family, and even when they are grouped according to similarity of function, their sequences share strikingly little homology or similarity to each other  <cit> . the third one is based on statistical and machine learning method, including support vector machines   <cit> , hidden markov models   <cit> , covariant discriminant   <cit> , nearest neighbor   <cit>  and other techniques  <cit> .

among them, svm that is based on statistical learning theory has been extensively used to solve various biological problems, such as protein secondary structure  <cit> , subcellular localization  <cit> , membrane protein types  <cit> , due to its attractive features including scalability, absence of local minima and ability to condense information contained in the training set. in svm, an initial step to transform protein sequence into a fixed length feature vector is essential because svm can not be directly applied to amino acid sequences with different length. two commonly used feature vectors to predict gpcrs functional classes are amino acid composition  and dipeptide composition   <cit> , where every protein is represented by  <dig> or  <dig> discrete numbers. obviously, if one uses aac or dipc to represent a protein, many important information associated with the sequence order will be lost. to take into account the information, the so-called pseudo amino acid composition  was proposed  <cit>  and has been widely used to gpcrs and other attributes of protein studies  <cit> . however, the existing methods were established only based on a single feature-set. and, few works tried to research the relationship between features and the functional classes of protein  <cit> , or to find the informative features which contribute most to discriminate functional types. karchin et al  <cit>  also indicated that the performance of svm could be further improved by using feature vector that posses the most discriminative information. therefore, feature selection should be used for accurate svm classification.

feature selection, also known as variable selection or attribute selection, is the technique commonly used in machine learning and has played an important role in bioinformatics studies. it can be employed along with classifier construction to avoid over-fitting, to generate more reliable classifier and to provide more insight into the underlying causal relationships  <cit> . the technique has been greatly applied to the field of microarray and mass spectra  analysis  <cit> , which has a great challenge for computational techniques due to their high dimensionality. however, there is still few works utilizing feature selection in gpcrs prediction to obtain the most informative features or to improve the prediction accuracy.

so, a new predictor combining feature selection and support vector machine is proposed for the identification and classification of gpcrs at the three levels of superfamily, family and subfamily. in every level, minimum redundancy maximum relevance   <cit>  is utilized to pre-evaluate features with discriminative information. after that, to further improve the prediction accuracy and to obtain the most important features, genetic algorithms   <cit>  is applied to feature selection. finally, three models based on svm are constructed and used to identify whether a query protein is gpcr and which family or subfamily the protein belongs to. the prediction quality evaluated on a non-redundant dataset by the jackknife cross-validation test exhibited significant improvement compared with published results.

methods
dataset
as is well-known, sequence similarity in dataset has an important effect on the prediction accuracy, i.e. accuracy will be overestimated when using high similarity protein sequence. thus, in order to disinterestedly test current method and facilitate to compare with other existing approaches, the dataset constructed by xiao  <cit>  is used as the working dataset. the similarity in the dataset is less than 40%. the dataset contains  <dig> protein sequences that can be classified into two parts:  <dig> non-gpcrs and  <dig> gpcrs. the  <dig> gpcrs can be divided into  <dig> families:  <dig> rhodopsin-like,  <dig> metabotropic glutamate/pheromone,  <dig> secretin-like,  <dig> fungal pheromone,  <dig> frizzled/smoothened and  <dig> camp receptor. for rhodopsin-like of gpcrs, we further partitioned into  <dig> subfamilies based on gpcrdb   <cit> , including  <dig> amine,  <dig> peptide,  <dig> hormone,  <dig> rhodopsin,  <dig> olfactory,  <dig> prostanoid,  <dig> nucleotide,  <dig> cannabinoid,  <dig> platelet activating factor,  <dig> gonadotropin-releasing hormone,  <dig> thyrotropin-releasing hormone & secretagogue,  <dig> melatonin,  <dig> viral,  <dig> lysosphingolipid,  <dig> leukotriene b <dig> receptor and  <dig> orphan. those subfamilies, which the number of proteins is lower than  <dig>  are combined into a class, because they contain too few sequences to have any statistical significance. so,  <dig> classes  are obtained at subfamily level.

protein represent
in order to fully characterize protein primary structure,  <dig> feature vectors are employed to represent the protein sample, including aac, dipc, normalized moreau-broto autocorrelation , moran autocorrelation , geary autocorrelation , composition , transition , distribution   <cit> , composition and distribution of hydrophobicity pattern . here  <dig> and  <dig> amino acid properties extracted from aaindex database  <cit>  are selected to compute autocorrelation, c, t and d features, respectively. the properties and definitions of amino acids attributed to each group are shown in additional file  <dig> and  <dig> 

according to the theory of lim  <cit> ,  <dig> kinds of hydrophobicity patterns include: , , , ,  and . the patterns  and  often appear in the β-sheets while the patterns ,  and  occur more often in α-helices. the pattern  is an extension of the concept of the "helical wheel" or amphipathic α-helix  <cit> . seven kinds of amino acids, including cys , phe , ile , leu , met , val  and trp , may occur in the  <dig> patterns based on the observed of rose et al  <cit> . because transmembrane regions of membrane protein are usually composed of β-sheet and α-helix, chp and dhp are used to represent protein sequence.

for the pattern , the chp is computed by eq. :   

where, n is the number of pattern in position i and i+ <dig> that simultaneously belong to any of  <dig> kinds of amino acids, and l is the sequence length. other chp are calculated by using the rule mentioned above. the dhp of pattern , which describes the distribution of the pattern in protein sequence, can be calculated according to eq. :   

where, s is a feature vector composing of  <dig> values that are the position in the whole sequence for the first pattern , 25% patterns , 50% patterns , 75% patterns and 100% patterns , respectively. how to calculate these values is explained below by using a random sequence with  <dig> amino acids, as shown in figure  <dig>  which consists of  <dig> patterns . the  <dig> patterns  included, cal , iqf , fkm , mdv , ctf , fyl , cfm , fmi , iri , caw . the first pattern  is in the pattern position of  <dig>  the pattern  of  is in the pattern position of  <dig> . the pattern  of  is in the pattern position of  <dig> . the pattern  of  is in the pattern position of  <dig> . the pattern  of  is in the pattern position of  <dig> . the first letter of the  <dig> patterns  are c, f, c, f, c, which is corresponding to the residue position of  <dig>   <dig>   <dig>   <dig>  and  <dig> in the sequence, respectively. thus, s =  <cit> . similarly, the dhp for pattern other than  is also calculated by using the rule.

the optimized feature subset selection
svm is one of the most powerful machine learning methods, but it cannot perform automatic feature selection. to overcome this limitation, various feature selection methods were introduced  <cit> . feature selection methods typically were divided into two categories: filter and wrapper methods. although filter methods are computationally simple and easily scale to high-dimensional dataset, they ignore the interaction between selected feature and classifier. in contrast, wrapper approaches include the interaction and can also take into account the correlation between features, but they have a higher risk of overfitting than filter techniques and are very computationally intensive, especially if building the classifier has a high computational cost  <cit> . considering the characteristics of the two methods, the mrmr belonging to filter methods is used to preselect a feature subset, and then ga belonging to wrapper methods is utilized to obtain the optimized feature subset.

minimum redundancy maximum relevance 
the mrmr method tries to select a feature subset, each of which has the maximal relevance with target class and the minimal redundancy with other features. the feature subset can be obtained by calculating the mutual information between the features themselves and between the features and the class variables. in the current study, feature is a vector contains  <dig> type descriptors values of proteins . for binary classification problem, classification variable lk ∈ <dig> or  <dig>  the mutual information mi of between two features x and y is computed by eq.:   

where, p is joint probabilistic density, p and p is marginal probabilistic density.

similarly, the mutual information mi of between classification variable l and feature x is also calculated by eq.:   

the minimum redundancy condition is to minimize the total redundancy of all features selected by eq.:   

where, s denoted that the feature subset, and |s| is the number of feature in s.

the maximum relevance condition is to maximize the total relevance between all features in s and classification variable. the condition can be obtained by eq. :   

to achieve feature subset, the two conditions should be optimized simultaneously according to eq. :   

if continuous features exist in feature set, the feature must be discretized by using "mean ± standard deviation/2" as boundary of the  <dig> states. the value of feature larger than "mean + standard deviation/2" is transformed to state 1; the value of feature between "mean - standard deviation/2" and "mean + standard deviation/2" is transformed to state 2; the value of feature smaller than "mean - standard deviation/2" is transformed to state  <dig>  in this case, computing mutual information is straightforward, because both joint and marginal probability tables can be estimated by tallying the samples of categorical variables in the data  <cit> . more explanation about the calculation of probability can be seen from additional file  <dig>  detailed depiction of the mrmr method can be found in reference  <cit> , and mrmr program can be obtained from http://penglab.janelia.org/proj/mrmr/index.htm

genetic algorithms 
ga can effectively search the interesting space and easily solve complex problems without requiring a prior knowledge about the space and the problem. these advantages of ga make it possible to simultaneously optimize the feature subset and the svm parameters. the chromosome representations, fitness function, selection, crossover and mutation operator in ga are described in the following sections.

chromosome representation
the chromosome is composed of decimal and binary coding systems, where binary genes are applied to the selection of features and decimal genes are utilized to the optimization of svm parameters.

fitness function
in this study, two objectives must be simultaneously considered when designing the fitness function. one is to maximize the classification accuracy, and the other is to minimize the number of selected features. the performances of these two objectives can be evaluated by eq. :   

where, svm _ accuracy is svm classification accuracy, n is the number of selected features, n is the number of overall features.

selection, crossover and mutation operator
elitist strategy that guarantees chromosome with the highest fitness value is always replicated into the next generation is used to select operation. once a pair of chromosome is selected for crossover, five random selected positions are assigned to the crossover operator of the binary coding part. the crossover operator was determined according to eq.  and  for the decimal coding part, where p is the random number of .      

the method based on chaos  <cit>  is applied to the mutation operator of decimal coding. mutation to the part of binary coding is the same as traditional ga.

the population size of ga is  <dig>  and the termination condition is that the generation numbers reach  <dig>  a detailed depiction of the ga can be reference to our previous works  <cit> .

model construction and assessment of performance
for the present svm, the publicly available libsvm software  <cit>  is used to construct the classifier with the radial basis function as the kernel. ten-fold cross-validation test is used to examine a predictor for its effectiveness. in the 10-fold cross-validation, the dataset is divided randomly into  <dig> equally sized subsets. the training and testing are carried out  <dig> times, each time using one distinct subset for testing and the remaining  <dig> subsets for training.

classifying gpcrs in superfamily level can be formulated as a binary classification problem, namely each protein can be classified as either gpcrs or non-gpcrs. so, the performance of classifier are measured in terms of sensitivity , specificity , accuracy  and matthew's correlation coefficient   <cit> , and are given by eqs. -.            

here, tp, tn, fp and fn are the numbers of true positives, true negatives, false positives and false negatives, respectively.

the classification of gpcrs into its families and subfamilies is a multi-class classification problem, namely a given protein can be classified into specific family or subfamily. the simple solution is to reduce the multi-classification to a series of binary classifications. we adopted the one-versus-one strategy to transfer it into a series of two-class problems. the overall accuracy  and accuracy  for each family or subfamily calculated for assessment of the prediction system are given by eqs. -.      

where, n is the total number of sequences, obs is the number of sequences observed in class i, p is the number of correctly predicted sequences of class.

the whole procedure for recognizing gpcrs form protein sequences and further classifying gpcrs to family and subfamily is illustrated in figure  <dig>  and the steps are as follows:

step  <dig>  produce various feature vectors that represent a query protein sequence.

step  <dig>  preselect a feature subset by running mrmr. select an optimized feature subset from the preselect subset by ga and svm. predict whether the query protein belong to the gpcrs or not. if the protein is classified into non-gpcrs, stop the process and output results, otherwise, go to the next step.

step  <dig>  preselect again a feature subset and further select an optimized feature subset. predict which family the protein belongs. if the protein is divided into non-rhodopsin like, stop the process with the output of results, otherwise, go to the next step.

step  <dig>  preselect a feature subset again and select an optimized feature subset. predict which subfamily the protein belongs to.

RESULTS
identification a gpcr from non-gpcr
at the first step of feature selection, only  <dig> different feature subsets are selected based on mrmr due to our limited computational power, and the feature subsets contains  <dig>   <dig>   <dig>  ......,  <dig>   <dig> and  <dig> features respectively. the performance of various feature subsets for discriminating between gpcrs and other protein is investigated based on grid search for maximal 10-fold cross-validation tested accuracy with γ ranging among 2- <dig>  2- <dig> ...,  <dig>  and c ranging among 2- <dig>  2- <dig> ...,  <dig> , and the results are shown in figure  <dig>  the accuracy for a single feature is  <dig> %. and the accuracy dramatically increased when the number of features increased from  <dig> to  <dig>  and achieved the highest values  while the feature subset consists of  <dig> features. however, the accuracy did not change dramatically when the number of features increased to  <dig> 

although the highest accuracy can be obtained by using the feature subset with  <dig> features, many features impede the discovery of physicochemical properties that affect the prediction of gpcrs. so, we perform further ga for the preselecting feature subset that consists of  <dig> features. figure  <dig> and figure  <dig> illustrate the convergence processes for ga to select feature subset. initially, approximate  <dig> features are selected by ga and a predictive accuracy about  <dig> % is achieved based on 10-fold cross-validation tested. along with the implementation of ga, the number of selected features gradually decreased while fitness is improved. finally, the good fitness, high classification accuracy  and optimized feature subset  can be obtained from about  <dig> generations. consequently, the optimal classifier at superfamily level is constructed with the optimal feature subset.

the results of the optimized features subset are shown in figure  <dig>  the optimized features subset contains  <dig> features, including  <dig> feature of cysteine composition;  <dig> features of dipc based on phe-phe, gly-glu acid, his-asp, ile-glu, asn-ala, asn-met and ser-glu;  <dig> feature of c based on polarity grouping;  <dig> features of t based on hydrophobicity and buried volume grouping;  <dig> features of d based on charge, hydrophobicity, van der waals volume, polarizability and solvent accessibility grouping;  <dig> features of nmbauto based on hydrophbicity, flexibility, residue accessible surface area and relative mutability;  <dig> features of mauto based on hydrophobicity, flexibility, residue volume, steric parameter and relative mutability;  <dig> features of gauto based on hydrophobicity and free energy;  <dig> features of dhp based on pattern . the results suggest that the order of these feature groups that contributed to the discrimination gpcrs from non-gpcrs is: mauto > dipc and d > nmbauto > t, gauto and dhp > aac and c.

recognition of gpcr family
following the same steps described above, the quality of various feature subsets are investigated at family level based on grid search and 10-fold cross-validation tested. the relationship between number of feature and overall accuracy is shown in figure  <dig>  a significant increase in overall accuracy can be observed when the number of feature increased from  <dig> to  <dig>  and the highest overall accuracy of  <dig> % can be achieved.

we also further perform ga for preselecting feature subset with  <dig> features to acquire an optimized feature subset. the processes of optimization are displayed in figure  <dig> and figure  <dig>  it can be observed that the number of features dramatically decreased from  <dig> to  <dig> when the number of generation increased from  <dig> to  <dig>  and the best fitness and highest overall accuracy of  <dig> % can be achieved. so, the optimal classifier with  <dig> features is used to construct classifier at family level.

the results of the optimized feature subset are also shown in figure  <dig>  the optimized features subset contains  <dig> aac,  <dig> dipc,  <dig> d,  <dig> nmbauto,  <dig> mauto,  <dig> gauto and  <dig> dhp features. the results reveal that the order of these feature groups that contributed to the classification gpcrs into  <dig> families is: mauto > dipc > d > nmbauto > dhp > aac and gauto.

classification of gpcr subfamily
because knowledge of gpcrs subfamilies can provide useful information to pharmaceutical companies and biologists, the identification of subfamilies is a quite meaningful topic in assigning a function to gpcrs. therefore, we constructed a classifier at subfamily level to predict the subfamily belonging to the rhodopsin-like family. rhodopsin-like family is considered because it covers more than 80% of sequences in the gpcrdb database  <cit> , and the number of other family in current dataset is too few to have any statistical significance. similarly, we also study the quality of various feature subsets from mrmr based on grid search and 10-fold cross-validation tested. the correlation between number of features and overall accuracy is also illustrated in figure  <dig>  overall accuracy enhanced when the number of features increased from  <dig> to  <dig>  and the highest overall accuracy of  <dig> % can be obtained by using the feature subset with  <dig> features.

in order to get an optimized feature subset, ga is further applied to further feature selection from a preselected feature subset with  <dig> features. the processes of convergence are shown in figure  <dig> and figure  <dig>  the number of features in optimized feature subset significantly decreased from  <dig> to  <dig> when the number of generation increased from  <dig> to  <dig>  and corresponding fitness value is significantly increased. subsequently, the number of features and fitness value maintained invariable. it clearly shows a premature convergence. however, the number of features decreased from  <dig> to  <dig> when the number of generation increased from  <dig> to  <dig>  indicating ga has ability to escape from local optima. the finally optimized feature subset with  <dig> features can be obtained within  <dig> generations. therefore, we developed a classifier by the features from the optimized feature subset for classifying the subfamilies of the rhodopsin-like family.

the composition of optimized feature subset is shown in figure  <dig>  the optimized feature subset contains  <dig> aac,  <dig> dipc,  <dig> c,  <dig> d,  <dig> nmbauto,  <dig> mauto,  <dig> gauto,  <dig> chp and  <dig> dhp features. the results suggest that the order of these feature groups that contributed to the prediction subfamily belonging to the rhodopsin-like family is: mauto > nmbauto > dipc > d and gauto > dhp > aac and c > chp.

comparison with gpcr-ca
to facilitate a comparison with gpcr-ca method developed by xiao  <cit> , we perform jackknife cross-validation test based on the current predictor. gpcr-ca is a two-layer classifier that is used to classify at the levels of superfamily and family, respectively, and each protein is characterized by pseaa, which is based on "cellular automation" and gray-level co-occurrence matrix factors. in the jackknife test, each protein in the dataset is in turn singled out as an independent test sample and all the rule-parameters are calculated without using this protein. the results of jackknife test obtained with proposed method in comparison with gpcr-ca are listed in table  <dig> and figure  <dig>  the performances of the proposed predictor  in predicting the subfamilies are summarized in table  <dig> 

it can be seen from table  <dig> that the accuracy, sensitivity, specificity and mcc by gpcr-svmfs are  <dig> %,  <dig> %,  <dig> % and  <dig> , respectively, which are  <dig> % to  <dig> % improvement over gpcr-ca method  <cit> . the results indicated that the gpcr-svmfs can identify gpcrs from non-gpcrs with high accuracy using optimized feature subset as the sequence feature.

as can be seen from figure  <dig>  the overall accuracy of gpcr-svmfs is  <dig> %, which is almost 15% higher than that of gpcr-ca. furthermore, the accuracies of fungal pheromone, camp and frizzled/smoothened family are dramatically improved. the accuracy by gpcr-svmfs for fungal pheromone family is 100%, approximately 93% higher than the accuracy by the gpcr-ca. the accuracies of camp and frizzled/smoothened are 100% and  <dig> % based on gpcr-svmfs, approximately 40% and 47% higher than the accuracy by the gpcr-ca, respectively. in additional, as for secretin and metabotropic glutamate/pheromone family, the predictive accuracies are  <dig> % and  <dig> % by gpcr-svmfs, approximately 23% and 16% higher than those of gpcr-ca, indicating gpcr-svmfs is effective and helpful for the prediction of gpcrs at family level.

as shown in table  <dig>  the accuracies of amine, peptide, rhodopsin, olfactory and other are  <dig> %,  <dig> %,  <dig> % and  <dig> %, respectively. meanwhile, we also have notice that the accuracy of nucleotide is lower than that of amine, peptide, rhodopsin, olfactory, which may be caused by the less protein samples contained in nucleotide class. although the accuracy for nucleotide is only  <dig> %, the overall accuracy is  <dig> % for identifying subfamiliy, indicating the current method can yield quite reliable results at subfamily level.

comparison with gpcrpred
furthermore, in order to roundly evaluate our method we also performed it on another dataset used in gpcrpred  <cit> , which is a three-layer classifier based on svm. in the classifier, dipc is used for characterizing gpcrs at the levels of superfamily, family and subfamily. the dataset obtained from gpcrpred contains  <dig> gpcrs and  <dig> non-gpcrs. the  <dig> gpcrs can be divided into  <dig> families:  <dig> class a-rhodopsin and andrenergic,  <dig> class b-calcitonin and parathyroid hormone,  <dig> class c-metabotropic,  <dig> class d-pheromone and  <dig> class e-camp. the class a at subfamily level is composed of  <dig> major classes and sequences are from the work of karchin  <cit> .

the success rates are listed in tables  <dig>   <dig> and  <dig>  and the results of gpcr-svmfs are compared with those of gpcrpred for the same dataset. from table  <dig> we can see that the accuracy of gpcr-svmfs is  <dig> % higher than that of gpcrpred based on dipc at superfamily level. as can be seen from table  <dig>  the accuracies for class a, class b and class c are 100%, which is almost 2%, 15% and 19% higher than that of gpcrpred, respectively. especially for the class d, the predictive accuracy is improved to  <dig> % by gpcr-svmfs, which is almost 45% higher than that of gpcrpred. as can be seen in table  <dig>  the accuracies of the nucleotide, viral and lysospingolipids are improved to  <dig> %,  <dig> %,  <dig> %, about 8%, 43% and 42% higher than gpcrpred. although the accuracy of cannabis is decreased from 100% to  <dig> %, the overall accuracy is improved from  <dig> % to  <dig> %. all the results show that gpcr-svmfs is superior to gpcrpred, which may be caused by the fact that optimized feature subset contains more information than single dipc, and therefore can enhance predictive performance significantly.

a in order to consistent with evaluation method of gpcrpred, 5-fold cross-validation is utilized.

a in order to consistent with evaluation method of gpcrpred, 2-fold cross-validation is utilized.

a in order to consistent with evaluation method of gpcrpred, 2-fold cross-validation is utilized.

predictive power of gpcr-svmfs
in order to test the performance of gpcr-svmfs to identify orphan gpcrs, a dataset  containing  <dig> orphan proteins are collected from the gpcrdb database . we further verify the  <dig> orphan proteins by searching accession number in the latest version of gpcrdb . the results indicated that  <dig> proteins,  <dig> proteins and  <dig> proteins belong to amine, peptide and nucleotide respectively. finally, the dataset of  <dig> proteins is constructed . the results imply that gpcr-svmfs is indeed powerful to identify orphan gpcrs.

in addition, the prediction power of gpcr-svmfs is also evaluated at family level and subfamily level by using  <dig> independent dataset, which are collected based on the gpcrdb . three of the  <dig> dataset at family level are rhodopsin-like, metabotropic and secretin-like, which contains  <dig>   <dig> and  <dig> proteins, respectively. other  <dig> dataset at subfamily level are amine, peptide, rhodopsin, olfactory and nucleotide. the  <dig> dataset is composed of  <dig>   <dig>   <dig>   <dig> and  <dig> proteins, respectively .

the results at family level are shown in table  <dig>  the proposed method achieves accuracy of  <dig> % for rhodopsin-like,  <dig> % for metabotropic and  <dig> % for secretin-like, and an overall accuracy of  <dig> % can also be obtained. the results indicate that the performance of gpcr-svmfs is good enough at family level.

the results for  <dig> subfamilies are listed in table  <dig>  the prediction accuracies for the rhodopsin, amine and peptide reach  <dig> %,  <dig> % and  <dig> %, respectively. for the largest subfamily  that contains  <dig> proteins, the accuracy achieves the highest values of  <dig> %. although the accuracy for nucleotide is only  <dig> %, the overall prediction accuracy achieves  <dig> % for classifying subfamily, indicating the gpcr-svmfs method can yield good results at subfamily level.

CONCLUSIONS
with the rapid increment of protein sequence data, it is indispensable to develop an automated and reliable method for classification of gpcrs. in this paper, a three-layer classifier is proposed for gpcrs by coupling svm with feature selection method. compared with existing methods, the proposed method provides better predictive performance, and high accuracies for superfamily, family and subfamily of gpcrs in jackknife cross-validation test, indicating the investigation of optimized features subset are quite promising, and might also hold a potential as a useful technique for the prediction of other attributes of protein.

authors' contributions
zcl conceived the idea and developed the programs, carries out the analyses and drafted the manuscript. xz contributed to the ideas on overall design, implementation. zd carried out data acquisition, guided the implementation of the work. xyz supervised the design of the system, and advised on the manuscript preparation. all authors read and approved the final manuscript.

supplementary material
additional file 1
eight amino acid properties extracted from aaindex database are selected to compute autocorrelation features.

click here for file

 additional file 2
the values of seven properties are obtained from aaindex database, and the definitions of amino acids attributed to each group are shown in the table.

click here for file

 additional file 3
more explain and an example to compute probability.

click here for file

 additional file 4
deorphan dataset. the file contains orphan protein's swiss-prot accession number and its corresponding sequence.

click here for file

 additional file 5
rhodopsin-like dataset. the file contains rhodopsin-like protein's swiss-prot accession number and its corresponding sequence.

click here for file

 additional file 6
metabotropic dataset. the file contains metabotropic protein's swiss-prot accession number and its corresponding sequence.

click here for file

 additional file 7
secretion-like dataset. the file contains secretion-like protein's swiss-prot accession number and its corresponding sequence.

click here for file

 additional file 8
amine dataset. the file contains amine protein's swiss-prot accession number and its corresponding sequence.

click here for file

 additional file 9
peptide dataset. the file contains peptide protein's swiss-prot accession number and its corresponding sequence.

click here for file

 additional file 10
rhodopsin dataset. the file contains rhodopsin protein's swiss-prot accession number and its corresponding sequence.

click here for file

 additional file 11
olfactory dataset. the file contains olfactory protein's swiss-prot accession number and its corresponding sequence.

click here for file

 additional file 12
nucleotide dataset. the file contains nucleotide protein's swiss-prot accession number and its corresponding sequence.

click here for file

 acknowledgements
the authors acknowledge financial support from the national natural science foundation of china , ph.d. programs foundation of ministry of education of china  and natural science foundation of guangdong province 
