BACKGROUND
many statistical models for studying the relationship between a response variable and a set of predictor variables have been developed over the years, e.g. generalised linear models  <cit> , survival models  <cit>  and multi class logistic regression models  <cit> . these models typically assume that there are many more observations than variables. however, with the advent of high throughput biotechnology data such as that collected by microarrays, snp chips and mass spectrometers, it has become possible to gather data sets with several orders of magnitude more variables than observations. in this paper we describe a unified mechanism for enabling the use of a wide variety of existing statistical models in the case that there are many more variables than observations. underlying this mechanism is a notion of model sparsity and the mechanism can be viewed as either likelihood based methodology with a sparsity penalty or a bayesian methodology with a sparsity prior. there is some expositional advantage to the bayesian approach so we will focus on that here. fully bayesian approaches to this problem do not seem tractable for the problem sizes to be considered.

the general approach and algorithm is described in the results section below along with comments on practical implementation, and a number of real life examples of application of the method. the numbers of variables involved in these examples range from thousands to millions. additional insight as to how the algorithm functions is described in additional file  <dig> for the case of linear regression.

before embarking on the description of the approach, we first introduce a small amount of notation. in the following we have n samples, and vectors such as y, z and μ have components yi, zi and μi for i =  <dig> ..., n. vector multiplication and division is defined component wise and Δ  denotes a diagonal matrix whose diagonals are equal to the argument. we also use ||·|| to denote the euclidean norm, and the l <dig> norm of a vector x is ∑i|xi|.

context and methods
we suppose that we are given an n by p matrix of data x with the number of variables p possibly, but not necessarily, much greater than the number of samples n. associated with this matrix is a response vector y, and we are interested in building a predictive relationship between y and x. such data matrices commonly occur with data collected from microarrays, snp chips, and mass spectrometers. let l be a likelihood function for a model we would like to fit to this data in order to achieve this. here β is a p by  <dig> vector of parameters of primary interest, and φ a q by  <dig> vector of parameters of secondary interest, such as scale parameters. example models include generalised linear models, multi-class logistic regression and proportional hazards survival models, however the discussion to follow is not limited to these models. we will describe a general method for simultaneous parameter estimation and variable selection which will cope with variable numbers in the order of millions for a wide variety of common  response models.

we begin with a bayesian perspective, and specify a prior for the p by  <dig> parameter vector β, which attempts to capture the notion that most of the components of β are likely to be zero or at least "negligible". we then maximise the posterior distribution of the parameters of interest to get estimates of β. to define the prior consider a two step process. first we generate a variance from a distribution with the property that there is a high probability that the variance will be "very small". given this variance, we then generate a parameter value from a normal distribution with this variance and mean value zero. applying this process independently for each component of β, the marginal distribution of β, which we use as our prior, can be written

  p=∫ν2ppdν <dig> 

where p is n). for this article we chose p=∏i=1pp where each of the νi <dig> has a scaled gamma distribution with common shape parameter  <dig> ≤ k ≤  <dig>  and scale parameter b >  <dig>  we will refer to this prior as the normal-gamma or ng prior below. this choice has worked very well in practice, however the methods do not depend on this choice. some other possible choices are discussed in the supplementary information, see also griffin and brown  <cit> . we choose an uninformative prior for φ.

the prior for β can be written as a product of components of the form

  p=πΓ)δk <dig> −k,δ=2b 

where k denotes a modified bessel function of the third kind  <cit> , and Γ denotes the gamma function. some interesting special cases of this prior are:

 k = 1

  p =  exp. 

this prior is used in the lasso  <cit> , and enables an l <dig> constraint to be imposed on the parameters in the model. however, for k <  <dig> the prior is stronger than the lasso "prior" and we focus attention on these priors here. note that reliable, very sparse models are of particular interest in the development of diagnostics for disease.

 k = 0

  p ∝ δ exp {-δ |β i|}/δ |β i| 

 k =  <dig>  δ =  <dig> 

  p ∝ 1/|β i| 

this prior corresponds to using a jeffreys hyperprior for the variances ν <dig>  see figueiredo  <cit>  and kiiveri  <cit> .

in our bayesian framework the posterior distribution of β, φ and ν given y is

  p ∝ l pp. 

by treating ν as a vector of missing data we can use the em algorithm  <cit>  to maximise the log of p to produce maximum a posteriori  estimates of β and φ. this approach gets around the issue of the non differentiability of the prior at zero. the prior above is such that the map estimates will tend to be sparse i.e. if a large number of parameters are redundant, many components of β will be zero. details of the algorithm are given below.

RESULTS
algorithm
the em algorithm for the general problem defined above can be described with the following steps.

step 1
set n =  <dig>  initialise φ, β and set tolerance parameters ε, ε <dig> and ε <dig> equal to 10- <dig> . choose values of k and δ in the prior .

step 2
for n ≥  <dig>  perform the e step by computing the conditional expectation d = , k, δ })- <dig> 

and

  q,ϕ)=e{log⁡p|y,β,ϕ}=l)− <dig> ||2) 

where l is the log likelihood function of y. here, and in the following, we adopt the convention that for any component of βn which is zero, the corresponding component of dn is by definition also zero and  <dig> = 0/ <dig>  more details of the derivation of  are given in appendix  <dig> in the supplementary information.

step 3
perform the m step, i.e. maximise q in  as a function of β. this can be done with newton raphson iterations as follows. set β <dig> = β and for r ≥  <dig>  βr+ <dig> = βr + αr ηr where αr is chosen by a line search algorithm to ensure q, φ) > q, φ) and

  ηr=Δ)−1) 

here, yn = x Δ , br=−∂2l/∂μr <dig> and μ r = x β r. stop when some convergence criterion is satisfied e.g |β r - β r+1| <ε, and let β * be the value of βr+ <dig> when iterations are terminated. note that in many cases  is simply a form of iteratively reweighted  ridge regression.

step 4
update parameter estimates as follows. first eliminate parameters whose values are "too" small. let s={j:|βj∗|>max⁡} and define

 βi={βi∗,i∈s <dig> otherwise. 

second, choose φ = φ + κ n ) where φ * satisfies ∂∂ϕl,ϕ) and kn is a damping factor such that  <dig> <κ n ≤  <dig> 

step 5
check convergence. if |β  - β | <ε  <dig> then stop, else set n = n+ <dig> and go to step  <dig> above. end of algorithm.

for the general case modifications are required if the regularised matrix in  is indefinite.

note that ∂2l∂2μr in step  <dig> above can also be replaced by its expectation e{∂2l∂2μr} which will be at least negative semi definite. negative definite  diagonal approximations to the second derivative will also generate ascent directions if used in the m step.

implementation
the prior distribution discussed here places much more weight on parameters being zero than is customary. there are many issues involved in the practical implementation of the procedure outlined above. some of these issues are discussed below.

initialisation
in general the posterior can have many local maxima so a critical part of the algorithm is the intialisation. another issue is that initial values too close to zero may also result in iterations converging to β =  <dig> 

a good initial value is one for which the likelihood function attains, or is very close to its global maximum. intuitively, this means we start at a point where the fit to the observed data is the best possible. to make progress from such an initial value, the algorithm can only decrease the second term in equation  by making one or more components of β smaller.  could be interpreted as a collection of pseudo t-statistics.) from such an initial value we can think of the algorithm as maintaining the best fit to the data possible whilst diminishing the importance of and eventually removing parameters from the model. parameters which can be totally removed from the model without affecting the optimal fit are likely to disappear first until a trade off between model complexity and goodness of fit, as measured by the likelihood function, begins as iterations continue.

for models in the exponential family class, for example generalised linear models, such an initial value can easily be obtained by performing a ridge regression of a transformed and possibly slightly perturbed version of the response vector y, see the supplementary information for more details.

the e step
the components of the conditional expectation required in  are given by the following expression

  e{νi−2|βi,k,δ}=δ|βi|k3/2−k)|)k1/2−k)|) 

for i =  <dig> ..., p, where k denotes the modified bessel function of the third kind and δ=2/b. the function k is a standard function in the r package  <cit> , see also zhang and jin  <cit>  for stand alone code. a sketch of the derivation of the above result is given in appendix  <dig> in the supplementary information.

some useful special cases of  are:

k = 1

  e{νi−2|βi,k= <dig> δ}=δ/|βi| 

k = 0

  e{νi−2|βi,k= <dig> δ}=1/|βi|2+δ/|βi| 

the m step
let p denote the number of parameters which are currently nonzero at iteration number n. we can use the same matrix identity referred to above to obtain expressions for  which require inversion of matrices of size min ) or less.

for p ≤ n use

  ηr=Δ)−1) 

and for p > n use

  ηr=Δ)) 

note that  appears to require the inversion of a p by p matrix, however the calculation can be done by inverting a p by p matrix since p-p columns of yn are identically zero, see the definition of yn in equation . by partitioning yn, βrand d into conformable zero and non-zero components  and  can be calculated efficiently. in fact it is only necessary to calculate ηr for parameters which are currently non-zero.

when the number of parameters p in the model becomes less than n the size of the matrices being inverted becomes p by p and continues to decrease as more parameters are eliminated from the model. note that the algorithm can be implemented to be o).

convergence
in practice the algorithm converges rapidly. to see the reason for this, differentiate  with respect to β to obtain

  ∂q∂β=∂l∂β−β) <dig> 

by the definition of the algorithm in section  <dig>  β  is defined so that the left hand side of  is zero. hence if the sequence , φ) converges

 )2)=β. 

for the ng prior, using abramowitz and stegun  <cit> , section  <dig> . <dig>  it can be shown that for small beta and  <dig> ≤ k ≤  <dig>  we have

  e{ν-2|β }~b/|β | <dig>  

and for  <dig>  ≤ k ≤  <dig> we have

  e{ν-2|β }~c/|β |  

where b and c are constants. it follows that the rate of convergence to zero of the outer iteration in the em algorithm is quadratic for  <dig> ≤ k ≤  <dig>  and varies from quadratic to linear as k varies from  <dig>  to  <dig> 

multiple solutions
multiple maxima of the posterior can be explored by sequentially running the algorithm and deleting selected variables from consideration in the next run. this often produces classes of models with similar predictive performance which can be used to provide alternative or expanded interpretations. predictions using these models can also be combined by majority voting schemes or model averaging.

when n <p the likelihood has flat spots as can be seen from the relation

  x β = x  

where r is orthogonal to the row space of x. using , given a starting value β <dig> with likelihood value close to the global maximum, random points with this same property can be generated as follows. generate a p by  <dig> vector of random variables n, compute

  r=−1x)nβr=β0+r 

it is easy to see that βr has the same likelihood value as β <dig> 

forcing variables into the model
the algorithm can be easily modified to force variables into the model  by simply not penalising certain coefficients.

selection of hyperparameters
in the prior discussed here, the choice k =  <dig>  δ =  <dig>  works surprisingly well in many cases, giving very sparse models with small cross validation errors. however, this prior can sometimes lead to the elimination of all variables. in such cases cross validated errors computed over a grid of k and δ values can provide guidance in selecting the hyperparameters. often, setting δ =  <dig> and just computing cross validated errors over a grid of values of k works well. note that any process for assessing the quality of the predictions from a model chosen in this way should explicitly include this selection process to avoid selection bias. we will expand on this below.

implementing multiclass logistic regression
to implement the algorithm for a particular model simply requires expressions for the first two derivatives of the likelihood function. see the supplementary information for details for multiclass logistic regression.

enlarged sets of predictors
as mentioned earlier, enlarged sets of predictor variables for biological interpretation can be identified by running the algorithm multiple times and removing variables previously selected from consideration. an alternative strategy, which can identify sets of important highly correlated variables is to define a new x matrix by clustering the columns of the original matrix x and taking means of the clusters  <cit> .

other models
it can be shown that the algorithm still applies if the likelihood function in  is replaced by some other goodness of fit criterion. for example, linear kernel support vector machines can be implemented with the above algorithm  by using the penalized hinge loss formulation and noting that

 | <dig> - x|+ = lim γ - <dig> log ) 

where | <dig> - x|+ = max ,0) and the limit means γ → ∞, see for example  <cit> . using the above criterion with the normal gamma prior, we can also fit the l <dig> penalised support vector machine  and a more strongly penalised version with no tuning parameters .

a minor modification to the e step enables l <dig> linear regression with l <dig> constraints  on the parameters to be fitted by the algorithm. there is also a penalised version of l <dig> regression with no tuning parameters .

note that we do not need to use linear or quadratic programming to fit these models. more details will be reported elsewhere.

testing
we give some examples of fitting models to data with orders of magnitude more variables than samples using various likelihood functions below. to eliminate selection bias  <cit>  in our assessment of predictions, we validate results either on a totally independent data set, or through the use of n fold cross validation in such a manner that for each fold the hold out sample plays no role whatsoever in the formulation of our prediction  <cit> . for models with no hyperparameters this means that for each fold the simultaneous model fitting and variable selection is redone and predictions then made for the holdout data. for models with hyperparameters, any cross validation necessary to estimate these parameters is also redone for each fold. where necessary, we will refer to the above as "including the variable selection or hyperparameter selection process in the cross validation". in the examples below, with the possible exception of the ordered categorical data example, selection bias has been accounted for by the above methods. the results for all the examples are for very sparse models found by the algorithm.

smoking data
for our first example we use the gene expression data  from spira et al.  <cit> . we analyse a subset of the data consisting of  <dig> current smokers and  <dig> who have never smoked. we treat smoking status as a binary "response" and search for genes which are predictive of this "response" in a logistic regression model. the number of variables  in this problem is  <dig> 

for the default values δ =  <dig> and k =  <dig>  the algorithm discovers a model with three genes for the full dataset. the  <dig> fold cross validated error rate, calculated as mentioned above, is  <dig> . the corresponding misclassification table is given in the supplementary information.

for illustrative purposes, we also computed the  <dig> fold cross validated error rates for a grid of values of the hyperparameters b and k in the ng sparsity prior, see supplementary information. the smallest value was  <dig> , corresponding to one misclassification. for k =  <dig> , and δ ≈  <dig>  the apparent error rate was  <dig> . for this combination, neighbouring grid values had similar error rates so the specific values of k and b are not critical. the model involved  <dig> genes. including the choice of k in the cross validation  gave a cross validated error rate of  <dig> . including the choice of both hyperparameters in the cross validation did not increase this error rate.

for comparison purposes we also used a support vector machine  <cit>  with recursive feature elimination  <cit>  to classify this data. the best model contained  <dig> genes and had an apparent  zero cross validated error rate. three of the genes were common to those found by our algorithm. when the variable selection process was included in the cross validation this error rate increased to about 10%.

with random forests  <cit> , using the top  <dig> variables ranked by standardised variable importance gave an out of bag error rate of  <dig> . including the variable selection step in the cross validation had neglible effect on this error rate.

enlarged lists of discriminating genes can be found by our algorithm by deleting the genes found and rerunning the algorithm. this can be repeated multiple times until there is no longer any discriminating power in the resulting models. for this data set almost all of the genes found by svm and random forests appear in this expanded list.

when classes appear to be linearly separable, experience with a variety of data sets with small to moderate sample sizes suggests that the methods described in this paper give comparable and sometimes marginally better results to those obtained by support vector machines or random forests. the main advantage is that there is no need to do additional steps such as recursive feature elimination or pruning with variable importance to arrive at a sparse model. the other advantage is that many different types of models can be handled in this one framework.

ordered categorical prostate cancer data
we analysed some data reported by tomlins et al.  <cit>  on stages of prostate cancer. the data set consisted of  <dig>  gene expression measurements obtained from  <dig> "samples" classified into a number of ordered categories. for illustrative purposes here we analyse the  <dig> observations with categories 1-benign, 2-cancer, 3-metastasized. we omitted  <dig> genes which had all values missing. the remaining missing values were imputed using a simple model involving the observed row and column means of the data matrix. to access the data in its original form see the supplementary information. using the algorithm in section  <dig> we fit the odds continuation ratio model  <cit>  with

 logit=θg+xitβ,1<g≤g 

where pig denotes the probability that observation i belongs to class g, pig∗=∑h=1gpig and xit denotes the ith row of the expression data matrix x. here g =  <dig> 

applying the algorithm to this data set produces a  <dig> gene model with the cross validated confusion matrix in table  <dig> below.

note the lower accuracy for class  <dig>  to see if this could be improved we did a weighted analysis with observation weights inversely proportional to the number of observations in each class, but with the resulting weights for class  <dig> being multiplied by  <dig>  rerunning the algorithm with these weights gave the results in table  <dig> below.

this time a model with  <dig> genes was identified of which three were in common with the previous model.

we should be cautious about this last model and ideally it should be validated with independent data, however it serves to illustrate the application of our methodology to ordered categorical data. other methods such as, support vector machines and random forests do not take into account any ordering present in categorical variables.

multiclass leukaemia data
to illustrate an application of the multiclass logistic regression model we consider the leukaemia dataset reported by ross et al.  <cit> . the data consists of microarray measurements from affymetrix u <dig> a and b chips. there were <dig> individuals classified into  <dig> sub types of leukaemia . we do a probe level analysis so p =  <dig> i.e. there are roughly half a million variables. the class names are .

applying the methodology described above, the leave one out cross validation error is  <dig> . the cross validated misclassification matrix is given in the supplementary information.

the method identified  <dig> probes which appear to be useful for sub-typing leukaemia. using random forests  <cit>   the out of bag error rate is  <dig>  using over  <dig> probes and  <dig>  for a model using the top ten probes ranked by standardised variable importance. this latter figure did not increase by including the variable selection step in the cross validation.

survival analysis
to illustrate the use of our method with survival data we fit a cox proportional hazards model  <cit>  to the lymphoma data set reported by dave et al.  <cit> . see the supplementary information for access details.

this data set consists of  <dig> "gene"  expression measurements from affymetrix u133a and b chips on  <dig> patients. censored survival times were available for  <dig> of these individuals. in dave et al.  <cit>  the data was divided into a training set of  <dig> observations and a validation set of  <dig> observations. allowing for missing survival times the training set has  <dig> individuals and the validation set  <dig> individuals. note that the validation set has censored observations.

applied to the training data, we used the algorithm of section  <dig> to identify  <dig> genes as being potentially associated with survival. using the baseline survival function and the coefficients of the linear predictor estimated from the training data we obtained  survival curves for each individual in the validation data set. from these predicted survival curves we calculated the probability of each individual in the validation set dying in a defined set of time intervals and computed the expected number of deaths in each of these intervals. we then calculated the observed number of deaths in these intervals for the validation data set. we included the censored individuals in this step by computing the conditional probability of a death  in any interval given the time was greater then the censoring time. as a consequence the "observed" counts have non-integer values. table  <dig> below shows the results for the model with three genes. taking the l <dig> norm of the observed minus expected counts on the validation data as a statistic, we then generated a null distribution for this statistic by permuting the rows of the x matrix for the training data  <dig> times, rerunning the algorithm each time and making a prediction on the validation data. the p value of our observed statistic was about  <dig> , which suggests the support for this model is not strong.

in their paper, using their survival signature analysis method, dave et al.  computed a survival index based on over  <dig> gene expression measurements. repeating the calculation above for their survival index on the validation data gave table  <dig> below.

note that on the basis of the l <dig> norm statistic, the  <dig> gene fit has a smaller l <dig> norm.

ethnicity and sex – perlegen snp data
we now illustrate how the method scales up to datasets with millions of variables. in a recent article, hinds et al.  <cit>  report on the collection and analysis of a large data set in which  <dig> individuals were genotyped for over  <dig>  million single-nucleotide polymorphisms . ethnicity and sex information for each of the  <dig> individuals was also recorded. using only snps on chromosomes 1– <dig>  the methods in this paper identified two snps which classified sex and three snps which classified ethnicity. the identified snps were validated with the hapmap data set  <cit> . a publication giving more details about these results is in preparation.

discussion
although in the above we have not provided details of the genes  in the models presented in the above examples, in cases when the gene function is known, the selected genes have a biologically meaningful function in the context of the dataset being analysed. specifically, for the smoking data we saw genes appearing in networks associated with biological themes that we'd expect from an assault such as smoking on tracheal epithelial cells. many of these are well documented in the literature, e.g. xenobiotic metabolism , genes associated with immune function  and inflammatory response. in addition there were genes involved in the early-immediate stress response  which is expected from a toxic challenge to cells. likewise, the genes in the leukemia classifier showed links with genes related to various aspects of the cell cycle, dna repair, dna replication and check-point controls as well as genes involved in cell growth and proliferative responses. finally for the perlegen snp data the ethnicity classifier used a snp which was associated with a gene which codes for skin colour.

biological interpretations like the above have also been reflected in our experience with these methods over a number of years.

concerning the computer time required to analyse these examples, on a  <dig>  ghz machine, the two class problem with  <dig> + variables ran in under one minute. the six class problem with roughly  <dig>  variables took about half an hour to run, and the two class problem with  <dig>  million snps  ran in about an hour and a half. examples were run in r with no particular optimisation of the code. the times for the snp example could most likely be reduced by the use of sparse matrices.

CONCLUSIONS
using a sparsity prior or sparsity penalty in conjunction with a likelihood function is a powerful approach to finding parsimonious models for datasets with many more variables than observations. the method is capable of handling problems with millions of variables and makes it possible to fit almost any statistical model with a linear predictor in it to data with more variables than observations.

in the linear case, and where comparison is possible, the methods described in this paper compare favourably with well known methods such as support vector machines and random forests. however, they have the advantage in that variable selection and parameter estimation occur simultaneously and no additional steps are required to obtain a sparse model.

an r library implementing the algorithm described in this paper is freely available for non-commercial use  <cit> .

supplementary material
additional file 1
supplementary information.

click here for file

 acknowledgements
i would like to thank professor philip brown for suggesting the use of the normal gamma prior and dr frank de hoog for insights into the em algorithm and its convergence. i would also like to thank the reviewers for their valuable comments.
