BACKGROUND
microarray technology is a powerful genomic approach that enables researchers to quantify the expression levels of large numbers of genes simultaneously in one single experiment. arrays can be single-channel , which quantify the absolute expression of genes in specific experimental conditions, or two channel . a key purpose of a two-colour microarray experiment is the identification of genes which are differentially expressed in two samples. although this technology has given an enormous scientific potential in the comprehension of gene regulation processes, many sources of systematic variation can affect the measured gene expression levels. the purpose of data normalisation is to minimise the effects of experimental and/or technical variations, so that meaningful biological comparisons can be made and true biological changes can be found within one and among multiple experiments. several approaches have been proposed and shown to be effective and beneficial in the reduction of systematic errors within and between arrays, both for single- and for double-channel technology  <cit> . some authors proposed normalisation of the hybridisation intensities, while others preferred to normalise the intensity ratios. some used global, linear methods, while others used local, non-linear methods. some suggested using spike-in controls, or housekeeping genes, or invariant genes, while others preferred all the genes on the array.

in general, microarray normalisation can be divided into normalisation within arrays, for the correction of dye effects, and across arrays, for the balance of the distribution differences among experiments. several pre-processing techniques recently proposed for two-channel technology allow the joint normalisation within and across experiments, as reported in the original papers . glog and q-spline transformations, in fact, are performed on the gene expression matrix where the two channels are considered separately, allowing systematic bias reduction within and across arrays. although several normalisation procedures have been proposed, it is still unclear which method uniformly outperforms the others under different experimental conditions. recent works  <cit>  compare, through simulated data, normalisation methods in terms of bias, variance, mean square error or leave-one-out cross-validation classification error. if we consider the two-channel technology, park et al.  <cit>  show that, in some cases, intensity dependent normalisation performs better than the simpler global normalisation, while  <cit>  raised the concern that removal of spatial effects may add additional noise to normalised data, suggesting that a safe alternative is to remove the intensity effect only at a local level. thus, the evaluation of normalisation's effects in microarray data analysis is still an important issue, since subsequent analyses, such as tests for differential expression, could be highly dependent on the choice of the normalisation procedure. for example, durbin et al.  <cit>  show that the log-transformed expression ratio has a greatly inflated variance for expression values close to  <dig>  this effect penalises differential expression, especially for high expression levels. hypothesis tests for differential expression may in fact be more effectively performed on data that have been transformed so as to have constant variance. hoffman and colleagues  <cit>  compare the effect of different normalisations on the identification of differentially expressed genes within affymetrix technology and using a real dataset. they observe, by comparing lists of genes, that the normalisation has a profound influence on the detection of differentially expressed genes.

moreover, the microarray quality control   <cit>  project, which is specifically designed to address reproducibility of microarray technology by comparing results obtained across different array platforms, chooses the statistical analysis on the base of the normalisation and gene selection technique as the crucial steps in order to improve reproducibility  <cit> .

when microarray experiments are adopted with diagnostic purposes, this result appears to be fundamental because scientists are looking for a list of a few pathology marker genes. marker genes can be defined as genes whose expression profiles are discriminating between case and control samples. it is likely to suppose that the complete list of markers of a condition is composed by hundreds of genes, highly correlated and mostly implicated in few signalling cascades. only few of them lie upstream these signalling cascades and are responsible of the differential expression of all the others genes. hence, if different pre-processing have an impact on the identification of differentially expressed genes, they could lead to different lists of markers. the aim of this work is to compare and evaluate the impact of various normalisation procedures proposed for two-channel array technology on the identification of marker genes. we shall use both simulated and real data derived by cdna and oligo microarray .

the use of a simulation approach allows us to study the sensitivity and specificity of the tests after normalisation and to compare different approaches' performances. however, simulation of dna microarray data can be questioned, mainly because  the relation between expression and experimental factors involved is not theoretically established, and  the statistical distribution of differential expression given by various causes across genes is still controversial. in order to address such issues, we adopt two different classes of simulation models.

although we found a limited difference of sensitivities and specificities for the tests after each normalisation, the study highlights a strong impact in terms of gene ranking, resulting in different levels of agreement between competing normalisations. finally, we show that the combination of two normalisations, such as glog and lowess, that handle different aspects of microarray data, is able to outperform the other individual techniques.

 <dig> 
RESULTS
 <dig>  simulated data
to ease reading of the results, we performed the comparisons in two stages, involving: 1) q-splines, quantile, enhanced quantile and enhanced q-spline, and 2) the best normalisation obtained at step 1) with all the other techniques.

we found that quantile and q-splines equally perform in terms of specificity and sensitivity across all the simulated scenarios both for gg/lnn models  and for albers' model . on the contrary, surprisingly, enhanced quantile and enhanced q-spline show extremely reduced performances . we deeply investigated this result. we found that, in all the experimental scenarios, the additional steps performed by the enhanced method after the quantile and q-spline normalisation do not recover further relevant information from the residual matrix . then, both with sam and with ebayes test we observed a strongly reduced fdr estimate that increases the number of false positives genes and strongly reduces the test sensitivity. in the light of these results, we decided to proceed by taking into consideration in step  <dig> only q-splines normalisations. in the following, step  <dig> results are reported.

in case of gg and lnn models without systematic bias  and of albers' with 10% of background level , all the normalisations show a similar performance. global and glog normalisations seem to perform slightly worse than the others, but using empirical confidence interval this difference is not significant . however, when a systematic bias is included  and with increasing background levels , the normalisations respond differently. in particular, if we consider albers' model, q-spline and glog seem to increase performance showing the best level of specificity and sensitivity. this result can be explained by the presence of negative values. negative values in normalisations based on log-ratio intensity transformation are necessarily treated as missing . the replacement of negative values with an arbitrarily known small value  has a general effect of slightly reducing specificity and sensitivity after all normalisations. the major effect is evident on the glog transformation that shows a dramatically reduced sensitivity in case of 150% background level .

differences among sensitivity and specificity of the normalisations in albers' simulated matrices have been quantified through the area under the roc curves . normalisations are ranked according to their auc so that the bigger the rank, the better the normalisation . to evaluate the reproducibility of our results and the influence that the test statistic sam had on the normalisations comparison, we re-calculated and compared roc curves using a different test statistic. roc curves and the ranking of normalisation obtained through auc using ebayes test are reported in additional file  <dig>  and additional file  <dig>  respectively. it is worth noting that results obtained with the ebayes test are totally in agreement with those obtained with sam test.

for each simulated scenario is also reported the ranking of the normalisations according to the auc: the bigger the rank, the better the normalisation.

differences observed in normalisations performance is evidently reflected in the gene ranking . we find that, on the first  <dig> genes, the highest overlapping rate with the lowess list is around 80% with values going down to 30% . agreement tends to reduce even more when comparing gene lists with replaced negative values . these results have been confirmed using ebayes test .

in general, we observe that the oslin procedure is essentially equivalent to olin, suggesting that the further scaling factor introduced in oslin is redundant.

lowess and olin tend to show similar performances, which implies that the optimal estimate for the smoothness parameter is usually close to the default one. however, in case of the well-known ma-plot "arrow head effect", typical of an array characterised by a large proportion of small constant values  the optimisation procedure erroneously captures an arrow head effect trend, while lowess with smoothing parameter set to the default value  ignores this trend.

the highly similar level of sensitivity and specificity of most normalisations, jointly with a poor overlapping in the gene lists, suggests that different pre-processing methods could be able of capturing alternative aspects of microarray data, for example by identifying complementary lists of marker genes. even if the identification of the best normalisation procedure seems to be unfeasible, the combination of different procedures could represent an efficient alternative. then, we evaluate the performance in terms of specificity and sensitivity on simulated datasets of glog and lowess normalisations in the following scenarios:  the union of the gene lists obtained separately from glog and lowess normalisations,  the intersection of the gene lists obtained separately from glog and lowess normalisations, and  the list obtained from the combination of glog and lowess normalisations. this last scenario should avoid missing negative values in case of a high level of background and guarantee an efficient intensity dependent normalisation. in addition, lowess normalisation effectively removes biases within each slide but does not account for differences across multiple slides, which, on the contrary, are provided by glog. specificity and sensitivity have been calculated by varying the number of top ranking genes from  <dig> to  <dig> with step  <dig>  in case of scenarios  and , the number of top ranking genes in both lists have been selected in order to obtain a union and intersection list of desired size. figure  <dig> shows the results either for gg, lnn or albers' models. in simulated datasets, the combination of glog and lowess proves to be better than the other combinations. this result suggests that combining these two normalisations is advantageous in terms of identification of differentially expressed genes.

 <dig>  real data
microarray normalisations are based on at least two fundamental assumptions: i) only a small portion of spots is differentially expressed and ii) differentially expressed spots are homogeneously distributed among the over and the under expressed ones. these assumptions are reasonable for most of large-scale genome experiments where only a small proportion of the entire genome is involved in the biological process studied, but could fail in case of a platform with only limited genome coverage or for specific experimental treatments.

therefore, we selected real datasets in order to consider different experimental situations. we chose experiments obtained with two spotted cdna and two spotted oligos platforms and characterised by i) a weak response in terms of differential expression, ii) a strong response in terms of differential expression, iii) a large number of negative values replaced, iv) a large number of negative values kept as such. table  <dig> briefly describes the datasets' characteristics.

for each dataset the table reports the organism, the platform, the number of features, the number of expected differentially expressed genes  , the mean number of negative spots per array , and if the negative spots are replaced or not. note that in dataset c each array contains four replicates of each gene, giving a total of  <dig>  unique features. therefore, the proportion of genes expected to be differentially expressed has been calculated on the total of unique features. expected number of deg is calculated as the average number of differentially expressed genes obtained with sam test after all the normalisations used in the study.

according to our results, differences among normalisation performances seem to be independent from array platforms and from the type of differential expression response  but rather are dependent from the presence and the way of dealing with negative values.

performance of the combination of glog and lowess, as well as of lowess and enhanced quantile, have been carried out through the identification in dataset d of a small list of true positives, retrieved from published biomedical research literature by bioinformatics organization inc. here, we include the combination of lowess and enhanced quantile in order to evalute if the performance of enhanced normalisation  could be badly influenced by the workman et al.  <cit>  strategy used for two-channel technology. the use of lowess and, then of enhanced quantile , should avoid this possibility. table  <dig> shows the rank of true positive features obtained after glog, lowess, union and intersection of gene lists obtained from glog and lowess separately, the combination of glog and lowess and the combination of lowess and enhanced quantile. the smaller the rank of true positives, the more efficient is the normalisation. the combination of glog and lowess shows the smallest rank for most of the genes, suggesting a better performance compared to the others and confirming, even with real data, the poor performance of the combination of lowess and enahnced quantile.

fc: fold change; upper and lower: confidence interval upper and lower bounds; symbol: gene symbol.

 <dig> 
CONCLUSIONS
the main aim of this research effort was to report on an exploratory study for benchmarking the impact of several normalisation techniques in detecting differentially expressed genes. the documentation of the simulation models, the experimental setup, the analysis on real data should enable the reader to assess the robustness and scope of the benchmarking.

results were presented in terms of mean sensitivities and specificities and mean overlapping rates of gene ranking lists. summarising our results, we are able to say that, in general, the comparison of the sensitivities and specificities shows limited difference in impact of preprocessing over the range of operating conditions. on the other side, the study highlights an evident impact in terms of gene ranking agreement. with equal levels of specificity and sensitivity, gene lists differ from an average of 40% of the genes with albers' model to an average of 20% with gg and lnn models .

this might have important effects on some microarray-based research, where, through gene ranking and discriminant analysis, a small set of genes is selected to become markers of the studied pathology. our study suggests that, putative marker genes obtained with different normalisations could be substantially different.

this is more evident in case of replacement of null or negative values where the higher the number of replacements, the lower the sensitivity and specificity of the test, and the lower the rate of agreement of gene ranking. therefore, the best pre-processing action might depend upon the distribution of the data, and a careful exploratory analysis is called for before applying normalisation.

real datasets  confirmed these results. differences among normalisation performances seem to be independent from array platforms and from the type of differential expression response , but seem to depend on the presence and the handling of negative values. we also show that the combination of glog and lowess may avoid the drawbacks given by the negative values  and may guarantee an efficient intensity dependent normalisation. the advantage of the combination is much more evident without replacement of negative values.

 <dig> methods
 <dig>  normalisation essentials
in this section, we give an overview of the essentials of the normalisation techniques that are taken up in our comparative study. since we are unable to cover all of the technical details in this article, we refer the reader to the relevant literature.

global normalisation
global normalisation  <cit>  is usually directed to balance the different incorporation effciencies of the two fluorophores  in the two-channel technology. global intensity normalisation relies on the assumption that the quantity of mrna is the same for both labelled samples. furthermore, assuming a symmetrical distribution of over- and under-expressed genes for thousands of genes in the array, these changes should balance out so that the total quantity of rna hybridising to the array from each sample is the same. consequently, the total integrated intensity for all spots should be the same in both the cy <dig> and cy <dig> dyes. under this assumption, a normalisation factor can be calculated and used to re-scale the intensity for each gene in the array.

lowess normalisation and its variants
lowess normalisation  <cit>  relies on the use of a non-linear regression technique  based on robust local regression of the log ratios of cy3/cy <dig> on overall spot intensity cy3*cy <dig> . the normalised m can be written as

 m' = m - c, 

where c is the lowess smoother. the normalisation model is based on the assumption that a significant fraction of the probes in the array is expressed at similar levels.

print-tip lowess normalisation  proposed by yang et al.  <cit>  takes into account possible spatial intensity artifacts introduced by robot print-tips during the spotting step. p-lowess is based on individual linear local regression  limited to a single print-tip group. in this way, each print-tip group has its own normalisation curve. the formula for the normalisation is

 m' = m - ci, 

where i =  <dig>  ..., k is the i-th print-tip group.

futschik and crompton  <cit>  show that the arbitrary use of local regression parameters can severely compromise the quality of normalised data. parameter choice has commonly been left to the user, and instructions on how to adjust the parameters to the underlying data structure are generally not given. in order to overcome these limitations, futschik and crompton  <cit>  introduce two normalisation schemes, optimized local intensity-dependent normalisation  and optimized scaled local intensity dependent normalisation , based on iterative local regression and model selection. olin is based on iterative local regression where parameters are optimised in each regression step by generalised cross-validation. oslin comprises olin procedure with a subsequent optimised scaling of the range of log-intensity ratios across the spatial array dimensions.

neural networks
tarca and colleagues  <cit>  propose a method based on a robust neural network model that uses the log-intensity ratio  as the independent variable, and the average log-intensity  as well as spatial location of spots as predictors. resistance to outliers is provided by assigning weights to each spot based on how distant their m values are from the median over the spots whose a values are similar, and also by using pseudo-spatial coordinates instead of spot row and column indices. the authors use a simple feed-forward neural network with sigmoid activation function suggesting three neurons in the hidden layer.

q-splines
q-spline normalisation  <cit>  uses the quantiles from each array and the target to fit a system of cubic splines to normalise the data. the target should be the  mean or median of each probe. the authors propose splines for their robustness in representing almost any smooth relationship, including the linear one. using quantile information provides a much easier fitting problem and avoids fitting the pairwise data directly, which often requires robust regression techniques.

quantile
originally, quantile normalisation  <cit>  was proposed as an across arrays normalisation suitable for single channel technology. we decided to evaluate the quantile performance using the same strategy proposed by workman et al.  <cit>  and wu et al.  <cit>  for the q-splines. the goal of quantile normalisation is to give the same empirical distribution of a target reference to each array. following wu et al.  <cit> , the reference target is defined as the geometric average of cy <dig>  channel. considering the simple case of dimension n =  <dig>  if two data vectors have the same distribution, a quantile-quantile plot will have a straight diagonal line, with slope  <dig> and intercept  <dig>  thus, if the quantiles of two data vectors are plotted against each other and each of these points are projected onto the 45-degree diagonal line, we obtained a transformation that gives the same distribution to both data vectors. quantile normalisation is the generalisation to n dimensions of the above transformation.

enhanced procedure
the enhanced normalisation procedure recently proposed by hu and he  <cit>  uses singular value decomposition  of the normalised microarray data matrix and of the correspondent residual matrix  to allow users to filter out noise and recover relevant information that might be lost in a given normalisation procedure. the goal of the procedure is retaining maximal relevant information in gene expression profiles. for an exhaustive description of the methodology, see the original paper by hu and he  <cit> . in this study, we apply the enhanced procedure to quantile and to q-spline normalisations.

variance stabilising normalisation
as alternative to any other pre-processing technique, rocke and durbin  <cit>  and huber and colleagues  <cit>  present independently a family of variance stabilising transformations based on the generalised logarithmic transformation .

glog assumes that raw gene expression intensities, y, can be modeled as the sum of three components:  average background noise, α,  true expression level, μ, multiplied by an exponential error term, η, normally distributed with zero mean and variance ση <dig>  and  an additive error term, ε, normally distributed with zero mean and variance σε <dig>  as

 y = α + μeη + ε. 

glog transformation, which can be equivalently applied to single- and double-channel microarray technology, should achieve absence of relation between mean and variance of the expression. it can be written as

 glog=log⁡2+c), 

with c=σε2eση2

 <dig>  simulation models
in this section, we document the simulation models that we have used in our analyses.

 <dig> . <dig> hierarchical models
generation of signal intensities
we adopt a mixture model strategy as in  <cit> . genes come from two different groups: differentially expressed  and equally expressed . each group is modelled by its own distribution. the data as a whole are modelled by a weighted mixture of these distributions, where the weights p and  correspond to the prior probabilities of being differentially expressed and equally expressed, respectively. if we write the expression value of the gene g as ygk, g =  <dig>  ..., n, in the channel k, k =  <dig>   <dig>  we have

 f=pf1+f <dig>  

according to the empirical bayesian approach, we suppose that the intensity values of the two channels ygk are random samples from the distribution fobs  with k =  <dig>   <dig>  respectively. in the ee case, we assume that the 2n values are independent, identically distributed, according to the distribution of fobs. hence, under the ee hypothesis, the marginal distribution is

 f0=∫))πdμg, 

where π is the prior distribution of the mean signal μg, representing variations in the mean intensity value of genes in the experiment  <cit> .

under the de hypothesis, the latent mean μgk of the sample of the channel k is different in each k. in particular, the two values of μgk are drawn independently from the distribution π, leading to

 f1=f0f <dig>  

where

 f0=∫)πdμg,f0=∫)πdμg. 

we considered the two mixture models of kendziorski et al.  <cit> . in the first model, named gamma-gamma , the intensities for the replicates in both conditions  are assumed to be independently generated from gamma distributions with a constant shape parameter α and gene-specific random scales λg, assumed to have a gamma distribution with shape hyperparameter α <dig> and scale hyperparameter ν. in the second model, named lognormal-normal , the log intensities are assumed to be normally distributed, with constant variance σ <dig> and gene-specific random means μg, that are themselves normally distributed with hyperparameters μ <dig> and τ. if a gene is selected to be equally expressed, then a value for the random parameter is sampled from its prior distribution, determining the distribution from which independent replicates for both conditions are produced. if a gene is selected to be differentially expressed, then two values for the random parameter are sampled from its prior distribution, determining the two distributions from which independent replicates in each condition are produced.

non-linear systematic bias
the hierarchical models gg and lnn simulate datasets without intensity dependent systematic bias. therefore, any normalisation becomes redundant. for this reason, we decided to introduce a systematic bias effect obtained through the addition  of an opportunely scaled component, inversely proportional to a.

 <dig> . <dig> albers' additive model
differently from the previous models, albers et al.  <cit>  propose a model specifically drawn to include several layers of bias representative of possible experimental factors influencing microarray experiments. albers' model has  <dig> parameters,  <dig> of which are known constants, while the others should be set by the user. the final log-expression signals, ygk, g =  <dig>  ..., n, k =  <dig>   <dig>  where n is the number of genes in the platform and k is the channel index, are composed by the following elements:  a gene expression value, ggk,  an expression change for differentially expressed genes, dgk,  a channel effect, ck,  a spot pin effect, sg,  a raw background gradient signal, bg,  a nonlinear effect, fnl,  a fish-tail effect  due to the log transformation for small expression values, td and  a random error due to unknown factors, egk. then

 ygk=td),zgk=ggk+dgk+ck+sg+egk, 

and the error term is assumed to be gaussian.

 <dig>  experimental setup
 <dig> . <dig> hierarchical models
we fixed parameter values for gg and lnn models by using estimates obtained on real datasets: for the gg model, we set  =  and for the lnn model, we set  = . under both models, the prior probability p of differential expression is set to  <dig> .

 <dig> . <dig> albers' model
additional file  <dig> reports the parameters setup used in albers' model. several of these values are proposed by the authors as estimates obtained on real datasets. three different background levels have been used. the maximum of the background signal  relative to the non-background signal has been set to 10%, 50% and 150%. in this way, different microarray data scenarios are obtained, characterised by different proportions of negative expression values. in the first scenario , expression values are mostly positive; in the last one , a large number of negative values is observed. albers' model allows the inclusion of several types of systematic biases .

 <dig> . <dig> simulation plan
for each model and set of parameters, we simulated  <dig> matrices with  <dig>  genes expression levels on  <dig> experiments separately for the cy <dig> and cy <dig> channels. so, each simulated matrix consisted of  <dig>  ×  <dig> values . each of the  <dig> matrices was pre-processed with  <dig> procedures, coded as: raw data, global normalisation, lowess, p-lowess, olin, oslin, neural network, q-spline with target cy <dig>  and q-spline with target cy <dig> , glog. gg and lnn models do not account for print-tips platform geometry, therefore p-lowess normalisation was not considered in the comparison of performances of these models.

at the end of the pre-processing phase, we obtained  <dig> different matrices for each simulation. sam analysis  <cit>  and empirical bayes test  <cit>  were performed on each matrix. figure  <dig> summarises the entire simulation plan.

to compute the average overlapping rates, we considered the following values for the length of the top ranking gene lists:  <dig>   <dig>   <dig>   <dig>  and  <dig> 

 <dig> . <dig> real data
we used two cdna expression datasets and two oligonucleotide datasets to validate our simulation results. all the datasets are publicly available at the geo database.

baird et al.  <cit>   studied expression profiling of  <dig> tumors representing various classes of bone and soft tissue sarcomas. in this study, we selected only the  <dig> ewing's sarcoma samples. the common reference was obtained by pooling sarcoma cell lines. expression datasets and platform annotation are available on the ncbi geo database with platform identification number gpl <dig> and reference series gse <dig> 

urban et al.  <cit>   analysed the rapamycin response in saccharomyces cerevisiae. global transcriptional analysis of rapamycin response was conducted on cells expressing either a wild-type or tor-independent allele of sch <dig>  in our work, we considered only samples gsm <dig>  gsm <dig>  gsm <dig>  gsm <dig>  gsm <dig>  gsm <dig>  expression datasets and platform annotation are available on the ncbi geo database with platform identification number gpl <dig> and reference series gse <dig> 

smith et al.  <cit>   studied the expression profiles of transcription factor deletion strains in the presence of oleate. mrna levels in each of four deletion strains  were compared to those in wild type cells by microarray analysis. there were two biological replicates for each experiment, and for each replicate both label orientations were analysed on arrays containing  <dig> replicate spots of each gene, resulting in a total of  <dig> replicate spots per gene. for our study we considered only the delta_adr <dig> samples. expression datasets and platform annotation are available on the ncbi geo database with platform identification number gpl <dig> and gpl <dig>  and reference series gse <dig> 

de pittà and colleagues  <cit>   obtained expression profiling of bone marrow from paediatric patients with acute lymphoblastic leukemia  using a dedicated muscle cdna array. patients were clinically classified into b-cell all , t-cell all  and all compared to a common reference  prepared from male fetal skeletal muscle. expression datasets and platform annotation are available on the ncbi geo database with platform identification number gpl <dig> and reference series gse <dig> 

by the analysis of four datasets, we are able to test the normalisation procedures in different situations:  either a large  or a small  proportion of genes expected to be differentially expressed;  either cdna  or oligonucleotide  microarrays;  microarrays with a large number of negative spots, either replaced with zero  or kept as such . see table  <dig> for a description of each dataset. due to the large amount of negative spots per array, we considered for the dataset a only  <dig>  genes .

from published biomedical research literature, bioinformatics organization inc. retrieved a list of  <dig> genes experimentally known to be deregulated in acute lymphoblastic leukemia, and stored them in a public database available at . of these  <dig> genes, only  <dig> were present in the custom array used by de pittà et al.  <cit> , among which  <dig> were found significantly deregulated. therefore, in dataset a these  <dig> genes were considered as true positives, in order to evaluate the performance of some normalisations.

 <dig> . <dig> evaluation criteria
to evaluate the impact of the normalisation techniques in detecting differentially expressed genes in simulated datasets, we compare the results of a significance analysis based on sam and empirical bayes test statistic after various normalisations.

sam test
sam test statistic dg is defined as  <cit> :

 dg=y¯g1−y¯g2sg+s <dig>  

where sg is the standard deviation and s <dig> is a positive costant, usually the 90th percentile of the sg distribution. values and significance of the sam statistic is obtained via a permutational approach  <cit>  as follows.

 <dig>  calculate the observed values

 dg=y¯g1−y¯g2sg+s <dig>  

for any g =  <dig>  ..., p.

 <dig>  order the observed values

 d ≥ d ≥ ... ≥ d. 

 <dig>  for any permutation k  of data calculate

 dg=y¯g1−y¯g2sg+s <dig>  

 <dig>  order the values dg

 d ≥ d ≥ ... ≥ d. 

 <dig>  define the mean quantity

 d=1k∑k=1kd. 

to identify differentially expressed genes, the observed values d and d are compared and a threshold Δ is defined such that the gene g is called differentially expressed if |dg - dg| > Δ.

empirical bayes test
the empiricial bayes test   <cit>  is based on a moderated t-statistic with a bayesian adjusted denominator similar to that proposed by tusher et al.  <cit> . ebayes uses a hybrid bayes approach in which gene variances are modelled by a prior distribution that is updated using the data to obtain posterior distribution. then, an estimate is derived from the posterior distribution. this shrinks the observed variances towards the prior mean. given a prior estimate p of the proportion of differentially expressed genes, the posterior probability that a gene is differentially expressed can be calculated. the b-statistic given by limma is the log-odd of being differentially expressed versus equally expressed. we calculated the adjusted p-values with benjamini and hochberg  <cit>  procedure in order to rank the genes; the lower the p-value, the more significant the result.

performance evaluation
in our analyses, the significance analysis is used to construct sensitivity  and specificity  for the test. for various thresholds , we identify the significant genes and compute the corresponding average sensitivity and specificity of the test.

agreement among the impacts of different normalisations is also evaluated by looking at the ranking induced on the genes by the absolute value of both statistics. genes are ordered according to the absolute value of each statistic from the highest  to the lowest . then, genes to which correspond rank less than or equal to  <dig>   <dig>   <dig>   <dig> and  <dig> are compared across normalisations. taking as reference lowess normalisation, the mean rate of common genes  in the two top ranking gene lists  has been calculated for various lengths of the list.

since on real datasets true positives are generally unknown, the procedures' agreement has been evaluated through the average overlapping rates in the top ranking gene lists. however, for dataset d a small list of true positive genes was available, therefore the ranks of the true positives were used to evaluate the performance of normalisations. further, we note that, the smaller the rank, the more efficient is the normalisation.

all statistical analysises have been performed with the r statistical package freely available on . packages used: biobase, ebarrays, marray, samr, vsn, olin, nnnorm, affy, limma.

authors' contributions
msm and dr performed all the statistical analyses  and results organisation. cr and mc conceived the study, participated in the design of the study and in the interpretation of the results and revised the manuscript. mc participated also in the coordination of the work. all authors read and approved the final manuscript.

supplementary material
additional file 1
figure s <dig>  examples of typical ma plots obtained with lnn and gg models without systematic bias , with lnn and gg models with systematic bias  and with albers'™ model with negative values  and with negative values replaced by positive constant . red points represent differentially simulated expressed genes.

click here for file

 additional file 2
figure s <dig>  specificity and sensitivity curves after quantile, qspline, quantile enhanced and qspline enhanced for albers' model with increasing percentage of background level with respect to expression level with and without replacing negative values.

click here for file

 additional file 3
figure s <dig>  lnn and gg models without non-linear bias. specificity and sensitivity  and average overlapping rates of top ranking gene lists detected as differentially expressed between lowess and the other normalisations.

click here for file

 additional file 4
figure s <dig>  specificity and sensitivity curves and overlapping rates of top ranking gene lists for albers' model with increasing percentage of background level with respect to expression level with and without replacing negative values, using the moderated t-test ebayes.

click here for file

 additional file 5
table s <dig>  area under the curve  of specificity and sensitivity of the moderated t-test ebayes after the normalisations, for albers' model with increasing percentage of background level with and without replacing negative values. for each simulated scenario, ranking of the normalisations according to the auc is reported.

click here for file

 additional file 6
figure s <dig>  overlapping rates of top ranking gene lists detected as differentially expressed between lowess and the others normalisations with 95% empirical confidence interval. panel a, c, e: results obtained from data generated by albers' model with 10%, 50% and 150%, respectively, background levels without negative values replacement; panel b, d, f: results obtained from albers' model with 10%, 50% and 150%, respectively, background levels with negative values replacement.

click here for file

 additional file 7
table s <dig>  parameters setting used in the albers' simulation model.

click here for file

 acknowledgements
this work was supported by university of padova grants cpdr <dig>  cpda <dig> and cpdr070805/07: "meta-analysis for graphical models with application to the study of gene regulatory networks", and by fondazione cassa di risparmio di padova e rovigo under the grant "a computational approach to the study of skeletal muscle genomic expression in health and disease".
