BACKGROUND
proteomic studies cover the identification of entire proteomes, the detection of post-translational modifications , protein quantitation, and the determination of protein interactions. the shotgun strategy by means of liquid chromatography coupled with tandem mass spectrometry  has been considered the method of choice when the analysis involves complex mixtures  <cit> . on the other hand, a single ms/ms experiment typically generates thousand of spectra from which usually less than 20% are correctly interpreted, clearly stressing the necessity of computational solutions for assessing each peptide-spectrum match   <cit> . note that database  search algorithms are far the most used approach to ms/ms spectrum interpretation. notably, mascot  <cit>  and sequest  <cit>  are currently the most known standard methods for db search. as a result, the main computational tools for psm evaluation were built to analyze db search algorithm results. in the context of peptide/protein identification, which is our focus here, there are currently two largely used techniques for assessing psms produced by db search methods: the construction of mixture models implemented in the peptideprophet  <cit>  approach and the target-decoy search strategy  <cit> .

in peptideprophet approach, standard statistical distributions are used to fit observed positive and negative score distributions. in the case of sequest, for instance, the parameters of gaussian and gamma distributions are pursued to identify the underlying score distributions of correct and incorrect hits, respectively. hence, the probability that a psm with a certain score is correct is computed using the corresponding density functions along with prior probabilities. as long as the assumed distributions fit the data appropriately, the probabilities are very accurate and can be used in protein inference as well. on the other hand, certain datasets might present completely different score distributions. when dealing with phosphoproteins, for instance, scores are normally lower than usual because the process of fragmenting precursor ions in mass spectrometry via low energy dissociation has a tendency to be biased towards phosphate groups, leading to the suppression of important fragment ions  <cit> .

in contrast, the target-decoy search strategy, works without any a priori assumption about the data, making it a good and general method for identification assessment in ms-based proteomics. in this strategy, besides using the target proteins in the search, a database composed by decoy  sequences is also included in the assignment procedure. a common approach is to generate decoy sequences by reversing the target ones, and both sets of sequences are then used as a composite target-decoy db for the search. the resulting false sequences have to be produced in a way that it is reasonable to assume that a wrong psm has an equal probability to come from either protein sequence . in this case, the number of decoy psms is an excellent estimate for the number of wrong hits among target psms. a desired false discovery rate  can be achieved by varying the score threshold and counting decoy results until reaching a suitable cutoff value. even though providing a very good method to select a set of psms with accurate estimate of its fdr, the target-decoy search strategy, as it was originally conceived, does not consider sensitivity, i.e., no computational strategy and performance metrics are applied to find alternative sets of psms having the same fdr but with higher number of hits  <cit> .

cerqueira et al.  <cit>  proposed a new strategy called mude  to extend the target-decoy method. using sequest for their experiments, the authors prove that a much higher sensitivity can be achieved. the enhancements are two-fold. first, the authors consider many more quality parameters than usual , namely, xcorr, Δcn, Δm, sprank, percions, and rt  p-value. second, in the mude approach, the problem of finding threshold values leading to the desired fdr is treated as an optimization problem in contrast with simplistic procedures usually employed to explore possible values. as a consequence, a much higher discriminatory power is achieved when compared to the traditional target-decoy search strategy and to peptideprophet, resulting also in a significant higher sensitivity for the same fdrs. note, however, that the mude approach provides linear decision boundaries to separate false from true positives. furthermore, according to the authors, the heuristic used to solve the proposed optimization problem has to be executed several times in order to visit many local optima, and the final result is a merge of several outputs obtained. to achieve the results shown in  <cit> , the authors performed  <dig> runs of the proposed procedure. each run takes on average  <dig> s, meaning a total running time of  <dig>  minutes, approximately. considering that a manual curation may take days or weeks, this is quite a good performance. on the other hand, it clearly demonstrates room for enhancements.

we present here mumal, a computational tool to perform multivariate analysis for the target-decoy search strategy using powerful machine learning techniques. this is an improvement to the mude method, where the optimization procedure is replaced by the application of neural networks  to find better decision boundaries, even in non-linearly separable data, and the resulting roc  curve is analyzed to further improve sensitivity. experiments were performed on the same data generated by sequest that was used to evaluate the mude approach. in this data, there are six datasets derived mostly from phosphoproteins, and five datasets from non-phosphorylated proteins. given a certain dataset, we start training a neural network to separate decoy from non-decoy psms. the features used for training are the six scores proposed in the mude procedure. in a second stage, the resulting roc curve of the nn model is analyzed to determine the best probability threshold leading to the highest sensitivity for the chosen fdr. the user has the chance to run the same procedure many times, using different parameter settings, and merge the best answers  of each run in a unique output, similarly to the mude pipeline. the difference is that with considerably fewer iterations, we could achieve significantly better sensitivities when comparing with mude. in our experiments, we have chosen fdrs varying from  <dig> to  <dig> , so that we could compare the number of psms our method and the mude approach could retrieve for the same error rates. the results were quite encouraging. for non-phosphodata, the sensitivities were ca. 26% higher, while phosphodata presented an average improvement of 24%. furthermore, the running time of our procedures was strikingly shorter. a nn model takes approximately the same time to be built when compared to a mude run. notice, however, that only few nn runs are necessary to achieve much better sensitivities. in our experiments, we performed six nn rounds for each data in contrast with the  <dig> runs of the mude approach. in summary, the proposed strategy is able to enhance sensitivity with a running time  <dig>  times faster than mude.

methods
ms/ms data
in this work, we used the same data generated from a lc-ms/ms approach ) described in the mude publication  <cit> . for more information on sample preparation details see cerqueira et al.  <cit>  and morandell et al.  <cit> . three datasets were produced from three independent phospho-enriched samples. ms/ms spectrum files were converted to dta files, the text-file format of sequest for ms/ms spectra, resulting in  <dig> ,  <dig>  and  <dig>  spectra, respectively. next, sequest  was run on this data to assign peptide sequences to each spectrum. each dataset  was divided in two parts, one containing spectra whose top result was reported as a phosphopeptide, and the other composed by spectra whose the best assignment indicated a non-phosphopeptide. each part was further split based on the precursor charge state. only charges + <dig> and + <dig> were considered. as a result, the three initial datasets generated twelve sets. these separations are necessary as score distributions may vary significantly from a dataset of phosphorylated proteins to another of non-phosphorylated proteins. important differences in scores are also noted in datasets with distinct precursor charge state  <cit> . the twelve datasets were labeled as s1_ph_ch <dig>  s1_ph_ch <dig>  s1_nph_ch <dig>  s1_nph_ch <dig>  s2_ph_ch <dig>  s2_ph_ch <dig>  s2_nph_ch <dig>  s2_nph_ch <dig>  s3_ph_ch <dig>  s3_ph_ch <dig>  s3_nph_ch <dig>  and s3_nph_ch <dig>  where "ph" and "nph" denote phosphodata and non-phosphodata, respectively, while "ch2" and "ch3" represent + <dig> and + <dig> charge states, respectively. the dataset s3_nph_ch <dig> was removed from our experiments as it has shown to contain fewer than  <dig> correct assignments. it was verified by a decoy db analysis and with trans-proteomic pipeline v <dig>    <cit> .

finally, in order to use retention time as a discriminatory feature in our method for identification assessment, the out files  of each set was converted to a unique idxml  file. this is the format used by the algorithm  for retention time prediction described by pfeifer et al.  <cit> .

database search details
following elias et al.  <cit>  recommendation, all searches used a database constructed as a composition of target protein sequences appended to their reverse . target proteins were obtained from the mouse ipi database   <cit> . the search parameters were set the same for all runs. enzyme: trypsin; missed cleavages: up to 2; fixed modifications: carbamidomethyl , methyl , methyl ; variable modifications: oxidation , phosphorylation , phosphorylation ; protein mass: unrestricted; mass values: monoisotopic; peptide mass tolerance: ± <dig> ppm; fragment mass tolerance: ± <dig>  da.

shotgun proteomics and decoy db analysis
the shotgun strategy by means of lc-ms/ms is currently the standard method for analyzing complex mixtures. this strategy arose from an analogy to shotgun dna sequencing, where small dna molecules are computationally assembled into the continuous target sequence. as illustrated in figure  <dig>  shotgun proteomics entails: the digestion of proteins in a complex mixture into peptides, the separation of these peptides by liquid chromatography , a continuous and automatic acquisition of peptide fragmentation spectra by tandem mass spectrometry, and, finally, the application of computational tools, such as sequest and mascot, to interpret each ms/ms spectrum, resulting in the identification of proteins present in the sample, including their abundance level and ptms  <cit> . an important demonstration of the power of this method is the work of washburn et al.  <cit> , where almost  <dig> yeast proteins were identified, comprising also low-abundance proteins such as transcription factors and protein kinases. the present work is based on the computational aspects related to peptide/protein identification using the shotgun approach. in particular, the following text focuses on the ms/ms spectrum interpretation problem and describes the elements involved in our proposed method.

in shotgun proteomics, a natural necessity has arisen to automatically evaluate resulting psms, given the huge amount typically produced in a single run. one of the most widely applied procedures to evaluate psms generated by db search methods is the target-decoy db search strategy. in this method, false  protein sequences are generated maintaining the amino acids distribution of real  protein sequences. the search is then performed either once using a composite db containing target sequences appended to decoy sequences or twice using the same parameters and each sequence db at a time. the most common ways to generate decoy sequences are reversing target ones, shuffling them, or using some randomization process  <cit> . the construction of a decoy db as proposed in literature allows the assumption that a wrong hit  might come either from a real sequence or a target one with the same probability. this means that the number of hits coming from decoy sequences can be taken as a very good estimate of the number of wrong psms coming from target sequences. the main advantage of this method is that there is no a priori assumption on data distribution, which made this strategy very popular in proteomics. particularly, the target-decoy db search strategy is frequently present in phosphoproteomics research, since scores of phosphodata have a very peculiar distribution  <cit> .

in this work, we used a composite db of target and reversed sequences. as decoy psms are clearly wrong, they are used to estimate the number of wrong hits among target hits, but they are not considered in the fdr calculation, as seen in previous works. hence, for a given dataset of psms, fdr is estimated by:

  fdr^=dtnt-dt. 

dt is the number of decoy psms filtered through a set of thresholds t, and nt is the total number of peptide identifications  using thresholds in t. figure  <dig> illustrates the estimation of fdr for different score thresholds.

as already mentioned, decoy db methods have been widely applied to find score thresholds leading to a desired fdr, particularly in the case of phosphodata with typically odd score distributions. however, to our best knowledge, this method has been used without any attempt to maximize sensitivity, where sensitivity here means the proportion of true identifications captured by the chosen thresholds. either only one quality parameter is varied or, even when more scores  are explored, after thresholds are determined that produce the desired fdr, no other score combination that might provide a higher number of identifications is investigated and verified. therefore, the inclusion of other parameters in the analysis as well as a more systematic and elegant way to explore them are a clear direction for improvements.

multivariate analysis in the target-decoy db strategy
using multivariate analysis in mude for psm assessment, sensitivity is expected to increase, i.e., a higher number of psms can be detected for a given error ε. this was previously illustrated by figures 3a and 3b <cit> . in figure 3a, twelve peptide hits are shown including their xcorr and Δcn  values. this example demonstrates that to obtain fdr =  <dig> using only xcorr, just three hits are retrieved. when Δcn is included, on the other hand, five psms are obtained with the same error. this is also emphasized in figure 3b where values of part  are plotted in the cartesian plane.

in mude, other four important parameters are included: Δm, sprank, percentage of ions found, and rt deviation , i.e., six features are considered for the assessment procedure instead of one or two as stated by previous works. additionally, mude presents an optimization procedure, termed ε-masp, to maximize sensitivity for a fixed error ε. even demonstrating a significant increase in sensitivity, this method presents two characteristics that could be further improved. first, the optimization method produces only linear decision boundaries. however, we show in figure 3b that a non-linear decision boundary  could provide an even higher sensitivity for the same fdr. second, the mude's optimization procedure has to be repeated several times in a typical run to ensure a high sensitivity. notice that non-linear learning algorithms can establish more appropriate decision boundaries, leading to high sensitivity, in a single run.

therefore, instead of pursuing a set of thresholds for psm scores, as stated in former procedures, our approach seeks now the establishment of a more complex function to combine such scores, representing a more accurate decision boundary. this is exactly what support vector machines  and neural networks can provide.

deciding the learning algorithm
before further developing our procedure for psm assessment, we performed a comparison between the svm approach and nns to decide which method should be chosen as the main learning algorithm in the mumal pipeline. we used the eleven datasets mentioned in section "ms/ms data" to analyze which approach could provide a higher sensitivity for a 1% fdr. according to elias et al. and balgley et al.  <cit> , this fdr represents the best trade-off between sensitivity and precision when assessing psms. see section "varying the discriminant probability to achieve a desired fdr" for details on how to calibrate a learning algorithm, using the roc curve and decoy hits counting, to obtain a decision boundary that provides the pursued fdr.

the comparisons were made using the weka  application programming interface   <cit> , which provides two different implementations of the svm approach: smo  <cit>  and libsvm  <cit>  as well as an implementation of a multilayer neural network with backpropagation. for nn runs, default parameter values were used. in the case of libsvm and smo, the only change in parameters was probability estimate = true to allow probability calculation instead of dichotomous classification of type "yes" or "no". for more details on parameters of these methods, see tan et al.  <cit>  as well as platt  <cit>  and fan et al.  <cit> .

the result can be seen in table  <dig>  it clearly demonstrates the superiority of nns when compared with svm. in all datasets, the number of extracted psms was significantly higher for nns. in some cases, it presented more than a two-fold increase. as described in the following sections, such derived datasets using the target-decoy approach can be considered as very noisy, since most non-decoy hits present similar characteristics as decoy hits. table  <dig> shows that nns were capable to cope with such a particular situation more appropriately when compared to svm.

the values indicate the number of psms that the learning method could retrieve when considering a 1% fdr. the nn values were significantly better. the dashes indicate that the algorithm could not find a set of hits with 1% fdr, i.e., there is no point in the roc curve corresponding to such an error rate.

given the results of this first experiment, we proceeded with the development of the proposed method using neural networks as the learning algorithm of our pipeline.

neural networks
the study of artificial neural networks is an effort to mimic biological neural systems with the objective to create a powerful learning technique  <cit> . similarly to human brain, a nn is comprised of a set of nodes interconnected by directed links. the first proposed model was called perceptron  <cit> . only two kinds of nodes  are present in this simple architecture: input nodes and one output node. nodes of the first type represent features, while ones of the second kind represent the model output. each input node is connected to the output node by a weighted link. the weights represent the strength of synaptic connections between neurons. note that the human learning process consists exactly of changing the strength of such connections due to some repeated stimulus. in a perceptron, the output node computes the weighted sum of the inputs, subtracts the result by a bias term, and uses what is called an activation function  to produce the final output   <cit> . hence, the process of training a perceptron is the adaptation of weights until getting an acceptable relation between input and output according to what is observed in training data.

in order to model more complex relationships between input and output values, the perceptron model has rapidly evolved to a more complete structure termed multilayer neural network. in this model, the network may contain various intermediary layers called hidden layers . besides, the network may apply more complex activation functions, such as sigmoid  and hyperbolic tangent functions. all this combined, including an output layer with possibly more than one node, allows the production of more flexible and useful decision boundaries. furthermore, the learning procedure may apply a method called backpropagation, where the deviation between observed and expected outputs is used in a sophisticated weight update formula in reverse direction, i.e., weights at level d +  <dig> are updated before weights at level d  <cit> .

we have chosen the multilayer with backpropagation approach implemented in the weka api  <cit> . our nn architecture is depicted in figure  <dig>  as can be seen, the input layer nodes correspond to the six features cited in the last section, there is one hidden layer , and the output layer has two nodes, since we wish to perform binary classification . for each data, we performed six runs using the same parameter variations . table  <dig> describes parameter details .

we performed six runs for each data using the settings shown in the table.

varying the discriminant probability to achieve a desired fdr
many binary classifiers, including binary nns, may build models to output probabilities instead of hard 0's and 1's. in this case, the model is normally built in such way that the probability  <dig>  is set up as the threshold value to decide to which class a given example belongs . in nns with sigmoid functions, for instance, the mapping between output values and probabilities are established using these functions, as illustrated in figure  <dig>  notice that negative values are mapped to probabilities lower than  <dig> , positive values are mapped to probabilities greater than  <dig> , and  <dig> corresponds exactly to p =  <dig> .

the learning procedure normally seeks to maximize the number of correctly classified instances, i.e., the accuracy. it is expected that our datasets lead to low-accuracy models, since our classes are decoy and normal hits. notice that most of normal hits  will have similar characteristics when compared to decoy hits . this is due to the fact that most of interpretations performed on ms/ms spectra are wrong. because of this property in the shotgun approach, our data can be thought as very noisy data, which makes the model construction a challenging task. in fact, the average accuracy obtained for our eleven datasets was 60% and the fdr for p > <dig>  in all cases was very high.

nonetheless, the nn training is just the first stage of our procedure. in order to achieve a more useful decision boundary with a maximum predefined fdr, or error ε, we propose a cost/benefit analysis for different probability thresholds as a second stage. after the model construction, we vary the discriminant probability until getting a value that leads to a fdr not greater than ε. this is exactly what roc  curves explore. a roc curve is a graphical plot of true positive rate vs. false positive rate for several distinct discriminant thresholds  <cit> . it allows to visualize which point could be selected as the best trade off between what is correctly captured by a chosen cutoff and the consequent error . figure  <dig> shows the roc curve generated from a nn model for s3_nph_ch <dig> . notice, however, that the fdr calculation here is performed according to equation  <dig>  for a given discriminant probability p, we count the number of examples n with probability >p and the number of decoy examples d among n. then, equation  <dig> is applied to estimate fdr.

as the model construction is performed to maximize accuracy, we expect maximization of sensitivities as well. notice that the mude approach also tries to maximize sensitivity. the difference in our case is that the models obtained here can construct non-linear decision boundaries, denoting the possibility of even higher sensitivities, as stated previously in the text.

framework for identification assessment
RESULTS
in this paper, we propose a multivariate decoy db analysis using neural networks and roc analysis to produce more flexible decision boundaries. as described for the mude procedure, we also take advantage of many important scores in contrast to the bivariate decoy analysis  of previous works. on the other hand, mumal achieves higher sensitivity and much faster running times when compared to mude, as can be seen in our experiments below. notice that psms are used to build a nn model, which, in turn, is applied to the same data as our goal is not to apply the obtained model to future unseen instances, but, instead, we want to separate correct from incorrect hits. hence, there is no sense here in applying traditional statistical methods to evaluate learning algorithm models, such as cross validation. the main measure to evaluate our models is the number of true positives that can be achieved for a certain maximum fdr.

our comparisons were performed on the peptide level. as previously demonstrated, improvements on peptide level lead also to improvements on protein level, possibly leading to a higher proteome coverage   <cit> . this is quite obvious, as proteins are inferred from peptide identifications. thus, we limit our analysis to the peptide level, i.e., the amount of correct psms our method could separate for a predefined maximum fdr. the experiments below demonstrate the superior performance of mumal regarding the main tools currently used for psm validation: mude, peptideprophet, and bide . see the work of cerqueira et al.  <cit>  for details on how these previous methods were applied to generate the curves shown next.

for phosphodata, we also included a bide analysis using xcorr and Δm. according to beausoleil et al.  <cit>  and jiang et al.  <cit> , Δcn scores are normally suppressed when a phosphopetide has more than one potential phosphorylation site. therefore, the use of Δcn may be inappropriate for phosphodata. as can be seen in figure  <dig>  the scenario has not changed much. mumal curves show once more its superior performance, demonstrating an improvement in sensitivity of 24% on average comparing with mude results. it is also worth noting that peptideprophet performance is inferior compared with the other procedures, confirming that the former is indeed not appropriate to phosphodata.

another comparative analysis was performed between mumal and mude by means of venn diagrams. in this experiment, we compared the number of exclusive identifications that each method could deliver for a 1% fdr. figures  <dig> and  <dig> demonstrate that our method could in most cases find a significantly higher number of exclusive hits. this is an important fact, since exclusive findings might refer to exclusive proteins or, at least, represent a higher coverage  or a higher number of matches  for proteins detected in both cases.

finally, figure  <dig> depicts an example of a spectrum detected by mumal in dataset s2_ph_ch <dig>  the same spectrum was disregarded by mude. a manual inspection reveals that this interpretation is probably correct. first, the spectrum has a typical prominent central peak  representing neutral loss of two h3po <dig> groups undergone by the doubly-charged precursor ion . second, the b/y series are mostly suppressed, which is also a strong characteristic of phosphopeptide spectra. finally, the protein that originated the assigned peptide is senp <dig> , for which phosphorylation site ser  <dig> is already reported in the literature  <cit> . notice that various large-scale gene expression studies demonstrate important variations in the level of senp <dig> in many different types of cancer  <cit> . bawa-khalfe et al.  <cit> , for instance, demonstrate that changes in the senp <dig> expression induce prostatic intraepithelial neoplasia. note yet that the datasets used here were originated from the work of morandell et al.  <cit> . in this work, a novel screening platform termed qiks is proposed to identify kinase substrates. particularly, the authors aimed at finding substrates of mitogen-activated protein kinase/erk kinase . they have listed hundreds of phosphorylated proteins using their platform. however, after inspecting their report, we could not find senp <dig>  this means that our method could detect a substrate they were not able to find using standard spectrum evaluation tools. considering that the protein senp <dig> plays a role in cancer, its phosphorylation sites might be an important information, since malfunction of phosphorylation is known to be related to various serious diseases, including cancer  <cit> .

CONCLUSIONS
it has been largely demonstrated that the target-decoy search strategy is a powerful tool for evaluating psms of ms/ms runs. nonetheless, the potential of this method has not been fully explored as sensitivity maximization is not taken into account in typical experiments. the mude approach treats the decoy analysis as an optimization problem, enabling a significant improvement in sensitivity. in this work, we present mumal, a psm evaluation pipeline that uses machine learning methods, namely neural networks and roc curve analysis, to promote an even higher increase of sensitivity, i.e., the retrieval of as many psms as possible for a fixed error rate. experiments demonstrate that our approach can establish better decision boundaries, embracing a higher number of true positives than mude and other standard methods.

the next step is to perform new experiments with alternative machine learning algorithms and, if they show promising results, to optimize their models to reach higher sensitivities. another future effort will focus on extending the method to cope also with mascot results.

with the new proposed strategy, experiments on ms-based proteomics will gain in performance with respect to both time and proteome coverage, so that a better understanding of cellular activities can be achieved, advancing ultimately the utility of proteomics in the process of discovery and development of new drugs.

competing interests
the authors declare that they have no competing interests.

authors' contributions
frc, ag, and cb designed all analyses; frc, rsf, apo, apg, and hjor were responsible for carrying out the analyses; frc, ag, and cb wrote the initial draft of the manuscript; all other authors contributed to posterior revisions to the final draft. all authors read and approved the final paper. 

addendum: url for software download
the software is open-source and is available under the url: http://sourceforge.net/projects/mumal/

