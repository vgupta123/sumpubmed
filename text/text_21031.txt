BACKGROUND
the use of large-scale experimental techniques and biomedical tools has increased the pace at which biologists produce useful information. this also promotes the growth of the scientific literature, which contains information on those experimental results in the form of free text that is structured in a way which makes it straightforward for humans to read but more difficult for computers to interpret automatically and search efficiently. as a consequence, there is increasing interest in methods that can handle collections of biomedical texts. such methods include systems that efficiently retrieve and classify information in response to complex user queries, and beyond this, systems that carry out a deeper analysis of the literature to extract specific associations.

information retrieval  deals with text analysis, text storage, and the retrieval of stored records having similarity between them  <cit> . in context of biomedical domain, ir systems are to retrieve documents/passages that a user might find relevant to his or her information need. what many information seekers, really desire to be provided short, specific answers to questions and put them in context by providing supporting information and linking to original sources  <cit> . there are situations when the terms retrieved by ir systems, are not the only desirably independent but associations among the terms within different contexts or a single text, which provide an insight into the text as answers, might be of interest in some specific domains like biomedical domain, text summarization, question answering systems and so on. 

in this paper, we focus on discovering term associations among the keywords from a query. taking all the keywords as a sequence, we consider some subsequences as terms and propose a factor analysis based model to provide knowledge for finding the importance of term associations statistically. in our scientific fields, variables such as "intelligence" or "leadership quality" can not be measured directly. such variables, called latent variables, can be measured by other "quantifiable" variables, which reflect the underlying variables of interest. factor analysis attempts to explain the correlations between the observed term associations in terms of the underlying factors, which are not directly observable. these latent factors can be considered the same as the hidden variables of "eliteness" introduced by robertson et al  <cit>  in order to gain some understanding of the relation among multiple term occurrences and relevance. the observations for the proposed approach can be obtained from the keywords that are extracted from the queries, and from the passages retrieved by an ir system. in order to find the latent factors for term associations, we compute the factor loadings  <cit>  using matlab  <cit> . then we calculate the communalities  <cit>  based on factor loadings to indicate the importance and reliance of latent factors and use them to recursively re-rank the baseline result for improving retrieval performance. in addition, in order to evaluate the superiority of the proposed approach, the generalized sequential pattern  algorithm is adopted as a comparison. 

the paper is organized as follows. first, we briefly present the experimental results and discussions in the results and discussion section, where the ir environment is introduced with the descriptions of the data sets, queries, evaluation measures, the ir system and indices. the comprehensive empirical study includes the analysis for the baselines, the proposed term association, the influence of different indices and k for the recursive re-ranking algorithm, the comparisons to the gsp algorithm and the official submissions. second, we show our contributions in the conclusion section. third, in the methods section, we propose our methods systematically and consistently. a term association approach is presented, followed by a factor analysis based model and a corresponding algorithm, including a recursive re-ranking algorithm. the related work is also presented in this section.

results and discussions
here we report the results obtained from a set of experiments conducted on the trec 2004- <dig> genomics data sets and  <dig> hard data set, in order to evaluate the effectiveness of the proposed model and algorithms.

experimental environment
data sets and queries
we evaluate the proposed model and algorithms on the trec 2004- <dig> genomics data sets, since we focus on the biomedical domain. furthermore, we also apply the trec  <dig> hard data set for evaluation.

trec  <dig> and  <dig> genomics data sets provide a test collection of  <dig>  full-text documents assembled with  <dig> queries in  <dig> and  <dig> queries in  <dig>  the trec  <dig> queries are in the form of questions asking for lists of specific entities. the definitions for these entity types are based on controlled terminologies from different sources, with the source of the terms depending on the entity type  <cit> . the trec  <dig> queries are derived from the set of biologically relevant questions based on the generic topic types   <cit> . all these queries are listed on the official genomics website at: http://ir.ohsu.edu/genomics.

trec  <dig> and  <dig> genomics data sets consists of a document collection for the ad hoc retrieval task which is a 10-year subset of medline with completed citations from the database inclusive from  <dig> to  <dig>  this provides a total of  <dig> , <dig> records  <cit> . each record is an abstract of a document. then in this paper, we take an abstract as a passage. there are  <dig> queries for each year respectively. more information can be found at: http://www.ncbi.nlm.nih.gov/.

trec  <dig> hard data set consists of entirely of english text, such as the agence france press , associated press , central news agency , la times/wash post , new york times , salon.com , ummah press , xinhua english  with the total collection of  <dig>  documents. in our research, we parse the documents into passages  <cit> . there are  <dig> queries used in this paper.

evaluation measures
the trec genomics track has three evaluation measures that are the document-level, the aspect-level and the passage2-level   <cit> . each of these provides insight into the overall performance for a user trying to answer the given queries and measured by some variant of mean average precision , which are briefly described as follows.

document-level
this is a standard ir measure. the precision is measured at every point where a relevant document is obtained and then averaged over all relevant documents to obtain the average precision for a given query. for a set of queries, the mean of the average precision for all queries is the mean average passage precision of that ir system.

passage-level
as described in  <cit> , this is a character-based precision calculated as follows. for each relevant retrieved passage, precision will be computed as the fraction of characters overlapping with the gold standard passages divided by the total number of characters included in all nominated passages from this system for the topic up until that point. similar to regular map, relevant passages that are not retrieved will be added into the calculation as well, with precision set to  <dig> for relevant passages not retrieved. then the mean of these average precisions over all topics will be calculated to compute the mean average passage precision.

passage2-level
this is a new character-based map measure which is added to compare the accuracy of the extracted answers and modified from the original measure passage map. passage <dig> treats each individually retrieved character in published order as relevant or not, in a sort of "every character is a mini relevance-judged document" approach  <cit> . this is done to increase the stability of the passage map measure against arbitrary passage splitting techniques.

gold standard
a gold standard is created by extracting out the relevance passages and entities for each topic. judges for the relevant passages and entities are recruited from the institutions of track participants and other academic or research centres. they are required to have significant domain knowledge, typically in the form of a phd in a life science. in summary, judges are given the following three instructions. first, reviewing the topic question and identifying key concepts. second, identifying relevant paragraphs and selecting minimum complete and correct excerpts. third, developing controlled vocabulary for entities based on the relevant passages and coding entities for each relevant passage based on this vocabulary  <cit> .

system
we used okapi bss  as our main search system. okapi is an information retrieval system based on the probability model of robertson and sparck jones  <cit> . the retrieval documents are ranked in the order of their probabilities of relevance to the query. search term is assigned weight based on its within-document term frequency and query term frequency. the weighting function used is bm <dig> 

 w=*tfk+tf*log//**qtfk3+qtf⊕k2*nq* 

where n is the number of indexed documents in the collection, n is the number of documents containing a specific term, r is the number of documents known to be relevant to a specific topic, r is the number of relevant documents containing the term, tf is within-document term frequency, qtf is within-query term frequency, dl is the length of the document, avdl is the average document length, nq is the number of query terms, the kis are tuning constants , k equals to k <dig> *  + b * dl/avdl), and ⊕ indicates that its following component is added only once per document, rather than for each term.

in our experiments, the tuning constant parameters k <dig> and b are set to be different values. k <dig> and k <dig> are set to be  <dig> and  <dig> respectively. furthermore, we have added the query expansion module on okapi bss, which provides two query expansion algorithms for constructing structured queries to deal with synonyms, the frequent use of acronyms and homonyms  <cit> .

indexing
one important issue that ir systems have to deal with is the size of the retrieved passages and the granularity of the indexed information. in the context of text retrieval, the granularity of the indexed text can be defined as the length of the indexed text unit and the size can be defined as the length of the retrieved passage. in this paper, we call an indexed text unit as a passage.

three indices are built on the  <dig> and  <dig> genomics data sets according to three passage extraction methods and a paragraph-based index is built on the  <dig> and  <dig> genomics data sets  <cit> . a paragraph-based index is set up on the  <dig> hard data set as well. the sentence-based indexing is based on passages each of which has up to  <dig> sentences. the paragraph-based indexing is generated on passages each of which is a paragraph. here a paragraph is defined as the sequence of sentences between the <p>and </p>tags from the html data set. the word-based indexing forms passages using a dynamic window  <cit>  .

experimental 
RESULTS
we report the baseline results in table  <dig>  which shows the performance under five parameter settings with three different indices in terms of the document-level, the passage-level and the passage2-level on the genomics 2004- <dig> data sets and hard  <dig> data set respectively. five groups have been set for the parameters of  with their indices. therefore, there are  <dig> runs on all five trec data sets. note that only a paragraph-based index is set up for the trec  <dig> and  <dig> genomics data sets and the trec  <dig> hard data set.

the baseline results are presented:  five parameter settings for  at the first and second columns;  three different indices, where "word" stands for the word-based index, "sentence" for the sentence-based index and "paragraph" for the paragraph-based index;  three evaluation measures as the document-level, the passage-level and the passage2-level;  five trec data sets as the trec 2004- <dig> genomics data sets and the trec  <dig> hard data set;  only a paragraph-based index is set up for the trec  <dig> and  <dig> genomics data sets and the trec  <dig> hard data set, as mentioned in the section of indexing.

corresponding to the baseline results, we generate the results of the term association approach using our proposed algorithms. the performance and improvements are presented in table  <dig>  the values in the parentheses are the relative rates of improvement over the original results.

the results of the term association approach are presented:  all the runs are under the same settings as the baselines in table  <dig>  including the parameter settings of , the indices and the evaluation measures;  the factor analysis base algorithms and the recursive re-ranking algorithms are applied;  the values in the parentheses are the relative rates of improvement over the original results.

influence of parameter settings and indices
in order to investigate the influence of different indices and parameter settings, we will deeply analyse the experimental results. first, taking the trec genomics  <dig> and  <dig> data sets as an example, we compute the max, min, mean and sample standard deviation of the baselines in table  <dig>  from this table, we can see how these settings effect the result, since there is a disparity between the max and the min values under all the measures. focusing on the sample standard deviation, the ssd values are calculated as a sample standard deviation of a discrete random variable. compared to the mean, the ssd also shows the influence of the different indices and parameter settings.

we compute the max, min, mean and sample standard deviation  of baselines on the trec genomics  <dig> and  <dig> data sets:  the parameter settings of  and the different indices effect the baseline results greatly, since there is a disparity between the max and the min values under all the measures;  the ssd values are calculated as a sample standard deviation of a discrete random variable;  the values in the parentheses are the relative rates of improvement over the means;  the ssd also reflects the influence of the different indices and parameter settings of .

to illustrate the results in table  <dig> graphically, we re-plot these data in figure  <dig> and figure  <dig>  the performance of the baseline results is shown in terms of the document-level, the passage-level and the passage2-level. the x-axis represents the evaluation measures, where "word", "sen" and "par" stand for the word-based, the sentence-based and the paragraph-based indices. the y-axis shows the map performance. this figure shows that the sentence-based index produces the best results in terms of the document-level, the word-based index for the best results in terms of the passage-level and the paragraph-based index for the best results in terms of the passage2-level. this finding also confirms our motivation for building up different indices for different information needs.

influence of term association
in order to illustrate the term association results in table  <dig>  we plot them graphically in figure  <dig> and figure  <dig>  it clearly shows that, for all the measures on five trec data sets, the term association approach always outperforms the baselines. the improvements in the parentheses explain the significance evidently. more interesting, the figures of the factor analysis results almost have the same distributions as the figures of baselines. the best factor analysis results always come from the best baseline results. the sentence-based index produces the best factor analysis results in terms of the document-level, the word-based index for the best factor analysis results in terms of the passage-level and the paragraph-based index for the best factor analysis results in terms of the passage2-level.

in order to illustrate the improvements of term association, in table  <dig>  we plot them graphically in figure  <dig>  figure  <dig>  figure  <dig> and figure  <dig>  there are two observations as follows. first, the positive values of the improvements notify that term association carries important weight on the retrieval results, which is much better than the baselines that only consider the unigram keywords independently. in other words, those bigram and trigram associations have more influential in the retrieval results than the independent keywords. second, the influence in terms of the passage levels  is greater than that in terms of the document-level. we also can see in figure  <dig>  figure  <dig> and figure  <dig>  that the absolute values of improvements on the passage-level are much higher than those on the document-level. this can be explained that term association is more efficient to be applied in the sentences or paragraphs compared to the documents.

influence of k for recursive re-ranking
we initialize the depth as k =  <dig> in the recursive re-ranking algorithm. the number k stands for the top k term associations weighted by the factor analysis based model. we recursively re-rank the retrieved passages according to whether the passages contain the top k term associations or not. we conduct a series of experiments with different settings of k values in order to investigate the influence of value k and find a local optimization value for the proposed algorithm. we first randomly choose five original baselines from our five data sets respectively, namely genomics  <dig>  genomics  <dig>  genomics  <dig>  genomics  <dig> and hard  <dig>  then the factor analysis model is applied on the baselines. five numbers such as  <dig>   <dig>   <dig>   <dig>   <dig>  are tested and the performance is shown in table  <dig>  we can see that number k affects the performance greatly when k is smaller than  <dig>  however, when k becomes larger than  <dig>  the final performance almost has no change. therefore, we get this local optimization number as  <dig> for k in the recursive re-ranking algorithm for all the runs.

the number k is the parameter in the recursive re-ranking algorithm:  the empirical study makes a local optimization number k =  <dig> as the final depth in the final experiments;  k stands for the top k term associations weighted by the factor analysis based model;  the recursive re-ranking algorithm will re-rank the baselines according to these k terms;  the more the results contain terms among these k terms, the higher ranking scores the results obtain;  five numbers such as  <dig>   <dig>   <dig>   <dig>   <dig>  are tested;  five original baselines from our five data sets respectively, namely genomics  <dig>  genomics  <dig>  genomics  <dig>  genomics  <dig> and hard 2004;  k affects the performance greatly when k is smaller than  <dig>  while the final performance almost has no change if k becomes larger than  <dig> 

comparison with gsp algorithm
we adopt the gsp algorithm as a comparison to our proposed approach. in order to map the gsp algorithm to our research problem, we treat the keywords extracted from the queries as the singleton items and n passages retrieved by the system for each query as the transaction database. therefore, the candidates of  <dig> - sequences are all the keywords, the k - sequences candidates are generated on the frequent  - sequences. for the support counting, we define the minimum support value corresponding to each query as follows. first, the counts of candidates are automatically calculated by the modified gsp algorithm, including all k - sequences. then, we simulate the counts as a non-parametric distribution. third, the 95% confidence interval of this distribution is computed, where the lower bound is the minimum support value for this gsp algorithm.

in this section, we study how the gsp algorithm performs on our five data sets. here we focus on the experimental results with the paragraph index under five parameter settings, as shown in table  <dig> 

the gsp algorithm is adopted as a comparison to the proposed approach:  the candidates of  <dig> - sequences are all the keywords, the k - sequences candidates are generated on the frequent  - sequences, after mapped the gsp algorithm to our research problem;  the counts of candidates are simulated as a non-parametric distribution, where the lower bound of the 95% confidence interval is the minimum support value for this gsp algorithm;  only the paragraph index under five parameter settings of  is considered;  the best results of the gsp algorithm are compared with the best of the baselines and the proposed term association approach;  "ta" stands for term association;  the values in the parentheses are the relative rates of improvement over the original baselines.

furthermore, we compare the best results of the gsp algorithm, the baselines and the proposed term association approach.

an interesting finding is drawn from the results of the gsp algorithm. the gsp algorithm works very well in terms of the passage-level and the passage2-level, while it is not good for the document-level. this can be explained by the following scenario. the frequent  <dig> - sequence t1t3t <dig> is found in the documents d <dig> and d <dig>  in d <dig>  t1t3t <dig> is contained in a short passage so that d <dig> earns good map results on the document-level and the passage-level. in the document d <dig>  the situation is that t <dig>  t <dig> and t <dig> are found in different passages respectively. since t1t3t <dig> is still found as a sequence based on the definitions, d <dig> is given a high weight and is going to earn good performance at least on the document-level. however, the standard evaluation does not think d <dig> is qualified to be a relative document so that d <dig> decreases the performance of the document-level.

compared to the gsp algorithm, the proposed term association approach outperforms the baselines and the gsp results on all the measures. the factor analysis based model considers not only the concurrence of the terms, but also the dependency, especially in the high order structure. in the gsp algorithm, the document d <dig> is given a good score. however, in the factor analysis based model, the factor loadings of t1t3t <dig> in d <dig> is very small, since t1t3t <dig> is not treated as a trigram term association. t <dig>  t <dig> and t <dig> are three unigram terms, while t1t3t <dig> is a frequent  <dig> - sequence in the gsp algorithm. so the proposed approach avoids assigning a high weight to the document d <dig> 

the major difference among our proposed approach, ngram and plsa, is that term associations are not dependent on the previous associations, whose reliance and importance are decided by the dependencies among the keywords in the passages, not by their probabilities upon the previous terms. for example, an interesting finding using factor analysis in this work, is that the bigram k1kj might have the highest reliance, even though their previous unigram term k <dig> or kj is not the most important for a query in some ir systems. and our experiment confirms that k1kj plays an important role in the improved re-ranking result. therefore, one of the major contributions of the proposed approach is to extract subsequences as term associations from a query without preliminary knowledge. this promotes us to employ the gsp algorithm as a comparison to evaluate the proposed approach statistically, but not to compare this approach with plsa and pca.

comparison with official submissions
in order to further evaluate the term association approach to improving performance, we compare the performance of the term association approach to the official submissions at the best and mean values on the five trec data sets in table  <dig>  since the submissions of the  <dig> hard data set are not officially released, we focus on the genomics data sets. we can observe that, for the mean performance, term association outperforms baselines and the official submissions. for some best performance, term association makes improvements on baselines, but is not as good as the official submissions. however, based on the discussion upon the influence of term association in the section of influence of term association, we believe we could achieve higher performance if we have better baselines.

the performance of the proposed term association approach is compared to the official submissions:  all the results are compared at the best values and the mean values;  the submissions of the trec  <dig> hard data set are not officially released;  "ta" stands for term association and "official" for official submissions.

a case study
topic  <dig> of the trec  <dig> queries is taken as an example. the description for topic  <dig> is "what serum  change expression in association with high disease activity in lupus?". nine keywords are extracted as serum, proteins, change, expression, association, high, disease, activity and lupus. the rest words are removed by the system as the stop words. the system stems the keywords as serum, protein, chang, express, associ, high, diseas, active and lupus.

the original information of the keywords for topic  <dig> is shown:  terms are extracted with stemming;  term counts are obtained from the first round retrieved passages, which are the top  <dig> retrieved passages as the baseline;  the percentage is calculated based on  <dig> terms;  the rank depends on the term counts;  the parameters for this baseline are  =  with the paragraph-based index.

term associations are presented by applying the factor analysis based model:  the top  <dig> term associations are shown among  term associations generated by the proposed approach;  term count are computed from the top  <dig> documents of the baseline;  the communality of each term association is calculated by the factor analysis based model;  the rank of term associations is given by their communalities.

the performance of term association is compared with the performance of baseline of topic  <dig> in terms of the document-level, the passage-level and the passage2-level:  the values in the parentheses are the relative rates of improvement over the baselines;  term association outperforms baseline.

first of all, we can see that no unigram is in the ranking association list. all the term associations in table  <dig> are bigrams and trigrams. since the term association improved result outperforms the baseline, it means that term association works very well on all the measures. therefore, term association is better than only considering the keywords independently. second, the trigram "high lupus serum" has the higher reliance than the bigram "activ serum", although the trigram's term count is only  <dig>  which is much less than the bigram's term count as  <dig>  this tells us that the term frequency might not make sense when compared to term association.

CONCLUSIONS
modelling term association for improving biomedical information retrieval using factor analysis, is one of the major contributions of our work. we investigate term association among the keywords from a query and then build up a factor analysis based model to investigate the significance of term association. the proposed approach works very well on five large trec data sets. our improved performance is among top trec official results submitted in the trec 2004- <dig> genomics data sets and the trec  <dig> hard data set.

term association considering co-occurrence and dependency among the keywords produces better results than the baselines treating the keywords independently. in the other hand, the unigrams, bigrams and trigrams are terms independently computed by the factor analysis based model, which means that the trigrams are not dependent on the bigrams' importance, and the bigrams are not dependent on the unigrams' importance. their importance is decided by the model and the appearances in the passages. this is also confirmed by the gsp algorithm.

in the term association approach, keywords and the retrieved passages are the observable data, and the factor analysis based model is built up to discover the unobservable latent factors. factor loadings are computed to indicate the weights of the common factors. communalities are calculated based on factor loadings to represent the importance and reliance of the corresponding terms associations. finally, a ranking term association list is given by the model. then we recursively re-rank the baselines and report the experimental results.

the experimental results show that term association outperforms the baselines and the gsp results on all the evaluation measures, which provides a promising avenue for improving the information retrieval performance. our future work includes investigating the plsa model on the genomics research. this is also our ongoing work.

