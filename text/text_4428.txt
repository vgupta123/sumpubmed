BACKGROUND
proteins represent the working molecules of a cell, but to fully understand cell machinery, studying the functions of proteins is not enough. the biological activity of a cell is not defined by the proteins functions per se  <cit> , what it is really important is the interactions among proteins.

a group of proteins that interact in order to regulate and support each other for specific biological activities is called a protein complex. protein complexes are one of the functional modules of the cell: an example of this protein function modules are rna-polymerase and dna-polymerase.

the concerted action of different functional modules is responsible of major biological mechanisms of a cellular process such as dna transcription, translation, cell cycle control, and so on. since a protein could have several binding sites, each protein can belong to more than one complex and exhibit more than one functionality. the basic element of these modules is the protein-protein interaction . a large amount of ppi data have been identified for different biological species by using high throughput proteomic technologies. of course experimentalists can take advantage of using different online databases containing a list of ppis for each species , but unfortunately available datasets are still incomplete and contain non-specific  interactions  <cit> , in fact only a few of interactions have been verified with small scale experiments  as real interaction with an emerging function.

usually, in bioinformatics a collection of these interactions is modelled as an undirected graph, the protein-protein interaction network , where nodes represent proteins and edges represent pairwise interactions: it allows us to exploit graph theory methods and solutions.

the task of exploiting biologically relevant modules in ppins represents an active research area in bioinformatics, not only for cell understanding, but also for new drugs developing; for example, several authors, as  <cit> , are studying the mechanisms that regulate the evolutionary crossroads of p <dig> complex, responsible for different aspects of animal life, in developing human cancer cells. then, identifying protein complexes with emerging function turns into extracting sub-networks with some emerging properties. because of the importance of isolating functionally coordinated interactions, a lot of models, algorithms and strategies have been introduced to extract interesting ppi subnetwork , but each of them has proper pros and cons.

a number of clustering-based approaches have been proposed to solve the protein complex prediction problem. a well know algorithm introduced by  <cit> , the molecular complex detection algorithm , makes use of local graph properties and it is aimed at finding densely connected regions in protein interaction networks. another algorithm based on local search is the restricted neighbourhood search clustering algorithm  developed by  <cit> . this algorithm searches for a low-cost clustering by first composing an initial random clustering, then reducing the clustering cost by a near-optimal strategy. a different strategy is adopted by the markov clustering algorithm   <cit> , that divides the graph by means of flow simulation. in facts, it separates the graph into different segments, with an iteration of simulated random walks within a graph.

it is possible to increase the reliability of the ppi data by means of preprocessing techniques. some preprocessing strategies are aimed at eliminating false positive  interactions from dataset obtained by online dbs. for example  <cit>  notices that the interactions not part of dense subnetworks, are more likely to be interactions that do not exist. to identify these false positives, authors combined two topological metrics named cluster coefficient  <cit>  and centrality  <cit> . also  <cit>  uses the same algorithms, but adopting a different methodology, integrating individual topological measures into a combined measure by computing their geometrical mean. a different approach to improve the quality of ppi datasets is adopted by  <cit> , that attempts to detect those interactions that are missed by large-scale experiments or, in other words, aiming at predicting false negative by means of a topological analysis.

obviously, the best combination of the proposed techniques depends on the problem and many researchers  <cit>  have proposed different workflows.

our approach differs from the previous ones since we face protein complex extraction problem using a knowledge-based decision support system . our kdss, combining the knowledge extracted from research papers covering a lot of different strategies and methodologies, is able to suggest and run a novel workflow of tasks for the presented issue.

as it will be highlighted in results and discussion section, the suggested workflow, using a test dataset, gives the best results with regard to the other, not suggested, workflows produced by the system and moreover it provides comparable results with respect to some of the common workflows found in literature. from this point of view, our kdss represents a valid and powerful instrument that can help an experimentalist to face and solve the problem of extracting protein complexes from a ppin, supporting him in the choice, configuration and running of proper tasks.

methods
knowledge-based dss is a category of dss built using an expert system  <cit> . these systems have their own expertise based on knowledge on many aspects of the problem: the application domain, the definitions of problems within that domain and the necessary skill to solve them  <cit> . the knowledge of the system is often coded as a set of rules by one or more human experts: this kind of systems are often referred to as rule-based expert systems.

examples of dss in bioinformatics are procksi  <cit> , a system that is able to put together various protein similarity measures in order to obtain the comparison of multiple proteins simultaneously; and interpret  <cit> , a software that offers support in the analysis of magnetic resonance spectroscopy  data.

along with the development of expert and decision support systems, in recent years in bioinformatics a new type of tools, called workflow management systems   <cit> , have begun to spread out. wfmss provide a simple way to build and run a custom experiment using the most common bioinformatics resources, like online databases, software and algorithms.

the most used and famous wfms for bioinformatics is taverna  <cit> : it is able to automatically integrate tools on databases available both locally and on the web in order to build workflows of complex tasks; to run the workflows and to show results in different formats. the system works by means of a graphical user interface  or a script language. a taverna plug-in, called taverna interaction service, was introduced in  <cit> ; it extends the functionality of taverna by defining human interaction within a workflow, once it is running. more in detail, this plug-in acts as a mediation layer between the automated workflow engine and human agents. in facts, it includes a review process, provided by external collaboration partners, invoked by sending emails to target users; they, in turns, can sent back a decision to the workflow produced by taverna. in this way, users can interact with a piece of data, such as for example an annotation of a genomic region, during the workflow execution.

other wfms for bioinformatics are biowep  <cit> , that allows the user to search and run a predefined set of workflows, already tested, validated and annotated; and biowms  <cit> , that is a web-based wfms built upon an agent-based middle ware architecture.

cited wfmss, however, do not have a knowledge base, nor make decision like kdss; the kdss we present, on the other hand, offers not only support in the choice of the proper strategy, tool and algorithm, but it helps the user to configure and to run them, step by step. for this reason our system can be seen as an ideal merging point between classical dss and emerging wfms. it provides both the tools/services needed to resolve a problem, and also the knowledge necessary to suggest a specific strategy and justify its choice.

system architecture
first basic ideas of the proposed system can be read in  <cit> : in this section we will briefly describe its architecture and then we will deepen main concepts at the basis of our kdss.

the system core is represented by a rule-based expert system  <cit> . the three main components of this system are the knowledge base , the reasoner and the executor: they interact each other as shown in figure  <dig> 

in the middle part there is the knowledge base: it contains all the information, called facts, that encode the expertise of the system about a certain application domain. facts are given a rigorous and organized structure by means of an ontology of concepts  <cit> .

in order to obtain a well formed knowledge base, we adopt a precise and robust organization which, at the same time, is shareable and easily expandable, presented in  <cit> . in facts, with the introduction of a proper ontology, we can obtain a logical description of a specific problem, share the information among software agents and reuse the specific knowledge domain. in other words, we adopt a paradigm that facilitate the generalization of the application domain and the modularity and the expandability of the represented knowledge. this paradigm, called data-problem-solver , is able to distinguish and separately model "'what i need"' , "'what to do"'  and "'how to do"' , or in other words, i/o data of the problem , the set o tasks  and the way to solve these tasks . in this way, we aim at using a very general purpose system based on a kb for rule-based expert systems, that is independent from a specific domain, reusable and expandable. as showed in figure  <dig>  there is another main element used for solving a specific problem, that is the tool concept; in facts, an instance of solver contains information about which tool  satisfies the purpose  that could solve a specific task. figure also reports the most important relationship among the tree main branches of the adopted ontology.

apart from the facts, kb also has a set of rules, in the typical form if <precondition>then <action>. the rules, acting on facts, have to be considered as the coding for heuristics, guidelines and strategies adopted by an expert of the domain. both facts and rules can be provided by one or more experts of the domain or can be extracted from experimental and scientific papers, clinical guidelines and so on.

the reasoner is an inference engine that uses the facts and rules of the kb in order to make decision: it selects the strategies and the related tools that accomplish the user request according to the actual problem and the input data.

the decision taken by the reasoner are suggested to the user that can either accept them or select other available strategies and tools. in any case, it is the executor that actually runs algorithm, tools and services. it can be considered as a sort of interface between the reasoner and the library of algorithms and processing tools available locally and over the internet. the executor also updates the kb with the processing results. new results produce new facts that eventually can trigger other rules.

decision making process
facts and rules of the kb are arranged into a set of decision-making modules, as reported in  <cit> . in this section we give a brief explanation of key features of decision-making modules.

the decision-making activity of the system is based on an organization of facts and rules arranged in functional modules called decision making modules. decision making activity is task oriented. each module has knowledge and skills, takes care of a specific part of the reasoning process and it is responsible for making decisions about a well defined task. facts could be shared among different modules, whereas each rule belongs to only a module. finally, each module can activate a previously defined solver; the same solver could be activated by different modules, by using different rules.

if one module has not the needed knowledge to resolve a part of its task, it can activate another module with the proper skill. this activation mechanism defines a tree structure among decision-making modules, where parent modules manage global and general tasks and children modules are responsible of the decision-making process regarding more specialized tasks.

for example, we can have specialized modules for dealing with preprocessing, visualization, or clustering operations that can be activated by general modules which supervise global task execution. the tree structure of modules can be also represented through a treemap  <cit> : in a treemap, modules and sub-modules are shown as nested boxes.

implementation details
system implementation is based on java technology; grace to its features, such as platform and location independence, portability, os independence, java represents a good support for the proposed work. the system also contains a "rule-based system" to manage the knowledge-base; the rule based engine adopted is jess  <cit> , the rule engine for the java platform. it supports declarative approach, acting at the decision making level. jess inference engine uses rete algorithm  <cit>  as pattern matcher.

the knowledge base have been modeled using one of the largest adopted tool for building ontologies, that is protege'  <cit> . protege' is useful for represent the knowledge used by the proposed architecture, because it implements a methodology for creating ontologies based on declarative knowledge representation systems. finally, in order to generate and visualize the interactive workflow of the system, we adopt jgraphx java swing library  <cit> , that is a product family of libraries providing features aimed at applications that display interactive diagrams and graphs. among the amount of applications provided by this library, we exploit its functionality related to process diagrams, workflow visualization and flowcharts.

RESULTS
the application scenario focuses on the complexes extraction problem, that contains in turns two main sub-problems: the preprocessing and purifying of ppi data and the protein complex clustering.

the rest of this section is organized as follows: in the next sub-section we introduce the dataset used in the scenario; then we show how the proposed method system integrates aforementioned approaches and how it helps users to face the protein complexes extraction problem. finally in the last sub-section the analysis of experimental results is discussed.

experimental dataset
in our experiments, among different available online databases of ppis network, we use the database of interacting proteins . the input dataset used in this scenario is a subset of saccharomyces cerevisiae ppi-network composed by  <dig> proteins and  <dig> interactions, as shown in table  <dig>  this table reports a list of  <dig> ppis: for each ppi is shown the uniprotkb id of the first protein, the uniprotkb id of the second protein and the dip id of the interaction between the previous pair of proteins. we chose this very simple dataset because it has been well studied by  <cit>  with small scale experiments  at biological interaction level. dip also provides a subset of ppis curated manually by experts, that are called core ppis. a well studied small set of ppi allows us to better describe how the system works, and the choices it takes; obviously we know this dataset is not representative of a whole ppi network, in facts it represents only a pretext for executing the system and obtaining some results comparable with the other papers in literature  <cit>  that use the same dataset.

there are  <dig> ppis among  <dig> proteins for the species saccharomyces cerevisiae. each row contains two ppis. for each ppi is shown the first protein uniprotkb id, the second protein uniprotkb id and the interaction pid id between the previous pair of proteins. the complete set of ppis for this species is available in scere <dig> txt file, provided by pid online database  <cit> .

system running
the experiment begins when the user asks the system to extract protein complexes from a ppi-network and inserts the chosen dataset, the system focuses on decision making modules responsible for the specific problem. more in details, there is the parent module, ppi complex extraction, that gives directives to two children modules: the first one, complex preprocessing, contains expertise about ppin preprocessing, whereas the second one, complex clustering, has the skill about clustering strategies and tools. this relation is shown in figure  <dig>  where the decision-making module tree and its treemap representation are presented. figure  <dig> also reports other three decision-making modules, namely interaction identification, cluster comparison and cluster identification, that however will not be activated during the proposed experiment so that they are drawn as dashed boxes. they represent more specialized tasks for improving complex preprocessing and complex clustering operations. some guidelines have been extracted from papers cited in background section, translated into rules and placed into the appropriate module. the aim of the parent module is to give focus to one of direct children, by means of some activation rules; the system exploits these rules to suggest to the user which strategy could be adopted.

the first operation of the system is to analyze the input data, in order to get the properties and parameters necessary to activate the proper rules. in this simple scenario, we take into account only a few of input features. first of all, the system compares the ppis of dataset with a list of core interactions, provided by dip for the saccharomyces cerevisiae species. in this case  <dig> of  <dig> interactions are reliable, because they are manually curated. then the system creates the undirected graph, the ppin, and checks if resulting network is scale-free, that is if its degree distribution follows a power law, at least asymptotically. in this scenario the ppin is not scale-free. since several authors  <cit>  demonstrate that most networks within the cell approximate a scale-free topology, then some of our ppis  could be false positives or new ppis could be not revealed  when dip dataset was created. for this reason, a rule that proposes ppin preprocessing, in order to change the geometry of the network, is activated.

when the user follows the system advice, according to previous rule, the ppi complex extraction module gives focus to the child module complex preprocessing.

according to the analysis phase, the system knows the ppin contains about 74% of core-interactions. since it has been estimated that approximately half the interactions obtained from high-throughput proteomic techniques may be false positives  <cit> , the rule suggesting to find and delete false positive ppis is not activated; in fact, cutting edges of ppin could implicate some core-interactions are deleted and moving core-interactions is lethal for biological networks. for this reason, the rule suggesting to add new ppis is activated.

when the user agrees to the advice, the system looks for tools implementing this strategy. in this simple scenario, the knowledge-base contains only a tool that can find and add some false negatives  in ppin: the detect defective cliques algorithm, created by  <cit> . when the user accepts to run the proposed algorithm, then the system informs that this algorithm requires, as input parameter, the number of common interactions between two defective cliques, and suggests to user a considerable value for the experiment.

when the user accepts the proposed value, the system executes the algorithm, that finds a new potential fn interaction between the proteins act <dig> and sla <dig>  at this moment, the ppin is composed by  <dig> proteins and  <dig> interactions; the user could either continue the experiment or execute another preprocessing tool .

if the user wants to try another solution before continuing the experiment and he does not want to accept the system advices, he could choose to follow the strategy to find and delete false positive ppis. in this case, the system saves results obtained so far and it proposes to run one of those algorithms that satisfy the selected strategy. the user selects the betweenness centrality algorithm from among three different tools available into the knowledge-base, because the system indicated this is the algorithm with the lowest computational cost. the result of betweenness centrality algorithm is a ppin with  <dig> proteins,  <dig> interactions and  <dig> core-interactions; then the system advices the user to change strategy and/or modify parameters because two core-interactions have been deleted.

the whole workflow is shown in figure  <dig>  at the intermediate abstraction layer, all the strategies within the boundaries of their respective decision modules are depicted, whereas at the lowest abstraction layer there are all the tools implemented in this scenario.

before concluding the experiment, the system proposes to visualize the output of mcl algorithm with the well know cytoscape tool  <cit> . visualization of clustering results, obtained through cytoscape, are shown in figure  <dig> and reported in table  <dig>  finally, the user obtains a solution, that he can further analyze according to its knowledge about the protein complex domain and/or using external evaluation parameters.

system output
the implemented workflow is composed by two algorithms in cascade: detective cliques  and mcl . the system running with standard parameters gives five protein complexes as result.

discussion
in order to test the result obtained by the system running, the biological significance of each protein complexes is validated by means of the gene ontology term finder web service  <cit> , that returns, for each complex, both the corresponding gene ontology term and the p-value  <cit> . this statistical measure gives the probability that a group of protein has been clustered by chance: the smaller the p-value, the higher the relationship between a protein complex and the assigned go term.

protein complexes obtained by system running are evaluated using two different criteria; in facts, some external and internal evaluation criteria have been analyzed.

the external criterion is based on the comparison among the results obtained through the proposed system and two different approaches proposed by  <cit> , that have been previously tested using the best parameter values for the dataset used in this paper. in particular,  <cit>  proposes pincoc, a co-clustering based approach for protein clustering, whereas  <cit>  introduces uvcluster, an agglomerative hierarchical clustering method: strategies implemented by these two research groups could represent a test-set for the proposed system.


g2/m transition of mitotic cell cycle

actin filament depolymerization

actin cytoskeleton organization

actin polymerization or depolymerization

rho protein signal trasduction
the table reports a comparison among the proposed approach and two different approaches, called respectively pincoc and uvcluster, that have been previously tested with this database. proposed system outperforms result of the other tools with respect to the complexes actin filament depolymerization and actin cytoskeleton organization.

with regard to the internal criteria, two different tests have been executed, aiming at demonstrating the workflow suggested by the system is the best than the other workflows the system could build, according to its knowledge. in particular, the first test reports a comparison among all the algorithms contained in the knowledge base of our kdss for the complex clustering problem: mcl, rnsc and mcode. it is worth remembering that the proposed scenario contains only three of the most common and high-performance algorithms for protein complex extraction, because they represent a simple set of tools able to demonstrate how good the system works. results of the first test are shown in table 4; the structure of the table is the same as the previous table. it is possible to see as the tool suggested by the proposed system reaches the smallest p-value for all the functional groups but the last cluster, where the proposed tool with standard parameters does not exhibit any result. this test is enough for demonstrating the system suggests the appropriate algorithm to the best of its knowledge.

table reports a comparison among three clustering techniques contained into the knowledge base of the system: mcl, rnsc and mcode. the suggested tool allows the system to reach the smallest p-values for all the complexes, but the rho protein signal trasduction cluster.

the second test aims at investigating about the preprocessing phase suggested by the system. in facts the system, according to its knowledge, can deal with the complex extraction problem using a preprocessing of the input ppi-network, by means of two strategies: finding the false negative ppi  or the false positive ppi . since the mcl algorithm proposed by the system is not sensitive to all three of the alternative ways related to the network preprocessing, we test the result of the preprocessing phase over the mcode tool, because a comparison of clustering algorithms for protein-protein interaction networks showed that mcode is sensitive to noise in the network  <cit> . for this reason, mcode is a suitable candidate for evaluating the effect of network preprocessing. in this scenario, the system proposes the first strategy and suggests to use the "detect defective cliques" tool. table  <dig> shows results of this last test. the first column contains the available preprocessing techniques; the second column reports the effect of the strategy on the network; next column reports the set of proteins for each clusters; the fourth column reports the fraction of proteins that have been identified as responsible of a biological process and the last column reports the p-value measure. table  <dig> shows the suggested algorithm reaches a smaller p-value  in the complex related to the actin cytoskeleton organization go term, therefore the system proposes once again the most appropriate algorithm to the best of its knowledge.

table reports a comparison among some of the preprocessing tools contained into the knowledge base of the system.the suggested tool allows the system to reach the smallest p-values for all the complexes.

CONCLUSIONS
in this paper, we presented a novel approach for the extraction of the protein complexes based on kdss. the system interacts with the user using its expertise about ppins. the system suggests to the user what are the strategies and algorithms suitable for the problem and, moreover, helps him providing the description, pros and cons of each available technique. finally the system also runs the selected tools, suggesting to the user what are the most common parameters for the specific situation and, during the experiment it builds a workflow of executed operations, enabling the chance of backtracking for exploring alternative paths. the presented results show that the workflow suggested by the system gives the best results with regards to the other workflows produced by the system itself and, furthermore, that workflow offers similar results when compared to other ppi extraction methodologies found in literature.

future work
in the near future, we will give research community free access to our system, thanks to the migration towards a web service. moreover we are working in order to enrich the knowledge base with skill regarding the proposed scenario and, at the same time, we will use our system architecture for facing other bioinformatics issues. finally we will give support to the developer community in order to provide a simple editor so that it will be possible to insert into the system further knowledge and expertise.

competing interests
the authors declare that they have no competing interests.

authors' contributions
af: software design, implementation, writing, assessment, discussions. mlr: software design, implementation, writing, assessment, discussions. rr: software design, discussions, writing. au: software design, discussions, writing, funding. sg: project conception, software design, discussions. all authors read and approved the final manuscript.

declarations
the publication costs for this article were funded by the cnr interomics flagship project "-development of an integrated platform for the application of "omic" sciences to biomarker definition and theranostic, predictive and diagnostic profiles".

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2013: computational intelligence in bioinformatics and biostatistics: new trends from the cibb conference series. the full contents of the supplement are available online at http://www.biomedcentral.com/bmcbioinformatics/supplements/14/s <dig> 
