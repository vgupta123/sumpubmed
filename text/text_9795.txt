BACKGROUND
biological sequence databases have continued to see an expansion in their size due to the large number of genome sequencing projects in the past few years. a large fraction of protein predictions submitted to databases are from microbial sequencing projects. whole genome sequencing of bacteria, archaea, and viruses from various environments has provided clues to their adaptability and evolution. to-date, there are over  <dig> completed prokaryotic genomes, with an additional 800+ in various stages of completion  <cit> . however, the microbes that we have thus far been able to cultivate, study in the laboratory, and sequence, constitute only a small fraction  of the microbes that exist in nature. this bottleneck is being addressed by the rapidly emerging area of metagenomics , where cultivation independent techniques are used to study the genomic sequences of organisms in a community  <cit> . in a typical metagenomic study, the dna is extracted from a sample  and directly sequenced   <cit> . data from various metagenomic studies  have provided clues to the roles and interactions the constituent microbes play in their communities, and also have pointed to an incredible diversity of these organisms both at the genomic level and at the protein level. the recent global ocean sampling  metagenomic study  <cit>  to explore microbial diversity in the world's oceans, alone contributed more than  <dig> million protein predictions to the existing protein databases, thereby more than doubling the number of the then known proteins; in addition, these predictions were also shown to be a valuable resource for protein family studies by virtue of their diversity and novelty.

while metagenomic data are proving to be very useful in addressing evolutionary and ecological questions relating to microbial communities, they are also quite challenging to deal with  <cit> . due to the current techniques used, the source organism of a metagenomic sequence is not known . furthermore, assemblies of metagenomic sequence data are typically fragmented. several factors influence the assembly quality of a metagenomic sample, including the amount of sampling of the community, sequence coverage of individual organisms, and strain or sub-ribotype variation in the community  <cit> . consequently, a large fraction of the protein sequences predicted in these data are fragmentary. furthermore, gene-finding is also made challenging due to the presence of organisms that have varied gc compositions, codon biases etc.  <cit> . several recent works have addressed these various challenges, including assembly  <cit> , binning  <cit> , gene identification  <cit> , and protein classification  <cit> .

the classification of proteins into families  serves the basis for further analyses of these families, including their structure and function  <cit> . proteins are grouped together either on the basis of their domains  <cit>  or on their full sequences  <cit> .

in this paper, we present a computational improvement to a sequence clustering method that we introduced previously to analyze large microbial metagenomic datasets, and that was used in the gos study  <cit> . this method could be used both to identify protein-coding genes in metagenomic data containing prokaryotic, viral and intron-less eukaryotic genomes, and to group related sequences into families . however, this method requires the availability of similarities for all sequence pairs. this was computed in  <cit>  using a blastp search  <cit> , which becomes prohibitively expensive with the ever increasing amount of metagenomic sequence data that are being generated; in fact the all-against-all blastp searches of the  <dig>  million sequences analyzed in the gos study  <cit>  required over  <dig>  million cpu hours . in this paper, we present an incremental clustering approach that is much faster than the original approach. it does not use the all-against-all approach, but at the same time, preserves the homology detection capabilities of the earlier method. the method described here is currently used in camera  <cit> .

implementation
previous approach
we first provide a summary of our original clustering approach and the data sets used  <cit>  so as to provide context to the method described in this paper. the original approach was intended to analyze the gos microbial metagenomic data in the context of a comprehensive set of known proteins. thus, data from other sources, namely, national center for biotechnology information 's non-redundant amino acid database  <cit> , ncbi's prokaryotic genome sequencing projects   <cit> , ensembl  <cit> , and tigr gene indices   <cit> , were also included in the study. the input to the clustering consisted of full length and partial length amino acid sequences from these various data sources. for the nucleotide sequence sets gos, pg, and tgi-est, the corresponding amino acid sequence sets consisted of six frame translations, also known as open reading frames , identified on the nucleotide sequences. only orfs of length  <dig> amino acids or more were used in the study. to accommodate partial nucleotide sequences , the standard orf definition was extended so that an orf is bracketed by either a start codon or the start of the nucleotide sequence, and by either a stop codon or the end of the nucleotide sequence.

an all-against-all blastp compute was used to identify the pairwise sequence similarity used for the clustering. given the size of the combined dataset , for efficiency purposes, the clustering proceeded in a series of steps. first, a non-redundant set of sequences was identified from the combined data set. this step used pairwise matches with 90% identity  covering at least 95% of the shorter sequence length. in the second step, pairwise matches covering at least 80% of the longer sequence length were used to construct a graph of non-redundant sequences, and dense subgraphs were identified in this graph. each dense subgraph is referred to as a core cluster, and corresponds to a sub-family of similar sequences. psi-blast profiles  <cit>  and ffas profiles  <cit>  were constructed for core clusters containing at least twenty sequences . the psi-blast profiles were used to recruit singletons to core clusters, and the ffas profiles were used to compare and merge related core clusters into final clusters.

two filters were applied to the resulting final clusters to separate clusters of protein coding sequences from clusters of spurious sequences. the first filter  identified shadow orfs, that is, spurious orfs that overlap with protein coding orfs. the second filter  identified clusters of conserved but non-coding orfs. these sequences show a lack of selection at the codon level and can be identified using their nonsynonymous to synonymous substitution ratios   <cit> . the two filters are also used in the incremental clustering method and will be described in more detail later.

input and strategy
the incremental clustering method has two inputs, a set c of  protein clusters and a set s of amino acid sequences. s is a set of amino acid sequences from some protein resource, or in the case of metagenomic or genomic data analysis, s is the set of orfs  identified from the nucleotide sequences  as described earlier. our method identifies and groups protein coding sequences into the existing protein clusters. in addition, it identifies novel protein families that have multiple members in s.

the incremental clustering method does not compare all sequences against each other. it does however incorporate varying homology detection capabilities. we use existing tools cd-hit   <cit> , psi-blast  <cit> , and ffas  <cit>  to perform sequence-sequence, profile-sequence, and profile-profile comparisons respectively. cd-hit is a fast sequence clustering algorithm that uses shared word counts as a filter to group highly similar sequences. each cd-hit cluster is summarized by a cd-hit representative sequence, which, by construction, is also the longest sequence in the cluster. cd-hit-2d, a variant of cd-hit, uses the same approach to identify sequences in a given set that are within a user-specified threshold to sequences in another set.

in the first stage of our incremental clustering method, cd-hit-2d is used to identify and recruit sequences in s that have high similarity  to sequences in c. in the second stage, the remaining sequences in s are clustered using cd-hit . for both cd-hit-2d  and cd-hit , the 60% identity clustering is achieved in multiple steps rather than a single step – a high threshold 90% identity clustering step followed by a lower threshold 75% identity clustering step, and a final 60% identity clustering step. this is done for two reasons . firstly, cd-hit and cd-hit-2d run much faster at a higher threshold  than at a lower threshold . if s contains many sequences that have high identity to those in c, the initial faster high identity threshold clustering can recruit many of these sequences, thereby reducing the size of following slower runs. secondly, in the current implementation of cd-hit, there are two modes for assigning a sequence to a cluster – assigning to the first cluster that meets the threshold , and assigning to the best matching cluster. the current parallel version of cd-hit does not have the later option  implemented yet. in its absence, the multi-step approach provides a way to approximate the desired quality.

in the final stage, psi-blast profiles for clusters in c and s are used to recruit sequences to these clusters. in addition, ffas cluster profiles are used to merge groups of related clusters. as clusters get larger and more diverse with the addition of new data, cluster profiles provide better homology detection ability , and this is the rationale for using psi-blast and ffas profiles in the final stage of the incremental clustering. as in the original method, we also detect and remove clusters containing spurious sequences.

cluster organization and definitions
we define a sequence s to be redundant if it has a match with ≥90% identity to a longer sequence and this match covers ≥95% of the length of s; otherwise s is non-redundant. following our previous work, every sequence is associated with a core cluster and a final cluster. one or more core clusters are grouped into a final cluster based on their ffas profile matches. in the discussions below, a core cluster is labeled big if has ≥ <dig> non-redundant sequences. each big core cluster has a psi-blast profile and an ffas profile associated with it; currently, both profiles are computed using the longest sequence as query.

we assume that each cluster in c is made of one or more core clusters. we use c-x to denote the set of all cd-hit representatives computed from core clusters in c at the x% identity level. for cd-hit computes at the 90% identity described below, it is assumed that the matches are relative to the length of the shorter sequence, where as for lower thresholds , it is assumed that the matches cover at least 80% length of the longer sequence.

incremental clustering method
the incremental clustering method is shown in fig.  <dig>  it has three main stages.

stage 1: 
in this stage, sequences in s that are quite similar to sequences in c- <dig> are identified. for reasons discussed earlier, this is carried out in three steps. sequences in s are first recruited to sequences in c- <dig> using cd-hit-2d at 90% identity. subsequently, as yet unrecruited sequences in s are recruited to c- <dig> at 75% identity and then at 60% identity using cd-hit-2d. the sequences from s that are recruited in the previous two steps are then clustered again at 90% identity to identify the redundant sequences within them. after the recruitment step, psi-blast and ffas profiles are constructed for newly formed big core clusters in c. also, profiles for those existing big core clusters that recruited sequences in s are refined. finally, the shadow orf filter is applied to s to identify and remove those unrecruited sequences in s that overlap  with the recruited sequences.

stage 2: 
the purpose of this stage is to identify  clusters of similar sequences that remain in s after stage  <dig>  unrecruited sequences in s from the previous step are clustered successively at 90%, 75%, and 60% identities using cd-hit to produce core clusters. if s is a set of orfs, then the shadow orf filter and the ka/ks filter are used to identify clusters of spurious sequences. finally, psi-blast and ffas profiles are computed for all newly formed big core clusters that are not labeled as spurious.

stage 3: 
big core clusters are compared using their ffas profiles. the comparisons are used to group these core clusters into final clusters. this is done by constructing a graph with nodes representing big core clusters. an edge exists between two nodes if the corresponding core clusters have a profile-profile match meeting a certain ffas score threshold. we set the score thresholds as a function of the profile length, with profile lengths ≤ <dig> set a threshold of - <dig>  lengths > <dig> set a threshold of - <dig>  and lengths in between having thresholds between the two values. each connected component in the constructed graph corresponds to a final cluster. finally, psi-blast profiles are used to recruit smaller clusters  to big core clusters. a sequence is recruited to a big core cluster if the profile-sequence match has e-value ≤ 1e- <dig>  and covers ≥75% of the sequence. a small cluster is recruited to a big core cluster if a majority of its sequences are recruited via psi-blast to this big core cluster.

shadow orf filter
in stage  <dig>  each unrecruited orf in s that overlaps  with a recruited orf is labeled as shadow and removed. two orfs on the same strand are considered overlapping if their intervals overlap by at least  <dig> bps. two orfs that are on the opposite strands are considered overlapping either if their intervals overlap by at least  <dig> bps and their 3' ends are within each others intervals, or if their intervals overlap by at least  <dig> bps and the 5' end of one is in the interval of the other. in stage  <dig>  a cluster is labeled as containing shadow orfs if at least a third of its sequences overlap  with sequences in a bigger cluster.

ka/ks filter
we use the ka/ks filter as described in  <cit> . for most proteins, ka/ks <<  <dig>  and for proteins that are under strong positive selection, ka/ks >>  <dig>  a ka/ks value close to  <dig> is an indication that sequences are under no selective pressure and hence unlikely to code for proteins  <cit> . weakly selected but legitimate coding sequences can have a ka/ks value close to  <dig>  these are identified by using a model in which different partitions of the codons experience different levels of selective pressure. a cluster is rejected only if no partition is found to be under purifying selection at the amino acid level. the ka/ks filter is implemented as follows. sequences in the cluster are first aligned with muscle  <cit>  and a strongly-aligning subset of sequences is selected for the ka/ks analysis. the codeml program from paml  <cit>  is run using model m <dig>  to calculate an overall  ka/ks value for the cluster. if ka/ks ≤ <dig> , the cluster is considered as passing the ka/ks filter . if not, the cluster is further examined by running codeml with model m <dig>  this partitions the positions of the alignment into three classes that may be evolving differently . a likelihood ratio test is applied to check if m <dig> explains the data significantly better than m <dig>  <cit> . if one of the resulting partitions has ka/ks ≤  <dig>  and comprises at least 10% of the sequence, then the cluster is considered as passing the ka/ks filter. if not, it is labeled as containing spurious orfs.

clustering output
final clusters that contain at least two non-redundant sequences and are not labeled as spurious  are referred to as good clusters, and only sequences in these clusters are labeled as predicted proteins. final clusters that contain only one non-redundant sequence but are not labeled as spurious, while not considered in the final statistics on predicted proteins, are kept around for future clustering.

RESULTS
the sequences and clustering results from our previous study  <cit>  that included ncbi-nr, ensembl, tgi-est, pg, and gos, are used here as the starting point. only those clusters  that were not labeled as spurious are considered here. these clusters constitute the set c. we will refer to c as camera clusters since this is currently available via the camera effort  <cit> . this data set was updated using the incremental clustering method with all sequences that were submitted to various public protein databases since the data freeze used for  <cit> . the new sequences were collected via the protein and nucleotide data archive  effort  <cit> . there were  <dig> , <dig> amino acid sequences  in the release we used. in addition, the published data from the hawaii ocean time series station aloha  metagenomic study  <cit>  was also used. this study generated  <dig>  sequencing reads from seven different ocean depths. we called  <dig>  orfs  on these reads and this was also made available as input to our incremental clustering method. thus the set s consisted of  <dig> , <dig> amino acid sequences.

the hardware infrastructure consisted of a compute grid with  <dig> nodes, each with dual xeon cpu  <dig>  ghz, 4gb ram; a total of  <dig> processors were available, with a varying number of them being used in the different stages of the incremental clustering.

since the various steps of the incremental clustering process have different run time complexities, the actual total time  taken in an incremental update is dependent on the input data and their recruitment to the existing cluster data. an update of the camera clusters with the nearly  <dig>  million panda and hot/aloha sequences took a total of  <dig>  cpu days . for this dataset, the majority of the time was spent in the psi-blast recruitment step , followed by the cd-hit computations  and the ffas profile comparisons . in contrast, an all pair sequence similarity computation of the  <dig>  million sequences alone, via an all-against-all blastp search, on the same hardware is estimated to take  <dig>  cpu days .

we used simulated data to evaluate the gene identification capability of the clustering approach on unassembled read sequences . an evaluation of a homology-based gene identification method  in a metagenomic setting has the additional complexity of accurately modeling the population structure of the microbial community. this is relevant in the context of identifying novel protein families that may be specific to a particular taxonomic group represented in the metagenomic sample. microbial population structures vary from environment to environment, and even over time in a given environment. for the current evaluation, we chose to avoid the population modeling issue and directly address gene identification on fragmentary sequences. sequence reads were generated from seventeen recent genome projects . data from these genome projects were not available at the time of construction of the camera clusters  <cit> . the list in table  <dig>  however, includes both closely related and distantly related organisms to those that contributed to the camera clusters. sequence reads of length  <dig> bp were randomly generated from each genomic sequence; a 2× coverage  was assumed in the generation so as to sample a larger number of genes in the genome. orfs  were generated from these reads using translation table 11; only orfs of length ≥ <dig> aa were considered. this resulted in a total of  <dig>  orfs from the seventeen genome projects. out of these,  <dig>  orfs overlapped  with a gene; we refer to this set as the reference set. the  <dig>  orfs constituted the input s to the incremental clustering method . our method labeled  <dig>  orfs as protein coding, of which  <dig>  were in the reference set. a total of  <dig>  orfs  from the reference set were incorrectly labeled as spurious by the filters.

a-archaea, b-bacteria, sn-sensitivity, sp-specificity.

our method has an average specificity of  <dig> % and an average sensitivity of  <dig> % on these genomes . the evaluation highlights several aspects of the method. first, our method, by design, has high specificity. this is a result of the conservative constraint that we use to label an orf as a predicted protein . second, the sensitivity of our homology-based method in detecting a new organism's genes is dependent on the representation in the existing protein clusters c  of this organism's taxonomic neighbours. for instance, our method has very high sensitivity  on candidatus pelagibacter ubique htcc <dig>  <cit> , an alphaproteobacteria that is present in ocean surface waters and is well represented in the gos data   <cit> . on the other hand, our method has a much lower sensitivity  on a soil bacterium acidobacteria bacterium ellin <dig>  <cit> , which belongs to the class acidobacteria; this class is not well represented in c. the sensitivity numbers of our method on these genomes also have to be placed in the context of the number of orfans  <cit>  seen in newly sequenced genomes. orfans are protein predictions that have no homology to known proteins. they have been seen to account for 25–30% of protein predictions in newly sequenced prokaryotic genomes  <cit> . our method will not label orfan sequences as proteins since they will fall into singleton clusters. the number of orfans will no doubt decrease as more related genomes from similar environments are sequenced. our approach retains these sequences for future clustering. finally, table  <dig> also shows lower sensitivity numbers for archaea  compared to bacteria . this is a consequence of a much sparser sampling  of archaea compared to bacteria, and therefore a relatively smaller representation of archaea in the existing protein clusters c.

we also compared the performance of our clustering approach to that of a non-homology based genefinder  using two metagenomic datasets, namely, the gos data and the hot/aloha data. from the input  <dig> , <dig> gos orfs  <cit> , our clustering approach identifies  <dig> , <dig> orfs as protein coding whereas metagene  produces predictions that can be mapped  to  <dig> , <dig> gos orfs. the two sets have  <dig> , <dig> predictions in common . we analyzed how many of the predictions unique to each method had matches to models in the pfam database  <cit> , since this database is widely used for functional annotation. of the  <dig>  metagene-only predictions,  <dig>   have matches  to pfam models, whereas of the  <dig>  clustering-only predictions,  <dig>   have matches to pfam models. on the hot/aloha data,  <dig>  of the input  <dig>  orfs are labeled as protein coding by our clustering. metagene  produces predictions that can be mapped to  <dig>  orfs. there are  <dig>  predictions in common to the two sets . of the  <dig>  metagene-only predictions,  <dig>  of the sequences have matches to pfam models, whereas of the  <dig>  clustering-only predictions,  <dig>  have matches to pfam models.

comparisons to metagene on the two datasets reveal common patterns. while both approaches agree on a large fraction of their predictions, metagene makes more predictions than our clustering approach, and this can be explained as follows. while it is possible for metagene to identify novel families even if only a single member is present, our conservative approach requires that multiple  members of a novel family are present. thus, we do not make protein predictions for these sequences that fall into singleton clusters. as previously stated, we do, however, retain these orf sequences for future clusterings and resolution. a comparison of predictions unique to each method using pfam models reveals that a larger fraction of predictions unique to our approach  have pfam matches. this is a consequence of using a homology-based approach.

we evaluated our clustering methodology in its ability to classify sequences into protein families. the lack of availability of an exhaustive data set that can be used as a gold standard to evaluate large scale computational protein classification , presents its own challenges. for our evaluations we used the domain architecture based approach  <cit> , which is an attempt to get at the full length matches using the pfam domain matches. we used the pfam results from  <cit>  together with pfam results on the hot/aloha set, and restricted our analysis to those clusters that contain sequences with pfam matches. briefly, the domain architecture for a sequence is defined to be the set of all pfams that have  matches to it. two sequences are defined to be unrelated if their domain architectures each have at least one pfam that is not present in the other's domain architecture. we also consider here a strict version, where two sequences are considered unrelated if either their domain architectures have no pfams in common, or when they do, then they each have at least one pfam that is not present in the other's domain architecture. using these definitions, we plotted the cumulative fraction of  clusters against the percentage of unrelated pairs they contain . these curves show that our clustering is quite consistent, that is, the clusters have a low fraction of unrelated sequence pairs; 98% of all clusters have no unrelated pairs . for the strict version, 91% of all clusters have no unrelated pairs . we also evaluated how often domain architectures are split across  clusters  and found that over 80% of the domain architectures appear in three or fewer clusters.

from the input sequence set,  <dig> , <dig> panda sequences  and, as mentioned previously,  <dig>  hot/aloha sequences  are labeled as predicted proteins. the incremental clustering update resulted in  <dig>  good clusters; see table  <dig> for cluster size distribution. these final clusters contained a total of  <dig> , <dig> sequences, with nearly 88% of the sequences in  <dig>  clusters that contain at least twenty non-redundant sequences. fig.  <dig> shows the log-log plot of final cluster size distribution to be consistent with a power law. as noted in previous studies  <cit> , the observed curve has an inflection point showing differing power laws governing the size distribution of very large clusters compared to the rest.

the size of a cluster is defined as the number of non-redundant sequences in it.

the panda sequences that are labeled as predicted proteins  have a great deal of redundancy. of these  <dig> , <dig> sequences,  <dig> , <dig>  are marked as redundant, with  <dig> , <dig>  being marked as redundant by existing cd-hit representatives in the camera clusters and the remaining being marked as redundant by other panda sequences. an examination of those panda sequences that are not labeled as predicted proteins by our clustering reveals that  <dig>   have a hypothetical or unknown in their sequence headers; these sequences may be organism specific proteins, which our current method will not identify, or they may be spurious protein predictions submitted to the public databases. the good clusters containing the largest number of panda sequences include reverse transcriptases, cytochromes, abc transporters, and dehydrogenases .

column  <dig> hints at the extent of redundancy in the panda set.

compared to the panda set, the hot/aloha predicted protein sequences show a lesser amount of redundancy.  <dig>  sequences  are marked as redundant, with  <dig>  being marked as redundant by existing cd-hit representatives from the camera clusters. a breakdown of the predicted proteins by their sample depths reveals differential abundances of many protein families. proteins involved in bioluminescence and families of transposases and integrases are more abundant with depth whereas proteorhodopsins and photolyases are more abundant in the surface and near surface water samples. these differences are a reflection of the environmental factors that shape microbial communities and have been noted previously  <cit> . table  <dig> lists the clusters containing the largest number of hot/aloha sequences.

in  <cit> , we reported on novel protein clusters  from the gos data that could not be linked to any of the then known families . we explored these clusters in the context of the current incremental data set. the panda set included protein predictions from recently sequenced microbes, including several marine prokaryotes that were sequenced by the gordon and betty moore foundation sponsored projects  <cit> .  <dig> of the originally labeled group ii clusters had at least one panda or hot/aloha sequence, with  <dig> containing ≥ <dig> of them. table  <dig> lists the genome projects present in the panda set that have the largest number of sequences in these clusters. this table shows that most of them are moore sponsored projects. it may not be surprising that most of the recruitment to the gos-only clusters is from new microbial sequences from the marine environment. nevertheless, these recently sequenced genomes can provide useful anchors to carry out further analyses of these protein families that could eventually provide clues to their functions and evolution.

a marine microbial genome projects funded by the gordon and betty moore foundation

the incremental clustering data is available for download from the publications and data section at camera  <cit> .

CONCLUSIONS
we presented an incremental clustering method that is a computational improvement to an earlier method to identify and classify proteins in large microbial metagenomic datasets. the resulting clusters can serve as the basis for further analyses including functional annotation and evolutionary studies of different protein families.

our method can be applied to metagenomic data sets that contain prokaryotes, viruses, and intron-less eukaryotic genomes. it has been applied to data generated by the sanger sequencing technology  <cit> , where current read lengths are ~ <dig> bp. innovations in sequencing technologies have resulted in several recent approaches that are cheaper and produce more sequence data  compared to sanger sequencing  <cit> . these next generation sequencing  methods, including the pyrosequencing based technology of  <dig> inc  <cit>  that currently produces reads of length ~ <dig> bp, have higher sequence read error rates . the high error rates can result in insertions or deletions in the nucleotide sequence that produce shifts in the reading frame, thereby adding to the complexity of the gene identification process. the orf generation based method described in this paper cannot be directly applied to these read data. they could, however, be applied to assembled contigs that have high coverage, and subsequently much smaller error rates. we are also currently developing an incremental clustering approach that does not require explicit generation of six frame translations to identify genes from these data. the extent and accuracy of gene calling and protein classification on data generated by other ngs methods that produce shorter reads  also needs to be explored.

an evaluation of our clustering method in identifying genes showed that the method has high specificity. the sensitivity of the method can be increased by developing orf confidence measures   <cit>  for sequences in singleton clusters. we also compared the performance of our clustering method to metagene in identifying genes in metagenomic datasets. while metagene makes more gene calls and is fast, our method takes a more conservative approach to identifying protein coding sequences  and at the same time, also groups related sequences into families. based on an evaluation using the pfam database, we also found that, compared to metagene, our homology-based method tends to pick up a larger fraction of sequences with matches to known protein families. thus, a metagenomic annotation system will benefit from making use of both types of approaches.

future modifications to improve the specificity and sensitivity of the clustering method will include alternate ways of constructing psi-blast and ffas profiles , and also approaches to detect and handle over- and under- clustering. even though we have presented our approach as an analysis tool for microbial metagenomic data, it is also applicable to analyzing finished or nearly finished prokaryotic genome projects. the clustering information presented here will be periodically updated with data from newer prokaryotic genome and metagenome projects. cluster annotation and linking to other already existing valuable protein resources, is currently being done, and will also be made available.

availability and requirements
cd-hit, psi-blast, ffas, muscle, and paml are important components of our incremental clustering approach. they are all published methods, and their availability and requirements are described at their respective homepages: cd-hit , psi-blast , ffas , muscle  and paml . the shadow orf filter code and the ka/ks filter code are available for download from the publications and data section at camera . operating system: linux; programming languages: perl and c; license: gnu gpl.

authors' contributions
sy and wl contributed to the design, implementation, validation of methods, analysis of results, and writing of manuscript. gs contributed to design and validation of methods.

