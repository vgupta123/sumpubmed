BACKGROUND
advances in high throughput data acquisition technologies have resulted in rapid increase in the amount of data in biological sciences. for example, progress on sequencing technologies has resulted in the release of hundreds of complete genome sequences. with the exponentially growing number of biomolecular sequences from genome projects and high-throughput experimental studies, sequence annotations do not keep pace with sequencing.

the wet-lab experiments to determine the annotations  are still difficult and time consuming. hence, there is an urgent need for development of computational tools that can accurately annotate biomolecular data.

machine learning methods currently offer one of the most cost-effective approaches to construction of predictive models in applications where representative training data are available. biomolecular sequence labeling is an instance of a supervised learning problem. given a data set i =  <dig> ⋯, n of pairs of sequences, xi =  and yi = , where yi, j in the output sequence is the label for xi, j in the input  sequence, j =  <dig> ⋯, m, the task is to learn a classifier that can predict the labels for each element of a new input sequence, xtest.

there is a large body of work on learning predictive models to label biomolecular sequence data. terribilini et al.  <cit>  trained naïve bayes classifiers to identify rna-protein interface residues in a protein sequence. yan et al.  <cit>  developed a two-stage classifier to identify protein-protein interaction sites. qian and sejnowski  <cit>  trained neural networks to predict protein secondary structure, i.e., classifying each residue in a protein sequence into one of the three classes: helix , strand  or coil . caragea et al.  <cit>  and kim et al.  <cit>  used support vector machines to identify residues in a protein sequence that undergo post-translational modifications.

typically, to solve the biomolecular sequence labeling problem using standard machine learning algorithms, each element in a sequence is encoded using a local, fixed-length window corresponding to the target element and its sequence context   <cit> . the classifier is trained to label the target element. this procedure can produce reliable results in settings where there exists a local sequence pattern that is predictive of the label for the target site. however, there are cases where the local amino acid distribution around functionally important sites in a given set of proteins is highly variable. for example, in identifying rna-protein and dna-protein interface residues from amino acid sequences, there is typically no consensus sequence around each site.

classifiers trained using machine learning to distinguish "positive" examples from the "negative" ones, must "learn" to do so by learning the characteristics associated with known "positive" and "negative" examples. the greater the commonality among members of a subset, the more likely it is that a machine learning approach will be successful in identifying the predictive characteristics.

against this background, we hypothesize that classifiers trained to label biomolecular sequence data can be improved by taking into account the global sequence similarity between the protein sequences in addition to the local features extracted around each site. the intuition behind this hypothesis is that the more similar two sequences are, the greater the likelihood that their functional sites have similar patterns. therefore, we propose to improve the biomolecular sequence labeling problem by using a machine learning approach, that is, a mixture of experts model that considers the global similarity between protein sequences when building the model and making the predictions. we evaluate our approach to learning a mixture of experts model on two biomolecular sequence labeling tasks: rna- and dna-protein interface prediction tasks.

RESULTS
the main result of our study is that taking into account global sequence similarity through the means of a mixture of experts model can improve the performance of the classifiers trained to label biomolecular sequence data.

the mixture of experts that exploits the global similarity between the protein sequences in a data set in addition to the local features extracted around each residue outperforms the baseline classifiers on the biomolecular sequence labeling task
we trained mixtures of naïve bayes  and logistic regression  classifiers on both rna- and dna-protein interface prediction tasks considered in this study to predict whether or not a residue in a protein sequence is an interface residue. we used various identity cutoffs to construct the data sets. the mixture of experts models have a hierarchical structure that is constructed using 2-way spectral clustering based on a global similarity functions, i.e., we computed the entries in the similarity matrix w by applying the needleman-wunsch global alignment algorithm on each pair of sequences. the blosum <dig> substitution matrix was used for costs. the resulting entries in the matrix w are normalized and scaled so that each value is between  <dig> and  <dig>  the mixture of experts models consist of nb and lr at the leaves, respectively .

we compared the performance of the mixtures of nb and lr with that of baseline nb and lr, respectively. with any classifier, it is possible to tradeoff the precision against recall. hence, it is more informative to compare the precision-recall  curves which show the tradeoff over their entire range of possible values than to compare the performance of the classifiers for a particular choice of the tradeoff. thus, we compared the pr curves for nb and the mixture of nb models as well as lr and mixture of lr models on both rna- and dna-protein interface prediction tasks. for both prediction tasks, the pr curves for the mixture of experts models dominate the pr curves of nb and lr models, that is, for any choice of recall, the mixture of experts models offer a higher precision than nb and lr . while this is true for any identity cutoff for both rna- and dna-protein sequence data sets, we show results only for 30% identity cutoff. the curves demonstrate that even for a very stringent cutoff, the mixture of experts that captures global similarity between sequences in the data set outperforms the other models.

in tables  <dig> and  <dig>  we show the classification results after evaluating the baseline models, nb and lr, and the mixture of experts models with nb and lr at the leaves, me-nb-global and me-lr-global, respectively, on the rna- and dna-protein sequence data sets for two identity cutoffs: 30% and 90%. the values in the tables are obtained using the default threshold θ =  <dig> . as illustrated in the tables, the mixture of experts models that capture global sequence similarity outperform the baseline models. for example, in the case of rna-protein data set at 30% identity cutoff, the mixture of experts, me-nb-global, achieves  <dig>  precision,  <dig>  recall,  <dig>  correlation coefficient,  <dig>  f-measure, and  <dig>  area under the roc curve, while the nb classifier achieves  <dig>  precision,  <dig>  recall,  <dig>  correlation coefficient,  <dig>  f-measure, and  <dig>  area under the roc curve . in the case of dna-protein data set at 30% identity cutoff, the mixture of experts, me-nb-global, achieves  <dig>  precision,  <dig>  recall,  <dig>  correlation coefficient,  <dig>  f-measure, and  <dig>  area under the roc curve, while the nb classifier achieves  <dig>  precision,  <dig>  recall,  <dig>  correlation coefficient,  <dig>  f-measure, and  <dig>  area under the roc curve .

the mixture of experts that exploits the global similarity between protein sequences outperforms a mixture of experts that exploits the local similarity between protein sequences
in order to verify that indeed global sequence similarity is helpful in improving the performance of classifiers, and that the improvement does not come from the more sophisticated structure of the model, we computed the entries in the similarity matrix w by applying smith-waterman local alignment algorithm with blosum <dig>  thus taking into account local sequence similarity . we also randomized the global similarity matrix computed previously and use this randomized matrix to construct the hierarchical structure of the mixture of experts models.

in tables  <dig> and  <dig> we show the performance of nb and mixture of nb models using global  and local  sequence similarities, as well as a random  sequence similarity for the default threshold θ =  <dig> . the results of our experiments show that the mixture of experts models that capture global sequence similarity outperform the other models in terms of the majority of standard measures for comparing the performance of classifiers used in this study . for example, for 30% identity cutoff, correlation coefficient increases from  <dig>   to  <dig>   on the rna-protein data set , and from  <dig>   to  <dig>   on the dna-protein data set . hence, we conclude that global similarity is helpful in improving the performance of classifiers trained to label biomolecular sequence data.

the mixture of experts has consistently higher performance than the baseline classifier for all identity cutoffs
we evaluated the effect of the identity cutoff to construct the non-redundant data sets on the correlation coefficient and f-measure for a range of sequence identity cutoffs from 30% to 90% . it is interesting to note that even at a very stringent sequence identity cutoff of 30% the difference in the correlation coefficient and the difference in the f-measure for the mixture of experts and the baseline classifiers is significant, on both rna- and dna-protein data sets.

the mixture of experts that exploits the global sequence similarity offers a higher precision than the ensemble of classifiers for the same recall
we also trained ensembles of nb and lr classifiers on both rna- and dna-protein interface prediction tasks to predict whether or not a residue in a protein sequence is an interface residue. an ensemble of classifiers  <cit>  is simply a collection of classifiers, each trained on a balanced subsample of the training data. the prediction of the ensemble is computed from the predictions of the individual classifiers .

we compared the performance of the mixtures of nb and lr with that of ensembles of nb and lr, respectively. in figures  <dig>   <dig>   <dig>  and  <dig> we show the pr curves for the mixture and the ensemble models on both rna- and dna-protein sequence data sets for 30% identity cutoff. as can be seen from the figures, the mixtures of experts consistently offer a higher precision than the ensembles of classifiers for the same recall. note that the pr curves of the ensembles are closer to those of the baseline classifiers.

discussion
reliable methods for identifying putative functional sites in protein sequences is an important problem with broad applications in computational biology, e.g., rational drug design. computational tools for identifying functional sites from sequences are especially important because of the cost and efforts involved in structure determination.

in this work we sought to improve the performance of classifiers that make predictions on residues in protein sequences by taking into account the global similarity between the protein sequences in the data set in addition to the local features extracted around each residue. we evaluated mixture of experts models that consider the global similarity between protein sequences when building the model and making the predictions on the rna-protein and dna-protein interface prediction tasks. two closely related models are the hierarchical mixture of experts model  <cit>  and the ensemble of classifiers model  <cit> 

hierarchical mixture of experts
the hierarchical mixture of experts model  was first proposed by jordan and jacobs   <cit>  to solve nonlinear classification and regression problems by combining linear models: the input space is divided into a set of nested regions and simple  models are fit to the data that fall in these regions. hence, instead of using a "hard" partitioning of the data, the authors use a "soft" partitioning, i.e., the data is allowed to simultaneously lie in more than one region.

the hme has a tree-structured architecture that is known a priori. the internal nodes of the tree correspond to gating networks and the leaf nodes correspond to expert networks. the expert networks output class probabilities for each input x, while the gating networks learn how to combine the predictions of the experts up the tree with the final prediction output by the root. the parameters of the gating networks are learned using expectation maximization algorithm  <cit> . the gating and the expert networks are generalized linear models.

ensemble of classifiers
an ensemble of classifiers is a collection of independent classifiers, each classifier being trained on a subsample of the training data  <cit> . the prediction of the ensemble of classifiers is computed from the predictions of the individual classifiers using majority voting. an example is misclassified by the ensemble if a majority of the classifiers misclassifies it. when the errors made by the individual classifiers are uncorrelated, the predictions of the ensemble of classifiers are often more reliable.

mixture of experts – our approach
our approach to learning a mixture of experts model takes into account the global similarity between biomolecular sequences in a data set. unlike the hme model  <cit> , we assume that the structure of our model is not known a priori. hence, to learn the hierarchical structure of the model, we use hierarchical clustering of the sequences in the data set. the leaf nodes consist of expert classifiers, while the gating nodes combine the output of each classifier to the root of the tree which makes the final prediction. the gating nodes combine the predictions of the expert classifiers based on an estimate of the cluster membership of a test protein sequence. following the approach taken by jordan and jacobs  <cit> , we considered a "soft" partitioning of the data, i.e., each sequence in the training set simultaneously lies in all clusters of the hierarchical structure with a different weight in each cluster. the combination scheme of the predictions of the expert classifiers and the "soft" partitioning of the data that considers the global sequence similarity differentiate our model from an ensemble of classifiers model.

CONCLUSIONS
identification of functionally important sites in biomolecular sequences has broad applications ranging from rational drug design to the analysis of metabolic and signal transduction networks. with the rapid increase in the amount of data  there is a growing need for reliable procedures to accurately identify such sites.

in this study, we have presented a mixture of experts approach to identification of functionally important sites from amino acid sequence of proteins that takes into account global similarity between the protein sequences. specifically, we systematically evaluated naive bayes and logistic regression classifiers, as well as mixtures of naive bayes and logistic regression in a sequence-based 10-fold cross-validation setup. the results of our experiments show that global sequence similarity through the means of the mixture of experts approach can be exploited to improve the performance of classifiers trained to label biomolecular sequence data.

