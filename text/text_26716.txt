BACKGROUND
in  <dig>  hamilton smith discovered that long dna molecules could be digested into a set of restriction fragments by the restriction enzyme hindii based on the occurrence of restriction sites with the sequences gtgcac or gttaac  <cit> . since that time, restriction enzymes have played a crucial role in many biological experiments, including genome editing  <cit> , gene cloning  <cit> , protein expression  <cit>  and genome mapping . restriction enzymes are commonly used to physically map genomes. in physical mapping, the restriction enzymes are used to cut a dna molecule at restriction sites with the goal of identifying the locations of the restriction sites after digestion. their positions in the genome are determined by analyzing the lengths of the digested dna. based on the experimental assumptions of digestion, there are two main types of digestions, a partial digest  <cit>  and a double digest  <cit> . constructing an accurate physical map following a partial digestion is a fundamental problem in genome analysis. in this work, we consider the partial digestion.

in a partial digestion experiment, one restriction enzyme is used to cut one or more target dna molecules at several specific restriction site. the digestion results in a collection of short dna fragments, and the lengths of these fragments are recorded in multiset a. attempting to reconstruct the locations of the restriction sites in the target dna molecules using multiset a is known as the partial digest problem, pdp. some modifications have been introduced into the partial digestion process to produce simplified variants of pdp. these variants include: the simplified partial digest problem, spdp  <cit> , the labeled partial digest problem, lpdp  <cit>  and the probed partial digest problem, ppdp  <cit> . in this work, we consider the pdp.

several algorithms  have been developed to solve the pdp. some of these algorithms have short running times, but the solutions are not exact. these algorithms  are based on heuristic and approximation strategies. other algorithms require a long running time in the worst case, but the solution is exact for any instance. these algorithms  are based on brute force or branch and bound strategies.

in this research paper, we describe an algorithm with a suitable run time that generates an exact solution for the pdp. the previous algorithms that yield an exact pdp solution can be divided into impractical and practical types. the impractical solutions are based on the brute force strategy and polynomial factorization  <cit> . the best known practical algorithm for pdp is the algorithm proposed by skiena et al.  <cit> , which is based on the branch and bound strategy. the algorithm is practical because the running time of the algorithm is o  for an average case, while exponential amounts of time were required for the worst case.

problem formulation and related definitions
before we give the formal definition of the pdp, we need the following related definitions.


definition  <dig>  <cit> : the difference of two multisets d and l denoted by d\l such that d\l = {x|x ∈ d and c = c − c > 0}, where c  denotes the number of occurrences of element x ∈ d in d.


definition  <dig>  <cit> : the sum or  of two multisets d and l denoted by d ∪+ 
l such that d ∪+ 
l = {x|x ∈ d or x ∈ l and c  = c  + c }.


definition  <dig>  <cit> : the differences of element y and a set x denoted by Δ  such that Δ = {|y − x
i| : x
i ∈ x}.


definition  <dig>  <cit> : the differences for a set x, denoted by Δx, is a multiset such that: Δx = {|x
j − x
i|, 0 ≤ i < j ≤ n − 1}.


remark 1: we can write Δx in another form Δx = i = 0n − 1 ∪+ 
Δ ), where x = {x
 <dig>  x
 <dig>  …, x
n − 1}.


the partial digest problem, pdp  <cit> : given a multiset of n=n <dig> positive integers d = {d
 <dig>  d
 <dig>  d
 <dig>  …, d
n − 1}. is there a set of n integers x = {x
 <dig>  x
 <dig>  x
 <dig>  …, x
n − 1} such that Δx = d ?.

we also need two propositions. the first proposition is used to give another formula for the difference between three multiple sets. the formula will be used to prove the correctness of our proposed method. the second proposition is used to illustrate how to construct an example for the worst case, which leads to an exponential time for skiena’s algorithm  <cit> .


proposition 1: let d, l and z be three multisets, then \z = d\.


proposition  <dig>  <cit> : let 0<ε<112n, a
1 = {1 − nε, …, 1 − 2ε, 1 − ε}, a
2 = {ε, 2ε, …, nε}, a
3 = {ε, ε, …, 2nε}, a
4 = {ε, ε, …, 3nε}, a
5 = {1 − 3nε, …, 1 − ε, 1 − ε} and d = f ∪ g where f and g are disjoint sets satisfying f ∪ g* = a
 <dig> and g* = {1 − g| ∀ g ∈ g}. let a = a
1 ∪ a
2 ∪ a
4 ∪ a
5 ∪ d ∪ { <dig>  1}, we can choose d such that giving Δa to skiena’s algorithm will take it at least Ω time to find a.

methods
in this section, we present three algorithms. the first one is the best previous practical algorithm, while the other two algorithms are the proposed algorithms.

best previous practical algorithm
the main goal of pdp is to reconstruct the elements, x
i, from a multiset, d, of n = n / <dig> integers by finding a set x such that Δx = d. the best known algorithm for solving the pdp is based on the branch and bound strategy. the main idea of this algorithm is to construct the set x incrementally. the algorithm is based on the depth-first search algorithm with two bounding conditions. we refer to this method as algorithm bbd .

observation
figure  <dig> represents the execution of the algorithm bbd on d = { <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  10}. in the figure, we use the following notations.fig.  <dig> tracing the bbd algorithm on d = { <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  10}


i. each node in the solution tree for the pdp represents a pair of sets  where the index k represents the level number, and the index i represents the node number in the level k.

ii. the number t inside the circle at the top right of each node represents the t-th calling for the procedure place.

iii. the symbol “⊥” is used when the current node does not generate any new elements.




it is clear that at the level  <dig>  the root contains the multiset d
00 = { <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  8} and the set x
00 = { <dig>  10}. in general, x
00 = { <dig>  width} and d
00 = d \ {width}, where d is the input of the pdp. additionally, each node  at the level k, k >  <dig>  of the solution tree generates at most two nodes at the level k +  <dig> as follows:add an element y to x
kj to generate x
k + 1i = x
kj ∪ {y}.

remove the elements of Δ  from d
kj to generate d
k + 1i = d
kj \ Δ.




where y = max  or y = width ‐ max. we also observe from fig.  <dig> the following:there are two identical subproblems  and , such that d
20 = d
21 = { <dig>   <dig>   <dig>  7} and x
20 = x
21 = { <dig>   <dig>   <dig>  10}.

the two identical subproblems lead to two identical solutions, { <dig>   <dig>   <dig>   <dig>  10} and { <dig>   <dig>   <dig>   <dig>  10}.




first proposed method
in this subsection, we propose an efficient method that reduces the running time of the pdp. the method is based on traversing the solution tree for the pdp using the breadth-first strategy instead of the depth-first strategy. we also consider the two bounding conditions that are used in algorithm bbd. moreover, we remove all identical subproblems at the same level. the main steps of our method are as follows:build the solution tree for the pdp using the breadth-first strategy, level by level.

before creating the nodes of the new level, we remove all repeated nodes existing in the current level such that any node appears only one time in the current level.




algorithm bbb  shows the steps of our proposed method to solve the pdp. the input of the algorithm is the multiset d that consists of n / <dig> elements. initially the algorithm starts with two sets and two lists. the two sets are x
00 = { <dig>  width} and d
00 = d \ {width}. the two lists are l
x = {x
00} and l
d = {d
00}. in general, l
d and l
x represent the lists of sets, d
kj and x
kj, respectively, at the current level, k, of the solution tree. the main step of the proposed algorithm is a while loop that represents the number of levels in the solution tree for the pdp. in each iteration k of the while loop, we will generate the elements of the next level, k +  <dig>  by calling the procedure generatenextlevel , k ≥  <dig>  the inputs of the procedure are three lists of sets, l
d, l
x and s, for the current level k. the outputs of the procedure are three lists of sets, l
d, l
x and s, for the level k +  <dig> 

the body of the procedure generatenextlevel consists of an initialization and a loop. in the initialization, we will use two auxiliary lists, al
d and al
x, which contain the sets d
k + 1j and x
k + 1j, respectively, for the next level in the solution tree. the initial value of the two lists is empty. the main loop in the generatenextlevel procedure represents the process of generating the elements of the next level and storing it in the two auxiliary lists al
d and al
x. each pair of sets, d
ki ∈ l
d and x
ki ∈ l
x, will generate at most two pairs of sets, as follows: 
d
k + 1j = d
ki\Δ  and x
k + 1j = x
ki ∪ {y } if the condition Δ  ⊆ d
ki is true.

 
d
k + 1l = d
ki \ Δ  and x
k + 1l = x
ki ∪ {width ‐ y } if the condition Δ  ⊆ d
ki is true.




the two sets, x
k + 1j and d
k + 1j and in a similar way, x
k + 1l and d
k + 1l, will be added to the auxiliary lists al
x and al
d, respectively if set x
k + 1j does not exist in list al
x.

the main loop of the generatenextlevel procedure will terminate when list l
d is empty. this means that all of the elements of the current level k are replaced by new elements in the next level k +  <dig>  in this case, we assign the lists al
d and al
x to l
d and l
x, respectively. figure  <dig> illustrates how this idea works.fig.  <dig> tracing the bbb algorithm on d = { <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  10}




now, we investigate how to test the equality between two nodes  and  in the solution tree for the pdp. the test can be performed by comparing the equality between d
ki and d
kj and the equality between x
ki and x
kj. the following theorems and corollary prove that the two nodes  and  are equal if the set x
ki is equal to the set x
kj.


theorem 1: given a subproblem  in the solution tree for the pdp, the relation d
ki = d \ Δx
ki is valid, where k ≥  <dig>  d is the input of the pdp, d
ki is a modified version of d produced by removing some of its elements and x
ki contains k +  <dig> elements of the candidate solution.


proof
: we will prove the theorem using mathematical induction.

first, we prove that the relation is true at k =  <dig> 

the algorithm starts by finding the maximum elements of the set d, width, and updates the value of the sets d and x. so, x
00 = { <dig>  width} and d
00 = d\{width}. from the definition of Δ, we have, Δx
00 = {width}. therefore, d
00 = d\Δx
 <dig> 

second, we assume that the relation is true at k .

third, we prove that the relation is true at k +  <dig> 

from the algorithm, the set d
k + 1j can be constructed from a node at level k, say . therefore, d
k + 1j = d
ki \ Δ  and x
k + 1j = x
ki ∪ {y}. this implies that d
k + 1j = \Δ, because d
ki = d \ Δx
ki. therefore, d
k + 1j = d \ ) . from definition  <dig> and because x
k + 1j = {y} ∪ x
ki, then d
k + 1j = d \ Δx
k + 1j.


theorem 2: if there are two subproblems  and  such that x
ki = x
kj, then d
ki = d
kj.


proof
: from theorem  <dig>  the following equations are valid d
ki = d\Δx
ki and d
kj = d\Δx
kj, for the subproblems  and , respectively. because x
ki = x
kj, then d
ki = d\Δx
ki = d\Δx
kj = d
kj.


corollary 1: if there are two subproblems  and  such that x
ki = x
kj, then  = .

final proposed algorithm
we proposed an enhanced version of algorithm bbb. the proposed algorithm improves the running time and storage of the bbb algorithm especially for the worst case. in the proposed algorithm, we try to reduce the memory consumption of the bbb algorithm without increasing its running time. the improved algorithm depends on the following two steps:building the solution tree for the pdp until a specific level α is reached by using the bbb algorithm.

for each node in the level α, building the remainder subtrees individually with the bbb algorithm.




figure  <dig> represents the idea described above for the proposed algorithm. we called the algorithm bbb <dig>  branch and bound based on breadth two times.fig.  <dig> strategy of the bbb <dig> algorithm




to determine the best value of α, we need to compute the memory complexity of the proposed algorithm for the worst case. each node in level k will be replaced by at most two nodes in the level k +  <dig>  0 ≤ k < α. therefore, the total number of nodes at level α is 2α. in each node, we store two sets, d
αi and x
αi, of total size o . hence, the total amount of storage necessary to reach the level α is o  memory for bbb <dig>  in the second step of the bbb <dig> algorithm, we apply bbb on each node individually. the maximum number of remaining levels is n − α and the total amount of memory required for the second step for the worst case is o . hence, the memory complexity of the bbb <dig> algorithm for the worst case is given by: mα=on22α+on22n−α 


let α
m be the number of levels that lead to the minimum memory required by bbb <dig>  the value of α
m can be computed by determining the number of levels, α, that minimizes the memory consumption m, for 0 ≤ α ≤ n −  <dig> the find_αm procedure computes the value of αm for the instance n in o time. the input of the procedure is the size of the multiset d, n, and the output of the procedure is αm. from the size of the multiset d, we can compute the value of n by solving the quadratic equation nn−12=n. the pseudocode of the find_αm procedure is as follows:

procedure find_αm 

begin
 n=1+1+8n <dig> 



 α=αm= <dig> 



 mmin=n22α+2n−α 


for α =  <dig> to n do


 m=n22α+2n−α 


if m < m
min then


 mmin=m 



 αm=α 


end if

end for




end

in algorithm bbb <dig>  we start by finding the value of α
m and then we build the solution tree until the level α
m is reached by using the breadth-first strategy and deleting all similar elements. at the level α
m, we have at most 2αm elements. after obtaining these elements, we consider each node as a root and then expand this node using the bbb algorithm. for the worst case, the number of handled elements at levels αm +  <dig>  αm + <dig>  …,n are  <dig>   <dig>  …, 2n−αm respectively, while the number of handled elements during execution of the bbb algorithm at levels αm +  <dig>  αm +  <dig> …,n are 2αm+ <dig>  2αm+ <dig>  …,2n respectively. therefore, bbb <dig> reduces the memory required by the bbb algorithm.

test methodology
in this section, we present the methodology that is used to evaluate the performance of the algorithms, bbd, bbb and bbb <dig>  according to their running times and memory consumptions.

platform specification
we implemented the algorithms on a dual octa-core processor machine  with 128 gb ram. each processor has a  <dig>  ghz speed with 20 mb of cache. the algorithms were implemented in c++ programs. the programs were compiled using g++ with the -o <dig> flag under 64-bit red hat enterprise linux  <dig> . in the experiments, we used a single core only.

simulated data
we studied the performance of the three algorithms, bbd, bbb and bbb <dig>  on different types of data and different sized data sets. we used two types of data described in previous studies. the first data set was a random data set   <cit> , while the second data set is the zhang data   <cit> . the rd was used to measure the performance of the algorithm for an average case; while the zd was used to measure the performance of the algorithm for the worst case. in terms of data set size, there are two factors that affect the performance of the algorithms for each data set. the first factor is the number of elements in the set x, n, while the second factor is the values of the n elements, m .

in the case of the rd, we assumed that there were n restriction sites in the dna segments distributed randomly. each input instance of the simulated data is a multiset d = Δx such that the set x contains n positive numbers randomly selected and each number is less than or equal to m. in the zd, the locations of the restriction sites were selected randomly according to proposition  <dig>  <cit> .

the values of n in the case of the rd are  <dig>   <dig>   <dig> … and  <dig>  while the values of n in the case of zd are  <dig>   <dig>   <dig>   <dig> … and  <dig>  the range of n for the zd is small because the running time for the algorithm was greater than 1 day when n >  <dig>  we also used different values of m as follows: m = n*q, where q =  <dig>   <dig>   <dig> and  <dig> .

running time and memory
for each value of n, we ran each algorithm  <dig> times with different inputs for the rd. for the zd, we reduced this number to  <dig> due to the increased running time of the algorithms, especially bbd. the running time for each algorithm for a fixed n represents the average time for all instances studied. if the running time of an algorithm was greater than 24 h for an input instance then the algorithm was terminated. therefore, the value of the algorithm for this input instance was omitted from the figures and the results.

we also measured the standard error of the mean  and coefficient of variation  for the three algorithms for rd and zd. finally, we used a non-parametric statistical test which is wilcoxon-signed-rank test  <cit>  to determine if there are a significant differences between the three algorithms in running time or memory consumption on rd and zd. we applied the wilcoxon-signed-rank test, at a significance level of  <dig> , to the following pairs: bbd & bbb, bbd & bbb <dig> and bbb & bbb <dig> algorithms with respect to running time and memory consumption for rd and zd.

we measured the running time in seconds using a c++ function. we also measured the memory consumed by the algorithm in megabytes using the linux command top.

RESULTS
the results from measuring the running times of the three algorithms on the simulated data are shown in fig.  <dig> and  for the rd and fig.  <dig> for the zd.fig.  <dig> running times of the bbd, bbb and bbb <dig> algorithms for the rd. the line for bbd algorithm is not complete because the running time is greater than the maximum value of y-axis or 24 h


fig.  <dig> running times of the bbd, bbb and bbb <dig> algorithms for the zd. the values on the y-axis are in log-scale




in the case of the rd, the results showed that the running times of the proposed algorithms, bbb and bbb <dig>  were less than the running time of bbd for all values of n and m as shown in fig.  <dig>  for large values of n and small values of q, the bbd algorithm required a large amount of time to find a solution, while the proposed algorithms, bbb and bbb <dig>  found the solution in a suitable time. for example, in the case of n ≥  <dig> and m = n *  <dig>  the running time for the bbd algorithm was greater than 24 h, while both proposed algorithms found the solution in time less than 13 min, as shown in fig. 4a and b. however, the difference in running times between the bbd algorithm and the proposed algorithms, bbb and bbb <dig>  decreased with increasing values of m. this behavior is attributed to the probability of repeated subproblems decreasing with increasing values of m. therefore, both algorithms, bbb and bbb <dig>  spend a large amount of time checking for the repetition of elements with a low probability of repetitions. additionally, for small m values the probability of repeated subproblems is high, thereby increasing the running time of the bbd algorithm. if we fixed the value of n, the running time for all algorithms decreased with increasing m values as shown in . both proposed algorithms behaved similarly with increasing m values. we also observed that the difference in the running times for two successive values of m is relatively small for both proposed algorithms  as shown in . on the other hand, the difference in the running times for the bbd algorithm for two successive values of m is large when m = n *  <dig> and m = n *  <dig> as shown in . finally, we found that there was little difference, increasing or decreasing, between the running times of bbb and bbb <dig> for all values of n and m. in general, with large values of n and m, the bbb <dig> algorithm was faster than the bbb algorithm. for example, in the case of n ≥ <dig> and m = n * q and q =  <dig> and  <dig> , bbb <dig> was slightly better than bbb.

for the zd, there was no change in the running time when we used different values of m for the three algorithms. therefore, we used the zd with the m value fixed at m = n* <dig>  the consistent performance of the algorithms using the zd with different values of m was due to the systematic selection of the elements of a according to proposition  <dig> 

the performance of the three algorithms for the zhang data set instances is given in fig.  <dig>  we observed that the running time of bbb <dig> was less than bbb and bbb was less than bbd for all instances. the percentage of running time improvement for bbb and bbb <dig> with respect to bbd increased with increasing n. in our studied cases, the running time was improved by at least 75%. moreover, the bbd and bbb algorithms cannot solve any instances with n ≥  <dig> and n ≥  <dig> in time less than 24 h respectively. in the other side, the bbb <dig> algorithm solves any instance with n≤ <dig> in time less than 24 h.

the improved running time of bbb algorithm can be attributed to its reduction of the number of subproblems handled at the levels α
m +  <dig>  α
m +  <dig> …,n in the solution tree for the pdp. therefore, the time spent checking subproblem repetition is reduced.

the results of measuring sem and cv for the running time of the three algorithms are shown in  for rd and zd. the results show that the values of sem and cv for bbd algorithm were very large compared to the values of sem and cv for bbb and bbb <dig> algorithms in case of rd. for the zd, the values of sem and cv of bbb and bb <dig> algorithms were less than the values of sem and cv of bbd algorithm in the most instances. moreover, the application of wilcoxon-signed-rank test shows that there was a significant difference between the following pairs of the algorithms as shown in :i. bbd and bbb in case of rd and zd.

ii. bbd and bbb <dig> in case of rd and zd.

iii. bbb and bbb <dig> in case of zd.




we also evaluated the algorithms in terms of their memory consumptions. the results of measuring the memory consumed by the three algorithms on the simulated data are shown in  for the rd and fig.  <dig> for the zd. for the rd, the results show that for small values of m such as m = n *  <dig> and n *  <dig>  the bbb and bbb <dig> algorithms consumed less memory than the bbd algorithm. in the case of large m values, the bbd algorithm consumed less memory than the bbb and bbb <dig> algorithms, but small m values increased the number of repeated subproblems. thus, the number of elements in each level is large for the bbd algorithm. therefore, the bbd algorithm consumed more memory than the two proposed algorithms. in general, the memory consumption of the bbb algorithm is a little better than that of the bbb <dig> algorithm.fig.  <dig> memory consumed of bbd, bbb, and bbb <dig> algorithms on zd. the values on the y-axis are in log-scale




for the zd, less memory was consumed by bbb <dig> than bbb for all instances. so, the bbb <dig> algorithm significantly reduced the memory required by the bbb algorithm. the percentage of improvement in memory consumption of bbb <dig> compared to bbb increases as n increases. in general, the memory required by the bbd algorithm is less than the bbb and bbb <dig> algorithms.

the results of measuring sem and cv for the memory of the three algorithms are shown in  for the rd and the zd. the results show that there were differences, decreasing and increasing, in the values of sem and cv of the three algorithms for the rd. for the zd, the values of sem and cv of bbd algorithm were less than the proposed algorithms in the most instances. moreover, the application of wilcoxon-signed-rank test, see , shows that there was a significant difference between the following pairs of the algorithms in the following cases:i. bbd and bbb on zd.

ii. bbd and bbb <dig> on zd.

iii. bbb and bbb <dig> on zd.




real data
we tested the final proposed bbb <dig> algorithm on real digestion experiments and simulated digestion experiments.

luciferase gene
we extracted the data from a partial digestion of the luciferase gene  <cit>  of length  <dig>  the partial digestion was performed with the enzyme taqi, which cuts the gene at the tcga sequence motif. the output of the partial digestion process is the multiset d consisting of the distances between the tcga locations on the luciferase gene, which is d = { <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  2009}. our proposed algorithms take the multiset d as input and output two solutions in the set s = {{ <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  2009}, { <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  2009}}. the solution x = { <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  2009}, represents the solution for the real data. this means that the map of tcga on luciferase gene at the locations  <dig>   <dig>   <dig>   <dig>   <dig>  and  <dig> 

e. coli k <dig> genome digestion simulation
we also tested our proposed algorithms with a simulated partial digestion experiment using the e. coli k <dig> genome version  <dig>  and a set of restriction enzymes . the size of the genome is  <dig> ,652 bp. for each restriction enzyme, we simulated the partial digestion experiment by cutting the e. coli k <dig> genome at all occurrences of the corresponding restriction site. then, we recorded the lengths of n cut fragments in d. the restriction enzymes, restriction sites, the number of cut fragments , n, times and memory consumptions are shown in table  <dig>  finally, we applied the final proposed algorithm to the extracted sets and in all cases the outputs contained the correct set of restriction site positions. it is clear that from table  <dig>  the running time and memory consumed of the algorithm increases with increase the value of n and n.table  <dig> the performance of the bbb <dig> algorithm for the restriction enzymes used in this study

n
n

abbreviations: n is the number of cut fragments, n is the number of restriction sites




CONCLUSIONS
in this paper, we addressed the pdp and proposed two algorithms, bbb and bbb <dig>  in the bbb algorithm, we built the solution tree for the pdp in the breadth-first manner instead of the depth-first manner taking into consideration two conditions of pruning and deleting all repeated subproblems in the same level. the bbb solves many instances which are not solved by bbd in time less than 24 h. the main disadvantage of bbb is that the memory consumed grows exponentially. in bbb <dig>  we reduced the memory required by bbb by solving the problem using two stages, with each stage working in the breadth-first manner. we also determined the number of the levels, which leads to reduced memory consumption without increasing the running time.

we measured the efficiency of the proposed algorithm compared to the best known practical algorithm on the basis of time and memory consumption. in the evaluation, we considered the following parameters:  types of data, rd and zd;  value of n; and  value of m. in the case of running time, the bbb <dig> algorithm is faster than other algorithms. the efficiency increased when the zd was used. in the case of memory, the bbd algorithm consumed less memory than other algorithms, but the running time was very slow especially for the zd. finally, we applied the bbb <dig> algorithm on luciferase gene and the e. coli k <dig> genome.

additional files

additional file 1: supplementary figure. figure f <dig> a–c represents the behavior of the running time for bbd, bbb, and bbb <dig> algorithms with different values of m and fixed value of n. the values on the y-axis are in log-scale. in figure a, the x-axis does not include the value of m = n *  <dig>  because the running time is greater than 24 h. 


additional file 2: supplementary data. calculation of the standard error of mean , coefficient of variation , and wilcoxon signed-rank test for the running time. tables s1-s <dig>  in sheet  <dig> and  <dig>  represent the calculation of sem and cv for the running time of bbd, bbb, and bbb <dig> algorithms in case of random data and zhang data respectively. tables s6–s <dig>  in sheet  <dig>  represent the wilcoxon signed-rank test between bbd and bbb algorithms for the running time of random and zhang data. tables s9–s <dig>  in sheet  <dig>  represent the wilcoxon signed-rank test between bbb and bbb <dig> algorithms for the running time of random and zhang data. tables s12–s <dig>  in sheet  <dig>  represent the wilcoxon signed-rank test between bbd and bbb <dig> algorithms for the running time of random and zhang data. 


additional file 3: supplementary figure. figure f <dig> a–d represents the memory consumed for bbd, bbb, and bbb <dig> algorithms on random data. 


additional file 4: supplementary data. calculation of the standard error of mean , coefficient of variation , and wilcoxon signed-rank test for the memory. tables s15-s <dig>  in sheet  <dig> and  <dig>  represent the calculation of sem and cv for the memory consumption of bbd, bbb, and bbb <dig> algorithms in case of random data and zhang data respectively. tables s20–s <dig>  in sheet  <dig>  represents the wilcoxon signed-rank test between bbd and bbb algorithms for the memory consumption of random and zhang data. tables s23–s <dig>  in sheet  <dig>  represent the wilcoxon signed-rank test between bbb and bbb <dig> algorithms for the memory consumption of random and zhang data. tables s26-s <dig>  in sheet  <dig>  represent the wilcoxon signed-rank test between bbd and bbb <dig> algorithms for the memory consumption of random and zhang data. 




