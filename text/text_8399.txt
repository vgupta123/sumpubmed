BACKGROUND
nonlinear regression, like linear regression, assumes that the scatter of data around the ideal curve follows a gaussian or normal distribution. this assumption leads to the familiar goal of regression: to minimize the sum of the squares of the vertical or y-value distances between the points and the curve. however, experimental mistakes can lead to erroneous values â€“ outliers. even a single outlier can dominate the sum-of-the-squares calculation, and lead to misleading results.

identifying outliers is tricky. even when all scatter comes from a gaussian distribution, sometimes a point will be far from the rest. in this case, removing that point will reduce the accuracy of the results. but some outliers are the result of an experimental mistake, and so do not come from the same distribution as the other points. these points will dominate the calculations, and can lead to inaccurate results. removing such outliers will improve the accuracy of the analyses.

outlier elimination is often done in an ad hoc manner. with such an informal approach, it is impossible to be objective or consistent, or to document the process.

several formal statistical tests have been devised to determine if a value is an outlier, reviewed in  <cit> . if you have plenty of replicate points at each value of x, you could use such a test on each set of replicates to determine whether a value is a significant outlier from the rest. unfortunately, no outlier test based on replicates will be useful in the typical situation where each point is measured only once or several times. one option is to perform an outlier test on the entire set of residuals  of least-squares regression. the problem with this approach is that the outlier can influence the curve fit so much that it is not much further from the fitted curve than the other points, so its residual will not be flagged as an outlier.

rather than remove outliers, an alternative approach is to fit all the data  using a robust method that accommodates outliers so they have minimal impact  <cit> . robust fitting can find reasonable best-fit values of the model's parameters but cannot be used to compare the fits of alternative models. moreover, as far as we know, no robust nonlinear regression method provides reliable confidence intervals for the parameters or confidence bands for the curve. so robust fitting, alone, is a not yet an approach that can be recommended for routine use.

as suggested by hampel  <cit>  we combined robust regression with outlier detection. it follows three steps.

 <dig>  fit a curve using a new robust nonlinear regression method.

 <dig>  analyze the residuals of the robust fit, and determine whether one or more values are outliers. to do this, we developed a new outlier test adapted from the false discovery rate approach of testing for multiple comparisons.

 <dig>  remove the outliers, and perform ordinary least-squares regression on the remaining data.

we describe the method in detail in this paper and demonstrate its properties by analyzing simulated data sets. because the method combines robust regression and outlier removal, we call it the rout method.

RESULTS
brief description of the method
the methods section at the end of the paper explains the mathematical details. here we present a nonmathematical overview of how both parts of the rout method  work.

robust nonlinear regression
the robust fit will be used as a 'baseline' from which to detect outliers. it is important, therefore, that the robust method used give very little weight to extremely wild outliers. since we anticipate that this method will often be used in an automated way, it is also essential that the method not be easily trapped by a false minimum and not be overly sensitive to the choice of initial parameter values. surprisingly, we were not able to find an existing method of robust regression that satisfied all these criteria.

based on a suggestion in numerical recipes  <cit> , we based our robust fitting method on the assumption that variation around the curve follows a lorentzian distribution, rather than a gaussian distribution. both distributions are part of a family of t distributions as shown in figure  <dig>  the widest distribution in that figure, the t distribution for df =  <dig>  is also known as the lorentzian distribution or cauchy distribution. the lorentzian distribution has wide tails, so outliers are fairly common and therefore have little impact on the fit.

we adapted the marquardt nonlinear regression algorithm to accommodate the assumption of a lorentzian  distribution of residuals. figure  <dig> shows three data sets which include an outlier. the solid curves show the results of our robust nonlinear regression, which are barely influenced by the outlier. in contrast, the dotted curves show the least-squares results, which are dramatically influenced by the outlier.

our method fits data nearly as quickly as ordinary nonlinear regression, with no additional choices required. the robust fitting method reports the best-fit values of the parameters, but does not report standard errors or confidence intervals for these values.

least-squares regression quantifies the scatter of data around the curve by reporting sy.x, sometimes called se, the standard error of the fit. it is computed by taking the square root of the ratio of the sum-of-squares divided by the number of degrees of freedom . sy.x is in the same units as the y values, and can be thought of as the standard deviation of the residuals.

the presence of an outlier would greatly increase the value of sy.x. therefore, we use an alternative method to quantify the scatter of points around the curve generated by robust nonlinear regression. we quantify the scatter by calculating the  <dig>  percentile of the absolute values of the residuals . we call this value  the robust standard deviation of the residuals .

outlier detection
after fitting a curve using robust nonlinear regression, a threshold is needed for deciding when a point is far enough from the curve to be declared an outlier. we reasoned that this is very similar to the problem of looking at a set of many p values and choosing a threshold for deciding when a p value is small enough to be declared 'statistically significant'.

when making multiple statistical comparisons, where do you draw the line between 'significant' and 'not significant'? if you use the conventional 5% significance level for each comparison, without adjusting for multiple comparisons, you'll get lots of false positives. the bonferroni method uses a lower cut-off defined as 5% divided by the number of comparisons in the family. it is a helpful tool when you are making a few comparisons, but is less useful when you make many comparisons as it can miss many real findings . benjamani and hochberg  <cit>  developed a method to deal with multiple comparisons that takes into account not only the number of comparisons but the distribution of p values within the set. when using this method, you set the value q in order to control the false discovery rate . if q is set to 1%, you can expect fewer than 1% of the 'statistically significant' findings  to be false positives, while the rest  are real.

we adapted the concept of fdr to create a novel approach to identify one or several outliers. first divide each residual by the rsdr. this ratio approximates a t distribution, which can be used to obtain a two-tailed p value. now use the fdr method to determine which of these p values is 'significant', and define the corresponding points to be outliers.

choosing an appropriate value for q
outlier elimination by the fdr technique requires that you choose a value of q. if q is small, then very few good points will be mistakenly identified as 'outliers', but the power to detect true outliers will be low. if q is larger, then the power to find true outliers is high, but more good points will be falsely identified as outliers.

the graphs in figure  <dig> show the consequence of setting q to  <dig> %, 1% or 10%. in each of the graphs, there is one outlier  that is placed just beyond the boundary of outlier detection. in every case, if the point were moved a tiny bit closer to the curve, it would no longer be detected as an outlier.

if q is set to 10%, outlier removal seems a bit too aggressive. the open circles in the right panels are not all that far from the other points. if q is set to  <dig> %, as shown on the two graphs in the left, outlier removal seems a bit too timid. points have to be pretty far from the rest to be detected. the two graphs in the middle panels have q set to 1%. the choice is subjective, but we choose to set q to 1%, although there may be situations where it makes sense to set q to a lower or higher value.

if all scatter is gaussian, how many 'outliers' are mistakenly identified?
we simulated  <dig> different situations . for each, we simulated  <dig>  data sets adding gaussian error, analyzed the results with the rout method , and recorded how many outliers were  eliminated. the fraction of data points eliminated as "outliers" ranged from  <dig> % to  <dig> %, with a median of  <dig> %. we observed no obvious correlation between fraction of points eliminated as 'outliers', and the choice of model, sample size, or amount of scatter but we did not investigate these associations in depth.

how well are single outliers detected?
the rout method  detected the outlier in all but  <dig> of these simulations. in addition, it occasionally falsely identified some 'good' points as outliers. it identified one point to be an outlier in  <dig> simulated experiments, two points in  <dig> experiments, three points in one experiment, four points in one experiment, and five points in one experiment. for each of the simulated experiments, we expressed the false discovery rate  as the number of 'good points' falsely classified as outliers divided by the total number of outliers detected. for the few experiments where no outliers were detected, the fdr is defined to equal  <dig> . the average fdr for the  <dig> simulated experiments was  <dig> %.

in  <dig> simulations, the rout method  detected the outlier in  <dig> % of the simulations. additionally, it mistakenly identified a 'good' point as an outlier in  <dig> simulated experiments, and two points in  <dig>  and three points in  <dig>  the average fdr was  <dig> %. this example shows that our method can also detect moderate outliers most of the time, maintaining the false discovery rate below the value of q we set.

how well are multiple outliers detected?
we used the same setup as figure  <dig> . when we simulated  <dig> data sets with nine  being outliers, the rout method  detected 86% of the outliers, with an average fdr of  <dig> %. with two outliers, it detected more than  <dig> % of them, with an average fdr of  <dig> %.

we also simulated experiments with moderate outliers, using the same setup as figure  <dig>  when we simulated  <dig> experiments with two outliers, 57% of the outliers were detected with an fdr of  <dig> %. when we simulated experiments where  <dig> of the  <dig> points were outliers, 28% of the outliers were detected with an fdr of only  <dig> %.

how well does it work when scatter is not gaussian?
even though the rout method was developed to analyze data where the scatter is gaussian with the addition of a few outliers, we wanted to know if it also works well when the scatter is not gaussian. we simulated  <dig> data sets where the scatter follows a t distribution with two degrees of freedom. figure  <dig> shows three of these data sets, showing the wide scatter.

is the method fooled by garbage data?
one fear is that an automated outlier rejection method might report seemingly valid results from data that is entirely random with no trend at all. figure  <dig> shows the first of  <dig> simulated data sets where we tested this idea.

we fit these simulated data sets to a sigmoid dose-response curve, fixing the bottom plateau and slope, asking the program to fit the top plateau and ec <dig>  both curve fitting methods  were able to fit curves to about two thirds of the simulated data sets, but the majority of these had ec <dig> values that were outside the range of the data. only  <dig> of the simulated data sets had ec <dig> values within the range of the data , and all the r <dig> values were less than  <dig> . our outlier removal method found an outlier in only one of these  <dig> simulated data sets.

these simulations show that the rout method does not go wild rejecting outliers, sculpting completely random data to fit the model. when given garbage data, the outlier rejection method very rarely finds outliers, and so almost always reports the same results  as least-squares regression.

what about tiny data sets?
another fear is that the outlier removal method would be too aggressive with tiny data sets. to test this, we simulated small data sets fitting one parameter  or four parameters . in both cases, when the number of degrees of freedom was  <dig> or  <dig> , our method never found an outlier no matter how far it was from the other points. when there were three degrees of freedom, the method occasionally was able to detect an outlier, but only when it was very far from the other points and when those other points had little scatter.

these simulations show that the rout method does not wildly reject outliers from tiny data sets.

discussion
overview
ordinary least-squares regression is based on the assumption that the scatter of points around a fitted line or curve follows a gaussian distribution. outliers that don't come from that distribution can dominate the calculations and lead to misleading results. if you choose to leave the outliers in the analysis, you are violating one of the assumptions of the analysis, and will obtain inaccurate results.

a widely accepted way to deal with outliers is to use robust regression methods where outliers have little influence. the most common form of robust regression is to iteratively weight points based on their distance from the curve. this method is known as irls . this method is popular because it can be easily implemented on top of existing weighted non-linear least-squares fitting algorithms. the drawback of most irls methods is that the weighting schemes correspond to underlying distribution densities that are highly unlikely to occur in practice. for this reason, we chose not to use irls fitting, but instead to use a maximum likelihood fitting procedure assuming that scatter follows a lorentzian distribution density. the best-fit parameters from this approach are sometimes called m-estimates.

robust methods are appealing because outliers automatically fade away, and so there is no need to create a sharp borderline between 'good' points and outliers. but using robust methods creates two problems. one is that while robust methods report the best-fit value of the parameters, they do not report reliable standard errors or confidence intervals of the parameters. we sought a method that offered reliable confidence intervals without resorting to computationally expensive bootstrapping, but were unable to find an accurate method, even when given data with only gaussian scatter.

the other problem is that robust methods cannot be readily extended to compare models. in many fields of science, the entire goal of curve fitting is to fit two models and compare them. this is done by comparing the goodness-of-fit scores, accounting for differences in number of parameters that are fit. this approach only works when both models 'see' the same set of data. with robust methods, points are effectively given less weight when they are far from the curve. when comparing models, the distance from the curve is not the same for each model. this means that robust methods might give a particular point a very high weight for one model and almost no weight for a different model, making the model comparison invalid.

we decided to use the approach of identifying and removing outliers, and then performing least-squares nonlinear regression. we define outliers as points that are 'too far' from the curve generated by robust nonlinear regression. we use the curve fit by robust nonlinear regression, because that curve  is not adversely affected by the outliers.

since outliers are not generated via any predictable model, any rule for removing outliers has to be somewhat arbitrary, if the threshold is too strict, some rogue points will remain. if the threshold is not strict enough, too many good points will be eliminated. many methods have been developed for detecting outliers, as reviewed in  <cit> . but most of these methods can only be used to detect a single outlier, or can detect multiple outliers only when you state in advance how many outliers will exist.

we adapted the fdr approach to multiple comparisons, and use it as a method to detect any number of outliers. we are unaware of any other application of the fdr approach to outlier detection.

the fdr method requires that you set a value for q. we choose to set q to 1%. ideally, this means that if all scatter is gaussian, you would  declare one or more data points to be outliers in 1% of experiments you run. in fact, we find that about 1â€“3% of simulated experiments had one  false outlier.

why do we find that outliers are identified in 1â€“3% of simulated experiments with only gaussian scatter, when q is set to 1%? the theory behind the fdr method predicts that one or more 'outliers' will be  identified in q% of experiments. but this assumes that the ratio of the residual to the rsdr follows a t distribution, so the p values are randomly spaced between  <dig> and  <dig>  you'd expect this if you look at the residuals from least-squares regression and divide each residual by the sy.x. our simulations  found that the average rsdr from robust regression matches the average sy.x from least-squares regression. but the spread of rsdr values  is greater than the spread of sy.x. in 1â€“2% of the simulated experiments, the rsdr is quite low because two-thirds of the points are very close to the curve. in these cases, the t ratios are high and the p values are low, resulting in more outliers removed.

what happens if the data set is contaminated with lots of outliers? since we define the rsdr based on the position of the residual at the 68th percentile, this method will work well with up to  <dig> percent outliers. in fact, our implementation of the outlier removal method only tests the largest 30% residuals. if you have more outliers than that, this method won't be useful. but if more than 30% of your data are outliers, no data analysis method is going to be very helpful.

when should you use automatic outlier removal?
is it 'cheating' to remove outliers?
some people feel that removing outliers is 'cheating'. it can be viewed that way when outliers are removed in an ad hoc manner, especially when you remove only outliers that get in the way of obtaining results you like. but leaving outliers in the data you analyze is also 'cheating', as it can lead to invalid results.

the rout method is automatic. the decision of whether or not to remove an outlier is based only on the distance of the point from the robust best-fit curve.

here is a bayesian way to think about this approach. when your experiment has a value flagged as an outlier, there are two possibilities. one possibility is that a coincidence occurred, the kind of coincidence that happens in 1â€“3% of experiments even if the entire scatter is gaussian. the other possibility is that a 'bad' point got included in your data. which possibility is more likely? it depends on your experimental system. if it seems reasonable to assume that your experimental system generates one or more 'bad' points in a few percent of experiments, then it makes sense to eliminate the point as an outlier. it is more likely to be a 'bad' point than a 'good' point that just happened to be far from the curve. if your system is very pure and controlled, so 'bad' points occur in less than 1% of experiments, then it is more likely that the point is far from the curve due to chance  and you should leave it in. alternatively in that case, you could set q to a lower value in order to only detect outliers that are much further away.

don't eliminate outliers unless you are sure you are fitting the right model
this example points out that outlier elimination is only appropriate when you are sure that you are fitting the correct model.

outlier elimination is misleading when data points are not independent
the left panel of figure  <dig> show data fit to a dose-response model using outlier elimination with q set to 1%. one point  is detected as an outlier. the right panel shows that the data really come from two different experiments. both the lower and upper plateaus of the second experiment  are higher than those in the first experiment . because these are two different experiments, the assumption of independence was violated in the analysis in the left panel. when we fit each experimental run separately, no outliers are detected, not even if we increase q from 1% to 10%.

outlier elimination is misleading when you haven't chosen weighting factors appropriately
the left panel of figure  <dig> shows data fit to a dose-response model. four outliers were identified with q set to 1% . but note that the values with larger responses  also, on average, are further from the curve. this makes least-squares regression inappropriate. to account for the fact that the sd of the residuals is proportional to the height of the curve, we need to use weighted regression. the right panel shows the same data fit to the same dose-response model, but minimizing sum of the squares of the distance of the point from the curve divided by the height of the curve, using relative weighting. now no outliers are identified. using the wrong weighting method created false outliers.

outlier elimination is misleading when a value can be an outlier due to biological variation rather than experimental error
nonlinear regression is usually used with experimental data, where x is a variable like time or concentration or some other variable you manipulate in the experiment. since all the scatter is due to experimental error, it can make sense to eliminate any extreme outlier since it is almost certainly the result of an experimental mistake.

in other situations, each data point can represent a different individual. in this case, an outlier may not be due to experimental mistakes, but rather be the result of biological variation, or differences in some other variable that is not included in your model. here, the presence of the outlier may be the most interesting finding in the study. while our outlier method might prove useful to flag an outlier in this situation, it would be a mistake to automatically exclude such outliers without further examination .

future directions
we foresee several extensions to our rout method. we plan to extend the method so it can be used when comparing alternative models. we also plan to investigate whether the procedure can be simplified when identifying possible outliers from a set of values assumed to be sampled from a gaussian distribution. another extension might be to base outlier detection on variations of the fdr method  <cit> .

CONCLUSIONS
we describe a new method for identifying outliers when fitting data with nonlinear regression. this method combines robust regression and outlier removal, and so we call it the rout method. analyses of simulated data demonstrate that this method identifies outliers from nonlinear curve fits with reasonable power and few false positives. we have implemented this method in version  <dig> of graphpad prism, which we anticipate releasing in  <dig> ().

