BACKGROUND
high density oligonucleotide tiling arrays allow the investigation of transcriptional activity, protein-dna interactions and chromatin structure across a whole genome. tiling arrays have been used in a wide range of studies, including investigation of transcription factor activity  <cit>  and of histone modifications in animals  <cit>  and plants  <cit> , as well as dna methylation  <cit> . analyses of these data are usually based either on a sliding window  <cit> , or on hidden markov models   <cit> . other approaches have been suggested, e.g., by huber et al.  <cit>  and reiss et al.  <cit> , but are less common.

parameter estimates for sliding window approaches as well as hidden markov models are typically ad hoc. although there are some notable exceptions in gene expression studies  <cit> , no established procedures exist to obtain good parameter estimates from tiling array data, especially in the context of chromatin immunoprecipitation  experiments. attempts have been made to obtain parameter estimates by integrating genome annotations into the analysis  <cit> . while this may provide good results when investigating transcriptional activity in well studied organisms, it is limited by the quality of available annotations. for chip-chip studies the required annotation data is unavailable. a method for the localisation of transcription factors from chip-chip experiments by keleş  <cit>  does obtain the required parameter estimates from the data and allows for variations in length of enriched regions.

methods designed for the analysis of chip-chip data focus almost exclusively on the study of transcription factors . while this is an important class of experiments, chip-chip studies are not limited to transcription factors, and the analysis of other chip-chip experiments may require new methods. one other area of active research that utilises chip-chip experiments is the study of histone modifications and chromatin structure  <cit> . although both types of experiment employ the same technology, there are several important differences between them. most importantly, the  <dig> bp of dna bound by a histone complex are considerably longer than the typical transcription factor binding site, and the histone modifications of interest are expected to affect several neighbouring histones. consequently the chip fragments derived from a transcription factor binding site all originate from a small region containing the given binding site while regions affected by histone modifications can be much longer than the chip fragments used. as a result of this, the data from histone modification experiments usually contain long regions of interest encompassing several non-overlapping chip fragments, rather than the short and relatively isolated peaks produced by transcription factor studies.

here we consider the analysis of data from a histone modification study in arabidopsis  <cit> . these data consist of four chip samples for histone h <dig> with lysine  <dig> trimethylation  and four histone h <dig> chip samples that act as a control. the aim of this analysis is to identify and characterise regions throughout the genome that exhibit enrichment for h3k27me <dig>  it is desirable to use a method which is specifically designed for the analysis of histone modifications or flexible enough to accomodate the varying length of enriched regions. furthermore, the method should obtain all parameter estimates from the data without the use of genome annotations and be robust towards outliers. amongst the methods discussed above tilemap  <cit>  comes closest to these requirements. although it was developed with transcription factor analysis in mind it is general enough that it should provide useful results for other chip-chip experiments. this is emphasised by its application to histone modification  <cit>  and dna methylation  <cit>  data as well as transtription factor analysis  <cit> . tilemap obtains some, but not all, of the required parameter estimates from the data. to provide a method which meets the requirements oulined above we develop a two state hmm with t emission distributions. all parameter estimates for the model are obtained by maximum likelihood estimation using the baum-welch algorithm  <cit>  and viterbi training  <cit> . these methods have the advantage that no prior knowledge about parameter values is required and one need not rely on frequently unavailable genome annotations. to assess the performance of our model, we apply it to simulated and real data. results are compared to those produced by tilemap. the remainder of this article is structured as follows. in section  <dig> the hidden markov model is developed and mles for all parameters are derived in section  <dig>  the performance of the resulting model is assessed in terms of sensitivity and specificity on simulated data in sections  <dig> .3– <dig> . <dig>  in section  <dig> . <dig> the model is used to analyse a public chip-chip data set  <cit>  and results are compared to the original analysis of these data.

 <dig> 
RESULTS
tiling array data consists of a series of measurements taken along the genome. typically, microarray probes are designed to interrogate the genome at regular intervals. design constraints such as probe affinity and uniqueness cause differences in probe density along the genome and can lead to large gaps between probes. here we assume that the probe density is homogeneous except for a number of large gaps where the distance between adjacent probes is larger than max_gap. in the following analyses we use max_gap =  <dig> bp. this is identical to the value used by zhang et al.  <cit> , allowing for a direct comparison of results. consider a chip-chip tiling array experiment with two conditions, a chip sample x <dig> targeting the protein of interest and a control sample x <dig>  each sample xl has ml replicates  providing measurements for k genomic locations. the measurements for each probe are summarised by the "shrinkage t" statistic  <cit> :

  yk=x¯1k−x¯2kv1k∗m1+v2k∗m <dig>  

where vlk∗ is a james-stein shrinkage estimate of the probe variance obtained by calculating

  vlk∗=λˆl∗slmedian2+slk <dig>  

and slk <dig> are the usual unbiased empirical variances and λˆl∗ is the estimated optimal pooling parameter

  λˆl*=min∑k=1k2). 

other moderated t statistics have been suggested and could be used instead, most notably the empirical bayes t statistic used by ji and wong  <cit>  and the moderated t of smyth  <cit> . all of these approaches are designed to increase performance compared to the ordinary t statistic by incorporating information from all probes on the microarray into individual probe statistics. here we choose the "shrinkage t" because it does not require any knowledge about the underlying distribution of probe values while providing similar performance compared to more complex models  <cit> .

 <dig>  hidden markov model
to detect enriched regions we use a two state discrete time hidden markov model with continuous emission distributions and homogeneous transition probabilities , i.e., the transition probabilities depend only on the current state of the model. the use of homogeneous transition probabilities assumes equally-spaced probes within each observation sequence as well as a geometric distribution of the length of enriched regions. as discussed above there will be some variation in probe distances. using a relatively small value for max_gap ensures that the assumption of homogeneity holds at least approximately. the two states of the model correspond to enrichment or no enrichment in the chip sample. the model is characterised by the set of states s = {s <dig>  s2}, the initial state distribution p, the matrix of transition probabilities a and the state specific emission density functions fi, i =  <dig>   <dig>  the emission distribution of state si is modelled as a t distribution with location parameter μi, scale parameter σi, and νi degrees of freedom.

the use of t distributions has the advantage that their sensitivity to outliers can be adjusted via the degrees of freedom parameter, making them more robust and versatile than normal distributions. this is particularly useful when ν is estimated from the data  <cit> . it should be noted that the yk modelled here are from a t-like distribution ). while this in itself might suggest the use of t distributions for the fis, they are primarily chosen for their robustness. in the following we will refer to this model by its parameter vector θ = , where θ <dig> is the ordered pair  and θ <dig> the ordered triple .

given a hidden markov model θ and an observation sequence y, it is possible to compute the sequence of states q = q1q <dig> ..qk that is most likely to produce y. there are several approaches to obtaining q  <cit> . usually q is computed either by maximising the posterior probabilities p, k =  <dig>  ..., k or by calculating the sequence that maximises p. the latter provides the single most likely sequence of states and can be computed efficiently by the viterbi algorithm  <cit> . for the particular model used here both approaches are equivalent.

 <dig>  parameter estimation
in this section we will discuss two different approaches to estimate θ for the model described in section  <dig> . the methods under consideration are the em algorithm, which is usually known as the baum-welch algorithm in the context of hmms, and viterbi training. while the baum-welch algorithm is guaranteed to converge to a local maximum of the likelihood function, it is computationally intensive. viterbi training provides a faster alternative but may not converge to a local maximum.

 <dig> . <dig> initial estimates
both optimisation algorithms discussed here require initial parameter estimates. these are obtained from the data by first partitioning the vector of observations y into two clusters using k-means clustering  <cit> . from these clusters the location and scale parameters of the corresponding states are obtained as the mean and variance of the observations in the cluster. in the following, ν <dig> = ν <dig> =  <dig> is used as initial estimate for the degrees of freedom parameters.

 <dig> . <dig> baum-welch algorithm
the baum-welch algorithm  <cit>  is a well established iterative method for estimating parameters of hmms. it represents the em algorithm  <cit>  for the specific case of hmms. this algorithm can be used to optimise the transition parameters θ <dig> as well as the emission parameters θ <dig>  each iteration of the algorithm consists of two phases. during the first phase, the current parameter estimates are used to determine for each probe statistic in the observation sequence how likely it is to be produced by the different states of the model. in the second phase, parameters for the emission distributions of each state are estimated using contributions from all observations, according to the probability that they were produced by the respective state of the model. the state transition parameters are updated in a similar fashion, accounting for the probability of transitions between states based on the observation sequence and the current model. after each iteration this procedure results in a model which explains the observed data better than the previous one, approaching a locally optimal solution. using this method parameter estimates are updated until convergence is achieved. the details of the resulting algorithm are outlined in section  <dig> .

this method of parameter estimation is computationally expensive and time-consuming for a typical tiling array data set. the computing time can be reduced by fixing the degrees of freedom for the emission distributions in advance, thus avoiding the root-finding required for the estimation of these parameters. while this does not provide the same flexibility as estimating the required degree of robustness from the data it reduces the complexity of the optimisation problem. it is noted by liu and rubin  <cit>  that attempts to estimate the degrees of freedom are more likely to produce results which are of little practical interest. the impact on classification performance of this choice is investigated in section  <dig> .

the formulation of the baum-welch algorithm used in this article is based on the description given by rabiner  <cit>  and on the em algorithm derived by peel and mclachlan  <cit>  for fitting mixtures of t distributions.

 <dig> . <dig> viterbi training
while the baum-welch algorithm described in section  <dig> . <dig> is expected to provide good parameter estimates, it is computationally expensive. a faster model-fitting procedure can be devised by replacing the first phase of the baum-welch algorithm with a maximisation step. this method was introduced in  <cit>  as segmental k-means and is now commonly referred to as viterbi training. unlike the baum-welch algorithm which allows each probe statistic to contribute to the parameter estimates for all states, viterbi training assigns each observation to the state that is most likely to produce the given probe statistic. thus each observation contributes to exactly one state of the model. while each iteration of this method is faster than one iteration of the baum-welch algorithm some iterations may decrease the likelihood of the model, thus failing to advance it towards a useful solution. see section  <dig>  for further details on the implementation of viterbi training used here.

 <dig>  testing
 <dig> . <dig> simulated data
to assess the ability to distinguish between enriched and non-enriched probes of the models obtained by the different parameter estimation methods discussed in section  <dig> , we simulate data with known enriched regions. to ensure that the simulation study is providing meaningful results, it is based as closely on real data as possible. to this end, two independent analyses of the h3k27me <dig> data published by zhang et al.  <cit>  are carried out, one using tilemap  <cit> , the other based on our model. the result of each analysis is used to generate a new dataset with known enriched regions. see section  <dig> for further details. in the following these data are referred to as datasets i and ii respectively. since the simulation procedure is likely to bias results towards the model that was used in the process, we concentrate on the analysis of dataset i, with some results for dataset ii presented for comparison. the use of data based on both models allows us to consider their performance under advantageous and disadvantageous conditions.

 <dig> . <dig> performance measure
the performance of different models on these data is determined in terms of false positive and false negative rates at probe level. while the relative importance of false positives and false negatives depends on the experiment under consideration, they are often equally problematic in the context of chip-chip experiments, especially when considering experiments which investigate differences between different cell lines or developmental stages, where all incorrect classifications are of equal concern. in this context, we define false positives as probes that are classified as non-enriched by the analysis of the real data but are called enriched in the subsequent analysis of simulated data, and vice versa for false negatives.

the output of each model is the estimated posterior probability of enrichment for each probe. in practice, probe calls  are generated from this posterior probability based on a  <dig>  cut-off. for any given model, classification performance will change with the chosen threshold. thus we assess model performance across a range of cut-offs, reporting the relative number of false positives and false negatives as well as the error rate. the latter is also used to determine the cut-off that minimises incorrect classification results, and model performance is judged on the numbers of incorrect classifications at this optimal cut-off and at the usual  <dig>  cut-off, and on the distance between the optimal cut-off and  <dig> . the trade-off between sensitivity and specificity provided by the different models is characterised with roc curves and the associated auc values.

another measure of interest is the ability to characterise the length distribution of enriched regions correctly. when studying chromatin structure the extent of structural changes is of interest; this is the case for the data studied in section  <dig> . <dig>  this property of the different models is investigated in section  <dig> . <dig> 

 <dig> . <dig> estimating degrees of freedom
we now consider the performance of both the baum-welch procedure and viterbi training when all model parameters, including the degrees of freedom ν, are estimated from the data. both parameter estimation methods are used to fit an hmm to datasets i and ii, and the performance of resulting models is assessed in terms of the achieved error rate , roc curves  and their associated auc  for both datasets. to assess how well these methods perform in comparison to an established algorithm, we also fit a tilemap model to the two simulated datasets. the three models are compared to each other, as well as an ad hoc model which simply uses, without optimisation, the initial parameter estimates used by the two parameter optimisation methods. when comparing the performance of these models on both simulated datasets, it is important to consider that the simulation procedure introduces a bias towards the underlying model.

all models with optimised parameters outperform tilemap on both simulated datasets. while tilemap performs well on dataset i it is only slightly better than the model with ad hoc parameter estimates.

estimating all parameters from the data with either the baum-welch algorithm or viterbi training leads to models with high sensitivity, producing fewer false negatives than tilemap for any given cut-off . at the same time they lead to an increased number of false positives  compared to tilemap, indicating a slight reduction of specificity. when considering the error rate it becomes apparent that both baum-welch and viterbi training provide a favourable trade-off between sensitivity and specificity. these models reduce the number of incorrect classifications compared to tilemap both at the usual  <dig>  cut-off and at the optimal cut-off. moreover, while the baum-welch algorithm and viterbi training both lead to models with an optimal cut-off close to  <dig>  , tilemap provides an optimal cut-off of  <dig> , indicating that it underestimates the posterior probability of enrichment. this becomes even more apparent when considering the result for dataset ii where the optimal cut-off for tilemap is at  <dig>  compared to  <dig>  for baum-welch and  <dig>  for viterbi training. this result suggests that tilemap is more tuned towards avoiding false positives than false negatives. from the above results we estimate that the weight given to false positives by tilemap is approximately  <dig>  and  <dig> times larger than the weight for false negatives on datasets i and ii respectively. the roc curves  provide further evidence that the models with mles outperform tilemap. although all three models perform well on dataset i, both parameter optimisation methods lead to better results than tilemap. the benefits of optimising parameter estimates are further highlighted by the performance of the model with ad hoc estimates that is used as starting point for the optimisation procedures. on both datasets, optimised parameters provide a notable increase in performance, with tilemap performing only slightly better than the ad hoc model on dataset ii.

 <dig> . <dig> fixed degrees of freedom
estimating ν, the degrees of freedom, for t distributions from the data is time-consuming and may not be very accurate, especially for relatively large values of ν. in this section we investigate the effect of fixing ν a priori for both states of the model. only the case ν <dig> = ν <dig> is considered here. the remaining parameters are estimated from the training data using the baum-welch algorithm and viterbi training with ν =  <dig>   <dig>  ...,  <dig>  for each value of ν, we report the error rate  as well as the auc  on the simulated data.

for the best combination of ν and cut-off, both parameter estimation methods result in models with a classification performance comparable to the case of variable degrees of freedom . while the baum-welch algorithm tends to produce models with an optimal cut-off close to  <dig> , viterbi training only achieves this for large values of ν. notably, the best classification performance of the viterbi trained model is achieved with  <dig> degrees of freedom and a  <dig>  cut-off compared to  <dig> degrees of freedom and a  <dig>  cut-off from baum-welch. this results in a decreased performance of the viterbi model relative to the baum-welch model at the  <dig>  cut-off.

 <dig> . <dig> convergence
to reduce the time required for parameter estimation it is useful to limit the number of iterations. while each iteration of the baum-welch algorithm is guaranteed to improve the likelihood of the model, small changes to the parameter values do not necessarily lead to significant changes in the classification result. furthermore, viterbi training is not guaranteed to converge to a local maximum of the likelihood function and a likelihood based convergence criterion may not be appropriate for this method. here we investigate the convergence of both algorithms based on the error rate and auc to gauge the number of iterations required to achieve good classification results. parameter estimation is performed with  <dig> iterations for both algorithms. current estimates are used to classify the test data at every 5th iteration and auc  and error rate  are determined.

the most striking difference in the convergence behaviour of the two methods is that viterbi training appears to obtain good parameter estimates within a small number of iterations. further iterations of the algorithm do not improve results substantially, whereas the baum-welch procedure provides parameter estimates that are better than the ones obtained by viterbi training, both in terms of likelihood and classification performance, but takes substantially longer to obtain these estimates. the baum-welch algorithm not only requires more iterations than viterbi training, but the time required for each iteration is also longer.

 <dig> . <dig> length distribution of enriched regions
when studying histone modifications one possible characteristic of interest is the length of enriched regions. to assess how accurately the different methods reflect the length distribution of enriched regions, we compare the length of regions predicted by tilemap and by the model  to the length distribution of enriched regions in the simulated data . note that this length distribution may vary from the one found in real data. nevertheless this comparison highlights some of the differences between the two models. quantile-quantile plots of the respective length distributions show that tilemap systematically underestimates the length of enriched regions  and figure  <dig> ). while this effect is relatively small on dataset i there is some indication that it increases with region length and long regions may not be characterised appropriately by tilemap ). this observation is further supported by the length distribution of enriched regions produced by tilemap on dataset ii ). enriched regions in dataset ii are generally longer than regions in dataset i. this difference is not captured by tilemap. both tilemap and the baum-welch trained model produce several regions that are shorter than the shortest enriched region in the simulated data ). there are two possible explanations for these short regions. they may be caused by underestimating the length of enriched regions, possibly splitting one enriched region into several predicted regions, or they may represent spurious enriched results produced by the model. in each case there is the possibility that the occurrence of extremely short regions is caused either by an intrinsic shortcoming of the model or by artifacts introduced during the simulation process. since the simulation relies on tilemap to identify enriched and non-enriched probes it is inevitable that some probes will be misclassified. subsequently these probes may be included in the simulated data, causing short disruptions of enriched and non-enriched regions. a sufficiently sensitive model could detect these unintended changes between enriched and non-enriched states.

to investigate further which of these is the case, we first examine the number of enriched probes contained in the short regions found by the baum-welch model and by tilemap respectively. the model with baum-welch parameter estimates found  <dig> regions with less than  <dig> probes. these regions contain a total of  <dig> probes of which  <dig> are in enriched regions. while this indicates that the majority of short regions is due to underestimating the length of enriched regions, several spurious probe calls remain. tilemap produced  <dig> regions with less than  <dig> probes, containing a total of  <dig> probes, of which  <dig> are in enriched regions. this is strong evidence that almost all of these short regions are caused by underestimating the length of enriched regions, and is consistent with the above observation that tilemap systematically underestimates the length of enriched regions.

to investigate whether the spurious short regions produced by the baum-welch model are due to an intrinsic shortcoming of the model or are artifacts introduced by the simulation procedure, we turn to real data. here we focus on enriched regions containing only a single probe, which are most likely to be false positives. on dataset i the baum-welch model produced six of these extremely short regions. one of these probes is a true positive from an enriched region containing ten probes, i.e., the length of this region is underestimated by the baum-welch model. of the remaining five probes three are identical, leaving three unique probes to be investigated further. for each of these three probes, we determine its position in the real data and its distance from enriched regions identified by tilemap and by our model . two of the probes are found to be located close to enriched regions identified by tilemap  and all three probes are contained within enriched regions identified by our model . this suggests that these probes may have been misclassified by tilemap during the original analysis, leading to an overestimation of the number of false positives produced by the baum-welch model on dataset i.

 <dig> . <dig> application to chip-chip data
to investigate the performance of our model further, we apply it to the data of  <cit>  and compare the result to the original analysis. based on the results of the simulation study  we use the following procedure:

 <dig>  quantile normalise and log transform data;

 <dig>  calculate probe statistics );

 <dig>  obtain initial estimates ;

 <dig>  use  <dig> iterations of viterbi training to improve initial estimates;

 <dig>  use  <dig> iterations of baum-welch algorithm to obtain maximum likelihood estimates;

 <dig>  apply resulting model to data to identify enriched regions.

this results in the detection of  <dig> h3k27me <dig> regions covering  <dig>  mb of genomic sequence. of these enriched regions,  <dig>  are overlapping at least one annotated transcript. a total of  <dig> or about  <dig> % of all annotated genes are found to be enriched for h3k27me <dig>  while most of the enriched regions cover a single gene, some regions are found to contain up to seven genes ). enriched regions are predominantly longer than  <dig> kb with some extending over more than  <dig> kb ).

to assess whether there is a difference between regions of the genome that show h3k27me <dig> enrichment and the rest of the genome, we investigate the density of genes in the neighbourhood of genes that appear to be regulated by h3k27me <dig>  and compare this to the gene density in other regions of the genome. for this purpose we obtain the gene density for the  <dig> kb upstream and downstream of each gene as / <dig> kb. the resulting gene densities for genes with and without enriched regions are summarised in figure  <dig>  there are visible differences between the two distributions which we test for significance with a two sided kolmogorov-smirnov test; this results in an approximate p-value of  <dig> × 10- <dig>  the significance of this result is further confirmed by a resampling experiment: the smallest p-value obtained from a series of  <dig> resampled datasets is  <dig> × 10- <dig> 

 <dig> 
CONCLUSIONS
with the use of mles for all model parameters, our model clearly improves classification performance on simulated data compared to ad hoc estimates, and outperforms tilemap. while our model produced some short regions that appear to be false positives, they are readily explained as a result of the simulation process. comparison of results on simulated and real data suggests that tilemap produced a large number of false negatives in the original analysis used as the basis for the simulation. inevitably, these false negatives were selected as part of non-enriched regions during the simulation process. the fact that the model with baum-welch parameter estimates was able to identify these isolated enriched probes despite the non-enriched contexts where they appeared emphasises the high sensitivity of the model.

tilemap's apparent tendency to penalise false positives more than false negatives clearly contributes to its relatively low performance in our comparisons which are based on the assumption that both types of error are equally problematic. while this is the case for the application considered here, one may argue that false positives are indeed of greater concern in some cases. when this is the case, tilemap's trade-off between sensitivity and specificity may lead to better results. however, it should be noted that the relative weights given to false positives and false negatives by tilemap can vary substantially between datasets. the parameter estimation procedure used for our model on the other hand provides consistent performance at the chosen cut-off.

the model-fitting procedure derived from the results of the simulation study  provides a fast and reliable approach to parameter estimation. this method retains all the favourable properties of the baum-welch algorithm while utilising the reduced computing time provided by viterbi training. the use of mles ensures that model parameters are appropriate for the data. results from the simulation study show that estimating model parameters from the data improves the model's ability to recognise enriched regions of varying length and generally improves classification performance.

 <dig>  future work
the analysis of the h3k27me <dig> data  largely confirms the analysis of  <cit>  although there are some notable differences. most importantly, the h3k27me <dig> regions detected by our analysis are longer than the ones determined by tilemap . while zhang et al.  <cit>  found few regions longer than  <dig> kb, our analysis indicates that over 70% of enriched regions have a length of at least  <dig> kb, with the longest region spanning over  <dig> kb. accordingly we find more regions that extend over several genes ). this may have implications for conclusions about the spreading of h3k27me <dig> regions in arabidopsis.

at this stage, the biological significance of the observed difference in gene density in the neighbourhood of enriched and non-enriched genes is unclear. however, it indicates that the two groups of genes differ in a significant way. this suggests that the partition into enriched and non-enriched genes produced by our analysis is indeed meaningful.

the hidden markov model presented in this article uses homogeneous transition probabilities, assuming that all probes are spaced out equally along the genome. to satisfy this assumption at least approximately, we use a fixed cut-off of  <dig> bp to partition the sequence of probe statistics such that there are no large gaps between probes. this arbitrary cut-off could be avoided by using a continuous time hidden markov model.

 <dig> methods
 <dig>  baum-welch algorithm
the baum-welch algorithm  <cit>  used to estimate parameters for our model is outlined in section  <dig> .2; further details are given below. computing the likelihood of the long observation sequences produced by tiling arrays involves products of many small contributions. this typically results in likelihoods below machine precision. to avoid this effect computations are carried out in log-space, using the identity

  ln = ln + ln -ln). 

in the following we use ln∑ to denote summations which should be computed via equation . the sequence of probe statistics y is split into d observation sequences y  such that the distance between probes within each observation sequence is at most max_gap and the distance between the end points of different observation sequences is greater than max_gap.

the emission distribution of state si is given as

  fi=ΓΓ−1σiπνi2σiνi)νi+ <dig>  

for a given parameter set θ we can obtain new parameter estimates for transition probabilities by calculating

  ξkijd=ln 

  =αkid+ln+ln++βjd−ln. 

here αk and βk are known as forward and backward variables. for observation sequence d, d =  <dig>  ..., d, they are defined as

  α1id=ln⁡+ln⁡, 

  αjd=)+ln, 

where  <dig> ≤ i ≤ n,  <dig> ≤ j ≤ n,  <dig> ≤ k <kd and

  βkdid= <dig>  

  βkid=ln∑j=1n]+βjd), 

where  <dig> ≤ i ≤ n,  <dig> ≤ i ≤ n, k = kd -  <dig>  ...,  <dig>  note that ln  is given by ln⁡∑i=1nαkdd we then calculate

  γkid=ln⁡ 

  =αkid+βkid−ln⁡ 

  =ln⁡∑j=1nξkijd. 

combining the estimates from all observation sequences we obtain new parameter estimates for the transition probabilities:

  ln⁡=ln⁡∑d=1dln⁡∑k=1kd−1ξkijd−ln⁡∑d=1dln⁡∑k=1kd−1γkid, 

  ln⁡=ln⁡∑d=1dγ1id−ln⁡. 

calculations for the re-estimation of θ <dig> may involve negative values and cannot be carried out in log-space.

to obtain the required parameter estimates we first define ln⁡=γkid and then compute

  ukid=νi+1νi+ <dig>  

  μˆi=∑d=1d∑k=1kdτkidukidykd∑d=1d∑k=1kdτkidukid, 

  σˆi=∑d=1d∑k=1kdτkidukid2∑d=1d∑k=1kdτkid. 

there is no closed form estimate for νi. to obtain νˆi one has to find a solution to the equation

  = <dig> 

where ψ is the digamma function. standard root-finding techniques are employed to find a solution to .

 <dig>  viterbi training
viterbi training provides a faster alternative to the baum-welch algorithm. see section  <dig> . <dig> for a high level description of the algorithm. details of the parameter estimation procedure are given below. instead of calculating the conditional expectation of the complete data log likelihood, this algorithm first computes the most likely state sequence q given the observation sequence y and the current model θ. the sequence y is partitioned according to q, assigning each observation to the state that it most likely originated from. new estimates for θ <dig> are then obtained by calculating

  pˆi=1d|{d= <dig> ...,d:q1d=si}|, 

  aˆij=|{d= <dig> ...,d:qkd=si and qk+1d=sj}|∑d=1d. 

updates for μ and σ are obtained as in section  <dig> . <dig>  the degrees of freedom ν can be either fixed in advance or estimated from the data using equation  by setting τkid= <dig> if = and τkid= <dig> otherwise.

 <dig>  simulated data
in a first step following the original analysis by  <cit> , tilemap  <cit>  is used with the hmm option to define enriched and non-enriched probes. note that, although this classification of probes is not perfect, it can be assumed that most probes are assigned to the correct group. the length distribution of enriched and non-enriched regions detected by tilemap is used to determine the length distributions for the simulated data after removing all regions that contain less than  <dig> probes . data are generated by first determining the length of enriched and non-enriched regions from the empirical length distributions and then sampling data points from the respective tilemap generated clusters. following this procedure,  <dig> sequences with one to ten enriched regions in each sequence are generated. a second dataset is generated by applying the model described in section  <dig>  note that, although this procedure relies on the classifications produced by the respective models, the resampling procedure will place individual probe values in a new context of surrounding probes, which may lead to different probe calls in the analysis of the simulated data. prior to analysis all data are quantile normalised.

 <dig> availability
the parameter estimation methods used in this article are available as part of the r package tilehmm from the authors' webpage  and from cran. the simulated data used in this study is available from the authors' web page.

 <dig> authors' contributions
ph conducted the research and wrote the manuscript. db critically revised the manuscript. gs conceived the project. db and gs provided supervision to ph. all authors have read and approved the final manuscript.

supplementary material
additional file 1
false negative probe calls resulting from different models. for any given cut-off tilemap produces more false negatives than the baum-welch and viterbi trained models.

click here for file

 additional file 2
false positive probe calls resulting from different models. for any given cut-off tilemap produces fewer false positives than the baum-welch and viterbi trained models.

click here for file

 additional file 3
origin of isolated enriched probes in dataset i. the isolated enriched probes identified in dataset i by the baum-welch model originate from enriched regions identified by the baum-welch model in the real data. two out of three probes are located close to enriched regions identified by tilemap.

click here for file

 acknowledgements
ph is supported by an mqres scholarship from macquarie university and a top-up scholarship from csiro. the authors would like to thank michael buckley for his helpful suggestions.
