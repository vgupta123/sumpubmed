BACKGROUND
liquid-chromatography mass spectrometry  is a ubiquitous platform for proteomic  investigations  <cit> . ms signal from hundreds to millions of ions can be quantitatively compared across experimental conditions in a fairly robust and repeatable way  <cit> . analyte quantities are captured directly in ms signal , while analyte identities are often elucidated or confirmed using ms/ms  fragmentation spectra  <cit> .

confidently matching ms <dig> analyte signal between runs  is difficult with complex samples  <cit> , so a variety of approaches to circumvent this problem have been explored. multiple reaction monitoring  can be effective for monitoring a relatively small number of pre-selected analytes with a high degree of confidence, but it is unsuited to discovery-based experiments. ms/ms based approaches  are also popular alternatives. however, due to low ms/ms capture rates  and true positive database match rates   <cit> , ms/ms driven approaches lack sensitivity compared to ms1-based approaches. although a data independent acquisition  approach may address some of the sensitivity deficiencies of ms/ms for identification, dia does not of itself address difficulties in correspondence and quantitation. hence, despite the availability of alternative approaches, the ability to match ms <dig> signal across experimental conditions is still highly desired.

numerous efforts, large and small, have focused on using ms <dig> signal to compare analyte quantities. ideally, solutions would focus on one of the several complex individual steps for data processing  <cit> . however, most are released as end-to-end solutions . this makes comparison to other competing algorithms virtually impossible, and is at least partially responsible for the lack of critical evaluations in the literature  <cit> , since testing a subcomponent of a full software system requires re-implementing that portion of the algorithmic pipeline. our awareness of this problem has been accentuated as we have recently undertaken a survey in each of several of the modular subproblems of lc-ms quantification. when one must distinguish the algorithmic details of several methods, or worse, implement them in code, one becomes painfully aware of the ambiguity in the terms currently used in ms data processing descriptions.

the lack of standard terminology has stagnated lc-ms data processing progress. without consistent, clear terminology researchers have no handles for searching the literature, requiring onerous literature searches which fail to capture all relevant publications. besides lack of access to possibly improved results, this leads to massive duplication of effort and few cross-tool evaluations since researchers are unaware of related efforts. a well defined vocabulary and problem domain also encourage and aid new-comers to the field--which currently poses a significant learning curve  <cit> --improving solutions to difficult data processing challenges. it is also much easier to re-implement solutions when both the what and how of a process are clearly understood. hence, an increase in term clarity has immediate impact on reproducibility--a requirement firmly enforced for sample preparation and wet-lab processing protocols but which is almost completely unenforced for data processing descriptions  <cit> . the rigor and statistical biases of ms data processing algorithms--which usually must be evaluated qualitatively by the user due to a lack of quantitative comparison  <cit> --are likely to be miscommunicated in the literature without a clear nomenclature, leading to overstating the significance of and confidence in experimental results.

hupo-psi  <cit>  and iupac  <cit>  have presented controlled vocabularies  for mass spectrometry. however, the shotgun-pattern coverage of current cvs falls short of a systematic coverage of the full range of relevant data processing concepts. in the literature, these gaps are filled with arbitrary and/or overloaded ambiguous terms, precluding experimental reproducibly and comprehension. to motivate the need for precise terminology in hand, we begin by showing how terms from current official ontologies are too vague to explicitly map molecular entities to ms signals, and we illustrate the inconsistency and ambiguity of current colloquially used terms. the details of what terms the psi and iupac committees choose to fill the present need will be a long process. as a critical first step towards eventual standardization, we have crafted a systematic and consistent map of terms that span the range of the required granularity to describe ms data processing. by approaching the nomenclature task from the perspective of the molecular provenance of the ms signal, the proposed nomenclature provides a chain of succinct and unique terms allowing unique description of every data concept from the signal created by a charged molecule down through each of its constituent subsignals.

that a community standard controlled vocabulary is necessary is abundantly obvious from the literature. consider, for instance, the usage of two of the most common ms-omics data concept labels. these lists are by no means exhaustive in references or instances.

the term feature is used for:

• the whole 3-d signal produced by one molecule type , across one or more charge states  <cit> .

• any of the many possible summaries of that signal as a single 3-tuple   <cit> .

• any subsignal of that whole 3-d signal in m/z, rt, or intensity  <cit> .

the term peak is used for:

• the whole 3-d signal produced by one molecule type , across one or more charge states  <cit> .

• any of the many possible summaries of that signal as a single 3-tuple   <cit> .

• any subsignal of that whole 3-d signal in m/z, rt, or intensity  <cit> .

in both cases, although each publication makes a distinction between the two terms, the difference is lost across the literature. these lists could be much longer if it was possible to identify the specific subsignal referred to by the author, a task impossible with the generality of current terms.

it should be abundantly clear that these terms convey very little useful information--certainly insufficient information for reproducibility. even terms with consistent use, there is a general lack of scope. for example, evenly the seemingly descriptive term monoisotopic peak cannot convey exactly what level of data processing has been used on the signal. is it the 2-d signal of the most abundant isotope signal in the whole 3-d signal produced by one molecule type  <cit> , the integration of that signal  <cit> , the integration of the intensity of the whole 3-d signal into one 3-tuple  <cit> , or something else? all of these uses fit the commonly understood definition, but none are specific enough to readily discern from just the term.

these examples briefly illustrate the ubiquity of overloading . overloading treats a term as a variable, whose meaning must be defined in detail for the scope of each publication it appears in. an adequate definition takes significant thought, some descriptive text, and usually a descriptive image. there simply isn't ample space in each manuscript to define a custom set of terms for ms-omics data processing. this results in insufficient definitions for terms or no definitions at all.

for instance, in a published algorithmic review paper  <cit> , the author provides a definition list to allow for the use of mathematical algorithm descriptions. these include symbols for peak area, number of chromatograms, peak maximum, peak end, peaks detected in a mass channel, raw height of a peak, and peaks detected in a chromatogram. but what is a peak ? what is a chromatogram? as illustrated, these terms are not universally defined, and the author does not define them. subsequently, the algorithms in the paper are irreproducible unless the reader is able to correctly guess the definition of these terms intended by the author, as the result is dependent upon the term definitions.

reproducibility is, in fact, at the heart of the nomenclature problem. an algorithm description is rendered useless if the terms used within it are ambiguous or undefined. in a modular approach to pipeline algorithm creation and testing  <cit> , data processing methods prior to the pipeline module of interest have to be exactly describable with concise terms to assure properly formatted input. in evaluating algorithms, knowing the exact format of the input data informs interpretation of the algorithm. if the data format is known, as well as the process used to transform, segment, and/or reduce the original, one can know what biases are intrinsic in the algorithm, as well as immediately suggest improvements. for example, an algorithm that uses the whole 3-d signal from one charge state of a molecule has more information to distinguish differences in a correspondence task than one that has reduced that signal to a single 3-tuple representation. do current cvs capture the required degree of descriptive specificity?

why current cv terms are insufficient
iupac  <cit>  and hupo-psi  <cit>  are organizations that specialize in standardizing nomenclature. their significant and useful controlled vocabularies address all aspects of ms experimentation. to date, most of the thousands of terms in these overlapping controlled vocabularies are focused on wet lab protocol and instrumentation. although there are some terms relevant to data processing, the need for a systematic and consistent approach with more coverage is apparent.

current cv ms data processing terms are ambiguous and inconsistent
the hupo-psi-ms obo has more ms data processing terms than iupac. most are generic to the point of extreme ambiguity. for example, the term mass spectrum refers to any segment of data with m/z and abundance axes: "a plot of the relative abundance of a beam or other collection of ions as a function of the mass-to-charge ratio ." this could refer to a host of different data segmentations, and seems to be a synonym for another term, profile spectrum, defined as "a profile mass spectrum is created when data is recorded with ion current  on one axis and mass/charge ratio on another axis" .

an equally ambiguous complementary term is provided to refer to the time and abundance axes: chromatogram, defined as "the representation of detector response versus time." this definition is not scoped at all with respect to the molecular entities whose signals are being measured. the term applies equally at any scope. in other words, any plotted entity that shows rt vs intensity qualifies as a chromatogram. likewise, the term total ion current chromatogram, defined as the "chromatogram obtained by plotting the total ion current detected in each of a series of mass spectra recorded as a function of retention time" fails to imply any sort of scoping, and, worse, can correctly apply to any entity that qualifies as a chromatogram.

the term peak is defined in the psi cv as "a localized region of relatively large ion signal in a mass spectrum." as defined this term cannot discriminate between a host of distinct data concepts . among other associations, the term peak is used as a qualifier to describe a process officially named peak picking, when profile data is converted to centroid data. thus, a peak can be any signal region  that consists of one or more centroids, which means any size subset of any data in any projection could be called a peak. the term has absolutely no specificity. the term area peak picking has a very unclear definition: "spectral peak processing conducted on the acquired data to convert profile data to centroided data. the area defined by all raw data points that belong to the peak is reported." intuitively, one would assume that area peak picking has to have a different meaning than regular peak picking, yet the distinction is not evident from the definitions, given the only difference is the addition of "the area defined by all raw points that belong to the peak is reported." what is meant by "peak"? reported to/by what? examples such as this suggest that at least some of the current cvs came about through submission of terms from software user manuals rather than a curated, concentrated effort.

it is unclear what a centroid spectrum is. the definition states "processing of profile data to produce spectra that contains discrete peaks of zero width. often used to reduce the size of dataset." however, a spectrum, as defined, can only have two dimensions: m/z and intensity. since a spectrum cannot have an rt dimension, a centroid spectrum must be the same thing as a peak. do they instead mean a peak picked profile signal summed through rt? it is unclear.

finally, the inconsistency of the psi cv is apparent. for example, the term feature is used in at least  <dig> definitions, but it is never defined. it is used to refer to at least a few different concepts, including the idea of a program parameter , the isotopic envelope , a psi-cv mass spectrum , and probably others . another term, mass trace, is similarly used in several definitions yet never defined. the implied use, like feature, overlaps the definitions of peak and chromatogram, making for considerable ambiguity . in addition to these generic terms, the psi cv provides two specific data concepts: deisotoping and charge deconvolution. deisotoping is referred to as "the removal of isotope peaks to represent the fragment ion as one data point and is commonly done to reduce complexity. it is done in conjunction with the charge state deconvolution." the concept described is worthy of a definition, but the one provided can be improved upon. a fragment ion is not a data signal, but a molecular object. however, deisotoping is an operation on a data signal. additionally, this term should not be specific to msn fragment ions, but also applies to non-fragmented ms <dig> data, such as an ms <dig> isotopic envelope. our nomenclature expands this term to include the logical wider use. charge deconvolution is defined as "the determination of the mass of an ion based on the mass spectral peaks that represent multiple-charge ions." deconvolution is already a widely used signal processing term  for resolving two overlapping signals into their constituent parts). the psi definition redefines an already widely used term to mean something other than what it means in all other contexts. what's more, the definition focuses on mass determination, not signal manipulation. it should be replaced.

current cvs don't describe all necessary data concepts for ms data processing
crucial concepts for ms data processing are missing from the current cvs. most of these relate to more specific concepts at higher granularity than is currently offered by the psi and iupac cvs. a rigorous cv should allow for unique terms to describe signals from a peptide/lipid/metabolite down to individual data points. with the current nomenclatures, it is impossible to describe data processing algorithms' details using standard terms.

explicit mapping of molecular entities to the signal their detected ions produce is essential to achieve clarity at all scopes of granularity. the psi and iupac terms do not quite do this. for example, the best term to refer to a host of loosely related concepts is ion: "an atomic, molecular, or radical species with a non-zero net electric charge." ion is a proper and correct term that is general to science, and this is the widely used definition. however, this term applies to a charged item of any size, and cannot distinguish among the instances of interest  and the smaller molecular charged units that are detected in a mass spectrometer experiment. specific and unique terms would provide the needed descriptive granularity.

an incomplete cv impedes algorithm implementation/comparison
writing code is a mathematically precise activity. reproducibility requires the exact same equation, as it were, to be reproduced. reproducibility requires specificity and clarity. any ambiguity, overloading, or lack of detail makes the process impossible. frequently, it becomes necessary to code up a published algorithm. this could be because the algorithm was published independent of a software platform, or because you want to see the results of one particular module of a full-service program. as a case study, we recently attempted to code up one module of the maxquant algorithm  <cit>  in order to compare results of a feature detection algorithm. the following text is the pertinent portion, and demonstrates just how difficult it can be to parse through an algorithmic description without a good nomenclature, even in a well-written, top-tier-published manuscript:

...peaks are detected in a conventional two-dimensional  way by first searching for local maxima of the intensity as a function of m/z. the lower and upper limits of the m/z interval for a 2d peak...are then determined by moving from the maximum position to smaller and larger m/z values, until either the intensity has dropped to zero, or a local intensity minimum has been reached....the centroid position of a 2d peak is then determined...if the peak consists of only one raw data point, then the m/z value of that point is taken as the centroid position. if there are two raw data points in a peak, then the centroid position is defined as the average of the two raw m/z values, weighted by the raw intensities....the 2d peaks in adjacent ms spectra are assembled into 3d peak hills over the m/z-retention time plane. two peaks in neighboring scans are connected whenever their centroid m/z positions differ by less than  <dig> ppm. if for a given centroid in ms scan n no matching centroid is found in scan  in the ± <dig> ppm mass window, then it is checked if there is a centroid in scan  in the same mass window to continue the peak in time. we adjusted the window size to  <dig> ppm by visual inspection of many very low abundant peaks....a 3d peak is defined as the maximal chain of 2d peaks that results from connecting the centroids in time direction in the described way. at least two centroids have to be matched together to form a 3d peak, i.e. centroids that cannot be matched to centroids in the two previous or the two next scans are discarded....if a minimum is found whose value is 1/ <dig>  of the lower of the two local maxima the 3d peak is split into two at the minimum position...

with unclear terms, translating a manuscript into code is very difficult, and very unlikely to produce what the author intended.

ambiguity also affects practitioners who do not write code. while programmers could invest the required time to study the source code to fill in details missing from the published algorithmic description, practitioners are limited to published descriptions. in the process of writing an exhaustive lc-ms correspondence survey  <cit> , we recently parsed through over  <dig> manuscripts describing correspondence algorithms, attempting to describe distinctions between them. each manuscript had its own unique ill-defined vocabulary. it was very difficult to decipher algorithmic differences from the manuscript alone, even with much more time and effort than would be available to a standard user.

an incomplete cv impedes literature reviews
ambiguity also leads to publication of non-novel approaches due to a general unawareness of algorithmic detail, a phenomenon we noted in the above-mentioned correspondence review. this is a compounding problem, as it creates even more methods that must be compared against and even more papers to read. for example, there is probably an algorithmic difference between tracmass <dig>  <cit>  and the corresponding module of maxquant. however, neither algorithm is described specifically enough for the reader or reviewers to tell, as witnessed by the lack of a citation to maxquant. this is just one case of the ubiquitous unintentional plagiarism in the ms data processing literature. clear nomenclature makes a thorough literature search tractable, alleviating the burden of discerning differences in algorithms and minimizing the republication of non-novel approaches.

community awareness and an immediate, intermediate response is the correct way of addressing this need
all controlled vocabularies are works in progress. standards committees are best at crystallizing and refining accepted practice, but the onus to invent or select appropriate terminology lies foremost with the community itself. a good example of this was the creation of a standard spectrum exchange format. the mzxml format was created and published by a small group of researchers  <cit> . after several years of use the hupo psi mass spectrometry working group produced the mzml format which was able to draw upon the experience gained from use of the mzxml format. although a data format and not a cv, the success of mzml shows the good that can come of a manuscriptdriven approach. mzxml was originally published as a manuscript, the sole product of a small group of researchers who noticed a problem and forwarded a solution. this was the genesis of community traction that culminated in the mzml standard, a significant step forward for all mass spectrometry users. an official nomenclature culminates with iupac  <cit>  and hupo-psi  <cit>  standards but the community cannot realistically expect nomenclature to begin there.

at present, the controlled vocabularies simply do not have coverage in the terms related to data processing. because the problem is so extensive, and because opinions run strong in the domain of nomenclatures, this problem is best solved by drawing attention to shortcomings while providing a framework for unambiguous terms. a manuscript to draw attention to areas that can be improved are a viable means of correcting them, as demonstrated in other cvs  <cit> . as we have shown by enumerating collisions, inconsistencies, and gaps in current terms, no small group of experts can successfully bring about a cv independent of an active, involved community, particularly when data processing represents only a small subset of the larger experimental community. it is unfair to represent the data processing portion of the current psi cv as the calculated and careful end product of a long, focused deliberation. inspection of the terms makes it clear that this is not only a living, changing document, but  seems to be, at least in part, an uncoordinated amalgamation of terms from different software groups. we submit that fostering community discussion in a peer-reviewed venue is at least as valid as open, uncoordinated, and seemingly minimally curated submissions to a standard.

RESULTS
the psi cv data processing terms are scant, inconsistent, and ambiguous for describing ms data processing. we propose a system for generating terms that allows greater unambiguous coverage of currently addressed concepts in a consistent and intuitive manner, facilitating reproducibility, comprehension, and searchability of data processing algorithms. the motivation at the heart of our proposed nomenclature is to explicitly map causal molecular entities to the signals they produce. an overview of all terms is presented in figure  <dig> 

base terms
molecule - the composite signals across charges of a unit that accepts charge. for instance, a lipid in a lipidomics experiment or a peptide in a bottom-up proteomics experiment .

chargite - a unit that accepts charge at a particular charge state .

neutromer - a neutromer is the portion of a chargite with a common number of neutrons .

isoneutromer - isomers that differ only in the number of neutrons they possess .

isotopomer -isomers with identical isotopic composition but different structure .

neutroid - an instantaneous centroided neutromer hull .

rt dimensional terms
these terms provide us with specific descriptive ability for the rt dimensional component of the data concept, allowing us to distinguish between signals that continue through the rt dimension and signals that occupy only one rt.

trace - a trace indicates a signal that extends into the rt dimension . for example, when we combine chargite and trace, we get a chargite trace, which is the unique whole  accumulated  signal generated by one molecule / charge state combination consisting of one or more neutromer traces equally spaced m/z 1/z apart . this can  be further qualified as either a chargite distribution trace or a chargite distribution hull. likewise, a neutromer hull is the unique whole  signal generated by the accumulation of instances of a given molecule at a given charge state whose molecular formula contains the same isotopic composition . a neutromer trace is the same signal but subjected to a centroiding algorithm reducing it from a collection of instantaneous neutromer hulls to a collection of neutroids . a molecular distribution trace is the set of whole  chargite distribution traces generated by one molecule across multiple charge states .

integrated - an integrated object has been summed through the rt dimension. for example, if we take a neutromer trace and sum its constituent neutroids, we will end up with a single 3-tuple consisting of m/z, rt, and intensity that can accurately be called a neutromer . however, by calling it an integrated neutromer trace, we retain a unique description of the original data structure as well as the data reduction process used.

max - the qualifier max implies that this object is the instantaneous slice of a trace object at a the rt of greatest intensity. max objects look exactly like those with that are integrated, however, this qualifier indicates that we are looking at a slice of the data structure in time, not a summation or average of the data through time.

average - the data concepts described by the qualifier average are, in appearance, the same as those in integrated, however the process to generate them involves taking the average of the intensity of the composite points, not the sum.

instantaneous - the qualifier instantaneous implies that this object is a spectral slice of a trace object at a given rt. the instantaneous objects look exactly like those with that are integrated, however, this qualifier indicates that we are looking at a slice of the data structure at one scan time, not a summation or average of the data through time.

m/z terms
when reducing data from its initial state , the resultant data concept  should be amended to include the data processing technique . for example, a chargite distribution trace can become an instantaneous chargite distribution if the data reduction consists of selecting a random scan of the chargite distribution. in this way, data reduction products that can result from multiple operations are disambiguated.

hull - refers to the convex hull fitted over the subsignal data points for the given scope. for example, a neutromer hull is the convex hull over the isoneutromers for a given neutromer.

distribution - connotes a the collection of neutroids from a chargite or molecule in a particular rt and m/z scope. for example, an max chargite distribution refers to the centroided chargite hull trace at the apex intensity .

deisotoped - the qualifier deisotoped implies that a chargite's intensities have been summed through the m/z dimension . for clarity, deisotoped should be used in combination with an rt qualifier indicating the method used for reducing through the rt dimension, such as integrated.

reduced - the qualifier reduced implies that the object has been combined through reducing charge states to the lowest common charge state. for instance, a reduced molecular distribution is the set of the composition of all chargite distributions in the molecular distribution .

clearer than colloquial terms
our suggested vocabulary alleviates much of the ambiguity in the current naming schemes employed in the literature. the following examples illustrate how the proposed vocabulary untangles the currently obfuscated terms in use.

chargite trace describes a concept for which the following terms have all been used: an eluting isotopic distribution  <cit> , a chromatogram  <cit> , an isotope series  <cit> , an isotope pattern  <cit> , an isotope-resolved mass spectrum  <cit> , an ion series  <cit>  and an isotopic cluster  <cit> . none of these terms differentiate between the concepts we refer to as chargite trace, instantaneous chargite hull, max chargite distribution, etc.

neutromer traces have been referred to as eluting isotopes  <cit> , single ion chromatograms  <cit> , peaks  <cit> , mass spectra  <cit> , and peak hills  <cit> . each of these terms are unclear. the problem with the term chromatogram is that is does not specifically refer to the elution profile of a single isotope. for example, an extracted ion chromatogram is an m/z slice of data that can extend across an entire run's rt. any term that uses peak is bound to be confusing due to the overuse of the term. like chromatogram, a mass spectrum can technically stretch across an entire m/z range and therefore does not specifically describe the m/z window of a specific molecule.

integrated chargite distribution trace has been called an isotope pattern  <cit> . however, many other concepts can accurately be called isotope patterns, such as a max isotopic envelope or an averaged isotopic envelope.

using this nomenclature, it is much simpler to clearly and unambiguously describe an ms data processing algorithm. the following text is the translation of the above-quoted maxquant text translated into the proposed nomenclature.

overlapping and/or contiguous neutromer hull traces are deconvolved by bisecting all contiguous neutromer hull traces with a local maximum bordered by local minima. each neutromer hull is centroided by taking the weighted average of the m/z values of its isoneutromers. neutromer traces are constructed from neutroids by the following method: for each scan, each neutroid within  <dig> ppm of an isotope trace from the previous scan or penultimate scan is aggregated to the closest  neutromer trace. all other neutroids are considered new neutromer traces for future scans. any neutromer traces with only one neutroid after all scans are included are culled. a postprocessing mechanism to address erroneously appended neutromer traces splits the neutromer trace anywhere a neutroid is found with intensity less than or equal to 1/ <dig>  of the lesser intensity of two surrounding local maxima.

not only does this text more readily reduce to code, it is easier to understand and takes up about half the text of the original. no term is used to mean more than one specific concept. the terms have a one to one mapping to the concept they refer to. this clarity readily expands to algorithmic descriptions across the subproblems of ms data processing, such as lc-ms correspondence.

expected objections
having discussed this nomenclature with many of our colleagues, we anticipate some objections and will address the most common here.

this nomenclature competes with psi/iupac
we are not advocating for a replacement of either iupac or psi controlled vocabularies, rather arguing that the subset of terms relevant to data processing have insufficient coverage and are ambiguous where defined . we are arguing that those terms relevant to data process discussed here ought to be replaced, and those missing ought to be added.

why don't you submit these to psi?
we are planning on doing so. however, this manuscript is not an attempt to change psi. standardizations do not drive the community, the community drives standardizations. there is a current dearth of appropriate terms to describe ms data processing. we have provided solid evidence that this is a problem, and we have proposed a nomenclature that solves that problem. we do not have a stake in psi and cannot control their nomenclature. however this nomenclature can be used immediately regardless of community response.

you have no data format
the community does not recognize a controlled vocabulary and an data format  as the same thing, as demonstrated by the fact that psi has data formats and a cv, and they are separate products. the first step is establishing terms that describe these concepts. producing a data format that represents these data concepts is a different problem that will have to be addressed in the future. having a data format before establishing that this is a problem, let alone before coalescing on an industry-wide acceptable solution, is a mistake. it would require that any software tools coded between now and then to be redone. instead, we focus on the first problem, which is establishing that there is a problem. we propose a nomenclature, but expect and look forward to many constructive criticisms to improve upon it. meanwhile, this manuscript serves as a cite-able lexicon for anyone who has the need to describe these concepts  yet has no available way of doing so within page limits. the comparison of analyte ms <dig> signal is central to many proteomic  workflows. standard vocabularies for mass spectrometry exist and provide good coverage for most experimental concepts, however their terms for data concepts and algorithms are either ambiguous or missing. without a standard, unambiguous nomenclature, literature searches, algorithm reproducibility, and algorithm evaluation for ms-omics data processing are nearly impossible. we show how terms from current official ontologies are too vague or ambiguous to explicitly map molecular entities to ms signals, and we illustrate the inconsistency and ambiguity of current colloquially used terms. we propose a set of terms for ms <dig> data processing which consists of a limited number of base terms along with qualifier terms allowing a vast number of ms <dig> data concepts to be succinctly, precisely, and intuitively described. we suggest this nomenclature as a beginning to, not the culmination of, the standardization process.

you should be improving mzquantml or mzidentml instead of doing this
the mzquantml and mzidentml formats are not cvs. they are data formats .

CONCLUSIONS
the ever-increasing number of ms-omics experiments drives a thriving ms-omics data processing algorithms field. however, the lack of an unambiguous vocabulary for ms-omics data concepts has created serious challenges for reproducibility and evaluation of data processing algorithms.

in this paper, we have highlighted the ambiguity of current vocabulary for ms-omics data processing. we propose an unambiguous vocabulary that spans the required granularity of md data processing concepts together with a visual lexicon for the proposed terms. by adopting these terms, authors can facilitate reproduction of their algorithms succinctly by providing a crystal-clear set of meanings for terms they use, vastly improving the reproducibility of their work.

competing interests and declarations
the authors declare that they have no competing interests. the publication costs for this article were funded by the university of montana office of research and sponsored programs.

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2015: selected articles from the 11th annual biotechnology and bioinformatics symposium : bioinformatics. the full contents of the supplement are available online at http://www.biomedcentral.com/bmcbioinformatics/supplements/16/s <dig> 

authors' contributions
rs, rmt and jtp all contributed to writing this manuscript.
