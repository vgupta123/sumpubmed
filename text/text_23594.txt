BACKGROUND
the past few years have seen an impressive growth in the amount of research dedicated to biomedical text mining, . the field that originally focused on medical text  <cit>  has expanded since the onset of the genomic era into the biological domain. research in the area includes work on information extraction from the biomedical literature  <cit> , as well as on information retrieval and text categorization  <cit> . the efforts on information extraction mainly concentrate on identifying bio-entities  and the relationships among them, while current efforts on information retrieval, with a few exceptions, aim at identifying documents for specific database curation tasks and categorization of papers into various ontological types  <cit> . we believe that an important first step towards more accurate information extraction and retrieval, lies in the ability to identify and characterize text that satisfies certain kinds of information needs. one goal of the work presented here is to identify properties of scientific text that have sufficient generality to transcend the confines of a narrow subject area, while supporting practical mining of text for factual information.

one of the most challenging aspects of biological text-mining is deciding how to use scarce resources to annotate text as a training corpus for machine learning. there are two objectives to be considered in this context. on the one hand, the corpus should be widely useful to the biomedical data-mining research community. on the other hand, it should lead to the development of practical and useful resources. regarding the first objective, cohen et al.  <cit>  have studied the most prominent corpora designed to promote biomedical text mining. they suggest four main characteristics of a corpus that make it user-friendly and promote a high level of use: documentation, balanced representation, recoverability, and data on inter-annotator agreement. documentation in the form of published annotation guidelines is important  <cit> . one of the purposes of this paper is to address this need. recoverability is the requirement that the original text remains available to users of the corpus. this ensures that no aspect of the original data is lost. balanced representation requires coverage of a broad range of text types that are encountered in practice. cohen et al. also suggest that abstracts may no longer be acceptable as the sole source of data. accordingly, in the work introduced here, the majority of our data is  sampled from the different sections of full-text journal articles . with respect to inter-annotator agreement, for the full corpus, all sentences are being annotated by at least three independent annotators, and we study and monitor the agreement among them. the preliminary data we report here was annotated by twelve independent individuals, and agreement among them is the topic of much of this paper.

the second objective is to make a difference on the practical side. our aim is to create a training corpus for automated classifiers, ultimately performing text-mining tasks that could support and expedite biomedical research. the basic task that we are aiming to address is the finding of reliable information. the fact that a gene is mentioned in the text and the text states, for example, that the gene is regulated by another gene, does not necessarily imply that the information is reliable or useful. krauthammer et al.  <cit>  suggested a critical examination of literature contents in molecular biology, and recent work by light et al.  <cit>  also examined the category of speculations versus definite statements made in the literature. as an example, one may compare the following statements taken from actual research reports:

"we suppose that an increased li in breast tissues of this group of patients may help explain the association between bc and thyroid autoimmunity." and

"hyphae-specific genes, hwp <dig>  rbt <dig> and ece <dig>  were activated in the elongated filaments caused by the cdc28p depletion."

the first sentence only speculates about a possible explanation for association between breast cancer and thyroid autoimmunity, but provides no evidence that it is true. the second statement, on the other hand, makes a definite assertion about the activation of genes and the relations among them. moreover, the use of the past tense form "were" provides indication that this was an actual finding in the study being reported. one of the important observations demonstrated by this example is that no deep knowledge of the fields involved, or even of the jargon used in these fields is required to draw such conclusions. following this line of reasoning, we devised criteria for characterizing statements made in the literature along several dimensions, based on certain types of meta-knowledge. these dimensions, which we introduce and describe in the methods section, include focus , polarity , level of certainty, strength of evidence, and direction/trend . the ultimate utility of a text-region, as a source for a certain type of scientific knowledge, can be evaluated based on its "coordinates" along these dimensions.

prior work on annotation of scientific text  focused on the partition of text into zones, according to the type of discourse and the components of scientific argumentation . teufel et al.  <cit>  designed an annotation scheme for text involving seven rhetorical roles, such as: background, basis, aim, own, etc., borrowed from rhetorical structure theory  <cit> . a more extensive hierarchical tree-structured scheme, was developed by langer et al.  <cit> . it consists of higher level nodes of general text types, such as background, and evidence, and sixteen more specific leaves such as research topic, data, results, and conclusions. mcknight and srinivasan  <cit>  studied structural categories: introduction, method, result, and conclusion, which commonly appear in scientific text. all of these approaches seek to categorize scientific text, in order to improve the understanding of content, with possible application to text-mining. however, these methods differ from our approach, as they all strongly rely on predefined structural roles or types of discourse.

among previous studies, perhaps closest to our intent is that of mizuta and collier  <cit>  on zone analysis, where zones are based on types of discourse. their work is based on, but significantly extends, the original framework proposed by teufel et al.  <cit> . they propose seven top level classes: background, problem setting, outline, textual, own, connection, and difference. the own category is divided into five subclasses: method, result, insight, implication, and else . annotation is typically assigned to a sentence or a group of sentences, but for a specified list of clause and phrase types a lower-level of annotation has proven necessary. due to language-complexities two levels of nested annotations are also supported.

our present study, much like the work of mizuta and collier, is motivated by the need to identify and characterize locations in published papers where reliable scientific facts can be found. however, our work differs from theirs in two main aspects, namely, the complexity and the specificity of the annotation task addressed. first, the annotation scheme suggested by mizuta and collier is quite complex; we believe that such an intricate scheme makes both annotation and utilization of the corpus more difficult, and requires more effort to yield practically satisfying results. second, their annotation scheme, , assumes that specific zones or types of discourse bear certain types of information. this annotation scheme ultimately limits the type of discourse and the areas in the document which can be identified, under the assumption that specific types of discourse or zones typically bear more relevant information than others. in contrast, we define a set of five general dimensions, along which each sentence or sentence fragment within the text is to be characterized â€“ regardless of its semantic contents or zone.

we ultimately plan to develop a battery of machine-learning methods to carry out such annotation. to enable that, a set of guidelines must be crafted and a corpus of training data produced of sufficient quality to support the proposed machine learning. our belief is that the greatest return will accrue from a relatively simple approach which envisions no sophisticated language understanding or discourse analysis. rather, we see the task as decomposition of meta-information about text into multiple relatively independent dimensions along which a human can discern a level of organization or information, roughly at the level of sentiment analysis  <cit> . we believe this kind of data will support a sufficiently high level of machine-learning to pay practical dividends.

the rest of the paper describes aspects of the guidelines that we have developed to characterize text fragments along the multiple dimensions mentioned above. the annotation guidelines themselves are the subject of the methods section, presented at the end of the paper. the results section reports the results from a test we conducted to evaluate these guidelines by measuring inter-annotator agreement within two groups of annotators. it is followed by a discussion and conclusion.

RESULTS
annotation task
we have developed the guidelines over a period of more than a year, through multiple iterations of testing and revisions. once the guidelines reached their current form,  we designed a formal preliminary test, before proceeding to the full-corpus annotation . ten research articles were randomly chosen from those published in  <dig>  and from these articles  <dig> sentences were chosen at random from the different sections of the papers, at a rate of approximately ten sentences per paper. these  <dig> sentences form the experimental corpus for a small annotation test.

annotator characteristics
the three of us each annotated the test corpus independently, forming one group of annotators. nine other independent individuals with basic scientific training  also annotated the corpus, forming the second group of annotators. while we as authors have had extensive experience with the guidelines, the other nine annotators were simply given the guidelines along with appendices containing annotated examples, and were asked to read them and apply the guidelines to the corpus.

we have analyzed the resulting annotations in several ways. table  <dig> summarizes the distribution of the tags assigned to the different fragments by the annotators. its first  <dig> rows show the distribution of the number of fragments into which the sentences were broken by the annotators. because there are some marked differences in the number of fragments produced per sentence, for subsequent rows we normalized the counted data, dividing counts by the number of fragments from which these counts were produced. this removes the number of fragments from affecting the comparison of the annotations along the other dimensions. there are still significant differences in annotator performance, as reflected by table  <dig> 

inter-annotator agreement
the variation in the data is not surprising. to examine the reliability of the annotations we directly examine agreement levels among annotators in several different ways, as described below. we chose not to use the familiar kappa statistic for two reasons. first, kappa values are not comparable across data sets and judgment tasks  <cit> . second, it is unclear what model for random agreement among judges is most reasonable for our task. for instance, a uniform distribution for random fragmentation will give an almost zero random agreement between judges and reduce kappa to percentage agreement. we thus directly analyzed the agreement among annotators along the five dimensions of annotation that we have defined above .

while the annotation agreement for each sentence was calculated along each of these five dimensions, in order to accurately assess agreement between two annotators on a given sentence, the two had to first produce the same number of fragments for that sentence. then they were counted as agreeing on a given dimension for that sentence if they assigned the same list of tags in the same order. thus, the fragments that the two annotators were working with did not have to be identical, but we assumed that if they assigned the same tags in the same order they were detecting the same information in the fragments. this assumption, while not formally validated, was typically satisfied in our many annotation experiments using the changing guidelines throughout the year.

there are two possible ways to handle a comparison of annotations that do not contain the same number of fragments. first, one can score such a comparison as a zero match along all five coordinates. while this is a harsh standard, it has the advantage of not over rating agreement between annotators because a disagreement in number of fragments is an indicator of some level of disagreement regarding the relevant characteristics of the text being annotated. as a second alternative, one can simply exclude from the analysis examples where the fragment numbers disagree. one would do this because in cases where annotators disagree on the number of fragments they may still substantially agree on the characteristics of the text. further, the annotations may be equally valuable for the eventual goal of learning how to annotate text. we have followed this more optimistic approach for the data reported in table  <dig>  but adhered to the harsh standard for the remainder of the data, reported in tables  <dig>   <dig>   <dig> 

the data in table  <dig> shows the pairwise inter-annotator agreement among the authors. they clearly demonstrate a high level of agreement on the number of fragments. we note that the level of disagreement on fragmentation number is important for the interpretation of the level of agreement on the five dimensions. if there is disagreement on the number of fragments, that sentence is excluded from the remainder of the analysis in table  <dig>  just for the data in table  <dig> we have used this method of dealing with disagreement in fragment number because we believe it gives a more accurate representation of the true level of agreement between annotators.

next we compared the performance of the author group  with that of the group of nine other annotators . that is, whenever there is an agreed upon majority annotation among the three authors, we compared that annotation with the majority annotation among the oth1â€“ <dig> group. the results shown in table  <dig> confirm the conclusion already evident from table  <dig>  that annotations are reproducible at a high level, even among annotators who have had only a brief experience with the guidelines.

to better understand the performance of individual annotators we performed two additional comparisons, both based on the same five dimensions as explained earlier.

a) to check how well the untrained annotators, oth1â€“ <dig>  performed with respect to the trained annotators, aut1â€“ <dig>  we scored the annotations from oth1â€“ <dig> based on their agreement with those of auth1â€“ <dig>  that is, for each sentence for which a fragment annotation produced by othx exactly matched the annotation produced by any one of us , othx received  <dig> point. a mismatch was assigned  <dig> points. this way, each annotator could score between  <dig> and  <dig> for each sentence, depending on the level of agreement with any one of us. we averaged the results over all  <dig> sentences. the results of this comparison are shown in table  <dig> 

b) we next compared the performance of each annotator against the majority obtained over all of aut1â€“ <dig> and oth1â€“ <dig> together. each annotator scored a point for an annotation along each dimension if it agreed with the majority. thus again the score ranged from  <dig> to  <dig> points per sentence. we averaged again over the  <dig> sentences. the results are shown in table  <dig> 

clearly there is a significant difference in performance among different annotators. table  <dig> shows that five of the other  annotators  scored at a level of approximately  <dig>  and above. the results in table  <dig> allow a more direct comparison of the performance of aut1â€“ <dig> and oth1â€“ <dig>  authors show the highest agreement with the majority, which is expected given their high level of training in the task. these results also show that all but four of the annotators oth1â€“ <dig> perform almost as well as the authors on the annotation task. based on these results it is expected that a simple use of the guidelines, even without additional instruction, can lead to consistent annotation, as measured by inter-annotator agreement, in about 50% of cases. we of course don't view this as the ultimate desired performance, and additional training is provided to ensure a consistently high level of annotation agreement throughout the data set.

discussion
it is challenging to find a non-trivial and useful annotation task that a human can perform and a machine can learn from human-generated data. we believe that we have identified five dimensions of human judgment tasks that are machine-learnable and do have practical implications. our belief that the tasks are machine-learnable is based on the relatively high level of agreement among untrained annotators, , who used only the guidelines. annotation tasks can vary significantly in terms of difficulty: for example, a survey by saracevic  <cit>  indicated that agreement among human judges varied between 40% and 75% for different tasks. our results of inter-annotator agreement of 70â€“80%  indicate that our annotation problem is relatively easy for the human annotators, which we expect to translate into learnability by machine learning algorithms. additional support for learnability comes from the observation that clues as to ratings on any of the five dimensions often come in the form of specific words or phrases that occur in the annotated text. this is similar to the sentiment analysis task  <cit>  where machine learning has given good results  <cit> . that said, sentiment analysis work  <cit>  also suggests that learning performance depends on topic, domain, and temporality. thus, conclusions from our work, in which we use biomedical text as a training set, will likely be limited to the sublanguage of biomedicine and not equally applicable to scientific text as a whole.

the variability in annotation agreement along the different dimensions, as summarized in tables  <dig> and  <dig>  suggests that categorization along these dimensions is not all of the same difficulty. we were surprised to find that rating of evidence is among the most challenging tasks. while identifying citations  is an almost mechanical task, there are many subtle ways in which words are used to indicate that a result is a consequence of the new research being reported in a paper . similarly, there are many ways to support a statement by eluding to previous work with no specific citation . analogous remarks apply to the rating of certainty. for the distinction between methodology versus science or general subject matter, we expect a limited set of clue words and phrases to be useful. we expect that distinction between general subject matter versus science or methodology may be the most challenging of all the tasks because of the open-ended nature of general subject matter. on a positive note, general subject matter is less common in scientific research articles. while we may therefore expect fewer training examples of general focus, it may make success on this one sub-task less critical to our overall project.

some insight can be gained from our data about training of annotators. obviously, a good understanding of the english language and experience in reading scientific literature are important for performing the annotation task as prescribed in our guidelines. it is surprising that even with these skills, , some annotators still performed poorly, as illustrated in tables  <dig> and  <dig>  these results strongly indicate that careful quality control is essential, and that poor performance calls for feedback, instruction, and retesting with either resolution of the difficulty or discontinuance from the task. to support such measures, we plan to first train the judges, and then have each sentence annotated independently by three different judges, as well as having different triples of judges assigned to different sentences.

CONCLUSIONS
we have presented guidelines for the annotation of text that have sufficient generality to transcend the confines of a narrow subject area, while supporting practical mining of text for factual information. we have identified five qualitative dimensions that we believe are useful in this respect: focus, polarity, certainty, evidence, and directionality. we define these dimensions and describe the guidelines we have developed for annotating text with regard to them. our initial work investigating the reliability of such annotations supports the feasibility of the approach.

our ultimate goal is the annotation of  <dig>  sentences, taken from diverse sources in the biomedical research literature. we believe that with triplicate annotations this will allow the training of machine learning algorithms to perform the annotation task at a useful level of accuracy. both the annotation and the training of machine learning algorithms are currently ongoing. should they prove successful, we foresee several areas of application. first, annotation of a large volume of literature and characterization of the literature along the dimensions proposed. this may shed light on the composition of different parts of research papers and even define the characteristics of different genres of biomedical research literature. another potential application is to combine these annotations with semantic analysis of text to produce a text-mining tool. for example, our annotations could guide entity recognition applied to subject-verb-object triples towards statements that are likely to be highly reliable, as they are supported by evidence or stated in the affirmative with high confidence. such techniques might also prove helpful to a question answering system and even to a document retrieval system. the scientific literature is vast and there is a wide variety of potential reasons for accessing it. one investigator may wish to obtain validated facts about a particular gene, thus looking for statements of high certainty about it. a second investigator may desire to examine contradicting statements regarding the expression of a gene; in this case statements mentioning the same gene but with opposite polarity and/or opposite direction/trend are important. a third investigator may wish to examine uncertain hypotheses regarding this same gene, which would involve looking for statements with a low certainty level. such statements may stimulate his thinking and lead him in new research directions. in fact we suggest that contradictions and speculations in the literature are likely to prove a fruitful source of new hypotheses. all of this is territory yet to be explored.

