BACKGROUND
with recently-developed methods for massively parallel dna sequencing it is now practical for individual labs to perform whole-genome or transcriptome sequencing of a wide variety of organisms, and to perform metagenomic sequencing of environmental samples. additionally, these new sequencing technologies are becoming widely used for reduced representation sequencing and genotyping of non-model organisms  <cit> , including those with no available genome sequence.

each of these applications involves de novo assembly from very large numbers of short reads. despite progress in recent years, de novo assembly remains a computationally challenging task. the current research for assembly with short reads is focused on de bruijn graph methods  <cit> . the nodes in a de bruijn graph are the k-mers of a pre-specified length k that are contained within the sequencing reads. two k-mers are connected in the graph if they are adjacent in at least one sequencing read. although de bruijn graphs provide a nice conceptual framework that cuts down on computation time, the size of the graph can be very large, typically including billions of k-mers for vertebrate-sized genomes.

in order to deal with the computational challenges of working with such large data sets, a number of methods have been proposed for storing k-mers efficiently. most de bruijn graph assemblers store k-mers using  <dig> bits to encode each nucleotide, so that each k-mer  takes bytes. the k-mers are then stored in a hash table, usually with some associated information such as coverage and neighborhood information in the de bruijn graph. the exact memory usage depends on the hash table used; for example, the assembly software abyss  <cit>  uses the google sparsehash library, which has minimal memory overhead http://code.google.com/p/google-sparsehash/. additionally, abyss can share the memory load across multiple machines, splitting up the hash table so that each potential k-mer is assigned to a unique machine, although this setup has more communication overhead across machines and requires additional work by the end user. a recently-developed program named jellyfish is specifically designed for k-mer counting   <cit> . it uses a "quotienting" technique  <cit>  to reduce the space needed to store each k-mer in a hash table, and it achieves much lower memory usage than other available methods. additionally,  <cit>  show how to compress both the de bruijn graph and the k-mer coverage counts to nearly the optimal. however this compression is done after all the k-mers have been counted, in contrast to jellyfish.

a complementary strategy for reducing memory usage is based on the observation that in current data sets, a large fraction of the observed k-mers may arise from sequencing errors. most of these occur uniquely in the data, and hence they greatly increase the memory requirements of de novo assembly without adding much information. for this reason, it is frequently helpful to either discard unique k-mers prior to building the graph, or to attempt to correct them if they are similar to other, much more abundant, k-mers  <cit> . for example, the team that sequenced the giant panda genome obtained 56-fold coverage of the  <dig>  gb genome on the illumina sequencing platform  <cit> . using a supercomputer with  <dig> gb of ram, the authors counted a total of  <dig>  billion 27-mers. after removing or correcting low-coverage k-mers, they eliminated 68% of the observed k-mers, reducing the total number to just  <dig>  billion. their genome assembly was based on this reduced set.

more generally, while the number of true k-mers in a genome sequence is at most the genome length, g , the number of spurious k-mers grows almost linearly with sequencing depth. to illustrate this, if we assume a uniform error rate α per nucleotide, then the expected number of spurious k-mers at sequence coverage c is , where l is the length of sequence reads.  then for example, at an error rate of 1% per base, read length of  <dig> bp, and k =  <dig>  the number of spurious k-mers would exceed the genome length g at just  <dig> -fold coverage.

however, even the seemingly simple goal of eliminating singleton, or low coverage, k-mers is computationally demanding in practice, since we do not know a priori which k-mers have low coverage. an obvious approach would be to simply load all observed k-mers into a hash table while counting the number of occurrences of each. but this task alone can easily overwhelm the memory of standard high performance machines.

the goal then is to implement a method for identifying unique k-mers , that makes highly efficient use of memory while providing efficient storage of k-mers with fast insertion and query times. the problem of counting the number of distinct k-mers is much easier if we are willing to settle for an approximate answer that works with high probability  <cit> .

here, we describe an approach to solving this problem by storing an implicit and highly compact representation of the observed k-mers, known as a bloom filter. a reference implementation, implemented in a c++ program called bfcounter, is freely available. we show empirical results of applying this method to published sequencing data. we also discuss possible extensions and further applications of the method.

RESULTS
the bloom filter
the bloom filter is a probabilistic data structure supporting dynamic set membership queries with false positives  <cit> . it allows us to identify in an extremely compact way all k-mers that are present more than once in a data set, while allowing a low rate of false positives. bloom filters have been used widely in computing applications, but to date rarely in bioinformatics, but see  <cit> .

the essential idea is illustrated in figure  <dig>  the bloom filter is a bit array b, initialized to be  <dig> at every position. we also define a set of d hash functions, h <dig>  ..., hd, where each hash function maps a given k-mer x to a location in b.

in order to insert a k-mer x into the bloom filter, we set all of the d corresponding locations in b to be 1; that is, we set b =  <dig> for i =  <dig>  ..., d. then, to determine whether a k-mer y has been inserted, we simply check whether each of the corresponding hash positions is 1: i.e., whether b are all set to  <dig> for i =  <dig>  ..., d. if this is the case, then we infer that y has probably been seen before. by construction, this procedure correctly identifies every k-mer that is present more than once in the data; however, the cost of very efficient memory usage is that we accept a low rate of false positives in which we infer that y has been seen previously, but in fact it has not.

the bloom filter has a tradeoff between memory usage  and the false positive rate. when storing n k-mers in a bloom filter of m bits, and using d hash functions, the false positive rate is approximately . given n and m, the optimal number of hash functions that minimizes the false positive ratio is  <cit> . in practice we may have a rough idea in advance about n, the number of k-mers, and we can select m as a fixed multiple of n. for example using m =  <dig> · n , and d =  <dig> gives a false positive ratio of  <dig> %. many variations and improvements have been proposed for bloom filters  <cit> ; or see  <cit>  for a survey.

storing and counting k-mers using the bloom filter
to count all non-unique k-mers we use a bloom filter b and a simple hash table t to store k-mers. the bloom filter keeps track of k-mers we have encountered so far and acts as a "staging area", while the hash table stores all the k-mers seen at least twice so far. the idea is to use the memory-efficient bloom filter to store implicitly all k-mers seen so far, while only inserting non-unique k-mers into the hash table.

initially both the bloom filter and the hash table are empty. all k-mers are generated sequentially from the sequencing reads. note that in most applications we do not need to distinguish between a k-mer and its reverse complement sequence. thus, as we read in each k-mer we also consider the reverse complement of that k-mer and then work with whichever of the two versions is lexicographically smaller .

for each k-mer, x, we check if x is in the bloom filter b. if it is not in b then we update the appropriate bits in b to indicate that it has now been observed. if x is in b, then we check if it is in t, and if not, we add it to t.

this scheme guarantees that all k-mers with a coverage of  <dig> or more are inserted into t. however a small proportion of unique k-mers will be inserted into t due to false positive queries to b. after the first pass through the sequence data, one can re-iterate over the sequence data to obtain exact counts of the k-mers in t and then simply delete all unique k-mers. the time spent on the second round is at most 50% of the total time, and tends to be less since hash table lookups are generally faster than insertions. a detailed pseudocode is given in figure  <dig> 

it is also possible to obtain approximate k-mer counts by iterating only once over the sequence reads. in this case we record a coverage count of  <dig> when first inserting a k-mer into the hash table t, and subsequently increment the counter for each additional observation of this k-mer. this means that the coverage counts for some k-mers are  <dig> higher than the true value, and some k-mers in t are in fact false positives .

higher coverage cutoffs
for some applications a higher coverage cutoff may be required to either filter out sequencing errors or to simply extract sequences of interest. the algorithm can be extended to use counting bloom filters, where each bit in the bit array is now replaced with a counter that uses only a small number of bits. if the desired minimum coverage is c we use an array of m ⌈log2⊥-bit counters. the counting bloom filter was introduced by  <cit>  to allow for deletions, but here we use the counts directly.

to check if a k-mer should be inserted into the hash table t we look to see if all of b are equal to c -  <dig>  otherwise we insert it into the bloom filter. when inserting a k-mer x, we set  

for i =  <dig>  ..., d. note that for a k-mer x, min{b|i =  <dig>  ..., d} gives an upper bound on the number of occurrences of x so far. of course the basic version simply corresponds to the case of c =  <dig> 

parallelizability
the algorithm is presented above as a standard single processor program and our current implementation is not multi-threaded.

nonetheless it would be possible to speed up the operations using multiple cores with lock-free data structures. this would require a non-blocking implementation of the hash table  <cit>  and a modification to the bloom filter. the bit array in the bloom filter is implemented as an array of word-sized integers, usually  <dig> or  <dig> bits. to avoid accidental collisions where two bit locations in the same word are updated, one can use "compare-and-swap"  operations on words to ensure atomic updates of each bit independently.

since the role of the bloom filter is to keep track of k-mers seen previously, this scheme could plausibly fail in the unlikely event that two occurrences of the same k-mer are inserted into the bloom filter simultaneously by different threads. in this case the two threads would both query the bloom filter for a k-mer, x, and after both receive a negative answer the two threads would insert x simultaneously. if x occurs exactly twice in the data set then we would fail to record it in the hash table and get a false negative, although this type of false negative seems unlikely to be a serious concern in practice. however this can be fixed by extending the bloom filter data structure to return the number of bits set to  <dig> when querying, and the number of bits changed from  <dig> to  <dig> when inserting. this makes insertion atomic, each thread can then determine when inserting a new k-mer into the bloom filter whether any other threads were inserting the same k-mer simultaneously by comparing the number of bits changed from  <dig> to  <dig>  if the two numbers do not match, we can infer that some other thread had already inserted the k-mer into the bloom filter and proceed with inserting the k-mer into the hash table.

implementation
we implemented this algorithm in a program called bfcounter in c++, available from http://pritch.bsd.uchicago.edu/bfcounter.html the source code is licenced under a gpl licence. for the implementation we used the google sparsehash library and a bloom filter library by a. partow http://www.partow.net/programming/hashfunctions/index.html. we store a 1-byte counter for each k-mer and by default k-mers take 8-bytes of memory with a maximum k of  <dig>  although if desired, larger k-mers can be specified at compile time. we require the user to specify an estimate for the number of k-mers in the sequencing data and use a bloom filter with  <dig> times as many bits as the expected number of k-mers this corresponds to a memory usage of 4-bits per k-mer and the optimal number of hash functions functions for the bloom filter is d =  <dig> 

example data sets
to illustrate the performance of the new method, we describe the analysis of two data sets of sequencing reads from human genomic dna. the first data set consists of  <dig>  m  <dig> bp paired-end reads from the illumina platform that mapped to chromosome  <dig>  these data, from hapmap individual na <dig>  are available from illumina at http://www.illumina.com/truseq/tru_resources/datasets.ilmn. this data set corresponds to approximately 32-fold coverage of chromosome  <dig>  a coverage-level that is typical of many contemporary sequencing studies. since the reads have already been mapped to a genome this likely represents a cleaner data set  than we would expect to get from unprocessed sequence data.

the second data set consists of genome-wide sequence data from the  <dig> genomes project pilot ii study  <cit> . individual na <dig> was sequenced at 40-fold coverage, using  <dig>  billion  <dig> bp paired-end illumina reads. the data were filtered to remove sequences with low quality scores and missing basecalls; they are available at ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data/na19240/sequence_read/.

our first application is to the 32-fold sequence data from chromosome  <dig> data. we collected all k-mers from the sequencing reads, using k =  <dig>  figure  <dig> shows the distribution of the number of times each k-mer is seen in the input data. out of  <dig> m observed k-mers, slightly more than half  are observed only a single time. the vast majority of these singleton k-mers  are not found in the reference genome and hence are most likely due to sequencing errors, thus supporting the approach of discarding or correcting these unique k-mers.

to evaluate the computational performance of bfcounter we compared it to jellyfish  <cit>  and to a naive k-mer counting program without any filtering. all comparisons were done on a 64-bit x <dig> intel xeon machine with  <dig> cores at  <dig>  ghz and  <dig> gb of memory running linux kernel version  <dig> . <dig>  the disks were all from shared network through lustre. all time measurements were done with the time unix command and memory usage was measured using strace.

the naive version simply stores all k-mers explicitly in a google sparsehash hash table and skips the filtering step. jellyfish is a sophisticated k-mer counting program that features support for multicore machines. furthermore jellyfish stores an implicit representation of k-mers in a hash table to save memory. the authors of the jellyfish program recently showed that their method provides large memory savings compared to other traditional methods for k-mer counting. jellyfish requires us to prespecify the size of the hash table to use; if the hash table fills up, the results are written to disk and merged later. to compare the programs we found the minimum size so that jellyfish could keep all k-mers in memory. for the second data set jellyfish could not fit all k-mers in memory with default parameters. to fit the hash table in memory we needed to set the number of reprobes to  <dig> by running jellyfish with the -p  <dig> option. for timing comparisons we run jellyfish in serial mode.

the increase in the number of k-mers affects the memory consumption directly. figure  <dig> plots the memory requirements of bfcounter, jellyfish and the naive version. the increase in memory levels off for bfcounter after about 7-fold coverage, whereas for the naive version and jellyfish the memory increases steadily as the number of k-mers grows.

memory and time usage for the  <dig> genomes data set . *the naive version ran out of memory after processing  <dig> % of the reads.

there are  <dig>  billion k-mers present in the sequencing reads, of which  <dig>  billion are unique and  <dig>  billion have coverage of two or greater . when bfcounter was run, about  <dig>  billion of the unique k-mers were stored in the hash table after the first phase which corresponds to a  <dig> % false positive rate for the bloom filter. thus, bfcounter stored 27% of the original k-mers after the first pass, and this was cut to 23% after false positives were removed.

as may be seen from the table, bfcounter uses considerably less memory than either jellyfish or the naive hash table method. indeed the naive method ran out of memory and was unable to complete. however, bfcounter takes approximately three times longer to run as jellyfish. part of the difference in speed is due to bfcounter taking a second pass through the data to obtain exact k-mer counts .

CONCLUSIONS
counting k-mers from sequencing data is an essential component of many recent methods for genome assembly from short read sequence data. however, in current data sets, it is frequently the case that more than half of the reads contain errors and are observed just a single time. since these error-containing k-mers are so numerous, they can overwhelm the memory capacity of available high-performance machines, and they increase the computational complexity of downstream analysis.

in this paper, we describe a straightforward application of the bloom filter data structure to help identify and store the reads that are present more than once  in a data set, and are therefore far more likely to be correct. by doing so, we achieve greatly reduced memory requirements compared to a naive but memory-efficient hash table method, as well as to jellyfish . for many applications, it may be sufficient to simply ignore the unique k-mers ; alternatively, users may prefer to "correct" reads by comparing unique k-mers to common k-mers  <cit> . in summary, the approach presented here could be straightforwardly incorporated into a wide variety of algorithms that start by counting k-mers.

our method trades off reduced memory usage for an increase in processing time. in many cases the memory limitation is a hard threshold and the counting of k-mers is only run once and a fixed set of k-mers is stored for future computation. for genome assembly methods the construction of de bruijn graphs dominates memory consumption  <cit>  and the time for completion can be several days  <cit> , depending on the amount of postprocessing.

authors' contributions
pm and jkp contributed ideas and participated in writing this article. pm designed the algorithm, implemented the software and ran the experiments. both authors read and approved the final manuscript.

funding
this work was funded by a grant from the national institutes of health: mh <dig>  jkp is supported by the howard hughes medical institute.

