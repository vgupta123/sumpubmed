BACKGROUND
with the democratization of ngs technologies, large amounts of genomic and transcriptomic data became available to scientists in a short time span  <cit> . however, this magnitude of sequence data has brought most researchers a new bioinformatics challenge: to analyse and mine very large datasets  <cit> . one of the areas of particular interest of ngs data analysis is the detection of sequence polymorphisms. this task, however, becomes particularly difficult when no reference genome is available, which is common in non model organisms. this problem is somewhat mitigated when the samples can be accurately identified   <cit> . however, if neither of these is accessible – such as in datasets with pools of individuals, looking for reliable variation can be a real problem. it was for this purpose that 4pipe <dig> was developed: to find variation in  <dig> est datasets where no reference sequence or strain information is available. this is especially useful for researchers who wish to find reliable variation in a panel dataset of pooled individuals to use as a starting point for designing genotyping arrays to further explore their data. the pipeline can provide very high quality snps as well as the flanking region sequence, necessary for the design of customized genotyping arrays, currently the most efficient way to extend snp genotyping from those found in a panel of samples to a larger set of individuals for population genomic studies  <cit> .

due to the nature of ngs data, any automated pipeline has to be strict enough as to follow a work-flow but, at the same time, flexible enough to serve the different purposes of each investigator. this is the role that 4pipe <dig> intends to take. although 4pipe <dig> is tuned for est data, it can also be used with genomic data and, to some extent, to help automate the process of gene discovery.

implementation
4pipe <dig> is written in python  <dig> and is licensed under the gplv <dig>  it is written in a modular manner that allows for relatively simple expansion of functionality.

most of the functions present in 4pipe <dig> result from the automation of already existing programs and the integration of their respective outputs. however, the variation detection routines are of original design and are based on three criteria, all of which can be adjusted by the user:base coverage – the minimum required coverage ; the default value is 15;

base variants – the minimum number of equal base variants required in a position ; the default value is 20 % of the minimum required coverage;

base quality – the average minimum quality of each of the base variants ; the default value is  <dig> 



this means that in order to consider a position of the alignment as a putative snp, the below condition must be verified: ∑r≥c∧∑v2≥vmin∧qv1¯≥qmin∧qv2¯≥qmin 

where “r” is the number of reads in the considered position, “c” is the minimum coverage as defined by the user, “v1” is the most frequent variant base type in the considered position, “v2” is the second most frequent variant base type in the considered position and “q” is the quality value.

4pipe <dig> uses a configuration file, called “4pipe4rc” with a simple and self documented syntax for setting variables such as the location of programs, the snp detection criteria and the parameters that should be passed to the external software. how the program uses this configuration file is explained in detail in the program documentation.

the analysis process is divided in  <dig> steps, each of which can be excluded from the run by issuing the appropriate arguments at run time. in step  <dig>  4pipe <dig> takes an sff file and, if all the steps are run, step  <dig> outputs a series of html formatted reports, compressed in 7zip. steps  <dig>  and  <dig>  are considered optional since they are not required for the snp detection routines. 4pipe <dig> requires the use of external programs, which can all be installed locally without root privileges . the distribution comes with a set of helper scripts to automatically download and install all of the required software. all of the required programs are available under open-source licenses and are free to use .

RESULTS
the analysis process
the above mentioned  <dig> steps can be described as follows :step  <dig> – extraction of the “fasta” and “fasta.qual” files from the original “sff” file. this step can be skipped if not dealing with  <dig> data.

step  <dig> – “cleaning” the sequences, by discarding low overall quality and short reads, as well as reads that contain contaminants matched against the “univec” database  <cit>  or any other contaminant database at the user’s discretion. this step uses the “sequence cleaner” program  <cit>  and can also be skipped if dealing with illumina data.

step  <dig> – assembling. this step uses mira  <cit> . a set of optimized parameters for snp calling is contained in the example configuration file.

steps  <dig> and  <dig> – snp gathering. resorting to the “maf” output from step  <dig> , potential snps are identified in the assembly. the result is a summary intermediate “tcs” file and a “fasta” file including all the “contigs” that contain putative snps . the software “pysam”  <cit>  is used in this step.

step  <dig> – characterization of the detected snps, by attempting to fit them into open reading frames . the result is a “fasta” file containing the orfs with the snps identified in the sequence title, as well as the orf frame allowing the quick assessment of the length and level of conservation of the snp’s flanking region. this step uses the “emboss getorf” program  <cit> . also in this step, blastx  <cit>  is run with the resulting orfs against a large protein database, such as ncbi’s “nr”. lastly, this step will produce an html formatted report with the characterized snps for easy referencing. the report is formatted as a table and can easily be transferred to any spreadsheet software for further data exploring. another output of this step is an additional html report with a compilation of various dataset metrics.

step  <dig>  – blast2go annotation; this step queries the contigs that contain snps against a large protein database such as ncbi’s ‘nr’ using blastx; these are then run through blast2go  <cit>  using blast2go4pipe, resulting in an annotation file that can be further analysed with blast2go itself.

step  <dig>  – ssr detection, by using “emboss etandem” to detect potential ssrs in the assembly. the required quality of the putative ssrs is defined in the configuration file.

step  <dig> – compression of all the relevant result files into a 7zip archive which simplifies the transfer of  results.

fig.  <dig> 4pipe <dig> flowchart. the rectangular shapes represent processes, the rhomboid shapes represent input/output files. the dashed arrows represent optional steps. the names inside square brackets are the names of the used external programs. the digits on the top right corner of each rectangle represent the step number of each process



example usage
a test dataset with documentation on example usage is provided with the software package. an example resulting report is also provided for the test dataset .

validation
in order to assess the efficiency of snp detection and the rate of false positives, and assess the best default values to use, an approach using reference data was used.

for this goal, two reference sequences of two e. coli strains were used . two  <dig> datasets were also downloaded from the ncbi sequence read archive   <cit>   for the same strains as the references.

to assess the number of snps between the strains that could be found on the  <dig> datasets, the  <dig> reads of one strain were mapped against the reference sequences of the other strain using bowtie <dig>  <cit> . atlas-snp <dig>  <cit>  reported  <dig> snps between the reference sequence ′85′ and the  <dig> reads of the strain ′79′, and  <dig> snps between the reference sequence ′79′ and the  <dig> reads of the strain ′85′.

4pipe <dig> was then run on the two merged  <dig> datasets, discarding all strain information.

although this validation method is not as good as true wet-lab genotyping, it is likely to be a good proxy, since atlas-snp <dig> is known to have very high sensitivity and specificity when dealing with  <dig> datasets  <cit> .

the results varied with the different tested parameters , but the best output was obtained with the default values of minimum coverage of  <dig> and minimum average quality of  <dig> per variant. this setup retrieved  <dig> snps, of which  <dig> did not match to any of those detected by atlas-snp <dig>  being thus, considered false positives .table  <dig> obtained and validated snps per parameter set. of the six tested parameter combinations, the lowest false positive rate was retrieved with the default values:  <dig> minimum coverage and  <dig> minimum average quality



although the number of provided snps is relatively low, due to the restrictive assembly and filtering parameters, we find this a good trade-off relative to the high confidence of the retrieved snps.

the task of snp calling in  <dig> data has been performed before on organisms without a reference sequence or strain information, with varying degrees of false positives. one such study, conducted using custom scripts for snp calling provided a false positive rate of 80 % on  <dig> retrieved snps  <cit> . another example, where the contigs of  <dig> snps were manually screened and selected, had a slightly better false positive rate of 45 %  <cit> .

the above mentioned studies are not directly comparable to the results of the benchmark performed here, since they are performed on different datasets, nevertheless they can be used to infer that, in general, 4pipe <dig> retrieves a smaller number of snps than other methods, but with a considerably lower false positive rate. since the main goal of this pipeline is to provide the user with high confidence snps for genotyping arrays, a rate of  <dig>  % false positives is a considerable improvement relative to the other mentioned approaches.

4pipe <dig> compared to other software
although 4pipe <dig> is specifically designed for the purpose of detecting variation when no strain information or reference sequence is available, other software exists that can be used for the same purpose, but which differs from 4pipe <dig> in some aspects:

qualitysnp  <cit>  – relies on cap <dig> for clustering the reads . requires perl, php, a configured webserver and a mysql database for snp retrieval. this means that root access to the machine in which the program is being run on is required. furthermore, qualitysnp has been superseded by the simpler and faster qualitysnpng  <cit> .

agsnp  <cit>  – relies on newbler assembler for clustering, and if strain information is not available, it further requires combining  <dig> data with illumina or solid data .

still other programs exist for snp detection, but they usually require either a reference sequence, such as atlas-snp <dig> or samtools, or strain information, such as discosnp++  <cit>   or dial  <cit> .

there is, however, another program that can be used for the same purpose as 4pipe <dig> – qualitysnpng. this program, however is not an analysis pipeline, but rather a snp caller for read alignments. it has a graphical user interface, which can be disabled for use in servers, but still requires “qt4” to be installed in the server . in order to compare it with 4pipe <dig>  we have modified the program to be usable without “qt4” installed  and provide a branch of 4pipe <dig> which is ready to use qualitysnpng , without requiring any further dependencies.

benchmarking the results of 4pipe <dig> with qualitysnpng as the snp caller, more snps were returned  than with our snp caller, but with a larger rate of false positives . therefore, the builtin snp caller was kept as default, but qualitysnpng can still be used from its own git branch if desired.

for the sake of completeness, we also made the snp calling on the benchmark dataset using the software discosnp++  with the most restrictive parameters, to minimize the number of false positives. this program retrieved  <dig> snps, of which  <dig> were considered true positives . as expected, this method retrieves more snps than both 4pipe <dig> and qualitysnpng since it takes advantage of strain information, but it still provides a somewhat higher false positive rate than 4pipe <dig> 

CONCLUSIONS
we present here an automated analysis process specifically designed for snp detection from  <dig> pyrosequencing transcriptome reads, which we named 4pipe <dig>  this is the first program specifically built to automate the whole process of finding putative snps in ngs datasets that lack both information regarding the origin of each read and a reference sequence. in-silico validation of 4pipe <dig> results using previously analysed reference data revealed good performance in the calling of high confidence snps.

the 4pipe <dig> pipeline, at the cost of retrieving a relatively low number of snps, has provided a lower rate of false positive snps than both an alternative snp caller  and an alternative software that uses strain information , as well as those obtained in previous studies that used different approaches for a similar type of data and goal.

since the main purpose of this software is to retrieve high confidence snps for further exploring, we expect the incremental contributions it brings to improve, speed up and facilitate research on the field of population genomics.

furthermore, we expect to implement new features in 4pipe <dig>  such as: graphics in the metrics report; indel variation finding; integration of alternative software ; process optimization for ngs technologies besides 454; switch from fasta + fasta.qual format to fastq. these are some of the planned features, but others can be requested and implemented, should there be demand for them.

availability and requirements
project name: 4pipe4

project home page:https://github.com/stuntspt/4pipe4

operating system: platform independent 

programming language: python

other requirements: python  <dig>  or higher for running the pipeline, several other programs for specific tasks 

license: gnu gplv3

any restrictions to use by non-academics: none



competing interests

the authors declare that they have no competing interests.

authors’ contributions

fpm has developed the software, drafted the snp detection routines and written the manuscript, bmv has extensively reviewed the code and assisted in the initial data analyses, sgs has provided valuable insights on snp data interpretation and proofread the manuscript, db has provided the samples and the datasets for the initial data analyses, and proofread the manuscript and osp has conceived of the study, and participated in its design and coordination.

