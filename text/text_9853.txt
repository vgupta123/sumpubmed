BACKGROUND
as the understanding of the molecular systems governing many aspects of cellular function improves it is becoming increasingly clear that the assumption of mass action kinetics in well-mixed volumes is often invalid. a good example is calcium signaling, which can be highly localized with very steep concentration gradients  <cit> . calcium signaling depends on the interaction between membranes where the calcium channels are located and the cytoplasm where calcium activates many different enzymes  <cit> . the membrane channels are often arranged into clusters containing only a few or tens of channels  <cit>  resulting in stochastic release events that have been observed experimentally  <cit> . in neurons, the resting concentration of calcium in dendritic spines, where it plays an essential role in triggering synaptic plasticity, corresponds to only a few ions in this small volume  <cit> , indicating that calcium dynamics can be highly stochastic  <cit> . moreover, dendritic spines have a typical morphology that strongly affects the inward and outward diffusion of molecules  <cit> . taken together, these considerations point to the need of software that supports the simulation of stochastic reaction–diffusion systems with an accurate representation of the complex geometries specified by the membranes of a cell and its intracellular organelles. in this paper we describe steps, stochastic engine for pathway simulation, which was designed to give modelers an efficient implementation with a sophisticated user interface.

stochastic reaction–diffusion can be solved using two fundamentally different approaches: particle-based or voxel-based methods. in the first method one keeps track of the brownian motion of each individual molecule in the simulation and reactions are based on collisions between molecules, while in the second approach the behavior of groups of molecules in subvolumes is computed using the laws of chemical kinetics, and diffusion is simulated as the transport of molecules from one subvolume to another. particle-based methods can be further divided into those that track brownian motion in open space  or those that use lattices on which molecules hop from one site to another . an advantage of these methods is the high physicochemical fidelity of the approach, but this comes at the price of having to track the behavior of every single molecule in the system. this is computationally expensive and may not always be relevant in a biological context.

stochastic voxel-based approaches compute changes in the number of molecules present in small volumes without distinguishing among individual molecules. this can be more efficient in large systems and also allows for easier combination of exact solution methods with approximative ones  <cit> , which may greatly speed up computations . a widely used approach to model chemical reactions is gillespie’s stochastic simulation algorithm   <cit> , which can easily be extended to deal with diffusion , a method commonly referred to as “spatial ssa” or “spatial gillespie”. steps implements a derivative of the ssa in tetrahedral meshes to model the geometry, which importantly allow for a much better morphological resolution than the cubic voxels used in most other ssa based software, e.g. mesord  <cit>  and neurord  <cit> .

this paper describes steps  <dig>  and is structured as follows: we first introduce the overall workflow and structure of steps and its multiple solvers, the rest of the paper largely focusing on the solver for stochastic reaction–diffusion. we next introduce the ssa, tetrahedral meshes and how to adapt the ssa to model diffusion in such meshes. we then demonstrate the accuracy of steps and compare efficiency to other reaction–diffusion simulators. we finish with describing the systems biology markup language   <cit>  import module and demonstrate simulation of some sbml models using steps.

implementation
steps overview
the user interface to steps is in python, a very powerful and versatile scripting language, while the core steps code is in c/c++ for high efficiency. figure  <dig> shows a typical steps workflow. everything in the python user front-end is contained in namespace ‘steps’, within which there are a number of modules that contain classes and functions separated by the different tasks required to build a steps simulation. this means that using steps largely consists of creating python objects to represent the various components of a reaction–diffusion model  and invoking their methods to set conditions and to control the simulation.

steps differs from many reaction–diffusion simulators in that the chemical model and the geometry are described completely separately, which can be an advantage due to the challenging task of creating a suitable complex geometry for spatial simulations. for example, with the two tasks separated in this way one researcher may dedicate all their time purely to mesh-construction while another researcher constructs the biochemical model. meshes may then be saved, shared and reused in other simulations as required. uncoupling the model description from the simulation means that complex initial conditions can be achieved  <cit>  and modified without making changes to the model. to compliment these possibilities compatibility with current developed standards such as sbml is also achievable, yet relying purely on sbml would disable some important features of steps such as those mentioned above.

running a steps simulation will usually involve creating one main python module, which will import steps modules and possibly other outside user-written modules for the model  along with some of the many powerful scientific tools available for python such as scipy and numpy. python is generally regarded as a relatively easy-to-learn, intuitive language and the basic skills required to run a steps simulation - such as creating and manipulating objects, running simple loops and perhaps reading and writing to files - can usually be acquired quickly. we give a brief overview of the main components of the steps python user interface:

steps.model
the steps.model module contains everything required to describe how chemical species in the model interact. for example, the chemical species themselves are described by creating instances of class steps.model.spec. interactions of chemical species are described by creating objects to represent chemical reactions and diffusion rules. at this stage nothing is said about where these interactions take place, although different rules are grouped into ‘volume systems’ and ‘surface systems’, which are the objects that connect the biochemical model with the geometry description.

steps.geom
the steps.geom module contains all the classes and functions required to describe geometry to which a biochemical model may be applied. the basic building blocks of geometry in steps are ‘compartments’ and ‘patches’. a compartment is a 3d volume with reflective boundaries in which molecules may diffuse and react, and can either be well-mixed , or described by a collection of tetrahedrons in a mesh. a patch is a 2d surface in which molecules may be embedded and is connected to one or two compartments. analogously to compartments, in a well-mixed description patches are defined only by area and in a spatial simulation they are described by a collection of triangles forming a surface within a tetrahedral mesh. ‘surface reactions’ may take place in patches, which describe both surface-volume and surface-surface reactions allowing, for example, a molecule that is diffusing in a volume to become embedded in a surface and a molecule that is embedded in a surface to diffuse to a neighboring volume. such features are used to model events such as ligand-binding and transport.

groups of reaction and diffusion rules  defined in the biochemical model may be added to all  compartments in the geometry, and any groups of defined surface reaction rules  may similarly be added to patches. grouping in this way can bring advantages of realism and complexity, such as allowing the mobility of a species to differ between different environments, and performance because, for example, it is possible to declare which reactions from the overall set of reactions will occur in any given compartment and thus save memory by omitting reactions that can never occur .

the overall geometry used for any given simulation must be either a collection of well-mixed compartments and patches, or a collection of compartments and patches all within a tetrahedral mesh. the steps.geom.tetmesh class, which represents a tetrahedral mesh, contains a vast amount of information about the tetrahedrons and the triangle surfaces in the mesh and their connectivity with many helper functions for retrieving this information. this can be vital for initializing conditions in and running a spatial simulation.

steps.rng
the steps.rng module contains the “mersenne twister”  <cit>  random number generator class that provides the random numbers required by the steps algorithms.

steps.solver
the steps.solver module contains all the simulation solvers available in steps. a solver requires access to a biochemical model description along with a geometry description in order to build and run a simulation. all solver classes are derived from an abstract base class, which means all solvers contain some shared functionality such as the ability to inject molecule species into a compartment, run a simulation for some time and record updated concentrations. separate solvers then implement some or all of the optional methods depending on whether the function makes sense for that particular solver. the main focus for this work is solver ‘tetexact’, which is a stochastic solver that supports complex morphology and diffusion and is described further in implementation of spatial ssa solver. in addition there are two other solvers available in steps  <dig> : ‘wmdirect’, which is a well-mixed stochastic solver based on gillespie’s ssa  <cit> , and ‘wmrk4’, which is a deterministic solver based on the runge–kutta method  <cit> .

gillespie stochastic simulation algorithm 
a brief description of the direct method formulation of the ssa and its implementation in the wmdirect solver can be found in the additional file  <dig>  for a more detailed overview of the algorithm and its background see  <cit> . the ssa is an event-driven algorithm that has been demonstrated to give an exact solution to the chemical master equation  <cit> . the fundamental assumption made of the system is that elastic  collisions greatly outnumber reactive ones, which means that molecules become distributed uniformly throughout the system volume and that their velocities become thermally randomized to the maxwell-boltzman distribution. further, molecules are assumed to occupy a volume which is negligible in comparison to the total system volume.

several optimizations to the original direct method exist  <cit> , such as the construction of a dependency graph  <cit>  so that only the propensities of affected reaction and diffusion channels are updated in each iteration. such an approach is adopted by steps in solvers wmdirect and tetexact, as well as some other subvolume-based software .

implementation of spatial ssa solver
as mentioned, the standard ssa assumes a well-mixed system. however, one can introduce spatial gradients into the ssa  <cit>  by modeling a system of well-mixed subvolumes with diffusion between them described as first-order reactions.

in the well-mixed formulation the reaction container is described only by its volume. in the spatial ssa solver this reaction container is broken up in ntet smaller subvolumes and each of these subvolumes is treated as a reaction container in its own right by cloning the reaction channels. this means that for m reaction rules, and assuming all reactions may occur in all tetrahedrons , the total number of reaction channels will be mreac = ntet * m. the state x of the simulation will also become much bigger, it consists of n * ntet integers, with xi,k,t representing the number of molecules of species i in subvolume k at time t.

if we make the subvolumes within a certain size window , the well-mixed assumption applies to each subvolume independently and one can accurately represent concentration gradients  <cit> . the rate of diffusion of molecules of species i with diffusion constant di between two neighboring subvolumes will depend on the shape of the subvolumes. in steps we have chosen to use tetrahedral meshes, a type of non-orthogonal, unstructured mesh in which the problem domain is decomposed into a connected set of tetrahedral elements  <cit> . since the tetrahedra do not have to be perfectly regular, they can smoothly follow any boundaries and can adapt their size to the local level of detail. quite often the subvolumes in these meshes are the voronoi elements surrounding all edge nodes. steps instead uses the tetrahedrons themselves, meaning that each tetrahedral voxel has  <dig> triangular sides and, through them, is connected to a maximum of  <dig> neighboring tetrahedra. compared to the voronoi description this reduces coding complexity , maintains control over subvolume size and allows for a far more accurate description of a surface that represents a membrane within a mesh.

diffusion of chemical species a between neighboring tetrahedrons k and l is simulated by the following reversible “reaction” channel:

  ak⇌dl,kdk,lal 

with the diffusion rates given by:

  dk,l=disi,kvkdxk,ldl,k=disi,lvldxl,k 

where s is the surface area of the triangle connecting tetrahedrons k and l, v is the volume of the tetrahedron and the distance dx is computed as the barycenter-to-barycenter distance, therefore dxl,k == dxk,l.

including diffusion greatly increases the total number of reactions channels: mtot = mreac + mdiff, with mdiff <  <dig> * n * ntet . however, a diffusion channel in steps actually consists of diffusion of a particular species from a tetrahedron to any one of its neighbors, with a total rate of diffusion equal to the sum of the rates in each direction. once a diffusion channel is chosen by the ssa, one of the possible directions is then chosen. this is mathematically equivalent to describing diffusion of a particular species from a tetrahedron as  <dig>  separate diffusion channels, but reduces memory requirements by approximately a factor of  <dig> with only a small cost to efficiency.

the use of a tetrahedral mesh greatly increases the complexity of the search for the next reaction in the ssa. since version  <dig> , the implementation of tetexact solver has therefore adopted the composition and rejection  method  <cit> , whose time complexity is constant even for systems with large number of reactions. the algorithm is described in more detail in additional file  <dig> 

importing and annotating tetrahedral meshes
high quality meshes are often essential to obtain accurate simulation results. instead of developing our own mesh generator we make use of powerful mesh generation software, such as cubit  <cit> , tetgen  <cit> , and gmsh  <cit> , and provide a set of utilities for importing tetrahedral meshes. this approach is significantly different from the approach taken by most other spatial ssa simulators, for example mesord  <cit>  and neurord  <cit> , where cubic mesh generation is included in the simulation.

the mesh importing utilities are carefully designed so that steps is not only capable of importing meshes from supported formats, but is also extendable. currently, one-step importing functions support three common mesh formats: the abaqus format exportable by cubit, tetgen’s own formats , and the msh ascii format used by gmsh. these import functions use the pure-python-based elementproxy class which provides generic mesh importing functionalities such as data storage, grouping and index mapping. during import, first a sufficient number of elementproxy objects are created of a type corresponding to the kind of geometry element in the mesh data . after that, data about each geometry element is inserted in its associated proxy. during the insertion, the proxy automatically assigns a steps index for the element and also records the index mapping between the import index and steps internal index of the element, which are accessible during later simulation. once all element data is inserted, the proxy objects can be directly used to create meshes, compartments and patches in steps, as well as to perform further mesh manipulations.

the mesh importing utilities in steps also provide a more advanced, flexible way to simulate systems with complex geometries. traditionally, meshes for subvolume-based ssa simulations have been constructed from combinations of standard geometry primitives such as cubes, spheres and cylinders  <cit> , where geometry features are highly abstracted. a typical representative of this method is the constructive solid geometry , adopted by mesord for geometry construction. this type of mesh is relatively easy to construct, but the highly abstracted models may not reflect real geometry constraints to the system and may produce inaccurate simulation results . a better, yet more challenging approach is to reconstruct volume meshes from biological data based on closed surface meshes. however, the surface meshes derived from series of electron microscope images  <cit>  are commonly unclosed and have small intersecting surfaces, thus they cannot be used directly in volume mesh generation. this problem can be solved by semi-manually preprocessing the surface meshes using mesh manipulation tools such as meshlab  <cit> . once volume meshes are generated from the cleaned-up surface meshes, they can be imported to steps for simulations. figure  <dig> gives an example steps simulation running in a reconstructed mesh with realistic geometry.

subvolume size
when generating a mesh for steps, and other stochastic voxel-based approaches to reaction–diffusion simulation, an important consideration is the size of the subvolumes. as described in implementation of spatial ssa solver, subvolumes are assumed to be of a size that represents a well-mixed region. most real biological systems will indeed exhibit a size-band where the natural motion of molecules maintains the well-mixed condition and for a steps simulation we should ensure that all tetrahedrons fall within this band for maximum simulation accuracy.

how can we estimate the size at which a system exhibits well-mixed behavior? by ‘well-mixed’  we mean that there are many more nonreactive collisions than reactive ones, quickly removing any spatial gradients that appear from phenomena such as chemical reactions or transport. at relatively large volumes, however, spatial chemical gradients can persist in a region, broadly speaking whenever reactions occur at a faster rate than the region can be smoothed by diffusion. if we were to represent such regions with well-mixed subvolumes we would lose spatial detail. determining the largest volume at which spatial gradients don’t exist is a good estimate of the upper-bound for the well-mixed condition and can be estimated mathematically by comparing reaction time to diffusion time in the continuous case. however, it would be naive to assume that below a certain volume the region is always well-mixed; subvolumes can also be too small  <cit> . sizes that are comparable to the size of a molecule are intuitively too small since we require that molecules are well-defined within subvolumes, and sizes smaller than the mean-free path of a molecule are also too small because there may not be enough elastic collisions taking place inside a region to keep it well-mixed. in a discrete description the minimum number of molecules in a populated tetrahedron is  <dig>  which means that, below a certain size , reaction time for a populated tetrahedron decreases with decreasing volume  and comparison to diffusion time for the discrete case is a good way to estimate the minimum subvolume size.

so there exists a window of subvolume size at which we can apply the well-mixed approximation, which may be slightly different for each particular model. it is important to determine this window, but we will show that it is usually relatively large for biologically realistic models and therefore poses only a minimal restriction on mesh-generation. simulation time increases with increasing number of tetrahedrons, so tetrahedron size is also an important consideration for simulation efficiency. the greatest simulation efficiency possible with acceptable accuracy would be with all tetrahedrons in the mesh at the upper-bound of acceptable size. however, the tetrahedrons that represent the space around complex boundaries may have to be significantly smaller than the largest acceptable size so as to represent the boundaries accurately. an ideal mesh for any given problem, therefore, is one that achieves the greatest simulation efficiency at which the well-mixed assumption holds, but with acceptable morphological resolution.

for an example problem with reasonable simulation parameters, if the fastest reaction in the system is a second order reaction a+b→kc with k = 100/μm.s, the slowest diffusion coefficient is d =  <dig>  μm2/ms, and concentrations are  =  =  <dig> μm, our upper bound estimate is approximately  <dig>  μm with a lower bound of approximately  <dig>  μm . this size window is large enough not to place much restriction on our mesh-generation at all. though a factor of  <dig> for the tetrahedron size may not sound like a large window, it means of the order of a  <dig> factor difference in volume. put another way, the number of tetrahedrons per cubic micron of mesh for this problem should number more than approximately  <dig> and fewer than approximately  <dig> , <dig> to ensure the well-mixed subvolume condition. there are several reasons why we may wish to stay significantly larger than the calculated lower bound in this example, the practical reasons that a mesh of  <dig> million tetrahedrons per cubic micron would consume enormous amounts of memory and result in an unnecessarily slow simulation, and another reason is that at  <dig> nanometers we are approaching the size of proteins.

we may find for other types of simulations that our window may even be larger than in this example. however, with slower diffusion the window may become narrower, but is often still large enough not to pose too much restriction on mesh generation. for example, with the same example parameters except for a diffusion coefficient of  <dig>  μm2/ms, which is about the slowest diffusion coefficient of the very largest proteins in water  <cit> , the mesh should contain more than approximately  <dig> tetrahedrons and fewer than approximately  <dig>  tetrahedrons per cubic micron, so some care should be taken in this case not to go below the lower bound of acceptable subvolume size . the crowded environment of the cell can cause the observed apparent diffusion coefficient to be lower than that in water by a factor that may be dependent on the size of the molecule  <cit> . however, crowding at the same time is expected to also decrease the rate of fast, diffusion-limited association reactions  <cit> . therefore a scenario where large, very slowly-diffusing molecules are involved in fast reactions that increase the minimum subvolume size enough to approach the upper bound is rather unlikely. the results section contains a validation of the well-mixed subvolume calculation for a problem with slow diffusion, approximately in the range for the apparent diffusion coefficient measured of large proteins in the cytoplasm.

recent proposals have been made to correct reaction rates at the algorithmic level if subvolumes approach a small, “critical” size  <cit> , however in steps we prefer to keep the larger minimum size as a constraint on the model, in effect constraining the subvolume size to be significantly larger than the “critical” size. this is partly because the small sizes typically involved often consume huge amounts of memory and slow simulations unnecessarily or may even be unattainable, so to go below the lower bound and approach the critical size is often impossible or impractical. in the case for very slow diffusion it may be necessary for modelers to take some care to ensure tetrahedrons do not become too small, but in practice, for most biologically realistic models, mesh generation is not significantly restricted by the subvolume size consideration and the major challenge is realistic boundary representation.

mesh quality
consideration should also be given to the quality of the mesh used for the steps simulation. tetrahedrons, compared to cubic elements, have the benefit of being able to adapt their size and shape to local levels of detail, however one must make sure that they do not become too irregular, or “stretched” in any regions of the mesh. in part this is due to considerations of subvolume size  and also to do with an assumed level of regularity for the derivation of the diffusion rates.

there are many different quality measurements for a tetrahedron and each mesh-generator usually comes with one or more from this set. the software may use these measures internally so as to ensure good quality mesh output, and can also report quality of the generated mesh to the user. the quality measure used in tetgen is the radius-edge ratio: the ratio of the radius of the tetrahedron’s circumsphere to the length of the shortest edge. this value is approximately  <dig>  for a perfectly regular tetrahedron. values up to  <dig> , which is currently the default value in tetgen for quality mesh generation, produce a mesh in which no tetrahedron is too stretched out. cubit incorporates many quality measurements, for example the aspect ratio beta  <cit> , which is the circumsphere radius divided by  <dig> times the inscribed sphere radius and takes acceptable values between  <dig>  and  <dig> 

steps itself comes with a quality measure that can be performed on the imported mesh, the radius-edge ratio that is also used in tetgen. as well as quality, it may be desirable for example to find the minimum, maximum and standard deviation of tetrahedron volumes so as to ensure they fall within the acceptable range, and this can be achieved with a simple loop over mesh elements in the python interface. steps will not fail to run a simulation on a poor quality mesh, since setting an internal tolerance may be too restrictive, so users should decide whether to perform their own analysis on a mesh to determine if it is of acceptable quality before running a simulation.

RESULTS
accuracy of geometry representation
to test our intuition that complex geometry may be better represented by tetrahedral meshes than cubic meshes we constructed five geometrical shapes each representing a dendritic spine on a neuron  <cit>  by a simple combination of a spherical head and cylindrical neck. all spines were generated randomly within constraints to cover a broad range recorded experimentally from rat purkinje neurons  <cit>  . we then compared the accuracy of these meshes for two biologically important measures: volume, important for chemical reactions and diffusion in the spine, and surface area, important for membrane transport mechanisms like voltage-gated calcium channels on the spine  <cit> .

for each spine shape, first an adaptive tetrahedral mesh was generated in cubit with the coarsest mesh  permissible by the software, and then a cubic mesh was generated in mesord from csg input, with the cube size controlled to result in a mesh with a similar number of subvolumes to the tetrahedral mesh . furthermore, for each spine a more detailed  tetrahedral and cubic mesh was generated, with a close match between the number of tetrahedral and cubic subvolumes. the more detailed meshes typically approached the approximate minimum subvolume size for a system of slow diffusion and fast reaction previously discussed, and so are approximately the most detailed mesh that would be acceptable for simulation. figure  <dig> shows spine # <dig> represented by both a tetrahedral mesh and cubic mesh in the coarser case.

for all meshes the volume and surface area of both the head and neck regions were measured and compared. figure  <dig> shows a plot of the normalized measurements. all meshes appeared to represent the spine head volume quite accurately, yet the cubic meshes often failed to represent the neck volume sufficiently, and only a marginal improvement was noticeable in the more detailed meshes. this demonstrates that, while one could always find an optimal cube size to represent any one region of a geometry accurately, the cube size will not necessarily suffice for other regions which may have different morphologies. this is a clear drawback for cubic meshes, which originates from the need for all subvolumes to be of the same size. any error in volume will of course produce an error in reaction rates as well as for diffusion rates. it may be possible with a very detailed mesh to represent all regions sufficiently, yet a larger number of subvolumes means a slower simulation and may result in loss of accuracy caused by the small subvolume size.

in terms of surface area in all cases the cubic meshes failed to represent the boundaries accurately, in fact slightly worsening in the more detailed meshes. this obviously arises from a discrepancy between the surface/volume ratio of a cube compared to a sphere or cylinder. this failure to represent surface area closely could for example be important if modeling the mobility of surface molecules, or if a density of surface molecules is specified in a model then the total number of molecules would end up being too high in a cubic mesh due to the larger surface area. in such a case it may be possible to overcome such difficulties by introducing a correction factor, yet a further complication is that, in tests, we determined that such a correction factor is not constant and varies considerably throughout regions of the spine meshes.

in all cases the tetrahedral meshes represented volume and surface area throughout all regions of the mesh with high accuracy.

validation
although steps uses established methods to simulate reaction–diffusion systems, errors can be made in the coding that would lead to erroneous results. therefore it is important to validate the accuracy of the program by simulating models for which the correct response is known. to our knowledge no standard benchmarking library for reaction–diffusion systems exists, so we developed a representative set of models that test different aspects of the code.

here we briefly describe each model and show the accuracy of the results in steps. the models and parameters used are described in detail in additional file  <dig> and in the model scripts which can be downloaded from the steps website.

simple well-mixed reactions
we tested steps accuracy for four types of reactions, chosen for their prevalence in real systems and models, as well as for their fitness for analytical investigation. the python interface to steps made it easy to take the mean and standard deviation of a large number of individual simulation runs and in each case the mean steps output was converted to a 95% confidence interval , which was then compared to the known analytical value .

one of our simplest validation systems, the first-order irreversible reactions system, is also perhaps one of our most important due to the fact that we test the resulting noise from our implementation of the ssa. this vital aspect of stochastic reaction–diffusion simulator output is usually insufficiently tested, often with simple visual comparison of the amplitude of the noise from the output of two different simulators. the standard deviation matched the analytical solution to the chemical master equation closely, and the mean behavior also behaved as expected  .

for first-order reversible reactions the steady state can be computed and the mean concentrations of the steps simulation evolved properly to this steady state  . for the second-order irreversible reaction with equal reactant concentrations  and unequal concentrations  the mean behavior of the steps simulations followed the analytical solutions .

we also tested a ‘production and degradation’ reaction system described by two reactions: a first-order annihilation reaction and a zero-order production reaction. such reactions, though they may be unphysical in biological systems, are useful simplifications that are commonly used in models and as such are supported in steps. due to the simplicity of this system an analytical solution to the steady-state version of the chemical master equation can be found . the stationary distribution from the simulation in steps followed the analytical prediction  , providing another validation of the noise in our ssa implementation.

in total for the well-mixed reaction systems, for  <dig> of  <dig> measurements the analytical mean fell within the confidence interval of steps output, a success rate of 96% which is approximately equivalent to the 95% success that is expected.

diffusion
many of the diffusion models could not undergo such precise statistical analysis as the reaction models because in most cases the analytical concentration of an exact position in 1-dimensional axial or radial space is compared to the mean concentration at the center of a small bin of finite tetrahedral volumes in steps, which is not a precise comparison. however, simply by visual comparison it could be seen whether steps output followed closely what was expected.

we first tested the most universal case: 3d diffusion from a point source in an infinite volume , which has a known analytical solution for the time and evolution of the radial mean concentration  <cit> . while we could ensure the absence of boundary effects, it was not possible to mimic a point source in a tetrahedral mesh. the small deviations between the analytical solution and the steps simulation at short distances from the source for early simulation times are due to the finite volume of this source . at further distances or later times the match between the mean of the steps simulation showed no significant deviation from the analytical solution.

next we tested three different scenarios for 1d diffusion: from a point source at one end of a finite tube , in a semi-infinite tube with a clamped concentration at the end  and in a finite tube with constant influx of the same species at both ends . these systems each have a known or deduced analytical solution of the time evolution of the axial mean concentration for 1d diffusion, which may in each case be compared to simulation output due to the radial symmetry of the problem. in all three cases the mean of the steps simulation and the analytical solution matched closely at all spatial locations for all times.

reaction–diffusion
in testing the combined simulation of chemical reactions and diffusion we were limited by the paucity of available analytical solutions. a first simple test was to add diffusion to an irreversible first-order reaction with an initial uniform concentration of the reagent. diffusion should not affect this process, which is confirmed  . next, a very discrete reaction–diffusion problem, typically containing only about  <dig> molecules in the system, was analyzed. this consisted of two reactions: a zero-order reaction and second order reaction  <cit> . ensuring that subvolume size was larger than the accepted lower bound, and significantly larger than the “critical value” discussed in  <cit> , the deviation of the stationary distribution of the reactant from the analytical solution to the master equation was small .

finally we present a reaction–diffusion system containing spatial gradients for which we found an analytical solution  <cit>  and where diffusion is important. this is a second-order degradation process where the initially separated reactants diffuse from separate halves of a tube and the assumption that they degrade so fast on contact that their concentration at the center is always zero. the analytical solution then corresponds to diffusion with a concentration clamped at zero, which matched the steps simulation closely .

algorithm efficiency
to make comparisons between the efficiency of the reaction–diffusion algorithm in steps to a similar tool we compared to mesord  <cit>  and to make comparisons to particle methods we chose smoldyn since it has been reported to be an efficient particle simulator  <cit> . all simulations were run on a macbook pro  <dig>  ghz intel core  <dig> duo processor and  <dig> gb  <dig> mhz ddr <dig> sdram. care was taken to ensure that the computer performance was as equal as possible for every test, with a measured pystone score of approximately  <dig> pystones/second.

precise comparisons of the subvolume approach employed by steps and mesord with the particle approach of smoldyn are difficult due to different factors affecting the efficiency of the two approaches, however we tested a range of simulation conditions with notable comparisons when the simulators are estimated to be at the most efficient with acceptable accuracy. the acceptable spatial resolution in steps and mesord is estimated as the size at which there are approximately  <dig> diffusion events per reaction event  per subvolume . smoldyn is not exact for all length time-steps due to the fact that the reactants can only undergo one interaction at the end of a time-step and it is not possible for a molecule to be in existence for less than the length of a time-step. this means that if products are involved in further interactions  then errors are introduced for large time-steps. an acceptable time-step in smoldyn is stated as that at which it is “significantly smaller” than the timescale of the fastest reaction in the system, which we will choose as the time-step that is  <dig> times lower than the fastest reaction in the system.

in a comparison to the smoldyn benchmark  <cit>  steps appears to perform favorably. this model is a michaelis-menten enzyme reaction and the simulation parameters suggest an upper bound of tetrahedron size in steps of  <dig> μm, which corresponds to approximately  <dig> tetrahedrons in the simulation volume. simulating in a mesh of  <dig> tetrahedrons took  <dig> seconds in steps compared to  <dig> seconds for the smoldyn simulation .

the more interesting comparisons in efficiency, however, come in larger models with a greater number of distinct diffusing species with multiple chemical interactions. we ran a test model in steps, mesord and smoldyn consisting of  <dig> molecular species in different concentrations, that diffuse with different diffusion constants, and interact by  <dig> reaction channels . steps and mesord runtime increases with increasing spatial resolution and smoldyn runtime increases with decreasing time-step, so we compared a range of conditions with an estimate of the point at which the simulators are most efficient yet accurate. all simulations were run  <dig> times and a median value taken, and in all cases the differences in runtimes between the  <dig> tests were found to be small.

the spatial ssa approach shows increasing benefit with larger numbers of molecules. in the second model a total of  <dig> molecules were injected, which corresponds to a concentration of ~ <dig> μm, and during simulation increased to around  <dig>  the upper-limit of acceptable spatial resolution was estimated to correspond to approximately  <dig> tetrahedrons in the mesh for steps and  <dig> cubes in the mesord mesh, and the fastest reaction time in smoldyn at the start of the simulation remained  <dig> ms. the steps simulation with  <dig> tetrahedrons took  <dig> seconds compared to  <dig> seconds in a mesh of  <dig> cubes in mesord. in smoldyn, with a time-step of  <dig>  ms, simulation time was  <dig> seconds. with approximately double the number of subvolumes in steps  and mesord  runtimes increased to  <dig> seconds and  <dig> seconds respectively. with twice the number of iterations in smoldyn  simulation time was  <dig> seconds.

direct comparison between steps and mesord show that steps performs significantly better in all tested conditions. direct comparisons to smoldyn are not possible, however something that can clearly be seen is that the magnitude of the slopes between the two different initial conditions means that, for a factor of  <dig> increase in the number of molecules, steps slows by a factor of approximately  <dig>  whereas smoldyn slows by a factor of approximately  <dig> and this factor is expected to become even larger in models with a greater numbers of molecules. notice, however, that a higher molecule number does mean a slightly finer mesh must be used in steps and mesord to ensure the well-mixed subvolume condition.

importing sbml models
steps provides thorough and well-validated support for sbml  <cit> , a common format for representing biochemical models. steps has been tested to successfully run the majority of the sbml test suite models and “curated” models from the biomodels database  <cit> , with results validated against published solutions. the high-level of support has largely been made possible by supporting mathml expressions. such expressions, which are very common in sbml models, often include simulation variables that must be stored and available to use by the solver. steps stores the mathml expressions in python structures, updates variables in the expressions during simulation, and can solve the expressions whenever necessary. this has been crucial for steps support for sbml components such as function definitions, initial assignments, assignment rules, rate rules, event triggers and event assignments. for the special case of reactions, steps is able to examine the form of the reaction kinetic law maths and separate them into two categories: those that can be represented as an ordinary reaction in steps, and those that must be solved by an approximate method because the kinetic law maths differs from a fundamental reaction . this ensures that a reaction will never be represented incorrectly in steps. any simulator, such as mesord, that does not provide thorough mathml support is limited in its support of many sbml components, and any stochastic simulator that does not examine the form of reaction kinetic law maths will represent most published models incorrectly. steps support extends to models containing multiple compartments and surfaces, along with volume-surface and surface-surface reactions. in the end steps successfully imported 654/ <dig> sbml test suite  <dig> . <dig> l3v <dig> models . solutions are provided with the test suite for every model, so by automated testing a large number of models could be imported, simulated in the deterministic wmrk <dig> solver in steps and results compared against given solutions. of the  <dig> models that failed  <dig> fell into  <dig> categories, those that contained:  0-dimension or 1-dimension compartments that are not supported in steps,  no chemical species  and  algebraic rules, which are difficult to support in a steps context and rarely appear in sbml models. the other models that failed included some that are not possible to run in a discrete stochastic context such as those that include partial stoichiometry or negative concentrations and as such are also not supported in the steps deterministic solver, which shares model construction with the stochastic solvers. steps also successfully imported 223/ <dig> curated biomodels database models , where possible the most recent versions of each model  were imported. the majority of the failures were again because of no chemical species in the model, partial or high reactant stoichiometry, and also included those with unsupported units such as amperes or volts.

imported models may be directly simulated in steps using the deterministic solver or the well-mixed stochastic solver, although many models are not suitable for stochastic simulation without some modification. it is worth noting that sbml models may potentially form the basis of simulations in the spatial stochastic solver, but not without some modifications; for example diffusion coefficients and non-uniform initial conditions must currently be defined outside of sbml.figure 12a shows a deterministic simulation in steps of model biomd <dig> from the biomodels database in comparison to a biomodels online simulation. this model of spontaneous calcium oscillations in astrocytes  <cit>  contains two compartments  with transport reactions between them. some reactions in the model can be represented as fundamental reactions, but some reactions contain complex maths, which is therefore converted to python structures allowing for solution by the approximate method. figure 12a shows close agreement between the steps simulation and the biomodels online simulation, and also matches the published results  <cit>  .

some sbml models are suitable for well-mixed stochastic simulation without any modification. one example is biomd <dig>  which contains a femtoliter compartment and micromolar concentrations. as described further in  <cit>  this is a larger model containing  <dig> species and  <dig> reactions, and also  <dig> events which represent ‘camp’ and ‘ca’ signals. at time  <dig> s a pulse of camp is introduced, and then intermittently between times  <dig> s and  <dig> s the reaction parameter controlling calcium influx is toggled between a low and high value by events. when running the simulation in steps the mathematics representing event triggers and event assignments is stored in python structures, which makes it possible to represent these important features of the model. figure 12b shows simulation results using the steps stochastic solver superimposed on deterministic results from a biomodels online simulation, displaying the two species directly involved in events: camp and ca. steps matches the biomodels online simulation results, with some small expected variability arising from the stochastic simulation.

discussion
advantages and disadvantages
as described by example in  <cit>  the python interface to steps brings a number of advantages over reaction–diffusion simulators that have a non-interactive interface. steps modelers have greater freedom and control over a simulation, and may utilize the many powerful scientific tools already available for python for tasks such as data analyses and visualization. in fact we believe a powerful interface is the only way to be able to achieve the complex tasks that go with initializing, running and collecting data from simulations in complex 3d geometries.

steps provides substantial functionality  in the python interface that a modeler may use for building a model and customizing a simulation, which can be particularly important for spatial simulations. examples are the steps.geom.tetmesh class functions that are crucial for acquiring and utilizing information about complex 3d mesh geometry, and the functionality in the steps tetexact solver that allows for the manipulation of molecule counts, reaction and diffusion rates, both compartment and patch-wide or individually for tetrahedrons and triangles. there are many possible applications for such functionality, including complex initial conditions  <cit> , chemical localization, control over reaction rates by external variables such as voltage or temperature, and support for some of the more advanced features of sbml such as rules and events. all functions are described in detail in the user documentation.

python is becoming particularly important in the neurosciences and as more and more neural simulators adopt a python interface the future may even see python used to glue simulators together so that phenomena on different spatial scales can be integrated, although there may be more efficient alternatives  <cit> . the python interface is an advantage now and as biological simulation becomes more and more complex, will be an advantage for the future.

steps is capable of accurately representing complex boundaries by supporting unstructured tetrahedral meshes. we demonstrated that while the regular cubic meshes supported by some other simulators are very easy to generate they limit the morphological resolution. tetrahedrons are able to adapt their shape and size to regions of high morphological detail, which means that such meshes  are able to follow the complex boundaries of a cell very closely. tetrahedrons may be larger for regions of less interest, which is an important consideration for simulator efficiency. however, with tetrahedron-based geometry comes the difficulty of generating high-quality meshes, which is a drawback for this approach. specialist mesh generation software is best left to this task and there are some powerful tools available, with common output formats supported by steps. complex boundary generation is not a unique problem to steps and particle-based simulators that support complex surfaces, which may for example be represented by collections of triangles or squares, may also require outside software to develop sufficient quality surfaces.

steps has the advantage of supporting both spatial and non-spatial stochastic simulations as well as deterministic simulations, and in the near future will even be able to combine spatial and non-spatial compartments in the same stochastic simulation. this may be a very important feature for efficiency in some models while still allowing for complex boundary representation. for example, if one wished to simulate calcium release from intracellular calcium stores in the endoplasmic reticulum , it would be vital to represent the resulting calcium gradients in the cytoplasm in a spatial compartment, yet the simulation would be severely slowed by simulating diffusion in the highly-concentrated er, which may only have a negligible effect on outcome. representing the er as a well-mixed compartment would reduce runtime considerably without affecting accuracy.

chemical accuracy of reaction–diffusion algorithms
any representation of a biochemical system on a computer is of course not an exact replication of the real system. simplifications are made, both by the modeling software and in the designed model itself, to ensure that the problem is tractable and may be simulated on the limited computational power available in an acceptable amount of time. for example, many approaches to biochemical reaction diffusion simulation ignore the crowded environment of the cell formed by macromolecular structures, a feature that can have a significant affect on apparent diffusion coefficients and reaction rates  <cit> . every approach to incorporating spatial detail into stochastic biochemical simulations makes a different set of simplifying assumptions, which means that the most accurate approach may depend on the properties of the simulated model.

a popular approach to reaction–diffusion modeling is based on smoluchowski theory  <cit> , which tracks diffusing point-like particles that may react when they fall within a certain distance of another reactive molecule. this theory has the advantage of including some consideration of molecule size, but is limited by its own simplifying assumptions about the system, which can lead to small errors if, for example, two reactants are at a similar concentration to each other  <cit> , which is of course often the case for biological systems. in addition, the theory is derived for a system with only one reaction present, making results for all systems with more than one reaction approximate. however, a small time-step can often ensure good accuracy, but this comes at a cost to efficiency.

the spatial gillespie approach makes the assumption that the subvolumes represent well-mixed regions and, as we have seen, at relatively very large or very small sizes this may not be the case, so some care must be taken by the modeler to ensure subvolumes are in the well-mixed range. a benefit of this approach is the potential gain in efficiency, but a drawback is that it is as yet unclear how to incorporate molecule size and macromolecular crowding into simulations, although this may be possible in the future. the unstructured mesh in steps already allows for complex boundaries that could potentially form impenetrable fixed structures within the volume that could go some way towards replicating the crowded environment of the cell. the incorporation of reactant size into the gillespie framework is an active area of research and it has already been found that for a unimolecular second-order reaction system in one dimension reactant size may be simply incorporated by replacing system volume with the “free volume”  <cit>  and in two dimensions the propensity function is still applicable, yet with a larger correction than just the free volume  <cit> . this suggests an extension to  <dig> dimensions will involve a correction to the propensity function based on the excluded volume from the molecules, which could potentially be found from a fixed user-defined parameter of molecule size. allowing the molecule size to be defined explicitly could ensure that it is always biologically feasible. where the implied molecules size is found intrinsically from reaction and diffusion parameters  the calculated binding radius can be very different from the physical size of the molecules; typical reaction and diffusion parameters of proteins give a binding radius that is unrealistically small  <cit> , and slow diffusion with a fast reaction can lead to a binding radius that is very large. in addition to this possibility, one intriguing approach to this problem is to apply a hybrid method where discrete and continuous spatial descriptions are both permitted, and the simulator combines the spatial gillespie method and brownian dynamics  <cit> .

despite their limitations, current voxel-based and particle methods can often both be shown to be good approximations for a range of biological conditions and a lot of useful information about many systems can be extracted from their simulation. in the future, as computational power increases, our understanding of cellular systems improves and new algorithms are developed , biochemical computing will surely become more and more powerful and accurate. only time will tell how significant these future improvements will be, or whether current methods are accurate enough for most studies. at present, with different methods for stochastic reaction–diffusion simulation within complex boundaries essentially producing the same results for a wide range of biological conditions, software efficiency, reliability and ease of use are often the most important considerations for a modeler.

software efficiency
the core algorithm in steps is an efficient implementation of the spatial gillespie approach to reaction–diffusion modeling and contains the potential for further improvements to runtime in the future with the introduction of approximate methods such as tau-leaping  <cit>  adapted for diffusion  <cit> . efficiency in a spatial gillespie simulation depends on the number of mesh subvolumes so care must be taken to ensure that, where possible, the subvolumes are close to the upper bound of accepted size, as discussed in subvolume size.

steps performed favorably in direct comparison to another subvolume-based simulator, mesord, in a wide range of conditions, which demonstrates the efficiency of the steps implementation. although it is difficult to precisely compare simulator efficiency between spatial gillespie and particle methods, partly because of the difficulty of pinpointing the exact point at which the simulator becomes accurate, what can clearly be seen by comparison to the efficient particle simulator smoldyn is that the spatial gillespie method in steps has a significant advantage over particle methods at higher molecule numbers. also, once accurate conditions have been met, steps performs better than smoldyn with increasing spatial resolution in steps compared to decreasing time-step and increasing accuracy in smoldyn. this is important because simulation conditions are often more detailed than the very upper-bound of acceptable conditions so as to ensure accuracy or because of complex boundary restrictions.

an important point is that voxel-based software such as steps only begins to lose accuracy when subvolumes become very small  and efficiency is low, whereas, conversely, accuracy in particle methods generally increases with smaller time-step and therefore lower efficiency. this means that spatial ssa methods are most accurate at efficient simulation conditions, whereas particle methods are generally most accurate at inefficient conditions.

validation
regardless of the approach and the capabilities of the simulator it is important that all supported features are validated by analyzing output, where possible comparing to known analytical solutions. validation ensures that there are no numerical errors resulting from bugs in the code so that the software may be reliably used for research purposes, and repeating this validation every so often checks that any recent changes to code have not resulted in loss of accuracy. steps is well tested and validated for the majority of its capabilities as we have reported in this paper, yet other simulators are often poorly validated and may be unreliable, particularly when it comes to capturing the noise resulting from stochastic chemical reactions. as reaction–diffusion simulators become more and more widely used to investigate the molecular properties of neural and other biochemical systems it is vital that each simulator is known to be reliable and accurate. for this reason it is important that standards for validation for reaction–diffusion simulators are developed, as has been achieved for example for the electrical properties modeled by neuronal simulators  <cit> . the set of validations that we have presented in this paper could contribute towards such a future reaction–diffusion standard.

the future
the future will see further additions to steps as more and more biological phenomena are added to models. in neurons in particular, the intracellular signaling pathways are highly coupled to the electric excitability of the cell through the activity of voltage-gated channels on the membrane. a powerful addition to future versions of steps will be the calculation of the potential across surfaces representing membranes within the tetrahedral mesh geometry, to which voltage-gated ion channels may be added. in the near future lateral diffusion will also be implemented to simulate the mobility of molecules in membranes. further ahead, one possibility is to allow meshes to dynamically alter their shape during simulation to replicate real changing cell shape. one potential application for this is the simulation of the enlargement of dendritic spines associated with long-term potentiation  <cit> .

future additions to steps will also be based on considerations of efficiency. since spatial simulations are mainly dominated by diffusion, the largest gain in efficiency may come with implementing approximate methods for diffusion.

currently steps is developed and tested under 32-bit systems, thus the simulation size is restricted by  <dig> gb of addressable memory, approximately  <dig> kinetic processes. although this restriction can be eased by converting steps to a 64-bit version, the simulation of very large scale systems on individual workstations can be impractical due to a long runtime. solution of this problem will depend on the development of an efficient parallel framework, where the whole system is distributed and simulated in different nodes of a computing cluster. a great challenge for such a parallel framework is the need to reduce network communication as well as preventing unnecessary rollbacks caused by state conflicts between nodes.

CONCLUSIONS
discreteness, stochasticity and spatial effects are vital considerations for capturing the dynamics of many cellular molecular systems, yet this high level of detail makes efficiency a particularly important consideration for tools such as steps that are designed to simulate such systems. efficiency is tied to accuracy, gains in one often coming at a cost to the other. steps employs the spatial ssa approach to discrete reaction–diffusion simulation, which is generally more efficient than particle-based methods, yet more abstracted conceptually. however, we have shown that there is usually no loss or a minimal loss of accuracy for biochemical systems, provided that due consideration is given to subvolume size. the optimized algorithm in steps was shown to out-perform both another ssa-based simulator, mesord, and particle methods by comparison to smoldyn, with increasing benefit in larger systems and increasing simulation detail. in terms of spatial accuracy, steps offers improvement over other spatial ssa software by supporting tetrahedral meshes, which provide higher morphological resolution than cubic voxels. the problem of representing complex boundaries in surfaces or meshes is best left to powerful, specialist software, and common formats are imported by steps. the distinction between biochemical model, geometry description and solver method offer a number of advantages, such as the ability to apply different simulation techniques to the same model, and to reuse complex geometry descriptions. solver accuracy was confirmed in an extensive validation suite consisting of a set of reaction, diffusion and reaction–diffusion systems. the python interface to steps was found to play an important role in almost all aspects of creating models, running test simulations and building additional features, including reliable support for sbml.

therefore, steps successfully combines both high performance and high accuracy within a powerful and user-friendly interface, allowing application to a large number of biochemical network models where stochasticity and spatial organization play a prominent role. the framework in steps offers the potential for future improvements to performance, such as approximate method implementation and parallelization, which will add more power and open up even more applications as larger systems may be simulated. further additions to the code will open up the exciting possibility of full integration with the electrical properties of the cell, allowing accurate and efficient parallel multi-scale neural simulations. for these and further future additions, which could for example potentially include new algorithms to represent the crowded environment of the cell, scientific accuracy and software efficiency will both continue to play prominent roles in steps development.

availability and requirements
project name: stochastic engine for pathway simulation

project home page:http://steps.sourceforge.net

operating system: platform independent

programming languages: c/c++, python

other requirements: python  <dig>  ~  <dig> 

license: gnu general public license version 3

source code and pre-compiled binaries for windows and mac os x are available at the home page where further information about steps can also be found, including the online user manual.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
ih participated in steps development, was the main author of the sbml import module, carried out validation, geometry accuracy and software efficiency studies, and drafted the manuscript. wc was involved in steps development, including implementation of the composition and rejection method, and performed all mesh constructions. sw conceived the steps workflow and carried out all the original steps development. eds conceived of and supervised the project and helped draft the manuscript. all authors contributed to the manuscript and read and approved the final version.

supplementary material
additional file 1
gillespie stochastic simulation algorithm , optimizations and implementations in steps. a description of the algorithms and optimizations used in steps solvers wmdirect and tetexact  <cit> . 

click here for file

 additional file 2
subvolume size. an analysis of acceptable tetrahedron size range for reaction-diffusion simulations in steps. 

click here for file

 additional file 3
cubic and tetrahedral mesh comparison. a statistical comparison between cubic and tetrahedral mesh representation of five different dendritic spine geometries. 

click here for file

 additional file 4
validation. detailed descriptions of the validation models used in this study, along with statistical analysis of results from steps simulations.  <cit> . 

click here for file

 additional file 5
simulator efficiency test model. a description of the model used to study efficiency of steps, mesord and smoldyn, along with model files for each simulator. 

click here for file

 additional file 6
figure s <dig>  sbml test suite support. all  <dig> models  of the sbml test suite  <dig> . <dig>  were imported, run in the wmrk <dig> deterministic solver in steps and results compared against given solutions. the chart shows the proportion of models supported  and unsupported . the unsupported models are separated into  <dig> categories.

click here for file

 acknowledgements
steps development was supported by hfspo  and oistpc.

cubit was used under rights and license from sandia national laboratories: sandia license number 09-n <dig> 

we thank michele mattioni of the european bioinformatics institute for the initial work on the sbml import module. mm was supported by the european molecular biology laboratory international phd program.
