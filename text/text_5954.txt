BACKGROUND
canonical correlation analysis   <cit>  is a statistical method for finding common information from two different sources of multivariate data. this method optimizes linear projection vectors so that two random multivariate datasets are maximally correlated. with advances in high-throughput biological measurements, such as dna sequencing, rna microarrays, and mass spectroscopy, cca has been extensively used for discovery of interactions between the genome, gene transcription, protein synthesis, and metabolites . because cca solution is reduced to an eigenvalue problem, multiple components of interactions with sparse constraints are readily introduced  <cit> .

kernel cca  was introduced to capture nonlinear associations between two blocks of multivariate data . given two blocks of multivariate data x and z, kcca finds nonlinear transformations f and g in a reproducing kernel hilbert space  so that the correlation between f and g is maximized. in order to avoid overfitting and to improve interpretability of results, sparse additive functional cca   <cit>  constrains f and g as sparse additive models and optimizes them using the biconvex back-fitting algorithm  <cit> . however, it is not straightforward to obtain multiple orthogonal transformations for extracting multiple components of associations. another method for finding nonlinear associations is to maximize measures of nonlinear matching, such as the hilbert-schmidt independent criterion   <cit>  and the kernel target alignment   <cit>  between linearly projected datasets x and z  <cit> . while these methods can obtain multiple orthogonal projections by iteratively analyzing residuals, it is impossible for these methods to remove irrelevant features, making them prone to overfitting.

in this paper, we propose two-stage kernel cca , which enables us  to select sparse features in high-dimensional data and  to obtain multiple nonlinear associations. in the first stage, we represent target kernels with a weighted sum of pre-specified sub-kernels and optimize their weight coefficients based on hsic with sparse regularization. in the second stage, we apply standard kcca using target kernels obtained in the first stage to find multiple nonlinear correlations.

we briefly review cca, kcca, and two-stage mkl, and then present tskcca algorithm. we apply tskcca to three synthetic datasets and nutrigenomic experimental data to show that the method discovers multiple nonlinear associations within high-dimensional data, and provides interpretation that are robust to irrelevant features.

cca, kernel cca, and multiple kernel learning
in this section, we briefly review the bases of our proposed method, namely, linear canonical correlation analysis , kernel cca , and multiple kernel learning .

canonical correlation analysis 
let d={}n=1n be n pairs of samples, where x
n and z
n are the n-th samples drawn from p- and q-dimensional euclidian space, respectively. let f
w‚â°w
t
x and g
v‚â°v
t
z denote the projection of x‚àà‚Ñùp by w‚àà‚Ñùp and that of z‚àà‚Ñùq by v‚àà‚Ñùq, respectively. the objective of linear cca is to find projections that maximize pearson‚Äôs correlation between fw‚â°{fw}n=1n and gv‚â°{gv}n=1n and formulated as the following optimization problem: 
 1a maxw‚àà‚Ñùp,v‚àà‚Ñùqcov 



 1b subject tovar=var= <dig>  


where var and cov denote the empirical variance and covariance of the data, respectively. the optimal solution  of eq.  is obtained by solving generalized eigenvalue problems and successive eigenvectors represent multiple components. the projections, f
‚àó=w
‚àót
x and g
‚àó=v
‚àót
z, are said to be canonical variables for x‚àà‚Ñùp and z‚àà‚Ñùq, respectively. if we introduce sparse regularization on w and v, we obtain sparse projections  <cit> .

kernel cca
in kernel cca , we suppose that the original data are mapped into a feature space via nonlinear functions. then linear cca is applied in the feature space. more specifically, nonlinear functions œïx:‚Ñùp‚Üí‚Ñçx and œïz:‚Ñùq‚Üí‚Ñçz transform the original data {}n=1n to feature vectors {,œïz)}n=1n in reproducing kernel hilbert spaces  ‚Ñçx and ‚Ñçz. inner-product kernels for ‚Ñçx and ‚Ñçz are defined as k
x=œï
xt
œï
x, and k
z=œï
zt
œï
z.

let us implement f
w and g
v by projections f
w‚â°w
t
œï
x and g
v‚â°v
t
œï
z. by introducing appropriate regularization terms, eq.  can be reformulated as the following optimization problem : 
 2a maxŒ±‚àà‚Ñùn,Œ≤‚àà‚ÑùnŒ±tkxkzŒ≤ 



 2b subject toŒ±tkx+nŒ∫2i2Œ±= <dig> 



 2c Œ≤tkz+nŒ∫2i2Œ≤= <dig>  


where k
x and k
z are n-by-n kernel matrices defined as nn‚Ä≤=kx and nn‚Ä≤=kz
 <dig>  i is the n-by-n identity matrix and Œ∫  is the regularization parameter.

once having obtained the solution of eq. , denoted by , canonical variables for x‚àà‚Ñùp and z‚àà‚Ñùq are given by 
 3a f‚àó=‚àën=1nkxŒ±n‚àó 



 3b g‚àó=‚àën=1nkzŒ≤n‚àó, 


respectively. as indicated by eq. , the nonlinear functions, œï
x and œï
z, are not explicitly used in the computation of kcca. instead, the kernels k
x and k
z implicitly specify the nonlinear functions, and the main goal is to solve the constrained quadratic optimization problem with 2n-dimensional variables.

multiple kernel learning
kernel methods usually require users to design a particular kernel, which critically affects the performance of the algorithm. to make the design more flexible, the framework of multiple kernel learning  was proposed for classification and regression problems  <cit> . in mkl, we manually design m
x sub-kernels {kx}m=1mx, where each sub-kernel kx uses only a distinct set of features in x. also, m
z sub-kernels {kz}l=1mz for z is also designed in the same manner. then, k
x and k
z are represented as the weighted sum of those sub-kernels: 
 4a kx=‚àëm=1mxŒ∑mkx 



 4b kz=‚àël=1mzŒºlkz, 


where weight coefficients of sub-kernels, {Œ∑m}m=1mx and {Œºl}l=1mz are tuned to optimize an objective function.

a specific example of this framework is the two-stage mkl approach  <cit> : in the first stage, the weight coefficients are optimized based on a similarity criterion, such as the kernel target alignment; then, a standard kernel algorithm, such as support vector machine, is applied in the second stage.

methods
in this section, we propose a novel nonlinear cca method, two-stage kernel cca , inspired by the concepts of sparse multiple kernel learning and kernel cca. in the following, we present the general framework of tskcca, followed by our solutions for practical issues in the implementation.

first stage: multiple kernel learning with hsic and sparse regularizer
in tskcca, sub-kernels are restricted to the same class as eq. , allowing us to express the kernel matrices k
x and k
z as 
 5a kx=‚àëm=1mxŒ∑mkx 



 5b kz=‚àël=1mzŒºlkz, 


where nn‚Ä≤=kx and nn‚Ä≤=kz. the goal of the first stage is to optimize the weight vector Œ∑=t and Œº=t so that kernel matrices k
x and k
z statistically depend on each other as much as possible, while irrelevant sub-kernels are filtered out.

the statistical dependence between k
x and k
z is evaluated by the hilbert-schmidt independent criterion  and approximated by its empirical estimator  <cit> : 
  <dig> ùîª=tr <dig>  


where h is an n-by-n matrix such that nn‚Ä≤=Œ¥nn‚Ä≤‚àí1n, and Œ¥nn‚Ä≤ is kronecker‚Äôs delta. tr denotes the trace. in our setting, optimization problem is reduced to a simple biliear form with respect to Œ∑ and Œº: 
  <dig> ùîª=Œ∑tmŒº, 


where m is a m
x-by- m
z matrix such that 
  <dig> ml=trhkzh) <dig>  


in addition to maximizing the dependency measure ùîª, Œ∑ and Œº should be sparse in order to filter out irrelevant sub-kernel matrices. to this end, we determine optimal weight vectors as the solution of the following constrained optimization problem: 
 9a maxŒ∑‚àà‚Ñùmx,Œº‚àà‚Ñùmzùîª=Œ∑tmŒº 



 9b subject toŒ∑‚â• <dig> Œº‚â• <dig> Œ∑2=Œº2= <dig>  



 9c Œ∑1‚â§c <dig> Œº1‚â§c <dig>  


where xp=1/p is the l
p-norm of the vector x and c
 <dig> and c
 <dig> are parameters . this optimization problem is an example of penalized matrix decomposition with non-negativity constraints  <cit> . accordingly, we can obtain optimal weight coefficients by performing singular value decomposition of matrix m under constraints. in this process, the i-th left singular vector Œ∑=,‚Ä¶,Œ∑mx)t as well as the right singular vector Œº=,‚Ä¶,Œºmz)t are obtained iteratively by algorithm  <dig> 





in algorithm  <dig>  s denotes the element-wise soft-thresholding operator: the m-th element of s is given by sign+, where + is x if x‚â• <dig> and  <dig> if x< <dig>  in each step, Œî is chosen by a binary search so that l <dig> constraints ‚à•Œ∑‚à•1‚â§c
 <dig> and ‚à•Œº‚à•1‚â§c
 <dig> are satisfied. in general, the above iteration does not necessarily converge to a global optimum. for each iteration, we initialize Œ∑
 with a non-sparse, left singular vector of m, following the previous study, to obtain reasonable solutions  <cit> .

the second stage: kernel cca
after learning kernels via penalized matrix decomposition as above, we perform the second stage of standard kernel cca  <cit>  to obtain optimal coefficients Œ±
‚àó and Œ≤
‚àó  with parameter Œ∫ for each pair of singular vectors {Œ∑,Œº}i=1rank. given test kernel {kx,test}m=1mx and {kz,test}l=1mz, test correlation corresponding to the i-th singular vectors is defined as correlation between ‚àëm=1mxŒ∑mkx,testŒ±‚àó and ‚àël=1mzŒºlkz,testŒ≤‚àó.

practical solutions for tskcca implementation
tskcca still has several options for sub-kernels to be designed manually. in this study, we focus on feature-wise kernel and pair-wise kernel defined in the following sections.

feature-wise kernel
feature-wise kernel was introduced to perform feature-wise nonlinear lasso  <cit> . in the previous study, using feature-wise kernels as sub-kernels in sparse mkl resulted in sparsity in terms of features since each sub-kernel corresponds to each feature. with x
nm and z
nl representing the m-th feature for x
n and l-th feature for z
n, respectively, we adopt the following gaussian kernel in this study: 
 10a nn‚Ä≤=exp‚àíŒ≥x <dig> 



 10b nn‚Ä≤=exp‚àíŒ≥z <dig>  


where Œ≥
x and Œ≥
z are width parameters. by applying feature-wise kernels, projection functions are restricted to additive models defined as f‚àó=‚àëm=1pfm and g‚àó=‚àël=1qgl, where fm:‚Ñù‚Üí‚Ñù  and gl:‚Ñù‚Üí‚Ñù  are certain nonlinear functions  <dig>  note that the number of sub-kernels, m
x and m
z, are equivalent to the number of features, p and q, respectively.

pair-wise kernel
we introduce pair-wise kernels as sub-kernels to consider cross-feature interactions among all possible pairs of features. since the sparseness is induced to the weight of sub-kernels, the pair-wise kernels result in selecting relevant cross-feature interactions. projection functions are defined as f‚àó=‚àëm<m‚Ä≤pfm,m‚Ä≤ and g‚àó=‚àël<l‚Ä≤qgl,l‚Ä≤, where fm,m‚Ä≤:‚Ñù2‚Üí‚Ñù and gl,l‚Ä≤:‚Ñù2‚Üí‚Ñù are certain nonlinear functions with two dimensional inputs. note that the number of sub-kernels, m
x and m
z, are, p/ <dig> and q/ <dig>  respectively.

preprocessing for mkl
we normalize the sub-kernels to have uniform variance in rkhs. this is an important procedure in the context of mkl because each feature-wise kernel has a different scale. this makes it difficult to evaluate weight coefficients  <cit> . to compensate for that, we calculate the variance œÉ
 <dig> in rkhs as 
 11a œÉ2=1n‚àënnœïxn‚àí1n‚àën‚Ä≤nœïxn‚Ä≤ <dig> 



 11b =1n‚àënnœïxntœïxn‚àí1n2‚àën,n‚Ä≤nœïxn‚Ä≤tœïxn‚Ä≤ 



 11c =1n‚àënnnn‚àí1n2‚àën,n‚Ä≤nnn‚Ä≤. 


dividing each sub-kernel by its variance k‚ÜíkœÉ <dig>  we can achieve normalization of each sub-kernel.

parameter tuning by a permutation test
when the kernel matrix k
x  is full rank, as is typically our case, kcca with a small Œ∫  can always find a solution such that the maximum canonical correlation nearly equals one. this property makes it difficult to tune the regularization parameters for the first stage c
 <dig> and c
 <dig>  to solve the issue, we introduce a simple heuristics.

the key idea is to conduct a permutation test for deciding whether to reject a null hypothesis that the maximal canonical correlation induced by i-th singular vectors is no more than those attained when x and z are statistically independent. since the p-value of this test is interpreted as the deviance between the actual outcome and those expected under the null hypothesis, we use it as a score to evaluate the significance of i-th singular vectors where smaller p-value is more significant.

algorithm  <dig> summarizes our implementation for the permutation test. only for the first singular vectors Œ∑
 and Œº
, this procedure is applied to various pairs of  that satisfy the constraints of 1‚â§c1‚â§mx and 1‚â§c2‚â§my  <cit> . among them, the pair with the lowest p-value is chosen as the optimal parameters of c
 <dig> and c
 <dig> 





for simplicity, other parameters, such as Œ≥ in the gaussian kernel and Œ∫ in kcca, are fixed heuristically. Œ≥
‚àí <dig> is set to the median of the euclidean distance between data points and Œ∫ is set to  <dig>  as recommended in the previous study  <cit> .

RESULTS
in this section, we experimentally evaluate the performance of our proposed tskcca, safcca  <cit> , and other methods using synthetic data and nutrigenomic experimental data.

dataset 1: single nonlinear association
to evaluate the ability to extract a single nonlinear association, we generated simple synthetic data which consisted of a single pair of relevant features in quadratic association and noise, in which standard cca and kcca are known to performance poorly  <cit> . let n and u denote the normal distribution with mean Œº, variance s
 <dig>  and uniform distribution supported in a, respectively. the synthetic data were generated as 
 x.m‚àºum= <dig> ‚Ä¶,dz.1=x.12+Œµz.l‚àºul= <dig> ‚Ä¶,dŒµ‚àºn, 


where d was the total number of dimensions and Œµ was independent noise.

the optimal model in each method was trained using n training samples. here, we assumed c
1=c
 <dig> in the range of 1‚â§c <dig> c2‚â§d <dig> and obtained optimal values using a permutation test with b= <dig>  the test correlation was evaluated with separate  <dig> test samples, averaged over  <dig> simulation runs as we varied the number of dimensions, the sample size, and the noise level.
 <dig> and z
 <dig>  among 2√ód sub-kernels in the first stage, especially in the case of n= <dig> and n= <dig>  as a result, it achieved better test correlation than safcca, especially with high-dimensional data, indicating that our method was sufficiently robust.
fig.  <dig> comparison of test correlation averaged over simulation runs in data  <dig>  the horizontal axis denotes the number of dimensions d, and the vertical axis denotes test correlations. the number of training samples is  <dig>   <dig>  and  <dig>  tskcca outperforms safcca, especially with high-dimensional data




in addition, fig.  <dig> shows average computation time for each method over  <dig> simulation runs with dataset  <dig>  computation time of tskcca was comparable with that of safcca, and could scale up with the feature size. note that all the experiments were performed on a macbook pro with intel core i <dig>  with  <dig> gb main memory. all the simulation programs were implemented in matlab ¬Æ;.
fig.  <dig> comparison of computation time for data  <dig>  the horizontal axis denotes the number of dimensions d, and the vertical axis denotes computation time in log-scale. the number of training samples is  <dig>   <dig>   <dig> for safcca and tskcca. computation time of tskcca is moderate and can be scaled




dataset 2: multiple nonlinear associations
to test whether our method could extract multiple nonlinear associations precisely, we generated the following data: 
 x.m‚àºum= <dig> ‚Ä¶,25z.1=x.1+exp+Œµ1z.2=x.22+sin+Œµ2z.3=|x.3|+1/)+Œµ3z.l‚àºul= <dig> ‚Ä¶,25Œµl‚àºnl= <dig> , <dig>  


first, we performed a permutation test with b= <dig> for ten singular vectors {Œ∑,Œº}i= <dig> corresponding to the ten highest singular values of m given by eq. . p-values of the top three were significant  and the rest were non-significant. this result suggests that only the three singular vectors included nonlinear associations.
. <dig> x
. <dig> and z
. <dig> were associated. the contributions of Œ∑ <dig>  Œ∑ <dig> and Œº <dig> in the second singular vectors were also dominant, indicating that x
. <dig>  x
. <dig> and z
. <dig> were associated. finally, the contributions of Œ∑ <dig>  Œ∑ <dig> and Œº <dig> in the third singular vectors were dominant, indicating that x
. <dig>  x
. <dig> and z
. <dig> were associated. some singular vectors averaged over  <dig> simulation runs are listed in table  <dig>  our results suggest that tskcca achieved feature selection precisely.
fig.  <dig> transformations f and g obtained with tskcca. the top three rows and the bottom row show the resulting functions corresponding to relevant and irrelevant features, respectively



Œ∑
1
 <dig>  

Œ∑
2
 <dig>  

Œ∑
3
 <dig>  

Œ∑
4
 <dig>  

Œ∑
5
 <dig>  

Œ∑
6
 <dig>  

Œº
1
 <dig>  

Œº
2
 <dig>  

Œº
3
 <dig>  

Œº
4
these results show mean weight coefficients  in  <dig> simulation runs. significant weight coefficients are bold faced




we further evaluated test correlation, precision, and recall averaged over  <dig> simulation runs. table  <dig> shows that safcca failed to detect all relevant features because it is not able to obtain multiple canonical correlations, while our method detected  <dig> relevant sub-kernels out of  <dig> in the first stage in most runs. note that the precision is the fraction of retrieved features that are relevant and recall is the fraction of relevant features that are retrieved.
tskcca can identify most relevant features through three significant singular vectors, while safcca can only identify a small set of them




dataset 3: feature interactions
to assess the capability of tskcca in discovering nonlinear interactions, we generated data with a product term: 
 x.m‚àºum= <dig> ‚Ä¶,dz.1=x.1x.2+Œµz.l‚àºul= <dig> ‚Ä¶,dŒµ‚àºn, 


where d was the number of dimensions. for this dataset, we used feature-wise kernels and pair-wise kernels as sub-kernels in order to handle both single feature effects and cross-feature interactions like the term x
.1
x
. <dig>  there were d+d√ó/ <dig> sub-kernels, the weight coefficients of which were optimized in our method.

first, to evaluate the performance of our method with feature-wise and pair-wise kernels, we obtained test correlations evaluated by individual test data  in different numbers of dimensions d. next, to evaluate the accuracy of feature selection of the model, we assessed recall and precision. average test correlations, recall, and precision over  <dig> simulation runs are shown in fig.  <dig>  our results illustrate that in the case of d< <dig> , our method successfully determined the relation between z
. <dig> and x
.1
x
. <dig> 
fig.  <dig> the performance of pair-wise kernels in data  <dig>   test correlations averaged over  <dig> simulation runs in different numbers of dimensions.  recall and precision averaged over  <dig> simulation runs in different numbers of dimensions. our method successfully extracts nonlinear associations with relevant features




dataset 4: nutrigenomic data
we then analyzed a nutrigenomic dataset from a previous mouse study  <cit> . in this study, expression of  <dig> genes in liver cells that would be relevant in the context of nutrition and concentrations of  <dig> hepatic fatty acids were measured on  <dig> wild-type mice and  <dig> ppar Œ±-deficient mice. mice of each genotype were fed  <dig> different diets with different levels of fat. for matrix notation, gene expression data were denoted by x‚àà‚Ñù40√ó <dig>  and data regarding concentrations of fatty acids was denoted by z‚àà‚Ñù40√ó <dig>  data were standardized to have a mean of zero and unit variance in each dimension. several linear correlations between x and z were detected by applying a regularized version of the linear cca  <cit> .

first, we performed a permutation test for sparse cca, kcca, safcca, and tskcca on parameters defined by equally-spaced grid points in order to identify significant associations in these data. in kcca and safcca, there were no significant associations; thus, we focused on sparse cca and tskcca in the following analysis. we identified two significant linear associations in sparse cca  and one nonlinear association in tskcca  with c
1= <dig>  and c
2= <dig> .

figures  <dig> and  <dig> show the results of feature selection of sparse cca and tskcca, respectively. genes selected by the first singular vector of our method have different expression levels in different genotypes , suggesting that our method successfully extracted the nonlinear correlation associated with genotypes.
fig.  <dig> feature selection of sparse cca in nutrigenomic data. left and right panels show selected genes and fatty acids, respectively. genes marked with asterisks show significantly different expression in different genotypes


fig.  <dig> feature selection of tskcca using nutrigenomic data. left and right panels show selected genes and fatty acids, respectively. genes marked with asterisks show significantly different expression in different genotypes. the left panel shows that the 1st singular vector extracts nonlinear correlations associated with the genotype




for further analysis, cross-validation was performed in  <dig> runs. in each run,  <dig> samples were randomly split into  <dig> training samples used for fitting models and  <dig> validation samples used for evaluating the canonical correlation for fitted models. figure  <dig> shows box plots of correlation coefficients in sparse cca and tskcca. left one represents the first canonical correlation coefficient in sparse cca and right one represents correlation coefficient obtained with the first singular vectors. significantly higher test correlation  were achieved by the first singular vectors of tskcca, indicating that it avoided overfitting despite having nonlinearity.
fig.  <dig> box plot of test correlations in nutrigenomic data. left and right panels show the box plot of  <dig> times test correlation using sparse cca and tskcca, respectively. tskcca achieves significantly higher test correlation through its first weight vector 




to account for interactions between features into our model, we calculated pair-wise kernels for nutrigenomic data. although the number of sub-kernels was huge , tskcca successfully extracted a significant association . to evaluate the stability of feature selection, we performed tskcca on  <dig> runs with data generated by random sampling of empirical data with replacement. table  <dig> shows the frequencies of features  selected across  <dig> runs, suggesting that pmdci played an important role within the interactions.



discussion
other researchers have employed the sparse additive model  <cit>  to extend kcca to high-dimensional problems, and have defined two equivalent formulations, such as sparse additive functional cca  and sparse additive kernel cca   <cit> . the former was defined in a second order sobolev space and solved using the biconvex back-fitting procedure. the latter, defined in rkhs, was derived by applying representer theorem to the former. given some function fm‚àà‚Ñçm, these algorithms optimize the additive model, f1‚àà‚Ñç <dig> f2‚àà‚Ñç <dig> ‚Ä¶,fp‚àà‚Ñçp. in contrast, our formulation supposes an additive kernel, such as ‚àëŒ∑mkm associated with rkhs ‚Ñçadd and finds correlations in this space. this approach enables us to reveal multiple components of associations.

some problems specific to kcca, such as choosing two parameters  and the number of components, remain unsolved. while cross validation is applicable to set these values  <cit> , they are fixed for simplicity in our study, based on the previous study  <cit> .

next, we discuss the validity of feature selection in nutrigenomic data performed using sparse cca and tskcca. in the original study, the authors focused on the role of ppar Œ± as a major transcriptional regulator of lipid metabolism and determined that ppar Œ± regulates the expression of many genes in mouse liver under lower dietary fat conditions  <cit> . they provided a list of genes that have significantly different expression levels between wild-type and ppar Œ±-deficient mice. while only a few genes selected by sparse cca were included in the list,  <dig> out of  <dig> genes selected with the 1st singular vector in tskcca were included in the list. this result shows that tskcca successfully extracts meaningful nonlinear associations induced by ppar Œ±-deficiency.

moreover, in our analysis of pair-wise kernels, most of the frequently selected pairs of genes retained pmdci known as a sort of enoyl-coa isomerases involved in Œ≤-oxidation of polyunsaturated fatty acids. this implies that the interactions of pmdci and other genes contribute to lipid metabolism in ppar Œ±-deficient mice.

many variants of sub-kernels, such as string kernels or graph kernels, can be employed in the same framework. in the field of bioinformatics, yamanishi et al. adopted integrated kcca , which exploited the simple sum of multiple kernels to combine many sorts of biological data  <cit> . this technique can be improved by optimizing weight coefficients of each kernel in the frame of tskcca. finally, if kernels are defined on groups of features, it enables us to perform group-wise feature selection, just like group sparse cca . it is beneficial to consider group-wise feature selection for biomarker detection problems.

CONCLUSIONS
this paper proposes a novel extension of kernel cca that we call two-stage kernel cca, which is able to identify multiple canonical variables from sparse features. this method optimizes the sparse weight coefficients of pre-specified sub-kernels as a sparse matrix decomposition before performing standard kernel cca. this procedure enables us to achieve interpretability by removing irrelevant features in the context of nonlinear correlational analysis.

through three numerical experiments, we have demonstrated that tskcca is more useful for higher dimensional data and for extracting multiple nonlinear associations than an existing method, safcca. using nutrigenomic data, our results show that tskcca can retrieve information about genotype and may reveal an interactive mechanism of lipid metabolism in ppar Œ±-deficient mice.

endnotes

 <dig> in this article, nn‚Ä≤ denotes the -th elements of the matrix enclosed by the brackets.


 <dig> in this article, x
.m denotes the m-th feature of x.

abbreviations
ccacanonical correlation analysis

hsichilbert-schmidt independent criterion

kccakernel canonical correlation analysis

mklmultiple kernel learning

rkhsreproducing kernel hilbert space

safccasparse additive functional canonical correlation analysis

tskccatwo-stage kernel canonical correlation analysis

