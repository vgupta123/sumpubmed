BACKGROUND
microarray technology, which can handle thousands of genes of several hundreds of patients at a time, makes it hard for scientists to manually extract relevant information about genes and diseases, especially cancer. moreover this technique suffers from a low signal-to-noise ratio. despite the rise of high-throughput technologies, clinical data such as age, gender and medical history, guide clinical management for most diseases and examinations. a recent study  <cit>  shows the importance of the integration of microarray and clinical data has a synergetic effect on predicting breast cancer outcome. gevaert et al.  <cit>  have used a bayesian framework to combine expression and clinical data. they found that decision integration, and partial integration leads to a better performance, whereas full data integration showed no improvement. these results were obtained by using a cross validation approach on the  <dig> samples in the van’t veer et al.  <cit>  data set. on the same data set, boulesteix et al.  <cit>  employed random forests and partial least squares approaches to combine expression and clinical data. in contrast, they reported that microarray data do not noticeably improve the prediction accuracy yielded by clinical parameters alone.

the representation of any data set with a real-valued kernel matrix, independent of the nature or complexity of data to be analyzed, makes kernel methods ideally positioned for heterogeneous data integrations  <cit> . integration of data using kernel fusion is featured by several advantages. biological data has diverse structures, for example, high dimensional expression data, the sequence data, the annotation data, the text mining data and heterogeneous nature of clinical data and so on. the main advantage is that the data heterogeneity is rescued by the use of kernel trick, where data which has diverse data structures are all transformed into kernel matrices with same size. to integrate them, one could follow the classical additive expansion strategy of machine learning to combine them linearly  <cit> . these nonlinear integration methods of kernels have attracted great interests in recent machine learning research.

daemen et al.  <cit>  proposed kernel functions for clinical parameters and pursued an integration approach based on combining kernels  for application in a least squares support vector machine . they explained that the newly proposed kernel functions for clinical parameter does not suffer from the ambiguity of data preprocessing by equally considering all variables. that means, a distinction is made between continuous variables, ordinal variables with an intrinsic ordering but often lacking equal distance between two consecutive categories and nominal variables without any ordering. they concluded that the clinical kernel functions represent similarities between patients more accurately than linear or polynomial kernel function for modeling clinical data. pittman et al.  <cit>  combined clinical and expression data for predicting breast cancer outcome by means of a tree classifier. this tree classifier was trained using meta-genes and/or clinical data as inputs. they explained that key metagenes can up to to a degree, replace traditional risk factors in terms of individual association with recurrences. but the combination of metagenes and clinical factors currently defines models most relevant in terms of statistical fit and also, more practically, in terms of cross-validation predictive accuracy. the resulting tree models provide an integrated clinico-genomic analysis that generate substantially accurate and cross-validated predictions at the individual patient level.

singular value decomposition  and generalized svd  have been shown to have great potential within bioinformatics for extracting common information from data sets such as genomics and proteomics data  <cit> . several studies have used ls-svm as a prediction tool, especially in microarray analysis  <cit> .

in this paper, we propose a machine learning approach for data integration: a weighted ls-svm classifier. initially we will explain generalized eigenvalue decomposition  and kernel gevd. later we will explore the relationships of kernel gevd with weighted ls-svm classifier. finally, the advantages of this new classifier will be demonstrated on five breast cancer case studies, for which expression data and an extensive collection of clinical data are publicly available.

data sets
breast cancer is one of the most extensively studied cancer types for which many microarray data sets are publicly available. among them, we selected five cases for which a sufficient number of clinical parameters were available  <cit> . all the data sets that we have used are available in the integrated tumor transcriptome array and clinical data analysis database . overview of all the data sets are given in table  <dig> summary of the  <dig> breast cancer data sets



case study
#
samples
#
genes
#
clinical variables
class 1
class2


microarray data
for the first three data sets, the microarray data were obtained with the affymetrix technology and preprocessed with mas <dig> , the genechip microarray analysis suite  <dig>  software . however, as probe selection for the affymetrix gene chips relied on earlier genome and transcriptome annotation that are significantly different from current knowledge, an updated array annotation was used for the conversion of probes to entrez gene ids, lowering the number of false positives  <cit> .

a fourth data set consists of two groups of patients  <cit> . the first group of patients, the training set, consists of  <dig> patients of which  <dig> patients belonged to the poor prognosis group and  <dig> patients belonged to the good prognosis group. the second group of patients, the test set, consists of  <dig> patients of which  <dig> patients belonged to the poor prognosis group and  <dig> patients belonged to the good prognosis group. the microarray data was already background corrected, normalized and log-transformed. preprocessing step removes genes with small profile variance, less than the 10th percentile.

the last data sets consists of transcript profiles of  <dig> primary breast tumors were assessed by using affymetrix u <dig> oligonucleotide microarrays. cdna sequence analysis revealed that  <dig> of these tumors had p <dig> mutations resulting in protein-level changes, whereas the remaining  <dig> tumors were p <dig> wt  <cit> .

clinical data
the first data of  <dig> patients contained information on  <dig> available clinical variables,  <dig> were excluded  <cit> : two redundant variables that were least informative based on univariate analysis in those variable pairs with a correlation coefficient exceeding  <dig> , and three variables with too many missing values. after exclusion of patients with missing clinical information, this data set consisted of  <dig> patients remained in  <dig> of whom disease did not recur whilst in  <dig> patients disease recurred.

the second data in which response to treatment was studied, consisted of  <dig> variables for  <dig> patients  <cit> . patient and variable exclusion as described above resulted in this data set. of the  <dig> remaining patients,  <dig> showed complete response to treatment while  <dig> patients were characterized as having residual disease.

in the third data, relapse was studied in  <dig> patients  <cit> . after preprocessing, this data set retained information on  <dig> variables for  <dig> patients. in  <dig> patients, no relapse occurred while  <dig> patients were characterized as having a relapse.

the fourth data  <cit>  consisted of predefined training and test sets same as that of corresponding microarray data. the last data set consisted of  <dig> patients with  <dig> available clinical variables  <cit> . after exclusion of patients with missing clinical information, this data set consisted of  <dig> patients of which  <dig> patients with p <dig> mutant breast tumor and the remaining patients without p <dig> mutant breast tumor.

methods
in the first section, we will discuss about gevd and represent it in terms of ordinary evd. then an overview of ls-svm formulation to kernel pca and least squares support vector machines  will be given. next, we formulate an optimization problem for kernel gevd in primal space and solution in dual space. finally, by generalizing this optimization problem in terms of ls-svm classifier, we propose a new machine learning approach for data fusion and classifications, a weighted ls-svm classifier.

generalized eigenvalue decomposition
the generalized singular value decomposition  of m×n matrix a and p×n matrix b is  <cit>  
  a=uΣaxt 

  b=vΣbxt 

where u, v are orthogonal matrices and columns of x are generalized singular vectors.

if btb is invertible, then the gevd of ata and btb can be obtained from equations  and  as follows: 
  ataxt−1=btbxt−1Λ. 

where Λ is a diagonal matrix with diagonal entries Λii=ΣaiiΣbii <dig>  i= <dig> …,n.

equation  can be represented in eigenvalue decomposition  as follows: 
 btb−1/2atabtb−1/2u=uΛ. 

where u=1/2− <dig>  the svd of matrix a−1/ <dig> is given below: 
  abtb−1/2=vΛut. 

the matrix −1/ <dig> is defined  <cit>  as follows: let evd of btb=tΣtt, where columns of t are eigenvectors and Σ is a diagonal matrix. 1/2=tΣ1/2tt and −1/2=tqtt, where q is a diagonal matrix with diagonal entries qii=−1/ <dig>  i= <dig> …,n.

ls-svm formulation to kernel pca
an ls-svm approach to kernel pca was introduced in  <cit> . this approach showed that kernel pca is the dual solution to a primal optimization problem formulated in a kernel induced feature space. given training set{xi}i=1n, xi∈ℝd, the ls-svm approach to kernel pca is formulated in the primal as: 
 minw,ei,bjpw,ei=γ2∑i=1nei2−12wtw, 

such that ei=wtφ+b, i= <dig> …,n, where b is a bias term and φ: ℝd→ℝdh is the feature map which maps the d-dimensional input vector x from the input space to the dh-dimensional feature space.

kernel pca in dual space takes the form: 
 Ωcα=λα  where α is an eigenvector, λ an eigenvalue and Ωc denotes the centered kernel matrix with ijth entry: Ωc,i,j=kxi,xj−1n∑r=1nkxi,xr−1n∑r=1nkxj,xr+1n2∑r=1n∑s=1nkxr,xs, with k=φtφ a positive definite kernel function.

least squares support vector machine classifiers
a kernel algorithm for supervised classification is the svm developed by vapnik  <cit>  and others. contrary to most other classification methods and due to the way data are represented through kernels, svms can tackle high dimensional data . given a training set {xi,yi}i=1n with input data xi∈ℝd and corresponding binary class labels yi∈{− <dig> +1}, the svm forms a linear discriminant boundary y=sign in the feature space with maximum distance between samples of the two considered classes, with w representing the weights for the data items in the feature space, b the bias term and φ: ℝd→ℝn <dig> is the feature map which maps the d-dimensional input vector x from the input space to the n1-dimensional feature space. this corresponds to a non-linear discriminant function in the original input space. vapnik’s svm classifier formulation was modified in  <cit> . this modified version is much faster for classification because a linear system instead of a quadratic programming problem needs to be solved.

the constrained optimization problem for least squares support vector machine   <cit>  for classification are defined as follows: 
 minw,b,e12wtw+γ12Σi=1nei <dig> 

subject to: 
 yiwtφ+b=1−ei,i= <dig> …,n 

with ei the error variables, tolerating misclassifications in cases of overlapping distributions, and γ the regularization parameter, which allows tackling the problem of overfitting. the ls-svm classifier formulation implicitly correspond to a regression interpretation with binary target yi= ± <dig> 

in the dual space the solution is given by 
 0ytyΩ+iγbβ=01n  with y=t, 1n=t, e=t, β=t, Ωi,j=yiyjk where k is the kernel function.

the classifier in the dual space takes the form 
 y=sign∑i=1nβiyikx,xi+b 

where βi are lagrange multipliers.

ls-svm and kernel gevd
ls-svm formulations to different problems were discussed in  <cit> . this class of kernel machines emphasizes primal-dual interpretations in the context of constrained optimization problems. in this section we discuss ls-svm formulations to kernel gevd, which is a non-linear gevd of m×n matrix a, and p×n matrix b, and a weighted ls-svm classifier.

given a training data set of n points d=xi,xi,yii=1n with output data yi∈ℝ and input data sets xi∈ℝm, xi∈ℝp  and xi are the ith sample of matrices a and b respectively).

consider the feature maps φ :ℝm →ℝn <dig> and φ: ℝp →ℝn <dig> to a high dimensional feature space , which is possibly infinite dimensional. the centered feature matrices Φc∈ℝn1×n, Φc∈ℝn2×n become 
 Φc=φx1t−μ^t;…;φxnt−μ^tt 

 Φc=φx1t−μ^t;…;φxnt−μ^tt, 

where μ^φl=1nΣi=1nφxi, l= <dig> 

ls-svm approach to kernel gevd
kernel gevd is a nonlinear extension of gevd, in which data are first embedded into a high dimensional feature space introduced by kernel and then linear gevd is applied. while considering the matrix a−1/ <dig> in equation  and the feature maps φ :ℝm →ℝn <dig> and φ :ℝp →ℝn <dig> described in previous section, the covariance matrix of a−1/ <dig> in the feature space becomes c≈ΦcΦctΦc−1Φct with eigendecomposition cv=λv.

while considering kernel pca formulation based on the ls-svm framework  <cit>  was discussed in section ‘ls-svm formulation to kernel pca’ and evd of cv=λv in primal space, our objective is to find the directions in which projected variables have maximal variance.

the ls-svm approach to kernel gevd is formulated as follows: 
  minv,ej=γ12etΦctΦc−1e−12vtvsuchthate=Φctv, 

where v is the eigenvector in the primal space, γ∈ℝ+ is a regularization constant and e are the projected data points to the target space.

defining the lagrangian 
 ℒ=γ2ete−12vtv−αte−Φctv,  with optimality conditions, 
 ∂ℒ∂v=0→v=Φcα  ∂ℒ∂e=0→α=γΦctΦc−1e  ∂ℒ∂αi=0→e=Φctv,  elimination of v and e will yield an equation in the form of gevd 
 Ωcα=λΩcα, 

where λ=1γ largest eigenvalue, Ωc, Ωc are centered kernel matrices and α are generalized eigenvectors. the symmetric kernel matrices Ωc and Ωc resolves the heterogeneities of clinical and microarray data by the use of kernel trick, where data which have diverse data structures are transformed into kernel matrices with same size.

in a special case of gevd, if one of the data matrix is identity matrix, it will be equivalent to ordinary evd. if ΦctΦc−1=i, then the optimization problem proposed for kernel gevd ) will be equivalent to optimization problem in  <cit>  for the ls-svm approach to kernel pca.

weighted ls-svm classifier
our objective is to represent kernel gevd in the form of weighted ls-svm classifier. given the link between ls-svm approach to kernel gevd in equation  and the weighted ls-svm classifier , one considers the following optimization problem in primal weight space: 
 minv,e,bj=γ12etΦctΦc−1e+12vtvsuchthaty=Φctv+b1n+e,  with e=t a vector of variables to tolerate misclassifications, weight vector v in primal weight space, bias term b and regularization parameter γ∈ℝ+. compared to the constrained optimization problem for least squares support vector machine   <cit> , in this case, the error variables are weighted with a matrix ΦctΦc−1/ <dig> 

the weight vector v can be infinite dimensional, which makes the calculation of v impossible in general. one defines the lagrangian ℒv,e,b;α=12vtv+γ2etΦctΦc−1e−αtΦctv+b1n+e−y, with lagrange multipliers α∈ℝn. ∂ℒ∂v=0→v=Φcα  ∂ℒ∂b=0→1ntα= <dig>  ∂ℒ∂e=0→α=γtΦc)−1e  ∂ℒ∂αi=0→e+Φctv+b=y 

elimination of v and e yields a linear system 
  01nt1nΩc+γ−1Ωcbα=0y 

with y=t, 1n=t, α=t, Ωc=ΦctΦc and Ωc=ΦctΦc.

the resulting classifier in the dual space is given by 
  y=∑i=1nαik+1γk+b 

with αi are the lagrange multipliers, γ is a regularization parameter has chosen by user, k=φtφ, k=φtφ and y is the output corresponding to validation point x. the ls-svm for nonlinear function estimation in  <cit>  is similar to the proposed weighted ls-svm classifier.

the symmetric, kernel matrices k and k resolve the heterogeneities of clinical and microarray data sources such that they can be merged additively as a single kernel. the optimization algorithm for the weighted ls-svm classifier is given below:

algorithm: optimization algorithm for the weighted ls-svm classifiergiven a training data set of n points d=xi,xi,yii=1n with output data yi∈ℝ and input data sets xi∈ℝm, xi∈ℝp.

calculate leave-one-out cross validation  performances of training set with different combinations of γ and σ <dig> σ <dig> , k) by solving linear system equation  and equation . in case the leave-one-out  approach is computationally expensive, one could replace it with a leave p group out strategy .

obtain the optimal parameters combinations  which have the highest loo-cv performance.



the proposed optimization problem is similar to the the weighted ls-svm formulation in  <cit>  which replaced ΦctΦc− <dig> with a diagonal matrix to achieve sparseness and robustness.

the proposed method is a new machine learning approach in data fusion and subsequent classifications. in this study, the advantages of a weighted ls-svm classifier were explored, by designing a clinical classifier. this clinical classifier combined kernels by weighting kernel inner product from one data set with that from the other data set. here we considered microarray kernels as weighting matrix for clinical kernels. in each of these case studies, we compared the prediction performance of individual data sets with gevd, kernel gevd and weighted ls-svm classifier. in kernel gevd, σ <dig> and σ <dig> are the bandwidth of rbf-kernel function k=exp−||x−z||22σ <dig> of clinical and microarray data sets respectively. these parameters were chosen such that the pairs  which obtained the highest loo-cv performance. the parameter selection  for the weighted ls-svm classifier are illustrated in figure  <dig>  for several possible values of the kernel parameters σ <dig> and σ <dig>  the loo cross validation performance is computed for each possible combinations of γ. the optimal parameters are the combinations  with best loo-cv performance. remark the complexity of this optimization procedure because both the kernel parameters  and γ need to be optimized in the sense of the loo-cv performance.overview of algorithm. the data sets represented as matrices with rows corresponding to patients and columns corresponding to genes and clinical parameters respectively for first and second data sets. loo-cv is applied to select the optimal parameters.



RESULTS
in all case studies except fourth, 2/3rd of the data samples of each class are assigned randomly to the training and the rest to the test set. these randomization are the same for all numerical experiments on all data sets. this split was performed stratified to ensure that the relative proportion of outcomes sampled in both training and test set was similar to the original proportion in the full data set. in all these cases, the microarray data were standardized to zero mean and unit variance. normalization of training sets as well as test sets are done by using the mean and standard deviation of each gene expression profile of the training sets. in the fourth data set  <cit> , all data samples have already been assigned to a training set or test set.

initially ls-svm classifiers have been applied on individual data sets: clinical and microarray. then we performed gevd on training samples of clinical and microarray data sets and obtained generalized eigenvectors . scores are obtained by projecting the clinical data on to the directions of gev. ls-svm model is trained and validated on scores corresponding to training set and test set respectively.

kernel gevd
the optimal parameters of the kernel gevd  are selected using loo-cv performance. we applied kernel gevd on microarray and clinical kernels. then we obtained the scores by projecting clinical kernels on to the direction of kernel gev. similar to gevd, ls-svm model is trained and validated on scores corresponding to training set and test set respectively. high-throughput data such as microarray have used only for the model development. the results show that considerations of two data sets in a single framework improve the prediction performance than individual data sets. in addition, kernel gevd significantly improve the classification performance over gevd. the results of the five case studies are shown in table  <dig> and figure  <dig>  we represent expression and clinical data with kernel matrix, based on rbf kernel function. the rbf kernel functions makes each of the these data which has diverse structures, transformed into kernel matrices with same size.comparison of the prediction accuracy of the classifiers. boxplots of the test auc values obtained in  <dig> repetitions for  <dig> breast cancer cases.  case i  case ii  case iii  case iv  case v.
comparisons of different classifiers : test auc of breast cancer cases


case i
case ii
case iii
case iv
case v

classifiers
p-value: a paired test, wilcoxon signed rank test.

cl and ma are the clinical and microarray kernels of rbf kernel functions.



weighted ls-svm classifier
we proposed a weighted ls-svm classifier, a useful technique in data fusion as well as in supervised learning. the parameters  and σ <dig>  σ <dig> the bandwidths of microarray and clinical kernel functions) associated with this method are selected by algorithm. in each loo-cv,  <dig> - samples are left out and models are built for all possible combinations of parameters on the remaining n− <dig> samples. the optimization problem is not sensitive to small changes of bandwidths of microarray and clinical kernel functions. careful tuning of γ allows tackling the problem of overfitting and tolerating misclassifications. models parameters are chosen corresponding to the model with highest loo auc. the loo-cv approach takes less than a minute for a single iteration of the first three case studies and 1- <dig> minutes for the rest of case studies. statistical significance test are performed in order to allow a correct interpretations of the results. a non-parametric paired test, the wilcoxon signed rank test   <cit> , has been used in order to make general conclusions. a threshold of  <dig>  is respected, which means that two results are significantly different if the value of the wilcoxon signed rank test applied to both of them is lower than  <dig> . on all case studies, weighted ls-svm classifier outperformed all other discussed methods, in terms of test auc, as shown in table  <dig> and figure  <dig>  the weighted ls-svm performance on second and fourth cases slightly better, but not significantly, than the kernel gevd.

to compare ls-svm with other classification methods, we have applied naive bayes classifiers individually to clinical and microarray data. in this case, the normal distribution were used to model continuous variables, while ordinal and nominal variables were modeled with a multivariate multinomial distribution. the average test auc of this method, when applied on five case studies are shown in table  <dig> naive bayes classifiers performance on clinical and microarray data sets in terms of test auc



data source
case i
case ii
case iii
case iv
case v


then we compare the proposed weighted ls-svm classifiers with other data fusion techniques which integrate microarray and clinical data sets. daemen et al.  <cit>  investigated the effect of data integration on performance with three case studies  <cit> . they reported that a better performance was obtained when considering both clinical and microarray data with the weights  assigned to them optimized ). in addition they concluded from their 10-fold auc measurements that the clinical kernel variant, led to a significant increase in performance, in the kernel based integration approach of clinical and microarray. the first three case studies, we have taken from the work of daemen et al.  <cit> . they have considered the  <dig> most differential genes selected from the training data with the wilcoxon rank sum test, for the kernel matrix obtained from microarray. the fourth case study, we have taken from the paper of gevaert et al.  <cit>  in which they investigated different types of integration strategies, with bayesian network classifier. they concluded that partial integration performs better in terms of test auc. our results also confirms that consideration of microarray and clinical data sets together, improves prediction performances than individual data sets.

in our analysis, microarray-based kernel matrix are calculated on large data set without preselecting genes and thus avoiding potential selection bias  <cit> . in addition, we compared rbf kernel with the clinical kernel function  <cit>  on weighted ls-svm classifier, in terms of loo-cv performance. results are given on table  <dig>  we followed the same strategy which was explained for weighted ls-svm classifier, except the clinical kernel function have been used for the clinical parameters. on three out of five case studies, rbf kernel functions performs better than clinical kernel function.comparisons of rbf with clinical kernel functions in terms of loo-cv performances



kernel functions
case i
case ii
case iii
case iv
case v


discussion
integrative analysis has been primarily used to prioritize disease genes or chromosomal regions for experimental testing, to discover disease subtypes or to predict patient survival or other clinical variables. the ultimate goal of this work is to propose a machine learning approach which is functional in both data fusion and supervised learning. we further analyzed the potential benefits of merging microarray and clinical data sets for prognostic application in breast cancer diagnosis.

we integrate microarray and clinical data into one mathematical model, for the development of highly homogeneous classifiers in clinical decision support. for this purpose, we present a kernel based integration framework in which each data set is transformed into a kernel matrix. integration occurs on this kernel level without referring back to the data. some studies  <cit>  already reported that intermediate integration of clinical and microarray data sets improves prediction performance on breast cancer outcome. in primal space, the clinical classifier is weighted with expression values. the solution in dual space is given on equations  and  which provides a way to integrate two kernel functions explicitly and perform further classifications.

to verify the merit of the proposed approach over the single data sources such as clinical and microarray data, the ls-svm were built on all data sets individually for classifying cancer patients. next, gevd and kernel gevd are performed. then the projected variances in the new space  have used to build the ls-svm. finally weighted ls-svm approach was used for the integration of both microarray and clinical kernel functions and performed subsequent classifications. thus weighted ls-svm classifier proposes a new optimization framework to solve the problem of classification using features of different types such as clinical and microarray data.

we should note that the models proposed in this paper are expensive, but less than the other kernel-based data fusion techniques. since the proposed weighted ls-svm classifier simplified both data fusion and classification in a single framework, it does not have an additional cost for tuning parameters for kernel-based classifiers. and it is given that, the weighting matrix should be invertible in the optimization problem of kernel gevd and the weighted ls-svm classifier.

in life science research, there is an increasing need for heterogeneous data integration such as proteomics, genomics, mass spectral imaging and so on. such studies are required to determine, which data sets are most significant to be considered as weighting matrix. the proposed weighted ls-svm classifier integrates heterogeneous data sets to achieve good performing and affordable classifiers.

CONCLUSIONS
the results suggest that the use of our integration approach on gene expression and clinical data can improve the performance of decision making in cancer. we proposed a weighted ls-svm classifier for the integration of two data sources and further prediction task. each data set is represented with a kernel matrix, based on the rbf kernel function. the proposed clinical classifier gives a step towards improving predictions for individual patients about prognosis, metastatic phenotype and therapy responses.

because the parameters  had to be optimized, all possible combinations of these parameters were investigated with a loo-cv. since these parameters optimization strategy is time consuming, one can further investigate a parameter optimization criterion for kernel gevd and weighted ls-svm.

the applications of proposed method are not limited to clinical and expression data sets. possible additional applications of weighted ls-svm include integration of genomic information collected from different sources and biological processes. in short, the proposed machine learning approach is a promising mathematical framework in both data fusion and non-linear classification problems.

competing interests

the authors declare that they have no competing interests.

authors’ contributions

mt performed the kernel based data integration modeling and drafted the paper. kdb and js participated in the design and implementation of framework. kdb, js and bdm helped draft the manuscript. all authors read and approved the final manuscript.

