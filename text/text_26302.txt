BACKGROUND
the alignment of two biological sequences is a fundamental operation that forms part of many bioinformatics applications, including sequence database searching, multiple sequence alignment, genome assembly, and short read mapping.

smith and waterman  <cit>  described a simple and general algorithm requiring o time and o memory to identify the optimal local sequence alignment score using a substitution score matrix and a general gap penalty function. gotoh  <cit>  showed that with affine gap penalties the optimal local alignment score could be computed in just o time and o memory.

when the optimal alignment score needs to be computed many times, for example when searching a sequence database, the computation time becomes substantial. several approaches have been pursued to reduce the time needed. heuristic approaches like blast  <cit>  are considerably faster, but are not guaranteed to discover the optimal alignment.

reconfigurable hardware in the form of fpga  can also accelerate the speed of alignment score computations. li et al <cit> . reported speeds equivalent to about  <dig>  billion cell updates per second  with dna sequences, on a state-of-the-art fpga board. search speed is often reported in gcups, which indicates the billion  number of cells in the alignment matrix , processed per second.

the algorithm can also be implemented with various forms of parallelisation in software running on more common hardware. pairwise alignment of separate sequences is in principle "embarrassingly" parallel because the computations for each pair of sequence are completely independent. alpern et al <cit> . suggested improving speed by performing several independent alignment score computations in parallel by dividing the bits of wide registers into several narrower units and using instructions to perform arithmetic operations on these units individually. this form of parallelism within a register was later made much simpler and easier by microprocessor manufacturers with the introduction of technologies like mmx, sse, sse <dig>  max, mvi, vis, and altivec, which are now generally referred to as simd technology. several implementations take advantage of the sse <dig> instructions available on intel processors  <cit> . the approach where parallelisation is carried out across multiple database sequences is also known as inter-task parallelisation, in contrast to intra-task parallelisation where the parallelisation occurs within a single pair of sequences.

efforts have since mostly concentrated on parallelisation within a single alignment of one pair of sequences. figure  <dig> illustrates the main approaches. wozniak  <cit>  suggested computing cells along the minor diagonal in the alignment matrix in parallel because these calculations are independent. rognes and seeberg  <cit>  found that using cells along the query sequence was faster despite some data dependences, because loading values along the minor diagonal was too complicated. farrar  <cit>  introduced a "striped" approach where computations were carried out in parallel in several separate stripes covering different parts of the query sequence to reduce the impact of some of the computational dependencies. farrar's striped approach is generally the fastest, and he has reported speeds of more than  <dig> and  <dig> gcups on four and eight cores, respectively  <cit> . szalkowski et al <cit> . described the swps <dig> implementation which uses the striped approach of farrar, claiming speeds of up to  <dig>  gcups on a quad-core processor. the performance was highly dependent on query length though. using the p <dig> query sequence, with a typical protein length of  <dig> residues, swps <dig> performance was roughly  <dig> gcups.

the cell processor manufactured by sony, toshiba and ibm, have one main core  and  <dig> minor cores . these cores have simd vector processing capabilities. cell processors are found in some ibm servers  as well as the sony playstation  <dig>  . several implementations for this processor have been described. swps <dig> by szalkowski et al <cit> . used the farrar approach and claimed speeds on the ps <dig> of up to  <dig> gcups. also here the performance was highly dependent on query length. with the p <dig> query sequence, swps <dig> performance was less than  <dig> gcups on the ps <dig>  wirawan et al <cit> . also employed the farrar approach in their implementation called cbesw and claimed speeds on the ps <dig> of over  <dig>  gcups, while the performance was  <dig>  gcups with the p <dig> query sequence. farrar also reported speeds of  <dig>  gcups on an ibm qs <dig> and up to  <dig>  gcups on the ps <dig> using the same query  <cit> . rudnicki et al <cit> . described an implementation that used parallelisation over multiple database sequences on the ps <dig> and reported speeds approaching  <dig> gcups.

graphics processors  can also accelerate alignments. several implementations have employed the cuda interface to nvidia gpus. the cudasw++ tool by liu et al <cit> . reportedly performed  <dig>  gcups on the single-gpu geforce gtx  <dig> and  <dig>  gcups on the dual-gpu geforce gtx  <dig>  ligowski and rudnicki  <cit>  reported speeds of up to  <dig>  gcups on the dual-gpu geforce  <dig> gx <dig>  in  <dig>  liu et al <cit> . reported speeds up to  <dig> and  <dig> gcups by cudasw++  <dig>  on the single-gpu geforce gtx  <dig> and the dual-gpu geforce gtx  <dig>  respectively. recently, ligowski et al <cit> . reported a speed of  <dig>  gcups on the geforce gtx  <dig> 

the main advantage of the inter-task parallelisation approach where multiple database sequences are processed in parallel as described by alpern et al <cit> . is that it simply avoids all data dependences within the alignment matrix. this approach does not seem to have been explored much in later implementations using simd technology apart from the work by rudnicki et al <cit> . however, in the gpu-based tools  <cit>  this approach is common. the aim of the present study was to explore the use of this approach further using simd on ordinary cpus.

here the algorithm is implemented on intel processors with ssse <dig>  <cit>  with parallelisation over multiple database sequences as illustrated in figure 1d. instead of aligning one database sequence against the query sequence at a time, residues from multiple database sequences are retrieved and processed in parallel. rapid extraction and organisation of data from the database sequences have made this approach feasible. the approach has been implemented in a tool called swipe. the approach also involves computing four consecutive cells along the database sequences before proceeding to the next query residue in order to reduce the number of memory accesses needed.

the performance of the new implementation has been extensively tested using different scoring matrices, gap penalties, query sequences and number of threads. the speed of swipe was almost constant at more than  <dig> gcups on a dual intel xeon x <dig> six-core processor system for a wide range of query lengths. swipe was about six times faster than swps <dig> and farrar's own implementation for a typical length query, but the factor varied between  <dig> and  <dig> depending on query length and number of threads used. two versions of blast were tested and the speed was found to be highly dependent on the score matrix. swipe was about twice as fast as blast using the blosum <dig> matrix, while blast was about twice as fast as swipe using the blosum <dig> matrix.

methods
benchmarking
benchmarking was performed on compute nodes in the titan high performance cluster at the university of oslo. entire nodes were reserved to ensure that no other major processes were running. all data was initially copied to a fast local disk to reduce the influence of the computer networks and minimize file reading time. the output from all programs was redirected to/dev/null to minimize performance differences due to the amount of output. all combinations of programs, number of threads, query sequences, score matrices, gap open penalties and gap extension penalties were run  <dig> times and the median total wall clock execution time was recorded.

software
-num_descriptions  <dig> -num_threads $t -matrix $m
command line variables: threads , score matrix file name , gap open  and extension  penalties , query file name , database file basename 

swipe was written mainly in c++ with some parts hand coded in inline assembler and some using sse <dig> and ssse <dig> intrinsics. it was compiled for  <dig> bit linux using the intel c++ compiler version  <dig> . source code is available at http://dna.uio.no/swipe/ under the gnu affero general public license, version  <dig>  an executable binary and score matrix files are also available at the same location. the same files are included in a gzipped tar archive as additional file  <dig> 

the source code for farrar's striped software was downloaded from the author's website and compiled with the gnu gcc compiler as specified in the supplied makefile  <cit> . the makefile was also modified to compile striped using the intel compiler, to see if there were any differences in performance.

the binary executable for the swps <dig> program was downloaded from the authors' website and used directly  <cit> .

precompiled binaries for blast and blast+ were downloaded from the ncbi ftp site  <cit> .

graphs were drawn using gnuplot version  <dig> .

hardware
performance tests were carried out on dell poweredge m <dig> blade servers with  <dig> gb ram and dual intel xeon x <dig> six-core processors running at  <dig>  ghz. the x <dig> processors have simultaneous multithreading capability also known as hyper-threading . with ht enabled, each of these computers has a total of  <dig> logical cores.

threads
the programs were run using  <dig> to  <dig> threads. to take full advantage of the hardware a number of threads equal to the number of logical cores is usually the most appropriate. there are however important differences between software on the effect of ht and in the ability to make efficient use of the cores available. to simplify comparisons, some of the tests were only performed with  <dig> or  <dig> threads.

database sequences
uniprot knowledgebase release  <dig>   <cit>  consisting of both swiss-prot release  <dig>  and trembl release  <dig>  of  <dig> may  <dig> was used for the performance tests. this database consists of  <dig>  <dig>  <dig> protein sequences with a total of  <dig>  <dig>  <dig>  <dig> amino acid residues. the longest sequence contains  <dig>  <dig> residues.

this database was chosen because the size was large enough to be realistic but smaller than the apparent  <dig> gb file size limit of some software. the database also did not contain any of the special j, o or u amino acid residue symbols that some of the software could not handle. finally, this release of the database should be available for download in the foreseeable future, making it suitable also for future benchmarking.

the current version of swipe will not work with databases split by formatdb into separate volumes. swipe therefore cannot search databases larger than about  <dig> billion amino acids.

the database was converted into fasta format by a simple perl script and then formatted with ncbi formatdb version  <dig> . <dig> into the ncbi blast binary database format  <cit> . this binary format was read by swipe, blast and blast+, while striped and swps <dig> read the fasta-formatted database file directly.

query sequences
the  <dig> query sequences with accession numbers p <dig>  o <dig>  p <dig>  p <dig>  p <dig>  p <dig>  p <dig>  p <dig>  p <dig>  q8zgb <dig>  p <dig> , p <dig>  p <dig>  p <dig>  p <dig>  p <dig>  p <dig>  p <dig>  p <dig>  q <dig>  o <dig>  p <dig>  p <dig>  p <dig>  p <dig>  p <dig>  p0c6b <dig>  p <dig>  p <dig>  q7tma <dig>  p <dig> and q9ukn <dig>  ranging in length from  <dig> to  <dig> residues were retrieved from the uniprot database  <cit> . most of them have previously been used several times for performance testing. to simplify comparisons, some of the tests were only performed with the  <dig> residues long p <dig> query, representing a protein of roughly average length.

score matrices and gap penalties
all  <dig> different combinations of amino acid substitution score matrices and gap penalties accepted by blast were tested. the matrices used were blosum <dig>  blosum <dig>  blosum <dig>  blosum <dig>  and blosum <dig> from the blosum series  <cit>  as well as pam <dig>  pam <dig>  and pam <dig> from the pam series  <cit> . matrices were obtained from the ncbi ftp site. rows and columns for stop codons  were removed from the matrices for compatibility with the swps <dig> program. swps <dig> would only run successfully using the blosum <dig>   <dig> and  <dig> matrices. to simplify comparisons, some of the tests were only performed with the blosum <dig> matrix and gap open and extension penalties of  <dig> and  <dig>  respectively, which is the blast default.

RESULTS
algorithm
the optimal local alignment score of two sequences can be computed using a dynamic programming approach. the recurrence relations for the algorithm of smith and waterman  <cit>  with the modifications of gotoh  <cit>  for affine gap penalty functions are shown below.            

the query sequence q of length m contains residues qi . the database sequence d of length n contains residues dj . hi,j is the score for aligning the prefixes of q and d ending in the alignment of residues qi and dj . ei,j and fi,j are the scores of aligning the same prefixes of q and d but ending with a gap in the query and database sequence, respectively. p is the score of aligning residues qi and dj with each other according to the substitution score matrix p. q is the sum of gap open and extension penalties while r is the gap extension gap penalty. s is the overall optimal local alignment score.

the calculations are carried out column by column. only parts of the h, e and f matrices need to be kept in memory: a single element of the f matrix as well as two arrays containing m elements each, corresponding to one column of the h and e matrices.

implementation
the main features of the implementation are described below.

parallelisation over sixteen database sequences
residues from  <dig> different database sequences are processed in parallel as indicated in figure 1d. these  <dig> residues are all simultaneously compared to the same query residue. the operations are carried out using vectors consisting of  <dig> independent bytes. the  <dig> residues are fed into sixteen independent channels. when the first of these sixteen database sequences ends, the first residue of the next database sequence is loaded into the channel. the databases sequences are read in the order they are found in the original database file. in contrast to the approach by rudnicki et al.  <cit> , the database is not sorted by sequence length. figure  <dig> illustrates this approach.

compact core code of ten instructions
the basis for the computations of the values in each cell in the alignment matrices are the recurrence relations described in the algorithm section. the computations can be written in as little as ten assembly instructions that constitute the core of the inner loop of the computations, as shown in figure  <dig>  these ten instructions compute in parallel the values for each vector of  <dig> cells in independent alignment matrices. the exact selection of instructions and their order is important; this part of the code was therefore hand coded in assembler to maximise performance. in the figure, h represents the main score vector. the h vector is saved in the n vector for the next cell on the diagonal. e and f represent the score vectors for alignments ending in a gap in the query and database sequence, respectively. p is the vector of substitution scores for the database sequences versus the query residue q . q represents the vector of gap open plus gap extension penalty. r represents the gap extension penalty vector. s represents the current best score vector. all vectors, except n are initialised prior to this code.

processing four consecutive cells along the database sequences
during the computations of the matrix cells, the values in the two arrays with the h and e values usually have to be read and written once for each matrix cell. these arrays are usually small enough to be cached at a close cache level, so the memory access time should not be a major concern, but they still need to be written and read back for each cell. since there are sixteen 128-bit xxm registers available and ample space for keeping the h, e and f values of a few cells in the registers, it is possible to reduce running time somewhat by computing a few consecutive cells along the database sequences before moving on to the next query residue. four consecutive cells was found to perform well. unrolling the inner loop once along the query sequence was also found to work well. the basic computing blocks then consist of two times four cells that are processed in each inner loop iteration as shown in figure  <dig> 

updating scores and padding blocks
when a new database sequence begins in one of the channels, the score of the previous database sequence must be recorded. in addition, the h, e and s scores from the previous sequence must be reset. when a new column is going to be processed, it is checked whether any database sequence ended in the previous column. if that is the case, special processing is carried out. the score of the sequences that ended are recorded. a mask is created and later used to reset the values of h, e and s in the appropriate channels before the new column is processed. the channels are filled, and one or more new sequences are started. a somewhat simpler and faster processing step is carried out for the new column if no sequences ended in the previous column. in the simpler processing step no scores need to be saved, no new sequences are started, and no mask need to be created or used. since most columns are of the simpler type the overall performance is mostly dependent on the speed of processing the simple columns. to simplify computations, each channel is padded with 1- <dig> null symbols after the end of a sequence if its length is not a multiple of  <dig>  to ensure that a new database sequence will only begin at the beginning of a new block of cells. this padding is indicated in pink in figure  <dig>  the padding increases the total number of cell a little bit, but allows the checks described above to be carried out only on every fourth residue.

computation of temporary score profiles
to make computations fast, it is essential that the vectors of substitution score values can be loaded quickly. each score vector corresponds to the score of a single query residue against  <dig> residues from different database sequences. a kind of temporary score profile is created as outlined in figure  <dig>  this score profile is valid for matching any query residue with  <dig> successive residues from  <dig> database sequences. for every fourth residue in the database sequences a new score profile must be constructed.

the temporary score profiles are created from the ordinary substitution score matrices  and the  <dig> Ã—  <dig> database sequence residues using a series of packed shuffle instructions . the shuffle instruction is only available on intel processors with supplemental streaming simd extensions  <dig> . on processors without ssse <dig> , the computations may be replaced by a kind of matrix transpose operation using a series of unpack instructions  with only a modest speed penalty.

score range and selection of arithmetic instructions
computations are initially performed using only a  <dig> bit score range. this allows  <dig> alignment score matrices to be computed in parallel using sse <dig> instructions. additions and subtractions are performed using signed and saturated arithmetics, while the maximum operations are carried out on unsigned numbers. only the  <dig> bit score range from - <dig> to - <dig>  or  <dig> to  <dig>  is used. all scores are biased by an offset of  <dig>  this range of values will ensure that signed saturated addition and subtraction works well on the lower boundary. also, the unsigned maximum works well in this range. the range enables the use of the packed maximum unsigned byte instruction  and packed add and subtract signed saturated bytes instructions , which are available on all sse <dig> processors and gives the highest speed.

an  <dig> bit range, which would allow the same number  of parallel computations as a 7-bit range, and at the same time allow a wider score range, is not used because it is slower. either the range from - <dig> to  <dig>  or the range from  <dig> to  <dig> could be used. there are both signed and unsigned versions of the instructions for parallel computation of the maximum of bytes  and for parallel addition and subtraction of bytes , but the pmaxsb instruction for the maximum of signed bytes was only recently available in sse <dig>  and is slower than pmaxub. unsigned additions and subtractions  may be used, but the addition of the score matrix vector then requires two instructions. first the score vector including a bias must be added; then the bias must be subtracted.

versions using 16-bit and 63-bit score ranges are also implemented and used when overflow is detected in computations with lower score ranges. the 16-bit version allows  <dig> parallel computations. when potential overflow is detected in computations with a narrow score range, the alignment score for that database sequence is recalculated using the next wider score range . because rather few sequences will usually reach a score that cannot be represented by  <dig> bits, the additional computation time for wider score ranges are usually negligible.

recalculations with a wider score range are carried out on a subset of sequences after all sequences in a chunk of database sequences  have been processed using the narrower score range.

reading the sequence database
database sequences were stored in the ncbi blast database format, produced by the formatdb tool. this is a binary format where the sequence information is split into at least  <dig> files: indices , headers  and sequences . the file format allows efficient reading of sequences into memory. protein sequences are stored using byte values in the range 1- <dig> and 26- <dig> representing amino acid residues a-i, k-n, p-t, v-z, u, o and j, respectively. sequences are separated by a zero byte, which simplifies the check for sequence ends.

database sequences are retrieved using memory mapping of the .pin and .psq files. this is an efficient and convenient method of accessing the sequences, in which the operating system manages reading data into memory from disk as necessary, concurrently with program execution. the sequence database is divided into  <dig> chunks per thread. each chunk contains approximately the same number of sequences. the sequences in one chunk are mapped into memory and processed before the next chunk is mapped. this results in a small memory footprint of the program.

multiple threads
swipe uses multiple threads  that work on different parts of the sequence database. the number of threads is specified when starting the program and should in general be equal to the number of cores of the computer. for the latest generations of intel processors with hyper-threading, a number of threads equal to the number of logical cores is usually most effective. chunks of database sequences are assigned to the threads as they are ready for more work, so the threads may not process exactly the same number of chunks each. results from the threads are inserted into a common hit list after each chunk is processed.

testing
the swipe software was benchmarked against blast, blast+, striped and swps <dig> under many different conditions to measure speed. the performance using a variable number of threads and the effect of query sequence length was studied. additionally, the impact of different scoring systems, both substitution score matrices and gap open and extension penalties was examined.

threads
query length
swipe had a rather flat performance curve. for queries shorter than about  <dig> residues there was a gradual loss in performance, especially when running with many threads. the speed was also slightly reduced for very long queries when using many threads. the performance ranged from  <dig>  to  <dig>  gcups for  <dig> threads and from  <dig>  to  <dig>  gcups for a single thread.

the swps <dig> program was very dependent on query length with speeds ranging from  <dig>  to  <dig>  gcups on  <dig> threads and between  <dig>  and  <dig>  gcups on a single thread.

the striped program compiled with the intel compiler was also quite dependent on the query length, in particular when running on  <dig> threads, with speeds ranging from  <dig>  to  <dig>  gcups. on a single thread, the speed of striped varied between  <dig>  and  <dig>  gcups. striped compiled with the gnu compiler was in general slightly slower, particularly with longer queries.

the speed of the blast programs seemed somewhat faster with longer query sequences with speeds ranging from  <dig>  to  <dig>  and  <dig>  to  <dig>  gcups for blast and blast+, respectively, but the performance varied a bit from sequence to sequence. there was a noticeable drop in performance with queries shorter than about  <dig> residues.

scoring systems
discussion
the swipe software greatly increases the speed of sequence database searches based on the smith-waterman algorithm compared to earlier simd implementations, being more than six times faster in realistic cases. swipe was found to be performing at a speed of  <dig> gcups with a  <dig> residue query sequence on a dual intel xeon x <dig> six-core processor system. the speed was a bit dependent on the query length, but independent of the scoring system used. the maximum speed corresponds to the processing of more than  <dig>  cells per physical core in each clock cycle.

using gpus, speeds of up to  <dig>  gcups have been reported for cudasw++  <dig>  on a nvidia geforce gtx  <dig> graphics card  <cit> . this is comparable to the expected performance of swipe on a quad-core cpu.

swipe scaled almost linearly with the number of threads used up to  <dig> threads, corresponding to the number of physical cores available. the x <dig> processors features hyper-threading which makes it possible to obtain extra performance using more than one thread per physical core, unless all execution units of busy. almost no increase in performance beyond  <dig> threads was observed, so apparently the execution units are fairly busy when swipe is running on  <dig> threads. swipe scaled much better than swps <dig> and striped with multiple threads. the maximum speed of swipe was  <dig>  times faster than swps <dig>  the fastest of these, using the  <dig> residue query. when all programs were running on a single thread, swipe was about  <dig>  times faster. there seems to be two equally important factors responsible for the differences in speed. based on the single thread performance numbers, it seems like the use of the inter-sequence parallelisation approach instead of the striped approach is responsible for about half of the increase in speed. efficient thread parallelisation seems responsible for the other half.

in the comparisons with the striped software it should be noted that farrar  apparently reported the "scan time" and not the complete running time for the program, excluding the time needed to read the database sequences into memory. the full running time is reported here. for the other programs, it is difficult to separate the scan time from the rest of the time used. striped reads the fasta formatted files directly and not files formatted by ncbi's formatdb tool. it appears that striped initially parses the entire fasta file and reads the database into memory using non-threaded code, resulting in low performance when using several threads and measuring the complete running time. excluding the time for database reading leads to shorter run times and higher performance numbers. on the other hand, it is probably faster to read files in the binary ncbi format than parsing the fasta text format.

the performance of swipe was not very dependent on query length, except for rather short and very long queries. overhead costs incurred for each database residue probably reduced the performance of swipe for the shortest queries. for the longest queries there was a performance decrease when many threads were running. this is probably due to the effects of memory caches, of which some are shared between cores. the programs based on farrar's approach were considerably more dependent on query length and performed better with increasing query length. the reason for this is probably that as the width of the stripes increase, the relative importance of the dependency between the stripes is reduced.

swipe was about twice as fast as blast using the blosum <dig> matrix, while blast was twice as fast as swipe using the blosum <dig> matrix. blast performance was found to be very dependent on the scoring matrix. the reason may be that blast in its heuristics uses an initial hit score threshold  that has a fixed default value  independent of the score matrix specified. score matrices with relatively high expected values, e.g. blosum <dig>  will then trigger more initial hits than other matrices, e.g. pam <dig> 

if one would like to search using a query profile  instead of a query sequence, the computation of the temporary score profile need to be carried out for every position of the query, not just for the  <dig> possible amino acid residues. this has not been implemented, but the resulting reduction in speed has been estimated to 30%.

the software described here should be considered a prototype indicating the performance potential of the approach. a later version that at least computes the actual alignments  and the statistical significance of the matches is planned.

CONCLUSIONS
efficient parallelisation using simd on standard hardware now allows smith-waterman database searches to run considerably faster than before. in the new tool swipe, residues from sixteen different database sequences are compared in parallel to one query residue. using a  <dig> residue query sequence a speed of  <dig> billion cell updates per second  was achieved on a dual intel xeon x <dig> six-core processor system, which is more than six times faster than software based on farrar's approach, the previous fastest implementation. furthermore, for the first time, the speed of a smith-waterman based search has been shown to clearly exceed that of blast at least with one particular scoring matrix.

since the slow speed has been the major drawback limiting the usefulness of smith-waterman based searches, the approach described here could significantly widen the potential application of such searches. other applications that require optimal local alignment scores, like short read mapping  <cit>  or genome sequence assembly  <cit>  could also benefit from improved performance of this method. the approach used here may probably also be applied to hmm-based searches.

supplementary material
additional file 1
source code. the source code of swipe version  <dig> , as well as a binary executable for 64-bit linux and score matrices are included in this gzipped tar archive file.

click here for file

 acknowledgements
this work was supported by the research council of norway with a coe grant to cmbn. thanks to the research computing services group at the center for information technology at the university of oslo for access to the titan cluster and other services.
