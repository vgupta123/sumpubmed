BACKGROUND
the kidney plays an important role in the maintenance of water and electrolyte balance, and the filtration and elimination of metabolic wastes and drugs from the plasma  <cit> . due to drug exposure and active transport and metabolism of drugs, the kidney is susceptible to drug-induced toxicity  <cit> . nephrotoxic drugs may perturb renal perfusion, induce loss of filtration capacity, and cause damage to the vascular, tubular, glomerular and interstitial cells in the kidney  <cit> . drug-induced nephrotoxicity can lead to acute kidney injury, or chronic kidney disease that may process to end-stage kidney disease  <cit> . however, nephrotoxicity of drug candidates is often detected only during the late phases of drug development, and accounts for 19% of drug attrition in phase  <dig> of clinical trials  <cit> . therefore, early, pre-clinical prediction of nephrotoxicity could help to prioritize drug candidates for further evaluations, increase the success rates of clinical trials, and reduce the overall time and cost of drug development.

renal proximal tubular cells  are a major target for drug-induced toxicity because they are involved in the regulation of filtrate concentration and drug transportation and metabolism  <cit> . current pre-clinical, in vitro nephrotoxicity predictors are usually based on protein- or gene-expression markers of immortalized renal proximal tubular cell lines  <cit> . most of these predictors have only been tested in around ten or less compounds  <cit> . recently, li et al. have developed an in vitro predictor based on the expression levels of two inflammatory markers, interleukin - <dig> and - <dig>  in human primary renal proximal tubular cells   <cit>  and human embryonic stem cell-derived hptc-like cells  <cit> . these markers were tested in a larger number of  <dig> compounds, and gave higher prediction accuracy than many previous predictors  <cit> . however, most of these existing predictors use simple linear thresholds to distinguish between the effects of nephrotoxic and non-nephrotoxic compounds, even though more than one markers  are measured from the cells. these manually-determined thresholds may be subject to human biases, and have difficulties in distinguishing features that are non-linearly separable  <cit> . therefore, we wonder if non-linear decision boundaries identified automatically using supervised classifiers can further improve the accuracy of nephrotoxicity predictors based on the il- <dig> and - <dig> markers.

supervised classifier is a computational algorithm that maps, or classifies, input data into different pre-defined categories based on a set of training data whose category membership is known. the support vector machine  algorithm is one of the most commonly used classifiers. it constructs classification boundaries based on soft margins that allow mis-classified data points, and is especially useful when the data is not linearly separable and/or the number of features is high  <cit> . the k-nearest-neighbor  and naive bayes classifiers are two other commonly used classifiers. in a k-nn classifier, the category membership of a data point is determined by a majority vote of its neighboring training data points  <cit> . naive bayes classifier is based on the bayes' theorem and assumes that the measured features are independent  <cit> . these two classifiers have the advantages of being simple and efficient, especially for low numbers of features  <cit> . finally, the random forest algorithm is a relatively new type of classifier based on ensemble learning of a set of decision trees  <cit> . in certain datasets, random forest may achieve higher classification accuracy than svm  <cit> . despite the popularity of these classifiers, their performances have not been systematically compared and studied under the context of nephrotoxicity prediction.

here, we report a systematic comparison of random forest, svm, k-nn, and naive bayes classifiers in predicting the ptc toxicity of  <dig> well-characterized compounds that are toxic or not toxic to ptc. the prediction is based on the il- <dig> and - <dig> expression levels measured by li et al. in hptcs  <cit> . we describe how the parameters of all the tested classifiers can be automatically determined without any manual intervention. we also compare the importance of il- <dig> and - <dig> in predicting ptc toxicity. finally, we also show that supervised prediction based on the best classifier, random forest, achieves higher accuracy than the threshold-based classifier used by li et al.  <cit> .

methods
dataset
we used the gene expression dataset generated by li et al.  <cit> . the dataset was collected from hptcs derived from three different human donors . the cells were exposed to  <dig> compounds for  <dig> hours, and the expression levels of il- <dig> and - <dig> were determined using quantitative polymerase chain reaction . the measured values were then averaged across three experimental replicates and divided by the vehicle control values . for each batch of hptcs, we obtained a final 41x <dig> floating point data matrix and presented it to each of the tested classifiers as described below . il- <dig> and - <dig> were selected because their expression levels are substantial increased in injured or diseased kidneys  <cit> . the  <dig> compounds can be divided into two categories  <cit> . the 'toxic' category has  <dig> nephrotoxicants that are known to be directly toxic to the human ptcs. the 'non-toxic' category has  <dig> nephrotoxicants that are not known to directly damage ptcs, and  <dig> non-nephrotoxic compounds. a more detailed description of the experimental protocols and compounds can be found in the report of the original study  <cit>  and another more recent study  <cit> . in these previous studies, simple classifiers based on manually determined thresholds were used, and cross validation was not used to test the performance of these classifiers. the mean balanced accuracy of these classifiers constructed using all the data points  was reported to be  <dig> %  <cit> .

classifier evaluation
we compared four different supervised classifiers, namely random forest, svm, k-nn and naive bayes . the maximum expression levels of il- <dig> and - <dig> induced by the tested compounds were used as inputs to the classifiers. for each classifier, we used a stratified 3-fold cross validation procedure  <cit>  to estimate its generalized classification performance. this procedure randomly divided the dataset into three roughly equal folds, one of which was used to test a classifier trained on the remaining folds. we repeated the whole cross validation procedure  <dig> times, each with a different random fold division. the final classification performance was obtained by taking the mean of all the obtained measurements.

we used three different classification performance indicators: sensitivity, specificity, and balanced accuracy. the definitions of these three measurements are as the following:

  sensitivity=tptp+fn×100%, 

  specificity=tntn+fp×100%,and 

  balanced accuracy=sensitivity+specificity <dig>  

where tp is the number of true positives, tn is the number of true negatives, fp is the number of false positives and fn is the number of false negatives. we performed all the analyses using the r statistical environment  on a personal computer equipped with an intel core i7-3770k processor and windows  <dig> operating system. the r source code used can be found in additional file  <dig> 

random forest
random forest is an ensemble learning method that constructs a large number of decision trees  during training, and predicts the category label of a new data sample by taking the mode of the labels predicted by these trees  <cit> . during training, a random subset of mrf features is selected, and the best spit of data points based on these features are used to construct a decision tree. we tested b =  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  and  <dig>  since our dataset only has two features, we set mrf =  <dig>  the "randomforest" library  under the r environment was used to perform random forest classification.

binary support vector machine
svm aims to construct a decision hyperplane with the largest margin that distinguishes data points from different categories  <cit> . let us denote the training data and category labels to be {xi,yi}, where xi ∈rn, yi∈{- <dig> }, and i= <dig> …,m. if the data points are linearly separable, the optimum decision hyperplane is w⋅x+b= <dig>  where  is a weight vector that is normal to the hyperplane, and b is a bias term. for all the input data , they must satisfy the following constraints:

  w⋅xi+b≥+1 for yi=+ <dig>  

  w⋅xi+b≤-1 for yi=- <dig>  

these two equations can be combined as:

  yi-1≥0 ∀i. 

the hyperplane with the maximum margin can be calculated through solving the following quadratic programming problem:

  minw,b12w <dig> subject to yi-1≥0 ∀i. 

to handle non-separable datasets, these constraints are relaxed with a positive slack variable ξi, where i= <dig> ,...,m   <cit> . then the optimization in equation  <dig> becomes:

  minw,b12w2+c ∑i=1mξi,subject to yi≥1-ξ i∀i,ξi≥ <dig>  

the upper bound for the error in the training dataset is provided by c ∑i=1mξi, where c is a regularization parameter. this optimization equation allows a trade-off between large margin and small error values.

in the case where a decision function is not linear, the data is mapped into a higher-dimensional space, and a hyperplane is constructed so that the data can be linearly separated in this new space. the projection x′=Φ is done through Φ:rn→f. this hyperplane that separates the two categories has a similar form as the linear case: w⋅x′+b= <dig>  the optimization equation is also similar to equation  <dig> except that the input now becomesx′. a lagrangian is constructed and can be transformed into a dual form:

  max ld= ∑iαi- <dig> ∑i,jαiαjyiyjk,subject to  ∑i=1myiαi=0 and 0≤αi≤c,∀i, 

where α <dig>  α <dig> ...,αi are the lagrangian multipliers. k is the kernel function with the form: k=Φ⋅Φ. the classifier based on the kernel function for a new input data xu is:

  g=sgn+b). 

more details about svm can be found in  <cit>  and  <cit> .

in our analyses, we tested four svm kernels, namely linear, polynomial, sigmoid and radial basis function . we optimized all the parameters, including c, γ, r, and degree of the kernels through an exhaustive grid search  <cit> . we used the 'e1071' library  under the r environment to perform svm classification.

k-nn classifier
a k-nn classifier classifies input data according to the labels of the k-nearest neighbors in the training data {xi,yi}. we calculated the euclidean distance between a new input data point xu, and each of the training data points xi using:

  d=xu-xi. 

the category label of xu was assigned based on the majority vote of the category labels of its k-nearest training data :

  gxu=argmaxyi ∑xi∈knnδ, 

where δ∈ <dig>  indicates if xi belongs to yi. we tested k =  <dig>   <dig>   <dig>   <dig> and  <dig>  and used the "class" library  under the r environment to perform k-nn classification.

naive bayes classifier
a naive bayes classifier assumes that each of the measured features contribute independently to the probability of a category label given an input data, p. according to the bayes theorem, we have:

  p=p⋅pp, 

where p is the probability that the input data xu belongs to the category yi, p is the probability of category yi, and p is the probability of the input data xu. the naive bayes classifier maximizes the probability among all the possible categories:

  gxu=argmaxyip⋅pp . 

as phas the same value for a given problem, we only need to maximize the numerator of equation  <dig>  since we assume all the features are independent, the classifier becomes:

  g′xu=argmaxyip⋅∏p, 

where f <dig> f <dig> ...,fn is the set of feature values in the input data point xu. we used the 'e1071' library  under the r environment to perform naive bayes classification.

RESULTS
random forest classification
we found that the performance differences between random forest classifiers using the different tested numbers of trees are very small . our results agree with previous observations that random forest classifiers do not overfit even for large numbers of trees  <cit> . therefore, we fixed the number of trees to be  <dig>  which gives the highest mean balanced accuracy , sensitivity , and specificity .

svm parameter optimization
the classification performance of a svm is closely related to its parameter values. a svm classifier based on the rbf kernel has two important parameters c and γ  <cit> . the c parameter determines the misclassification penalty, and the γ parameter determines the width of the rbf kernel. we tested c and γ values ranging from 10- <dig> to  <dig>  during each trial of the cross validation procedure, we always determined the optimum c and γ values based on the training data of the current fold . these optimum values might slightly change from fold to fold due to the different training data used. using this optimization procedure on our 41-compound dataset, we found that the mean classification performance across all folds and trials for a rbf-based svm classifier are  <dig> % ,  <dig> % , and  <dig> % . we also used similar optimization procedures to optimize the parameters of svms based on the linear, polynomial , and sigmoid kernels.

svm classification using linear, polynomial, sigmoid and rbf kernels
the performance of a svm is also closely related to its kernel function. a linear kernel is simple, fast, but may not work well when the dataset is not linearly separable. polynomial, sigmoid or rbf kernels can provide complex decision boundaries, but may also lead to the problem of overfitting  <cit> . to determine the best kernel for our dataset, we compared the classification performance of svm classifiers based on linear, polynomial, sigmoid and rbf kernels using a stratified 3-fold cross validation with  <dig> random trials . the parameters of these classifiers were optimized as described in the previous section. we found that the rbf kernel had the highest balanced accuracy  and sensitivity , and second highest specificity . our results suggest that the il- <dig> and - <dig> expression levels are not linearly separable in the original feature space, and the mapping of these two features into a higher dimensional space using a rbf kernel helps to distinguish the toxic and non-toxic compounds.



k-nn classification
we found that the optimum number of nearest neighbors  for k-nn classifiers is three . although the mean specificity of the classifiers increases with k, the mean sensitivity starts to decrease after k =  <dig>  at this optimum k value, we found that the mean classification performance across all folds and trials for k-nn classifiers are  <dig> % ,  <dig> % , and  <dig> % .

comparison between random forest, svm, k-nn and naive bayes classifiers
after optimizing the parameters of all the classifiers, we performed a systematic comparison of the performances of these classifiers in classifying our 41-compound dataset . we found that random forest and svm have the highest and second highest, respectively, values of balanced accuracy, sensitivity and specificity among all the classifiers. the k-nn classifier  has higher sensitivity, but lower specificity, than the naive bayes classifier. based on these results, we conclude that a random forest classifier has the best overall performance, and will be used for all of our subsequent analyses.



feature comparison
the expression levels of il- <dig> and - <dig> increase in hptcs in response to compounds that are toxic to human ptcs  <cit> . previously, li et al. found that il- <dig> is more discriminative than il- <dig> in classifying toxic and non-toxic compounds, but they also concluded that the combination of these two features do not provide additional advantages  <cit> . however, this previous analysis was performed using a classifier based on manually optimized thresholds, and the two features were thresholded independently. we wonder if multivariate classifiers, which construct decision boundaries in multi-dimensional feature spaces, may give better performance than classifiers based on individual features. similar to the previous study, we found that random forest classifiers based on il- <dig> only have higher balanced accuracy , sensitivity , and specificity  than random forest classifiers based on il- <dig> only . interestingly, we also found that the combination of il- <dig> and - <dig> gives better classification performance than individual features . similar trends were also observed for classifiers based on svms . our results are consistent with our findings in the previous section that il- <dig> and - <dig> expression levels are not linearly separable, and therefore they can be better separated if a decision boundary is constructed for both features simultaneously. we conclude that both il- <dig> and - <dig> are good and necessary features in the prediction of ptc toxicity.



construction of final classifiers using all compounds
finally, we trained a random forest and a svm classifier using all the  <dig> compounds, and compared their classification performances to the threshold-based classifier  used by li et al.  <cit>  . we computed the receiver operating characteristic  curves  <cit>  for these three classifiers , and measured the areas under the roc curves . roc curves that are closer to the upper left corner have higher auc values and more desirable classification performances. we found that the random forest classifier has higher mean auc , accuracy , sensitivity  and specificity  than the threshold-based classifier . the perfect auc score indicates that the toxic and non-toxic categories can be fully separated by the random forest classifier. the svm also performs better than the threshold-based classifier, but poorer than the random forest classifier . we also noticed that most of the toxic compounds mis-classified by random forest classifiers are usually also mis-classified by threshold-based classifiers. for example, when using a threshold-based classifier, two compounds, namely ifosfamide and germanium oxide, were mis-classified in hptc <dig>  <cit> ; but when using our random forest classifier, only ifosfamide was mis-classified. altogether, our results suggest that a random forest classifier based on il- <dig> and - <dig> expression levels can be used to automatically predict drug-induced ptc toxicity.



CONCLUSIONS
in summary, we have performed a systematic comparison of the performances of four supervised classifiers, namely random forest, svm, k-nn and naive bayes classifiers, in predicting nephrotoxicity based on the il- <dig> and - <dig> expression levels. all parameters of the classifiers were determined automatically without any user intervention. we found that random forest classifiers have the highest overall classification performance . furthermore, we also found that il- <dig> is more predictive than il- <dig>  but a combination of both markers gives higher classification accuracy. finally, we also show that a final random forest classifier trained automatically on the whole 41-compound dataset has higher classification accuracy than a previous threshold-based classifier  <cit>  . this better performance is likely due to the non-linear and multivariate decision boundaries generated by the random forest classifier. our results suggest that a random forest classifier based on these two markers can be used to automatically predict drug-induced nephrotoxicity.

our methods are general and can be easily applied to test and identify other potential nephrotoxicity markers based on gene expression levels, metabolic profiles, or cellular phenotypes. the classification performance of our classifier may also be further increased by combining markers from these different modalities, and also by increasing the number of training compounds. an important application of our automated classifier is to predict nephrotoxicity of novel chemical compounds identified from large-scale screening of small-molecule or natural product libraries. this will allow early selection and prioritization of compound candidates for further drug development, animal tests or clinical trials, which are costly and time-consuming processes. by focusing on smaller numbers of drug candidates that are less likely to induce nephrotoxicity, the drug or compound discovery process will be more efficient, and the chance of successful clinical trials will also be increased.

competing interests
the authors declare that they have no competing interests.

authors' contributions
sr performed the computational analysis, yl and dz provided the gene expression measurements, lh and dz conceived of the study, and sr and lh prepared the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
raw_data.csv · file format: csv file · title of data: raw data used in our study · description of data: il- <dig> and - <dig> expression levels of hptc <dig>   <dig> and  <dig> treated with  <dig> compounds.

click here for file

 additional file 2
nephrotoxicity_r_code_v <dig> .zip · file format: zip file · title of data: supervised classification source code · description of data: r source code to regenerate the results in our study.

click here for file

 declarations
the work is supported by a grant from the joint council office  development program, agency for science, technology and research , singapore; and the bioinformatics institute and the institute of bioengineering and nanotechnology, biomedical research council, a*star, singapore. publication charges for this work was funded by the bioinformatics institute, biomedical research council, a*star, singapore.

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2014: thirteenth international conference on bioinformatics : bioinformatics. the full contents of the supplement are available online at http://www.biomedcentral.com/bmcbioinformatics/supplements/15/s <dig> 
