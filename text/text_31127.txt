BACKGROUND
the spread of microarray technology has made possible the routine and simultaneous measurement of expression profiles for tens of thousands of genes. in the case of photolithographically synthesized high-density oligonucleotide arrays as described in  <cit> , the technology for hybridizing rna on chips and quantitating fluoresence-intensity data has been highly standardized and automated. the results are then related to the biology of interest, both through exploratory methods  and a large and growing number of sophisticated prediction and classification algorithms . yet the very first step on which these procedures rely is still open to discussion: the derivation of a numerical summary value that is both representative of a gene's relative expression level and reasonably free of technical variation, summarily referred to as low-level analysis.

the need for a summary function is due to the setup of high-density oligonucleotide arrays, where each gene is probed by a set of paired oligonucleotides: one of each pair matches the target sequence on the probed gene perfectly , the other has one altered central base-pair , where the mms serve to establish a reference for non-specific hybridisation. while the full set of pms has been used successfully for detecting differential expression  <cit> , there is usually a strong interest in having one number that represents the relative abundance of a gene on a chip. the most common summary measures use a non-model-based robust averaging of measurements in a probe set, such as affymetrix's mas <dig> expression value  <cit> , or a model-based expression index  or a log-additive robust-multichip-average  across chips.

the second crucial aspect of low-level analysis is the control of technical variation between chips, which is introduced by the measurement process during sample preparation, labelling, hybridization, and scanning. technical variation of this kind and the need for a corrective normalization procedure are not specific to high-density oligonucleotide arrays, but are a general feature of mrna measurement, e.g. for cdna microarrays  <cit> , northern-blot analysis or rt-pcr  <cit> . numerous procedures have been suggested, differing in their assumptions on what feature of the data remains constant across chips and can therefore be used for normalization  <cit> .

comparative evaluation of different approaches to low-level analysis has so far been limited to artificial data sets, where differential expression is due to spiked-in rna or mixtures and dilutions of rna from different sources  <cit> . this has the obvious advantage that the true expression ratios are known . consequently, different approaches can be compared in regard to bias  and variance . results so far indicate that there is generally a trade-off between the two, and it seems fair to say that no current method is optimal under all circumstances.

the choice of low-level analysis and especially the choice of normalization have severe impact on the subsequent analysis of the expression data  <cit> . given the wide range of methods available, it would be useful to have a method for assessing their relative merits for a concrete data set, without reference to an external spike-in or dilution data set. this is especially true if we have to assume that our data set is not as well behaved as artificial data, either in terms of the percentage of differentially expressed genes or in terms of rna quality, or both, as for the clinical data set on breast cancer described in the methods section. in this paper, we propose that by studying coregulation or correlations between random pairs of genes, we can compare different summary measures and assess the effect of different normalization procedures. our underlying hypothesis is that given a modern large-scale chip covering a large percentage of a species' genome, randomly selected pairs of genes will be on average uncorrelated. note that we do not claim the absence of all biological correlation between genes, but rather that the number of connections between genes in regulatory pathways is small compared to the number of all possible combinations of genes; this argument is given more detail in the discussion. consequently, a low-level analysis strategy will be deemed suitable for a given data set, if the resulting normalized expression values are on average uncorrelated for randomly chosen pairs of genes. lack of correlation is not assessed via formal tests, but by easily adaptable graphical tools that do not rely on stringent conditions for validity.

we proceed as follows: first, we establish relationships between lack of normalization and correlations between randomly selected genes for three important summary measures; then we show that the default normalization schemes associated with these summary measures do remove the correlations to a large degree, but not completely, with varying amounts of residual correlation. we also show that where available, housekeeping gene normalization is inferior to default normalization in removing random correlation, and we relate random correlation to the number of unexpressed genes in the data. we conclude by discussing the results and the underlying assumption of our approach as well as considerations for its practical implementation, and point out both limitations and possible extensions.

RESULTS
lack of normalization is associated with random correlation
we first calculated raw unnormalized mas <dig>  rma, and mbei expression values for the breast cancer, dilution, and spike-in data sets as described in the methods section. the breast cancer data set is an example of a clinical data set from a real patient population, which is expected to have greater biological variation than the dilution and spike-in data sets. we then computed the pearson correlation coefficients for  <dig> random pairs of probes for each data set.

as shown in the upper part of figure  <dig>  the distributions of the correlation coefficients are centered far away from zero for each data set and expression measure. there is clearly a large amount of excess correlation that is unrelated to biological relationships between genes. the similarity of expression between random pairs of genes across chips is due to technical differences between chips which have not been normalized out. this is a striking example of statistical confounding, where genes are apparently correlated for some underlying non-biological reason.

we have also found that the technical correlation between genes is inversely related to the variability of the genes involved. this can be seen in the lower part of figure  <dig>  where the correlations between the random pairs are plotted against the product of their standard deviations: the average correlation  is highest for genes with small standard deviations and decreases with increasing variability. this fits well with what we would expect from assuming a simple additive chip effect as the source of chip-to-chip variation; even though this is certainly an oversimplification, the corresponding model fits the general shape of the data well enough .

default normalization removes excess correlation
we calculated the same expression measures for the same data sets as above, but applied the default normalization procedure suggested for each expression measure: for mas <dig> expression values, we normalized to the global mean within each array, for rma values, we applied the quantile normalization, for mbei we applied the invariant set normalization, see methods. the upper part of figure  <dig> shows that in all cases, the default normalization step was sufficient to remove excess correlation and center the distribution of the correlation coefficients at zero.

in the following, we will refer to unwanted correlation artifacts after normalization as residual correlation. although we observed no residual correlation for the whole set of genes, there was no guarantee that this would hold for certain subsets of genes: an ideal normalization should remove the residual correlation for any sufficiently large subset of genes. therefore, we investigated the pattern of correlations for pairs of genes with different intensity and variability across chips.

genes with low variability are poorly normalized by rma and mbei
we previously described the systematic inverse relationship between correlation and variability. although the default normalizations strongly reduced the scale of this correlation for all three expression measures, we still found a significant relationship between correlations and variability for rma and mbei, especially for the breast cancer data. the lower part of figure  <dig> shows the average correlations between genes grouped by the product of their standard deviations; this is the same summary line as in figure  <dig>  but without plotting the individual points contributing to it. the residual correlations were smaller than before normalization, but the approximate confidence intervals show them to be highly significant. the shape of the relationship also changed and did no longer follow any simple model.

we found that the residual correlations were both absolutely larger and more significant for rma than for mas <dig>  for mas <dig>  only the subset of genes with the lowest variability showed significant correlation, all of it positive and less than  <dig> . in contrast, for rma and mbei, several of the low-variability classes showed significant positive correlation, up to  <dig>  for the breast cancer data set. in addition, we observed small but significant negative correlations for genes in the middle range of variability for the breast cancer and dilution data.

thus the analysis shows that rma and mbei do not provide properly normalized expression values for genes with low variability, particularly for the clinical data. we will explain this pattern later in terms of absence and intensity of genes.

normalization on housekeeping genes fails to remove excess correlation
the hgu133a chips that were used for the breast cancer study contain  <dig> probes for generic housekeeping genes, whose expression is assumed to be constant on average for most or all experimental conditions. consequently, it has been suggested to use these housekeeping genes for normalization, by adjusting the expression level on each chip so that the average expression of the housekeeping genes is constant across chips . to date, there is no convincing evidence whether this method actually works or not, and it seems that some research groups are using it.

the correlation test given in figure  <dig> shows that for the mas <dig>  rma and mbei methods of computing expression values, the housekeeping gene normalization failed to remove the excess correlation. there was nonzero average correlation over all genes, indicating a general failure of normalization. the systematic inverse relationship between correlation and variability were at higher levels throughout the range of variability compared to the default normalizations. the failure of housekeeping gene normalization was particularly severe for rma.

note that even if the amount of residual correlation shown in figure  <dig> for mas <dig> housekeeping-genenormalized values looks small, the impact on the subsequent high-level analysis can be serious. figure  <dig> shows the distribution of  <dig> gene-wise t-statistics for the housekeeping-normalized and the global-mean-normalized breast cancer data. each t-statistic compares the mean expression level between  postmenopausal women who are users of hormone replacement therapy  versus  those who are not; see . the t-statistics for the housekeeping-normalized values are globally shifted below zero, indicating a genome-wide down-regulation of thousands of genes. in contrast, the t-statistics based on global-mean-normalized values are centered around zero, suggesting a much less pronounced difference between hrt users and non-users. in this example, the global-mean-normalized results are biologically much more plausible.

absent genes are poorly normalized by rma and mbei
in each tissue, only a limited number of genes will be expressed in quantities above the detection limit, usually much fewer than the number of genes available on modern large-scale chips. the purpose of pairing pm and mm probes is to detect which genes are reliably expressed , and for which genes the observed intensities are dominated by technical and biological noise . the most common method of classifying genes as either present or absent is based a non-parametric test for the pm/mm pairs .

there is currently no consensus on how to use these detection calls. all methods report expression values of all genes including the absent genes, so in principle the analyst might ignore the issue of absent genes and treat all genes as present. intuitively the absent genes will be measured with a lot of noise, but will they be properly normalized, i.e., will the measurements be unbiased?

in order to study the success of normalization of measured expression of absent genes, we classified all genes as either present or absent based on affymetrix's present calls . for all data sets, genes were most frequently either completely absent or completely present across all chips .

consequently, the pairs of genes in our random samples could naturally be divided into three classes: those averaging few or no present calls between them, those averaging almost a 100% present calls, and those averaging around 50% present calls . these classes correspond naturally to pairs where both genes were mostly absent, or both mostly present, or where one was mostly absent and the other mostly present; by cutting at 33% and 67% average present calls as indicated in the histograms in the upper part of figure  <dig>  we managed to separate these groups evenly.

to provide more information, the average correlation for each subset was again plotted against variability; see lower half of figure  <dig>  generally, the average correlation was highest for pairs of absent genes, indicating failure of normalization of measured expression of absent genes. this was most serious for rma: excess correlations were consistently and strongly positive for absent pairs and negative for absent/present pairs for all data sets. only for present pairs, correlations were mostly non-significant and small in absolute value. correlations for mas <dig> were throughout smaller and less significant, with no clear pattern between the three groups of pairs. mbei showed the same pattern as rma, though somewhat weaker.

this result implies that, at least in case of rma and mbei, measured expressions of absent genes were poorly normalized, so analyses of absent genes should be avoided or at least viewed with caution. this interpretation is supported by figure  <dig>  which shows the distribution of t-statistics comparing hrt-users and non-users as above, but only for genes that were not detected  on all  <dig> chips ; the distributions for mbei and especially rma indicate strong and wide-spread regulatory effects of hrt, which seems biologically implausible, especially for genes measured at the detection limit throughout the data set.

while the absence or presence of a gene could be assessed via other potential quality control measures, the affymetrix detection call seems to provide useful information for gene filtering.

note that the summary curves of mean correlation shown in figure  <dig> are the weighted means of the curves by presence status shown in figure  <dig>  we can, for example, explain that the high correlations at low variability for rma in figure  <dig> are mainly due to absent/absent pairs in the expression data. the slight negative dip for genes at the middle range of variability in figure  <dig> is the effect of an incomplete cancelation between the positive correlations for absent/absent pairs and the negative correlations for absent/present pairs in this range.

residual correlation is only weakly related to the expression level of genes
detection of a gene is trivially related to the relative abundance of its mrna in the sample. thus, genes that are expressed at the lower end of the detection range are much more likely to be absent. this might indicate that the relationship between the absence/presence of genes and their residual correlation is in fact due to their difference in abundance, and that by focusing on genes with a minimum expression level, we could avoid residual correlation altogether.

in summary it seems strongly preferable to define a gene filter according to absent/present calls than according to the gene intensity levels.

note that correlation between the intensity and presence of genes is reflected by the number of pairs that contribute to each curve in figure 8: there were relatively more present/present gene pairs and less absent/absent pairs at high intensities, and vice versa for low intensities; curves with lower pair counts have correspondingly wider confidence intervals.

filtering out absent genes reduces residual correlation
discussion
the assumption of zero correlation
as some genes are connected in biochemical pathways, the hypothesis that random pairs of genes will be on average uncoregulated or uncorrelated seems counterintuitive, but it is really a question of scale. for a moderately large chip of  <dig> probe sets, there are about  <dig> million possible pairwise correlations, the huge majority of which will be extremely unlikely to be biological. any random sample of probe set pairs will contain only a small percentage of pairs representing an unequivocal biological relationship, and additionally, negative and positive correlations will tend to cancel each other out during averaging. we can demonstrate this for the breast cancer data set. on the affymetrix hgu133a chip, we find represented  <dig> kegg pathways, organising  <dig> probe sets or 14% of all probe sets on the chip . this constitutes an as highly-organised subset of the genome as we can currently hope to select, with numerous probe sets appearing in multiple pathways, thereby establishing numerous cross-correlations between pathways. figure  <dig> shows the boxplots of correlations for  <dig> randomly selected pairs of genes from this subset, firmly centered at zero for all three expression measures. so even for this special subset of many coregulated genes, the average correlation of a random pair of genes is zero.

the simple model of lack of normalization
the model described in the methods section only assumes differences in mean intensity between chips. this corresponds to the simple global mean normalization commonly used for the mas <dig> expression values. figure  <dig> confirms that this model  describes the average behavior of the correlations  adequately for all data sets, suggesting that global mean normalization is indeed suitable for mas <dig> data.

apart from mas <dig>  the model fits adequately only for the rma-based correlations in the breast cancer data, suggesting that global mean normalization on the probe-set level may be attempted in this case, but that it is not generally suitable for rma and mbei data. still, figure  <dig> shows that correlations decrease systematically with the variability of the gene pairs for all expression measures, and it may be possible to describe this relationship by extending the simple model, e.g. by allowing the array effect θ in equation  <dig> to be correlated with the gene effects ψi.

the bad performance of housekeeping genes
the use of housekeeping genes seems reasonable when studying a small number of genes under controlled experimental settings, or where the choice of one or several housekeeping genes can be motivated biologically. for the breast cancer data, which was collected in a real clinical setting, where samples are both genetically heterogenous and potentially genomically unstable, it is much harder to believe in the common expression of housekeeping genes. therefore we argue that the failure of housekeeping normalization in this example is not due to the procedure per se, but to our inability to identify a suitable set of housekeeping genes, and the use of the generic set of genes suggested by the chip manufacturer. even for northern-blot analysis and rt-pcr, where housekeeping normalization is the default, an uncritical use of housekeeping genes has been shown to lead to unacceptable results  <cit> .

comparison of mas <dig>  rma and mbei
it has been suggested that the generally much lower variability of rma and mbei for low-intensity probe sets is a clear advantage of these model-based expression measures over the simpler mas <dig>  <cit> . our results however indicate that this low variability may well be misleading: rma and mbei values for absent probe sets, which constitute the vast majority of low-intensity probe sets, show the strongest residual correlation. this indicates that rma and mbei values for low-intensity probe sets that are reported without regard for their absence/presence status will be compromised by lack of normalization . it seems therefore that rma and mbei estimate expression of low-abundance genes in a biased, but very precise manner. minimizing variability as much as possible only makes sense for unbiased estimato rs: if the variability of the estimate becomes small relative to the bias, we get a dangerous sense of confidence in an estimate that is not quite what we think it is. in the same way, the large variability of the mas <dig> values at low intensities may well hide an amount of bias comparable to that of rma and mbei: as long as the variability of mas <dig> is large compared to the bias, we will not be lead to make inappropriate conclusions based on possibly biased estimates; in that sense the mas <dig> estimates for low-intensity genes are more honest and better normalized than the corresponding rma and mbei values. it is interesting to note that bolstad et al. have already described the choice between different low-level approaches in terms of bias  and variance   <cit> . our results suggest that a) the same trade-off applies when looking directly at the expression values, instead of comparing aggregated fold changes and test statistics between different biological conditions, and b) the trade-off is more disadvantageous for the model-based expression measures than generally thought.

the underlying lack of normalization of rma and mbei for absent genes could be due to the computation of the expression values, or the normalization step, or a combination thereof. preliminary results  indicate that the first step, the summarization of the individual probe intensities through the expression measure, seems to be responsible in both cases. if this can be confirmed, a possible explanation would be that the models used  may not be appropriate for absent genes .

improving low-level analysis
in a recent paper, choe et al. have evaluated the performance of a wide range of low-level analysis methods and test procedures in detecting differential expression in a carefully constructed spike-in data set  <cit> . they report 70% sensitivity at 10% false discovery rate for their top-ranking combinations clearly there is still ample room for improvement in current low-level methodology. we want to outline here shortly how our approach could be used to guide this effort.

the authors of  <cit>  found that an additional second step of normalization on the probe-set level improved the performance of mas <dig>  rma, and mbei in detecting differential expression . we have applied the same renormalization to our data sets , the results are shown in figure  <dig> 

we found that renormalization reduced residual correlation for all data sets and all expression measures. indeed, for mas <dig> the correlations are not significantly different from zero at any lag, indicating perfect normalization as measured by our criterion. rma and mbei show strongly reduced levels of residual correlation, but are still well above the levels of the original mas <dig> as seen in figure  <dig> 

it is interesting that the ranking of the original and renormalized expression measures in terms of normalization quality  corresponds closely to the ranking by performance in detecting differential expression found by choe et al. . this suggests that the lack of normalization that our method is able to measure is indeed relevant for the ability to detect regulated genes.

additionally, figure  <dig> gives an indication of how the newly renormalized expression measures may be further improved. e.g. for the renormalized mas <dig>  there is clearly little need to work on the normalization aspect; modifications of the expression measure could instead aim at reducing the variability of mas <dig> values, possibly by using the information in the mm probes as weights in the summary measure.

renormalized rma and mbei on the other hand still suffer from insufficient normalization; as we perform already normalization steps on both the probe and the probe set level, it seems promising to focus on the intermediate steps like the fitting of the multi-chip model and to study whether these steps are prone to systematic biases.

limitations
the only condition for using the correlation test is a fairly large chip, with probes covering a wide range of the genome under study. for chips that are designed to study only a few related pathways or highly specialized tissues with only a couple of hundred probe sets, the zero correlation assumption may not hold, because the genes from which we want to sample randomly have already been pre-selected by the chip design. the example of the kegg probe sets on the u133a chip suggests though that several thousand probe sets organized in a hundred and some pathways is a safe size.

it should be pointed out that this approach is not limited to high-density oligonucleotide chips. the same argument for between-chip normalization holds in principle for cdna or any other two-color microarray system, although the usual intensity-based normalization between dye channels on the same chip simplifies the situation somewhat  <cit> .

CONCLUSIONS
we have presented a simple graphical method for assessing the quality of low-level analysis of oligonucleotide array expression data. the main advantage of our approach lies in the fact that we do not make use of external reference data, but instead exploit the internal correlation structure of large expression data sets. this allows us to select, evaluate, and modify low-level procedures for specific data sets. in order to demonstrate the use of and usefulness of our approach, we have applied it to three large data sets and three widely used low-level methods . we found a number of interesting results: a) for a large breast cancer data set, normalizing to housekeeping genes does not work at all, regardless of expression measure; b) normalization quality for all three data sets and all three expression measures is closer related to the absence/presence status of a probe set than to its intensity level; c) rma and mbei normalize absent probe sets poorly for all three data sets; d) removing pre-dominantly absent probe sets improves normalization for all data sets and all expression measures. the cutoff percentage of absent calls for a probe set to be included in the analysis can be chosen based on our graphical criterion. we have also evaluated the effect of a second round of normalization on the probe set level data. we found that this improved normalization significantly for all three data sets, in a manner consistent with the observed improvements in the detection of gene regulation  <cit> .

