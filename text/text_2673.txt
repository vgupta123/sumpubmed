BACKGROUND
next-generation sequencing  technologies, such as illumina's genome analyzer  <cit>  and roche/454's gs flx  <cit> , produce massive volumes of data. for instance, illumina's genome analyzer iix can produce up to  <dig> million  <dig> bp paired-end reads in a single run  <cit> . increasing availability of high volume data is opening new possibilities to researchers. these include assessment of rare variants in viral populations via deep sequencing, metagenomic sequencing of bacterial communities, and pooled resequencing of human chromosomes. extracting meaningful information from these kinds of sequencing projects is often difficult, however, due to the error rates associated with ngs. separating true variants from sequencing errors remains challenging. furthermore, analysis is complicated by an ever-increasing variety of downstream software, and a lack of clear standards  <cit> . both selecting the most appropriate sequencing technology, and choosing the appropriate software package and parameter values for data analysis are typically done via a 'hit and miss' approach - a costly exercise, even in the world of 'cheap' ngs.

simulation of ngs data, followed by software benchmarking, presents an alternative approach. early published simulators include genfrag  <cit> . its usefulness for modern ngs projects is limited by a simplistic error model, a single input genome, and a lack of quality score information. samtools  <cit>  also supplies a simulator, however it uses a uniform error rate. a uniformly increasing error rate is used in a slight improvement released as 'dwgsim' <cit> . ngs features highly heterogenous error profiles  <cit> , so the usefulness of this simulator must be questioned.

there is growing evidence that sequence context  influences error rates in both roche/ <dig> and illumina sequencing  <cit> . this awareness has led to more advanced simulators such as metasim and flowsim  <cit> . while metasim generates reads from many input genomes and uses sequence-context error models, it cannot be trained on real data and does not assign quality values to reads, limiting its potential applications. the recent program flowsim is the most realistic ngs simulator to date, with advanced error modelling and quality scores  <cit> . however it operates only in 'flowspace' and is therefore entirely limited to simulation of roche/ <dig> pyrosequencing data. likewise, the unpublished simulator simseq  <cit>  empirically captures some characteristic features of illumina error models, however only allows a single input genome, does not empirically derive all parameters, and cannot simulate roche/ <dig> data. art  <cit> , an unpublished cross-platform simulator, also uses context-dependent error models and does assign quality scores. however it appears limited to a single genome and does not allow training on user's own data sets. thus there is a need for a realistic, cross-platform ngs simulator, as multiple sequencing platforms are likely to persist, each with their own strengths and weaknesses  <cit> .

here, we describe gemsim - a general, error model based simulator of ngs sequencing data. it uses the generic and standardised formats sam   <cit>  and fastq   <cit> , thus ensuring gemsim's applicability to both current and emerging ngs technologies. gemsim creates empirical error models from real ngs data, facilitating technology-, machine-, and even run-specific simulation. gemsim considers a sequence-context composed of a window of three bases before the current base, the current base, and one base after the current base . gemsim also assigns realistic, empirically-derived quality scores to simulated single or paired-end reads. it can draw reads from either single or multiple genomes or haplotype sets, making it applicable to deep sequencing, metagenomic, and resequencing projects. we demonstrate gemsim's usefulness for evaluating error models and benchmarking downstream analysis software by using gemsim to capture the error profiles of two different paired-end illumina runs and one roche/ <dig> titanium run, and by simulating reads from a set of in silico generated buchnera aphidicola haplotypes. we then attempt to identify snps using the popular program varscan  <cit>  and assess the effects of different error profiles and technologies on snp-calling accuracy.

implementation
gemsim is implemented in python as a command line package, consisting of the four programs gemerr, gemhaps, gemreads, and gemstats. the gemsim workflow is as follows:

gemerr
gemerr generates empirical error models from real data. a sam format alignment of control data is used as input. a list of polymorphic sites or sites which are known to differ from the reference genome may also be supplied; these sites are then considered to be true snps and are ignored during error model calculation.

reads are sequentially parsed, tracking the total number of reads and read length distributions. for paired-end reads, insert size, whether the read is the first or second read in the pair, and the proportion of properly aligned pairs are also recorded. for each base of each read the following information is then stored: a) nucleotide type and base position in read; b) mismatch or true base for the position; c) indels following the current position; d) preceding three bases in the read; e) following base in the read, and f) quality scores for true and mismatch bases and insert bases. although it is mainly the sequence preceding the current position that is known to affect error rates  <cit> , the following base in the read is tracked to allow accurate simulation of indels within homopolymers. sequence aligners record these errors either at the start or end of a homopolymer. by taking the following base into consideration, indels are only inserted once within long homopolymers, at the end, rather than potentially multiple times within the homopolymer. empirical distributions for tracked information are stored to a file and used as error models for input into gemreads.

if a particular sequence-context word is not contained at least some minimum × number of times  within the reference genome, then the model is updated using information based on the longest nucleotide sequence that occurs at least × times in the reference. for instance, if aactg is missing from the reference, however actg can be found five times, then the matrix entry where t is the current position, g is the following position, and aac are the three bases before the current position is updated by considering all other matrix entries where the two bases before the current position are ac, the current position is t, and g is the following position.

gemstats 
gemstats takes an error model generated by gemerr, and calculates a set of statistics based on this model - for paired-end reads, models for the first and second read in a pair are considered separately. statistics reported include the overall mismatch, insertion, and deletion error rates; the error rate for each nucleotide; and the error rate by base position within the read. additionally, any sequence-context words with an error rate more than two standard deviations greater than the average error rate are also reported.

gemhaps 
gemhaps takes a genome sequence, and an input command specifying haplotype frequencies and the number of snps in each haplotype . the positions of mutations are randomly determined, and haplotypes are written to a file for input into gemreads. alternatively, users may create their own tab-delimited haplotype file .

gemreads
gemreads requires as input an error model file , a fasta genome file or directory of fasta genomes , an optional haplotype file , and a tab-delimited species-abundance text file . additionally, the user specifies whether the reference genome are circular or linear, and which quality score offset to use . the requested number of single or paired-end reads are generated as follows, and output as fastq files:

 read length and insert length  are randomly chosen from the empirical distributions defined by the error model .

 in metagenomics mode, an input reference genome is chosen with probability proportional to the species abundance and genome size.

 read location and direction are chosen at random, and the read is copied from the input genome.

 read is assigned to a haplotype  and updated with snps, where appropriate.

 errors are introduced according to the error models, accounting for read position, sequence-context word, and 1st or 2nd read in pair .

 quality scores are assigned using recorded empirical distributions contained in the error model file.

for paired-end reads, steps 1- <dig> are repeated, however the insert size is used in conjunction with the previous read position and direction to determine the read location. our error model also tracks how many read pairs have one read that does not align. we simulate this by generating a read that consists entirely of ns with low quality scores, forcing it not to map. for instance, for  <dig> bp paired-end illumina sequencing data, one read would contain  <dig> ns, each with a quality score of 'b' or  <dig> 

RESULTS
data processing and performance
we used gemsim to calculate error models for and simulate reads from three different sequencing runs: illumina genome analyzer iix with illumina sequencing kit v <dig> chemistry ; illumina genome analyzer iix with trueseq sbs kit v5-ga ; and roche/ <dig> flx titanium . for the illumina simulations, error models were calculated from phix control lane data aligned with novocraft v <dig> . <dig>  <cit> . soft-clipping was disabled, reads aligning equally well to two genomic positions were randomly aligned to one of them, and the insert size was set with a standard deviation of  <dig>  all other parameters were given their default values.  <dig> %  and  <dig> %  of reads aligned. for the roche/ <dig> simulation, we used aligned plasmid control data from a hepatitis c virus study  <cit> . this alignment was performed using mosaik v <dig> . <dig>  <cit> . the maximum percentage mismatch allowed was increased to 1%, while all other parameters were set as recommended for roche/ <dig> titanium.  <dig> % of these reads aligned. simulated reads were drawn from a set of b. aphidicola haplotypes, created by gemhaps using the b. aphidicola cc reference genome . the number of simulated reads was five million and one million for illumina gaii and roche/ <dig> flx titanium respectively. this gave a reference coverage of around 1000×. for consistency, alignment of simulated reads was done using novocraft for the illumina data and mosaik for the roche/ <dig> data  <cit> . memory and runtime data are summarised in table  <dig> and show that gemsim can be run with modest memory requirements  on a single cpu/desktop computer within a reasonable time frame. as memory requirements are dictated mainly by the size of the sequence-context word and read length, they are largely independent of the number of reads processed or simulated. runtime scales linearly with the total number of bases processed or simulated.

memory requirements and runtime are given separately for the two main programs within the gemsim workflow, gemerr and gemreads. both programs were run on a single intel xeon e <dig>  <dig>  ghz cpu. to facilitate comparison, the number of bases processed to create the error models is given for gemerr, while the number of simulated bases is given for gemreads. for the simulations, all reads were generated from the same set of b. aphidicola haplotypes  using the appropriate error model.

5-mer presence and frequency
our approach to error modelling is dependent on k-mer choice, which needs to be long enough to capture sequence-context information, but also short enough to be represented in the reference genome to be simulated and the control genomes used for error modelling. all possible 5-mers were represented more than four times in the b. aphidicola reference genome, while  <dig>  of all 6-mers were found less than four times. furthermore, more than 90% of all possible 5-mers were found four or more times in both the phix and the plasmid genomes, used for modelling illumina and roche/ <dig> errors, respectively. less than 30% of all possible 6-mers were present four or more times in these two genomes, while all possible 4-mers were found more than four times in the plasmid genome, and all but one in the phix genome . this suggests that a k-mer length of  <dig> provides an appropriate balance between capturing relevant sequence-context information and the possibility of overfitting the data .

the fact that most 5-mers are contained within the control genomes used also supports the notion that the derived error models can be used to accurately simulate reads from any unrelated reference genomes. for the 10% of 5-mers not well represented within the control genomes, gemsim derives an error rate based on the relevant 4-mer .

error analysis
error models for illumina v <dig>  illumina v <dig>  and roche/ <dig> were analysed with gemstats. error rates are summarised in table  <dig>  striking differences between the error profiles of illumina v <dig> and illumina v <dig> are apparent, justifying the need for empirical chemistry- or run-specific error models when simulating ngs data. combining results for the first and second reads in a pair, illumina v <dig> had an error rate of  <dig> %, five times lower than the error rate of illumina v <dig> . illumina v <dig> also showed large differences between the first and second reads in a pair with second reads having considerably greater error rates. roche/ <dig> gave the least errors and its mismatch error rate of  <dig> % is in line with recently published estimates  <cit> . consideration of the top five mismatches within their sequence-context suggests that all runs had some trouble with homopolymers, particularly in g or c homopolymers for the illumina runs. this problem has been well document for roche/ <dig> data  <cit> , and there is some evidence that homopolymers are also problematic for illumina data  <cit> . our data also supports the known problem with the cgg motif in illumina sequencing  <cit> .

values give the error rates for each technology. several measures of error rate are given, including: overall error rates; average error rate for each nucleotide; error rates for the five sequence-context words most likely to result in mismatches ; and average insertion and deletion rates. for the top five mismatches, the sequence-context word is given with the actual mismatch base in bold .

for all three simulations, error rates increased  towards the tail end of the read . while this is a recognised issue for illumina sequencing, the literature is ambiguous with respect to roche/ <dig> sequencing data  <cit> . while a positional effect for roche/ <dig> can be seen in figure  <dig>  the magnitude of this effect is extremely small when compared to illumina sequencing data and would probably not be observed if only reads with length <  <dig> bp are considered. the positional increase in error rate for illumina v <dig> is also rather modest, and is only slightly more elevated for the second read in the pair. in contrast, illumina v <dig> exhibits a strong influence of base position within the read on error rate, characterised by sudden peaks and a marked increase in error rates near the end of the read.

while insertions were roughly twice as common as deletions in the illumina runs, in general, indel error rates in both illumina runs were two to three orders of magnitude lower than in the roche/ <dig> run. this is consistent with previous findings that mismatches are the predominant forms of error in illumina, while indels are the most frequent errors in roche/ <dig> sequencing  <cit> , showing that the gemsim error-modeling approach captures these key characteristics of the different sequencing platforms.

snp-calling
for each error model, simulated reads were drawn from a set of five haplotypes derived from the b. aphidicola reference genome, with frequencies of 1%, 3%, 5%, 7% and 84%. the four low-frequency haplotypes each contained  <dig> randomly placed snps and the same haplotypes  were used for all simulations and downstream analysis.

snp-calling was performed using varscan v <dig> . <dig>  <cit>  on pileup files generated with samtools v <dig> .12a  <cit> . as we were interested in identifying low-frequency true snps and associated false positives , the minimum variant frequency parameter was set to zero. minimum coverage was set to  <dig> and the minimum number of reads supporting each snp was set to five. as varscan largely depends on individual base quality scores to distinguish between true snps and sequencing errors, we varied the minimum average quality  parameter from  <dig> to  <dig> and investigated its interaction with specific error profiles and snp-calling accuracy.

snp-calling was highly accurate for snps with a frequency > 3% for all sequencing platforms simulated. roche/ <dig> showed a slightly lower true positive rate  than the illumina simulations. when using varscan with a m.a.q. of  <dig>   <dig> out of  <dig> snps were identified . in contrast, all  <dig> snps were identified from both illumina simulations. upon closer inspection,  <dig> of the false negatives with true frequency of 1% failed to be supported by five reads. all the remaining false negatives were associated with homopolymer indel errors. inspection of the pileup file showed these snps were contained in the data; however varscan reported them as indels instead.

any inaccurate snp calls within +/- 1% of a known haplotype frequency were classed as false positives. for illumina v <dig> and roche/ <dig>  all false positives had a frequency under 1% . illumina v <dig> showed a drastically increased false positive rate, as can be expected from the higher average error rate of this run. despite this, false positives were still restricted to under 3%  population frequency. as m.a.q. was increased, however, some false positives with a true frequency of 1%  were now given a frequency by varscan of 3% . this can be seen as spikes in the frequency = 3% graph, between m.a.q.  <dig> and m.a.q.  <dig>  this can be understood by considering how the m.a.q. parameter works. the varscan manual states that m.a.q. is the 'minimum base quality at a position to count a read'  <cit> . this means varscan uses m.a.q. to select a subset of reads from which to make a call. thus increasing m.a.q. reduces the sampling of reads, which in turn reduces the accuracy of the snp frequency calculation when frequencies are small. this highlights the need for a detailed understanding of any chosen data and analysis pipeline and the value of performing benchmarking with simulated data.

varscan also has a minimum coverage parameter and its strong interaction with the m.a.q. parameter is shown by table  <dig>  when calculating coverage, varscan only 'counts' bases that have a quality above a given m.a.q and thus coverage effectively decreases as m.a.q. increases. this interaction explains why a m.a.q. value of  <dig> gives the most accurate overall results for illumina v <dig>  however results in no snps being called for illumina v <dig>  as we set the minimum coverage to  <dig>  any genomic position where varscan counts less than  <dig> reads will be ignored. using a m.a.q. of  <dig>  for the illumina v <dig> simulation less than  <dig> % of the genome is ignored, whereas for illumina v <dig> 100% of the genome is ignored. although increasing m.a.q. decreases false positives, there is a clear trade-off between decreasing false positives by eliminating low-quality bases and increasing false negatives by disregarding good data. this again reinforces the need to understand individual sequencing runs, even if they originate from the same technology . without simulation, it would be difficult to choose an optimal m.a.q. value and almost impossible to interpret any findings. following this simulation, for a b. aphidicola sequencing experiment resembling our illumina v <dig> simulation a m.a.q. of  <dig> will give confident and accurate results. furthermore, we expect to identify 100% of snps with frequency > = 3% and 71% of snps with frequency of 1% . we also expect that across the length of the genome,  <dig> false positives with frequency of 1%  will also occur.

m.a.q gives the minimum quality a base within a read must have for varscan to 'count' it. at each position with the genome, there must be at least  <dig> bases  with a quality above the value of m.a.q, for the genomic position to be considered by varscan when scanning for snps. for example, a value of zero means there were no sites within the reference genome where there were more than  <dig> aligned bases with a quality score greater than the specified m.a.q.

CONCLUSIONS
by considering errors within their sequence-context in real data, gemsim captured known features of illumina and roche/ <dig> error profiles, thus validating our approach. gemsim therefore facilitates independent, objective, and comparable simulation of both roche/ <dig> and illumina sequencing data. analysis of the error models created by gemsim also provided new insights into error profiles, specifically that there were substantial differences between the error profiles of the illumina v <dig> and illumina v <dig> sequencing runs. while it is not clear whether this difference is due to the change in chemistry or to some other factors, it does show that sequencing runs can vary substantially, even when performed by the same sequencing provider using the same machine. furthermore, differences between these error profiles have a substantial impact on downstream analysis, as shown by our study of snp-calling accuracy in simulated data.

our findings call for empirically derived, run-specific error models in sequencing simulation. gemsim meets this need with a set of python scripts that can be run on a standard desktop computer. by allowing analysis of run-specific error models created from the user's own data, gemsim helps researchers to identify unique features of their data - understanding of which may be invaluable for downstream data analysis. error-model analysis also facilitates quality control and identification of 'bad quality' sequencing runs, such as the illumina v <dig> run described here. finally, simulation of sequencing data based on empirically-derived error models allows researchers to choose the most appropriate sequencing platform for their project, assess the impacts of errors on downstream data analysis, and objectively interpret any findings.

gemsim is most suited to simulating resequencing or metagenomic projects where known reference sequences exist, as it relies on the presence of a reference sequence to initially generate reads . for example, gemsim can be used in a deep resequencing project to establish at which coverage any further sequencing may not provide any increase in snp detection accuracy. gemsim may also prove itself valuable in developing and benchmarking de-novo assemblers, by assessing how well a known genome can be reconstructed from simulated reads. by providing a manually modified reference genome to gemsim, users could also simulate reads to assess the detection of large genomic rearrangements via de-novo assembly.

future improvements to gemsim may include increasing the number of bases tracked before the current position during error model construction, as it is possible that error profiles are even more heterogenous than reported here. for roche/ <dig> sequencing, indel errors are known to increase with increasing homopolymer length, while there is evidence to show that illumina sequencing accuracy can be influenced by the sequence up to  <dig> bases before the current position  <cit> . currently, the number of bases before the current position is limited by both memory requirements and the need for the sequence-context word to be present in the control dataset. with future improvements in computing power and memory handling, it will be feasible to allow users to optionally increase the sequence-context word length, when appropriate.

as new sequencing technologies emerge, we will also continue testing and developing gemsim for compatibility. recently released platforms, such as the illumina's miseq and pacific biosciences rs system, can be readily assessed as they are compatible with the two generic formats required by gemsim, fastq and sam.

the error models described in this paper are provided with the gemsim package as generic technology-specific error models, for users who do not have access to control data. new error models for different platforms and chemistries will be supplied, as they become available.

availability and requirements
project name: gemsim.

project home page: http://sourceforge.net/projects/gemsim/

operating system: platform independent.

programming language: python  <dig> 

other requirements: numpy, python  <dig> 

license: gnu gpl v <dig> 

any restrictions to use by non-academics: none.

competing interests
the authors declare that they have no competing interests.

authors' contributions
km wrote the gemsim code, km, fl and tt participated in data analysis, km drafted the manuscript and fl and tt revised it, and km, fl and tt contributed to study design and conception. no funding bodies contributed to study design or data analysis. all authors read and approved the final version of the manuscript.

