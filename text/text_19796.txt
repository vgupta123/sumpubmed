BACKGROUND
as the number, size, and complexity of life science databases continue to grow, data integration remains a prominent problem in the life sciences. these disparate databases feature diverse types of data including sequences, genes, proteins, pathways, and drugs produced by different kinds of experiments, including those that involve high-throughput technologies such as dna microarray, mass spectrometry, and next generation sequencing. the challenges involved in integrating such data include inconsistency in naming, diversity of data models, and heterogeneous data formats. the benefits of integrating these disparate sources of data include discovery of new associations/relationships between the data and validation of existing hypotheses.

numerous life science databases can be accessed publicly via the web. the data retrieved from different databases are displayed using the hypertext markup language  and rendered by web browsers . hypertext links are used to connect data items between different web pages. data integration using hypertext links, however, is burdensome to the user  <cit> . html works well to expose the results of scripted  queries but does not expose the database structure to data users who would wish to construct their own queries. to automate integration of data in html format, we need to rely on methods such as screen scraping to extract the data from the html documents and integrate the extracted data by custom scripts. this approach is vulnerable to changes in display and location of web pages. such changes, together with changes in database structure, significantly increase the code complexity of data integration. to address this problem, approaches have been developed to facilitate data integration on a larger scale. some representative approaches include ebi srs  <cit> , atlas  <cit> , discoverylink  <cit> , biokleisli  <cit> , biozon  <cit> , etc. in general, these approaches fall into two categories: data warehouse and federated database. the data warehouse approach relies on data translation in which data from different databases are re-expressed in a common data model on a central repository. the federated approach features query translation in which data are kept in their local databases and a global query can be translated into a set of local database subqueries whose results are unified and presented to the user. there are pros and cons for each approach. data warehouses typically wrestle with the concurrency issue . each time a member database is changed, the data translation code will need to be modified and/or re-executed, depending on the nature of the change. on the other hand, data warehouse query performance is good because queries are run locally. in the federated approach, data concurrency is not an issue, but query speed may be slow, especially when large amounts of data are transferred over the network.

the semantic web  <cit>  transforms the web into a global database or knowledge base by providing: i) globally unique names through the uniform resource identifiers , ii) standard languages including the resource description framework , rdf schema , and the web ontology language  for modeling data and creating ontologies, and iii) a standard query language – sparql  <cit> . enabling technologies such as ontology editors , owl reasoners  and triplestores with sparql endpoints  help make the semantic web vision a reality. while these core and enabling technologies are maturing, there are new technological developments that can help push the semantic web to a new level of data interoperability. for example, linked data  <cit>  is a method of exposing, sharing, and connecting data via dereferenceable http uri's on the semantic web. a dereferenceable http uri serves as both an identifier and a locator. the key idea is that useful information should be provided to data consumers when its uri is dereferenced. using the linked data approach, not only do data providers make their data available in the form of rdf graphs, but data linkers can also create new rdf graphs that consist of links between independently developed rdf graphs provided by different sources. examples of linked data  are listed on linking open data   <cit> . vocabulary of interlinked datasets   <cit>  is an emerging standard for using a rdf based schema to describe linked datasets accessible through dereferenceable http uri's or sparql endpoints. it provides a common vocabulary that can be used to describe datasets to assist the discovery of relevant linked data; or be associated with each dereferenceable data uri, enabling search engines or query mediators to follow the links in the web of data.

the biordf task force is one of the several task forces in the semantic web for health care and life sciences interest group   <cit>  chartered by the world wide web consortium . hcls ig develops, advocates for, and supports the use of semantic web technologies for biological science, translational medicine and health care. the different task forces  <cit>  involve participants from different organizations in both the industry and academia, who work together to solve particular sets of problems using semantic web technologies. as a consequence of such community support, we have witnessed growth in the use and demonstration of semantic web technologies in life science data integration. to date, most of the semantic web data integration approaches in life sciences use the data warehouse method. as pointed out in  <cit> , there is a need to invest more efforts in exploring how to use the semantic web to implement the federation approach. this paper describes a journey to realizing this implementation in the context of neuroscience data federation. the federation involves sparql endpoints, linked datasets, and other formats including spreadsheets and semantic tags. this work represents a community effort carried out by the biordf task force  <cit>  within the charter of the hcls ig.

neuroscience use case
in the previous charter of the hcls ig, the biordf task force focused its effort on data conversion and integration in the neuroscience context. two main projects were derived from this effort. one of them was the creation of a central knowledge base  that housed a variety of biomedical data including neuroscience data that were converted into rdf/owl. the other project involved conversion of several senselab databases  including neurondb, modeldb, and brainpharm into owl ontologies. both projects were described in w3c, which were made available to the public. in addition, a demo at the www <dig> conference in banff, alberta, canada showed queries that integrated multiple datasets stored in the hcls kb to answer specific neuroscience research questions.

in the renewed charter, the biordf task force leverages the previous effort to expand the neuroscience use case. figure  <dig> depicts different categories of brain data across multiple levels including the anatomical level , neuronal level , and synaptic level . at the anatomical level, the brain is divided into different regions. different brain regions are responsible for different behaviors/functions. for example, the hippocampus plays a major role in memory formation, while the cerebellum is a brain region that is important for movement. at the neuronal level, neurons  and their connections are of concern. a neuron has different parts: dendrites , axons , and soma . although most neurons contain all of the three parts, there is a wide range of diversity in the shapes and sizes of neurons as well as their axons and dendrites. the red arrow represents an action potential  that travels from the soma to the end of the axon . this electrical impulse triggers the release of molecules called "neurotransmitters"  from the axon terminal into the synapse . before entering the synaptic space, neurotransmitters are contained in vesicles. the electrical impulse pushes these vesicles towards the cell membranes of the axon terminal. when the vesicles are in contact with the cell membranes, they break and neurotransmitters diffuse across the membrane into the synapse. once released into the synapse, the neurotransmitters may bind to special molecules, called "receptors" , that are located within the cell membranes of the dendrites of the adjacent neurons. this, in turn, stimulates or inhibits an electrical response in the receiving neuron's dendrites. while some neurotransmitters act as chemical messengers, carrying information from one neuron to another, other neurotransmitters  act as neuromodulators, modulating the re-uptake of other transmitters  back to the axon of the originated neurons. this mechanism is important for maintaining the normal activities of neurons. for example, disturbance of dopamine transmission can cause parkinson's disease, in which a person loses the ability to execute smooth, controlled movements. drugs like cocaine inhibit the re-uptake of dopamine, affecting the pleasure system of the brain.

for the banff demo, a query endpoint was created from which to issue a query by identifying and locally aggregating the data sources necessary to answer the scientific question associated with our use case into a data warehouse. however, most users would prefer to ask a scientific question from a single interface, with answers from relevant sources, regardless of whether the source is remote or local, and without being required to know which data sources have information related to their question. this scenario points to the need for federating queries that can reach out to remote triplestores, relational databases, and data repositories. data distribution is an underlying notion in the semantic web, where the amount of data that can be practically aggregated into a data warehouse is limited by hardware and software constraints. furthermore, we have already mentioned the maintenance problems that stem from the data warehouse approach. in practice, resources will periodically appear on the web that can be useful for various types of research. this makes federation attractive because it can make it possible to query data where it resides without the level of effort and infrastructure needed to add it to a data warehouse.

as illustrated in figure  <dig>  it is important to integrate data across different levels to support integrative neuroscience research. such data integration presents a great challenge as different types and levels of data are available in different formats and are widely scattered over the internet. as listed in the neuroscience database gateway   <cit> , there are nearly  <dig> neuroscience databases accessible over the web. this is by no means an exhaustive list. in addition, there are web resources that are not databases . this paper describes how we meet the data integration challenge by using a range of semantic web technologies. instead of using a warehouse approach, we embark on a journey toward the query federation approach by developing different interfaces to meet different needs and purposes.

methods and 
RESULTS
this section first describes the data sets that we have used for query federation. these publicly available data sets represent diverse types of neuroscience and biomedical data that are provided and maintained by different sources in different formats. some of these data sets are available in rdf/owl format via linked data or sparql endpoints, while others are available as relational databases or files/documents in other formats such as xml, html, and spreadsheet. while each of these data sets serves a specific purpose, combining them provides a broader context for supporting integrative or translational research. we describe how to implement federation and description of these data sets using a range of semantic web technologies/approaches including sparql endpoints, standard-based middleware technologies, query translation, and void. the scope of our work is broad in the sense that a wide range of techniques and diverse types of data sets with heterogeneous formats are covered.

data sets
we have created two health care and life science knowledge bases  at two sites  using different triplestore technologies , created and included data sources that are in other formats , and used rdf/owl data sources created by others.

the hcls kb at deri galway contains over  <dig> million rdf statements. major constituents of the hcls kb at deri are the neurocommons knowledge base  <cit>  and the datasets generated by the "linked open drug data" task force  <cit>  of the w3c. the neurocommons knowledge base consists of a sizeable collection of rdf/owl versions of biomedical databases and ontologies, such as the open biomedical ontologies , medline, gene ontology annotation , medical subject headings , the brain architecture management system, parts of the senselab neurobiology databases and others. the linked open drug data datasets contain pharmacologically and medically relevant datasets dealing with data about drugs, pharmacokinetcs, disease-gene associations, clinical trials data and related topics.

data in the hcls kb at deri galway are exposed in accordance to the 'linked data' paradigm: for example, the uri  is used to identify the entrez gene record  <dig> inside the knowledge base. resolving this uri in a common web browser yields the human-readable representation of the linked data about this record that is contained in the knowledge base. if the same uri is resolved by an rdf-enabled client, the client can extract these statements in machine-readable rdf format. entities that are referenced in statements as subjects, predicates or objects are identified by linked data uris as well. this makes it possible for rdf-enabled clients to incrementally 'crawl' the web of linked data that surrounds any given database record or other entity. as an alternative to this incremental, entity-by-entity exploration, the hcls kb at deri can also be queried through a sparql endpoint .

the hcls kb at freie universitaet berlin  <cit>  uses the allegrograph rdfstore as a high-performance, persistent rdf graph database. it supports sparql, rdfs++, and rule-based prolog reasoning from java applications. the knowledge base contains complementary data such as parts of the senselab neurobiology databases and associated traditional chinese medicine, gene and diseases information  <cit> . sparql endpoints, linked open data, and web views  give access to the allegrograph triplestore data.

in addition to the hcls kb's, our federation involves external rdf/owl data sources developed by third parties, such as dbpedia  <cit>  and bio2rdf  <cit> .

not all the knowledge used by scientists for their research is available as structured data. a lot remains only accessible as spreadsheets or html web pages. social tagging tools, such as del.icio.us  <cit> , allow scientists to annotate html web pages with tags for organizing their personal knowledge and sharing with their colleagues. however, without controls, tags can be obscure and error-prone and bring gaps to the linking of the knowledge generated at different time by different contributors. in our use case study, we used two social tagging tools, namely atags  <cit>  and faviki  <cit> , both of which support the use of controlled terms for tagging and achieve bringing semantics and structure to the tags.

as part of our contribution we have developed atags, which offer a simple mechanism for capturing biomedical statements in rdf/owl format and publishing them anywhere on the web. the atag generator prototype suggests entities from dbpedia and other domain ontologies that can be used to describe the content of simple biomedical statements in an unobtrusive 'tagging' workflow. the atag generator functionally can simply be added to any web browser with the atag bookmarklet. after adding the bookmarklet to the browser, a user can navigate to any web page , highlight a relevant statement in the text, and click the bookmarklet. the atag generator then allows the user to add and refine semantic tags that capture the meaning of the statement in a machine-readable format, interlinking it with existing ontologies and linked data resources. the atag generator encodes the output as xhtml+rdfa, using parts of the sioc vocabulary  <cit> . the resulting xhtml+rdfa can be embedded anywhere on the web such as in blogs, wikis, biomedical databases or e-mails. an exemplary xhtml page where atags about the gabab neuroreceptor have been collected can be found on  <cit> . the rdf statements contained in this xhtml page can be programmatically retrieved using rdfa services like swignition  <cit>  via the url  <cit> .

faviki provides similar functionality as atags, but is focused on classical tagging/bookmarking of entire web pages. it proposes dbpedia uris to users so that they can annotate web pages with common, pre-defined tags. also, the functionality of faviki can be used in any web browser through a faviki bookmarklet. when loading a web page, faviki will automatically propose a set of dbpedia uris by using the zemanta api  <cit>  to analyze the content of an html web page and to search for appropriate uris. it can auto-complete a tagging term using the google search api, controlling the terms used in the tagging. bookmarks can be browsed by tag clouds or programmatically accessed using the embedded rdfa . again, statements about pages tagged with"gabab receptor"  can be retrieved with the swignition service via the url of .

query federation
another contribution is that we have developed a number of approaches for querying and displaying rdf data from a distributed set of repositories. these approaches include the receptor explorer, an application for navigating receptor related information, the aida toolkit, a generic set of components supporting search and annotation of semantic datasets and federate, a query federation framework. all of the investigated approaches have the following in common:

• rdf/owl data is retrieved at runtime from multiple distributed repositories.

• client applications are able to access this data as if it were contained in a single virtual repository.

the approaches differ along two axes:

• the application stack layer at which the virtual repository abstraction is supported: for the receptor explorer, this is the enterprise service bus  api exposed to client applications. for aida web services employed by the aida search client, this is the service-oriented architecture  api exposed to client applications by the wsdl. federate supports this abstraction at the sparql query interface.

• generality of approach: the receptor explorer, although built using a generic esb framework, is focused on a single scenario involving the retrieval and display of specific receptor-related data. the aida search interface, also an end-user tool, is more general-purpose and can be utilized to explore a wide range of rdf data retrieved from multiple locations. as a federated query engine, federate has the broadest range of application with the potential to be leveraged in various rdf-based applications that query a range of datasets and repositories.

receptor explorer
the receptor explorer is a proof-of-concept application that enables users to retrieve and navigate through a collection of receptor related information stored in multiple rdf repositories  and associated web sites . in this use case, "receptor" serves as a unit for data federation. the genes involved in encoding a receptor are used to retrieve related publications. the retrieved publications are in turns used to find the corresponding clinical trials and to connect researchers. therefore, the receptor explorer can help translational research in terms of connecting basic neuroscience research with clinical trials. in addition, it plays a role in social networking in terms of connecting researchers studying the same receptor.

as shown in figure  <dig>  the receptor explorer is comprised of a browser-based ui layered on top of a semantically enabled esb application that communicates via sparql protocol with a set of remote rdf repositories. this architecture gives the client application a federated view of the underlying rdf data by delegating client requests  to esb services that coordinate the execution of separate sparql queries on the various remote endpoints.

the esb application is implemented using vectorc's semantic service bus framework  <cit> , a set of semantic web extensions to the mule esb  <cit>  that leverages the jena  <cit>  and sesame  <cit>  frameworks. the semantic service bus enables the development of applications that query, transform, route and perform reasoning over rdf data and associated ontologies with application configuration and message processing topology declaratively specified in a set of xml configuration files.

as shown in figure  <dig>  the receptor explorer queries the deri hcls kb, dbpedia, bio2rdf and linkedct.org to support the following workflow:

• the user first selects a receptor from the list created by merging the dbpedia and senselab receptor trees .

• the dbpedia receptor tree is rooted at 

• the senselab receptor tree is rooted at 

• overlaps are computed using the senselab-to-dbpedia owl mapping file  <cit> 

• the set of genes  involved in the selected receptor are retrieved from dbpedia .

• the description and image for the receptor, if available, are retrieved from dbpedia and displayed along with hyperlinks to the associated wikipedia and dbpedia urls for the receptor .

• the user selects one of the displayed genes and the set of pubmed publications that reference the gene are retrieved from bio2rdf. the publications display includes hyperlinks to the associated pubmed urls .

the set of clinical trials that reference the retrieved publications are retrieved from linkedct.org. the clinical trials display includes hyperlinks to the associated clinicaltrials.gov urls.

aida toolkit for browsing and searching
the aida toolkit  <cit>  is directed at groups of knowledge workers that cooperatively search, annotate, interpret, and enrich large collections of heterogeneous documents from diverse locations. it is a generic set of components that can perform a variety of tasks such as learn new pattern recognition models, perform specialized search on resource collections, and store knowledge in a repository. w3c standards are used to make data accessible and manageable with semantic web technologies such as owl, rdf, and skos. aida is also based on lucene and sesame. most components are available as web services and are open source under an apache license. aida is composed of three main modules: storage, learning, and search. the modules employed in the biordf pilot are search and storage. the use case here is broader than that of the receptor explorer described previously. in addition to receptors, the user can query diverse types of biomedical entities such as disease, gene, pathway, and protein across different repositories including the literature.

the web browser-based aida search interface was originally designed to allow the browsing and search of vocabularies available in the skos  format from sesame repositories. selected concepts from a given vocabulary can be employed in the search of a lucene index available from the aida search interface. such search makes use of both the knowledge resources stored in a triplestore and a lucene index of document resources, effectively federating triplestores and indexes. lucene search is typically demonstrated with the medline index. the background of figure  <dig> shows the search of the medline index using the go term "gaba-b receptor activity". in order to prototype an interface for exploring knowledge base contents in biordf, we employed an adaptation of the aida search interface that we call a 'skos lense' in order to browse the subsumption hierarchies of ontologies contained in the hcls kb. once our storage component had been augmented to connect to virtuoso , we were able to view the class hierarchies contained in a given named graph of the hcls kb and search rdfs:labels for strings. this made the aida search interface useful for browsing and searching vocabularies, and ontologies, as well as text corpora. this is the first step in a prototype application designed to support query building and federated query.

an example query scenario where the 'skos lense' would be useful is in the case that the user wants to query about a particular term but does not know the valid identifier to use for that term in the query . it is possible to use the aida search interface to search through the actual rdfs:labels of the named graph for the gene ontology  in the deri hcls kb for "gaba-b receptor activity" and drag the desired term to the query builder. the term that is added to the  query is the corresponding go id  that is useful in the hcls kb sparql query . in such a way, we have made it possible for the user to work with a human-readable label "gaba-b receptor activity" instead of a machine-based id "", yet build a valid sparql query. the same web interface can be used to browse and search a medical vocabulary such as snomed-ct or mesh from, for example, a sesame hcls terminology repository hosted at duke university in order to build and execute a query on the virtuoso hcls kb at deri or allegrograph hcls kb at the freie universität berlin.

federate
working at a lower level than aida or receptor explorer, federate offers a step in the direction of enabling the system to integrate resources without procedural programming or warehousing. this tool uses a simple, practical approach to federate queries over multiple remote data stores, including relational databases, xml and xhtml documents, and semantic web data stores. the approach uses a sparql query  to specify a series of graph patterns to retrieve from remote sparql services. the order of the graph patterns provides a query orchestration, and variables shared between the graph patterns express the way information from these remote sources is joined. the use case of federate is that higher level tools like receptor explorer and aida can use federate to implement query federation not only against triplestores but also relational stores. below we provide a technical discussion of federate.

the following query gathers receptor data from source <dig> and source <dig>  connecting them by a common human entrezgene identifier:

   select ?iupharnm ?type ?label

   ...

   graph <source1> {

      ?iuphar   iface:iupharname   ?iupharnm .

      ?human   iface:iuphar   ?iuphar .

      ?human   iface:genename   "gabbr1" .

      ?human   iface:entrezgene   ?humanentrez }

   graph <source2> {

      ?gene   db:entrezgene   ?humanentrez ;

      ?gene   a   ?type ;

      ?gene   rdfs:label   ?label }

a researcher would compose such a query by forming a question, considering the popular expression of the data that question touches, and locating the resources serving this data. the ease of re-use of ontologies in the semantic web and the existence of a standard protocol for the sparql query language have led to a proliferation of resources serving queries like this. 

federate determines which variables in each graph constraint are present in the select, or are referenced in subsequent graph patterns. starting with the first graph constraint, federate translates the constraint into a subordinate sparql query.

   select ?iupharnm ?humanentrez

      from <source1>

   where {

      ?iuphar   iface:iupharname   ?iupharnm .

      ?human   iface:iuphar   ?iuphar .

      ?human   iface:genename   "gabbr1" .

      ?human   iface:entrezgene   ?humanentrez }

subsequent subordinate queries are accompanied by bindings constraining the variables bound by earlier queries. these can be expressed either as standard sparql constraints:

   from <source2> where {

      ?gene   db:entrezgene   ?humanentrez ;

      ?gene   a   ?type ;

      ?gene   rdfs:label   ?label

      filter 

   }

or by an extension to sparql called sparqlfed, which ships bindings around in a tabular form:

   ... ?gene   rdfs:label   ?label }

   bindings  {

         

         

   }

sparql is an effective language for unifying data over diverse sources because it has a simple data model  which can express existing data formats, e.g. relational databases or spreadsheets. we pay particular attention to the relational database, as it holds the majority of the computer-processed data in the life science domain. it is common that each record in a database describes some object in the domain of discourse. for example, a biological pathway database will have records for different pathways and records for the genes involved in those pathways. in this respect, each column in the record asserts some property about the object of discourse, an assertion which is written in rdf:

   @prefix db: <> .

   db:apoe db:is_involved_in db:alzheimer_disease_pathway.

the prefix "db:" is associated with a unique identifier for this database. given a unique identifier, it is easy to imagine the rdf view of that database. it was asserted earlier that rdf encourages the sharing of terms, but the terms in our  rdf view of the database are specifically unique. there are many different schemes for mapping relational data to rdf; federate uses a sparql construct to map from the rather proprietary view of the database described above to terms chosen to be shared with others in a community. the community can range from world-wide to intra-laboratory; a sharable data format benefits anyone who will share any of their data with anyone else.

we used federate to connect some of the data sources described in table  <dig>  we started with a relational database containing data from the iuphar database of receptor nomenclature and drug classification  <cit> . this database includes entries that contain the entrez gene identifier  <dig> . a sparql construct mapped this very flat database to a more structured rdf form :

   construct {

      ?iuphar   interface:family   ?family .

      ?iuphar   interface:iupharname   ?iupharname .

      ?human   interface:iuphar   ?iuphar .

      ?human   interface:entrezgene   ?human_entrez_gene.

   } where {

      ?a   receptors:family   ?family .

      ?a   receptors:official_iuphar_name   ?iupharname .

      ?a   receptors:human_entrez_gene   ?human_entrez_gene .

   }

an identifier associated with the "receptors:" prefix provides the proprietary rdf view of the receptors database while terms with the "interface:" prefix would be a product of e.g. community consensus amongst neuroscientists. the mapping  creates another view of the database allowing users to ask questions in terms of the shared ontology. neither the proprietary nor interface views were ever materialized. the subordinate queries, expressed in terms of the interface data structure, were passed to a sparql server which was configured with the database identifier and the construct mapping. this server used the construct mapping to calculate a query which would work over the proprietary view, and then used the databases identifier to transform that query into a sql query. as a result, that portion of the query was executed directly on the relational database in a single sql query.

our orchestration queries went on to join the iuphar data with data from atags and dbpedia. examples include asking for all the properties of a particular receptor, which would supplement the information that a linked data browser could present to a user, and asking for specific properties, like the supertypes of a particular protein.

while we used federate's transformation rules to map between proprietary and interface data structures, they can be used for mapping between arbitrary schemas, with the same execution efficiency. the interface side of construct mappings constitute descriptions of the data available at query services. these descriptions can be passed to users and researchers, helping them compose orchestration queries. the interface patterns may also be used by tools which associate these patterns with services in order to automatically generate orchestration queries, allowing users to ask questions without knowing which services are needed to answer them. methods like void can complement these descriptions, providing the necessary information for network query optimizers which consume sparql queries and equivalent queries, orchestrating efficient queries over sets of services.

description of linked data sets
to show how query federation may benefit from the machine-readable description of data sets, we created the mapping of neurondb receptor and neuron classes,  with dbpedia resources. the mappings were expressed using owl:sameas statements. part of the mapping was automatically created using the trial version of topbraid composer  <dig> beta  <cit> , a commercial ontology editor and semantic web platform. the automatic mapping result was then manually corrected.

in order to help application developers to understand how best to query these datasets and to use the mapping information, we adopted the void to describe the two datasets and their mappings. by the time of writing, this is the first real example demonstrating how void can be used to describe research datasets.

the great number of openly accessible datasets makes the integration of life science data more viable. however, the challenge remains to automate the discovery and selection of appropriate datasets needed by an application, and to understand how best to query the data and optimize these queries. linked data method encourages data providers to publish links between their datasets and others. being able to automatically locate such information will provide a higher point of departure for data integration. void is proposed to enable the discovery and usage of linked datasets or datasets accessible through sparql.

void is a rdfs-based vocabulary  <cit>  and a set of best practice instructions  <cit>  guiding the publication of and extension to void. the principle of the void effort is to use real requirements to guide the scope of the design, and to re-use existing vocabulary wherever possible. therefore, the creation of new classes and properties under the void namespace  is kept to the minimum. currently, void defines two classes in the void namespace: a void:dataset to represent a set of rdf triples that are published, maintained or aggregated by a single provider, and a void:linkset to represent the interlinking between datasets. details about the vocabulary can be found at  <cit> .

• the senselab dataset  is published by senselab  and contains information about receptors and neurotransmitter receptors. this information about the dataset can be used by application developers to discovery datasets relating to neuro receptors. application developers can also use the sparql endpoint and example uris given in the void description to start queries to this dataset.

• the dbpedia dataset  used in our use case is openly accessible under the gnu license and last updated on "2008-11-17".

• the mapping between :senselabontology and :dbpedia is published as a separate dataset, i.e., :senselabmapping, which is available at . the property void:subset defines that, conceptually, there are two sets of links published in this mapping file, one mapping the neuro classes  in the two datasets and the other mapping the receptor classes .

• :senselabneurotodbpedia is one set of the mappings. application developers can use the void description about this void:linkset to understand the type of the links created for senselab and dbpedia, to find out the number of links created in the mapping, etc. we could also publish example neuro uris to guide the query federation of these data sources.

• the complete void description can be found at  <cit>  void descriptions can be written in any rdf format . one recommended way to publish a void description is to advertise it in the semantic sitemap  <cit> , which is an extension to conventional sitemap files for describing the datasets hosted by a server. in this way, the void descriptions can be consumed by semantic search engines like sindice  <cit>  or data query federation engines like federate to provide dynamic federation of distributed datasets.

discussion
triplestore technologies have helped speed sparql query performance and make management of large numbers of triples efficient. the different triplestore technologies, however, make it impossible to have cross-access to the datasets residing in separate triplestores. sparql can help solve this issue of interoperability by allowing datasets in each triplestore to be accessed via standard sparql queries issued by clients  to the common sparql endpoint service. this approach allows cross-links to be created at the programming level. linked data is a different way of publishing data on the semantic web that makes it easy to interlink, discover, and consume data on the semantic web. linked data browsers  have been developed to allow the user to display data and traverse the links. in addition, it allows semantic web crawlers to crawl the linked datasets for indexing purposes. recently, third party tools such as pubby  <cit>  have become available for creating linked data interface to sparql endpoints. in addition, triplestores like virtuoso provide the linked data option as part of their functionality. in addition to making links and supporting data browsing, linked data should support flexible querying. projects like squin  <cit>  have begun to address this.

while we were in the process of establishing receptor mappings between senselab and dbpedia, we discovered a semantic mismatch between wikipedia and dbpedia in terms of neurotransmitter receptor classification. as shown in figure  <dig>  there is a wikipedia article page describing neurotransmitter receptors . on that page, a table is provided that lists the known neurotransmitter receptors. as shown in the table, different types  and subtypes  of neurotransmitter receptors have been identified. for example, glutaminergic receptor is a type of neurotransmitter receptor and nmda receptor is a type of glutaminergic receptor. when we compared this information with the structured description of neurotransmitter receptors in dbpedia , we noticed that neurotransmitter receptor is defined as an instance of the class "receptors" in dbpedia . in other words, neurotransmitter receptor cannot have any subclasses, which is not consistent with its definition in wikipedia. we reported such an inconsistency to dbpedia and the problem will be addressed. one outcome of our work is that it has led to an improved quality of dbpedia. in the future, these manual corrections can also be complemented by automated procedures, such as described in  <cit> .

the need for common uri identifiers is the following: in order to perform data integration or machine inference across multiple data repositories, we must be able to unify on the elements of the triples that we are using. another requirement is that we do not want the truth values for statements based on these identifiers to change because the identifiers change. for example, if the object of one triple is the name of a protein  and the object of another triple has precisely the same protein with the same name, then we can connect the triples and, effectively connect the rdf graphs in which they are stored. such unification is hard unless exactly the same identifiers have been used to refer to the same things, i.e. unless the identifiers are semantically-equivalent uri's. unfortunately, due to the unavailability of a central registry, the tendency on the web is towards both synonymous identifiers, i.e. different identifiers that refer to the same thing  and polysemy, or the use of the same name to refer to different things . these problems can be verified by visiting the ncbo bioportal  <cit>  and searching for a particular protein or disease. many essential concepts are reused in different ontologies and thereby acquire different uri's. if the semantics of the unmatched identifiers have been formalized, ontology alignment could be required in order to match them. if the semantics of the synonymous uri's haven't been formalized, the process of alignment can be even more costly and uncertain, sometimes requiring an interview process with the responsible parties. therefore, it is much more robust if the semantics of the identifier is available as rdf from the dereferenced http uri. the use of human-readable names as identifiers has a few disadvantages such as the fact that names can be deceiving and names can change. consider the name of the disease mycosis fungoides. the name actually refers to a non-hodgkins lymphoma, a disorder that has nothing to do with fungus but whose name implies a fungal relation due to a historical artifact. if we had classified this disease as a fungus-related disorder because of its name, this could have led us to faulty conclusions. in a second example, consider two data sources that refer to the dopamine receptor, one as 'dopaminergic receptor', the other as 'dopamine receptor'. such data sources would not unify if they relied on human-readable identifiers even though they refer to exactly the same thing. a central registry that can supply stable access to permanent identifiers could alleviate these problems. one attempt to establish a reliable and stable system for such identifiers is the shared names initiative  <cit> . in addition, scientific domain authorities play an important role in uri harmonization and nomenclature standardization.

workflow bears some similarity with query federation in the sense that it orchestrates services including query services to perform a  task. the results or data produced by a workflow can be fed as input to other workflows. data provenance  is an important topic in workflow. it is also relevant to query federation. for example, the integrated data/results produced by a particular query federation task can be tracked using a data provenance approach.

as rules are part of the semantic web stack , query federation can benefit from the use of rules and rule chaining. for example, a rule may be devised to infer a certain type of receptors as potential drug targets if such receptors are found highly expressed in a brain region under a certain neurological disease state as compared to the normal state. semantic web rule language   <cit>  and rule interchange format   <cit>  are two broad efforts of introducing rules to the semantic web . the rule responder system  <cit>  uses distributed rule-based inference services which are deployed on an enterprise service bus to implement complex conditional decision logic, distributed delegations/process logic as well as dynamic reaction logic. the rule responder middleware can directly be built on top of federate to access data via federated sparql queries and process this data using the rule-based inference services to answer and prove complex conditional research queries.

CONCLUSIONS
we have demonstrated the use of a set of state-of-the-art semantic web technologies in support of a neuroscience query federation scenario. the federation approaches presented in our paper meet different purposes. the aida approach is intended for someone who wants to explore the options by browsing and/or querying broadly against different data sources. it supports different search interfaces including text-based searching and sparql querying. the receptor explorer is designed for users who have a very specific focus. in this case, it allows receptor-based queries to be performed across different data sources. the user interface is designed for someone who may not be familiar with sparql. finally, federate is a query translation approach that is used by developers for implementing software systems that support query federation across multiple data sources in different formats, including legacy data sources such as relational databases.

while named graphs allow identification of data graphs, void can be used to provide a structured summary of the types of data contained in a single graph or interlinked graphs. our exercise of describing datasets using void has demonstrated the feasibility of this vocabulary for capturing key information about datasets that can be used to facilitate data discovery, such as the subject of the dataset, and to facilitate the use of the dataset, such as the sparql endpoint of the dataset. for example, a dataset containing a list of receptors may be summarized to have the data type "receptor", and a different dataset containing a list of drugs may be summarized to have the data type "drug". these void descriptions then could be used to reveal these datasets to a data link creator who is interested in establishing links between drugs and receptors . void can also be published for each dereferenceable uri of a dataset to define, for example, the dataset that this uri belongs to, allowing one to "follow-the-nose"  <cit> .

while the semantic web offers a global data model including the use of uniform resource identifiers , the proliferation of semantically-equivalent uri's hinders large scale data integration. in addition, the quality of data in terms of both content and structure has a great impact on the value of query federation. our work helps push the envelope of these semantic web technologies in relation to the biosciences. it also helps stimulate further research, investigation, and participation at the community level. our journey will lead us to a new frontier of life science web that enables federation of machine-readable and human-readable web resources.

future directions
we have identified the following areas for guiding our future work.

• we will identify new use cases including new data sets, which will demonstrate data integration of basic, clinical and translational neuroscience  research. we will incorporate new datasets as deemed appropriate into hcls kb's to increase both the scope and power of query federation.

• we will work with different communities including ontology and neuroscience communities to address areas such as uri harmonization, which require both technical and social collaboration.

• we will explore how to expand the individual federation approaches described in the paper as well as investigate how to combine them to increase the power of query federation. for example, with the maturation of void and its wider adoption by linked data publishers, we are expecting more full-fledged, dynamic data query federation enabled by the void descriptions about distributed data sources on the web. for example, we may restrict queries to datasets based on void descriptions such as date and size. a combination between void query engines and a rule-based query federation approach  <cit>  promises a powerful federation system in support of translational research.

• we will work on creating web applications that make creating, publishing and searching of rdf/owl intuitive and simple for biomedical researchers and clinicians.

availability and requirements
aida toolkit: the software is available at  and may be freely downloaded under the apache license.

federate: the data and software are available at  and may be freely downloaded and used with no license requirements.

hcls kb hosted by deri, galway, ireland 

hcls kb hosted by the freie universitaet berlin: 

competing interests
the authors declare that they have no competing interests.

authors' contributions
kc is the coordinator of this project and biordf task force.

hrf was responsible for the receptor explorer.

msm was responsible for the aida toolkit and contributed to the discussion of uri's.

ep implemented federate.

ms was responsible setting up the hcls kb at deri galway, the senselab-dbpedia mappings and the atags generator. he also contributed to the underlying biomedical use case and the generation of exemplary biomedical data.

jz was responsible for applying faviki to the generation of additional exemplar biomedical data and for creating void descriptions to describe the senselab and dbpedia datasets and their mappings.

ap was responsible for developing and setting up the complementary hcls kb at freie universitaet berlin and the rule responder api.

