BACKGROUND
in machine learning, model quality is most often limited by the lack of sufficient training data. in presence of data from different but related tasks, it is possible to boost the performance of each task by leveraging all available information. multi-task learning , a subfield of machine learning, considers the problem of inferring models for each task simultaneously while imposing some regularity criteria or shared representation in order to allow learning across tasks. there has been an active line of research exploring various methods , providing empirical findings  <cit>  and theoretical foundations  <cit> . most of these methods assume uniform relations across tasks. however, it is conceivable to leverage mtl methods by taking into account the degree of relatedness among tasks. recently, this direction has been investigated in the context of hierarchies  <cit>  and clusters  <cit>  of tasks, where the relation across tasks as well as the models for each task are inferred simultaneously.

in this paper, we follow this line of research and investigate multitask learning scenarios where there exists a latent structural relation across tasks. in particular, we model the relatedness between tasks by defining meta-tasks. here, each meta-task corresponds to a subset of all tasks, representing the common properties of the tasks within this subset. then, the model of each task can be derived by a convex combination of the meta-tasks it belongs to. moreover, the latent structure over tasks can be represented by a collection of the meta-tasks. information is transferred between two tasks t, t′ with respect to their relatedness according to the latent structure .

clearly, such an approach is highly sensitive to the chosen structure and in the absence of prior knowledge, learning the latent structure is a crucial component of mtl with non-uniform relatedness. starting from a special case, where there exists a single meta-task consisting of all tasks , we show that inferring the latent structure can be cast as a multiple kernel learning problem, where the base kernels are defined with respected to dirac kernels  <cit>  that establish relatedness of all possible task combinations and hence correspond to all possible meta-tasks.

kernel methods are used in a wide-range of problems, as the kernel abstracts the input space from the machine learning algorithm. one can use several kernels to incorporate different aspects of the same instance  and combine them into the same optimization problem. multiple kernel learning can be used to determine the combination of kernels that is best for the problem at hand. this is done by learning an optimal weighting of the individual kernels along with training a predictor.

our contribution is the combination of mtl and mkl to address the central question in multitask learning, of how to identify the relationships between tasks and to translate them into meaningful parameters in the formulation of the used learning algorithm. we show that mkl can be used to 1) refine a given hierarchical structure that relates the tasks at hand and 2) to identify subsets of tasks for which information transfer pays off in absence of prior information on task relations.

besides applications in natural language processing  <cit>  and medical domains, multitask learning is particularly relevant to computational biology. in this setting, tasks often correspond to organisms, giving rise to a whole range of problems. the fact that the availability of data describing the same biological mechanism in several organisms is a reoccurring theme makes the multitask learning approach particularly well suited for many applications in the field. there has been previous work using domain adaptation  in the context of splice site prediction  <cit> . furthermore, it was shown  <cit>  that multitask learning can be used to leverage the state-of-the-art in peptide mhc-i binding prediction, which is a problem relevant for vaccine design. given the success of mtl in computational biology and highly structured relation across organisms , we apply our method to two important computational biology problems, namely mhc-i binding prediction and splice site prediction. the competitiveness of our results shows the validity of our approach.

preliminaries
in a single-task supervised learning scenario, a sample of example-label pairs d = {}i= <dig> …,n is given, where the xi live in an input space x and yi ∊ {− <dig> } . the goal is to learn a function f such that f ≈ yi that generalizes well to unseen data.

before we describe our formulation of mtl as mkl approach, we briefly review the formulations of mtl and mkl that lay the foundations for our approach.

multitask learning
in mtl  <cit> , we are given one labeled sample dt for each of t tasks. similar to the single-task supervised learning scenario, we are now interested in obtaining t hypotheses ft, one for each task.

we will formulate our method based on the support vector machine , which has proven to generalize well  <cit> , scales to large amounts of training data  <cit>  and is able to incorporate arbitrary data sources by means of kernels . the generalization to other learning approaches appears straight-forward as we mainly consider the extension of kernels to reflect task similarity, although details regarding the learning of their linear combination may differ.

therefore, we start out with a regularization-based multitask learning method that was similarly proposed in the context of svms  <cit> . the basic idea is that models wt are learned simultaneously for all tasks. information transfer between tasks is achieved by sharing a general component  and enforcing similarity of each wt to w <dig> in the joint optimization problem via regularization. we use the following formulation, leaving out some constants for readability  

where l is the hinge loss, l = max{ <dig> − yz, 0}.

it was shown in  <cit> , that the dual formulation of the above corresponds to the standard svm using a modified kernel function:  

where kb denotes the base kernel that captures the interactions between examples from all tasks and

   

here, t denotes the task of example xi. in the above formulation,  is composed of the general kernel kb and the kernel δt,tkb that captures only intra-domain interactions. in  <cit> , the latter kernel is referred to as dirac kernel. a slightly more general formulation of  is the following, which allows to adjust the trade-off between the general kernel and the task-specific kernel:  

where β <dig>  β <dig> ≥  <dig> and β <dig> + β <dig> =  <dig> 

clearly,  is a convex combination of base kernels and thus a valid kernel. mkl is a technique to learn the individual weights of a weighted linear combination of kernels. thus, it seems natural to utilize mkl to learn an optimal weighting for .

multiple kernel learning
lanckriet et al. considered conic combinations of kernel matrices for classification  <cit> , leading to a convex quadratically constrained quadratic program. later on, it was shown that the problem can be formulated as a semi-infinite linear program, allowing to use standard svm solvers  for solving the reoccurring sub-problems  <cit> . only recently, methods were proposed to generalize mkl to an arbitrary lq-norm  <cit> .

learning with multiple kernels gives rise to m different feature mappings ϕm: x → hm, m =  <dig> …, m, each leading to a kernel km for a hilbert space hm. in mkl, we consider linear mixtures of kernels , where βi ≥  <dig>  to avoid non-convexity, the original parameter vector  is substituted . for an in depth discussion of this, please consider  <cit> .

we use the following formulation in the primal:

     

where l is the hinge loss, l = max{ <dig> − yz, 0} and q denotes the norm used to penalize the weights β. to solve the above optimization problem, we follow ideas presented in  <cit> , <cit>  to iteratively solve a convex optimization problem involving only the β’s and then to solve for w only. this method is known to converge fast even for a relatively large number of kernels  <cit> .

multitask multiple kernel learning
to be able to use mkl for multitask learning, we need to reformulate the multitask learning problem as a weighted linear combination of kernels. in the spirit of equation  <dig>  the basic idea of our decomposition is to define task-based block masks on the kernel matrix of kb. given a list of tasks t = {t <dig> …,tt}, we define a kernel ks on a subset of tasks s ⊆ t as follows:  

where t denotes the task of example x. here, each s corresponds to a meta-task as defined in the introduction. in the most general formulation, we define a collection i = {s <dig> …,sp} of an arbitrary number p of task sets si, which defines the latent structure over tasks. this collection leads to the following linear combination of kernels, which is positive semi-definite:  

using , we can readily utilize existing mkl methods to learn the βi. this corresponds to identifying the groups of tasks si for which sharing information leads to improved performance. after training using mkl, we have obtained a classifier ft for each task t:  

where n is the total number of training examples of all tasks combined.

what remains to be discussed is how to define a collection i of candidate subsets si , which are subsequently to be weighted by mkl. we consider two scenarios, one where we assume to have access to a hierarchical structure relating the tasks at hand and one, where we assume no prior knowledge given about task relations. generally, however, it is possible to utilize prior domain knowledge indicating how tasks are related to design an appropriate i.

powerset mt-mkl
with no prior information given, a natural choice is to take into account all possible subsets of tasks. given a set tasks t, this corresponds to considering the power set p of t  ip = {s|s ∈ p ∧ s ≠ Ø}.

clearly, this gives us an exponential number  of task sets si of which only a few will be relevant. to identify the relevant task sets, we propose to use an l1-regularized mkl approach  to yield a sparse solution. most subset weights will be set to zero, yielding only a few relevant subsets with weights greater than zero. we expect that the examples in these subsets come from similar distributions and that it is therefore beneficial to consider interactions between them, when obtaining a multitask predictor.

while l1-regularization of mkl results in a sparse combination of kernels, it does not address the computational complexity of the optimization problem over this exponential search space. with the current implementation, the method is limited to approximately  <dig> tasks depending on the number of training examples and available resources. however, there are techniques to handle the case where the number of tasks may become prohibitive, for instance, as proposed in  <cit> . the idea is to iteratively generate new kernels based on the current solution . these methods are known to converge to the optimal solution, if one can identify appropriate kernels in a larger set. in the current case, this could be done by solving an integer linear program.

hierarchical mt-mkl
in the second scenario, we assume that we are given a tree structure g that relates our tasks at hand . in this context, a task ti corresponds to a leaf in g. assuming hierarchical relations between tasks is particularly relevant to computational biology where often different tasks correspond to different organisms. in this context, we expect that the longer the common evolutionary history between two organisms, the more beneficial it is to share information between these organisms in a mtl setting. we can exploit the hierarchical structure g to determine which subsets might play a role for multitask learning. in other words, we use the hierarchy to restrict the space of task sets. let leaves = {l|l is descendant of n} be the set of leaves below the sub-tree rooted at node n. then, we can give the following definition for the hierarchically decomposed kernel function  

as an example, consider the kernel defined by a hierarchical decomposition according to figure  <dig>  clearly, the number of βi corresponds to the number of nodes. for a perfect binary tree this leads to 2m −  <dig> nodes, where m is the number of leaves/tasks. we expect that learning the contributions of the individual levels of the taxonomy makes sense for cases, where the edge lengths of g are unequal.

relation to task similarity
the learned weights βi reflect the importance of the subset si. clearly, if two tasks tk and tl are often jointly present in subsets with high weights, we expect those tasks to be similar to each other. one can infer a measure of pairwise similarities between tasks γk,l from the weights βi of the subsets si. we define the collection of task sets containing task tk as . using this definition, we can define the similarity γk,l between two tasks by summing up the weights of the shared task sets si

    

this similarity measure can be used for downstream analyses, as it provides insight about the task relationships. a high γk,l between tasks suggests a considerable resemblance between tasks and could help to generate domain knowledge .

RESULTS
we performed experiments in two settings. in the first setting, we considered a set of mhc-i  proteins. here, we assume we are not given any prior information to relate them. in the second setting, we used splice site data from  <dig> organisms and assumed that the task relationship is given by a hierarchical structure according to their evolutionary history. the examples are string data over an alphabet {a,t,g,c}  in the splicing case and the alphabet of  <dig> amino acids in the mhc-i case. to incorporate string features, we used the weighted degree string kernel  <cit> , which amongst other kernels such as the spectrum kernel  <cit> , has been successfully employed in problems from computational biology.

in addition to the two mkl-based methods, we evaluated the following baseline methods:

• union - one global model is obtained on the union of examples from all tasks.

• plain - for each task, a model is trained independently, not taking into account any out-of-domain information.

• vanilla mtl - our algorithms consists of two components - the mtl formulation and the adjustment of weights βi with mkl. in the vanilla approach, we fix all weights at βi =  <dig> 

experiments were performed by using cross-validation for model-selection on the training splits. we only tuned one hyper-parameter c, for which we considered values between  <dig>   <dig> and  <dig> on a logarithmic scale in  <dig> steps. after having obtained an optimal regularization parameter, a classifier is retrained on all training splits and final performance is obtained on a dedicated test set, that was not involved in hyper-parameter selection.

mhc-i binding prediction using powerset mt-mkl
in this experiment, the task is to predict whether a peptide binds to a certain mhc molecule  or not . it has been previously shown that sharing information between related molecules  and thus casting the problem in a multitask learning scenario, can be beneficial  <cit> . in the mhc setting, different tasks correspond to different mhc proteins. the data consists of peptide sequences of length l =  <dig> for  <dig> tasks. in total, we are given  <dig> examples . for cross-validation, the data was split randomly into  <dig> splits of the same size. unlike the setting of splice site prediction, we do not have a hierarchical structure relating our tasks at hand. to demonstrate that meaningful groups of tasks can be identified by powerset mt-mkl, we do not assume any prior knowledge of task relationships. please note, however, that we do have access to the sequences of the mhc-i proteins. we use these sequences to evaluate the task similarities obtained by our approach.

we report the area under the precision recall curve  for the individual tasks in figure  <dig> and the summary of performances in table  <dig> 

from figure  <dig>  we observe that the mkl-based approach outperforms the baseline methods. furthermore, simply combining the data for different tasks to obtain a single model  does not outperform the naïve method of obtaining an individual classifier for each task . this hints at rather large differences between the tasks. learning the weights with mkl, improves performance compared to the vanilla mtl approach, which already outperforms the other two baselines.

using mkl, we successfully identify groups of tasks among which information sharing is sensible, thus allowing for a smart combination of information from different tasks in the absence of prior knowledge.

the improvement in performance over the vanilla mtl method is relatively small . however, we are compensated for this by simultaneously obtaining a sensible task structure.

splice-site prediction using hierarchical mt-mkl
in this setting, we take into account a given hierarchy  relating the  <dig> organisms in our data set. the data set consists of  <dig> examples for  <dig> tasks, each at a positive to negative ratio of 1: <dig>  similar to the one used in  <cit> . the data is split into  <dig> splits, three splits with  <dig> examples each and a large test split with  <dig> examples. the dataset was created that way to establish a scenario where positive training examples are extremely rare.

we report the area under the precision recall curve , which is well suited for unbalanced data sets. for the vanilla mtl method, we use the given hierarchy g to define the initial task sets, but not further optimize their individual influence.

from figure  <dig>  we can make a few very interesting observations. first, in accordance with the results from the mhc-i experiment , the non-sparse hierarchical mt-mkl methods outperform the baselines union and plain.

the second observation is that we get different results for different q-norms. in particular, we see a degraded performance for q =  <dig>  which complies with our expectation that weights for this approach  should not be sparse. for the q-norms that we considered, q =  <dig> performs best. lastly, we can show that we are able to outperform the vanilla mtl method  by refining the task relations given by the structure g with mkl. intuitively, using hierarchical mt-mkl corresponds to estimating the edge lengths of g, whereas the other method is restricted to directly using the similarities encoded into the taxonomy.

CONCLUSIONS
we have presented a principle way of formulating multitask learning as a multiple kernel learning approach. following the basic idea of task-set-wise decomposition of the kernel matrix, we present a hierarchical decomposition and a power set based approach.

these two methods allow us to elegantly identify or refine structure relating the tasks at hand in one global optimization problem. we expect our methods to work particularly well in cases, where edge weights differ within the hierarchical structure or where the task structure is unknown.

our experiments illustrate that the mt-mkl approach on the power set of all tasks works well for the mhc binding problem: first it increases the accuracy of the predictors compared to the baseline methods and second, the inferred task similarity reflects the prior knowledge that is available for this problem. also for the splice site prediction problem where the task hierarchy is given by the organisms’ phylogeny, our approach manages to achieve an improvement over standard approaches. using mkl on top of regular multitask learning methods may uncover latent task structure and thereby provide insight into the problem domain, which might be relevant to downstream analyses. in conclusion, this work constitutes a valuable proof-of-concept outlining a principle way of using mkl to improve multitask learning.

list of abbreviations used
mkl: multiple kernel learning; mtl: multi task learning; mhc: major histocompatibility complex; svm: support vector machine; auprc: area under the precision recall curve.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
christian widmer worked out the idea and implementation, performed the experiments and prepared part of the manuscript. yasemin altun was involved in the discussions, the development of methods on which this paper is based and the preparation of the manuscript. nora c. toussaint contributed to the discussions, provided the data for the mhc-i experiments and contributed to the preparation of the manuscript. gunnar rätsch came up with the original idea for the project and supervised the project at each step.

