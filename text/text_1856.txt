BACKGROUND
next generation dna sequencers and online social media produce a data deluge requiring new tools for storage, representation, visualization, querying, interaction, and integration  <cit> . data streams are leading to interesting observations. google used a single information source, the search logs of terms entered by users, to predict a recent flu outbreak nearly two weeks ahead of the cdc  <cit> . developing more powerful methods requires integration of multiple information sources and connecting it in ways that are timely, relevant, and capable of distinguishing useful information from noise. some of these capabilities have been explored in simulation systems that model the spread of disease  <cit> . there is a pressing need for a bio-security system that will do this in near real time.

biosites
the biological signature identification and threat evaluation system  is a prototype effort to develop a secure, authoritative, predictive and complete reference standard for bio-threat detection and mitigation that will support detection r&d and lead to near real-time bio-surveillance and bio-threat identification. biosites is currently in incubation status at oak ridge national laboratory; the work discussed in this paper is part of the biosites effort.

biosites implements methodologies that deal with complex bio-surveillance data integration requirements. no single institution owns all of the data necessary for bio-surveillance. it is highly heterogeneous and requires petabytes of storage space. the biosites knowledgebase is being constructed assuming that the data infrastructure will be composed of multiple distributed and interoperable data repositories serving as reference catalogs. this work is mainly focused on the principles for constructing a semantic knowledgebase capable of integrating diverse data repositories and data streams. the section titled ‘the biosites streaming data kernel’ provides an overview of how data streams through the biosites system and clarifies requirements of the biosites knowledgebase. a more thorough discussion of the streaming kernel is the topic of a pending publication.

the data integration challenge
bio-surveillance requires rapid integration of vastly different types of information. resources such as the antibiotic resistance genes database  <cit> , mvirdb  <cit> , supertoxic  <cit> , and pig – the pathogen interaction gateway  <cit>  are all excellent examples of molecular catalogues used in bio-surveillance applications. these repositories can be combined with geo-location, time and important information from the literature to form a model of the entities and relationships involved as a disease spreads. a key challenge is to codify this heterogeneous data and information in a computationally useful manner that meets bio-surveillance requirements. the first step to doing this is to integrate the data in a semantically consistent data model.

in bioinformatics, dominant approaches to heterogeneous data integration include:  an operational data store such as chado  <cit>  – where heterogeneous data is integrated into a unified model by importing each data source into a unified relational schema, and  ‘knuckles-and-nodes’  <cit> , where data is distributed and integrated through the use of ontologies. biosites requires the ability to integrate heterogeneous data streams, and distributed reference catalogs, making an ontology centric approach more appropriate.

the knuckles-and-nodes approach is also used in the semantic web  <cit> . the semantic web is to data what the world wide web is to documents; a globally linked data store where data elements are distributed across multiple servers on the internet. the semantic web’s representation is a multi-relational directed graph  implemented through the resource description framework   <cit> . a large part of the semantic web, relevant to genetics and biology has been constructed by the bio2rdf project  <cit>  and this vast resource was used in the construction of biosites.

this article introduces scenario driven data modelling . sddm is a data integration process that defines the required relationships between diverse sources of data, data streams, and software components. sddm is closely related to the concept of data mashups or “purpose driven, customized data integrations that facilitate question answering on a topic of interest”  <cit> . scenarios  are like mashups, they are purpose driven and use similar integration strategies, but are larger in scope and complexity because they include analytical methods and software components designed for analysing data that exists outside the semantic web. these analytical methods and software components analyse data as a stream, and when they find matches, they integrate data into the semantic web.

significance
people consume content on the web. to navigate the web, a person starts with one article, and then navigates to other articles; building an interpretation of the world as they follow hyperlinks, consume additional content, and perhaps ‘teleport’ randomly to other articles and topics. data is presented on the web when it is imbedded into human readable html or images and served on request. as the amount of data available is exploding, it is increasingly likely that people traversing the web will miss important content. machines need to be used to help augment human search capabilities and reduce the time it takes to find and communicate important information. however, with data represented in free text, machine interpretation is limited because we have not discovered a general-purpose way to compile natural language down to a level understood by computer programs. it is however possible to write simpler software with very limited goals that has a limited understanding of free text - search engines being good examples.

before delving into the details of the sddm process, we will first discuss the significance of using the semantic web as a platform for bio-defense. internationally, professionals and researchers are currently developing assays, assembling information, coordinating prevention efforts, notifying first responders and the media, and identifying and characterizing new pathogens. while each of these efforts has merit, specialization tends to create silos where individuals who need to communicate with each other may not connect because they have not each identified the value in the connection. a top down solution to this problem is to build cross-cutting communities focused on integrative approaches. a bottom up approach to the problem uses social networks to connect individuals and focus on specific problems. both of these approaches are important, but focus on solving integration problems through people and personal connections. in both approaches, software systems are isolated systems, integration of information takes place in the humans who use these systems. near real time bio-threat prevention and detection requires integrated software systems.

a more powerful alternative approach to isolated systems is an integrated and global platform that includes the semantic web and its capabilities to ascribe meaning to and draw inferences from data. sddm allows automated integration methods that becomes extremely useful when dealing with large, complex streaming data that otherwise would not be humanly possible to accurately and reliably integrate. sddm enables distillation of massive data amounts and computing of analyses into a set of facts and a resolution of their accuracy for decision making. the rdf data representation also accommodates changes in representation and ownership beyond what has been previously possible.

RESULTS
utilizing sddm, we defined and constructed a system that integrates heterogeneous data based on a bio-surveillance scenario that operates in near real-time, meeting scenario requirements. the sddm process and the example of its implementation presented here, represents an iterative refinement of a data integration process. sddm combines data feeds, private and public data stored in files or databases and the semantic web. sddm starts with a scenario, or free text description, and then gradually updates the scenario into a multi-relational-directed graph encoded in rdf. data stored in flat files or databases is integrated by constructing software called rdfizers  that convert traditional data representations into multi-relational-directed graphs . rdfizers were built for promed to link molecular concepts to time and geo-location. resources available on the semantic web  were used to link molecular concepts to dna and protein sequences.

the sddm process also integrates streaming data. the resulting software prototype enables streaming data from genetic sequencers to be compared to sequences identified in bio2rdf, and, when a match is found, rdf statements integrate the new findings into the semantic web. this data integration approach was applied to the construction of a near real-time and completely automated analysis system capable of monitoring the ndm- <dig> gene globally. this proof of concept further illustrates the power and flexibility of ontology-centric data integration approaches.

the biosites streaming data kernel
the biosites streaming kernel looks at feeds of data and attempts to match elements in the knowledgebase to data in the stream. if data in the stream is matched to specific elements in the catalog, then an advisory is published to alert first responders and analysts. these advisories feed the decision support system. the system architecture consists of four subsystems: data feed generation , data stores , content delivery  and the streaming kernel. figure  <dig> illustrates a module view of the biosites architecture that shows the system’s principal components. biosites detectors feed a massive amount of data continuously to the biosites system. example feeds include web crawlers, dna sequencing machines, logistics information, weather sensors, and satellite imagery. as this information streams past the biosites system, useful connections and knowledge are automatically extracted and then integrated directly into the semantic web or presented to users.

although there are many combinations of data important in situational awareness and response, this article focuses on data integration dealing with time , space  and genetics .

scenarios
a biosites scenario is a structured description of a malicious action or series of actions that causes harm or disruption in health, the economy, or day–to-day activities in people, crops or livestock. a scenario could be based on real world events or be hypothetical. a scenario need not be confined to a specific organism or spread mechanism, but these could be attributes of a specific scenario. outside of the bio-security arena, a scenario could be used to improve public health by addressing naturally occurring pathogens. a scenario identifies the downstream data users and uses. these drive data collection and refinement requirements. biosites scenarios are concerned with identifying data streams that provide temporal, spatial and sequence information when combined. a scenario is used to scope the software development process. for example, a scenario requires one or more catalogs that are later used by sensors of the biosites kernel . a scenario also identifies possible data streams that will be monitored and algorithms that can be used in monitoring those data streams. a scenario may be extremely specific or more general in the way that it references data stream elements and catalogs. for example, a scenario could state ‘a toxin was used’ instead of ‘botulinum toxin was used’.

a scenario begins as a free text document. an expert constructs a scenario, integrating the sensitivity and specificity of each data source. this results in an integration of information that is not misleading to users. elements in scenario documents map directly to software components used in the biosites streaming kernel. each scenario document corresponds one-to-one with a biosites controller responsible for generating advisories related to that scenario. the controller is a state machine that when it finds all of the conditions required  in the scenario, it publishes an advisory and integrates information into the biosites data store.

molecular biology is just too complex to cover every possibility. simple scenarios give us a starting point and basic capabilities. these capabilities can be leveraged later to discover trends and patterns present in multiple scenarios.

the biosites data store
the biosites data store  combines data that exists inside the semantic web and data available on the internet. data available on the internet is handled as data streams that flow through biosites analysis routines . these analysis routines compare a reference catalog to data found in the stream. if a match is found, it forwards the information to the controller. the controller can then choose to integrate the structured information delivered by the sensor into a local rdf model that represents our current understanding of events, genetic elements, places, people and other variables. this rdf model is connected, via links to other resources on the semantic web, so as other information is deposited, the rdf model improves.

example of sddm applied to a global health threat
this section provides a demonstration of the sddm approach, building a multi-relational directed graph about the ndm- <dig> gene  <cit>  that confers antibiotic resistance to some of the most powerful antibiotics and is an emerging global health threat. a scenario was constructed based on the publication by kumarasamy et al.  <cit> , “emergence of a new antibiotic resistance mechanism in india, pakistan, and the uk: a molecular, biological, and epidemiological study”.

sddm step 1: a scenario is selected for refinement
in the first step kumarasamy et al. is analyzed and specifics are added about the genetics of ndm- <dig> , times when ndm- <dig> has appeared, and geo-locations where ndm- <dig> has appeared, to construct the following scenario:

antibiotic resistance gene ndm- <dig> located on gram negative plasmid

gram-negative enterobacteriaceae with resistance to carbapenem conferred by new delhi metallo-β-lactamase  <dig>  is a public health threat.

“resistance to beta-lactam antibiotics has become a particular problem in recent decades, as strains of bacteria that produce extended-spectrum beta-lactamases have become more common. these beta-lactamase enzymes make many, if not all, of the penicillin and cephalosporin ineffective as therapy. extended-spectrum beta-lactamase–producing e. coli are highly resistant to an array of antibiotics and infections by these strains are difficult to treat. in many instances, only two oral antibiotics and a very limited group of intravenous antibiotics remain effective. in  <dig>  a gene called new delhi metallo-beta-lactamase  that even gives resistance to intravenous antibiotic carbapenem, were discovered in india and pakistan in e. coli bacteria. ndm- <dig> is known to exist on a plasmid and has been transferred across bacteria via horizontal gene transfer.

increased concern about the prevalence of this form of "superbug" in the united kingdom has led to calls for further monitoring and a uk-wide strategy to deal with infections and the deaths. susceptibility testing should guide treatment in all infections in which the organism can be isolated for culture.” <cit> 

• biosites detector  <dig> : first responders/clinicians isolate the laboratory strain using the techniques outlined in the cdc manual  <cit> . when the patients remain ill, the samples are sequenced with a  <dig> pyrosequencer. geo-location of where the sample was obtained is imbedded in metadata accompanying the sample. note, this includes all  <dig> runs taken from the sick, not just e. coli infections.

• biosites detector  <dig> : as new promed articles are published, software serializes the articles and streams them through the biosites kernel. note these articles may not relate to ndm- <dig> 

• biosites data catalog_ <dig> : three representative sequences were found at ncbi with gi numbers:  <dig>   <dig> and  <dig> 

• biosites data catalog_ <dig> : occurrence of ndm- <dig> can be found in historical promed articles:  <dig> ,  <dig> ,  <dig> ,  <dig> , and  <dig> .

• biosites sensor method_ <dig> : a nucleotide level blast  for detecting a match between catalog_ <dig> and the sequence reads from detector  <dig>  the blast alignment must match the region of the sequence corresponding to the ndm- <dig> gene.

• biosites sensor method_ <dig> : gps coordinates are obtained for the source location of the sample and imbedded in the metadata of read sets from sequencing machines. each sample obtained will be scanned to identify a match to positions in catalog1003_ <dig>  if the gps coordinates do not match coordinates in the existing catalog, then this indicates a possible new location where ndm- <dig> has spread.

• biosites sensor method_ <dig> : promed mail articles are scanned using data catalog_ <dig> for additional content relating to ndm- <dig> 

sddm step 2: create first iteration of rdf multi-relational directed graph
step  <dig> of the sddm process focuses on conversion of the scenario into a multi-relational directed graph. to convert the scenario into a mrdg, biosites scenario ontology and protégé  <cit>  are used to re-write the scenario into a rdf/xml document. creation of the rdf graph begins with a single node, representing the biosites scenario document . rdf statements then connect the biosites scenario to resources on the semantic web. this example scenario is based on the pubmed article  <dig>  so the scenario:sourcearticle property was used to connect the scenario object to the bio2rdf_pubmed: <dig> article . this indirectly links mesh terms  <cit> , go terms  <cit> , pubmed articles  <cit>  and similar entities already connected via the bio2rdf project. these terms and articles can be used to find scenarios that are related. this multi-relational directed graph is not a static entity, as more information is identified; the graph is updated . this multi-relational directed graph links resources already on the semantic web into a data structure that is used by biosites sensors. the result at this stage is a minimal graph in that it only represents what was included in the scenario with direct links to extermal resources on the semantic web. the multi-relational directed graph constructed in this step of the sddm process links location , dates , and molecular objects .

sddm step 3: identify and convert to rdf those resources required by the scenario but absent in the semantic web
the construction of the two catalogs described in the scenario, ndm- <dig> genes and promed articles related to ndm- <dig> , illustrates two common situations. the first situation links elements in the scenario document to elements that already exist  in the semantic web using rdf statements. the second situation involves creating new semantic web content by rdfizing data represented in traditional structures and then linking the scenario document elements to the newly created semantic web content.

in this example, a search was used to identify examples of the ndm-1gene. the current bio2rdf release  did not contain annotated sequences for the gi numbers outlined in the scenario. this required the construction of bio2rdf entries for each of the sequences identified in the scenario. the bio2rdf software suite was used for the conversion  <cit> . an rdf catalog  was created to link the scenario object to each known instance of the ndm- <dig> gene . interproscan was used to connect each bio2rdf_gi instance linked by catalog_ <dig> to functional annotation on the semantic web . functional annotation properties include gene ontology terms, hmmer  <cit>  profiles, and scientific terms. this functional annotation connects entities identified in this scenario to catalogs used in other scenarios that have similar functional annotation. in the future, this will help generalize scenarios and perhaps even automatically generate them. for example, ndm- <dig> is annotated as a beta-lactamase, which based on sequence similarity, matches several entries in the mvirdb database  <cit> . mvirdb is applicable in many scenarios involving category a agents.

a second catalog, data source  <dig>  , was constructed to represent historical articles from promed. an rdfizer was constructed to convert historical promed articles into rdf. for each article, we extracted the article id, date, subject line and text, and populated a simple rdf graph. promed articles referencing ndm- <dig> were mined to identify the time and location of ndm- <dig> events. the technology does not yet exist to reliably extract locations from the articles and associate them with events, so manual extraction of the locations for those articles relating to ndm- <dig> was required. three rdfs:classes where used in construction of the multi-relational directed graph representation for the disease spread described in promed; scenario:event, scenario:outbreak, and geonames:feature. these rdfs:classes  <cit>  correspond to time , time intervals , and geo-location . a scenario:event describes a real world event as it is described in promed. data associated with this event  are embedded as rdfs:literals  <cit>  and associated with the event object. concepts described in the event such as symptoms, locations, genetic sequences and other articles are described by resource description framework schema   <cit>  statements that link the scenario:event to rdfs:resources on the semantic web. figure  <dig> illustrates how events are linked to locations, using the rdfs statement <scenario:event1003_ <dig> scenario:haslocation geonames:2635167>. geonames: <dig> represents the united kingdom. geonames maintains synonyms for this location, gps coordinates, elevation, population, and other geographic specific information. scenario:outbreaks are used to connect sets of scenario:events. a scenario:outbreak contains events that co-occur in time and have an occurrence of a disease/infection greater than would be expected in an area. scenario:outbreak objects are used in the system to model disease spread over time and associate disease spread with genetic variation. figure  <dig> shows a simplified version of the multi-relational directed graph constructed as a result of steps  <dig> and  <dig> of sddm process.

sddm step 4: identify data feeds
this set of the sddm process primarily focuses on the identification of data sources . in the ndm- <dig> scenario, we identified the sequence read archive at ncbi  as a source of genetic information , and promed  <cit>  mail as a source for geo-location and time based information. for the sequence data, we implemented a biosites detector that monitors ncbi entrez for new or updated submissions to the sequence read archive . if a new or updated submission is detected, reads from the submission are streamed across the network to every computer running a sensor that subscribes to the sra biosites detector. a program to automatically download new promed articles as they are published and stream them through the biosites kernel was developed. articles in the promed data stream are not automatically converted to rdf in the streaming process. however, if a new article matches a catalog in the biosites system  , then relevant elements of the article are extracted and represented in rdf. this process, although subjected to some automation, was mostly a manual effort.

sddm step 5: identify analytical routines for comparing information in the data stream to the concepts outlined in the scenario
scenarios require the use of one or more sensors to analyze streams of data coming from detectors. in the ndm- <dig> scenario, the sensor  was defined to compare sequences in a data stream to the ndm- <dig> catalog  based on alignment. in this case, we used the biosites blast sensor to match reads in the read set to sequences in catalog_ <dig> . when a read set matches a sequence in catalog_ <dig>  rdf statements describing the match can be added to the multi-relational directed graph .

the ndm- <dig> scenario also required the construction of an additional sensor  to compare global positions and text in promed articles . newly published promed articles are scanned to identify delhi metallo-β-lactamase  <dig>  or close synonyms. if the article matches, it is then scanned for location matches in the geonames database. all matches are geo-coded and associated with one or more new events. this information is serialized to the ndm- <dig> controller where it is converted into rdf.

sddm step 6: based on the scenario and the resulting mrdg, identify the data outputs that meet end-user decision support needs and define queries to produce those outputs
the last step in an iteration of the sddm process requires using the scenario derived data model as a queryable resource of decision support information for consumption by an end-user. in the ndm- <dig> scenario, the following questions can now be answered by querying the data model:

-when did the ndm- <dig> events occur?

-in what locations around the globe did ndm- <dig> occur?

-what genetic variation in the ndm- <dig> gene was found?

each of these questions can be answered by traversing the multi-relational directed graph starting at the scenario node. for example, to find the dates of all ndm- <dig> events, first follow the outgoing edges from scenario:scenario <dig>  if the node reached is of type scenario: event, then report the date property. otherwise, if it is of type outbreak, follow all outgoing edges to reach nodes of type scenario:event and report their date . graph based traversals can be easily extended to support queries to find geo-location and genetic variation information on this ndm- <dig> data model. more complex queries are also possible using path algebra  <cit> . sensors continuously deposit more rdf statements into the multi-relational directed graph as they analyze data streams. as the multi-relational directed graph gets updated, the traversal behaviour and results will change. this forms the basis for user interfaces.

traversing the generated model
from a technical perspective, traversing the model generated by sddm requires constructing programs that crawl the linked data on the semantic web. here we will illustrate how these programs can be constructed using the gremlin programming language . gremlin is a scripting language optimized for traversing graph structures, much the same way a person may navigate the web. gremlin is built on top of the groovy programming language which itself is built on top of java. java code and groovy code will both work inside of gremlin. gremlin itself sits in a technology stack for interacting with graph databases that includes pipes, blueprints, rexter, and many graph databases including neo4j, orientdb, dex, and rdf sail graphs. gremlin is similar to sparql, it is a language for expressing queries over multi-relational graphs. gremlin is functionally superior to sparql, but lacks the intuitive and simple syntax. both gremlin and sparql can easily be imbedded into software written in java. here we will limit the discussion of the gremlin features to those queries alluded to in the previous section.

the first query considered in the previous section was when did the ndm- <dig> events occur? the following gremlin code loads the scenario rdf model into memory, adds namespaces to increase usability and readability, seeds the start of the query at the scenario <dig> node  and then traverses the model. a list of common namespaces is in table  <dig>  the traversal starts at node  <dig> . the statement v.out retrieves all of the nodes of type biosites:outbreak reachable from v. a statement of type v.out() gets the uri of the node, a statement of type v.oute() gets outgoing edges from v. these statements can be chained together to complete a query. an example of such a traversal is shown below.  

note, that the semantic web is easily traversed using the same techniques. for example to traverse the bio2rdf repository from ’bio2rdf_interpro:ipr001036’ ,do:  

this allows programs to update information when the local copy gets out of sync with the copy at bio2rdf.

the second question, ‘in what locations around the globe did ndm- <dig> occur?’ is answered by a very similar traversal. the added .value asks gremlin to display only the values on the property nodes and not the uri. this is also easily applied above to get the value for dates.  

to answer the question, ‘what genetic variation in the ndm- <dig> gene was found?’ we wrote the following script that traverses the graph for protein sequences, and then does a system call to the muscle multiple sequence aligner.  

the output of this traversal is as follows:  

we have only begun to scratch the surface of what is possible using gremlin to traverse graphs. as a jvm language, gremlin has the full power and flexibility of java and groovy to implement graph traversal algorithms. these algorithms can manipulate multiple graphs, traverse the semantic web, cache important subgraphs for use as catalogs, and process data streams.

discussion
the sddm process was invented to solve a very daunting challenge; to be able to detect an outbreak as early as possible and perhaps even prevent it. early detection requires the capacity to monitor data feeds such as online medical records, detectors such as jbaids, doctor’s blogs and social media for early indicators of an outbreak or a new disease. prevention requires the ability to look deeper into the causative agents of the disease such as genetic components and quickly manufacture and deploy countermeasures. the capacity to do both these well is found in our ability to consume a massive amount of data as it is produced in real time, in our ability to integrate multiple sources of information of widely different types and our ability to understand the limitations and potentials of each data source.

the simple example presented here falls short of completely addressing this grand challenge in several respects. first, there is increasing evidence that the mainstream media  can be quite slow to respond to an outbreak. for example, in the recent e. coli scare in germany, promed sent it’s first alert on may  <dig>   <dig>  however the first cases occurred nearly a month earlier, other sources of information are possibly better suited to early detection than promed and the mainstream media. while genetic sequencing has begun to be used as a diagnostic tool, it is not currently easily accessible globally. however, unlike pcr, sequencing will not miss subtleties such as signatures of genetic engineering or novel horizontally transferred genes. so, recognizing that they are not perfect, we chose these two feeds in our sddm example because they represent very different data types, and therefore have different properties. from a technical perspective, a demonstrated capability to integrate data as divergent as free text and dna sequence makes integration of more structured information such as medical records, output from diagnostic tools, and over the counter sales believable.

these data sources are not representative of all that is needed in bio-surveillance they are just two examples. the focus of this article is not to pit one data feed against another. rather, the requirements of bio-surveillance will be met by simultaneous integration of tens or perhaps even hundreds of data feeds. sddm is a technique that integrates multiple sources of information, each with different advantages, resolutions, timeliness, and capabilities. sddm benefits from rdf as a robust and flexible data model for representing information of different types. rdf can effectively represent geographic positions, events, timelines, genetic features, people, social networks, publications, and even concepts extracted from free text. another rdf advantage is that researchers in different fields can cooperate and build extremely complex models. sddm takes advantage of these models using dbpedia, geonames, bio2rdf, and other ontologies and resources on the semantic web. a second attribute of sddm is that algorithms, processing methods and analytic routines are custom built. this allows the scenario writer to focus on the needs outlined in the scenario instead of building general purpose software that is overly complex and not based on meeting objectives. sddm uses the idea of a scenario to reduce complexity. the scenario describes, in concrete terms, the goal of the integration. elements outside of this goal are ignored. to be viewed as a success, a scenario driven integration need only meet the requirements specified in the scenario. it is up to the broader community to determine if the scenario meets their needs.

a common criticism regarding the use of scenarios is that they may help model past events, but ‘lightning never strikes the same place twice’. the argument is that scenarios help prepare us for something that will never happen again and will not prepare us for the next threat. the true power of this approach emerges when the collection of scenarios grows. it is built like wikipedia, one article is not very useful, the true power of the platform is realized when thousands of articles are contributed. we can never anticipate every possible threat, but a library of known threats provides the capability to generalize. it may be possible to connect scenarios in unforeseen ways and thus build software that is broadly applicable. this is possible because causative agents will exist in the rdf graph . a simple way to connect two scenarios is through a causative agent they both share . shortest path algorithms are one way to find if two elements are related in a graph. if two elements are related, then software can be generalized to detect both elements.

materials and methods
application of sddm to the ndm- <dig> example produces a scenario with detectors, catalogs, and sensors for the biosites system. here elements of biosites are further described to provide context for how a scenario derived from sddm would be processed.

software and techniques used in the streaming data kernel
each component of the streaming data kernel was coded in java. each component or program continually runs; when it receives a new message, it processes the message and then may send messages to other software components. activemq was used to connect each of the following biosites software components: detectors, sensors, routers, and controllers. software component a is connected to software component b using publish-subscribe. briefly, activemq implements publish-subscribe through the use of a broker. brokers run continuosly. they can run on the same machine as the program that wishes to send messages, on the same machine where the program receiving messages resides or on a totally different machine. programs that wish to send messages, send them to the broker using the activemq api. programs that wish to receive messages connect to the broker and notify it that they wish to receive all messages sent by a particular program. activemq is a messaging protocol that allows components to be coded in many languages, including c++, .net, perl, java, python and ruby.

additional languages and technology were also used in the construction of the streaming data kernel, mainly due to programmer choice. the sequence read archive detector was written in perl. it queries entrez  <cit>  on a regular schedule  to find new submissions. when a new submission is found, it downloads the data and metadata  for each submission. the perl program then calls a java executable that uses jaxb  to parse the contents of the xml-metadata into java objects, place the content of these objects into a message, contact the broker, and forward a message to downstream routers. this message also contains a uri corresponding to a local location where the raw sequence data is stored. routers pick up this message and examine the metadata of the message and forward the message to sensors interested in reads of a specific type. the sequence read archive for example currently contains ‘whole genome sequencing’, ‘metagenomics’, ‘transcriptome analysis’, ‘resequencing’, ‘synthetic genomes’, ‘forensic or paleo-genomics’, ‘gene regulation study’, ‘cancer genomics’, ‘population genomics’, ‘rnaseq’, and ‘other’. sensors or analysis algorithms will only be interested in a subset of these study types, so they will subscribe only to those studies of interest through the activemq broker running on the routing machine. sensors receive the message, download the raw sequence data, and begin computational analysis on it. in the case of the ndm- <dig> blast sensor, the sensor has a copy of the ndm- <dig> sequences. when it receives a message that new sequences are available, it downloads a subset of the data, performs a system call to format the blast database on the new sequences, and then performs another system call to run blast for all of the ndm- <dig> sequences against the newly formatted blast database. it then parses the output, and if there is a match, publishes the match to an outgoing activemq queue. controllers that will render the advisory then consume this message. the ndm- <dig> sensor updates its copy of the ndm- <dig> sequences periodically by connecting to the server that contains the ndm- <dig> scenario through an activemq broker. when the server that contains the scenario receives the request, it performs a query to find all sequences on the server related to the ndm- <dig> catalog, and then sends the updated sequences back to the ndm- <dig> sensor in fasta format. a similar method is used when the sensor has found a new ndm- <dig> gene and would like to add it to the catalog on the scenario server.

the promed data stream was constructed using the groovy programming language. this program periodically looks at the sequential id’s generated by the promed mail website when a new article is published. the promed mail detector forwards the article via an activemq broker where it is picked up by the promed ndm- <dig> sensor. this sensor has a copy of all promed mail articles relating to ndm- <dig>  currently, it does a pattern match on the incoming article to determine if it is similar to the ndm- <dig> articles in the reference catalog. if a match is found, it forwards an rdfized version of the article to the ndm- <dig> scenario server where it is added to the catalog. this consists of parsing the title, date, promed id and content into rdf statements. it is not currently able to infer structure about entities in the article and create rdf statements from raw text.

hardware used
the biosites kernel is currently being run on a linux system with  <dig> compute nodes, a management node, a backup management node and a data staging storage system. each compute node has dual amd opteron  <dig>   <dig>  ghz processors . each compute node has  <dig> gb ddr <dig>  <dig> mhz registered ecc memory  and  <dig> x  <dig> tb disk drives . data staging storage with  <dig> tb raid array. local storage on the nodes is currently 62% allocated, mostly by short read data and web data on the hadoop filesystem. this system currently runs  <dig> concurrent analysis algorithms or biosites ‘sensors’. these sensors have various functions, but a majority perform sequence analysis. the system also currently runs up to  <dig> concurrent ‘detectors’ that stream data for analysis to the biosites system. a single server in the same class as each node in the cluster is used to store our bio2rdf mirror. each running controllers are also running on the same hardware,  <dig> controller per instantiated scenario .

to contribute a scenario, or to add additional elements to the multi-relational graph as it is refined, a knowledgeable user need only use a personal computer and the protégé software. protégé can import external resources such as ontologies given they are of manageable size.

software and techniques used in the semantic catalogs
an rdf catalog is not all that different from html. rdf can be stored on files in the filesystem and then served by a webserver just like html content. for advanced queries, rdf can also be stored in a graph database. examples include orientdb, neo4j, dex, jena’s tdb, allegrograph and virtuoso . graph database queries are executed in sparql or gremlin depending on the database. bio2rdf consumes about  <dig> tb in size for a complete mirror, and each shard of the database is stored in a virtuoso instance. neo4j was used to import the promed mail archive for easy searching.

in the case of ndm- <dig>  the scenario object was copied from the ontology editing computer and rdf triples where imported into a neo4j sail store for access over rexter. this allows a very small scenario model  to interoperate seamlessly with bio2rdf, geonames, and dbpedia infrastructures. each of these resources provides sparql endpoints for querying and http access for navigation. graph traversals are implemented in gremlin.

CONCLUSIONS
the sddm process uses a scenario to create software and a data representation. the result is a multi-relational directed graph containing the entities and relationships between entities required for the biosites system. this allows the biosites system to perform distributed, heterogeneous data integration in a manner not otherwise possible with traditional data modeling techniques.

this work provided an example where sddm was successfully applied to complex data integration challenges. this example was chosen to be relatively simple for clarity. the process created a model of the emerging ndm- <dig> health threat, identified and filled gaps in that model, and constructed reliable software that monitored data streams based on the scenario derived multi-relational directed graph. because of the complexity of the problem, the sddm process significantly reduced the software requirements phase by letting the scenario and resulting multi-relational directed graph define what is possible and hence set the scope of the user requirements. approaches like sddm will be critical to the future of data intensive, data-driven science because they automate the process of converting massive data streams into semantic graphs or computable knowledge.

competing interests
the authors declare that they have no competing interests.

authors' contributions
dq and tb conceived of the idea of sddm. dq, tb and sg constructed the scenario. dq designed and built the scenario ontology. dq and sg built the rdf graph for ndm- <dig>  dq wrote the content for the article. all authors reviewed, edited, and contributed to the final manuscript.

glossary
annotation- is a note that is made while reading any form of text.

biosites- is a comprehensive program to develop a secure, authoritative, predictive, and complete reference standard for biological threat mitigation that will support detection r&d and lead to near real-time bio-surveillance.

catalog- a catalog is a collection of objects from a data repository. biosites catalogs are constructed by making a scenario:catalogx node and then connecting that node to other nodes on the semantic web.

data mashup or mashup - a purpose driven, customized data integration that facilitates question answering on a topic of interest.

edge – the connection between two nodes in a graph is called an edge.

event- single incidents that happen at a determinable time and location.

namespace- a namespace name is a uniform resource identifier . typically, the uri chosen for the namespace of a given xml vocabulary describes a resource under the control of the author or organization defining the vocabulary, such as a url for the author's web server. in rdf and owl namespaces are abbreviated namespace:content, where namespace is a uri prefix, and content is the ontology term or individual data item.

ndm-1- is an enzyme that makes bacteria resistant to a broad range of beta-lactam antibiotics. these include the antibiotics of the carbapenem family, which are a mainstay for the treatment of antibiotic-resistant bacterial infections. the gene for ndm- <dig> is one member of a large gene family that encodes beta-lactamase enzymes called carbapenemases. bacteria that produce carbapenemases are often referred to in the news media as "superbugs" because infections caused by them are difficult to treat. such bacteria are usually susceptible only to polymyxins and tigecycline.

node – graphs are constructed from nodes and edges. nodes represent data or ontology classes. edges connect nodes together in the construction of a graph.

object – also called an rdf individual, this is a specific instance of a data element. in multi-relational directed graphs, nodes represent objects.

ontology- in computer science and information science, an ontology is a formal representation of knowledge as a set of concepts within a domain, and the relationships between those concepts. it is used to reason about the entities within that domain, and may be used to describe the domain.

outbreak- are occurrences of a disease/infection greater than would be expected in an area.

owl- the web ontology language  is a family of knowledge representation languages for authoring ontologies. the languages are characterised by formal semantics and rdf/xml-based serializations for the semantic web. owl is endorsed by the world wide web consortium .

property – an ‘edge’ in a multi-relational directed graph. this edge has a relationship type .

protégé’- is a free, open source ontology editor for editing rdf, owl and other resources on the semantic web.

rdf- the resource description framework  is a family of world wide web consortium  specifications originally designed as a metadata data model. it has come to be used as a general method for conceptual description or modelling of information that is implemented in web resources, using a variety of syntax formats.

rdfs – the resource description framework schema.

rdfizers – software that reads traditional data representations such as relational databases, text files, and data streams and converts the content into a multi-relational directed graph. rdfizers do not always perform rigorous analysis, so information can be lost in the process.

scenario- for biosites, a scenario is a structured description of a malicious action or series of actions that causes harm in health, economics, or quality of life.

sddm- scenario driven data modeling.

semantic web- is a "web of data" that enables machines to understand the semantics, or meaning, of information on the world wide web.

