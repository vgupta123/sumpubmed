BACKGROUND
using microarrays for hypothesis generation and validation has become routine in virtually every area of biomedical research. however, the majority of generated data is underutilized since publications reporting microarray results often focus on a small subset of the results they feel are most relevant to their research focus, even if other interesting observations may be present in the data. thanks to the minimum information about a microarray experiment   <cit>  to standardize descriptions and reproducibility, along with the requirement imposed by most journals to make microarray data publicly accessible, this wealth of data is accessible to the community. microarray data repositories, such as gene expression omnibus   <cit> , arrayexpress  <cit> , stanford microarray database  <cit>  contain growing amounts of gene expression data from multiple biological organisms and treatments. for example, besides just storing microarray data, geo also provides a simple web interface to analyze and compare individual gds  files. arrayexpress also provides a means to access its database via expressionprofiler software  <cit>  and with bioconductor r package for subsequent analysis. however, the ability to conduct a large-scale meta-analysis focused on a specific condition or disease, is not provided and neither is it trivial to compile a list of datasets associated with such conditions that are directly comparable. one of the chief concerns is the heterogeneity among probe design within microarray platforms, laboratory variations, and methods of data pre-processing.

whether the meta-analysis is focused on specific experimental types  <cit>  or is aimed at a global assessment of gene expression patterns across all experiments  <cit> , the major hurdle is that it heavily depends on the quality of the underlying data. for example, if a probe on one platform hybridizes to a much longer transcript than on another platform, the probe intensity will appear constitutively higher and direct comparisons will suggest differential expression. the accuracy and reproducibility of commonly used microarray platforms has been hotly debated with results ranging from relatively discouraging  <cit>  to promising  <cit> . a multi-center consortium, microarray quality control  performed independent assessment of gene expression data reproducibility and found results to be very optimistic  <cit> . furthermore, maqc ii analysis confirmed biological differences as the most readily detected value  <cit> . however, a human factor  is one of the most important pieces on the analysis  <cit> . as such, detection of true biological differences require a comprehensive method of diverse microarray data integration conducted with an understanding of the underlying technical and biological issues.

gene nomenclature poses another serious problem in comparison of different microarray platforms. gene identifiers, such as genbank, illumina ids, affymetrix ids have different underlying annotations and are not directly comparable. several attempts have been made to overcome those hurdles. two tools, list of lists-annotated   <cit>  and list to list   <cit>  were created to compare gene lists against microarray data from different platforms, different nomenclatures, or even different organisms. however, these tools rely on published data and need to be manually curated. a cancer-oriented database, oncomine  <cit> , was developed for queuing gene expression profiles in different tumor types and tissues. cellmontage allows users to correlate a custom pre-processed gene expression dataset with geo datasets grouped by platforms  <cit> . m2db is a curated microarray database which is designed for easy quality control and retrieval of raw and normalized microarray data  <cit> . a major drawback of these tools is that they require different input format and produce different results often biased towards particular tissue, platform  and/or disease .

several papers have been published that thoroughly outline the challenges, opportunities and recommendations for standardization of microarray meta-analysis, discussing the benefits and pitfalls, and comparing methods for data processing  <cit> . rather than develop a different method, the work reported here is based on a combination of the recommendations reported in these studies . in addition, we consider fundamental properties of microarray data distributions  <cit>  to standardize different experimental data for meta-analysis. we present a straightforward method of extracting gene expression data from publicly available datasets, performing quality control of the data, comprehensively mapping it to entrez gene ids and creating a gene expression matrix from multiple experiments. our method is based on the use of intrinsic properties of microarray data, adjusted by quantile normalization to unify distribution of gene expression across diverse experiments and to accurately determine a level of technical noise. we compile a gene expression matrix from  <dig>  human 1-color microarray experiments, establish noise level and make expression values comparable across datasets. we showed our pre-processing steps increase recall and precision for prediction analysis.

methods
obtaining one-color microarray data
geo datasets  files were downloaded from ftp://ftp.ncbi.nih.gov/pub/geo/data/soft/gds/ and uncompressed from .gz compression format. files were selected for processing if the following fields were dataset_sample_organism=“homo sapiens”, dataset_type=”nucleotide” or “gene expression”, dataset_channel_count=”single” or “1”, and dataset_value_type=”count”. this ensures only raw data from one-channel human microarray samples were processed.

probe mapping
probe mapping was done using a relational database, assigning unique entrez id identifiers based on gene names and accession numbers downloaded from ncbi . unmapped probes were stored in a file and examined if any of the platforms was unmapped due to absence of its mapping data in the database. each technological platform has unique set of probes targeting different gene regions. moreover, some probes recognize particular isoforms of the same genes, such as implemented in affymetrix and illumina platforms. affymetrix uses extensions to its unique ids, such as “_at” indicating a probe recognizing a unique gene isoform. “_s_at” extension indicates a probe can recognize multiple isoforms of the same gene. illumina flags its ids by “i”  and “a” . due to aforementioned problems of linking diverse manufacturer’s ids to unique gene identifiers it is logical to use probes recognizing all isoforms of a given gene. such probes should have maximum expression value relative to other probes that only recognize individual isoforms of the same gene. therefore, the maximum expression value was selected from multiple probes targeting the same gene.

data pre-processing
basic parameters were calculated for each dataset, namely, mean and median. datasets with mean or median equal to  <dig> were excluded, as well as the datasets with mean to median ratio equal to or less than  <dig> . data for each experiment were sorted and distribution of expression values around maximum was examined. due to technological imperfections some genes demonstrate extreme saturated measurements at the high end of expression spectrum. such expression values would negatively influence the following quantile normalization step by introducing artefacts, and should be adjusted. overall, no more than  <dig> % of maximum expression values showed extreme measurements. for each experiment,  <dig> % of genes with highest expression values were selected, and a minimum expression value  among them was identified. expression of these genes was replaced with this flooring value. each experimental dataset was then adjusted to fit within  <dig> -  <dig>  range.

low expression values in each microarray dataset constitute technical noise that can be approximated with normal distribution  <cit> . to make distributions of different datasets equal the data fit within 0- <dig>  range were quantile-normalized  <cit> . briefly, quantile normalization replaces distributions of each dataset with an average distribution, calculated from an average of sorted expression values across all datasets. this step does not alter the data structure  but makes it possible to define common noise threshold and directly compare expression level across the whole matrix.

validating data preprocessing steps
we explored the data structure in 1) the matrix containing raw expression values; 2) matrix containing scaled to 0- <dig>  range data; and 3) matrix containing scaled and quantile normalized data. we performed gene ontology prediction analysis  <cit>  and calculated precision/recall based on the number of correctly predicted gene ontologies . we show our results in a form of f-measure, a test for accuracy that considers both precision and recall .




RESULTS
a total of  <dig>  gds files were downloaded, out of which  <dig> contained raw gene expression data from human single-channel microarrays. probe identifiers were mapped to entrez ids, totaling  <dig>  genes.

quality control: mean/median ratio
single-channel microarray data follows well-defined distribution that can be approximated with a lognormal model  <cit> . genes expressed below noise level form a pronounced normal distribution at low expression levels while genes expressed above noise are spread across the whole spectrum of expression. these properties of microarray data distribution dictate that its mean and median parameters can’t be equal, whereby a median should be always smaller than a mean. thus, the mean/median ratio in a dataset should always be more than  <dig>  investigation of mean/median ratio  in all datasets proved this to be the case for majority of datasets. median value for mm ratio was  <dig>  . seven datasets  with mm ratio less than or equal to  <dig>  were removed as suspects for bad quality data. a total of  <dig> datasets with  <dig>  experiments were used for further analysis.

data re-scaling
while remaining datasets did not contain negative values, raw expression data distribution may show inconsistent intensities at very high expression level  <cit> . manual inspection of all datasets for possible outliers identified on average  <dig> %  of the top expression values showing extreme measurements. such extreme measurements would distort data rescaling in unpredictable ways. to minimize the impact of such outliers they were set to their minimum value . this step included selection of top  <dig> % of genes with highest expression level in each unprocessed experiment, identifying their minimum and setting them to that minimum. this change on  <dig> % expression values did not affect data distribution . the resulting matrix of  <dig>  experiments containing expression values for  <dig>  genes was scaled to 0- <dig>  range.

quantile normalization
gene expression matrix from  <dig>  experiments contained data from different experimental platforms. affymetrix platforms delivered highest number of datasets, with  <dig> datasets done on affymetrix human genome u133a array  followed by  <dig> datasets on affymetrix human genome u95a array  platform .

even though different platforms yield comparable experimental results  <cit> , the data from different platforms produced different distributions. that is, the parameters of the lognormal model that can be fitted to them  <cit>  differ . to make them comparable we applied quantile normalization  <cit> . quantile normalization only rescales data distributions to make them fit to an average distribution calculated from all datasets. an example of data before and after normalization is shown in figure  <dig> 

distribution after quantile normalization
data before and after quantile normalization correlated with each other , r2= <dig> . quantile-normalized data from all experiments has the same distribution  and preserves the rank-order of genes by expression level in each dataset. a normal distribution was fitted around the peak of low expressed genes and its parameters were determined. mean was determined to be  <dig> and standard deviation  was  <dig>  as such, the noise level threshold, commonly defined as  <dig> sd above mean was determined to be  <dig>  above which the level of gene expression can be detected with 99% accuracy. thus, genes with expression values > <dig> in the resulting matrix can be considered expressed with p <  <dig>  in any given experiment.

testing the effect of normalization on predicting gene function
it is hard to empirically demonstrate the superiority of one normalization approach over another. it is, however, important to know if your methods of normalization had a positive effect. recently, we described a means of using a global microarray meta-analysis of gene-gene co-expression patterns to predict gene function  <cit> . the basis for this is successful grouping of gene-gene co-expression patterns from heterogeneous datasets to identify gene pairs whose co-expression is strongly correlated regardless of the experimental condition. genes that have strong correlations across datasets tend to share similar biological functions. we found the top  <dig> co-expressed genes  tend to accurately predict the go categories of any given gene of interest  <cit> . as we report in a related study  <cit> , we find that the predictiveness of these genes rapidly declines the more that are taken for analysis, suggesting the best accuracy will result from effectively identifying and grouping the most correlated genes. technical noise, however, should affect the cohesiveness of pairings and, consequently, the precision by which we can predict biological function. we hypothesized that raw data should produce more erroneous predictions because it is noisier and contains distortions within the data distributions that should affect accurate measurement. thus, if the normalization scheme is effective and biologically relevant, it should increase the accuracy by which gene function can be predicted on the basis of global co-expression patterns in raw data.

to test this, we conducted a global meta-analysis to predict go categories.  <dig>  genes with known go categories were used for analysis. the most highly correlated genes in terms of their co-expression patterns across all  <dig>  microarrays were identified using metrics described in a related work  <cit> . for each gene, its top  <dig> genes were used to predict it’s go category. the results were compared against its annotated go category, looking for the fraction of predicted categories that were exact matches to the annotated categories. in this analysis we wanted to identify the effects on both precision  and recall . as such, we used f-measure /2)) to estimate the effect of pre-processing steps and normalization on predicting gene functions.

we compared predictions using the raw data, data that was scaled and outliers eliminated as described, and data fully normalized as described herein. we found using the raw data, the f-measure was  <dig> . scaling the data to a common 0- <dig>  range increased the f-measure to  <dig> , and using quantile normalized data allowed us to reach an f-measure of  <dig> . these results demonstrate that each pre-processing step increases the ability of the global meta-analysis to identify biologically relevant patterns of co-expression.

discussion
one enabling factor for high-throughput data standardization is that the input data files should have a well-defined structure for mapping data elements. specifically, within geo, the gds  files work best for that purpose, since they are reassembled by geo staff and stored in text files with information fields. gds files are standardized versions of the originally submitted gse  files. gse datasets lack a standardized structure and will require manual reassembly, which currently lags the production of such datasets and is a reason others have chosen to exclude them as well  <cit> ,  <cit> .

one of the primary challenges in standardizing datasets is choosing an appropriate identifier for each gene. fortunately, many tools exist to convert microarray ids between database probe names and among the more developed are david  <cit>  and resourcerer  <cit> . one dedicated effort, ailun, attempted to map all geo ids to a unique identifier  <cit> . our solution here was to try to map to a common, widely used identifier, the entrez gene ids  <cit>   as defined by ncbi. entrez’s “one gene – one id” concept suits well for the purpose of bringing expression values from multiple probes targeting one gene to a single placeholder, the only drawback being that some probes on lesser used platforms may not map to a gene id. the number of such experimental probes varies significantly from platform to platform. ultimately, next-generation sequencing will provide an even bigger challenge for meta-analysis, as many sequences identified will not have existing identifiers.

single-channel microarray data have been shown to exhibit a well-defined distribution of their values  <cit> . a frequency histogram of low-expressed values shows a peak that approximates a normal distribution, the parameters of which serve as a platform-defined  noise threshold. expressed probes are those on the rightmost tail of the distribution -the highly expressed values. the nature of these experimental data distributions dictates that the mean and median of the data can’t be equal. indeed, as shown on figure  <dig>  the mean/median ratio for the majority of the datasets differed from  <dig>  this parameter is a quick and simple estimate of microarray data quality, and should be included in data quality control.

data from different technological platforms have different levels of noise and probe intensity and direct comparisons could be misleading without first correcting for these issues. quantile normalization  <cit>  is a means of normalizing data distributions to an averaged distribution across all datasets. as expected , an average distribution of all datasets, showed pronounced peak of low-expressed genes, which can be fitted with normal distribution. although quantile normalization changed expression values slightly to make them fit to an average distribution, such changes are gradual. if a gene is co-expressed with another gene, a slight change in expression level even of both genes will not change the fact of their co-expression. quantile normalization may slightly influence calculation of pearson product-moment correlation coefficient, a metric for estimating correlation between two vectors, although we do not anticipate it to be a major problem.

each experimental platform may suffer from erroneous or artefactual expression measurements, such as negative values, or extremely high expression. since gene expression can’t be negative, such values, if present, should be flagged. in the present study datasets that passed quality control did not have negative values, which does not guarantee their absence in other datasets that could be added to the total pool in the future. we observed extremely high expression measurements across several datasets . such outliers may negatively affect performance of quantile normalization by skewing the average distribution in an unpredictable manner. therefore, we “floored” top  <dig> % of the data to decrease outliers’ effect. although this “flooring” did not noticeably affect co-expression analysis  we feel this negligible data treatment is an important precaution against possible technology-related errors.

to our surprise, even raw data can be used for predicting gene ontology categories using a global meta-analysis of all available data  <cit> , although precision and recall are slightly less than processed data. closer examination of the data revealed that even in raw data, gene-gene co-expression patterns are nonetheless discernable. this is similar to considering gene ranks  <cit>  which are also retained in the raw data. re-scaling the data, however, allowed more precise predictions to be made, since common thresholds could be applied. quantile normalization further increased recall/precision of the predictions, because more exact thresholds can be defined. this step is imperative for comparative meta-analysis of data subsets, where absolute level of expression among different conditions and platforms should be compared.

CONCLUSIONS
in this work we addressed potential pitfalls and problems associated with microarray meta-analysis of large number of disparate microarray experiments  <cit>  and present specific steps and precautions. as such, the current paper aims at providing a means for creating a global, unified and directly comparable matrix of expression values associated with individual microarray experiments. we provide a framework for researchers to use pre-processed datasets of interest for their own research.

competing interests
the authors declare that they have no competing interests.

authors' contributions
jdw conceived of the project. mgd designed, implemented and tested the approach to normalization. both authors wrote the manuscript. all authors read and approved the final manuscript.

