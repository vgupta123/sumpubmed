BACKGROUND
the sequencing of complete genomes has accelerated biomedical research by promising a greater degree of understanding of the biological processes encoded. the role of computational biology has become increasingly important because there is a significant need for progressing beyond statistics and data clustering. new analysis methods are needed that afford the ability to use more of the generated data, thus revealing more insights into biological processes. mathematical optimization algorithms, and related machine learning techniques arising in computer science, have emerged as having significant potential for knowledge discovery in biological data sets arising in genomics, metabolomics and proteomics investigations. these methods are attractive given they are capable of identifying multivariate interactions associated with biological complexity, e.g., co-expressing genes in single or related pathways. further, the gene selection procedure is data driven, thus providing an unbiased approach to knowledge discovery.

the emphasis on the search for parsimonious models – that is, models which attain high accuracy while minimizing the number of features – is now standard operating procedure in machine learning. currently, many machine learning feature selection methods applied to microarray data explicitly eliminate genes which are redundant, in terms of discriminative or predictive value, with an existing set
 <cit> . a recurring theme in this body of literature is to select a small set of “top ranked” genes. parsimony leads to computational advantages when analyzing large-scale data and can improve the signal-to-noise ratio. however, when applying machine learning methods to biological data, the advantages of parsimony are less clear.

microarray data typically includes highly correlated gene expression levels associated with a  biological process. yet feature selection methods tend to eliminate genes which fail to provide additional discriminatory power to the model. to the machine learning expert, this yields the desired parsimonious model. to the biologist, the suppression of potentially biologically-relevant information can impede process discovery.

we hypothesize that genes selected as most important to a parsimonious model are in fact only a small subset of a large set of highly discriminative, and thus informative, genes. to test this hypothesis, we employ a sparse support vector machine to build an accurate model using only a small fraction of the features in the data. we then repeat this process, but at each iteration we remove all the genes selected by previous iterations from the data. at each iteration, the model must be built without the “most important” genes identified previously. we call this process iterative feature removal .

if our hypothesis holds, there should be several iterations where the accuracy remains high, even as the top genes are iteratively removed from consideration. eventually, the process will have removed all the relevant genes, and the accuracy should fall to random performance. our results support this hypothesis. on the evaluated data sets, ifr demonstrates that at least  <dig> iterations are required before we see a significant loss in discriminatory power, providing hundreds of genes for further analysis.

RESULTS
data sets
for this study, we examine four microarray data sets from the literature, as summarized in table
 <dig>  the influenza data is from the duke pan-viral challenge
 <cit> . this data has gene expression levels taken from peripheral blood samples of human subjects exposed to h3n <dig> and h1n <dig> influenza. the h3n <dig> data consists of the gene expression levels of  <dig>  genes collected from  <dig> human subjects in approximately 8-hour intervals. the h1n <dig> data measures the same set of genes, but on a separate set of  <dig> subjects, using the same collection methodology. in both h3n <dig> and h1n <dig>  approximately half of the subjects became symptomatic, the others remained healthy.

influenza 14-16
influenza 11-14
lung cancer
prostate cancer
bcell lymphoma
influenza 14- <dig> and 11- <dig> represent different temporal intervals of the data from
 <cit> . the former is used to compare with other published results, and the latter is used because it provides more training samples for the automated analysis. all data sets except the bcell lymphoma have defined training and test partitions.

we use two subsets of the influenza data, labeled as “influenza 14-16” and “influenza 11-14” in the table. the first uses only those samples from time intervals  <dig> through  <dig>  the second from intervals  <dig> through  <dig>  the 14- <dig> data are used in
 <cit> , and are claimed to reflect peak symptom expression in the subjects. we employ this partition for the purposes of comparison with other published results. however, our analysis leads us to believe that intervals 11- <dig> represent the true period of peak symptom expression with the additional benefit of providing more samples for training and testing. as done in
 <cit> , we use the h3n <dig> data for training and h1n <dig> is withheld for testing. data was downloaded from the duke websitea.

the other three data sets, lung cancer, prostate cancer, and bcell lymphoma, were downloaded from the kent ridge biomedical data set repositoryb. the bcell lymphoma data differs from the others in the following ways. first, it is smaller, both in terms of the number of samples and the number of features. secondly, there are missing values in the data. we replace missing values with the average expressions for the same gene over all samples. thirdly, there is no defined distinction between training and testing samples. we randomly withheld 25% of the samples for testing .

we refer the reader to the original sources, cited in table
 <dig>  for additional details about the data.

iterative feature removal
our iterative feature removal  process works by repeatedly building a predictive model on training data using a classifier that assigns non-zero weights to only a minimal subset of non-redundant features. in other words, if a gene does not improve the classification accuracy, it is not included in this minimal set. at each subsequent iteration, we remove all features that were selected in previous iterations, limiting the model construction to the remaining features. thus, we are effectively removing highly discriminatory genes from the data, and forming a new model without these genes. we observe that for a surprisingly large number of iterations, the resulting model is essentially as highly discriminatory. for example, we can remove the best  <dig> genes whose subsets had accuracies of over 90% on the influenza testing data, and still discover new genes that can classify with essentially the same accuracy.

the genes in each set are selected based on their predictive power as a group and we observe that they do not necessarily classify well individually, i.e., they may not provide univariate separation of the data. in contrast, they provide a multivariate model for classifying the data that captures potentially complex biological relationships amongst the variables.

given the small number of samples in a high-dimensional space, model over-fitting is a concern and must be carefully monitored. at each iteration, we gauge the quality of the model by testing its accuracy on data that was not included in the training. a description of the sparse svm used for ifr and other methodological details are provided in the methods section. results in additional file
 <dig> illustrate that the ifr procedure can be effective even when using other classification engines.

figure
 <dig> shows the results of applying ifr to the influenza and lung cancer data sets. additional file
 <dig> contains ifr plots for the prostate cancer and bcell lymphoma data, with the same qualitative characteristics. additional files
2-
 <dig> provide further details of the results shown in figure
 <dig>  additional files
 <dig> and
 <dig> provide the genes selected at each iteration when sparse logistic regression is used instead of ssvm in the ifr process. the results support our hypothesis that there are many sets of highly predictive features that can be identified in the data beyond the first optimal set identified by a parsimonious model.

due to the small number of samples, test accuracy from one iteration to the next can vary substantially. this is due, in part, to the fact that the genes are being selected by an optimization criterion on the training data and, as such, this selection is ignorant of the test data. it also suggests that, in general, one should avoid using a hard threshold for stopping the iterative removal process.

the interpretation above is complicated by the fact that when fitting a separating hyperplane to a small number of points in a high dimensional space, there can be many equivalently-optimal partitions. there is thus some element of luck in model selection when validated against withheld datac. this effect is less pronounced in the lung cancer data where there are more test samples. when the test accuracy is viewed using a rolling average , the trend becomes clearer.

the training fit  is perfect through the first  <dig> or so iterations, even as test accuracy varies from trial-to-trial. when the training accuracy is 100% and the test accuracy is low, we infer that the model is over-fit, meaning that the model does not generalize beyond the training data. genes selected by those iterations showing model over-fitting are not considered strongly discriminative in general, and they are considered to be unrelated to the biology that distinguishes, e.g., symptomatic and asymptomatic subjects in the influenza data.

in the lung cancer data, there is a sharp increase in test accuracy near the end of the iterations. this behavior is explained by the uneven distribution of test samples. approximately 89% of the test data are from adenocarcinoma  tissue samples. after all informative genes have been removed using the ifr process, the training samples can not be accurately fit to the data , even as the test accuracy spikes. in this case, a hyperplane has been constructed such that essentially all points are being classified as adca, which results in an artificially high test accuracy.

more iterations uncover more biology
we have demonstrated that a significant number of feature subsets selected via ifr have essentially equivalent predictive power and hence relate to potentially discriminative biological processes. our hypothesis suggests that, in many microarray data sets, there is substantial additional biological information contained in the features that remain after the first optimal set is removed. given that genes express in pathways, and can be highly correlated, it is perhaps not surprising that even 20- <dig> sets can yield highly accurate predictions. while these genes are redundant from the perspective of machine learning, they are essential to the process of biological discovery. since all the retained sets are predictive, we assert that they play a role in characterizing the differences between the classes in the data.

to evaluate how the resulting larger set of informative genes relates to discriminatory biological pathways, we follow a two-pronged approach, one relying on manual analysis, the other automated. the first approach uses manual expert analysis of the features selected using ifr. the disadvantage of this approach is that it is labor-intensive and thus difficult to apply to cross-validate results. the automated approach relies on employing the gene ontology , queried via the gather system
 <cit> , to identify pathways associated with sets of genes. this approach allows us to design a fully-automated system and to perform repeated trials with cross-validated results. the manual approach takes longer and requires domain expertise, but can yield a more in-depth understanding of the biological significance of the gene subsets, and avoids reporting irrelevant or misleading annotations that the automated system sometimes provides.

there is some similarity between this knowledge-driven analysis and feature enrichment strategies, such as gene set enrichment analysis 
 <cit> . gsea uses pathway memberships of selected genes to expand the feature set to include additional related genes that were not part of the original selection. in contrast, here we use pathway membership knowledge to group only genes that were selected via ifr, and thus are known to have discriminative power. in the following, we show that grouping genes according to pathways yields powerful classifiers, but these classifiers are not the result of enriching the feature selection.

manual analysis of genes selected via ifr
we used the influenza 14- <dig> data for our manual analysis because this allows for direct comparison to other published results.

we examined the  <dig> most predictive iterations from our ifr process for association with known pathways activated by influenza infection. this number of iterations should not be viewed as a hard cutoff and should be driven to some extent at least by the additional biological information being gained. we made our selection by stopping at the iteration before the consistent decline in test accuracy begins, and where the variance in test results remains relatively low.

as described below, we accumulated related genes across these predictive sets and organized them by pathway. as long as the predictive accuracy of a set is high, the actual iteration that the gene was selected should not be viewed as a strong indicator of its biological importance.

table
 <dig> lists selected biological pathways we found represented by the set of genes found within the first  <dig> iterations of ifr. also shown in the table is the classification accuracy of a model built using only the specified set of genes for each pathway. we find that all pathways yield quality classification results, and several are highly accurate, with accuracies on the h1n <dig> test data over 90%. other pathways not listed here may also be present, but an exhaustive identification is not necessary to support our hypothesis.

selected pathways relating to genes taken from the first  <dig> iterations of ifr on the influenza data. the iteration that the genes were discovered is in parentheses. acc indicates the classification accuracy  using only the genes in each list.

figure
 <dig> shows which pathways were represented by the genes selected at each iteration. no single pathway was represented in all iterations, and no single iteration had representatives from all pathways.

discussion of selected pathways in influenza data
several markers of the interferon pathway were found in our analysis  and the analysis of chen et al.
 <cit> . the interferon cytokines  themselves are not in this subset, nor did we find them to be in our most predictive iterations. that may be in part due to the paracrine, rather than endocrine, nature of these proteins, and that the samples were blood samples, rather than samples directly from the site of infection and viral replication such as the respiratory epithelium. we found many interferon pathway members oas1- <dig>  oas-l, psme1/ <dig>  isg <dig>  ifi <dig>  ifi <dig>  ifi <dig>  ifi <dig>  ifit <dig> and ifitm <dig>  for example, all increased in active infection. interleukin  <dig>  and the il- <dig> receptor α chain, which are induced by the interferons
 <cit> , were also found to increase in symptomatic subjects in our analysis.

using the  <dig> most predictive iterations from ifr to look for more biologically relevant pathways activated by infection, we noted two other major cytokine pathways, in particular tnf α and il- <dig> β also played a role. these two proinflammatory cytokines are, like the interferons, well known for their roles in antiviral responses and, in contrast to the interferons, have endocrine effects, notably causing fever
 <cit> . in addition to exploring these pathways, we also noted whether these genes were expressed at higher or lower levels in ill subjects compared to asymptomatic subjects and looked for gene profiles with large differences in expression levels as these would be both useful diagnostically as well as give clear indications of the active pathways. tnf α induced proteins tnfaip <dig>  tnfaip <dig>  and tnfaip <dig> as well as tnf super family  <dig>  all increased. additionally, tnf receptor super family 10b  is the receptor for trail/tnfsf <dig> but was decreased in symptomatic subjects. three other tnfrsf members were found to be highly predictive in this analysis, tnfrsf <dig> , tnfrsf <dig> , and tnfrsf <dig> , although the expression levels between symptomatic and asymptomatic subjects were only slightly different. all four tnfrsf members are critical for t cell activation and maturation
 <cit> .

il- <dig> β, il- <dig> receptor type i, il- <dig> receptor accessory protein, and il-1rl <dig> were also found to be modulated by infection. il- <dig> receptor type i and il- <dig> receptor accessory protein decreased in symptomatic subjects, which is somewhat paradoxical as these form the receptor complex for il- <dig> β. many b cell genes all decreased upon active infection including igm heavy chain, igd heavy chain, cd <dig>  cd <dig>  cd <dig>  cd <dig>  cd <dig>  cd79a, and cd <dig>  this may be due to activation and sequestration of b cells in lymphoid tissues in response to the infection. similarly, cd8a  and ly <dig> and kir2dl <dig>  also decreased, suggesting that these cells were also being activated and sequestered in tissues at the site of infection.

chemokines and their receptors are also important small signaling molecules. cxcl <dig>  which activates chemotaxis in effector t and b increased after infection, but the neutrophil-specific chemokine cxcl <dig> decreased. ccr <dig> increased with illness, and is a receptor for many of the ccl chemokines. ccr <dig>  the receptor for ccl <dig>  decreased upon illness. ccrl <dig>  also increased
 <cit> . the fractalkine receptor cx3cr <dig> increased probably due to trafficking of monocytes
 <cit> .

caspases are involved in apoptotic death of cells. caspases casp <dig>   <dig>  and  <dig> all increased. several programmed cell death genes, pdcd1lg <dig>  pcdhga <dig>  and pcdha <dig>  also increased in symptomatic subjects. tlr <dig> increased in symptomatic subjects, unexpected as its most well-known ligand is flagellin. complement component c <dig> decreased, but the complement receptors cr <dig>  c3ar <dig> both increased. chen et al.
 <cit>  had also found c1qa and c1qb as part of their top  <dig> genes.

several genes involved in antigen presentation to t lymphocytes were also found to be highly predictive. chen et al., found tap <dig> upregulated in symptomatic subjects, and our analysis also found tap <dig>  tap1/ <dig> form a transporter complex to carry peptides into the endoplasmic reticulum for loading into class i major histocompatibility complex  proteins. mhc class i protein/human leukocyte antigen -b was also upregulated in symptomatic subjects, along with the related protein hla-e, which is an activator of nk cells. in contrast, in the class ii chains hla-dqa <dig>  hla-dqb <dig>  hla-dpa <dig> and hla-dob <dig> were higher in asymptomatic subjects.

combining discriminative pathways
in addition to what has already been described, our hypothesis further suggests that better diagnostic tools can potentially be designed by performing a deeper analysis of the discriminative features selected via ifr. if true, then certain combinations of features from the discriminative pathways should be able to yield classification accuracies that exceed any single iteration because we can improve the signal-to-noise ratio by first selecting discriminative genes and then combining the information from complementary pathways.

as stated in
 <cit> , a necessary and sufficient condition for an ensemble to be more accurate than any of its constituent classifiers is when the individual classifiers have predictive power , and are diverse . combining the classifiers derived from multiple pathways can, in principle, yield a stronger discriminative model when the pathway-specific classifiers have uncorrelated errors.

to test whether this is true, we combine pairs of pathways and measure the classification accuracy using the influenza data. we generated a support vector machine classifier from each pair of pathways listed in table
 <dig>  for each pair, we combined the features, trained the classifier using the h3n <dig> data, and measured the classification accuracy using the h1n <dig> data. with  <dig> pathways, there are  <dig> unique pairings. the results of the top  <dig> are shown in table
 <dig>  the scores for the remaining  <dig> are available in additional file
 <dig>  by comparison, table
 <dig> shows previously published classification results on the same data using the same protocol. several of our single pathway classifiers are competitive with published results, and all of our top- <dig> pathway pair classifiers are as good or better, providing strong evidence for the benefit of ifr for feature selection.

classification accuracies can improve when combining pathways. we computed the classification accuracies when using all pairs of the pathway gene lists presented in table
 <dig>  top sixteen results are shown.

in this table, the accuracy of our best single-pathway and best pathway-pair classifiers are compared to the best classifiers reported by chen et al.
 <cit> . the same protocol is followed: classifiers are trained using h3n <dig> data from time intervals 14- <dig> and tested on h1n <dig> from the same time period.

in searching for discriminative pathways using ifr, we found that we could construct a perfect classifier using the genes identified with b cell maturation and activation  combined with those identified as relating to cell adhesion molecules . the classifiers trained from these two pathways make different errors. the bcell classifier misses samples: , while the cam classifier misses samples: . the only overlap between the two sets of errors is sample  <dig>  but when the two pathways are combined to train a single classifier, no errors are made.

we computed the t-test scores for all genes in the influenza data set. ranked according to t-test, the  <dig> genes comprising the combination of the bcell and cam classifiers are shown in table
 <dig>  only one is within the top  <dig> t-test scores, and only five are within the top  <dig>  in fact, all but a few of the pathway and pathway pair classifiers with high discrimination consist primarily of genes with relatively low-ranking t-test scores. . this highlights the disadvantage of feature selection methods that start by throwing away “low ranking” genes. doing so prevents the discovery of interaction effects among genes which are not, by themselves, discriminative.

the bcell+cam classifier is 100% accurate on the test data, while consisting of genes that are relatively low ranking in terms of univariate separability. the table shows gene names and the t-test rankings within the total of  <dig>  genes . a blank cell separates the  <dig> bcell genes from the  <dig> cam genes.

figure
 <dig> compares the genes from the bcell+cam classifier to an equal number of the top univariate ranked genes. the accuracy of a classifier constructed from the top  <dig> genes according to t-test ranking is  <dig> %, which is lower than many of the top individual pathway classifiers and lower than all of the top  <dig> pathway pair classifiers. the t-test statistic score for each gene is shown at the top of the figure in the respective columns. it is interesting to note that the top univariate genes all have higher expression levels for symptomatic patients, while the bcell+cam genes are mixed.

the weakness in the manual analysis presented herein, including the results of the pathway and pathway pair classifiers, is a lack of cross-validation due to the limits posed by human annotation. we address this limitation by automating our pathway analysis using queries to the well-known gene ontology database. these results are presented next.

automated analysis of genes selected via ifr
as we did in the previous section, we first present the pathways associated with each iteration of ifr, but this time the pathway annotations are derived from go using the gather web query service. we present the biological significance of features selected beyond the first optimal subset in the lung cancer data. as a reminder, this data consists of two classes of tumors, and so the selected features are those that can be used to differentiate between the two, not to discern whether a tissue sample is cancerous. following that, we present automated pathway and pathway pair classification using four microarray data sets.

gene ontology annotations
the go annotations associated with the features selected at each of the first thirty iterations are shown in figure
 <dig>  to enhance readability, those labels that are represented by only a few genes across all iterations are not shown. complete lists of all genes removed at each iteration are available in additional file
 <dig>  while some of the annotations provided via automated queries may be irrelevant, this figure illustrates that different iterations are associated with different subsets of biological factors. there are factors that are common to many of the subsets, yet no single label is present in all iterations.

the annotations in figure
 <dig> are those from depth  <dig> in the gene ontology structure. the ontology follows a tree-based organization where lower depths are associated with more specific labels. not all genes are annotated to all depths in the ontology, and thus it is possible to have no annotations at a given depth for a selected set of genes, as observed in the figure for iteration  <dig>  we selected depth  <dig> for our analysis because it has a balance between label completeness  and specificity . the figure suggests that genes associated with phosphorus metabolism can discriminate between the two types of lung cancer tumors, as this annotation is represented in  <dig> of the  <dig> iterations. at the same time, none of the genes known to be associated with phosphorus metabolism were selected within the first six iterations, which covers approximately the first  <dig> selected genes. unlike ifr, methods focusing on learning a minimal set of 30- <dig> discriminative genes  would be unlikely to designate this pathway as being discriminative.

by considering a larger pool of correlated genes, a biologist might discover important information that otherwise could be masked by feature selection methods optimized for removing predictive redundancy.

discriminative pathways
as we did with the manual analysis, we can use our automated approach to discover discriminative pathways and pathway pairs. our protocol was to use 50-trial random subset cross-validation on the training partition to identify features associated with the top models. after selecting the top-performing models from the validation stage, we then tested on the withheld data. more details on this procedure can be found in the methods section.

we also performed the same cross-validation without using multiple ifr iterations, instead using the sparse svm  to select the optimal set of features to classify the data. this is equivalent to using only the first iteration of ifr. the three methods  differ in how genes are selected, but a common classifier engine is used to compare performance. once the features are identified, validation and test accuracies are computed using a standard  svm built from the training data using the selected features, and evaluated on the validation or test data. thus, performance differs due to the features, not the classifier.

finally, since there can be several models from the validation trials with near equivalent performance, we also report on the average test accuracy of the top five models selected from the validation results.

table
 <dig> shows that the pathway and pathway pair classifiers constructed from the genes selected via ifr have discriminative power comparable to the optimal set of genes selected by ssvm, over four data sets. figure
 <dig> shows that the best pathway and pathway pair classifiers are constructed using genes with generally much lower univariate power  than those selected by ssvm.

cross-validated classification results compared using features selected from discriminative pathways and pathway-pairs, and the “optimal” set of features selected by sparse svm . the latter is equivalent to the first iteration of ifr. the mean and standard deviation of  <dig> cross-validated trials are shown, followed by the accuracy of the best model from the validation trials applied to withheld test data. also shown is the average accuracy of the top  <dig> models on the withheld test data.

in summary, the pathway, pathway-pair, and ssvm feature selections were found to perform comparably on the withheld test data for the lung cancer, prostate cancer, and bcell lymphoma data sets. there was, however, a statistically significant improvement using the pathway-pair features in the influenza data. significance was measured using welch’s two-sample t-test between methods, evaluated on the test accuracies returned by the top five models from each. p-values for all comparisons can be found in additional file
 <dig>  with notable results discussed below.

the benefit of using ifr is that several predictive pathways can be identified with performance similar to that of the optimal model, but with the advantage of providing more insight into the biology of the discriminating factors.

in the lung cancer results, there is some evidence that pathway-pair feature selection performs better than ssvm , but the p-value is too high to be conclusive. since all methods perform well, ceiling effects may inhibit the ability to differentiate amongst them.

in the influenza 11- <dig> results, there was strong evidence that the pathway-pair features yield better classifiers than those selected via ssvm . additionally, there was some evidence that even individual pathway features perform better than ssvm , and that genes selected via pathway-pair analysis outperform those of individual pathways . the results of the automated analysis and the manual analysis when applied to the influenza data appear to be consistent. one aspect of the influenza data that is different from the other three is that the training and validation data is of a slightly different pathology  than the test data .

the results from the prostate data show no statistically-significant differences among the three methods, even though the validation distribution for ssvm has a higher mean accuracy and narrower variance. the features selected by the top performing pathway, pathway-pair, and ssvm model yielded classifiers exhibiting the same accuracy when tested against the withheld test data, correctly classifying  <dig> of  <dig> test samples . the average test accuracies from the top five models of each method show no statistically-significant difference. with ssvm, the accuracy of the cross-validation result appears to overestimate the performance of the classifiers when applied to withheld data. this phenomenon often indicates that there is a substantial difference in the test data from the training, and that the model is over-fit to the training data. from the description of the data as provided by the kent ridge bio-medical data set repository, we note that the test data was compiled from an independent experiment from a different lab with different collection methods. in this case, the pathway-oriented analysis was found to provide more consistent performance  than the parsimonious features selected via ssvm. a similar trend was noted for the influenza data.

there is no statistically-significant performance differences amongst the methods applied to the bcell lymphoma data. the top model from each method, when applied to the withheld test data, correctly classified  <dig> of  <dig> samples . the bcell lymphoma data only has  <dig> testing samples, so we observe much higher variance in classification accuracy, where a single misclassification drops the score by over eight percentage points. we note that for all three methods, the top test performance was within one standard deviation from the cross-validated mean. the bcell lymphoma data differs from the rest in that it is smaller, both in terms of samples and features, and it has missing values, which were imputed using averages.

CONCLUSIONS
existing feature selection methods
saeys et al. provides a review of feature selection techniques common in bioinformatics
 <cit> , categorizing methods as being filters, wrappers, or embedded. filters are feature selection techniques that select features without reference to a classifier by employing various statistical analysis techniques. wrappers are methods that select potential feature sets and then use a classifier to determine the quality of the selected features. embedded feature selection techniques are those in which the feature selection is intrinsic to the classifier, such as employing l <dig> regularization on a linear classifier, resulting in a model built using only a small number of features .

a support vector machine  is a popular machine learning method that maximizes the width of the margin surrounding a hyperplane that separates classes of data
 <cit> . svms have shown promise in classifying samples based on genomic data
 <cit> . svms have also been employed for feature selection using wrapper and embedded methodologies. svm-rfe is an svm-based feature selection approach that recursively eliminates genes from the data set that are considered lacking in discriminative power
 <cit> . similar to our method, svm-rfe also iteratively removes genes, but it removes the least predictive genes until an optimally parsimonious model is obtained, thereby also eliminating redundant genes. in contrast, we iteratively remove the best features, i.e., most highly discriminatory, in order to demonstrate that there exist many subsets of highly discriminative genes. we assemble all sets of discriminatory genes produced by the iterative removal to form one large set of biologically important genes. our perspective is that while genes may be redundant in terms of a model’s optimization criteria, they are not necessarily biologically redundant, and may be key to developing a full understanding of the interacting biological processes.

redundancy has previously been explored in the context of gene expression data. yu et al. provide formal definitions of “strongly relevant,” “weakly relevant,” and “irrelevant” features
 <cit> . in the context of their discussion, weakly relevant features are redundant with one or more other features. the approach that we advocate in this paper suggests that even weakly relevant features may have key biological implications. xing et al. discusses markov blanket filtering to iteratively remove redundant features
 <cit> . yeung et al. points out that there may be equally discriminative gene subsets, and proposes a method to select the top genes by applying bayesian model averaging
 <cit> . however, rather than exploring the totality of these subsets, they employ bayesian model averaging to further condense the set of features to the smallest possible number. as described, their method is limited to finding the  <dig> best genes. the authors assert the advantage of their method is having high prediction accuracy while using smaller numbers of genes than competing methods on several microarray data sets.

selecting genes by ranking according to a univariate measure is a common practice among biologists. significance analysis of microarrays 
 <cit>  is a scoring system based on a set of t-test statistics. rankgene
 <cit>  computes gene rankings using any of several possible univariate measures, including t-test, information gain, and sum of variances. however, as demonstrated by our results and additional file
 <dig>  genes with limited univariate class separation can have substantial predictive power in a multivariate setting. it is possible that features relating to complex interaction effects can be missed with univariate analysis.

broader impact
what is the goal of feature selection on data sets consisting of a small number of samples of microarray data? one goal might be the development of a diagnostic tool. another goal may be to use the feature selection as an aid in further discovery and learning about a particular biological process.

regarding the first goal, developing a classifier using a data set with a small number of samples can lead to overly optimistic results. while the classifier may be able to yield high accuracy on the test data, little can be said of how well it could perform as a diagnostic tool in the general population. imagine developing a decision tree for classifying a notional influenza data set. with only one question: “does the subject have a fever?”, the influenza data could be perfectly partitioned into symptomatic and asymptomatic patients. some small set of genes could be identified that essentially answer this question. while the selected set of genes may be a perfect classifier on the data set, it would be useless as a diagnostic tool in the clinic, lacking any specificity as to the cause of the fever . to restate: identifying a set of features that perfectly separates test data is not the same as identifying a set of features that will diagnose a disease in general. the former only approximates the latter when enough variety is present in the training data.

lacking a large sample size, using feature selection methods to reduce the decision to a small set of features exacerbates this issue. training a classifier on a larger set of genes is more likely to lead to the development of a model with greater specificity than training a classifier using a minimum set of genes found to be discriminative in the training data. consider again the example above, where the classifier learns to partition the data based on the presence of a fever. if instead the classifier were asked to fit a much larger set of genes, then more factors would be involved in the prediction. instead of generating a false positive for every patient exhibiting a fever but not having the flu, the more complex model would generate false positives only for patients having a larger number of factors similar to the symptomatic training set.

regarding the second goal, predictive redundancy is not the same as biological redundancy. for example, when considering the host response to a pathogen, there may be several sets of genes which are equally predictive on the test data. one set might relate to the innate immune response , and another set might relate to an acquired immune response . either set may work for partitioning the test data into symptomatic/asymptomatic subjects. a feature selection technique that suppresses redundant features might eliminate all genes related to process b without loss of predictive accuracy. yet when the goal is to select features to generate insight into the biology, the elimination of genes associated with process b is a problem, not a benefit.

the broader impact of the work presented in this paper is to challenge the paradigm that seeks to identify a small number of discriminative features for microarray data, or other high-dimensional biological data with relatively few samples. classifiers built in this way are unlikely to be useful as diagnostic tools unless they are trained using a very large number of samples that better approximates the target population for the tool. feature selection to aid in understanding a biological process based on gene expression data is most useful when a maximally-sized subset of informative genes can be presented to the researcher.

summary of significant findings
iterative feature removal provides a process for the unbiased selection of discriminative sets of features from a high-dimensional data set. the procedure results in a deeper mining of relevant information by iteratively suppressing top weighted features in order to unmask the less-obvious features that are nearly equally discriminative.

we recognize that others have previously observed the phenomenon where top genes can be removed from a microarray data set and a subsequent classifier built to similar accuracy using the remaining set . however we are the first to present a method for feature selection that leverages this phenomenon as a core mechanism.

when applied to microarray data, ifr identified sets of genes that were highly predictive even when the sets were composed of genes that, taken individually, appear non-discriminatory. through automated and manual expert analysis of the selected genes, we demonstrated that the selected features exposed biologically-relevant pathways that other approaches would have missed.

the pathway analysis provides an alternative way of grouping the genes, as opposed to by ranking, and also potentially provides more insight to the biologist. by showing that strong classifiers can be built without consideration to the iteration number or any other ranking , we prove that strong discriminative power can reside in lower ranking genes.

