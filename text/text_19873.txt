BACKGROUND
retrieving information from the biomedical literature involves the identification and analysis of documents from millions indexed in public databases such as pubmed  <cit> . the size of this widely used database has a negative impact on the relevance of users’ query results; simple free-text queries would return many false positives. additionally, when reading a document of interest, users can query for related documents. query expansion or reformulation is used to improve retrieval of documents relevant to a free-text query or related to a document of interest.

various query expansion or reformulation strategies have been proposed in the biomedical or genomics field  <cit> . a user’s free-text query defining the need for some information can be enriched with common synonyms or morphological variants from existing or automatically generated thesauruses, terms can be weighted, and can also be corrected for spelling errors. by default in pubmed, free-text queries are reformulated with medical subject headings  terms. the mesh thesaurus is a biomedical controlled vocabulary used for manual indexing and searching pubmed. relevance feedback methods involve the user in selecting relevant documents from results of an initial query in order to reformulate it, and pseudo relevance feedback  methods consider the top documents returned by the initial query as relevant in order to reformulate the query, avoiding additional user interaction  <cit> .

alternatively, content similarity algorithms are used to compare biomedical documents. when applied on freely available abstracts in pubmed, such algorithms use words, as well as other features available in indexed abstracts  or features processed by specific algorithms   <cit> . however, when a single document is used as input  algorithm used to display a list of related documents in pubmed  <cit> ), its abstract might not have enough content to allow proper retrieval. using the full text offers one possibility for expanding the information related to one document, and is increasingly used as more full text manuscripts become available from large resources such as the pubmed central  database and its open access subset   <cit> . another possibility is given by the references associated to the article by citation: either cited documents or documents citing it. for a given scientific document, finding the cited references is straightforward since they are usually listed in a dedicated section. in contrast, finding its referring citations requires mining all existing scientific documents, which might be impractical.

using related references by citation has been already used for classification of documents. for example, it was shown that algorithms based on shared references or citations can outperform text-based algorithms in a digital library of computer science papers  <cit> . papers were compared using three bibliometric similarity measures: co-citation   <cit> , bibliographic coupling   <cit> , or both  <cit> . similarly, it was shown that citation-based algorithms performed better than non-citation-based algorithms such as pmra in a small dataset of surgical oncology articles  <cit> . ranking algorithms were based on impact factors, citation counts and google™’s pagerank  <cit> . however, the opposite conclusion was drawn in another document clustering task  <cit> , i.e. citation-based algorithms performed worse than text-based algorithms. authors used a graph-based clustering technique that groups documents with respect to their connections to other documents in the citation graph. sentence level co-citations were also shown to be relevant for finding related articles  <cit> . articles were related to each other by graph random walks in a co-citation graph. also, the citation context  provides different information than the cited abstract  <cit>  and was used for classification  <cit> .

references in scientific documents may contain relevant and related information but their usefulness in retrieving related documents  from large sets of biomedical documents, starting from a query formed by one single manuscript, still has to be demonstrated.

in this article, we have studied articles in pmc-oa and the impact of using the text from their referenced documents by a query expansion method. we tested different subsets of references and observed that cited references indeed improve the task of retrieving documents related to a single document.

methods
pubmed abstracts
pubmed citations were downloaded in xml format and data was extracted only from citations with an abstract in english. the extracted data relevant to the present study was composed by the pubmed identifier , the title, the abstract, and the mesh annotations. the latter were extracted from xml tag descriptorname having option majortopicyn value equal indifferently to ‘y’ or ‘n’. a list of nouns from both the title and the abstract was generated by the treetagger part-of-speech processor   <cit> . a stop word list was used to filter out common and irrelevant terms. these lists of nouns were used as classification features by the medlineranker algorithm .

pubmed central open access subset  full-text documents
information related to references  in a document was extracted from the open access subset of pubmed central   <cit> , a biomedical literature database of full-text documents. they were downloaded in xml format  and parsed to extract the following data stored in a local mysql  database: title, pmid, authors, date, document section, and type of document. after removing overlapping and not well formatted xml documents where standard tags cannot be identified,  <dig>  documents were retained for analysis. document sections were identified by keywords that appear in their header . the most common sections were represented within the following classification: ‘introduction’, ‘materials and methods’, ‘results’, ‘discussion’ and ‘conclusions’. headers that could not be assigned to any of these sections or that could be assigned to several  were labelled as ‘unknown’.

document classification
document classification was performed by the medlineranker web tool  <cit> , which processes biomedical abstracts from pubmed. medlineranker implements a linear naïve bayesian classifier that is trained on a set of documents representing a topic of interest  in comparison to random documents or the rest of pubmed . after training, the algorithm ranks a third set of documents . each set is defined as a list of relevant pubmed identifiers . nouns in abstracts are used as classification features . full text, annotations or metadata  are not taken into account. counting multiple instances of nouns in the same abstract was shown not to improve performance significantly  <cit>  and is not used by medlineranker. for each scored abstract, an associated p-value is defined as the proportion of documents with higher scores within  <dig>  random recent abstracts.

we used a local database to build queries to the medlineranker soap web service . our database provided the training set as pmids of documents cited in a query document. background sets were composed of random articles or the rest of pubmed. in all the benchmarks, we used non overlapping training and background sets, and test sets were processed using a leave-one-out cross validation procedure. scripts and statistical analyses of the data mining method were programmed using perl  <dig> . <dig> and r  <dig> . <dig>  <cit> . it is important to note that medlineranker processes only pubmed abstracts, and that information on cited documents was used only to build appropriate training sets.

benchmark 1
a first benchmark was performed to assess if references, used to train a classifier, allow accurate classification of the citing document from a large set of random documents. a total of  <dig>  articles were randomly selected for this test. for each of them, we built a training set composed by pmids of its references, which was used to rank the article with respect to the rest of pubmed. as mentioned above, we prepared the training, background and test datasets so that they had no single document in common.

benchmark 2
in a second benchmark, we assessed the usefulness of references to retrieve documents related to the topic described in the citing document. manual annotations of pubmed entries with mesh terms provide accurate sets of topic-related documents . we selected six topics represented by the following mesh terms: 'breast neoplasms', 'alzheimer disease', 'stem cells', 'phosphorylation', 'oligonucleotide array sequence analysis' and 'randomized controlled trials as topic'. there were  <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> pmc-oa articles annotated with these mesh terms, respectively. the task consisted in finding related documents to a query document by classifying the pmc-oa dataset in two sets of related and non-related documents.

for each mesh term m, we built a list of positive pmc-oa documents , a list of negative pmc-oa documents , and a background set .

for each positive document, we built several training sets composed by either its own pmid, pmids of all of its references, or pmids of references cited in particular sections. then, medlineranker was trained with each training set and the background set to rank the all pmc-oa . the overlap between different sets, including cited articles in the training set, was removed before training. results obtained using only the abstract of the query document and not its references were taken as baseline.

given a p-value threshold, medlineranker returns a list of candidate abstracts from the test set. that list of candidates includes a set of true positives if they belong to the positive set and a set of false positives otherwise. the true positive rate  is defined as the number of true positives in the list of candidates divided by the total number of positives. the false positive rate is measured as the number of false positives in the list of candidates divided by the total number of negatives. classification performance is then measured by the area under the receiver operating characteristic  curve. mann–whitney u tests  <cit>  were used to compare distributions of areas under the roc curve. tests having a p-value below  <dig>  were considered significant.

comparison to pseudo relevance feedback 
in benchmark  <dig>  we also compared our proposed query expansion method using all cited references of the query document to prf. as described above, for each mesh term m, a positive, a negative and a background set were defined. for each positive document, the query expansion was defined by the top  <dig> pmc-oa documents returned by an initial ranking of all pmc-oa documents using the single positive document for training versus the background set. the positive document and the additional  <dig> pmc-oa documents were then used to train medlineranker versus the background set to rank the all pmc-oa. only this second ranking was evaluated by the area under the roc curve.

scoring schemes
medlineranker uses a naïve linear bayesian classifier. for comparison, we have implemented pmra and bm <dig> formulas in medlineranker. formulas of pmra and bm <dig> apply to the comparison of only two documents. in medlineranker, each document from the test set is compared to the training set which could contain several documents. in this case, documents of the training set are merged and considered as a single one.

medlineranker
the medlineranker algorithm consists on comparing noun usage into a set r of relevant abstracts  of size nr and a set r’ of irrelevant abstracts  of size nr’ . for a given abstract, each feature i  is given a weight wi by the following formula:

 wi=tr,i1-tr,i/tr',i1-tr',i 

it is the refactored-for-speed weight which allows summing of only nouns that occur in the abstract  <cit> , where the posterior estimate of the frequency of feature i in relevant documents tr,i is defined as:

 tr,i=nr,i+2trnr+4tr 

this estimate uses the split-laplace smoothing introduced in  <cit>  to counteract class skew, where nr,i is the occurrence of noun i in relevant documents, and the laplace-smoothed probability of relevance tr is defined as:

 tr=nr+1n+ <dig> 

where n is the total number of documents. tr',i, and tr’ are obtained from the same formulas by replacing r by r’ where nr’,i is the occurrence of noun i in irrelevant documents.

finally, the score of a given abstract a is the sum of its noun weights:

 scoremedlineranker=∑i∈awi 

pubmed related articles 
we implemented the scoring function pmra with optimal parameters from  <cit> , defining the similarity of document c and d:

 scorepmra=∑t∈cwt,c*wt,d 

where wt,c is the weight of term t in document c. this weight is defined as:

 wt=1+μλk-1e-μ-λl-1idft 

where μ= <dig>  and λ= <dig>  as proposed in  <cit> , k is the number of occurrences of term t in the document, l is the length of the document in words, and idft is the inverse document frequency for term t defined as:

 idft=log1+numberofdocuments1+numberofdocumentscontainingtermt 

okapi bm25
we implemented the scoring function called okapi bm <dig>  <cit>  based on the formula used in  <cit> . this score comparing document q and d is defined as:

 scorebm25q,d=∑i∈didfi*nik1+1ni+k11-b+bdavgd 

where the ni is the frequency of term i in document d. |d| is the length of the document d in words, avg is the average document length, and b= <dig>  and k1= <dig>  as proposed in  <cit> . idfi is the inverse document frequency of term i defined as:

 idfi=log <dig> +n-di <dig> +di 

where n is the total number of documents in the dataset and di is the number of documents containing term i.

RESULTS
as training an accurate classifier with only one document is a challenging task and reflects real use cases, we have tested the relevance of using freely accessible data from referenced documents . after analyzing data on documents and references from the pubmed central open access subset , we have addressed the following questions: are references cited in a document relevant to discriminate this document  from random ones? is the relevance of cited references to classify the citing document dependent on the section in which they appear? we have also compared this method with pseudo relevant feedback and several scoring schemes.

pubmed central open access subset data
a local database was first built to store data of  <dig>  open access documents from pmc-oa. for each document, information about its cited references, including in which manuscript section they were cited, was also included in the database. a total of  <dig> , <dig> references to cited documents were then retrieved . finally, we stored in the database the list of topics defined by mesh annotation  associated to each article.

of all pmc-oa documents,  <dig> % were covered by pubmed . the most common document types were research article , review article , and case report  . they were largely covered by pubmed .

of all references cited in pmc-oa documents,  <dig> % were covered by pubmed. the most common reference types were journal , book , and web page . only references to journal documents were largely covered by pubmed .

in benchmarks shown below, training sets can be composed of articles cited from different sections. in principle, the more documents in the training set, the better the classification. therefore, we examined in more details the distributions of references per section . full text documents contained on average  <dig>  references. ‘introduction’ and ‘discussion’ sections contained a fair average number of references . fewer references were obtained from the ‘conclusion’, ‘results’ or ‘materials and methods’ sections were fewer .

benchmark 1: retrieving a document using its references
the first benchmark was performed to determine if references cited by a document allow the classification of the citing document from a set of random documents . in principle, the set of references or a subset of it is expected to be strongly associated with the same topics of the original document. for this benchmark we used a test set composed of  <dig>  randomly chosen documents. medlineranker was used to rank each document with respect to the whole set of  <dig>  documents using the references cited in various sections. the output ranks of these documents were analyzed .

using references from the full text  provided the best rankings for the citing documents followed by ‘introduction and discussion’ , ‘introduction’  . using the ‘discussion’  led to more variability, though the median rank was still below  <dig>  other sections showed clearly worse results, with the ‘methods’ and ‘results’ sections showing very high variability, and the ‘conclusion’ being totally irrelevant. these results show that references are highly related to the topic of the article where they are cited. therefore, they could be used to retrieve more documents related to the citing document.

benchmark 2: retrieving topics-related documents using references from a single document
next, we wanted to evaluate how the performance of topic-related document retrieval from a single query document supplemented with cited references is affected by the topic of the query document. we chose six particular topics in pmc-oa documents represented by their mesh annotations . each pmc-oa document related to these topics  and its cited references were used to classify the rest of pmc-oa in two sets of topic-related and non-topic-related documents.

comparing the distributions of roc areas, training sets composed by cited references from the full text , the ‘introduction’  or the ‘discussion’  always returned significantly better results  than the baseline  , with higher effect size than other training sets except for topic ‘oligonucleotide array sequence analysis’ where references from the ‘methods’ and ‘results’ sections performed well.

for this reason, references from the ‘introduction’ or the ‘discussion’ sections were also joined in an additional training set , which performed similarly to using the set of all references.

the ‘conclusion’ training set showed non significant results  except for topics ‘breast neoplasms’ and ‘oligonucleotide array sequence analysis’ although medians were very close to the baseline . this could be expected from the low number of references associated to this section .

note that performances reported above for references taken from each section correlate with the number of documents sharing the query mesh annotation . interestingly, for the term ‘randomized controlled trials as topic’, we found very few cited documents sharing the annotation but classification performances were still good. this highlights the usefulness of algorithms that do not use annotations but only words in text.

comparisons
additionally to the built-in medlineranker scoring scheme based on naïve bayesian statistics, pmra and okapi bm <dig> scoring schemes were also used for comparison in benchmark  <dig>  we produced results for the baseline  and for the training sets composed by all cited references  . on the baseline, all scoring schemes showed close results although pmra’s median was often slightly higher. medlineranker and bm <dig> scoring schemes produced always significantly better results than their respective baselines . on the contrary, results for pmra were always significantly worse than the baseline .

we compared our proposed query expansion method using cited references to an implementation of prf. expansion used text from top  <dig> returned documents from an initial query based on the single query document. prf was significantly better than the baseline in  <dig> topics and significantly worse in one topic . our method significantly outperformed prf in  <dig> topics , and prf was significantly better in  <dig> topics  but with lower fold changes .

discussion
a simple and popular request in document retrieval is to find the bibliography related to one single document, as implemented in the pubmed related articles  feature  <cit> . text mining algorithms for document retrieval are optimally trained with large enough sets of relevant text  <cit> , thus using a training set composed of one single article is challenging. here we have evaluated the potential of the expansion of single article training sets with the bibliography cited in the article. as shown in a previous study about keyword content in full text biomedical articles  <cit> , manuscript sections are relevant in the retrieval of the article topic. thus, we also explored how retrieval of related documents depends on the use of references from different manuscript sections.

while the pubmed biomedical literature database contains millions of freely available abstracts, information on cited references was found in  <dig>  full text documents from the open access subset of the pmc database  <cit> . consequently, the proposed approach is limited by accessibility to full-text documents. note that the number of open access pmc articles is currently too small for some text mining studies. for instance, the biocreative iii challenge first intended to run a text mining competition using full text articles to extract or retrieve protein-protein interaction  data or documents relevant to ppi information but finally only abstracts were used due to the very small overlap between pmc and known manuscripts cited in ppi databases  <cit> .

the size of pmc-oa was also too small to have an interesting overlap with existing text corpora such as from the trec genomics tracks and ohsumed  <cit> . consequently, we have used mesh term annotations to define related documents as documents sharing a same mesh term. this could be refined taking advantage of the mesh vocabulary hierarchy, including for example children terms. our second benchmark could be seen as a mesh indexing task for which various methods were proposed . differently from these methods, we focused only on selected topics representing exemplary biomedical research fields avoiding general topics such as ‘human’ or ‘europe’; we also did not investigate the indexing of several or all mesh terms simultaneously. different benchmarks would be needed in order to compare our method to existing mesh indexing algorithms.

while availability of full text is a limitation, mapping of references to pubmed is not an issue for most pmc-oa documents and their cited references as shown by our study . moreover, the average number of references in documents  shows clear potential for improving classification results   <cit> .

we demonstrated that cited references found in a document can accurately discriminate this document from a random set . using all references led to better results than using the baseline  or references cited from particular sections. however, it was interesting to note that gathering references from ‘introduction’ and ‘discussion’ showed similar performance : these two sections may contain most of the topic-related data useful for classification  <cit> . this is supported by the higher number of citations found in these sections  and the enrichment of these cited references in similar mesh annotations . this result may be of interest for users of support vector machines or other similarly computing-intensive methods, since reducing the number of documents or features in the training set would shorten the training procedure without affecting performance  <cit> .

query expansion by citation context was already shown to be effective  <cit>  although terms from citation context describe general aspects of the topic of an article and classification performance may decrease with topic specificity  <cit> . topic-dependent results were also found in mesh indexing  <cit> . in our study, we also noted that classification performance using references by section was dependent on the topic. in general, ‘methods’ and ‘results’ sections performed worse. but, these sections performed better for the technical topic ‘oligonucleotide array sequence analysis’ . the decision to limit the use of cited references to a given section to train a text classifier must therefore depend on the topic. the choice of the scoring scheme is also critical since the query expansion could be detrimental to the performance, such as for the pmra scoring scheme. notably, the implementation of the latter is based on a publication from  <dig>  <cit>  and differs from the current version available to pubmed users .

comparison to an implementation of pseudo relevance feedback  was significantly favourable to our method in  <dig>  out of  <dig> topics. contrary to our method, prf was not systematically better than the baseline but other implementations of prf may perform better, especially when weighting differently text from pseudo relevant documents  <cit> . nevertheless, a major advantage of prf is that it is not limited by access to full-text documents.

while we have focused on the biomedical field, it would be interesting to generalize its conclusions to other fields; this would need further benchmarks. only pubmed and pmc-oa were used as source of text and references data while other databases may be valuable such as google scholar or private content from some scientific publishers. nevertheless, we have used the largest biomedical resources providing free content and widely used by the community, which allow reproducing and studying ways to improve upon our results. only few selected topics were analyzed in detail, though they represented different biomedical research fields and the first benchmark  could be considered as a topics-independent proof of concept. still, we have observed some degree of topic-specific behaviour, but a more thorough study including more topics may reveal interesting results.

CONCLUSIONS
in conclusion, we have demonstrated the usefulness of cited references to expand text used by classifiers using as input a single document. choosing all cited references is the safest choice while references from a particular section might not be suited for some topics. implementation of such method may be limited by access to full-text articles or data on cited references, but can significantly outperform pseudo relevance feedback methods  and will further improve in the near future due to the growth of the open access scientific literature.

abbreviations
pmra: pubmed related articles; pmc: pubmed central; pmc-oa: pubmed central open access subset; pmids: pubmed identifiers; mesh: medical subject headings; roc: receiver operating characteristic; prf: pseudo relevance feedback

competing interests
the authors declare that they have no competing interests.

authors’ contributions
conceived and designed the experiments: j-ff, fmo and maa-n. performed the experiments: j-ff and fmo. analyzed the data: j-ff, fmo and maa-n. wrote the manuscript: j-ff, fmo, ir and maa-n. all authors read and approved the final manuscript.

