BACKGROUND
the number of public online bioinformatics resources has grown exponentially over the past few years. bioinformatics professionals can access and use a large number of resources for their research --including databases, tools and services. discovering and accessing the appropriate bioinformatics resource for a specific research task has become increasingly important, as suggested in earlier reports  <cit> .

to address this issue, various significant projects and initiatives have been carried out, leading to several pioneering indexes of bioinformatics resources that are currently available over the internet. for instance, bioportal  <cit>  is a web repository of biomedical ontology resources, developed at the national center for biomedical ontology   <cit> . this application enables collaborative development of biomedical ontologies. bioportal includes a service called open biomedical resources  for annotating and indexing biomedical resources  <cit> . resources are annotated using concepts from a domain ontology. the obr service enables researchers to locate resources by specifying ontology concepts.

other examples of such indexes include the biomedical database collection compiled by galperin  <cit> --a yearly updated web-based list of molecular biology databases sorted in alphabetical order-- or the bioinformatics links directory   <cit>  --a catalogue of links to bioinformatics resources, tools and databases classified into eleven major categories-- where resources can be searched using keyword-based queries.

the european bioinformatics institute  provides a searchable index of an alphabetically-sorted inventory of bioinformatics resources  <cit> . resources are classified according to the type of service they provide--databases, tools and  services. the index includes both internal and external resources.

the biomoby platform provides an annotated registry of bioinformatics web services enabling other applications to integrate and use such services  <cit> . there are various major installations of biomoby, such as, for instance, the planet consortium  <cit> , the australian centre for plant functional genomics  <cit> , the generation challenge program of the consultative group for international agricultural research  <cit> , genome canada  <cit> , or the spanish national institute of bioinformatics   <cit> . they provide access to different bioinformatics resources depending on their own interests and needs.

a consortium composed of the seven us national centers for biomedical computing  <cit>  has recently developed another index of bioinformatics resources called itools  <cit> . web services are used here to provide access to resources that are annotated according to their functionality. a web-based interface enables researchers to locate the resources they need using advanced search and visual navigation tools. another initiative for organizing bioinformatics resources is the bio2rdf system  <cit> . this system considers the information contained in biological databases, providing uniform access to biological data stored in public databases. data is converted into rdf format using a common reference ontology. more than  <dig> publicly available databases --e.g. entrez gene, omim, go, obo, pdb, genebank, prosite, etc. -- have been successfully integrated, including more than  <dig> million rdf documents.

in this paper we present a tool to automatically create a searchable index of bioinformatics resources from the scientific literature. the resources are annotated with metadata regarding their functionality. metadata are extracted from abstracts of research papers retrieved from pubmed®  <cit>  and the isi web of knowledge®  <cit>  using pattern-matching techniques. the generated index can be incrementally updated by feeding our tool with abstracts of articles describing new resources. in the evaluation section, we present and analyze some figures that compare the results of our approach with other existing indexes.

methods
we extract relevant information about bioinformatics resources from papers published in high-impact journals. we focus on manuscript abstracts, which tend to condense the key information. we use classic natural language processing techniques--such as, for instance, tokenizers, parsers, transition networks or part-of-speech taggers--to extract this information. these techniques provide the basic framework for extracting data from textual resources  <cit> . all the techniques we have used are detailed in the following sections. the outcome of this process is an indexed knowledge base including the most relevant information about the resources.

building the knowledge base
we propose a five-phase method for automatically generating a knowledge base of bioinformatics resources using research paper abstracts. figure  <dig> shows an overview of the proposed methodology. next we detail the five activities.

abstract selection and generation of abstract surrogates
first we retrieve a collection of papers from well-known scientific literature web sites. each paper is then analyzed to produce a structured surrogate, including the title, authors, abstract and electronic references to the paper--i.e. the pubmed® identifier . the structure of the surrogate and a complete example is shown within the "papers" section in table  <dig>  such surrogates simplify the pattern-matching process. for instance, the title of the paper usually contains the name of the resources. we use that knowledge as a heuristic to discard unnecessary searches in other sections of the manuscripts.

the table shows an example of all the information about resources in the biri knowledge base. for each resource, the system stores the information that it has extracted from the literature. additional links, if any, are provided to the resource's official web page and to the pubmed® website.

surrogate pre-processing
once the surrogates have been generated, the titles and abstracts are divided into sentences, using strong punctuation marks as sentence delimiters. then, each sentence is pre-processed by a lexical analyzer. the analyzer extracts the words of the sentences using blanks and punctuation marks as word delimiters. this produces a sorted set of tokens tagged with lexical annotations. next, each token is labelled with its corresponding part-of-speech  tag using a probabilistic pos-tagger  <cit> . stop words--such as articles, prepositions, etc.--are also filtered during this process. finally, a stemming activity is performed to reduce each token to its root form, thus returning a set of lexemes tagged with morphological information  <cit> .

manual generation of patterns and transition networks
to automatically extract information from the abstracts, a group of five experts with background in different bioinformatics and text mining techniques created a set of linguistic patterns and then analyzed the selected abstracts. the set of linguistic patterns characterizes the type of information to be extracted. we used an initial training set of  <dig> abstracts of randomly chosen papers from pubmed®, describing bioinformatics resources--e.g. database and software papers published in the bmc bioinformatics journal  <cit>  or application notes from the bioinformatics journal  <cit> . the selected abstracts were analyzed to discover and identify three different types of patterns: i) resource-naming patterns , ii) functionality patterns , and iii) classification patterns . rnps aim to automatically extract the resource names together with their corresponding url. conversely, fps aim to extract short textual descriptions of resource functionalities. by contrast, cps focus on either i) the category of the resource or ii) its target domain.

a set of categories and domains was defined by the team of experts based on the taxonomy developed by the bioinformatics links directory. this set includes the key categories of bioinformatics resources--e.g. databases, annotation services, visualization tools, etc. each category is represented by a preferred name, together with a set of synonyms. each resource category is linked to a collection of topics for its target domain. for instance, examples of domains for the "database" category are "dna", "microarray", "polymorphisms" or "protein". table  <dig> shows the complete list of biri categories and domains.

the table shows the full list of biri categories and domains. this list is composed of  <dig> categories and nine domains. each domain has an associated set of categories and each category can belong to several domains.

once the experts had identified the patterns, they were translated into transition networks   <cit> . tns are simple but effective abstract machines that determine whether a string belongs to a language defined by a regular expression. we adopted this approach since tns are simple yet powerful tools suitable for performing pattern-matching tasks. in our approach, tns are used to recognize instances of the patterns in the abstracts. we defined two different tns. the first tn is designed to detect and extract the names of the resources together with brief descriptions of their functionalities. this tn is based on the extracted rnps and fps. conversely, the second tn is designed to classify the resources into different categories depending on their functionalities and target domains. figure  <dig> shows an extract of the first tn. this is represented as a finite state machine. each state is represented by a numbered circle. the labelled edges show the required textual input for transiting from one state to another. given an input, the aim is to reach a final state  from an initial state . if a final state is reached, then the tn outputs the extracted information for the matched resource.

information extraction
extracted patterns are applied to the abstracts as follows. first, the abstract is analyzed by the first tn to locate the substrings that match any of the patterns. then, the relevant information is extracted from the substring. for instance, the input sentence "the fssp database of structurally aligned protein fold families" matches the rnp . in this case, the analyzer identifies "fssp" as a proper noun. another example of rnp could be  that would match the sentence "embl-align: a public nucleotide and amino acid multiple sequence database. in this case, "embl-align" would be recognized as a proper noun.

fps are often more complex to match than rnps and cps since their associated patterns require further lexical components for instantiation--e.g. verb tenses, part-of-speech tags, etc. for instance, the input sentence "a novel method for fast and accurate multiple sequence alignment" matches the following pattern , where jj represents an adjective, cc identifies a conjunction and nn is a common name. we use special characters of regular expressions to specify the patterns, where '?' represents optionality, '+' one or more occurrences and '*' zero or more occurrences of the component. in the last example, the functionality extracted would be "sequence alignment". during the pattern-matching process, the instantiation of one of the patterns triggers a procedure that extracts and stores the relevant information in the knowledge base.

resources classification
once the name and the description of the functionalities for a given resource have been extracted, we run the second tn to perform the classification. the tn is fed with the extracted description of the functionality of the current resource, and it tries to match this description with a resource category and domain. if successfully matched, the resource is labelled with the preferred term associated with the respective category or domain. note that a resource could be assigned to several categories or domains. for instance, one possible pattern for detecting and extracting a resource category and domain is: . applying this pattern to the sentence "amigene: a tool for annotation of microbial genes", the category we get is "annotation" and the resource domain is "genes".

additionally, any extra information--such as resource inputs and outputs--can also be extracted by the second tn when available. we refer to the type of data received by the resources when they are invoked as 'inputs' and to the type of data they return as a result as 'outputs'. for instance, applying the pattern  to the sentence "we present here a new tool for discovery of unstructured or disordered regions within proteins.", "proteins" would be the resource input data.

data curation
once all relevant information contained in the abstracts has been extracted, a team of experts reviews the contents of the knowledge base to assess its correctness. the curation process mainly focuses on the detection of categories and domains incorrectly assigned to specific resources. then, the experts compare the previously extracted functionality with the assigned categories and domains. if any errors are detected, the inaccurate entries are removed from the biri knowledge base.

description of the knowledge base
once the information has been filtered, it is stored in a knowledge base. for each discovered resource, we record the following data: i) the name of the resource, ii) its corresponding url, iii) a set of natural language descriptions of its functionalities, iv) the resource's assigned categories, v) its target domain and vi) the resource's inputs and outputs when available. using the same example as above, this information could be extended and used to automatically orchestrate workflows involving multiple resources. the knowledge base also stores data about the original papers, including the paper's title, author, abstract and pubmed® and isi web of knowledge® identifiers.

RESULTS
we implemented the data extraction tool and the web services layer  providing the query primitives to access the knowledge base using the java programming language and associated technologies  <cit> . this includes the java web services developer pack   <cit> .

the system architecture includes a web services query layer developed to cover the contents of the knowledge base. to test the functionality of the developed wsl, we created a pilot web application on top of the wsl to provide users with a searchable web index of bioinformatics resources, by using popular web browsers including internet explorer®, mozilla firefox® and safari®.

as figure  <dig> shows, the pilot web application provides the following search capabilities: i) retrieve all the information related to a specific resource given its name, and ii) search for relevant resources belonging to a given category and/or target domain. in both cases, the user is presented with a list of records matching the user query and containing basic information on the retrieved resources. this includes the resource name with a link to the complete information of the resource, a short textual description of the provided functionality, and its assigned categories and domains. note that the indexing engine may have classified a single resource into multiple categories and domains, thus producing several entries in the results list. clicking on the resource name, the system shows the complete information on this resource, including additional links to i) the official web page of the resource  and ii) the actual paper record at the pubmed® website from which the information was extracted.

a use case
let us suppose that a researcher wants to identify resources for analyzing information from microarrays. the web application will show the existing categories and domains in two combo-boxes. these categories and domains are dynamically loaded using different web services. when the user selects a category, the domain combo-box is automatically updated to show the domains associated with the selected category only. similarly, when the user selects a domain; the category combo-box is updated with the appropriate categories. for instance, the researcher may select "analysis" from the combo-box category. then, the combo-box domain will be automatically updated with all the available domains for the chosen category. it includes an extra option for selecting "all" domains. the researcher will select the "microarray" domain from that list and then click on the search button. the category and domain selection order is not mandatory. the user could select the domain first and the category afterwards.

after the user clicks the search button, the web application displays a table with information about the resources contained in the knowledge base that meet the user-imposed restrictions. in the above example, the table contains eleven entries, as shown in figure  <dig>  each entry contains the name of and a link to the resource, its functionality, category and domain. further details on a given resource can be retrieved by clicking on the resource name link.

the web application has been tested with multiple queries selecting different combinations of categories/domains. users could also search for a resource by its name. searches by name are case-insensitive. the results would be similar but restricted to the specified resource.

another interesting feature of the application is to incrementally update the index with new resources. this is achieved by entering the respective research papers in the update module. this module verifies the new papers added to prevent double entries. papers that have been previously processed by the system are passed up, and the others are applied as described above. users can also contribute by suggesting the inclusion of additional tools/abstracts by sending an email to the contact address provided on the biri web page.

evaluation
as stated previously, resources are automatically classified and indexed according to a custom-created list of categories and domains shown in table  <dig>  this list is composed of  <dig> categories--i.e. resource types--targeting nine different domains--i.e. data types. we conducted an experiment using an input set of  <dig> abstracts extracted from the isi web of knowledge®. to create the test set, we queried the system with the term "bioinformatics resources". then, we sorted the results by impact factor and date, and finally we selected the first  <dig> documents  according to this classification . we also included a small number of documents  unrelated to bioinformatics resources to test the robustness of our approach. as figure 5a shows, the first tn extracted the names of  <dig> resources --i.e.  <dig> manuscripts  did not produce anything. from these  <dig> manuscripts,  <dig> papers  were discarded even though they contained information about bioinformatics resources and the remaining  <dig> papers were the control set that we manually created and included for this experiment.

regarding the functionality extraction process, the first tn discovered  <dig> descriptions of functionalities. they were assigned to the  <dig> identified resources. note that a single resource may be assigned to one or more functionalities. as figure 5c shows, a high percentage of the extracted functionalities  provided complete and coherent descriptions. conversely, 10% were incomplete descriptions that still provided valuable information for automatically extracting their associated category and domain. the remaining 2% matched up with incorrect or incoherent descriptions. the coherence of the extracted functionalities was manually assessed to verify the correctness of the extraction method.

conversely, figure 5b shows the results of the classification process carried out by the second tn. this was fed with the set of descriptions extracted by the first tn. the second tn successfully assigned at least one category to  <dig> resources, and at least one domain to  <dig> resources. table  <dig> shows some sample entries of the generated knowledge base.

the table shows examples of the knowledge base contents according with the resources shown in figure  <dig>  after executing a query, this is the information provided by the biri web application. it includes resource names and links, functionalities, categories, and domains.

these entries present the extracted information for four different bioinformatics resources. the first column shows the name of the extracted tools--i.e. "crosslink", "fatcat", "matras" and "genepublisher"--whereas the second column provides a short description of their functionality. the third column shows the category in which the corresponding resource has been classified. this classification is further refined in the fourth column, which lists the specific application domain for this resource. for instance, as shown in the table  <dig>  the "crosslink" resource has been classified under the category "exploration" and targets the "rna" domain. therefore, "crosslink" can be regarded as a tool to explore rna sequences. also, additional information can be retrieved for each resource. table  <dig> shows an example of all the information biri provides about any resource. this information has been automatically extracted from the scientific papers using the proposed method. it contains the resource name, functionalities, categories, domains, inputs, outputs and paper from which the information has been extracted. links to the official web site of the resource and to the whole paper --in pubmed®-- are provided if they are available in the abstract.

to evaluate the suitability of our approach, we have compared biri with other existing public indexes according to two dimensions: i) features and characteristics provided by the indexes, and ii) the number of resources contained in each index that also appear in biri. table  <dig> focuses on the first dimension--i.e. features and characteristics of the indexes. the information used to complete table  <dig> has been extracted from the literature and the official web sites where the indexes are publicly available. we considered five different features: i) generation of the index, --i.e. whether the index was created manually or automatically--, ii) indexation of external resources--i.e. whether other resources have also been indexed in addition to resources that actually belong to the institution or consortium that set up the index--, iii) the type of search capabilities provided by the index, iv) whether or not the resources are annotated, and v) whether the index includes any form of resource classification. as shown in table  <dig>  all the indexes that have been analyzed provide some kind of resource classification and annotation. most of these indexes consider external resources and provide other advanced search capabilities. the major differences lie in the index generation process since, in most cases, the indexes are manually generated.

we compare existing indexes containing information about resources. we considered several classification criteria: i) automatic index generation, ii) whether or not the index indexes external resources, iii) whether the index interface provides advanced search capabilities, iv) whether or not the resources are annotated, and v) whether the index establishes a resources classification.

we compare the resources contained in the curated biri knowledge base with the resources indexed by other public indexes at the time of writing this paper. the table shows: i) the total number of resources indexed by each index, ii) the number of matches found between biri and the index, and iii) the number of resources existing in biri knowledge base that do not exist in the other index.

discussion
based on pattern matching methods, our method can automatically create a knowledge base of bioinformatics resources by i) detecting resource names, ii) extracting short descriptions of functionalities and iii) classifying the extracted artefacts according to a list of categories and domains of bioinformatics resources, which extends the bld classification  <cit>  on which our list is based. we believe that creating a standardized taxonomy or ontology of bioinformatics resources is a crucial task to facilitate collaborative and integrative approaches  <cit> .

for the methodology proposed in this paper to work, the bioinformatics resources have to have been previously published and indexed in pubmed® or the isi web of knowledge®. otherwise, the resources would never be found using this method. in contrast, our approach guarantees that only high-quality resources are indexed. once these resources have passed a peer-review process, confidence in their quality can be actually higher.

our approach extends most of the search capabilities provided by other existing tools. in addition, the index is automatically generated and updated in our approach. the update process is a very time-consuming and tedious task that is usually performed manually by groups of experts. applying the same methodology detailed above, the contents of our index and knowledge base can be automatically updated by just entering new manuscripts and abstracts into the tool.

our index provides users with advanced search capabilities. it can, for instance, perform complex searches, such as searching for resources matching a definite category and/or target domain. as stated previously, our knowledge base is built automatically using pattern-matching techniques, whereas other indexes are created, maintained and updated manually. besides, our knowledge base can be automatically updated with new resources simply by feeding the developed information extraction tool with manuscripts describing recently developed tools, databases and services.

considering other existing indexes, our knowledge base provides additional information, such as, for instance, the target domains where the different types of resources can be applied or the resource inputs and outputs. using the resources' names and extending the information about inputs and outputs, biri could be useful for automatically orchestrating workflows like applications combining several resources. an example of resource combination through workflow definition is described in a previous work carried out by the authors, where multiple databases are queried to retrieve information regarding the proteins involved in a genetic disease  <cit> . the results provided by a database are used to build the query for the next database.

additionally, text-mining based methods for information extraction reported that they could benefit from manuscripts with more structured abstracts  <cit> .

CONCLUSIONS
our tool automatically extracts and organizes relevant information about bioinformatics resources from research papers describing the resources. our method, based on a domain-independent approach, can be used to create inventories targeting different scientific fields  <cit> . for instance, this approach is currently being applied in the european commission-funded action-grid project, coordinated by the authors  <cit> . in one of the workpackages of this project, we are creating an inventory of bioinformatics and nanomedical resources that is intended to help researchers in these areas  <cit> .

several initiatives have been carried out aimed at cataloguing the existing bioinformatics resources. although our tool could work as a standalone application, it has not been designed for this purpose. our tool is intended not as a replacement for but an add-on to existing indexes and applications. we are working on integrating our tool as a plug-in for other consolidated applications, such as bioportal or biomoby. additionally, tools for defining workflows, such as taverna  <cit> , could also benefit from the information provided by an extended version of our index.

the inventory of resources could also be collaboratively updated by other external contributing researchers. this collaborative approach is being successfully applied in other fields, such as, for instance, developing biomedical ontologies at the ncbo  <cit> . at the ncbo, different collaborative tools--developed using web  <dig>  techniques--are available for developing biomedical ontologies. researchers can contribute by entering new information or comments about the existing information.

authors' contributions
gdlc conceived and participated in the design of the study and drafted the manuscript. mgr conceived and participated in the design of the study and drafted the manuscript. sc participated in the design of the study and the evaluation of the results. ddli participated in the design of the study and implemented the system. vm conceived the study and helped to draft the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
pubmed identifiers list. this file contains the list of the  <dig> pubmed identifiers selected to create the test set.

click here for file

 acknowledgements
the present work has been funded by the spanish ministry of science and innovation , the european commission through the acgt integrated project , the action-grid support action  and the comunidad de madrid, spain.
