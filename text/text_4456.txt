BACKGROUND
molecular understanding of tumor heterogeneity is key to personalized medicine and effective cancer treatments. numerous studies have identified molecularly distinct cancer subtypes among individual patients of the same histopathological type by performing a high-throughput gene expression analysis of the patient tumor samples  <cit> . while microarray-based gene expression estimates are often not precise or quantitative enough for applications in the clinical setting, the expression profile data from microarrays are the basis for the widely used oncotypedx  test, which predicts the risk of recurrence in patients with early stage breast cancer  <cit> . the oncotypedx assay analyzes the expression of  <dig> genes by rt-qpcr to provide a recurrence score that is unique to each patient  <cit> . more recently, the development of next-generation sequencing  based techniques, rna-seq  <cit> , is enabling gene expression analysis to yield a much greater resolution for accurate identification of different isoforms. while several genome-wide expression profiling studies have dramatically improved our collective understanding of cancer biology and led to numerous clinical advancements  <cit> , the use of genomics based molecular diagnostics, such as oncotypedx  <cit> , in clinical practice still remains largely unmet for majority of human cancers  <cit> .

a crucial step in the translation of gene signatures derived from high-throughput platforms is validation in a clinical setting, using robust and quantitative assay platforms  without loss of any classification accuracy  <cit> . a major bottleneck in translating the prognostic or molecular subtyping statistical models is lack of adaptability of the derived models from one analytical platform to another. in other words, assuming that we have gene expression data for a set of tumor samples  from two different analytical platforms, "can a statistical model derived on data from one platform  accurately predict the class labels using data from another platform  for the same patient samples?" while several normalization strategies, such as locally weighted scatter plot smoothing   <cit> , rank and quantile normalization methods  <cit> , have been successfully applied to eliminate systematic errors in data from a same platform, these methods are not appropriate for normalization of data from different profiling platforms  because of the differences in the data scales and magnitude. in such cases, researchers usually accept the normalized data in the original analyses, and harvest the list of differentially expressed  genes from each study by rank ordering. then, the genes are prioritized by comparing the lists of up- and down-regulated  genes between studies, rather than comparing individual expression values. however, these pre-processing methods are not useful in developing platform-independent statistical models.

data discretization is a popular data pre-processing step used in supervised learning for creating the training sets. data discretization transforms continuous values of feature variables to discrete ones  <cit> . it can significantly impact the performance of classification algorithms in the analysis of high-dimensional data  <cit> . different data discretization methods have been developed that can be categorized as:  supervised vs. unsupervised methods depending on the availability of class labels;  global vs. local methods considering all or only one feature to discretize; and  static vs. dynamic methods based on interdependency between attributes. many discretization techniques have been applied to analyze gene expression data, for example, to devise a new approach to explore gene regulatory networks  <cit> , and as a pre-processing step to improve classification accuracy using microarray data  <cit> . while the previous studies have used the discretization method as a pre-processing step to design and apply the statistical models on data from one platform , our goal in this study is to evaluate three unsupervised data discretization methods--equal width  binning, equal frequency  binning, and k-means clustering--in combination with different feature selection and machine learning methods for deriving the most accurate classification model from one platform , and apply it to data from another platform  for molecular subtype prediction of a future cancer patient.

feature selection algorithms seek for a subset of relevant features to use in model construction in order to simplify and reduce over-fitting of the models. the wrapper, filter, and embedded methods are the three main categories that have been widely used in biomedical research to deal with a large feature space  <cit> . briefly, wrapper algorithm uses a predictive model that scores on a new each subset to train, and test on the remaining set; filter algorithm uses a direct measure instead of the error rate estimate to score a feature subset; embedded algorithm integrates feature selection as part of the model construction process including the recursive feature elimination  algorithm. in this study, we adopted two advanced feature selection algorithms based on svm and rf, and one filter method using the coefficient of variation , a statistical measure to find highly variable genes.

using a subset of most important genes  screened by the variable selection methods, numerous classification methods have been applied to tackle disease sample classification problems. for example, svm was applied for characterizing functional roles of genes in yeast genome and cancer tissues  <cit> , rf for classifying cancer patients and predicting drug response for cancer cell lines  <cit> , nb  for classification on prostate cancer  <cit> , and pam  for molecular classification of brain tumor and heart disease  <cit> . these studies, however, focused largely on the data from one platform such as microarray, although cross-platform data analysis would help find robust gene signatures. recently, we developed pigexclass  <cit> , platform-independent isoform-level gene expression based classification system, that captures and transfers gene signatures from one analytical platform to another through data discretization. pigexclass is an integrative system that consists of data discretization, feature selection, and classification. the application of pigexclass has led to the development of a novel molecular classifier  for diagnosis of gbm subtypes  <cit> . motivated by the importance of data discretization step in pigexclass algorithm, in this paper we evaluated the performance of three data discretization methods together with four popular machine learning algorithms to derive reliable platform-independent multi-class classification models; specifically, predicting the four known subtypes of gbm patient samples from the same platform as well as independent platforms.

RESULTS
data-discretization retained the classification accuracy with fewer number of variables for data from same platform
because gene isoforms  whose expression levels do not vary much across the samples are less useful for discriminating the four gbm subtypes, we selected  <dig>  isoforms with the highest variability across the samples, using cv . to search for an optimal bin number k for the discretization, we explored various bin sizes including the optimal bin number  based on dougherty's formula  <cit> , and chose the bin number of k =  <dig> as it consistently achieved good accuracy. then we applied two advanced feature selection algorithms, svm-recursive feature elimination   <cit>  and rf based feature selection   <cit> , and prepared independent training and testing datasets by dividing the exon-array samples into four fold; 3/4th  for training and 1/4th  for testing. we describe below the classification performance for each variable selection method--cv, svm-rfe and rf_based_fs.

first, we trained the classifiers with the features ranked by the cv that represent high generic variability. overall, the accuracy of the derived classifiers was within the range of  <dig> - <dig> % for fc and  <dig> - <dig> % for discretized data , suggesting that the discretization retained the classification accuracy of the respective models. more importantly, svm achieved similar accuracy with equal-w binning using only  <dig> features in comparison to without discretization. for rf, nb and pam the classification accuracies and the number of variables used in the models did not differ significantly between the discretized and non-discretized data. we then trained the classifiers by considering only the top  <dig> features that can be clinically testable by, for example, rt-pcr. we observed that svm with k-means clustering yielded the best accuracy of  <dig> % .

# number of variables in the classification model

comparison of classification methods both trained  and tested  on exon-array data. the best accuracy  achieved by each combination of the four classifiers and three feature selection schemes are presented, with number of features used in the best fitted model is shown in parenthesis. the models were built by stepwise addition of feature variables into the model by considering the top  <dig>  ranked feature variables. best accuracy, achieved with the least number of features, is marked in bold for each classification method.

the classification models were trained  and tested  on exon-array data. highest accuracy for each classification method is marked in bold. while svm in combination with rf_based_fs performed best whit the highest accuracy for both fc data  and k-means discretised data, the other three classifiers  in combination with rf_based_fs achieved comparable classification accuracies on eaual-w discretized data.

second, we evaluated the classification performance using the features ranked by svm-rfe. accuracy of the classifiers ranged from  <dig>  to  <dig> % for both fc and discretized data . again, svm showed similar accuracy between discretized and fc data, but required lot fewer variables in the model that was trained on equal-w binning data. similarly, rf showed similar accuracy between discretized and non-discretized data, but the rf model trained on equal-f binning data used only  <dig> variables in comparison to  <dig>  variables required for fc data. interestingly, nb not only improved the classification accuracy with equal-f binning data but also used much fewer number of variables  to achieve the higher accuracy. for pam, the classification accuracy and number of variables in the models remained similar between fc and discretized data. using the top  <dig> features, svm still attained the best accuracy with equal-f binning .

lastly, we used the features selected by rf_based_fs to assess the classifiers' performance. accuracy of the classifiers did not fluctuate much by staying within the range of  <dig> - <dig> % for both non-discretized and discretized data . overall, all the classifiers tested retained their highest accuracies, but with significantly fewer number of variables in the final models. while svm achieved the best accuracy  with fc, it retained comparable accuracy  with just  <dig> variables in the model trained on equal-w binning data in comparison to  <dig> variables in the model trained on fc data. similarly, both rf and nb models trained on equal-w binning data achieved similar accuracy with fewer number of variables in comparison to fc data. interestingly, pam model trained on equal-w binned data slightly improved the accuracy with lot fewer variables in comparison to fc data.

in summary, all the classifiers trained and tested on the discretized data from same platform resulted with lot fewer number of variables, yet retaining the high accuracies in comparison to the corresponding models that were trained on fc data. overall, while svm achieved the best accuracy, equal-w discretization in combination with rf_based_fs helped build the classification models with significantly lower number of variables in the final models.

data discretization improved cross-platform predictions
in order to evaluate the accuracy of classification models on data derived by different gene-expression platforms , we trained the classifiers using the data from exon-array and tested on matched rna-seq datasets for the same tcga samples. first, we observed that the classification framework resulted in poor classification accuracies when the classification and feature selection algorithms were trained on fc data from exon-array data and tested on corresponding fc data from rna-seq platform . the best accuracy of  <dig> % on fc data was achieved by rf with rf_based_fs with just  <dig> variables in the final model. however, with data discretization we observed significant improvements in the performance of the classification framework. below, we report the classification performance in more detail based on testing of the models on data from  <dig> rna-seq samples.

# number of variables in the classification model

comparison of classification methods trained on exon-array  and tested on rna-seq . the best accuracy  achieved by each combination of the four classifiers and three feature selection schemes are presented, with number of features used in the best fitted model is shown in parenthesis. the models were built by stepwise addition of feature variables into the model by considering the top  <dig>  ranked feature variables. highest accuracy, achieved with the least number of features, for each classification method is marked in bold.

for cv based feature selection, the classification accuracy of the models trained on fc  was rather poor and ranged from  <dig>  to  <dig> % . however, the accuracy of the models built on equal-f binning data achieved higher and stable accuracy, ranging from  <dig>  to 100%. notably, the svm classifier accomplished the best accuracy of 100%  with equal-f binning followed by k-means with svm . equal-w binning improved the accuracy for rf , but not for the other three classifiers. when using the top  <dig> features in the final model, rf with equal-f binning correctly predicted  <dig> samples out of  <dig>  achieving ~90% accuracy .

the classification models were trained on exon-array  and tested on rna-seq  data. highest accuracy for each classification method is marked in bold. only rf with equal-f binning achieved greater than 90% classification accuracy.

similarly, for svm-rfe features selection, the prediction accuracy of the models on fc data is quite low, within the range of  <dig> - <dig> %. while equal-f binning improved the accuracy of all the four classifiers, equal-w binning improved the accuracy for svm and rf only . most notably, with equal-f binning discretization, svm classifier achieved the highest accuracy using  <dig>  features. for both equal-w binning and k-means clustering discretization, rf achieved the best performance. using the top  <dig> features, rf with equal-f binning achieved  <dig> % accuracy that is  <dig> % higher than the best accuracy with fc .

for rf_based_fs, the classification accuracies were dramatically improved for the models trained on discretized data, with equal-f binning showing the highest improvement with more than 90% accuracies for all the models . models built using k-means based discretized data also showed significant improvement with fewer number of variables in the final models. considering only the top  <dig> features, rf with equal-f performed  <dig> %  accuracy whereas rf with fc correctly predicted only  <dig> samples out of  <dig> . we present the sensitivity and specificity measures for each classifier trained on the top ranking  <dig> features from exon-array data and tested on corresponding data from rna-seq in table  <dig> 

sensitivity  and specificity  of the classifiers trained on the top  <dig> feature variables from exon-array data and tested on the independent rna-seq data for prediction of the four gbm subtypes.

in summary, we found that equal-f binning based discretization performed best, followed by k-means clustering based data discretization. equal-w binning improved only for rf and not for other classifiers for cross-platform class label predictions.

discussion
the evaluation of the three unsupervised discretization methods using our integrated classification framework revealed that the addition of discretization step into the learning framework led to a large average increase in classification accuracy for all the classification models trained on data from one gene expression platform and tested on corresponding data from a different platform. specifically, the best method, equal-f binning, improves performance of all the classifiers and feature selection methods for cross-platform transfer of the derived models.

in machine learning, data discretization is primarily used as a data pre-processing step for various reasons, for example,  for classification methods that can handle only discrete variables,  for improving the human interpretation,  for faster computation process with a reduced level of data complexity,  for handling non-linear relations in the data, e.g., very highly and very lowly expressed genes are more relevant to cancer subtype, and  to harmonize the heterogeneous data. in this study, we showed that simple unsupervised discretization indeed improved the classification accuracy by harmonizing the data that come in different scale and magnitude from different gene expression platforms. the discretization step lead to numerically comparable measures of gene expression between different platforms, and translate the classification models  across platforms. however, the discretization methods applied in this study have some limitations. for example, equal-w binning is prone to outliers that may skew the distribution  <cit> . the k-means discretization performed relatively well with the cv and rf based feature selection schemes. the known drawback of this clustering discretization, however, is in choosing initial cluster centroids which in general is randomly assigned and less robust to outliers; additionally, it is sensitive to the number of clusters affecting classification accuracy. the equal-f binning performed superior in this study and appeared to be feasible.

the choice of classification algorithms is often important dealing with a certain dataset as each of the algorithms has its own strengths and weaknesses. we experimented on the four state-of-the-art machine learning approaches based on maximum margin, decision tree, probabilistic and clustering classification. while svm achieved the best accuracy, the performance of rf was more consistent when tested with various numbers of features and data types. we used the linear kernel svm because it is known to be less prone to overfitting than nonlinear kernels such as radial basis function ; intuitively, the rbf kernel could perform better when the data is linearly not separable or the feature and sample spaces are well balanced. pam and nb also performed fairly well with the features chosen by rf_based_fs. nb is known to be robust with irrelevant features, but the performance would be quickly degraded when correlated features are added.

CONCLUSIONS
for training and testing the models on data from same platform, all the classifiers built with features selected by rf_based_fs led to robust and accurate predictive models regardless of the data format. while data discretization step does not significantly improve the accuracy of the classifiers, it significantly reduced the number of variables in the final models. for cross-platform training and testing of the classifiers, equal-f binning outperformed fc, equal-w binning and k-means clustering. with equal-f binning, rf_based_fs identified important features more efficiently than the cv and svm-rfe when fewer gene isoforms are considered in classification. based on these encouraging results, we propose an integrative machine learning framework that involves feature selection, data discretization, and classification model build up by training and testing for independent platform . we anticipate that the application of this machine-learning framework, which includes data discretization as a key step, will provide quantitative and reproducible stratification of cancer patients with prognostic significance, the potential to improve precision therapy and the selection of drugs with subtype-specific efficacy. more importantly, the approach presented here is generally applicable to other cancer types for classification and identification of molecular subgroups.

