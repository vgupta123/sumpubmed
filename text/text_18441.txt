BACKGROUND
classification results are the ultimate judge of the success of whether a given feature or feature set is useful in the channel current-based signal analysis platform. emission inversion and the addition of a spike density feature are shown to noticeably improve performance and are folded into a previously presented architecture  <cit> . it is also shown that emission variance amplification  greatly reduces computation complexity and makes analysis of levels that are not well defined possible, while over-zealous use of tuning parameters can destroy kinetic information and thus render a channel current blockade signal useless. a new, efficient hmm-with-duration is proposed as a solution  <cit> . finally, although adaboost was not able to reproduce the best classification results obtained from a carefully selected feature set, adaboost is shown to be useful in several situations, including ab initio feature selection, and post feature selection pruning that offers similar results  to pca-based feature selection on the same data . moreover, adaboost serves to validate the current, manually designed feature set.

nanopore detector
the nanopore detector generates the data used in later stages of the channel current cheminformatics signal analysis architecture. a lipid bilayer supports the biologically-based channel. the channel used in what follows consists of a protein heptamer formed by protein monomers secreted by staphylococcus aureus. alpha-hemolysin is used as the channel in the nanopore device due to its stable conformation  and its overall geometry . the data consists of current reading through this channel. dna and rna interaction with the channel during translocation is non-negligible, but not strong enough for the molecule to get "stuck." although dsdna is too large to translocate, about ten base-pairs at one end can still be drawn into the large cis-side vestibule. this permits very sensitive experiments since the ends of "captured" dsdna molecules can be observed for extensive periods of time to resolve features, allowing highly accurate classification of the captured end of dsdna molecules  <cit> . in previous experiments, single molecules such as dna have been examined in solution with nanometer-scale precision using nanopore blockade detection  <cit> . in early studies  <cit> , it was found that complete base-pair dissociations of double stranded dna to single stranded dna could be observed for sufficiently short dna hairpins. in later work  <cit> , the nanopore detector was used to read the ends of double stranded dna molecules and was operated as a chemical biosensor. in  <cit> , the nanopore detector is used to observe the conformational kinetics of the end regions of individual dna hairpins.

cheminformatics overview
the prototype channel current cheminformatics signal processing architecture "closes the loop" on the architecture previously presented in  <cit>  . the signal processing architecture is used to perform a preliminary test of pattern recognition informed  sampling control. as the nanopore detector generates data, a simplified time-domain finite state automaton , shown in the figure in additional file  <dig>  is used for signal acquisition . the bottom part of the figure in additional file  <dig> describes the fsa that is used to find captures in a channel current data file. only transitions between states are shown. staying in the current state does not require any updating of the state of the fsa. transitioning to another state requires only the recording of that sample index if the capture state is entered or exit. note that only the current reading of the current observation and the current level count are needed to determine the state of the current observation. the current reading is used to determine the level and the current level count is used to ensure an actual level and not noise in the channel. the top part of the figure shows a sample channel current blockade signal colored to correspond with the fsa. once the signal is acquired, it is passed on to a generic hmm that is used to characterize current blockades and extract features  <cit> . during this step, the parameters of a generic-hmm are estimated using expectation maximization  to effectively de-noise the signal  <cit> . after this stage, the extracted feature vector is passed on to an off-line-trained svm. the classification result yielded by the svm is then used to close the sampling control loop, i.e., undesirable molecules, or undesirable orientations of "capture", can promptly be ejected . further details on recent results on pattern-recognition-informed sampling control are presented in  <cit> . in this paper machine learning techniques and results, primarily in feature extraction and feature selection, are presented.

hmm feature extraction
an hmm is used to de-noise and extract features from the acquired channel current signal. the hmm is implemented with fifty states. the only parameters necessary for determining a state is the current reading  at a given point in the signal. this current reading is normalized to the baseline – taken to be the average current reading just before the capture event occurred. an average baseline reading of 120pa and a current reading of 70pa, for example, corresponds to a normalized value of  <dig> % baseline. then, using a bin size of one, the value of  <dig> is used as the current state.  for most of the data studied in these experiments, almost all capture events take place between 20- and 70% blockade. thus, only fifty states are used in an effort to help ease computational complexity – as input scales linearly, computation time scales quadratically. in the implementation of the hmm, the states are chosen with this observation in mind. in the previous example, our state of  <dig> would correspond to state  <dig>  this process of scaling raw data to actual states is referred to as "quantization".

after the data is quantized, five rounds of expectation maximization are run to obtain accurate estimates of emission and transition probabilities. initially, emissions for each state 'l', corresponding to a blockade at level 'l', is set to a gaussian with mean at l and unit variance. in addition, all transitions are equally likely. expectation maximization serves to obtain a more accurate measure of emissions and transitions based on the observed signal. a standard viterbi algorithm is then run in order to de-noise the signal – that is, obtain the most likely path of states that created the observed signal. the process of finding the most likely path of states obtained by the viterbi algorithm typically reduces the noise in the channel current signal.

after the viterbi algorithm is run, a 150-component feature vector is created for the given signal. each feature vector consists of three distinct sets of information. the first  <dig> components come directly from the  <dig> previously described states of the hmm. these components are level occupation probabilities  for each state calculated after the viterbit trace back algorithm yields the most likely path. the second set of  <dig> components is composed of the variances of the emission probabilities. the third and final set of  <dig> components is composed of a weighted sum of transition probabilities from the dominant levels of a given signal.

one refinement to the standard implementation of an hmm, presented here, involves the initial manipulation of the emission probabilities as they are entered in to the hmm. the emission probabilities are the main place where the observed data is brought into the hmm-em algorithm and can be viewed conceptually as the probability of emitting a hidden or true state given an actual or observed state. by exchanging the roles of the true and actual states, an additional contribution arises that is approximately a locally distributed entropy that is introduced at the cellular level in the standard viterbi dynamic programming table . while the exact theoretical underpinnings of this method are still being researched, it is clear that this "emission inversion" improves classification performance.

in addition to the 150-component feature vectors and the emission inversion technique already described, additional information can also be extracted. the effects of the addition of a spike density feature are explored, where a spike is defined as an anomalous, deep blockade of channel current from the lower level of a given signal.

another variation on a standard hmm, emission variance amplification is discussed. here, the goal is to obtain dwell time information for the levels of a given molecule. from this information, the half-life, and thus, the stability of a given level can be determined. however, channel current data is noisy and building a finite state automaton to accurately model this noisy data can be difficult. moreover, this model would not be easily re-usable for other channel current analysis without significant restructuring and re-tuning. here, an hmm with eva is used to reduce the gaussian noise bands around a given level while still strictly retaining transitions between levels. this method was first introduced in  <cit>  and is used here to obtain the new results.

adaboost
adaptive boosting  is typically used for classification purposes. in general, adaboost is an iterative process that uses a collection of weak learners to create a strong classifier. training data is given a weight, and at each iteration, the weak learners are trained on this weighted data. weights for these data points are then updated based on the error rate of the weak learner and whether a given data point was classified correctly or not. the consensus vote at each iteration is treated as a hypothesis, and weights are given to a hypothesis based on its accuracy. at the end of the iterative process, final classification is done using all hypotheses and their corresponding weights . in this way, adaboost is able to use a set of weak learners to generate a strong classifier.

as a classification method, one of the main disadvantages of adaboost is that it is prone to over training. however, adaboost is a natural fit for feature selection. here, over training is not a problem, as adaboost finds diagnostic features and those features are passed on to a classifier that does not suffer from over training such as a svm. for this function, a modified form of adaboost is introduced.

svm classification
support vector machines  are variational-calculus based methods that are constrained to have structural risk minimization  such that they provide noise tolerant solutions for pattern recognition  <cit> . simply put, an svm determines a hyperplane that optimally separates one class from another . once learned, the hyperplane allows data to be classified according to the region in which it resides.

the svm approach encapsulates a significant amount of model-fitting information in its choice of kernel. in some sense, the svm kernel provides a notion of distance to the decision hyperplane. novel, information-theoretic, kernels were successfully employed for notably better performance over standard kernels in prior work <cit> .

thus, svms are fast, easily trained, discriminators  <cit> , for which strong discrimination is possible without the over-fitting complications common to neural net discriminators  <cit> . in these experiments, svm classification performance is used as the benchmark for testing the validity of the various feature extraction permutations that are explored. this idea is a natural fit since one of the overarching goals of the nanopore detector is to be able to classify molecules based on their behavior in the channel. furthermore, svms provide a natural confidence factor that can be leveraged when closing the sampling control loop.

RESULTS
in what follows, results are described for the proposed extensions and improvements to existing methods in the feature extraction architecture. improvements in feature extraction and types of features are discussed. specifically, emission inversion, the addition of a spike density feature, and hmm with eva are discussed. in addition, a new method of feature selection is shown. the effects of using adaboost on a full set of transition probabilities versus a scheme for manually compressing transition probabilities are shown.

emission inversion
observed data is brought into the hmm/em process chiefly through the emission probabilities. through running the hmm in debug mode and observing the interactions of various components, an interesting twist on traditional emission probabilities was found – when the observed states and emitted states share the same alphabet the roles of observed states and emitted states can be reversed for possible improvement to classification performance.

data used from these experiments were the 9bphp data shown in figure  <dig>  for each of the binary classification problems considered, three different feature sets were chosen to analyze the effect of data inversion on svm classification performance. the three sets selected for comparison were the manually designed 150-component feature vectors described in background, the first set of  <dig> level occupation features from that 150-component set, and the second set of  <dig> variances on the emission probabilities from that 150-component set. the 9at vs. 9ta, 9cg vs. 9ta, and 9gc vs. 9ta binary classification cases were selected to be shown here as they provide typical examples of the entire result set.

experimentally, this emission inversion works well with channel current data as shown in figure  <dig>  these figures show svm classification performance for the various feature sets just described using both a standard hmm implementation and a hmm implemented with data inversion as described here. the y-axis measures classification accuracy  and the x-axis shows a tuning over the kernel parameter. the symmetric entropic kernel was used in this study as it has been shown to work well with channel current data in previous experiments  <cit> . the performance benefit is shown most notably in figure 5a. in the case where the 150-component feature set was used, inverting the emissions yields a 5% peak increase in accuracy. this result is stable over a range of kernel parameter. for the case where the first  <dig> components were studied, a slight increase in classification performance as well as an increase in stability is observed. in fig. 5c, a slight boost in classification performance is observed while a significant increase in stability is observed.

in nearly all cases studied, inverting the emissions provides a performance increase in accuracy, stability, or both accuracy and stability. for some molecules, this performance increase was more significant than others and in one case, out of the ten permutations studied, performance was marginally better using a standard hmm without emission inversion .

spike analysis
in addition to the level occupation probability, emission probability, and transition probability, the spike density from the lower level of a given molecule has been identified as a possibly significant feature. a spike event – an anomalous, deep blockade of channel current – from the lower level is conceptually seen as a fraying of the last few termini of a given molecule. thus, a measure of spike density can yield information about the stability of the final few base pairings. for this analysis, data obtained from collaborators at nasa/ames was used . here, the analysis is centered on two very similar 9gc molecules. on one of the molecules, the terminal guanine base was modified in an effort to simulate radiation damage. a blockade level histogram of the two signals  shows that there is high similarity between the blockades produced by the two molecules.

the spike detection method presented in  <cit>  was used to identify spikes and extrapolate true spike counts as shown in the figures in additional files  <dig> and  <dig>  in those figures the blue curve represents actual spike counts observed versus a given cutoff. the red curve is drawn tangent to the observed curve. thus, the true spike count is the reading as the tangent line crosses the x-axis. the molecule studied is a  <dig> base-pair hairpin that is the radiation damaged dna model  , with terminal guanine unaltered in the "non-radiated" molecule. the spike count plots show increasing counts as spike cut-off thresholds are relaxed . the linear phases of spike count increase, with threshold relaxation, is associated with instances of anomalous "spike noise" and forms the basis for a heuristic for defining the spike feature. plots are automatically generated using gnuplot and automatically fit with extrapolations of their linear phases at the group's tools website. the extrapolations provide an estimate of "true" anomalous spike counts – counts associated with terminus fraying in the captured dna hairpin . the radiated form of the molecule frayed  <dig>  times on average , and is shown in additional file  <dig>  while the non-radiated molecule only frayed  <dig>  times a second, on average, while in its lower-level state .

building on the efforts in  <cit> , this spike density feature was used as a single feature and concatenated to the end of the 150-component feature vector . the results of this analysis are shown in figure  <dig> . incorporation of this spike feature for this data set leads to classification with approximately 5% greater accuracy over a wide range of tuning parameters. it is noteworthy that the addition of only one extra feature, the spike density feature, yields a significant performance increase.

dwell time analysis using emission variance amplification
another important feature of a channel current blockade signal is the duration of blockade levels. however, acquiring level duration information is a non-trivial task due to a significant gaussian noise band around blockade levels. the goal here is to use emission variance amplification in the hmm with em to drastically reduce noise in the signal while still precisely retaining level transitions. by retaining the level transitions, the integrity of the kinetic information – level dwell times in this case – remain in tact.

data used for this analysis was gathered from a simple study of dna-dna annealing using the nanopore detector and a y-aptamer transduction platform. results on blockade states observed for y-aptamer overhang+complement binding study are shown in the figures in additional files  <dig> and  <dig>  additional file  <dig> shows the 150-component feature vector profiles for the y-aptamer that binds a 6a ssdna, for signals before and after introduction of that six adenosine ssdna . additional file  <dig> shows the dwell time distributions for the three dominant levels of the y-aptamer . for further details and results, see the work presented in  <cit>  in this same journal.

visually, the results of eva can be seen in figure  <dig>  note that as the variance is amplified from the original setting of  <dig>  the noise band around a given level is reduced significantly. moreover, even though many spike events are destroyed, transitions between dominant levels – and thus level dwell times – are strongly retained. after eva pre-processing, a trivial finite state automaton can now extract dwell time information. this fsa only needs a current reading and a duration  to characterize any given level. without eva, a wide range of current cutoffs or even some more complex model would be needed to characterize a given level. but, using this simplified fsa, dwell time distributions for the studied data were easily obtained . from these dwell time distributions, the half-life – and thus a measure of level stability – can be gathered. this half-life is an important kinetic characteristic for a biologist or chemist studying the properties of a molecule. future work will evaluate whether half-lives of levels or even entire dwell time distributions can be useful in improving classification performance.

feature selection with adaboost
as has been shown in the spike analysis, careful selection of features plays a significant role in classification performance. however, adding non-characteristic or noisy features will hurt classification performance. in addition, recall from the discussion in background that the last set of  <dig> components from the baseline 150-component feature vector are compressed transition probabilities. with a  <dig> state hmm, there would be 50* <dig> or  <dig> possible transitions. however, a means of compression is necessary because many of these transitions are very unlikely and contribute noise to the feature vector. without compression, classification performance suffers as a result, yet it is uncertain as to whether diagnostic information has been inadvertently discarded in the manual compression of the transition probabilities. an automated approach is desired to solve the issue of feature selection. here, a hybrid adaboost approach is used as an automated, objective means of feature selection.

the data studied for feature selection include the 9cg vs 9gc and 9gc vs 9ta binary classification problems from the 9bphp data used in the data inversion analysis . the 9gc vs 9ta set was studied first. since the 9gc vs 9ta case is one of the easier classification problems with this dataset, the 9cg vs 9gc case was also analyzed. this case is among the hardest binary classification problems in this dataset.

figures  <dig>   <dig>   <dig> show the results of this automated feature selection analysis . figure  <dig> shows the effects of adaboosting off of the full, uncompressed feature vectors. these feature vectors are comprised of the  <dig> blockade level components , the  <dig> variances on the emission probabilities , and the full  <dig> transition probabilities. using a svm to classify all  <dig> features shows a notable decrease in classification accuracy and a significant decrease in the stability of classification results. adaboost is used to select the top  <dig> diagnostic features. these  <dig> features are extracted from the full 2600-component set of features and passed on to the svm for classification. in this case, classification outperforms both the full 2600-component set and the manually designed 150-component set. the curve denoted by "first 50" represents the first  <dig> blockade level probabilities. this set is the best performing manually designed set, and outperforms the adaboost selected feature set in both performance and stability. figure  <dig> shows the results of adaboosting off of the manually designed 150-component feature set in the case of the 9gc vs 9ta binary classification problem. there is a notable performance increase in classification accuracy and stability.

discussion
in what follows, the pros and cons of each proposed method presented in the background and results sections are discussed. in addition, proposed fixes and future work is discussed.

emission inversion
emission inversion involves exchanging of the roles of emitted states and observed states. the exact theoretical underpinning of exchanging these roles is not yet completely understood . in some sense, however, classification performance is the ultimate judge of the validity of a given method. as described in the results section, the svm classification performance is strongest when using emission inversion.

there are currently a couple of caveats. emission inversion only works when the emitted and observed states share the same alphabet – with the channel current blockade analysis platform this restriction holds. another caveat is that this method may be strongly data dependent. only channel current data has been studied using this method for feature extraction, and it is entirely possible that emission inversion does not improve classification on other datasets. in this particular application, the adaboost feature selection provides a simple fix to the choice of what features to use. simply create datasets that include extracted features from both a standard hmm implementation and a hmm implementation with emission inversion and let adaboost select the most diagnostic features in an automated way.

spike analysis
the results described above clearly show that spike density from the lower level is an important feature. obtaining the spike density feature  is straightforward. however, adding this feature to the existing 150- or 2600-component feature sets currently requires tuning. simply adding the spike density feature to an existing feature vector already containing  <dig> features will obscure the effect of the spike density feature almost completely. thus, a weight must be added to this new feature. should the weight be too heavy, though, the effect of the other features will be obscured. currently, the weighting factor is tuned over in order to arrive at a weighting such that the spike density feature improves classification without obscuring the contribution of other features.

a few automated solutions are suggested for future work. one proposed solution is to simply add the un-weighted spike density feature to the existing feature vector and use adaboost to select the most diagnostic features. this approach will essentially create a weight for the spike density feature. that is, by removing many components that only add noise to a given feature vector, the remaining features are given more weight. another solution that is currently being worked on is to fold the definition of a spike into the hmm. this solution requires a non-trivial amount of work as the entire definition of a state has to be entirely reworked. moreover, the definition of a state must be considered carefully such that a state explosion  does not occur.

dwell time analysis
preprocessing channel current blockade data using a hmm with eva significantly reduces the complexity of dwell time analysis. within a reasonable range of values for eva factor, the noise bands around levels are significantly reduced while level transitions are retained. however, if too large of an eva factor is used then transitions can be destroyed and the channel current signal will be mangled beyond use. although this problem is not significant for a wide range of eva factor, a hmm with duration  <cit>  will retain transitions and can eliminate this problem altogether.

another aspect of the dwell time analysis that will be explored in future work is the effect of dwell time information on classification. dwell time distributions for dominant levels should be characteristic for a given signal and thus improve classification performance. however, a significant amount of data is generally necessary to generate accurate dwell time distributions. in the current architecture,  <dig> ms of channel current blockade are analyzed to create one feature vector. it is unclear as to whether  <dig> ms will be enough data to overcome this limitation on sparseness of data. a longer signal trace could be analyzed, but computational complexity grows quadratically as signal input increases linearly. here, the use of a distributed hmm has been developed to allow for the processing of enough data to provide accurate dwell time statistics while still meeting reasonable time constraints , using a simple distribution analogous to the chunk processing that is employed for the svm training  <cit> .

feature selection
typically adaboost is used as a classification method. but due to the limitations discussed in the background section, svms provide a much more robust means of classification for channel current data. however, adaboost is still useful in feature selection, and that is the main use we have for adaboost in the work presented here. the weighting schemes in the adaboost algorithm are a natural fit for feature selection as the weights indicate which features are most diagnostic for a given classification problem.

adaboost does require some subtle tuning. as can be seen in the algorithm shown in figure  <dig>  adaboost does not have a natural end point. unlike an svm, adaboost does not converge on a solution. the number of iterations in the adaboosting algorithm must be tuned over in order to ensure accurate results. another tuning parameter is the number of diagnostic features "d" to select from the original feature set "o". should d be chosen too small, diagnostic features existing in o will be excluded and classification performance will suffer. should d be chosen too large, noisy features existing in o will exist in d and classification performance will suffer. in general, though, the choice of d does not present a great problem as svms are robust and can learn well in the presence of noise and non-diagnostic features. experimentally it has been observed that it is more important that d not be chosen too small as opposed to too large.

it is also important to note that automated feature selection using adaboosting was not able to reproduce results obtained from the "best-case" manually designed feature set . nonetheless, feature selection using adaboost is an important technique. it allows for the automated exploration of the effect of many different features and feature sets. in addition, adaboosted feature selection would be useful in problems where the definition of states do not lead to an easily designed manual set of features.

CONCLUSIONS
several new techniques and improvements on existing techniques in the channel current signal analysis platform have been introduced. data inversion was introduced and was shown to be an improvement over the standard implementation of a hmm in regards to channel current data and final classification performance. previous methods for spike feature extraction were folded into the current architecture. in addition, a new method for analyzing dwell times, emission variance amplification was applied to the hmm. finally, a hybrid adaboost approach was introduced in an effort to improve the feature selection process. not only are these techniques useful improvements for the current signal process architecture, but several techniques introduced here also provide means to move forward with future research as detailed in the discussion section.

