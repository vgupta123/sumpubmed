BACKGROUND
to date, most approaches to the bionlp event extraction task  <cit>  use a single model to produce their output. however, model combination techniques such as voting, stacking, and reranking have been shown to consistently produce higher performing systems by taking advantage of multiple views of the same data  <cit> . system combination essentially allows systems to regularize each other, smoothing over the artifacts of each . to the best of our knowledge, the only previous example of model combination for the bionlp shared task was performed by  <cit> . using a weighted voting scheme to combine the outputs from the top six systems, the task organizers obtained a 4% absolute f <dig> improvement over the best system used in isolation.

in this paper, we explore several model combination strategies. we aim to uncover the answers to these related questions: which strategies are effective and why? how do the outputted event structures change after performing model combination? finally, are there systematic errors which can be corrected to improve performance further?

we show that using a straightforward model combination strategy on two competitive systems  produces a new system with substantially higher accuracy. this is achieved with the framework of stacking: a stacking model uses the output of a stacked model as additional features. to put the results in perspective, we also experiment with two simpler model combination techniques where systems are run independently and their outputs are combined via union or intersection.

our base models are the umass  <cit>  and stanford  <cit>  event extractors. we initially considered combining these models using voting and reranking strategies. however, it seemed that given the performance gap between the two models, the best option was to include the predictions from the stanford system into the umass system . this has the advantage that one model  determines how to integrate the outputs of the other model  into its own structure. in the case of reranking or voting, the combined model is required to output a structure constructed from the structures produced by the input models. in other words, each portion of the resulting structure originates from at least one of the base models. with stacking, one model determines how to integrate the outputs of the other model and the resulting structure can contain novel constructions. however, as it turns out, these novel constructions have low precision in our case. this can be understood using the same intuition that underlies the voting or union strategies - if a structure has been produced by multiple independent models, it is more likely to be correct. novel events resulting from stacking have essentially been produced by neither base model and thus tend to be inaccurate. we show that by removing these novel events from our output, our state-of-the-art results can be improved further.

the bionlp shared task
the bionlp shared task involves extracting a set of biomolecular events from natural language text in a given document . by biomolecular events, we mean a change of state of one or more biomolecules. more concretely, let us consider part  of figure  <dig>  we see a snippet of text from a biomedical abstract and the three events that can be extracted from it. we will use these to characterize the types of events we ought to extract, as defined by the bionlp  <dig> and  <dig> shared tasks. note that for the shared task, entity mentions  are given by the task organizers and hence do not need to be extracted.

the event e <dig> in the figure refers to a phosphorylation of the traf <dig> protein. it is an instance of a set of simple events that describe changes to a single gene or gene product. other members of this set are: gene expression, transcription, localization, and catabolism. each of these events has to have exactly one theme, the protein whose state change is described. a labeled edge in figure 1a shows that traf <dig> is the theme of e <dig> 

event e <dig> is a binding of traf <dig> and cd <dig>  binding events are special in that they may have more than one theme, as there can be several biomolecules associated in a binding structure. this is in fact the case for e <dig> 

in the top-center of figure 1a we see the regulation event e <dig>  such events describe regulatory or causal relations between events. other instances of this type of events are: positive regulation and negative regulation. regulations must have exactly one theme; this theme can be a protein or, as in our case, another event. regulations may also have zero or one cause arguments that denote events or proteins which trigger the regulation.

in the bionlp shared task, we are also asked to find anchor  tokens for each event. these tokens ground the event in text and allow users to quickly validate extracted events. for example, the anchor for event e <dig>  is "inhibit," as indicated by a dashed line.

instead of directly working with the event representation in figure 1a, both the umass and stanford systems extract labeled graphs in the form shown in figure 1b. the vertices of this graph are the anchor and protein tokens. a labeled edge from an anchor e to a protein token p with role label r indicates that there is an event with anchor e for which the protein p plays the role r. an edge with role r from anchor e to anchor e' means that there is an event at e' that plays the role r for an event at e. this representation is used by the umass system to define extraction as a compact optimization problem. a related representation is used by the stanford system to tackle extraction as dependency parsing . if a graph can be drawn on a plane without crossing edges, we say that the graph is projective . figure  <dig> shows examples of projective graphs while figure  <dig> contains an example of a non-projective graph. we define the non-projectivity of a graph as the number of crossing edges in it. for more details about mapping back and forth between events and labeled graphs, we point the reader to  <cit> .

the bionlp  <dig> shared task  <cit>  consists of a single domain, genia  while the bionlp  <dig> shared task  <cit>  expands the genia domain and adds two additional domains, epigenetics and post-translational modifications  and infectious diseases  . our experiments in this paper are over the  <dig> shared task corpora.

model combination approaches
our primary approach consists of a stacking model that uses the predictions of a stacked model as features. in the following sections, we briefly present both the stacking and the stacked model and some possible ways of integrating the stacked information. we also describe two simpler model combination techniques  for comparison.

stacking model
as our stacking model, we employ the umass extractor  <cit> . it is based on a discriminatively trained model that jointly predicts anchor labels, event arguments and protein pairs in bindings. we will briefly describe this model but first introduce three types of binary variables that will represent events in a given sentence. variables ei,t are active if and only if the token at position i has the label t. variables ai,j,r are active if and only if there is an event with anchor i that has an argument with role r grounded at token j.

in the case of an entity mention, this means that the mention's head is j. in the case of an event, j is the position of its anchor. finally, variables bp,q indicate whether or not two entity mentions at p and q appear as arguments in the same binding event.

two parts form our model: a scoring function, and a set of constraints. the scoring function over the anchor variables e, argument variables a and binding pair variables b is

 sdef¯¯∑ei,t=1st+∑ai,j,r=1sr+∑bp,q=1sb 

with local scoring functions stdef¯¯〈wt,ft〉, srdef¯¯〈wr,fr〉 and sbdef¯¯〈wb,fb〉.

our model scores all parts of the structure in isolation. it is a joint model due to the nature of the constraints we enforce: first, we require that each active event anchor must have at least one theme argument; second, only regulation events  are allowed to have cause arguments; third, any anchor that is itself an argument of another event has to be labeled active, too; finally, if we decide that two entities p and q are part of the same binding , there needs to be a binding event at some anchor i that has p and q as arguments. we will denote the set of structures  that satisfy these constraints as .

stacking with this model is simple: we only need to augment the local feature functions ft , fr  and fb  to include predictions from the systems to be stacked. for example, for every system s to be stacked and every pair of event types  we add the features

 fs,t′,ts=1hs=ts∧t′=t0otherwise 

to ft . here hs  is the event label given to token i according to s. these features allow different weights to be given to each possible combination of type t' that we want to assign, and type ts that s predicts.

inference in this model amounts to maximizing s  over . our approach to solving this problem is dual decomposition  <cit> . this technique exploits the fact that while inference in the full problem may be intractable, it usually contains tractable subproblems for which efficient optimization algorithms exist. in dual decomposition, these algorithms are combined in a message passing scheme that often finds the global optimum of the full model. when a global optimum is found, dual decomposition also provides guarantees that prove the optimality of this solution.

for our event extraction model we divide the argmax problem into three subproblems:  finding the best anchor label and set of outgoing edges for each candidate anchor;  finding the best anchor label and set of incoming edges for each candidate anchor; and  finding the best pairs of entities to appear in the same binding. for all of these problems, efficient algorithms can be derived  <cit> .

for learning the parameters w of this model, we employ the online-learner mira  <cit> . mira iterates over the training data and compares the gold solution with the current best solution according to w. if both solutions disagree, w is adapted such that the gold solution would win with sufficient margin if the problem was to be solved again. we refer the reader to  <cit>  for further details on both inference and learning.

stacked model
for the stacked model, we use a system based on an event parsing framework  <cit>  referred to as the stanford model in this paper. a high level description of the system relevant to the experiments in this paper follows. to train the stanford model, first event structures are projected to dependency trees in a process similar to that in figure 1b. these dependency trees are tree-rooted dependency graphs where nodes are event anchors or entities and the labeled, directed edges are relations, e.g., theme and cause. this projection eliminates some of the more complex aspects of event structures which cannot be captured easily in dependency trees, primarily events or entities with multiple parents. words that do not take part in any events are removed in the dependency trees and multiword anchors of events are replaced with their syntactic heads. an example of this conversion can be seen in figure  <dig> 

after conversion, the dependency trees are parsed using an extension of mstparser  <cit>  which includes event parsing-specific features. to parse, mstparser creates a complete graph with entities and event anchors as nodes. for each edge in the complete graph, mstparser assigns a score using the features along that edge and the feature weights learned from training. at this point, the highest scoring parse  can be decoded using several possible algorithms. for example, the algorithm that gives mstparser its name is the maximum-spanning tree algorithm which searches for a tree that spans all nodes in the graph and obtains the highest sum of edge scores. once parsed, the resulting dependency tree is converted back to event structures. training mstparser involves learning feature weights which separate correct edges from incorrect edges during parsing.

of particular interest to this paper are the four possible decoders in mstparser since they result in four different models. these decoders come from combinations of feature order  and whether the resulting dependency graph is required to be projective. first-order features are features taken from a single edge  while second-order features include features over two adjacent siblings along with their parent. non-projective decoders would seem to be useful for this task. in genia,  <dig> % of the documents contain at least one non-projective arc . this portion of the data can only be captured by non-projective decoders.

for brevity, the second-order non-projective decoder is abbreviated as '2n', first-order projective decoder as '1p,' etc. when referring to stanford models, we always specify its decoder. each decoder presents a slightly different view of the data and thus has different model combination properties. projectivity constraints are not captured in the umass model so these decoders incorporate novel information. drawing on techniques from statistical constituency parsing  <cit> , we employ a reranking framework to further improve performance and capture global features of event structures. the existing features are restricted to functions of a single edge in the first-order model and two adjacent siblings in the second-order model. however, some phenomena of event structures span larger structures . to switch to a reranking framework, we extend the decoders to n-best decoders which return the n highest scoring parses for each sentence  rather than just the single highest scoring parse. note that our non-projective decoders have only approximate n-best decoders  resulting in suboptimal reranker models in some cases. the reranker rescores each parse in the n-best list and returns the highest scoring parse. these scores are based on features of the global event parsing structure as well as including metadata about the parse . the reranker can be also used for model combination when given the output from multiple n-best lists. in this case, unique parses are merged and the original number of decoders producing the parse and the scores from the decoders are added to the parse's metadata. while the primary focus of this paper is on using stacking for model combination, a small number of experiments study the performance of using the reranker for model combination.

using the stanford model as a stacked model
the projective stanford models are helpful in a stacking framework since they capture projectivity which is not directly modeled in the umass model. of course, this is also a limitation since actual bionlp event graphs are dags, but the stanford models perform well considering these restrictions. additionally, this constraint forces the stanford model to provide different  results.

to produce stacking output from the stanford system, we need its predictions on the training, development and test sets. for predictions on the test and development sets, we used models learned from the complete training set. predictions over training data were produced using cross-validation. obtaining predictions in this way helps to avoid scenarios in which the stacking model learns to rely on high accuracy at training time that cannot be matched at test time.

we used  <dig> cross-validation training folds for ge,  <dig> for epi, and  <dig> for id. to produce predictions over the test data, we combined the training folds with  <dig> development folds for ge,  <dig> for epi, and  <dig> for id.

note that, unlike stanford's individual submission in the bionlp  <dig> shared task  <cit> , the stacked models in this paper do not use the reranker. this is because it would have required making a separate reranker model for each cross-validation fold.

training the stacking model took about two hours on a  <dig> core machine. the stacked model needed about three hours on a single core machine for each fold. since the stacking model and each fold of the stacked model can be trained in parallel, the overall training time is about five hours if sufficient cores are available.

intersection and union
we investigate two baseline techniques for model combination: intersection and union. both of these are similar to their standard set theory operations except that instead of using strict equality for events, we allow events to be equal if they match according to the bionlp approximate recursive scoring metric.

RESULTS
results on test sets of all tasks we submitted to, for three models. we list recall, precision, and f <dig> using the standard bionlp approximate recursive metric. for the ge and id datasets, the stanford model used all four decoders with the reranker. for epi, the stanford model used only the 1n decoder with the reranker. in all three domains, the stacked umass←stanford model  used all four decoders from the stanford model as inputs. the "faust " is created by removing all events which don't occur in either the umass or stanford models .

we compare the umass and stanford standalone systems to the umass←stanford model . this model uses the four stanford predictions  as stacking inputs to the umass model. for all four tasks we observe substantial improvements due to stacking. the increase is particularly striking for the epi track, where stacking improves f <dig> over the umass model by more than  <dig>  points on the core metric.

to analyze the impact of stacking further, table  <dig> shows a breakdown of our results on the genia development set. presented are f <dig> scores for simple events, binding events, regulation events, and overall performance. we compare the standalone umass system, various standalone stanford models and stacked versions of these . the last line, "umass←stanford  without novel events," will be covered later in the discussion section.

bionlp f <dig> scores on the development section of the genia track  for several event categories and overall f <dig>  the best scores from each column are bolded.

remarkably, while there is a  <dig> point gap between the best individual non-reranked stanford system  and the standalone umass systems, integrating the stanford 1n prediction still leads to an f <dig> improvement of  <dig> point. this can be seen when comparing the umass, stanford , and faust results. we also note that stacking the projective stanford  and stanford  systems helps almost as much as stacking all four stanford systems combined. notably, both stanford  and stanford  do not do as well in isolation when compared to the stanford  system. when stacked, however, they do slightly better. this suggests that projectivity is a missing aspect in the umass system. this is explored in detail in the discussion section.

the "umass←stanford " and "umass←stanford " lines represent experiments to determine whether it is useful to incorporate only portions of the information from the stanford system. umass←stanford  adds only the anchor predictions from the four stanford models and does not significantly improve over the standalone umass system. umass←stanford  includes only links between event anchors and their arguments and achieves a small improvement over the standalone umass system but with scores significantly lower than when all information is stacked. given the small gains for both of these over the original umass system, it is clear that stacking information is most useful when attached to both anchors and arguments. our theory is that most of our gains come from when the umass and stanford systems disagree on anchors and the stanford system provides not only its anchors but also their attached arguments to the umass system. otherwise, the umass system has little incentive to use the newly proposed anchors. this is supported by a pilot experiment where we trained the stanford model to use the umass anchors and saw no benefit from stacking .

we provide a breakdown of our results on the epigenetics track in table  <dig>  this time in terms of recall, precision, and f <dig>  as before, there is a substantial boost in performance from stacking. the umass model obtains an f <dig> of  <dig> % on its own and jumps to  <dig> % when given all four stanford decoders as stacking input. for this dataset, the non-reranked stanford decoders are not as far behind the umass system, with the 1n decoder only 1% lower than the umass system. when combined with a reranker, the stanford model  even outperforms the umass model. however, it is significantly more difficult to use the output of the reranked stanford model as stacking input. stacking would require nested cross-validation to produce outputs from the reranker over the training set. this is because training the reranker itself requires running cross-validation over the training set. consequently, we were unable to perform experiments stacking the umass model with the outputs from the reranked stanford models. the "stanford " line shows an experiment where only the 1n and 2p decoders are used as input to the reranker. the hope was that the best single model  would be improved by adding the most complementary decoder  but the net result does not outperform the 1n model.

bionlp f <dig> scores on the development set of epi using the core metric.

results on the development set for the id track.

another possible approach to stacking conjoins all the original features of the stacking model with the predicted features of the stacked model. the hope is that this allows the learner to give different weights to the stacked predictions in different contexts. however, incorporating stanford predictions by conjoining them with all features of the standalone umass system  in table 4) does not help here. it appears that the increased data sparsity from the larger number of features ends up hurting performance.

in table  <dig>  we show a number of different model combination techniques for umass and stanford models. the top table summarizes relevant results on genia from previous tables. the second table shows the results of using the baseline model combination techniques to combine stanford results with the standalone umass model. for each stanford model, we show its independent performance as well as the effect of intersecting or unioning it with the output from the umass model. none of these methods significantly improve performance above umass's standalone performance on this dataset . unsurprisingly, the intersected results have significantly higher precision while the unioned results have significantly higher recall.

stacking and reranking outperform the intersection and union model combination baselines. the first section of the table summarizes the results from the umass and stacked models. the second section gives the performance of each stanford model alone and when combined with the pure umass model via the intersection and union methods. in the last section, we evaluate the intersection and union baselines using only the four stanford models as inputs. the "stanford " line represents using all four individual decoders without model combination . in "stanford ", the reranker was used to combine the four decoders into a single output before being intersected or unioned. all results are on the development set for the genia track.

one additional form of model combination is shown in the "stanford " line in table  <dig>  bottom table. recall that the reranker itself can be used to combine the outputs of multiple models. allowing the reranker to choose the best events from the n-best lists from all four decoders yields an f <dig> of  <dig> %. combining the four decoders instead via unioning  yields an f <dig> of  <dig> % whereas combining them using the intersection baseline gives an f <dig> of  <dig> %. thus, reranking also improves over the union and intersection baselines. creating a direct comparison of stacking versus reranking for performing model combination is left as future work.

we observed that the id and ge corpora were similar in their annotations. this allowed us to apply techniques from domain adaptation. our hypothesis was that it might be possible to augment the training data of the smaller id corpus with the training and development data from the larger ge corpus. merging both training sets is reasonable since there is a significant overlap between both in terms of events as well as lexical and syntactic patterns to express these. when building our training set we add each training document from ge once, and each id training document multiple times--this lead to substantially better results than including id data only once. for the umass system, two copies of id were used whereas the optimal performance for the stanford system came from using three copies. experiments on the stanford parser with the 2n decoder can be seen in table  <dig>  while performance initially dips after adding ge data, once enough id data has been added to adjust the distribution, the overall f <dig> improves. with three copies of ge, gains occur primarily in recall  since the additional data helps to recognize additional patterns. precision drops from  <dig> % to  <dig> % since the data from ge is not exactly from the same domain.

impact of merging several copies of id training set with ge training and development sets for the stanford model. f <dig> scores are on id development data . best performance is achieved when using three to five copies of id to allow ge data to be used without overwhelming the id data.

discussion
generally, stacking has led to substantial improvements across the board. there are, however, some exceptions. one is binding events for the ge task. here the umass and several reranked stanford models still outperform the best stacked system . likewise, for full papers in the genia test set, the umass model still does slightly better, with  <dig> % f <dig> compared to  <dig> % f <dig>  this suggests that a more informed combination of our systems  could lead to better performance. for example, a naïve implementation could be to simply avoid stacking for binding events, and within full papers. in order to better understand where the improvements from stacking were coming from, we performed several forms of error analysis. the first examines to what extent projectivity is a factor. the second traces where events proposed by the stacked models originate and explores the novel events generated as a result of stacking.

non-projectivity analysis
one hypothesis is that projectivity plays a large role in where improvements from stacking occur  <cit> . this is because the umass model does not include projectivity constraints while the projective decoders from the stanford model  do. since umass←stanford  and umass←stanford  perform almost as well as the stacked model with all four decoders , projectivity does appear to be a factor at first. in figure  <dig>  we show how well models create event structures with the correct amount of non-projectivity. despite allowing for non-projective structures, the stanford non-projective  generally produce completely projective event structures. the projective stanford decoders  naturally predict nearly projective outputs . the umass and faust systems predict non-projective structures much more frequently, though rarely with the correct amount of non-projectivity. between the umass and faust systems, there doesn't appear to be a large difference in the predicted projectivity. thus, there is little evidence that the output from the stanford models greatly influences the projectivity of the faust model via stacking.

however, in figure  <dig>  we show that while the predicted amounts of projectivity do not change much between the umass and faust systems, we generally see larger improvements in documents with non-projective structures. these boxplots provide the distributions of differences in accuracy  between two models. each chart includes a boxplot for the distributions of these differences both for projective documents  and non-projective documents. each boxplot shows the median value , 25th and 75th percentiles , and  <dig>  times the interquartile range . for example, the upper left chart shows that the differences between umass and the stanford  models on non-projective documents tend to be positive--that is, the umass system generally performs better on these documents with a median difference of  <dig> event. in these boxplots, the four stanford decoders perform similarly to each other with respect to umass. despite this, when comparing the 1n and 1p decoders , we see that there is a small improvement on non-projective documents. performance on projective documents is nearly identical in this case modulo outliers. a similar trend can be seen in the comparison between the faust and umass systems. putting these two figures together, a larger story can be seen. none of the stanford models predict high numbers of non-projective structures while the umass system occasionally overpredicts non-projectivity. when stacked with stanford decoders, it receives a soft constraint to produce more projective structures.

despite this, the faust system behaves similarly to the umass system in terms of its predicted projectivity. thus, while most of the improvements from stacking occur on non-projective sentences, their non-projectivity is mostly unaffected and the improved performance must come from other factors in the structure. that is, the event structures predicted are more accurate but their overall projectivity is not significantly changed.

event origin analysis
next, we investigate the origins of events proposed by the stacked models. specifically, we aim to answer which base models originally proposed each event  and how many events were novel in the stacked model's output. to perform this analysis, we collect the outputs from the stanford , umass, and faust models. in the following, we will treat each model as the set of its predicted events. we place each event e ∈ faust in one of four classes:

 only 2pe∈faust ∩ only umasse∈faust ∩ bothe∈faust ∩ novele∈faust −  

we allow an event to be contained by one of these classes  if it matches one of the events in the set according to the bionlp non-approximate recursive scoring rules  <cit> . for each event type, we show the number of events in each class and the frequency of events that were correct . we can now see that the majority of events are not novel and most originate from the intersection of the stanford  and umass models. furthermore, the novel events are frequently incorrect. there are several compatible explanations. one is that the majority of novel events are in the binding and regulation categories which tend to be more complex and thus harder to predict. additionally, novel events by definition are ones that haven't been proposed by either model and thus haven't been "vetted" as closely. contrast this with the high precision of events that come from both base models. since novel events proved to be unreliable, we experimented with removing them from the output of the stacked model and reevaluated. the net effect is a  <dig> % increase in bionlp f <dig> to  <dig> % for the genia development section . on the genia test section, this results in a  <dig> % improvement over the state-of-the-art result from the faust system . as expected, the gains come from a 5% boost in precision and a 2% drop in recall. in figure  <dig>  one can also see that the largest single band consists of gene expressions proposed by both the umass and stanford  models. in general, gene expressions have very high precision and are easy to recognize. this leads to a large intersection between the models' proposed events.

CONCLUSIONS
we have exploring different methods of model combination for biomolecular event extraction. the leading technique in our experiments, stacking, was used by the faust entry to the bionlp  <dig> shared task. by using the predictions of the stanford models as features of the umass model, we substantially improved upon both systems in isolation. this helped us to rank first in three of the four tasks we submitted results to. remarkably, in some cases we observed improvements despite a 5% f <dig> margin between the models being combined.

stacking and reranking outperform the union and intersection baselines as model combination techniques. by allowing the better performing base model  to flexibly determine how to incorporate new information from the other base models, the model combination can be done in a more informed and finer-grained fashion.

our analysis has shed light on where the improvements from stacking originate. while stacking does not improve the model's ability to predict the correct levels of projectivity, it does primarily improve the performance on non-projective documents. additionally, while stacking can generate novel events, these turn out to be low in precision and ultimately harmful to overall performance.

there are many possible avenues to pursue in the future. while this paper explored stacking the four related stanford models, using a broader set of base models would certainly improve performance with minimal effort . additionally, further attention could be placed on the specific features for stacking. in this study, we explored two feature templates for stacking  but there is likely a middle ground to allow the stacking model to incorporate the predictions from the stacked model more finely. finally, it is worth investigating incorporating novel components into the umass dual decomposition framework, e.g., the maximum-spanning tree component from the stanford model.

competing interests
the authors declare that they have no competing interests.

authors' contributions
dm and sr conceived and designed the project with help of ms, cm, and am. sr implemented the original umass model. dm and ms implemented the original stanford model. sr and dm performed the model combination experiments and initial analysis. dm performed the non-projectivity and event origin analyses with help from sr and cm. dm and sr drafted the manuscript with help from cm. cm and am helped direct the experiments. all authors read and approved the final manuscript.

tables
throughout these tables, the notation l←r denotes that predictions from model r were used as stacking input to model l. note that a previous paper  <cit>  contained incorrect f <dig> scores from the stanford system. these have been corrected here.

