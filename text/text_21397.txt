BACKGROUND
a cell constantly responds to its changing environment and intracellular homeostasis. this is achieved by a signal transduction system that detects the signals, assimilates the information of diverse signals, and finally transmits its own signals to orchestra cellular responses. many of such cellular responses involve tightly regulated transcriptomic activities, which can be measured by microarray or rna-seq technology and used as readouts reflecting the state of the cellular signaling system.

reverse engineering the signaling system controlling gene expression has been a focus area of bioinformatics and systems biology. however, this task is significantly hindered by the following difficulties: 1) a transcriptomic profile of a cell  at a given time represents a convolution of all active signaling pathways regulating transcription in the cells, and 2) the states of the majority of these signaling pathway are not observed, making it a challenging task to infer which genes are regulated by a common signal pathway, and it is even more challenging to reveal the relationships among signaling pathways.

different latent variable models, such as principle component analysis  <cit> , independent component analysis  <cit> , bayesian vector quantizer model  <cit> , network component analysis , and non-negative matrix factorization  <cit>  models have been applied to analyze transcriptomic data, with an aim to represent the states of latent pathways using latent variables. despite the different strengths and limitations of these models, they share a common drawback: the latent variables in these models are assumed to be independent, i.e., the latent variables are organized in single “flat” layer without any connection among them; as such the models lack the capability of representing the hierarchical organization of cellular signaling system.

figure  <dig> illustrates the task of reverse-engineering a transcriptomic regulation system. figure 1a illustrates the well-appreciated hierarchical organization of signaling molecules in cells and how the information encoded by signaling molecules are compositionally organized. it also shows that the convoluted signals eventually are emitted as changed gene expression. at this stage, all the hierarchical information of the signaling system is embedded in the data, a vector of gene expression value, in the form of context-specific and compositional covariance structures. when given a collection of transcriptomic profiles collected under different cellular conditions , the ultimate task is to recover the structure of the signaling systems shown in fig. 1a, but the goal remains unattainable with current methodologies. in this study, we hypothesize that the hierarchical organization of cellular signals can be partially reconstructed by models capable of discovering and representing the context-specific and compositional covariance structure embedded in transcriptomic data. to this end, recent development in deep hierarchical models, commonly referred to as “deep learning” models, e.g., the autoencoder  shown in fig. 1c, afford us the tools to reverse engineer the signaling systems of cells by mining systematic perturbation data.fig.  <dig> an overview of studying molecular signaling transduction using an autoencoder. a an example of molecular signaling transduction. b an example of the heatmap of gene expression microarrays. c an autoencoder model consisting of hierarchically organized hidden variables. after the model was trained, we evaluated the information learnt from the autoencoder model by testing whether the information carried by hidden variables in the autoencoder has real biological entities



in this family of deep hierarchical models, multiple layers of hidden  variables are organized as a hierarchy, which can be used to capture the compositional relationships embedded in the transcriptomic data in a distributed fashion, i.e., different layers can capture different degrees of detail. for example, the relationships between tfs and their target genes can be captured by a hidden variable layer  immediately above the observed the layer of observed gene expression variables, whereas the function of pathways regulating tfs can be represented by higher hidden layers. therefore, deep hierarchical models provide an abstract representation of the statistical structure of the transcriptomic data with flexibility and different degrees of granularity. we hypothesize that, if accurately trained, a deep hierarchical model can potentially represent the information of real biological entities and further reveal the relationships among them.

in this study, we designed and trained a sparse deep autoencoder model to learn how the information is encoded in yeast cells when subjected to diverse perturbations. our results indicate that deep learning models can reveal biologically sensible information, thus learning a better representation of the transcriptomic machinery of yeast, and we believe that the approach is applicable to more complex organisms.

methods
in this study, we investigated using the autoencoder model  <cit>  and sparse autoencoder model  <cit>  to represent the encoding system of the signal transduction systems of yeast cells. before introducing the autoencoder model and sparse autoencoder model, we will first briefly review restricted boltzmann machines  as building blocks for the autoencoder.

restricted boltzmann machines 
a rbm is an undirected probabilistic graphical model that consists of two layers of stochastic binary variables : a visible layer v ∈ { <dig>  1}d and a hidden layer h ∈ { <dig>  1}f. the energy function e of the state {v, h} of the rbm is: evhθ=−a⊺v−b⊺h−v⊺wh=−∑i=1daivi−∑j=1fbjhj−∑i=1d∑j=1fvihjwij 

in this equation, the binary state of visible variable i is represented by vi, the binary state of hidden variable j is by hj and the model parameters are θ = {a, b, w}. the bias for visible variable i is ai the bias for hidden variable j is bj and the weight between visible variable i and hidden variable j is wij.

the joint distribution of the hidden and visible variables is defined using a boltzmann distribution, and the conditional probability of the states of hidden variables and visible variables are as follows: prvhθ=1zθexp−evhθzθ=∑v,hexp−evhθprhj=1|v=σbj+∑i=1mwijviprvi=1|h=σai+∑j=1nwijhj  where σ is the logistic function 1/), m is the total number of visible variables and n is the total number of hidden variables.

the efficient algorithm for learning parameters of the rbm model was introduced in detail in literature and our previous work  <cit> .

autoencoder
unlike a rbm, which captures the statistical structure of data using a single layer of hidden nodes, an autoencoder uses multiple layers in a distributed manner, such that each layer captures the structure of different degrees of abstraction. as shown in fig. 1c, an autoencoder contains one visible  layer and one or more hidden layers. to efficiently train the autoencoder, we treat it as a series of two-layered restricted boltzmann machines  stacked on top of each other  <cit> . the inference of the hidden node states and learning of model parameters are performed by learning the rbm stacks bottom-up, which is followed by a global optimization of generative parameters using the back-propagation algorithm. more details of the algorithm and pseudo code for training an autoencoder were discussed in both literature and our previous work  <cit> .

sparse autoencoder
in a conventional rbm model, each hidden unit is fully connected to the observed variables. after training, there is usually a non-zero weight between each pair of visible and hidden nodes. based on the assumption that the change in gene expression due to a specific perturbation—most microarrays in this study are experiment-vs-control—is likely meditated by a small number of tfs or pathways, we adopted the sparse autoencoder model  <cit>  to simulate the cellular response to perturbations. the sparse autoencoder model enables one to specify that only a certain percent of hidden nodes have a high probability to be set to  <dig>  by adding a penalization term to the optimization function. optimization of the traditional rbm is performed by minimizing the negative log-likelihood of the data during rbm training within an autoencoder: minimizeθ−∑1=1slog∑j=1nprv <dig> hj1θ 

where s is the total number of samples, n is the total number of hidden units and θ = {a, b, w}. the sparse rbm adds the regularization term  <cit>  into the optimization: minimizeθ−∑l=1slog∑j=1nprvl,hjl|θ+λ∑j=1n|p−1s∑l=1sehjl|vl| <dig> 

where λ is the regularization constant and p is a constant  controlling the sparseness of the hidden units hj. for the traditional rbm, the parameters are updated just based on the gradient of the log-likelihood term. but for the sparse rbm, the parameters are updated not only based on the gradient of the log-likelihood term but also the gradient of the regularization term.

non-negative matrix factorization
non-negative matrix factorization  has been applied to reduce the dimension of expression data from thousands of genes to a handful of hidden representations   <cit> . nmf is an algorithm based on decomposition by parts that can reduce the dimension of a matrix v  <cit> . v=w∗h 

given that the gene expression data is represented as matrix v, nmf factorizes it into a basis matrix  and a coefficient matrix . all three matrices should have non-negative elements. the number of hidden regulators is pre-defined, and is usually much smaller then the number of genes. in this study, we used the matlab function nonnegative matrix factorization “nnmf” to perform nmf analysis.

model selection of autoencoder and sparse autoencoder
we performed a series of cross-validation experiments to search for an “optimal” structure for autoencoders and sparse autoencoders. we adopted a four-layered autoencoder to represent the hierarchical structure of biological processes shown in fig. 1c. we then explored models with different numbers of hidden units in each hidden layer. we set the initial structure of both autoencoder and sparse autoencoder to the following ranges: h: 100–428; h: 50–100; h: 50; and h:  <dig>  we iteratively modified the structure of the model by changing the number of hidden nodes within a layer using a step size of  <dig> for the first and second hidden layer. then we explored all combinations in the range stated above. in this case, the total number of models tested is  <dig>  for both autoencoder and sparse autoencoder. for the sparse autoencoder, we chose three sparsity constants that are  <dig> ,  <dig>  and  <dig> . under each particular setting, we performed ten fold cross-validation to assess the performance of a model.

we used two criteria of evaluating the performance of the models. one is the reconstruction error, which is the difference between the original input data and the reconstructed data after training the model  <cit> . due to the sparse features of the sparse autoencoder, we used bayesian information criterion   <cit>  as another criteria for comparing models. bic combines the factors of likelihood and number of free parameters to be estimated. the model with the lowest bic is preferred. bic=−2⋅lnl^+k⋅lnnl^=∏i=1npm1−p1−m where l^ is the maximized value of the likelihood function of the model, k is the number of free parameters to be estimated, n is the number of samples, p is the probability predicted from the model for a gene to be active in an experiment, and m is the true binary state of a unit in the input data.

mapping between the hidden units and known biological components
based on the weights between each hidden unit in the first hidden layer and all the visible units , we used a threshold  to cut the edges between a hidden node and the observed genes, such that an edge indicates that the hidden node regulates the gene. we then identified all genes predicted to be regulated by a hidden node as a gene set. based on the dna-protein interaction table  <cit> , we also identified the gene set regulated by a known tf. we then assessed the significance of overlapping of gene sets regulated by hidden nodes and tfs using hypergeometric testing.

consensus clustering of experiment samples
consensus cluster clustering  <cit>  was used to cluster the experiment samples using different datasets as input. the r implementation of clustercons  <cit>  was downloaded from cran . the inputs for consensus clustering are the samples represented using original gene expression values, nmf megagenes values and the states of hidden variables under all experiment samples respectively. the partition around medoids  and k-means algorithms were used as base clustering algorithms. the inputs for cluster by cluster consensus clustering are the samples represented using samples clusters derived from the nodes from different hidden layers as features. if one sample belongs to a sample cluster, its input value is  <dig>  otherwise, its input value is  <dig> 

finding pheromone related hidden units
we calculated the significance between the state of a hidden node and the state of proteins related to pheromone signaling pathway by using the chi-square test. first, we used a threshold  to designate the state of a hidden unit as active or inactive based on its activation probability. then, for each hidden unit, we created a contingency table to collect the counts of the joint state of the hidden node and whether any member of the pheromone pathway is perturbed in a specific experiment. we used the contingency table to perform the chi-square test. we used a p-value of  <dig>  as the significance threshold.

gene ontology analysis
go  <cit>  provides a standard description of what gene products do and where they are located. one of the frequently used databases that provide go information for yeast is saccharomyces genome database sgd. we first used the combination of weights  <cit>  between neighboring hidden layers to get the weights between the hidden units in a particular hidden layer and the genes. a gene is regarded as being regulated by a hidden unit if their weight is in the top 15 % of all weights. when a gene set of interest associated with a hidden unit is available, we used the method mentioned in  <cit>  to summarize the go terms capturing as much as semantic information associated with those genes  <cit> . we identified the go terms that could summarize the largest number of genes, while undergoing a minimal information loss.

RESULTS
training different models for representing yeast transcriptomic machinery
we collected a compendium of  <dig> yeast cdna microarrays from the princeton university microarray database , and we combined them with  <dig> microarrays from the study by hughes et al.  <cit> , which was used in a previous study of the yeast signaling system  <cit> . the combined dataset is ideal for studying the yeast signaling system because it represents a large collection of perturbation experiments that are of biological interest. for example, the data from the study by hughes et al.  <cit>  were collected from yeast cells with genetic perturbations  or chemical treatments, and similarly the microarrays from the database were collected from specific conditions and contrasted with “normal” growth condition. taking advantage of the experiment-vs-control design of cdna microarrays, we identified differentially expressed genes  in each array and retained  <dig> genes that were changed in at least 5 % of the microarrays. we then represented the results of each microarray experiment as a binary vector, in which an element represented a gene, and its value was set to  <dig> if the gene was differentially expressed, and  <dig> otherwise. thus, each microarray represented the transcriptomic changes in response to a certain condition, presumably regulated by certain specific signaling components, which is unknown to us.

we investigated the utility of the autoencoder model   <cit> , with one observed layer representing the microarray results and  <dig> hidden variable layers  representing the yeast signaling components in yeast transcriptomic machinery. in this model, a hidden node is a binary variable, which may reflect the state of a collection of signaling molecules or a pathway, such that the switching of the node state between  <dig> and  <dig> can reflect the changing state of a pathway.

the probabilistic distribution of the state of a node in a given layer is dependent on the nodes in the adjacent parent layers, defined by a logistic function. the directed edges between nodes of adjacent layers indicate that, conditioning on the state of nodes in parent layer, the nodes in a child layer are independent. in other words, the statistical structure  among the nodes in a child layer is captured by the nodes in the parent layer. for example, in our case, if the nodes in the 1st hidden layer  represent the states of transcription factors, then co-differential expression  of a set of genes is solely dependent on  the tfs that regulate the genes. similarly, the co-regulation of tfs is determined by its parent layer, which may reflect the state of signaling pathways. thus, this model is suited to capture the context-specific changes and compositional relationship among signaling components in a distributed manner. the model is referred to as autoencoder because, when given a collection of observed data, it learns to use hidden nodes to encode the statistical structure of observed data, and it is capable of probabilistically reconstructing the observed data.

since the autoencoder model in our study is biologically motivated, we hypothesize that the nodes in the first hidden layer would likely capture the signal of tfs. thus the number of nodes in this layer should be close to the number of known tfs for yeast, of which there are around  <dig> well-characterized yeast tfs  <cit> . however, for a given microarray from a perturbation experiment, genes that respond to a specific perturbation are likely regulated by a few transcription factors. thus we also investigated a model referred to as sparse autoencoder  <cit> , which performs regularized learning of model parameters and allows a user to constrain the percent of nodes in a layer that can be set to the “on” state, see methods for details. in our experiment, we constrained that, in the first hidden layer, around 10 % of hidden nodes should be used to encode the changes in a microarray.

we first evaluated how adding a sparse regularization term influenced the state of hidden units . we trained a conventional autoencoder and a sparse autoencoder  using the microarrays. for each microarray, the models probabilistically inferred the state of each hidden node . figure  <dig> shows the histogram of the expected states of the nodes in the first hidden layer associated with all microarray samples. in the conventional autoencoder, a relatively larger number of the nodes in the first hidden layer had a non-zero probability to be  <dig>  , whereas the majority of the hidden nodes in the sparse autoencoder model were expected to take a value of  <dig>  . thus, the sparse autoencoder strives to use less hidden nodes to encode the same statistical structure in the observed data, instead of using every hidden node, with each contributing a little to the expression of genes. this is a desired property conforming to our assumption that the response to a specific perturbation should be encoded by a relatively small number of tfs.fig.  <dig> the histogram of the expected states of hidden units  in the first hidden layer for the conventional autoencoder  and sparse autoencoder  respectively. for both models, the number of hidden units from the first hidden layer to the fourth hidden layer is  <dig>   <dig>   <dig>  and  <dig> respectively. the sparsity threshold for the sparse autoencoder is  <dig> . a hidden unit has a state under each experiment condition. therefore, the total number of states for all hidden units is the number of experiment condition  * the number of hidden units . the x-axis is the probability of a hidden unit to be on ranging from  <dig> to  <dig>  the y-axis is the count of states



we further evaluated how well models with different architectures  can be used to represent the transcriptomic machinery of yeast. table  <dig> shows the results of a limited model selection experiment based on our biological assumptions . we calculated the reconstruction error, which is the sum of the differences across all microarrays between the observed expression state  of genes in microarrays and the expected states of genes reconstructed by the autoencoder. the results indicate that models of the same type  could learn to encode data with a similar accuracy across the range of the architectures studied here, although the sparse autoencoder had higher reconstruction errors.table  <dig> reconstruction error of models with different architectures



while the results indicate that the reconstruction errors of sparse autoencoder models were a bit higher than the ones of the traditional autoencoder, it should be noted that the sparse autoencoder reconstructed the same data with a much smaller number of hidden variables. from the perspective of the minimum description length  principle  <cit> , a model is preferred if it can encode the information of a dataset with a minimal description length while achieving a similar or better reconstruction of data. in information theory, the description length is measured as the number of bits needed to encode the data, and in our case each bit is encoded by a hidden node. thus, the sparse autoencoder potentially is a more desirable model even if it suffers a higher reconstruction error. to quantify and compare the utility of conventional and sparse autoencoders, we calculated the bayesian information criteria  of the models, and the results are shown in table  <dig>  the results indicate that the bic of the sparse autoencoder with an architecture consisting of hidden layers with  <dig>   <dig>   <dig>   <dig> hidden nodes  respectively is the lowest  among the compared models. since the number of hidden nodes in the first hidden layer of this model agrees better with the knowledge of the number of transcription factors, we chose to investigate the results derived from this model in the following sections.table  <dig> bic scores of different models


autoencoder

sparse autoencoder
the cells show the bic score  and the individual terms of the bic . the numbers in the parentheses associated with each architecture  indicate the number of hidden nodes in 1st – 4th hidden layers



distributed representation enhances discovery of signals of tfs
the motivation of using a hierarchical model is to allow latent variables in different hidden layers to capture information with different degree of abstraction in a distributed manner. when modeling transcriptomic data, one goal is to discover the signals of tfs. in a sparse autoencoder, it is natural to expect that the 1st hidden layer should capture the signals encoded by tfs. we test this hypothesis by evaluating the overlap of the genes predicted to be regulated by a hidden node in the 1st hidden layer and those known to be regulated by a tf. a statistically significant overlapping between them is shown in fig.  <dig> fig.  <dig> mapping between transcription factors  and hidden variables in the first hidden layer. results for sparse autoencoder  and nmf  are shown. the transcription factors  are arranged along x-axis, and the hidden variables are arranged along y-axis. each point in the figure represents the value of –log of the enrichment score between genes regulated by a hidden node and a tf. the pseudo-color bar shows the scale of the –log



indeed, the results indicate that sparse autoencoder is capable of capturing and representing the information of tfs, in that there is an almost one-to-one mapping between hidden nodes and known tfs as shown in fig. 3a. for a few hidden variables that are significantly mapped to multiple tfs, we further investigated if these tfs are members of known tf complexes  <cit> . as an example, we found that a hidden node is significantly mapped to aft <dig> and put <dig>  which are two yeast tfs known to cooperatively regulate genes involved in ion transportation  <cit> . as another example, a hidden node is mapped to both msn <dig> and msn <dig>  <cit> , which belong to the same family and form hetero-dimers to regulate genes involved in general stress response.

to demonstrate the advantage of hierarchical and distributed representations, we compare our results with another latent variable model commonly used to represent microarray data, the non-negative matrix factorization model   <cit> . the nmf can be thought of as a model consisting of an observed layer  and a single hidden layer , which is used to capture all signals in embedded in microarrays, whereas the same information is distributed to multiple layers of hidden variables in the sparse autoencoder. we trained a nmf model with  <dig> “metagenes”, which is the same as the number of hidden nodes in the 1st hidden layer of the sparse autoencoder, and the results of mapping between latent factors and tfs are shown in fig. 3b.

indeed, the results clearly demonstrate the expected difference between the two models. with the capability of capturing the context-specific and compositional relationship of signals regulating expression in a distributed manner, the hidden nodes in the 1st hidden layer clearly capture the specific signals of tfs, whereas the signals regulated tfs are delegated to the higher level hidden nodes. in contrast, with only a single layer of latent variables, all signals in the data need to be “squeezed” into these latent variables, such that a latent factor  has to represent the signal of multiple tfs. therefore, the results support our hypothesis that, benefitting from the distributed representation of the statistical structures across multiple hidden layers, the sparse encoder can concisely learn and represent the information of biological entities, in this case the tfs.

latent variables can capture the information of signaling pathways
we further investigated whether certain hidden nodes can represent the states of well-known yeast signaling pathways, i.e., whether the state of a hidden node can be mapped to the state of a collection of proteins in a pathway. in a previous study  <cit> , we were able to recover the pheromone signaling pathway and a set of target genes whose transcription were regulated by the pheromone pathway, by mining the systematic perturbation data from the study by hughes et al.  <cit>  in which  <dig> genes involved in yeast pheromone signaling were perturbed by gene deletion. in the current study, we identified the microarray experiments in which the aforementioned  <dig> pheromone-pathway genes were perturbed, and we examined if the state of any hidden node was statistically associated with perturbation of pheromone pathway, using the chi-square test . interestingly, we found  <dig> hidden nodes in the 1st hidden layer that are significantly associated with the perturbation experiments, one with a chi-square test p ≤  <dig> e- <dig>  and the other with a p ≤  <dig> e- <dig>  further inspecting the genes predicted to be regulated by these hidden nodes, we found a significant overlap between the pheromone target genes from our previous study and the genes regulated by these hidden nodes . these results indicate that the hidden nodes of the sparse autoencoder model are capable of capturing the signals of specific yeast pathways. however, it should be noted that, by design, a hidden node in the high level layers of the sparse autoencoder might encode the signals of multiple pathways that share strong covariance.

the hierarchical structure captures signals of different degrees of abstraction
one advantage of the hierarchical structure of an autoencoder is to represent information with different degrees of abstraction. intuitively, the lower level hidden layers should capture the highly specific signaling pathways or signaling molecules, such as tfs, whereas the high level hidden layers may encode more general information. to test this hypothesis, we identified the genes regulated by each hidden node by performing a linear weight combination experiment  <cit>  . we then applied a semantic analysis method previously developed by our group  <cit> , which identifies the most appropriate concept from the gene ontology  to summarize the genes. interestingly, we found that genes regulated by aft <dig> and put <dig> are significantly enriched among the genes regulated by a hidden node in the 2nd hidden layer, and the genes regulated by this hidden node is summarized by the go term go: <dig> , shown in fig.  <dig>  using the same method, we found another hidden node whose related genes were annotated with go: <dig> . thus, the results indicate that the distributed representation of information enables the hidden nodes at the different level of hierarchy to capture information of different degrees of abstraction.fig.  <dig> example of hierarchical gene ontology  map for hierarchical hidden structure



concise representation enhances the discovery of global patterns
given a large comprehensive dataset, it is often interesting to learn if distinct perturbations affect common biological processes of the cell  <cit> . a common approach to discover such patterns is to perform clustering analysis and examine if certain samples  are clustered together. in general, the result of a clustering analysis is significantly influenced by whether the features representing each sample are informative. non-informative features may not reveal any real information, whereas features concisely reflecting the states of cellular signaling system may provide insights regarding the system. to examine if the signals represented by the latent variables are more informative than original gene expression values and nmf metagene values, we represented the samples in our dataset using the original gene expression values, nmf metagene values and the expected states of the hidden nodes in a hidden layer respectively, and we then compared the results of consensus clustering .fig.  <dig> clustering of experiment samples represented using original gene expression data , nmf metagenes  and expected state of hidden nodes in the first hidden layer . consensus clustering results show how consistently a set of samples is assigned into a common cluster during repeated clustering experiments using samples with replacement from a dataset. a yellow box indicates a set of samples that are consistently assigned to a common cluster, and the brightness of yellow reflects the consistency



the results clearly demonstrate that, if samples are represented in the high-dimensional gene expression space, the majority of the samples cannot be grouped into distinct clusters. when we use the low-dimensional nmf metagenes space, it performs slightly better than using original gene expression space. but, the clusters derived are still not well separated. in contrast, when samples were represented using the expected states of hidden nodes of the 1st hidden layer, the samples can be consistently separated into distinct clusters. representing samples using the expected states of the nodes from other hidden layers also produced clearly separated clusters . the results indicate that the states of hidden nodes are much more informative in terms of representing distinct characteristics of individual samples, thus enabling clean separation of samples by the clustering algorithms. although it would be interesting to systematically inspect the common characteristics of the samples in terms of whether the perturbation experiments affect common signaling pathways, such an analysis requires broad and in depth knowledge of yeast genetics, which is beyond the expertise of our group.

information embedded in data is consistently represented in different hidden layers
we hypothesized that, in a successful hierarchical representation of a dataset, the information embedded in data should be consistently encoded by different hidden layers, even though two different layers are of different dimensionality and identity of the nodes are totally different. in other words, when a sample is represented by the state of the nodes from different hidden layers, the characteristics that distinguish this sample from others  should be retained despite being represented by the nodes from different layers. to test this hypothesis, we first performed consensus clustering using nodes from different hidden layers as features to get sample clusters, and then we compare if members within a cluster derived using one representation significantly overlap with the members from another cluster derived using a different representation. figure  <dig> shows the results of assessing the overlaps of the samples clusters derived using the nodes from the 1st and 2nd hidden layers as features respectively. the results indicate that the majority of the clusters derived with different representations agree. interestingly, a cluster derived from the 2nd hidden layer as features subsumes  two clusters derived using the 1st hidden layer as features, indicating that the 2nd layer captures more general information. the results indicate that, even though the dimensionality and identity of features of each hidden layer are significantly different, the information encoded by the hidden nodes in a sparse autoencoder is preserved across different layers.fig.  <dig> cluster by cluster clustering for clusters in the 1st hidden layer and the 2nd hidden layer. the x-axis is the  <dig> sample clusters using the states of hidden units in the 2nd hidden layer indexed by the superscript . the y-axis is the  <dig> samples clusters using the state of hidden units in the 1st hidden layer indexed by the superscript . for example,  <dig> is the first sample cluster using the states of hidden units in the 1st hidden layer and  <dig> is the first sample cluster using the states of hidden units in the 2nd hidden layer. a yellow box indicates that the members of two clusters  significantly overlap, with the brightness of yellow reflecting the degree of overlap



CONCLUSIONS
in this study, we investigated the utility of contemporary deep hierarchical models to learn a distributed representation of statistical structures embedded in transcriptomic data. we show that such a model is capable of learning biologically sensible representations of the data and revealing novel insights regarding the machinery regulating gene expression. we anticipate that such a model can be used to model more complex systems, such as perturbed signaling systems in cancer cells, thus directly contributing to the understanding of disease mechanisms in translational medicine.

competing interests

the authors declare that they have no competing interests.

authors’ contributions

lc and xl designed the study, performed the computational experiments, interpreted the results and wrote the manuscript. cc participated in the design of the study, collected and preprocessed the experiment data. vc performed the go analysis and participated in the manuscript writing. all authors read and approved the final manuscript.

