BACKGROUND
for successful pharmaceutical treatments the full knowledge about the metabolic and regulatory networks of a system is needed. only if those pathways are known completely, can we hope to understand how an organism circumvents the blocking of a receptor in one pathway by employing an redundant analog or why a drug not only interacts with its intended target t but also with targets s, i, d and e, thereby creating side effects.

despite an ever growing number of known gene sequences and elucidated protein structures the knowledge of the molecular components of biological systems remains incomplete. even for simple model organisms, only  <dig> – 50% of the metabolite structures are known  <cit> . for that reason, projects such as the human metabolome project  <cit>  have been started to catalogue the metabolomes of whole organisms.

nuclear magnetic resonance  is still the only analytical technology capable of giving enough information for elucidating the molecular structure of an unknown substance. this becomes especially important in metabolomics research, where experiments – often measured with the more sensitive mass spectrometry  instruments – detect differential mass signals, but in many cases even tandem ms will not give enough information for a de-novo characterisation of the metabolite.

powerful chromatographic and spectroscopic methods are available to alleviate this lack of knowledge on metabolite structures, but nothing close to an automatic method with high throughput capabilities has yet been developed  <cit> . systems aimed at doing this are known as computer assisted structure elucidation  systems. one way to perform case is to generate putative molecules in agreement with certain input data, predict spectra for each of the candidates and rank them by similarity of predicted and experimental spectra. a rudimentary prediction can be done by database lookup, but since these can never contain all possible structures, a prediction for arbitrary structures is needed.

for spectrum prediction of 13c and other nuclei, an established method dating back to  <dig> are the hierarchically ordered spherical environment  codes  <cit> . for proton 1h nmr spectra more elaborate algorithms have been suggested. this is mainly justified by the strong influence of 3d spatial effects on proton shifts, which are not included in the original hose code specification. therefore methods using only 2d parameters are considered insufficient  <cit> . stereochemical enhancements of the hose code specification have been described in  <cit> .

the first attempts towards proton nmr predictions were based on string-based descriptions similar to hose codes  <cit> . commercial packages also use such methods, combined with other strategies . later so-called additivity rules were established, which are manually created rules describing the effect of certain functional groups. they were motivated mainly by the lack of sufficiently large databases, which are needed for string-based descriptions  <cit> . finally machine learning methods, most prominently neural networks, were used  <cit> .

the problem of predicting spectra for arbitrary compounds based on a collection of samples is a classic application of machine learning methods. by including descriptors reflecting distances in euclidean space, stereochemical information can be included easily by these methods. in this paper, we perform a comparative study of selected methods for the purpose of proton nmr spectrum prediction, we also include hose-code based prediction for comparison.

methods
in this section we first give a formal definition of the prediction problem, then we describe the test- and training set and the descriptors used to evaluate the machine learning algorithms.

an nmr spectrum can be modelled as a collection of individual shifts of atoms. figure  <dig> shows a graphical representation of a molecule, the corresponding 1h spectrum with the individual peaks and the assignment between shifts and atoms.

in machine learning algorithms, the input is given as feature vector representing the properties of the atom , and the output is either the target class  or a numerical target value .

the mean absolute error  and the standard error  of the prediction are calculated as:

 mae=∑|δexp−δpred|n 

 se=∑2n 

with δexp being the experimentally observed chemical shift of an atom, δpred the predicted chemical shifts of the same atom and n the number of data points. to estimate the generalisation error, we performed a  <dig> fold cross validation by first applying a random permutation to the data set, then dividing the data set into  <dig> equally sized disjunct partitions in terms of the total numbers of molecules and in each iteration predicted one of the disjunct partitions by the complementary remaining  <dig> partitions in each iteration, constituting always an overlapping yet different training set. attention has to be paid to ensure that all protons of the molecule under prediciton are excluded from the training set. otherwise, shifts would be predicted based on inference from neighbouring protons, resulting in overfitting.

in order to compute the mae and se errors for the classification tasks, the response variable  was discretised into bins of size  <dig>  ppm, resulting in  <dig> different classes  .

molecular and atomic descriptors
the proper choice of descriptors has a major influence on the performance. to capture dependencies between the shifts, the protons have to be described together with their chemical context in the molecule. descriptors can be derived from atomic, bonding, and molecular properties and should capture the atom and its environment at different levels of structure resolution in addition to diverse physicochemical properties of a compound.

 <dig> descriptors in total were calculated using the qsar  package of the cdk   <cit> . this package implements a wide range of molecular and atomic qsar descriptors.

for our study we decided to combine the descriptors from a study by meiler, who used artifial neural networks to predict chemical shifts in proteins.  <cit>  with most of those from work by aires-de-sousa  <cit> . from  <cit>  we excluded those relating to the backbone structure, which are not applicable to non-proteins.

an overview of the descriptors is shown in figure  <dig>  a more detailed description of the descriptors is shown in table  <dig> of additional file  <dig>  the calculated descriptors for the whole database are available in  additional file  <dig> 

in order to encode the 3-dimensional environment in the descriptors of a particular atom  meiler  <cit>  suggests to calculate a set of  <dig> basic descriptors for the atom the hydrogen is bound to and for the  <dig> atoms closest by bond  and for the  <dig> atoms closest in space  , thus producing a 3-dimensional basis. calculating these altogether  <dig> descriptors +10) is relatively easy since a small set of descriptors is repeatedly calculated for different atoms. as a notable difference to meiler's work, which tackles large proteins, in our case the  <dig> closest atoms by bond often already contain most or all atoms of a small organic molecule.

the set of descriptors suggested by aires-de-sousa and coworkers  <cit>  has some descriptors being physico-chemical  and additionally average/minimum/maximum values of atoms in the spheres round the proton are used. the radial distribution function  of several factors  adds sensitivity for the 3-dimensional environment of the atom in question. rdfs contain informations about the interatomic distances in a molecule, unweighted or weighted by different atomic properties such as atomic mass, electronegativity, van der waals volume and atomic polarisability  <cit> . aires-de-sousa also suggests to divide the atoms into four groups: protons attached to atoms in aromatic rings, non-aromatic p systems, rigid aliphatic systems and non-rigid aliphatic systems.

the descriptors suggested by meiler  <cit>  were slightly modified by taking first the  <dig> closest atoms in space and then the  <dig> closest by bond, if not included in the first set , which we found to give an improved result. from the measurement conditions  only the solvent was kept in the final feature set, however a large percentage of those entries were unknown solvents.

test- and training data set
compounds for training and test data are taken from the nmrshiftdb database, an open access, open submission, open source database for small organic molecules and their assigned nmr spectra  <cit> . the data quality in nmrshiftdb has recently been evaluated in a separate study based on cross validation with data from an external data source  <cit> . all molecules shown in this paper can be accessed via their molid on  using the "go directly to id" function. the assignments in nmrshiftdb are all hand-curated and checked by human reviewers. at the time of writing it comprised  <dig> structures with altogether  <dig> spectra, of which  <dig> were proton spectra . successful predictions previously reported were done with much smaller data sets . it should be noted that the nmrshiftdb data have been collected over time by different contributors, they are not in any way selected for the purpose of prediction. to verify a broad coverage of the known organic chemistry space we mapped our data to a descriptor space defined by one of the largest available collections of organic structures, the pubchem database  <cit> , which we downloaded from . the mapping was done by calculating for each proton a selected set of atomic descriptors such as effective polarisability, π/σ partial charge and s electronegativity for the proton itself, π/σ electronegativity and partial charge for the connected heavy atom, and similar descriptors applied to spatially neighbouring atoms . the descriptor values from both data sets were analysed by pca analysis, shown in figure  <dig>  the nmrshiftdb is by far smaller than pubchem, but the main clusters of the pubchem data are represented in nmrshiftdb as well.

for the shift prediction we extracted the protons directly attached to a carbon. shifts on hetero-atoms are much more influenced by the experimental conditions ), which are not subject of our study. the resulting set consists of  <dig> measured 1h shift values from  <dig> to  <dig>  ppm of  <dig> different unique molecules with a mass range of  <dig> amu to  <dig> amu .

the protons were categorised as suggested in  <cit> : a) aromatic rings 21%, b) non-aromatic π systems 7%, c) rigid aliphatic systems 27% and d) non-rigid aliphatic systems: 45%.

for each proton we calculated the descriptors described in table  <dig> of additional file  <dig>  attributes containing more than 50% missing values  were omitted from the data set. this affected mainly the more distant sample points of the rdf based descriptors and the higher ranks of the  <dig> topologically closest atoms not contained in the  <dig> closest by distance. numerical attributes containing less than 50% na values were imputed with their mean values. na values in categorial columns were treated as independent values themselves and were not imputed. after preprocessing the data set was of size  <dig> *  <dig> attributes . normalising the data was performed by scaling to the maximum value attribute-wise for numerical attributes. nominal attributes were assigned integer values in increasing order for distinct attribute levels and scaled accordingly to the same range as the numerical attributes. for clustering and regression purposes, and for computation of distance matrices, we concentrated solely on the numerical attributes. however, in terms of the classification tasks we combined both nominal and numerical attributes.

calculation of 3d coordinates
three dimensional coordinates are often not available to the experimentalist and are not included in nmrshiftdb. they were calculated using the software package corina  , the industry standard for coordinate generation. it respects stereo hints  given in the structures, so that conformations measured and entered by the contributor are taken into account. when a spectrum is measured in solution different conformations usually contribute to the observed chemical shifts. this situation makes the definition of the 3d environment of a proton rather difficult. for the structures in nmrshiftdb, where several conformations are possible a plausible one of them was chosen by corina.

algorithms for prediction
a large number of algorithms for classification tasks and regression have been proposed in the past. we have used algorithms integrated in the two open source projects r  <dig> . <dig>  <cit>  and weka  <dig> . <dig>  <cit>  because the consistent programming interfaces allowed to evaluate a broad range of algorithms. as a result of preliminary tests we thoroughly investigated the performance of the following algorithms for the prediction tasks.

instance based learners
ibk  <cit>  is an instance based learner with k being the number of neighbours. it predicts new data by simply considering the class values of the k nearest neighbours, thereby using a statistical method to 'weed out' irrelevant or noisy instances. it belongs to the class of so called lazy learners, because no concepts are produced, it merely stores the typical values without processing the data in any way.

support vector machines
support vector machines are based on statistical learning theory and belong to the class of kernel based methods  <cit> . the basic concept of svms is the transformation of input vectors into a higher dimensional feature space where a linear separation may be possible between the class members. in this feature space the support vector learning algorithm maximises the distance between the class members of the training set in order to achieve a good generalisation.

decision trees
decision trees recursively split attributes in a top-down manner. the attribute with the highest normalised information gain  is used to make the split decision. the algorithm then recurses down the tree until a leaf with the minimum desired number of instances is reached. a well-known decision-tree algorithm is id <dig> and its successor c <dig>  .

random forest
the random forest classifier was developed by leo breiman and adele cutler  <cit>  and consists of many decision trees. the algorithm combines breiman's "bagging"  <cit>  idea and ho's "random subspace method"  <cit>  to construct a collection of decision trees with controlled variations. a training set for a tree is constructed by choosing n samples with replacement from all n available training cases . at each node a random subset of variables is used to determine the splitting decision. finally the mode of all classes by the individual fully grown and unpruned trees is returned.

bagging
bagging  was proposed by leo breiman in  <dig> to improve the classification by combining classifications of randomly generated training sets. bagging can be used with any type of model. new training sets are constructed by sampling uniformly from the original training set with replacement. the models are fitted using the bootstrap samples and combined by averaging the output  or voting .

boosting
boosting is a form of reinforcement learning  <cit> . for each call a distribution of weights is updated to indicate the importance of examples in the data set for the classification. on each round, the weights of each incorrectly classified example are increased, so that the new classifier focuses more on those difficult examples.

multivariate linear regression
this is a generalisation of a simple linear regression of two variables. a continuous metric response variable y  gets approximated through a linear combination of n multiple influence quantities and has the form: y = a <dig> * x <dig> + ... + an * xn + b + ϵ.

feature selection
to reduce the number of input variables without a manual  choice, we used the variable importance measure as reported by random forest to select the relevant features associated with the target class.

a random forest constructs n unpruned trees by drawing n bootstrap samples. the variable importance measurement is performed on the test set established by the bootstrap method: given a data set of d tuples the data set is sampled d times with replacement constituting the training set. the data tuples that did not make it into the training set form the test set. the probability of not being chosen is d. if d is large, the probability approaches e- <dig> =  <dig> . thus,  <dig> % form the training set and  <dig> % form the test set. the random forest procedure provides two importance measures:

• mean decrease accuracy : it is constructed by permuting the values of each variable of the test set, recording the prediction and comparing it with the unpermuted test set prediction of the variable . for classification, it is the increase in the percentage of times a test set tuple is misclassified when the variable is permuted. for regression, it is the average increase in squared residuals of the test set when the variable is permuted. a higher %incmse value represents a higher variable importance.

• mean decrease gini : measures the quality  of a split for every variable  of a tree by means of the gini index. every time a split of a node is made on a variable the gini impurity criterion for the two descendent nodes is less than the parent node. adding up the gini decreases for each individual variable over all trees in the forest gives a fast variable importance that is often very consistent with the permutation importance measure. a higher incnodepurity value represents a higher variable importance, i.e. nodes are much 'purer'.

the two variable importance measures are drawn separately for each tree of the forest and finally are averaged over all trees of the forest and there is no clear guidance on which measure to prefer.

RESULTS
before applying the machine learning methods, we examined the characteristics of the molecular descriptors in our evaluation data set.

numerical attributes
we performed a pca analysis of the  <dig> numerical attributes. in figure  <dig> four major groups are visible. the bulky group on the left side consists in fact of two subgroups, predominantly representing aromatic rings including a heteroatom or protons contained in aliphatic ring systems with adjacent p systems. the group in the lower center contains e.g. aliphatic protons with adjacent oxygens , whereas at the center to the right there are mostly aliphatic ring systems with heteroatoms and on the far right side predominantly alkyl-groups are visible. broadly speaking there is a strict linear separability achievable by drawing a line from the top left-hand corner to the bottom right-hand corner of the diagram. even the two groups on the left side are strictly linear separable. each proton in the diagram carries a symbol according to its proton category and is coloured by its shift value. there is a general trend for decreasing shift values from left to right, but within groups variability is rather strong. notably, the four different proton categories do not unambiguously reflect the visibly detectable groups. the corresponding loadings plot is provided in figure  <dig> of additional file  <dig>  and reveals that especially the  nearest heavy atoms influence the separation into the groups shown.

categorial attributes
machine learning comparison
we used the data set presented in the last section to evaluate the performance of the selected machine learning methods and training strategies. the most simple approach is the multivariate linear regression based on the numeric descriptors alone, which has an mae of  <dig>  ppm. using the same descriptor set, but the non-linear svm regression, the result can be improved to  <dig>  ppm. the j <dig> and random forests use the categorial descriptors in addition to the numeric features. an illustrative comparison of the actual versus the predicted shift values using random forest can be found in figure  <dig> 

the settings for the algorithms were mostly according to default settings provided by the packages used in this study. best results for j <dig> were obtained for a minimum number of two instances per leaf. a larger number of instances deteriorated the performance slightly. for the random forest we used an unlimited depth of trees and the default number of trees . for the instance based learners  the optimal number of neighbours was one, larger number of nearest neighbours again worsen the performance gradually. for the support vector regression we utilised the radial basis kernel and a grid search for the optimal parametrisation values.

boosting had the largest impact on the j <dig> classifier, both when used alone and in combination with feature selection. we also evaluated the bagging  training method, and achieved a marginally larger error compared to boosting .

the accuracy rate as given by the mean absolute error  of the prediction algorithms is shown in figure  <dig>  the mean absolute error varies between  <dig>  ppm and  <dig>  ppm depending on the method. linear regression, being a simple method, performs worst .

misclassifications
diastereotopic protons attached to carbon atoms are topologically equivalent but face different 3d environments due to a single 3d conformation being more or less randomly chosen for our computation. as has been stated before, the use of a single 3d conformation is artificial and introduces a potential source of error. a rigorous treatment would create a set of the most frequent 3d conformation in solution as well as an assessment of their likelihood of occurrence. shifts would need to be predicted for each conformation and then averaged weighted by their probability. this approach would a) generate computation times rendering the application useless for our purpose and b) require training data with measured shifts for single conformations in solution – something which is clearly not achievable but could be approximated through rigorous ab-initio calculations of chemical shifts for ensembles of conformations. for many methylene proton pairs and any dimethyl group that is a rotatable fragment, accounting for multiple conformations would average out differences such as those seen figure  <dig> .

there are a number of protons for which all of the investigated machine prediction algorithms  at a time fail to correctly predict the chemical shift with an underlying error rate of >  <dig>  ppm . the shifts of these protons cannot accurately be predicted with either a reduced or the full descriptor set or any special settings for the algorithms. therefore they are affected by a fundamental problem. in figure  <dig> the 'hard-error' – misclassifications contributing to the overall error are displayed. from this plot one can judge, that the 'hard-error' – misclassifications do not arise because they are outliers. when scrutinising those erroneous shift predictions in detail we encountered the following types of misclassifications :

• diastereotopic methyl-groups .

• methylene-protons in chains .

• methylene-protons in rigid rings .

• false entries such as . here virtually no systematic errors were detected.

in addition, training data may be imperfect due to missing stereochemical information for diastereotopic protons. if no stereochemistry is given in nmrshiftdb, the shifts are randomly assigned to the atoms in space. if this assignment is wrong and there are enough correct examples in the database, the prediction may still be right, but different from the actual assignment. an example for this are most likely atomids  <dig> and  <dig> in table  <dig>  we believe that those types of false assignments may be encountered in the database to a not negligible amount. consequently, when applying feature selection a potential set of descriptors, possibly explaining the difference in shifts, are clipped away, because due to the false assignments they become meaningless. note that in figure  <dig> for the indices between approximately  <dig> and  <dig> there is only one single  – misclassification. this corresponds to the shift range of  <dig>  ppm to  <dig>  ppm, where most of the protons are attached to aromatic six membered rings.

hose codes
to compare the machine learning algorithms with a fundamentally different approach, we used hose  codes for the prediction . table  <dig> illustrates the connection of hose codes and chemical shifts. hose codes  <cit>  are a well established method for prediction of especially 13c-nmr spectra. their use for 1h spectra is discouraged since they do not include three-dimensional information. however, we wanted to see how they performed and did a cross-validation with hose codes on the same data set as used for the other methods with a six sphere fall back. this means that we created a six-sphere hose code for each atom in the test set and tried to match this hose code in the "training" set . if one or more values were found the average was considered for prediction. if no matches were found, we backed up sphere by sphere until a hit was found. in the worst case it will be found in sphere one, since that contains the atom neighbour and the bond order, which for protons is always a single-bonded carbon . alternatively, an averaging of the lexicographically next neighbors in the next sphere could be employed. to our surprise, hose codes produced the lowest mean absolute error of  <dig>  of all methods tested .

CONCLUSIONS
we have applied a selection of machine learning algorithms to one of the largest public available data sets of 1h nmr spectra with the goal of predicting spectra for the use in computer-assisted structure elucidation of biological metabolites. this result will enable the implementation of open tools for the elucidation of unknown compounds with high-throughput nmr analysis of biological systems.

the best predictions were obtained with hose codes, random forests, j <dig> decision trees for mixed categorial/numerical features and support vector machine regression if only the numerical features were used. more important than the choice of the machine learning algorithm is the selection of the molecular descriptors. we were able to drastically improve the overall accuracy by reduction of the full descriptor set from  <cit>  and  <cit>  to the first  <dig> descriptors shown in figure  <dig>  instead of a manual  selection scheme we used the most predictive descriptors reported by the random forest. with less input variables the machine learning methods create more compact models and are less prone to over-fitting problems. furthermore, we found that bagging and boosting increased the prediction rate throughout all the predictors for both the regression and classification tasks. we discussed different sources of errors which are mostly related to stereochemical properties of the protons and possible mis-assignments in the data set. in this respect machine learning methods help to discover inconsistencies in large databases to improve the overall data quality.

hose codes leading to the best prediction results certainly came as a surprise for proton spectrum prediction because common wisdom expects a significant dependence of proton spectra on 3d effects which are not explicitly taken into account. a downside of hose code might be the reliance on a lookup table which requires a large sample set for meaningful predictions. strictly speaking, hose codes cannot be considered as being a predictive classifier, but more rather an elaborated 'look-up-procedure', comparable to the instance based lazy learners like ibk. compared to hose codes, the prominent advantage of the machine learning methods is certainly that they revert to an mathematical model as the underlying predictive basis. also, three dimensional molecular properties are involved in the prediction.

acd labs  report a standard error of  <dig>  ppm for their software acd/hnmr  <dig>   <cit> , and cambridgesoft chemdraw  <dig>  reports  <dig>  ppm, while we achieve an intermediate result of  <dig>  ppm for the best classifier. both are computed on a database size of approximately  <dig>  shift values, which is not available to the public. a direct quantitative comparison of our results and the commercial software is not possible since  size and quality of the data sets differ and  the results are obtained with different evaluation methods . like any other more or less randomly collected dataset, the nmrshiftdb will face a certain amount of duplication of certain frequently occuring cases. these cases have an increased likelyhood of occuring unchanged in both training and prediction set and may therefore bias the results of especially the hose code prediction. this bias is reduced if the number of spheres taken into account increases.

the results we have presented were obtained using straight forward molecular descriptors and proven machine learning methods. for the given data set they can be considered as a baseline system, against which future shift prediction systems can be compared.

authors' contributions
sk was lead developer of the nmrshiftdb database and performed the work on descriptor calculation. be and sn performed the data mining experiments and provided expertise on artificial intelligence. cs conceived and supervised the research project. all authors contributed to the manuscript.

supplementary material
additional file 1
supplement. plots and tables referenced by main article.

click here for file

 additional file 2
descriptor set. the data provides the complete set of descriptor values in matrix form for the h-nmr shifts upon the prediction is based.

click here for file

 acknowledgements
molecular networks gmbh, erlangen, germany provided the three-dimensional coordinates using the corina program. the nmrshiftdb project has been funded by deutsche forschungsgemeinschaft. thanks to all nmrshiftdb contributors for their contributions of data to nmrshiftdb.
