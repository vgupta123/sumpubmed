BACKGROUND
the increasing number of sequenced genomes makes it important to develop methods that can assign functions to newly discovered genes in a timely and cost-effective manner. traditional laboratory methods, while accurate and reliable, would require enormous effort and time to identify functions for every gene. computational approaches that utilize diverse biological datasets to generate automated predictions are useful in this situation as they can guide laboratory experiments and facilitate more rapid annotation of genomes.

existing computational approaches to gene function prediction have relied on a variety of genomic and proteomic data. exploiting the similarities between dna or protein sequences to infer gene function was the first approach tested and has been the most widely used approach to date. later, the usefulness of other types of genomic and proteomic data in this problem is also proved. researchers have used microarray expression data  <cit> , protein 3d structures  <cit> , protein domain configuration  <cit> , protein-protein interaction networks  <cit> , and phylogenetic profiles  <cit>  to predict functions of genes. recently, inferring gene function simultaneously from different types of biological data has been shown to deliver more accurate predictions and has attracted considerable research interests  <cit> .

many methods for inferring functions of genes from heterogeneous datasets share a common framework in which a functional association between genes is first constructed and then used as input for learning algorithms. a functional association can be represented as a network with nodes corresponding to genes and edges representing the co-functionalities of gene pairs. in such a network, each edge is usually assigned a weight representing the strength of the co-functionality relationship between the gene pair. a network of this kind is typical constructed in two steps. first, each dataset is used to create an individual network that captures the co-functionality of gene pairs, as implied by this dataset. for vectorial data, one can calculate edge weights as the similarity scores between genes using appropriate similarity metrics, for example the pearson correlation coefficient, and then form the networks by means of neighboring node connections. data already given in forms of networks, for example protein-protein interactions, are used directly. the second step constructs a single combined association network by integrating the individual ones. a strategy commonly used in this step is to form the combined network as a weighted sum of individual ones. here, each network is weighted according to its usefulness in predicting annotations for a group of genes that share a known specific function. previous studies have used various regression or other learning based algorithms to estimate network weights.

given a functional association network, the next step is to use this network to propagate functional labels from a group of annotated genes to other genes. there are two main types of approaches for this step. approaches of the first type create a kernel function from the co-functionality relationships encoded in the network and use this kernel with kernel-based classification algorithms  <cit> . in such approaches, genes with known annotations serve as labeled examples for training. approaches of the second type use graph-based algorithms, which propagate labels from annotated genes to other genes based on graph proximity. methods in this group range from simple nearest neighbor counting algorithms  <cit> , to more sophisticated statistical methods such as graph-based semi-supervised learning algorithms  <cit> , and markov random fields  <cit>  . on a number of benchmark datasets, graph-based and kernel-based approaches have shown comparable prediction accuracy, but graph-based approaches are generally faster  <cit> .

the prediction accuracy of both graph-based and kernel-based approaches largely depends on the ability of the network to capture the functional associations between genes. to improve the network quality, previous studies have focused on improving the integration step, or more precisely, on learning optimized weights for individual networks, and little effort has been applied toward improving the combined networks after they are constructed.

in this study, which is an extension of our previous work  <cit> , we assume that the network integration step is already done and focus on optimizing the produced network. given a combined network and a set of annotated genes that serve as training examples, we present a method for learning networks of improved quality. this is done in two steps: in the first step, the method learns a measure of similarity between pairs of genes; in the second step, the method reweights the network's edges using the similarity measure it just learned. here, we are inspired by previous work in ranking and multimedia retrieval domains which improves search results by learning a measure of semantic similarity from online datasets and using it to rank multimedia objects  <cit> . in learning, the algorithm iteratively updates a similarity function so that it gives higher scores to pairs of similar objects and lower scores to dissimilar or randomized pairs. when learning ends, semantically related objects are more likely to get higher similarity scores. once the similarity scores are learned, we use them to re-weight the edges of the input network.

in predicting gene function, discriminative learning algorithms are challenged by the small number of positive genes  for many categories, which is known as the problem of learning with unbalanced data. this problem is less critical for similarity learning methods because they tend to assume a weaker form of supervision than in classification, in which no labels are provided. moreover, whenever genes annotated to functional categories are available, the category labels induce a notion of similarity across pairs, and this similarity can easily be incorporated into the learning process. thus, similarity learning offers a more flexible framework than classification algorithms and can handle problems associated with unbalanced data in a natural way. another challenge for gene function prediction algorithms is speed, especially when assigning functions in large genomes comprising tens of thousands of genes. here, we use a learning algorithm that scales to the large genome size. the algorithm achieves computational efficiency due to several factors. it exploits sparse representations of genes when computing similarity, it does not require a similarity function to be symmetric or positive, and it is based on an online passive-aggressive algorithm that is known to converge quickly after being presented with only a handful of training examples.

we evaluated the effectiveness of the method  in predicting gene ontology  functional categories of genes in yeast and human using several datasets. as shown by the results, slan was able to learn networks that yielded more accurate predictions, as compared to predictions produced by fixed networks. in a comparison with two state-of-the-art gene function prediction methods, slan achieved higher prediction accuracy even when given as input a combined network produced by a simple integration method. the results also show that the method scales well with the number of genes.

methods
the proposed method predicts gene function using the following steps:  learning a measure of functional similarity between gene pairs;  using this similarity measure to form new association networks; and  inferring functions of genes from the new networks. in the following sections, we first describe the algorithm that learns similarity functions from data, and the method for selecting training samples. next, we describe how new networks are constructed using the learned functions. then, we give a brief review of the algorithm that predicts gene function from reconstructed networks. finally, we describe the datasets and input networks used in our experiments.

the similarity learning algorithm
assume there are n genes g <dig>  ..., gn, the first d genes of which have annotations in forms of go terms, where each go term corresponds to a category of gene function, and the remaining genes are new, the annotations of which are unknown and to be predicted. we also assume we are provided as input a functional association network with n nodes; each node corresponds to a gene, and each  edge represents the evidence of a functional association between the gene pair. each edge connecting gene gi and gene gj is assigned a positive weight aij , which shows the strength of this association. such a network can be constructed from heterogeneous datasets using network integration methods like the one presented in  <cit> . using the set of d genes with known annotations as training data, our method estimates a measure of semantic similarity that reflects the functional relatedness between gene pairs. with the learned similarity measure, we reconstruct the input network and use the new network to predict function for new genes. note that, in the learning phase, the method has access only to a fragment of network comprising d annotated genes, while in prediction it uses the full network of n genes.

the similarity learning algorithm we use in this study requires input objects be represented as vectors of real-valued features. to transform genes into vectors of real numbers, we apply a feature map Φ∈ℜd, which represents gene gl as the following column vector

  Φ={al <dig> …,ald}t, 

where ali for i =  <dig> ..,d are edge weights taken from the input network. intuitively, each gene is represented by its similarities to the d annotated genes, according to the given network.

now, let training signals be given in the form of a set p of gene triplets , where genes g and g+ are in a stronger functional association than genes g and g-. the goal is to learn a similarity function s that assigns higher similarity scores for pairs of more functionally relevant genes, that is s >s, ∀.

here, we adopt the learning algorithm originally proposed for image search applications  <cit> . the algorithm learns a similarity function that has the bilinear form:

  sw=ΦtwΦ, 

where w∈ℜdxd is a parameter matrix. it is important to note that, in practice, a widely used preprocessing step is to sparsify the association network by keeping only k strongest connections for each gene . in such a sparse representation, only k elements of feature vector Φ are non-zero. therefore, the computation of function sw has complexity o regardless of d. this property makes the computation of the similarity function efficient when d is large.

in the learning phase, the algorithm estimates a parameter matrix w such that gene pairs in stronger functional associations are assigned higher scores. specifically, for all triplets  ∈p , the algorithm seeks to find a matrix w such that s is larger than s with a safety margin of 1:

  sw-sw≥ <dig>  

for triplet  the algorithm computes the following hinge loss function:

  lw=max+sw,0), 

when the safety margin  is violated, this loss function is positive, making a penalty to the objective function. the algorithm then tries to minimize a training objective function that accumulates losses over all training data:

  lw=∑∈plw, 

this objective function is minimized by applying the passive-aggressive algorithm  <cit>  iteratively over training triplets. first, the algorithm initializes w to some matrix w <dig> . then, in each iteration, the algorithm selects at random a triplet  ∈ p and computes the hinge loss according to . if lw  =  <dig>  or, equivalently, sw-sw≥ <dig>  no update is made. otherwise, it solves the following convex problem with a soft margin:

  wi=argminw∥w-wi-1∥2frob+αξ, 

subject to:

  sw-sw≥1-ξ,ξ≥ <dig>  

where ||.||frob denotes the frobenius norm, and ξ is a slack variable. the intuition behind this update is to keep wi close to wi- <dig> from the previous iteration while minimizing the current loss. here, "aggressiveness" parameter α controls the trade-off between the two objectives. this optimization problem can be solved by the lagrange method, resulting in the following update:

  wi=wi-1+τivi, 

where

  τi=min{α,lwi-1||vi||2frob}, 

and

 vi=t, 

where Φi denotes the i-th element of Φ.

this learning procedure continues until a stopping condition is satisfied, and the corresponding w is returned. in practice, one can select the best w by using a heldout validation set: the accuracy is measured on the validation set and learning stops when the accuracy becomes saturated. as reported in  <cit> , using this method to select w provides good generalization while reduces learning time.

estimating pairwise similarities between training genes
the algorithm described in the previous section requires training signals in forms of triplets . from the set of d genes with known annotations it is important to choose only right triplets so that genes g and g+ are functionally similar while genes g and g- are not. for cases in which each gene has a single function, selecting such a triplet is straightforward in that pairs of genes that share a function are more similar than pairs of genes with different functions. this leads to a simple strategy, in which one can select a gene g, find a gene with the same function as g to provide an instance of g+, then find a gene without that function to provide an instance of g-. in practice, however, a gene can have multiple functions or participate in multiple biological processes. moreover, genes are often annotated with functions that form hierarchies, as in the case of go or funcat categories  <cit> . these properties make it more complex to quantify the functional similarity between gene pairs when choosing triplets for training.

a number of methods and metrics have been proposed to quantify the semantic similarity between go terms . in this study, we use resnik's measure  <cit>  - one of the most stable and widely used similarity metrics for biomedical ontologies like go  <cit>  - to estimate functional similarities. resnik defines the semantic similarity between a pair of go terms c <dig> and c <dig> as the information content  of their most informative common ancestor  according to the go graph:

 simc <dig> c2 = iccmica 

the ic of a term is defined as the negative log of the probability that this term appears in a collection of gene annotations.

since a gene can be annotated with multiple go terms, we estimate the similarity between a pair of genes by combining the resnik's measures of their annotations. there are several combination strategies including maximum, average, only exact matches, or sum of all pairs  <cit> . here we adopt the best-match average combination method: we take only the best matched terms and estimate their average resnik-based similarities. this combination method has been reported to give intuitive and stable results in several benchmarks  <cit> . in our experiments, resnik's similarities between go terms as well as similarities between gene pairs were computed by using the gosemsim package  <cit> .

selecting training triplets for learning function specific similarity measures
using the learning procedure described in the previous section, for each functional category c, we estimate a parameter matrix wc. in the next step, wc will be used to construct a new association network that is specific for c. note that, it is more computationally efficient to estimate and store a single functional network for all categories. however, because a gene can have multiple functions, such a single network may be insufficient to represent all the co-functionality relationships between genes. maintaining one separate network for each function can provide more information to make accurate predictions.

strategies for selecting training triplets
for a given functional category c, the following procedure is used to select a gene triplet for training. first, select at random a gene annotated with c, which will be g. then, select at random another gene also annotated with c, which will be g+. finally, select at random a negative gene g-that satisfies the following:  it is not annotated with c and any descendant term of c in the go graph, and  its similarity score with respect to g is lower than a threshold, where the similarity score r is estimated using the method described in the previous section. a threshold of  <dig>  was used in our experiments, meaning that two genes were deemed to be not functionally related if their similarity score was lower than  <dig> .

while g and g+ are sampled uniformly, we consider three ways to sample g- from those satisfying the two above conditions:

- uniform sampling. this strategy considers all negative genes equally.

- sampling a negative gene less relevant to g with a higher probability. the intuition behind this strategy is that we update the similarity function so that it returns true scores for the most dissimilar pairs of genes first. a negative gene is sampled with probability /z where r is the resnik-based similarity score between this gene and g, and z is the normalization factor.

- sampling a negative gene more relevant to g with higher probability. this strategy attempts to first update the similarity function on borderline genes, that is, negative genes having similarity scores near the threshold. specifically, a negative gene is sampled with probability r/z, where r is the similarity score between this gene and g, and z is the normalization factor.

we initialized w0c to an identity matrix, i.e. w0c = i, and used a validation set to select the best wc: the prediction accuracy was periodically measured on the validation set after a predefined number of iterations; learning stopped when accuracy became saturated and the corresponding wc was returned.

constructing new networks
once similarity functions are learned, the next step is to construct new association networks, one per a go term. for each gene gi from the annotated and unannotated gene sets, we create its feature vector using equation . then, for each functional category c, we use similarity function swc with matrix wc to compute the c similarity score between genes gi and gj and use this score as the weight a'ij of the edge connecting the genes .

because swc is not symmetric, i.e. swc and swc are not necessarily the same, we compute a'ij as follows:

  aij′=+swc)/ <dig> 

to sparsify the newly constructed networks we keep only k connections with the largest weights for each node and remove the remaining connections.

inferring gene function
given an association network a'={aij′},i,j= <dig> ..n, any of existing graph-based classification algorithms can be used to infer functions of unannotated genes. in this study, we use the semi-supervised learning algorithm by zhou et al.  <cit>  for this step.

let y denote a label vector, each element yi of which represents the prior knowledge about gene i having  the function of interest. we assign labels + <dig> to positive genes, that is, genes known to have the given function, and assign labels - <dig> to negative genes. here, we consider a gene negative if it is not annotated with the given function and any of its descendants according to the go graph. following mostafavi et al.  <cit> , we assign a prior value yi=d+-d-d++d- for genes with unknown annotations, where d- and d+ are the numbers of negative and positive genes in the training set, respectively. this prior value is used to reflect the class imbalance nature of the gene function prediction problem, in which the number of negative genes is typically much larger than the number of positive genes.

the learning process consists of estimating a score fi ∈ for each gene gi. once this score is estimated, the algorithm classifies the gene into having or not having the given function by thresholding the score. score fi is obtained by minimizing the following objective function:

  ∑i=1n2+σ∑i,j=1naij′2=t+σftlf, 

where d is a diagonal matrix with and dii=∑j=1naij′ and l=d-a' is the graph laplacian matrix. this objective function has two terms: the first term constrains score fi not to change much from prior label yi, and the second term encourages adjacent nodes to have similar scores. parameter σ trades off these two competing objectives. this optimization problem has the following solution:

  f=-1y, 

input networks
up to this point, we assumed that the input combined network was given. in practice, one can obtain such a network by using any existing network integration method. in our experiments, we considered a very simple integration method in which the combined network is created by summing over individual networks, and all the networks have the same weight. we sparsified the networks by keeping only  <dig> edges with the largest weights for each node and removing the rest. the number of  <dig> edges for each node was chosen based on the results from  <cit> .

datasets
we used two datasets in two species  to evaluate the effectiveness of the proposed method.

the yeast dataset
the yeast dataset is provided by barutcuoglu et al.  <cit>  and contains various genomic and proteomic data for  <dig> yeast genes. there are four types of data: microarray data, transcriptions factor binding sites, protein-protein interactions, and co-localization of gene products in a cell. the interaction, co-localization, and binding site data are binary, and microarray data are real-valued.  <dig> go terms selected from the biological process vocabulary of go were used as labels to annotate the genes. to ensure consistency among the training labels, all annotations were up-propagated, that is if a gene is assigned to a term in the go graph, it is also assigned to all ancestors of this term. this procedure was applied in all our experiments.

the human dataset
we used the human dataset provided by mostafavi et al.  <cit> . this dataset contains various biological data collected from eight sources for  <dig> human genes. the data include omim diseases associated with genes, domain compositions, protein interactions, transcriptional modification data, and gene expression data. gene expression data are real numbers while the other data are binary. the genes in this dataset were annotated with terms from the biological process vocabulary of go. the same procedure as used for the yeast dataset was applied to up-propagate the annotations. to guarantee the same experimental conditions, we followed the steps described in  <cit>  to create individual networks for the eight data sources. for each dataset, we computed the association between a pair of genes as the pearson correlation coefficient  of the two feature vectors representing these genes. we kept only positive pcc values and set negative ones to zeros. for protein interaction data, in addition to networks computed by using pcc, we also used the interaction networks directly.

RESULTS
we used 3-fold cross validation to evaluate the effectiveness of the proposed methodology  in predicting go functional classes for the two datasets and compared the results against those of two other methods . the performance of each method under test was measured by computing the auc score, which is the area under the receiver operating characteristic  curve. auc is a measure of choice when assessing the performance of methods that returns continuous scores such as the method we use in the prediction step. an auc score of  <dig> corresponds to perfect classification with negative examples successfully separated from positive ones, while random guessing results in an auc score of  <dig> . for each split of a dataset into training and test sets, we withheld 25% of the training set to use as a validation set for determining the stopping point of the learning algorithm. we computed auc scores on the validation set every  <dig> iterations, and stopped learning once the accuracy became saturated. in the following sections we report the auc scores averaged over three folds.

all three sampling strategies, i.e. uniform sampling, sampling less relevant negative genes with higher probabilities, and sampling more relevant negative genes with higher probabilities yielded similar auc scores but the third strategy was significantly faster than the first and second ones as it required fewer training iterations. in the following section we report results when the third sampling strategy was used.

results on the yeast dataset
comparison with sw
in the first experiment, we compared our method  with the sw method by mostafavi and morris  <cit> , using the yeast dataset. sw is a fast network-based method that achieved leading prediction accuracy in a number of gene function prediction benchmarks  <cit> . the sw algorithm integrates multiple networks, each of which is computed from a dataset, into a single combined network that it then uses to infer gene function. in sw, a combined network is a weighted linear combination of individual networks. sw formulates the network integration problem as a linear regression problem and simultaneously optimizes the weights over a group of related functional categories. because our method and sw use the same algorithm to predict gene function from an association network, the difference in network quality is the only factor that makes the accuracy of the two methods different. thus, the superiority in prediction accuracy of either method would mean that this method produces networks of better quality. we used the matlab implementation of sw provided by its authors with all parameters set to default values.

the auc scores of sw and slan for  <dig> go terms are shown and compared in figure  <dig>  out of  <dig> go terms, slan achieved higher auc scores than sw for  <dig> go terms, and sw scored higher or equally in the remaining  <dig> cases. over all  <dig> go terms, slan achieved an average auc value of  <dig> , and sw achieved an average auc value of  <dig> . the result of a wilcoxon signed rank test showed that the difference in auc scores between slan and sw was significant, with p-value =  <dig>  × 10- <dig> 

to understand the behavior of the algorithm, we inspected intermediate results of the learning step. we found that most go terms, for which slan achieved lower accuracy than sw, were associated with a similarity learning step that stopped immediately because it decreased the prediction accuracy. an explanation for this result is that the similarity functions cannot capture all the functional associations between genes, especially when these associations are complex. a typical situation, in which such complexity arises, is when each gene has multiple functions which themselves are related. it is also possible that the use of the graph-based algorithm that predicts gene labels from a network may inherently cause early cessation of the similarity learning step. this semi-supervised algorithm relies on the global structure of the network, which means that the solution depends on every association, including associations between negative genes. since the similarity learning procedure ignores such associations, it can decrease accuracy in some functional classes. in such cases, further learning would lead to undesired effects, which can be prevented by early stopping with the help of a held-out set.

comparison with hierarchical decision tree ensembles
in the second experiment, we compared our method with clus-hmc-ens  <cit>  - a recently proposed method that does not rely on association networks, thus represents another class of gene function prediction methods. clus-hmc-ens takes as input vector representations of genes and classifies genes into functional groups by learning an ensemble of decision trees. the trees are "hierarchical" in the sense that they exploit the hierarchy nature of go and each tree can make predictions for all classes at once. we used the implementation of this ensemble method provided by its authors. clus-hmc-ens was run with all default settings - the settings that provided the best performance in previous experiments.

results on the human dataset
in the next experiment, we evaluated and compared slan and sw on the human dataset. this dataset contains more go terms than the yeast dataset, and the number of positive genes annotated to a term ranges from three to  <dig>  because the prediction accuracy of a classification algorithm depends on the size of training data, we grouped the results into four categories corresponding to four groups of go terms with  <cit> ,  <cit> ,  <cit> , and  <cit>   positive genes, as done in  <cit> . in figure  <dig>  we summarize the average auc scores of each method for each of the four evaluation categories. as shown, slan scored lower than sw for go terms with  <cit>  positive annotations but achieved higher average auc scores than sw in  <cit>  and  <cit>  categories. the results also show that slan produced more accurate predictions than sw for the overall category  <cit> , which included all the go terms used in the dataset.

the fact that different methods achieve the best performance in different evaluation categories, as observed in this experiment, was also reported in  <cit> , suggesting that there is rarely a single method that delivers the best result in all situations. a possible way to achieve superior performance in all prediction scenarios is to use a combination of different methods. the superiority of slan over sw in categories with small number of positive annotations  shows its appropriateness for scenarios when few positive training examples are available. despite the fact that slan achieved lower auc value than sw in one of three individual categories, the superior accuracy of slan over all the go terms used  demonstrates its ability to improve networks produced by a simple network integration method with fixed and equal network weights.

prediction accuracy bias over the go functional groups
the results above revealed differences in the performances of the methods tested across go terms. for some go terms, the proposed method showed better prediction accuracy, whereas for other go terms, it gave less accurate predictions than other methods. given this observation, we asked on which go terms there are the largest variations in prediction accuracy between our and other methods. in our investigation, auc was used as the relative measure of performance for comparing slan and sw. we calculated the difference in the auc scores between slan and sw for each go term, sorted go terms in order of increasing difference, and examined those go terms with the largest auc differences. because auc =  <dig>  corresponds to a random guess, we set the minimum auc score for each method to  <dig>  by using max in the comparison, that is, Δauc = max - max. the lists of the go terms with the largest |Δauc| for the yeast and human datasets are given in figures  <dig> and figure  <dig>  respectively. the figures show large differences in prediction accuracy for some go terms, suggesting that each prediction method is more appropriate for certain functional groups. for example, on the human dataset, the term "response to active oxygen species " was accurately predicted by slan, and poorly predicted by sw. the difference in the auc corresponding to this term was nearly  <dig> . in contrast, the term "regulation of dna replication " was accurately predicted by sw, but poorly predicted by slan. we also observed that the largest |Δauc| from the yeast dataset were smaller than those from the human dataset, mainly because the subset of go terms used in the first dataset was smaller and less diverse  <cit> .

computational time
as mentioned above, the third sampling strategy required fewer training iterations than the other two in all experiments, suggesting that one should optimize the similarity function on more difficult training triplets first. in this section, we report the computational time when the third sampling method was used. on average, training of the similarity functions over all  <dig> functional classes on the yeast dataset using a uniformly weighted network input saturated after  <dig>  million iterations  and took  <dig> minutes on a single cpu of a modern pc running linux. in contrast, clus-hcm-ens took more than  <dig> hours to learn an ensemble of  <dig> decision trees on the same data set. since clus-hcm-ens is much faster than other classifier-based methods that create one binary classifier for each functional class  <cit> , these results suggest that our method compares favorably with classifier-based gene prediction algorithms in terms of speed. on the human dataset, similarity training stopped after  <dig>  million iterations on average and took less than two and a half hours. the running time on the human dataset indicated that although slan was slower than some network learning approaches, such as the ones proposed in  <cit> , its computational complexity is acceptable, even for gene function prediction in large mammalian genomes.

CONCLUSIONS
in this study, we propose a new method for optimizing functional association networks that are used in predicting gene function. while existing approaches focus on constructing combined networks from individual ones, the proposed method focuses on improving combined networks already constructed. by using similarity learning algorithms originally developed for multimedia search applications, our method can produce new association networks with improved prediction accuracy. in experiments with yeast and human, the networks optimized by our method yielded significant improvements in terms of auc scores, and the learning time was acceptable even for the large human genome. the results show that it is possible and useful to optimize combined networks before using these networks for prediction, and this optimization step can be performed by learning appropriate similarity measures from data. the proposed method can be applied to networks produced by any integration algorithm, thus provides a good complement for existing approaches. other applications of similarity learning, for example in computing network weights during the integration phase, will be investigated in future work.

competing interests
the authors declare that they have no competing interests.

authors' contributions
tmp conceived of the study, participated in its design, and drafted the manuscript. npn participated in study design, implemented the experiments, analyzed the data, and helped to draft the manuscript. all authors read and approved the final manuscript.

