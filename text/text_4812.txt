BACKGROUND
the increasing usage of high-throughput methods is changing the biosciences. the analysis of the resulting data often generates a list of genes that fulfil certain selection criteria. such a list can be, for example, a cluster of co-expressed genes, genes up-regulated in disease samples or genes representing a similar phenotype in a knock-out experiment. the resulting gene lists are often too large for direct manual analysis, and they regularly contain many false positive genes. therefore, it is standard practice to use large scale functional classifications, like gene ontology  or mips funcat  <cit> , to aid the analysis. these functional classifications in combination with statistical analysis methods enable the filtering of occasional false-positive genes, emphasize over-represented functional classes and generally facilitate the analysis . this has taken the analysis of biological function from the single gene level to the more informative biological process level. functional classes can also be used to select the optimal set of clusters  <cit> , to find heterogeneity in the expression of functional groups  <cit> , to evaluate clustering results  <cit> , to analyze differential gene expression at the gene class levels  <cit> , as an input data for the prediction of interacting genes  <cit> , or to analyze the functional heterogeneity of the genes in the reported gene list  <cit> . indeed, the field has seen an explosion of methods, which aim to report the most important functional features for a group of genes or proteins  <cit> .

despite its significant benefits, the standard enrichment analysis of functional classes has still some notable unsolved problems. good quality annotation is one of the most critical requirements for class data analysis. this quality can be weakened by the lack of information or disinformation in the class annotations for the analyzed organism  <cit> . the functional class analysis also requires gene id mapping. this is often a nontrivial task as there are frequently id matching and conversion problems . the id matching and conversion problems again increase the disinformation in the used annotations. one further challenge in the analysis of the over-represented functional classes is the existence of multiple biologically very similar go classes. these are caused by the strong correlations between the analyzed functional classes. the repetitive occurrence of similar classes related to the same biological theme often masks other weaker but biologically equally relevant themes from the end user  <cit> . further problems in the analysis with functional classes include the selection of a suitable null hypothesis  <cit> , sharp binarization of continuous data for over-representation analysis  <cit>  and others  <cit> . however, these are considered to be outside the scope of this manuscript.

the wealth of available go analysis methods and the analysis related problems raise the need for detailed evaluation of the available go analysis tools. indeed, publications simply often report findings from some real-life datasets, by the promoted method. however, this does not unambiguously quantify the performance of the compared methods. such quantification would require that we would clearly define positive and negative features and test methods on how well they can be separated.

there are two potential ways for a detailed evaluation of go analysis. one can do the analysis using various very well known datasets, and test whether the obtained results correspond with earlier knowledge. the drawback of this method is that currently there are, to our knowledge, no datasets available, where all the reported functional classes could be classified clearly as either true or false positive. this makes it hard to quantify the performance of compared methods with real data. furthermore, this method does not allow repetitive testing with a large number of positive and negative datasets. another option would be to generate a large set of artificial datasets with both positive and random signal and to see how well the analysed method separates them. although the generation of negative signal can be a simple random sampling from the gene pool, the generation of a positive test signal has not been clearly discussed in the literature.

in order to aid the comparison of various methods, the current work proposes a novel methodology for creating benchmark go datasets with known over-represented  classes. posgoda  takes as an input a large set of genes with go classifications. this set can be, for example, all known genes in the analyzed genome or all the genes from a popular gene expression chip. datasets can be created with varying size of gene lists, with varying level of over-representation signal and with varying number of independent signal classes. to compensate the larger overall signal, which automatically arises from the larger number of independent signal classes, posgoda allows the choice between a number of methods for normalizing the overall signal. note that by starting with a real classification, the generated datasets will represent all the features present in the real data, such as class correlation and hierarchical class structure. an earlier work  <cit>  used the term plasmode dataset to separate similar gene expression datasets from purely artificial datasets.

the inputs for posgoda are the signal range of p-values, the size of the output gene list, and the number of independent signal classes. its outputs include a list of independent go terms, selected as positive, a gene list which shows signals in the selected classes, and the p-value signal embedded in the go class. posgoda takes the input signal range of p-values and, if required, adjusts them using the number of signal classes. next, it selects the independent signal classes from the randomly ordered go class list. these independent classes are tested, one at the time, looking for the class over-representation level that is closest to the selected signal level. if the resulting signal from the class deviates too much from the desired signal level, the search is redone a few times with the same class, before moving to the next class in the class list. this is repeated until the desired number of signal classes is reached. once the signal classes have been defined, posgoda creates the gene list, with the required number of members from each signal class. if this resulting gene list is smaller than the desired gene list size, posgoda adds randomly selected false positive genes to obtain the required size.

related work has been presented before  <cit> . however, there are major differences. i) posgoda defines the added number of class members based on the resulting hypergeometric p-value, which has to be within the signal range given by the user. earlier works defined this as raw percentage of class size, which will generate different signal levels for different size of classes and gene list. ii) posgoda controls the potential correlations between the classes that yield a positive signal and furthermore the potential correlation between go terms via a third intermediate class. these limitations ensure that we generate two separate signals instead of multiplying the same signal twice, and thus generating a single stronger signal. iii) posgoda includes various methods that can be used to scale the signal, when embedding it to a larger number of signal classes. here, the scaling helps the positive signals to stand out from the large pool of random signals. however, it should noted that posgoda can be used also without the signal scaling.

we hope that posgoda will improve go analysis especially by allowing the repetitive testing of tools on artificial datasets. these enable the evaluation using statistical methods, like the receiver-operating characteristics  curve or t-test to quantify the separation between positive and negative features or datasets. furthermore, the analysis can be repeated with different signal parameters, to see areas of optimal performance for various methods. in addition, we highlight specific topics in which posgoda can be used:

• the generated datasets can be used to test the existing or novel methods that filter correlations from go structure  <cit> . this shows how well they are able to report positive go classes or go classes that are strongly linked to positive go classes.

• the generated datasets can be used to test go data clustering methods  <cit> , as genes belonging to different positive classes are expected to represent separate clusters or cores for clusters. datasets can be used also to test clustering of go classes.

• evaluating how well asymptotic methods  approximate the hypergeometric test.

• testing and evaluating methods and their parameters during method development.

it is also relevant to notice that there are two issues that posgoda cannot evaluate. these are the correctness of the input go data matrix, and the testing of enrichment scoring functions that are potentially better than the hypergeometric p-value.

we demonstrate the usefulness of posgoda by generating a large pool of datasets with positive signals for testing various go analysis tools. the main emphasis in our comparison is on the evaluation of various methods that filter correlation from the go structure. our results show significant variation between methods in their ability to emphasise the independent over-represented go classes.

RESULTS
due to the nature of the current work, the results section first shows some of the features of the positive datasets generated by the methodology. we briefly describe the workflow of posgoda, as a part of the results. the details on each step are given in the materials and methods. further details are given in supplementary text. we also demonstrate the usage of the generated datasets in two comparisons. the methods, some of the results from the performed comparisons and demo datasets are available from our web site .

complete method workflow
posgoda requires from the user a binary go matrix , where one denotes the membership of the gene in the specific class. with this real go dataset we make sure that our data has the type of structure that actual biological datasets also have. posgoda also requires the user to calculate the nclass * nclass correlation matrix that represents the correlation of the go classes, and an estimate for effective number, neff  <cit> . nclass refers here to the exact number of classes, whereas neff refers to the estimated actual number of independent classes . the correlation matrix is used to evaluate the independence of the signal classes and neff is used to scale the signal levels. a standard matrix rank function is used for the estimation of neff throughout the current work.

the user is required to input the minimum and maximum p-values, which define the signal range; the size of the reported gene list; the number of signal classes; and the method for normalizing the signal with multiple classes. here normalization refers to inverse function for selected multiple testing correction.

the first steps of posgoda are shown in figure  <dig>  a more detailed description in pseudocode is given in the supplementary text s <dig> . posgoda starts by normalizing the minimum and maximum p-values, given by the user . the normalization depend on the effective number and the selected signal normalization method. then posgoda selects suitable functional classes by looking at the randomly ordered classes, and excluding classes that have unwanted correlation with classes already included in the class list . the resulting go class list will be then used as an input to step  <dig>  a target signal level is randomly selected between the minimum and maximum p-values  after this, posgoda tests whether it is possible to obtain a close enough signal with the selected class . this might not be possible with classes having very few members or classes that include practically all the genes. if the search fails, new signals, sampled from the signal range, are tested with the same class a few times , before moving to the next class in the class list . this algorithm reports the required number of genes from one class for the suitable signal level. if the desired number of signal classes cannot be found, the class list is re-sampled and the whole search process is started again . as different classes have a different number of correlating classes, the order in which the classes are selected in the random process affects the number of available independent go classes. however, in most of our tests, the first or second random list of classes generated adequate results.

the final steps of the workflow are shown in figure  <dig>  these steps select the actual genes to the generated gene list. here, the required number of genes is selected from the class members to the gene list. selection favours genes that are not members of any other signal class. this avoids the addition of extra members to other signal classes, while selecting members for one class. finally, the size of the resulting gene list has to be controlled. too small a gene list is corrected by adding genes that do not belong to any of the selected signal classes  to the gene list. too large a gene list is corrected by redoing the whole search process.

as an output, the method reports the binary vector for genes represented by the go matrix, having one for the genes that were selected to the gene list. also a list of column numbers for the classes having the positive signal is reported. furthermore, our supplementary functions can be used to print the corresponding gene names or positive class names to text files. the whole program is coded in matlab . the purpose of using a script language is that other method developers could easily create scripts for repetitive testing of various methods and also to allow easy modification of the generated method.

signal level evaluation
throughout the first analysis steps we use go data for a subset of the yeast genome  with all the three subsets of go . we use a gene list  size of  <dig> genes and matrix rank to estimate neff. we also calculated the binary correlations for this test dataset. the dataset, the calculated correlations and neff estimates are available in the supplementary web page. note that the results should not be affected by the selection of different species, selecting various subsets of the genome pool or using only one subset of go or even a totally different hierarchical classification dataset.

first, we wanted to know if we are able to create the correct signal level for the selected signal classes, when we scale the signal using the selected p-value combination methods. to confirm this, we produced  <dig> datasets with i)  <dig> or  <dig> signal classes ii) using fdr or holm's method for signal normalization iii) with p-value signal ranging from  <dig>  to  <dig> . the process creates altogether  <dig> signal classes for evaluation, when  <dig> signal classes was used and  <dig> classes when  <dig> signal classes was used. table  <dig> summarizes the results for each dataset. what we hope to see is that when we reverse the signal normalization, we would have p-values in the aimed signal range. table  <dig> shows the selected percentiles from each dataset for p-values calculated for signal go classes with and without the signal normalization.

table shows the summary of results with reported percentiles. signal generation is tested with  <dig> and  <dig> positive classes and with signal normalizations that use holm's method and fdr. note that  <dig> percentile limit and  <dig> percentile limit stay within the required  <dig> - <dig>  range. minimum and maximum show deviation from the signal range but the deviation is always only appr. 10% of the allowed signal.

the results show that 90% of the data fall within the aimed  <dig>  -  <dig>  signal range. furthermore, the deviations from this signal range are quite small. the deviations are approximately 10% from the aimed signal level, which should not disturb the analysis based on these datasets. note that we are bound to have some error as we are trying to match a discrete variable  with a continuous signal level. therefore posgoda has been designed to allow a small error from signal range.

signals obtained from one example dataset
the signal that is obtained from the data is illustrated in detail with the top-scoring go classes for one of the datasets in table  <dig>  data was produced with fdr normalized signal levels and with five signal classes. the required signal level was set to . table  <dig> shows the reported classes. what can be seen immediately is that embedding a signal in one class usually causes a similar signal also in other similar classes. these classes show strong correlation with the original signal class. the repetitive occurence of go classes representing very similar functions is often also noticed in the real datasets  <cit> . these results confirm the benefits of filtering correlating classes, as many of the strongly correlating classes represent practically the same signal. without the filtering, one could have selected repetitively these almost identical classes as signal classes, resulting in the multiplication of the same signal. furthermore, one can see that the number of classes with strong correlations varies significantly. for example, we see four classes having correlation stronger than  <dig> , and additionally three classes having correlation stronger than  <dig>  with class 'rna ligase activity', whereas the class 'response to inorganic substance' has not got any correlations with other classes in the result list. table  <dig> also demonstrates that the selected signal classes do not correlate with each other. this is natural as posgoda does not accept signal classes that are correlated with each other.

table shows  <dig> most over-represented go classes from the dataset, ordered with the log <dig>  dataset was generated with artificial signal embedded to five classes, shown with bold font in the class list. table shows also correlation of each reported class with these signal classes. the clear strongest correlation is shown here with bold font. signal classes are marked in columns using their ranking in the result list. notice the varying number of strong correlations that different signal classes have.

comparison of different go class ranking methods
here we demonstrate the usefulness of posgoda by comparing different go class ranking methods. we compare the ability of different methods to report positive classes among their top k classes . we compare three methods: standard go class list obtained from the david server  <cit> , and the parent-child  <cit>  and the topology-elimination  <cit>  algorithms implemented in ontologizer software  <cit> .

it is current standard to analyze the obtained results from class over-representation methods using the number of positive classes over different ranks in the sorted class list  <cit> . however, we take a slightly different approach and focus on the difference in the cumulative sum of positive classes between two compared methods. this analysis puts emphasis on the difference in results, rather than the actual results. furthermore, we plot the various percentiles  in order to show how stable the difference is between the compared methods.

most over-representation analysis methods may report a class that is close in the go class structure to the positive class instead of the exact positive class. we considered that these should be also included in the evaluation. however, we wanted to down-weight their contribution to the final result. therefore, we decided to weight these classes according to their correlation with the correct signal class. this is a unique, simple and intuitive method that automatically evaluates how similar the reported class is to the correct signal class.

figures  <dig>   <dig> show differences in cumulative sums, when both ontologizer algorithms are compared with the standard go list. figure  <dig> shows that the topology-elimination algorithm clearly outperforms the standard list, especially across the top- <dig> ranks. this is highlighted by the 25th percentile, showing that in three quarters of the test runs topology-elimination outperformed the standard sorted go list.

in summary, the comparison suggests that the best performance is obtained with the topology-elimination algorithm whereas the difference between parent-child algorithm and standard go list was not clear cut. however, this analysis should be repeated using different parameters for artificial data generation to further confirm the results.

comparison of two go data clustering methods
we further demonstrate the usefulness of posgoda by demonstrating its use in the comparison of go clustering methods. here our aim is to see how well the generated clustering can separate the embedded signal classes, and also to monitor how large a portion the positive classes, or positive clusters, form of the clustering output.

we selected two algorithms, with different approaches, for our evaluation. one method, called generator  <cit> , clusters genes using the available go data. these gene clusters are then used to look for over-represented go classes . another method, implemented in the david webserver  <cit> , clusters the same data, but instead of clustering genes it clusters go classes. it combines the correlating classes together as a single cluster. both methods aim to represent the heterogeneity of the reported functional annotations and to limit the number of correlating classes that are seen with the over-represented classes.

there is no fixed way currently to the analysis go clustering tools. therefore, we propose a few simple rules for evaluation:

• go clustering tools try to predict the positive classes. therefore we have used prediction related measures, positive predictive value , sensitivity and f <dig> score, to quantify the prediction performance.

• we selected three top go classes from each cluster as the result, i.e. the predicted positive go classes.

• although we ranked clusters within the clustering, we decided to treat them here as equally good.

we omit details from our analysis. briefly, generator generated between  <dig> to  <dig> clusters, whereas david generated between  <dig> to  <dig> clusters. this suggests that generator clustering is closer to the correct cluster number, five. david, on the other hand, usually ranked the positive class better in the ranked list of the reported clusters. we summarize the results in table  <dig>  the table shows the portion of true positive go classes among the predicted positive classes, represented by ppv , and the portion of predicted positive classes of the positive classes, represented by sensitivity. these two measures are combined in the f <dig> score. the results suggest that generator performs clearly better with this type of datasets. although its sensitivity is slightly smaller, the ppv for generator is frequently ten times better. weaker sensitivity might result from the selection of single clustering level from the generator output for the evaluation . however, this could be corrected in manual analysis by monitoring several levels from the reported non-nested hierarchical clustering  <cit> .

table presents the positive predictive value , sensitivity  and f <dig> score for generator and david clustering results. furthermore, we show the difference of f <dig> scores between two methods. notice that the f <dig> score for generator is consistently better, as is shown by the difference in the last column. david outperforms generator only in sensitivity, but this is natural as david created 3- <dig> times more clusters.

discussion
we have presented a methodology  with supplementary tools for generating positive go datasets. the results suggest that posgoda generates datasets with the required signal. we present the analysis of effective number, a variable used for scaling the signal, similarly to earlier work  <cit> . however, we have used here a simpler approach, implementing the matrix rank function. currently, it still seems to be common to use the number of classes as the normalizing variable, although the number of classes ignores the correlations in the go datasets, and is often bound to create a too strong correction to signal levels. our results in table  <dig> illustrate this, as randomly selected classes can have up to  <dig> strong correlations.

posgoda includes a simple unique evaluation of the independence of the go signal classes. this evaluation is based on the correlation of the go classes  <cit> . this measure is independent from the graph distance between the go classes and simply answers to the question: how similar are the classes being compared? the measure can be used also to compare, say, mips functional classifications, swissprot keywords and go classifications with each other. also, the whole proposed methodology could be similarly used with any binary classification matrix or combination of binary matrices.

there are a number of ways to use posgoda. it could be used to evaluate methods on how close the reported p-values get to the p-value signals used in the posgoda data generation. however, such analysis would require that the data matrix used by posgoda comes from a very reliable source. the correctness of signal levels also depend on the effective number and the used signal correction method. furthermore, potential differences could be caused by the exclusion of some go evidence codes in the evaluated method and there are no 'golden rules' stating which evidence codes should be included in the analysis and which should be excluded. therefore, we chose to monitor the rank of positive go classes in the reported go class list. ranked list analysis is a robust approach that omits the actual p-value scores. it also resembles the explorative analysis frequently done with go analysis tools. our unique and simple addition to this analysis was the inclusion of go classes that show strong correlation with one of the positive classes, as also positive. however, we weighted them using the correlation with the correct positive class. this is a novel, simple and intuitive way to downweight those classes as weaker hits than the correct positive classes.

our comparison also included methods that generate clusters from go data. here we again looked for the ability of the methods to select the signal classes or other class that correlates strongly with a signal class. the results suggest, with clear difference, that the clustering created with generator is better than the clustering by david. however, the results might change when testing with different dataset parameters and if false positives and false negatives are not weighted equally.

the compared tools used different versions of go, with the david server having go structure from jan  <dig>  the differences between this and newer go structures might explain part of the differences between methods. however, out of all the classes that were in our test dataset, less than a half a percent was missing from the oldest go structure . furthermore, this effect is lessened in our analysis by counting also strongly correlating classes as positive. this helps, as the old version of go structure has most likely already neighboring classes which correlate with added class.

posgoda has potential further applications. one could link the genes with differential expression score and compare the various go based threshold free gene expression analysis methods. this would simply require the selection of a potential model for positive and negative signal, like in some earlier research  <cit> . our method differs from these two publications in that these works tested for the performance at the artificial go class level, whereas our test datasets allows testing with the whole correlation structure of go. we have implemented posgoda for monitoring the separation between the positive and negative go classes within each positive go dataset. however, it would be interesting to test various methods that summarize the total signal across all the go classes  <cit> . these methods could be tested by using positive datasets, generated by posgoda, and similar random samples from the background population. such analysis, however, would require testing of a large number of artificial datasets with varying parameters. finally, we wish to point that the evaluation using artificial datasets cannot replace the evaluation done with real datasets, which should be used in combination with artificial datasets in evaluations.

CONCLUSIONS
we hope that the proposed methodology would encourage the scientific community to more thoroughly test the various methods available for various functional genomics and go datasets, and to stimulate discussion about the possibilities on testing these methods. furthermore, we want to draw attention to some of the problems in the positive data generation, observed when using the real data go matrix.

