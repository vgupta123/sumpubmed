BACKGROUND
a common task in computational biology/bioinformatics and computational chemistry/chemometrics  is to model a dependent variable from a set of independent variables; this gives insight into the workings of the process being modeled and enables prediction of future observations. typical examples include analyzing potential drug activity through proteochemometrics and quantitative structure-activity relationship  modeling  <cit> , discovering gene regulatory binding-site modules  <cit> , and predicting clinical outcomes of cancer from gene expression data  <cit> . however, recent articles have indicated that predictive modeling approaches have not fully fulfilled expectations for solving real problems. this issue has for instance been discussed in the fields of qsar  <cit>  and microarray gene expression data modeling  <cit> . while some of the problems may be attributed to incorrect use and interpretation of the models, others can be ascribed to improper model selection and assessment. our aim is here to address the latter issue by introducing the c1c <dig>  a general framework for model choice and assessment.

let d = {xn, yn} be a dataset, where xn = ' is an n × px matrix whose ith row, xi, is the value of a px-vector of independent variables associated with yi, the ith row of the n ×  <dig> matrix, y = '. a statistical model, ml can be used to characterize the relation between xn and yn. in general, given the dataset d, ml must be chosen from a set of models, m={m <dig> ...,mm,...,mm} according to some criterion . the most common way to select ml from m in bbcc is to use k-fold cross-validation; that is, the dataset d is split into k mutually exclusive subsets, d <dig> ..., dk,..., dk, of approximately equal size and ml <dig>  is picked to minimize the function:

  c0=1n∑k=1k∑i=1ndkl), 

where l is a loss function and d-k denotes that the kth subset was excluded from d during the model fitting; ndi is the number of observations in subset di; and m =  <dig> ..., m, where m is the number of models in m. this model selection method may seem straightforward and intuitive, however it neglects the fact that all the data at hand is used to make the model choice. thus, we no longer have an independent testset to assess the chosen model by. the result is that, typically, c <dig> underestimates the generalization error , defined as the expected prediction error over an independent test sample. this problem has been highlighted in relatively recent works  <cit> , but was noted initially in  <dig>  <cit> . to obtain a more accurate generalization error estimate, the model selection process must be separated from the model assessment in terms of the data that is used. ideally, if data were abundant and easily produced, we would set aside a large test dataset and use it to assess – but not to choose! – the model ml, and subsequent model refinements could be assessed with new, unseen data. in practice, this is however often impossible since bbcc data is typically scarce, and expensive to produce. the luxury of large independent testsets can thus rarely be afforded. to tackle this problem, freyhult et al.  <cit>  suggested using a k-fold cross-validatory assessment of an h-fold cross-validatory model choice, ml†, as a way of simultaneously choosing ml and assessing its performance; thereby separating the model selection from its assessment. the model is assessed by the function

  c†=1n∑k=1k∑i=1ndkl,d−k)), 

where ml†  is the cross-validatory choice of ml based on d-k; that is, the model ml that minimizes the function

  cd−k†=1nd−k∑h=1h∑j=1ndhl), 

where d-hk denotes the dataset d with the kth and hth subsets omitted. in the present work we build on and generalize this idea into the c1c <dig> framework. in general, the number of models in m is huge, thus it is unfeasible to go through even a small subset of them manually. hence, for a framework such as the c1c <dig> to be useful in practice, automated methods for searching the model space m are necessary; in this sense the c1c <dig> is similar to the automatic modelling approaches taken in for instance  <cit> . here, the specific use of the c1c <dig> is demonstrated by applying it together with two search methods to simulated and real-world datasets. the results are compared to those obtained by employing the function  for model selection and assessment. in the interest of clarity, we have restricted our attention to the study of model choice and assessment within a linear model class, mridge  for a quadratic loss function. we discuss the results of the demonstrations, the pros and cons of the generality of the c1c <dig>  and set out some directions for further research.

RESULTS
algorithm
let c <dig>  c <dig> ∈ c = {c <dig> ..., cj}, where c is a set of model assessment criteria and c <dig>  c <dig> represent two specific criteria . further, let s ∈ s, where s is a set of search methods; let l ∈ l, where l is a set of loss functions; let g denote a sequence of data processing steps  and let g' contain the results of g applied to d-k ; let r be a positive integer and k an integer between  <dig> and n, where n is the number of observations. the c1c <dig> procedure is outlined with the following pseudocode:

initiate m, g, l, c <dig>  c <dig>  r, and k.

for  {

   a. partition data, d = {dk}k =  <dig> ..., k.

   for  {

      b. apply g to d-k. save results in g'.

      c. search m using the data d-k and c <dig> as objective function. assume ml is found to maximize  c <dig>  save ml.

      d. apply g to dk using g'.

      e. assess ml using c <dig> and dk. save assessment result.

   }

}

the data partitioning in  separates data for the model choice from data for the model assessment. note that the partitioning is dependent on the choice of c <dig> and does not necessarily need to be done in a cross-validation fashion. for instance, the choice c <dig> = ". <dig> estimator"  <cit> , partitions the data by independently sampling n rows from d with replacements and lets the observations not included among the sampled observations constitute the test set. the output from the c1c <dig> is also dependent on the choice of c1; for example, the choice c <dig> = c <dig> = bayesian information criterion  would not give a direct estimation of the generalization error, but rather an assessment of model overfitting. to clarify the roles of g and g', we give the following example: let g only contain a processing step that scales to unit variance. in  g is applied to d-i and the standard deviation of each column of d-k is saved in g'. in , g' is applied to dk, that is, the columns in dk are scaled using the standard deviations calculated in . this treatment of g ensures that dk indeed constitutes an independent testset. the 'for loop' over r is introduced to enable calculation of confidence intervals for estimates and, by averaging the estimates over r repetitions, it permits reduction of the variance in parameter and error  estimates by a factor of 1/r.

datasets used in the testing
both the simulated and the real data used for evaluating a new method or algorithm should reflect typical dataset properties found in real-world application domains. examples of such properties in bbcc are multicollinearity, a large number of independent variables relative to the number of observations, and binary and categorical independent variables.

simulated data
we simulated datasets as follows:

let Δ = i =  <dig> ..., p, where δi={1 iff xi is in the model0 else, represents a subset of xn, and let βp = )i =  <dig> ..., p, where βi={βi≠0 iff δi=10 else, is a vector of regression coefficients. the data matrix, xn was sampled from a twenty-dimensional multivariate normal distribution. thus, xi ~ n <dig>  i =  <dig> ..., n, where  <dig> is a twenty-dimensional vector of zeroes and Σ <dig> is either the i <dig> identity matrix or a covariance matrix, s <dig>  estimated from a real in-house qsar dataset that originated from hiv protease inhibitors. the hiv qsar dataset contains highly correlated independent variables resulting in an s <dig> with many large absolute values in the off-diagonal elements. three δi in Δ were chosen to be nonzero and equal to one; the positions were chosen at random to be  <dig>   <dig>  and  <dig> . the corresponding regression coefficients, β <dig>  β <dig>  and β <dig>  were, for simplicity, all set equal to  <dig>  the variables  <dig>   <dig>  and  <dig> were slightly correlated with an estimated covariance matrix Σ=..

datasets were generated assuming that yn followed a linear model according to: yn = xn βp + εn, where εi~n. four datasets were simulated in order to evaluate the c1c2s performance in settings where n <px, n > px, and where the observations were sampled from an orthogonal multivariate normal distribution or not, according to the following schema:

 <dig>  n =  <dig>  Σ <dig> = i20

 <dig>  n =  <dig>  Σ <dig> = i20

 <dig>  n =  <dig>  Σ <dig> = s20

 <dig>  n =  <dig>  Σ <dig> = s20

the simulated datasets are available in csv format from additional files  <dig>   <dig>   <dig>   <dig> 

the selwood dataset
this is a real dataset, made available from a website  <cit>  and originally published in  <dig>  <cit> . it is a widely studied dataset . it contains one dependent variable,  <dig> independent variables, and  <dig> observations. the  <dig> independent variables correspond to numerical descriptions of molecules  designed to capture their physicochemical properties. the dependent variable is the in vitro antifilarial activity of the molecules. this dataset exhibits extremely strong correlations between the independent variables and contains real valued, binary and categorical independent variables. it is known from previous studies that this dataset contains nonlinearities, but that decent models can be found using linear methods.

testing
to demonstrate the use of the c1c <dig>  it was applied to the simulated and real-world datasets described above . below we describe the choices for r, k, m, g, l, c <dig>  c <dig>  and s and the motivation for each selection.

choice of r and k
the larger the choice of r, the higher the accuracy in the estimate of the generalization error; the choice of r is thus constrained by time and is dependent on the size of the dataset and the computational complexity of the choices of m, g, l, c <dig>  c <dig>  and s. the choice of k is a trade-off between bias and variance; the larger the k, the more variance and the less bias in the estimates of the generalization error  <cit> . r was here set to  <dig> and k to  <dig> 

choice of m
we make the assumption that a normal linear model forms a reasonable approximation of the data. this model is given by: yn = xn βp + εn, where the subscripts denote the number of rows in a matrix, βp are regression coefficients, and εn~n. further, because n <p and the columns in xn are highly correlated in some of the datasets, we decide to use the ridge estimator, β^pridge , of the regression coefficients, βp. let mridge be the linear class of models given by: yn = xn βp + εn, where βp is estimated by β^pridge. we thus choose m=mridge. the problem of model choice within mridge reduces to the problem of variable selection, i.e. choosing which subset of the p columns in xn to include in the model, and the problem of choosing the ridge penalizing parameter λ . hence, letting Δ = i =  <dig> ..., p  represent a subset of xn, we want to choose Δ and λ using the c1c <dig> framework. a choice of Δ and λ for given values of r and k will be termed "an estimate" of Δ and λ, respectively, and be denoted Δ^ and λ^. averages of estimates over the k folds and the r repeats in the c1c <dig> are denoted Δ^¯ and λ^¯, respectively.

choice of g
as the columns in the selwood dataset are measured in different units using different scales, we choose to make g contain mean centering and scaling to unit variance processing steps.

choice of l
we use the standard quadratic loss function given by:

 l)=) <dig>  

choice of c <dig> and c2
others  <cit>  have suggested choosing c <dig> = c <dig> = cross-validation. here, we choose c <dig> = cross-validation and c <dig> = bic. hence, in this demonstration we assess a model choice mlridge∈mridge according to:

  c1=1n∑k=1k∑i=1ndkl,d−k)), 

where mlridge,bic  is the mlridge chosen according to bic based on d-k; that is, the value of mlridge that optimizes the function:

  cd−k2=log⁡p−df2log⁡nd−k, 

m =  <dig> ..., m, where m is the number of models in mridge. df in  is the number of free parameters in the model mmridge . the choice of c <dig> is motivated by that we wish to get a direct estimate, ε^gen, of the generalization error, εgen, of our model choice. provided that the assumptions behind bic are fulfilled, the choice c <dig> = bic has several advantages over c <dig> = cross-validation, including: a reduction of bias in parameter estimates  <cit> , a reduction of variance by the rao-blackwell type relation derived in  <cit> , and a drastic reduction of the computational cost of the procedure.

choice of s
a genetic algorithm  was chosen as a search method because it is very easy to adapt to different situations and in general effective for nondeterministic polynomial-time hard combinatorial problems, such as the problem of estimating Δ  <cit> . a trial solution in the ga is here a varying length chromosome that contains a real-valued number representing λ and a vector of integers representing the indices of the δi in Δ that are nonzero. the fitness function is our choice of c <dig>  for the simulated datasets, we also chose to run the c1c <dig> with a brute force search method: for each λ ∈ { <dig> . <dig> . <dig> ...,10} an exhaustive search in the Δ space  was performed. this enabled comparisons between the ga method and a search method guaranteed to find the optimal model .

some remarks regarding the demonstration
to enable comparisons with the estimates of Δ, λ, and εgen obtained with repeated k-fold cross-validation, the demonstration described above was repeated with the function  used as a criterion for model choice and for assessing the model. note that, since the c1c <dig> includes the 'for loop' over r,  was repeated r =  <dig> times, each time with a new, independent data partitioning. this was done to facilitate an impartial comparison between the two methods.

the demonstration of the c1c <dig> framework can be compared with the work of for instance nicolotti and carotti  <cit> , where a genetic algorithm was employed to estimate Δ. in contrast to that approach, the c1c <dig> framework completely separates model choice and assessment whereby more accurate generalization error estimates in general are achieved. further, the use of specific ad hoc objective functions is avoided by choosing c <dig> to be a formally derived model selection criterion, and simultaneous estimation of model parameters other than Δ  can be afforded. typically, in works that have employed a search method for estimating Δ, a given number of nonzero δi in Δ is assumed . therefore it was of interest to investigate the effect of this assumption on producing good estimates of Δ and εgen. this can be tested for the simulated datasets in the demonstration, where the number of nonzero δi is known. the c1c <dig> was therefore applied to the simulated datasets both with an assumption about the number of nonzero δi and without the assumption. for simplicity , we assumed the correct number of nonzero δi.

results of the testing
simulated datasets
the four simulated datasets in combination with the use of either the c1c <dig> or repeated k-fold cross-validation for model choice and assessment, the ga or the brute-force search method, and either with or without the assumption of prior knowledge of the number of nonzero δi constitute a two-level, five-factor, full factorial experimental design. the c1c <dig> and the repeated k-fold cross-validation were applied four times to each factor combination, thus providing four replicates of the whole demonstration for the simulated data. the design can be analyzed within the normal linear model

  wiv = γ <dig> + γ <dig> z1i + γ2z2i + γ3z3i + γ4z4i + γ5z5i + η i , 

where zji, j =  <dig> , <dig> , <dig>  are factors corresponding to c1c <dig> or repeated k-fold cross-validation model choice and assessment, brute force or ga search, Σ <dig> = i <dig> or Σ <dig> = s <dig> in the multivariate normal distribution from which the data was sampled,  <dig> or  <dig> observations, and assuming three nonzero δi or no such assumption, respectively. i goes from  <dig> to  <dig> in  as there are  <dig> factor combinations in four replicates. wiv, v =  <dig> , <dig>  are response variables defined according to the following: the euclidean norm wi <dig> = ‖Δ^¯−Δ‖i was used to measure how well Δ on average was estimated, wi <dig> = λ^¯i was used as a response variable in the λ case , and wi <dig> = |ε^gen−ε˜gen|¯i was used to measure how well the generalization error ε on average was estimated; ε^gen denotes the estimate of εgen for given values of r and k; ε˜gen=1n∑j=1n) <dig> denotes the generalization error estimate obtained by using the corresponding choice of model, ml, to predict the response values in a large  external test set, generated in the same way as the dataset used for choosing the model and estimating ε^gen; the bar denotes the average over the r·k individual estimates. the generalization error can be decomposed into three parts: one irreducible error , the squared bias, and the variance. the latter two are dependent on the model choice and consequently the generalization error is dependent on the model choice. we here assume that the large-sample estimate of the generalization error, ε˜gen, closely represents the true generalization error, ε, for a given model choice.

the results for choosing a model mlridge∈mridge for the simulated datasets are available in additional file  <dig>  where ‖Δ^¯−Δ‖, λ^¯, and |ε^gen−ε˜gen|¯ are tabulated for each factor combination and replicate. the parameter estimates for fitting the model  using ‖Δ^¯−Δ‖, λ^¯, and |ε^gen−ε˜gen|¯ as response variables are shown in tables  <dig>   <dig>  and  <dig>  respectively. all fitted models were highly significant ; residual plots showed no large deviations from the assumptions of normality of error distribution , homoscedasticity, and independent errors . a few outliers were however observed, probably resulting from "unfortunate" data partitions.

c1c <dig> – the c1c <dig> was used , ga – the ga search method was used , cor – correlated independent variables in the dataset   <dig> – n =  <dig> observations in the dataset , all – no assumption regarding the number of nonzero δi .

see table  <dig> for notation explanation.

see table  <dig> for notation explanation.

selwood dataset
applying the c1c <dig> to the selwood dataset yielded Δ^ that on average contained  <dig>  ±  <dig>  nonzero δ^i. the  <dig> most frequently picked variables were the partial atomic charge for atoms atch <dig>  atch <dig>  and atch6; electrophilic superdelocalizability for atom esdl3; the van der waal's volume vdwvol; the surface area surf_a; the principal moments of inertia mofi_y and mofi_z; the principal ellipsoid axis peax_x; the partition coefficient logp; and the sum of f substituent constant sum_f . the estimation of λ by λ^¯ was  <dig>  ±  <dig>  and the estimation of the generalization error by ε^¯gen was  <dig>  ±  <dig> , where ε^¯gen is an average over the r·k ε^gen produced in the c1c <dig> 

applying repeated k-fold cross-validation for model choice and assessment to the selwood dataset gave on average  <dig>  ±  <dig>  selected variables. the  <dig> most frequently picked variables included the same  <dig> variables picked by the c1c <dig>  plus dipmom , atch <dig> , and dipv_y . the estimation of λ by λ^¯ was  <dig>  ±  <dig>  and the estimation of the generalization error by ε^¯ was  <dig>  ±  <dig> , where ε^¯ is an average over the r·k ε^ produced in the repeated k-fold cross-validation.

implementation
computer programs to implement the c1c <dig> were written in java  as a part of the library p, that will serve as the data analysis plugin for bioclipse  <cit> . p is available under the gspl license from the website  <cit> ; it is open source and free for academics. it has a modular architecture that enables plugging in new features, including modeling methods, model selection criteria, and search procedures. p relies on a modified version of the jgap library  for the genetic algorithm computations . the r-package, pvclust  <cit> , was used for the cluster analysis .

discussion
simulated datasets
the model  fitted to w = ‖Δ^¯−Δ‖  showed a relatively clear significant difference  in average Δ estimates depending on whether the data came from a multivariate normal distribution with Σ <dig> = i <dig> or Σ <dig> = s <dig>  furthermore, we observed significant positive impacts on average Δ estimates with more observations and knowledge about the number of nonzero δi. all these findings were expected; highly correlated variables should provide worse estimates of Δ, whereas more observations and trustworthy prior knowledge should provide better estimates. a significant improvement in average Δ estimates was observed when using the brute-force search compared to the ga. the ga on average selected slightly more variables than needed and than what the brute-force method did. no clear significant difference could be seen between using the c1c <dig> rather than repeated k-fold cross-validation.

it can be shown that the optimal choice of λ  tends to zero as the number of observations tends to infinity and decreases with decreasing number of variables  and with decreasing correlations between the independent variables. the model  fitted with w = λ^¯ as a response  showed that the average estimated λ was significantly smaller for the data that came from a multivariate normal distribution with Σ <dig> = i <dig> compared to Σ <dig> = s <dig>  when more observations were used, and when prior knowledge about the number of nonzero δi in Δ was assumed. although the true value of λ is not known, these results are thus consistent with theory and provide evidence that both the c1c <dig> and repeated k-fold cross-validation gave reasonable estimates of λ in the demonstration. however, the average λ estimates are not equal to zero for all orthogonal datasets, presumably due to the stochastic nature of the ga and to errors in yn. no significant differences were observed between using the ga or the brute force search methods or between the c1c <dig> and the repeated k-fold cross-validation.

fitting model  with w = |ε^gen−ε˜gen|¯ as a response  showed that the average error estimates were significantly worsened with the assumption of a given number of nonzero δi and that no significant difference was observed when using the ga or the brute force method, or when the independent variables in the dataset were correlated or not. these findings might seem confusing given that the assumption of a given number of nonzero δi, the use of the brute-force search method, and uncorrelated independent variables all improved model selection. the findings can be explained by the fact that, in general, without an assumption of a given number of nonzero δi, when using the ga for searching the model space, and when independent variables were correlated, more nonzero δi are on average selected . thus the chances of also selecting the correct ones improve. this implies that it is worse to estimate at least one δi =  <dig> with δ^i =  <dig> than to estimate all δi =  <dig> with δ^i =  <dig> and at least one δi =  <dig> with δ^i =  <dig>  this makes sense, because the former models are incorrect, whereas the latter ones contain the true model, but are inefficient due to their unnecessary large size. the average error estimates were significantly improved with a large number of observations and when the c1c <dig> was employed to produce the estimates compared to when the repeated k-fold cross-validation was used . the latter result seems contradictory with that no clear difference was found between the average Δ estimates produced with the c1c <dig> and those obtained with the repeated k-fold cross-validation . it can however be explained by studying the r·k individual Δ estimates, where a clear  positive effect could be observed when using the c1c <dig> compared to the repeated k-fold cross-validation. the individual Δ estimates were thus worse when repeated k-fold cross-validation was used, resulting in worse generalization error estimates. however the average Δ estimates from the respective method were almost the same. this observation is seconded by the higher confidence intervals of the average Δ estimates produced with repeated k-fold cross-validation . the finding that the c1c <dig> produces more accurate generalization error estimates than repeated k-fold cross-validation is consistent with the results presented in for instance  <cit>  and provides evidence for that a complete separation of the data used for model choice and the data used for model assessment is necessary to obtain better estimates of the generalization error.

selwood dataset
the result of estimating Δ was, expectedly, less clear when applying the c1c <dig> and repeated k-fold cross-validation to real-world data; the selwood dataset is particularly difficult to model due to the extremely high correlations between variables , the low observation-to-variable ratio, and deviations from the linearity and homoscedasticity assumptions .  <dig>  out of the  <dig> variables were on average selected by the c1c <dig> and  <dig>  by k-fold cross-validation. interestingly, the  <dig> most frequently picked variables selected by the c <dig> c <dig> is a proper subset of the  <dig> most recurrently selected variables by k-fold cross-validation. hierarchically clustering the  <dig> most frequently picked variables chosen by k-fold cross-validation  using the absolute correlation as a distance measure revealed three distinct clusters and one subcluster . good models  for the selwood dataset can be achieved by selecting logp and one variable from the set of variables in the blue subcluster  and one from the set of variables in the green cluster . logp appears to be sufficiently different from the rest of the variables in the red cluster to improve model performance. the variables in the respective blue and green clusters are highly correlated and it is sufficient to have one variable from each cluster in a model.

models containing logp and one variable each from the green and blue clusters have high predictive power and comply to the quick rules for credible predictive models proposed previously  <cit> . furthermore, these models have been found credible in the works of others  <cit> . the c1c <dig> chose  <dig> variables belonging to the green, red, and blue clusters, whereas k-fold cross-validation chose an additional three variables: atch <dig> in the green cluster, and dipmom and dipv_y belonging to the third distinct  cluster. variables from the yellow cluster do not improve the internal predictive ability when testing models containing logp and one variable from the respective green, blue, and yellow clusters on the whole selwood dataset ; this result is supported by the work of nicolotti and carotti . more variables in the selwood dataset were thus on average selected with repeated k-fold cross-validation than when using the c1c <dig> , including two that not seem to improve the predictive ability of the models. the generalization estimates obtained with the k-fold cross-validation was lower than those obtained with the c1c <dig> . although these differences are not highly significant, it is tantalizing to arrive at the conclusion that the models selected by repeated k-fold cross-validation in this particular case are more prone to overfitting and that this is reflected in the lower generalization error estimates.

the c1c2
we have here introduced the c1c <dig> framework for simultaneous model choice and assessment. the main idea is a complete separation of the choice of a model and its assessment in terms of the data used for each task. the c1c <dig> was applied to the problem of choosing a model, mlridge∈mridge. previously, others have described methods that, within the linear model, tackle the problem of regression coefficient shrinkage and variable selection simultaneously, for example, the lasso  <cit> . however, the c1c <dig> framework is general and is easily applied to other settings. for instance, different choices of c <dig> are favorable in different situations; akaike's information criterion   <cit>  is known to be consistent within the linear model if the true model is very complex, whereas bic is favorable within linear models of finite dimension  <cit> , and cross-validation is preferable to use in situations where the degrees of freedom of a model is difficult to define, and so forth. the search method can also be tailored to the problem at hand; for instance, brute-force methods are advantageous for small problems, whereas gas are faster and thus applicable to larger problems. moreover, if required, m can be chosen to contain nonlinear models, l can be chosen to be exponential in order to increase the penalty on outliers, and instead of using the search method to produce an estimate of Δ  we can let g contain a dedicated variable selection method. the cost of this generality is uncharacterized convergence rates  of the parameter estimates, which is coupled to the need of employing a general search method  rather than solving standard convex problems. running the c1c <dig> r times enables averaging of estimates and calculation of confidence intervals, but renders problems in choosing which out of the r models to use for interpretation and future predictions. a potential remedy to these problems is, instead of choosing a model, to employ all chosen models in a stacking-like schema . testing this idea and further testing of the c1c <dig> for other choices of m, g, l, c <dig>  c <dig>  and s will be pursued in future research.

CONCLUSIONS
we have presented some evidence that suggests that the c1c <dig> works well in terms of choosing the correct model and produce good estimates of the generalization error. it was demonstrated to perform well within a penalized linear model, even for "difficult" datasets with highly correlated independent variables, a low observation-to-variable ratio, and deviations from model assumptions . however, more research is needed to fully assess the methods performance for more general, for instance nonlinear, models and to provide theoretical insight to frameworks such as the c1c <dig>  the c1c <dig> is general and reasonable choices of m, g, l, c <dig>  c <dig>  and s help in achieving as unbiased estimates with as low a variance to as low a computational cost as possible. a framework that completely separates model choice from assessment in terms of used data, like the c1c <dig>  should always be employed for model selection and assessment in order to avoid positive bias in the generalization error estimates and, ultimately, to avoid false conclusions and using dubious models to direct further research.

n denotes the number of observations in a dataset, p the number of variables, Δ represents a given subset of the p variables, and λ the ridge regression parameter.

