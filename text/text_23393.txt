BACKGROUND
phylogenetic reconstruction is the process to determine the evolutionary histories among organisms. while biologists primarily use dna or protein sequences to study phylogenies, higher-level rearrangement events such as inversions and transpositions are proving to be useful in elucidating evolutionary relationships. as a result, researchers have used the rearrangement of gene orders to infer high-quality phylogenies  <cit> .

given a set of dna sequences, we can use procedures such as bootstrap to assign confidence values to edges  in phylogenetic trees  <cit> . edges with high confidence values  are generally considered acceptable. however, such procedures are impossible for gene order data since essentially gene orders can be viewed as one character with a very large number of states  <cit> .

several papers presented a jackknife procedure to overcome the problem  <cit> . however, there are many questions to be answered regarding the performance of jackknife. for example, we need to know how many genes should be removed and how many replicates are needed. we even do not know if jackknife on gene order data will converge. we also need to know above what threshold of confidence values can we claim an edge correct.

in this paper, we conduct a set of experiments to tackle these questions. the remainder of this paper is organized as follows: we first review gene order data and genome rearrangements, along with general bootstrap and jackknife procedures. we then provide details of our experiments. in the result section, we determine good rates of jackknife, the number of replicates required, and the accuracy of confidence values.

gene orders and rearrangements
we assume a reference set of n genes {g <dig>  g <dig>  ⋯, gn}, and a genome can be represented by an ordering of these genes. each gene is assigned with an orientation that is either positive, written gi, or negative, written -gi. gene orders can be rearranged through events such as inversions and transpositions. let g be the genome with signed ordering of g <dig>  g <dig>  ⋯, gn. an inversion between indices i and j  produces the genome with linear ordering  

the inversion distance between two genomes is the minimum number of inversions needed to transform one into the other. hannenhalli and pevzner  <cit>  developed a theory for signed permutations and provided a polynomial-time algorithm to compute the edit distance  between two signed permutations under inversions. however, the minimum distance may significantly underestimate the true number of events that have occurred. several true inversion distance estimators have been proposed and among them, the ede correction  <cit>  is the most used.

there are several widely used methods to reconstruct phylogenies from gene order data, including distance-based methods , bayesian  and direct optimization methods . using corrected inversion distances, wang et al. showed that high-quality phylogenies can be obtained using distance-based methods such as neighbor-joining and fastme  <cit> . on the other hand, although badger, grappa and mgr are more accurate, these methods are computationally very demanding and may not be able to analyze datasets when genomes are distant.

several other methods have been proposed. for example, mpbe  <cit>  transforms adjacency pairs from the signed permutation into sequence-like strings, while the method proposed by adam et al.  <cit>  used common intervals  to represent gene orders as binary strings. in mpbe, each gene ordering is translated into a binary sequence, where each site from the binary sequence corresponds to a pair of genes. for the pair , the sequence has a  <dig> at the corresponding site if gi is immediately followed by gj in the gene ordering and a  <dig> otherwise. these transformed strings are then inputs to the ordinary sequence parsimony software  to obtain a phylogeny. for a complete review, please see  <cit> .

bootstrap and jackknife
bootstrap is commonly used to assess the quality of sequence-based phylogenies. the bootstrap procedure generally starts with creating new alignments by randomly picking alignment columns from the original input alignment and reconstruct a tree independently on each new alignment. a consensus tree is then constructed to summarize the results of all tree replicates. the confidence value for an edge in the consensus tree is defined to be the number of replicates in which it appears. if the confidence value for a given edge is 75% or higher, the topology at that branch is generally considered correct.

although the above bootstrap procedure can be applied to methods such as mpbe where each character of the converted string is treated independently. however, it is not possible to perform this procedure in grappa, mgr and most other methods , since for these methods, gene order data can be viewed as one character with 2nn! possible states for genomes with n genes  <cit> .

there are several other ways to apply disturbance to gene order data and assess the robustness of the data. for example, one can randomly remove a genome from the dataset or randomly perform a number of events on the gene orders. however, even with  <dig> genomes, removal of just one may not introduce enough disturbance. on the other hand, there are many parameters to consider in the latter approach: we need to determine what kind of events to be included, which evolutionary model to use and how to apply the events, how many events to apply, and if we should apply the same amount of events on each genome. since we still do not have a good evolutionary model for genome rearrangements, it will be difficult to develop an assessment method based on this approach.

several researchers  began to use a procedure called jackknife to overcome the problem  <cit> .

however, to our knowledge, no detailed study on the performance of this method has been conducted.

in general, the jackknife procedure is performed using the following steps:

• generating k new sets of genomes by deleting some genes. orders of the remaining genes are preserved with respect to their orders in the original genomes.

• reconstructing tree replicates from these new genomes.

• computing a consensus tree and corresponding confidence values on all internal edges.

a consensus tree can be obtained using majority rule, i.e. the consensus only contains edges that exist in more than half of the input trees. the extended majority rule method uses the majority rule result as a start and greedily adds edges that occur in less than half of the input trees, with the aim that a full binary tree can be obtained. in this paper, we use the consense program in phylip  <cit> . we find that the extended majority rule consensus trees generally outperform those computed with majority rule.

RESULTS
determining jackknife rate
indeed, jackknife has been used for sequence data before, although it is not as common as bootstrap. felsenstein suggested for dna sequences, that "one way to make the jackknife vary as much as the boot-strap would be to delete half of the characters, at random, in each replicate  <cit> ." farris later stated that 50% deletion is too severe  <cit>  and suggested the rate of 1/e ≈ 37% should be used. the jackknife rate  is critical for gene order data as well: leaving too few genes out would not produce enough disturbances to the original data, while removing too many genes would make the data totally unrecognizable. the jackknife rate of 50% was adopted by the limited number of papers where jackknife were used  <cit> . however, no discussion was given on the choice of such rate.

to determine the good jackknife rates, we conduct the following experiments: given a dataset, we choose the jackknife rate from 0%  to 90%  and run  <dig> replicates on each rate. we then use fastme to reconstruct a phylogeny tree for each replicate. for each rate, we obtain a consensus tree and compare it with the true tree. the above procedure is repeated for all datasets, and the average rf rates  <cit>  are shown in figure  <dig> 

we find from figure  <dig> that the jackknife rates of 40% and 50% produce similar results. to determine which one is better, we make further investigation on the quality of inferred trees by removing low supporting branches  from the consensus trees. figure  <dig> shows the results from datasets with  <dig> genes; the measurements are false negatives  and false positives  errors  <cit> . in this figure, both 40% and 50% rates produce trees with very low fp errors  and the results are comparable: 40% has slightly better performance for lower evolutionary rates , while 50% is better for r ≥  <dig>  however, using 50% jackknife rate generates much higher fn errors for all datasets, especially when r <  <dig>  based on this comparison, we use the rate of 40% in all our other experiments.

number of replicates required
in  <cit> , the authors used  <dig> replicates to obtain the confidence values, following traditions in bootstrap. pattengale et al.  <cit>  discussed the number of replicates for dna bootstrap and conducted a complete research about finding the correct number of bootstrap replicates. they found that this number indeed varies in a big range. to find out the requirement of replicates in gene order data, we conduct similar testing:

• for a given dataset, generate k replicates using jackknife rate of 40%, starting from k =  <dig> 

• randomly split the k replicates into two equal sized subsets s <dig> and s <dig>  each containing k/ <dig> replicates.

• compute a consensus tree t <dig> from subset s <dig> and compare it with the consensus tree t <dig> obtained from s <dig> 

• stop if t <dig> and t <dig> are very close; otherwise, increase k by  <dig> and repeat the above procedures.

we use the weighted robinson-foulds   <cit>  distance to determine the difference between t <dig> and t <dig>  the wrf distance can be computed as following: for two consensus trees t <dig> and t <dig>  assume t <dig> has n <dig> bipartitions and t <dig> has n <dig> bipartitions, and the confidence value for each bipartition is  <dig> ≤ w ≤ 100%. let w <dig> be the summation of the confidence values of all the bipartitions in t <dig> that are not in t <dig> and w <dig> be the summation of the confidence values of all the bipartitions in t <dig> that are not in t <dig>  the wrf distance is then  

to minimize the variation of results due to random splitting, we repeat the above process for  <dig> times, and calculate the average wrf distance between t <dig> and t <dig>  if this distance is small enough , we can assume that enough amounts of jackknife replicates are generated because we keep getting the same consensus trees from different splits. otherwise, we have to increase k and repeat the process until we achieve a satisfying average wrf distance. we call the jackknife procedure converging when there is no need to add more replicates, and the final value of k is called the converging point for that dataset. figure  <dig> shows the distribution of converging points. for the  <dig> datasets with  <dig> genes, about 50% trees require only  <dig> replicates to converge, while about 30% datasets require more than  <dig> replicates. for datasets with  <dig> genes, almost all datasets require only  <dig> replicates. these experiments suggest that similar to sequence data, using jackknife on gene order data should choose a different number of replicates for each dataset, and  <dig> replicates may not be enough for many datasets, especially when the genomes are small. we also notice some datasets require a very large number of replicates to converge . these datasets all have very large pairwise distances , thus fastme is not very accurate, making the jackknife procedure hard to converge.

threshold of confidence values
the confidence values of internal edges are perhaps the most valuable information obtained through the jackknife procedure. however, as in bootstrap, the meaning of these values is always up for interpretation. the most important question is to determine where to draw the threshold so that edges with confidence values higher than this threshold can be trusted, whereas edges with lower values can be discarded.

we design the following experiments to find out a good threshold value:

• for each dataset, determine its converging point k and compute a consensus tree on these k replicates.

• for a given threshold value m, contract all edges with confidence values below m.

• compare the true trees with the contracted trees to obtain fp and fn rates.

• repeat the above procedures for  <dig> ≤ m ≤  <dig> 

however, the fn rates are very high for these low thresholds, especially when the genomes are distant.

figures  <dig> and  <dig> show the average fp and fn rates respectively for different threshold values, with comparison to the fp rates of the phylogenies obtained from the original genomes, i.e. the genomes without removing any gene. we observe that by doing jackknife, about 95% bad edges can be identified if the threshold value is set at 85%. in other words, jackknife is very much needed for gene order phylogeny study.

by comparing all values presented in figures  <dig> to  <dig>  we suggest the use of threshold value of 85%, which results in the best balance of fp and fn. under the extreme case, using m = 85%, almost 50% true branches can be resolved with only 10% chance of error, and the expected fp rates are ≤ 3%.

however, the high fn rates may reflect that too many potentially good edges are discarded due to low confidence values. to identify how many of such branches are wasted, we check each low support edge and determine if it is indeed a false positive. figure  <dig> shows the percentage of such mistakenly discarded edges, under different threshold values. we are surprised to find that for m = 85%, almost two thirds of branches are not used due to low confidence values, although these branches occur in the true tree, and thus should not be thrown out. these errors may be introduced by the phylogenetic methods , the consensus method, or the jackknife procedure itself.  further investigations are needed to reduce these errors to improve the performance of jackknife.

methods
in this paper, we concentrate our experiments on simulated datasets so that the quality of jackknife replicates can be assessed against the known true tree. in our simulations, we generate model tree topologies from the uniform distribution on binary trees, each with  <dig> leaves. on each tree, we evolve signed permutations of  <dig> and  <dig> genes using various numbers of evolutionary rates: letting r denote the expected number of inversions along an edge of the true tree, we use values of r =  <dig>   <dig>   <dig>  ⋯,  <dig> for  <dig> genes and r =  <dig>   <dig>   <dig>  ⋯,  <dig> for  <dig> genes. the actual number of inversions along each edge is sampled from a uniform distribution on the set . for each combination of parameter settings, we run  <dig> datasets and average the results.

we always use fastme to obtain phylogenies since it is very accurate with corrected inversion distances  <cit> . other methods  will take very long time for datasets with  <dig> genomes and large r values.

we assess topological accuracy via false negatives and false positives  <cit> . let t be the true tree and let t' be the inferred tree. an edge e in t is "missing" in t' if t' does not contain an edge defining the same bipartition; such an edge is called a false negative . the false negative rate is the number of false negative edges in t' with respect to t divided by the number of internal edges in t. the false positive  rate is defined similarly, by swapping t and t'. the robinson-foulds  rate is defined as the average of the fn and fp rates. an rf rate of more than 5% is generally considered too high  <cit> .

CONCLUSIONS
we have conducted extensive experiments to validate the performance of jackknife on gene order phylogenies. these testings show that jackknife is very useful to determine the confidence level of a phylogeny, and a jackknife rate of 40% should be used. however, although a branch with support value of 85% can be trusted, low support branches should not be discarded without further investigation. the jackknife rate of 40% is very close to the suggested rate of 37% for sequence data  <cit> , thus we need to conduct theoretical analysis on the foundation of jackknife on genome rearrangements. all our experiments are conducted with fastme, experiments using other methods should be conducted to further evaluate the performance of jackknife.

competing interests
the authors declare that they have no competing interests.

authors' contributions
all authors contributed to the development and implementation of the methods. js, yz and jt were in charge of conducting simulations and analyzing results. all authors read and approved the final manuscript.

