BACKGROUND
data preprocessing is a major step in data mining. although time-consuming, it improves data quality so they can be properly mined, thus producing more accurate, interpretable, and applicable models. many techniques can be applied to data preprocessing  <cit> , including data cleaning, data integration, and data transformation. in predictive machine learning problems, there is an input x and an output y; the task is to learn how to map the input to the output. such a mapping can be defined as a function y = g where g is the model and θ its parameters  <cit> .

although we can find numerous algorithms for prediction, many of them only work by producing a predictive function that indicates to which target value the objects belong. however, in some data mining problems, it is necessary to have a better comprehension of the induced models. decision trees are models well understood by users. indeed, freitas et al.  <cit>  support the use of decision trees models, instead of black box algorithms, to represent, graphically, patterns revealed by data mining, for example, support vector machine  or neural networks models. still according to these authors  <cit> , the hierarchical structure developed can emphasize the importance of the attributes used for prediction.

the incorporation of context-aware data preprocessing to improve mining results is an active area of research. baralis et al.  <cit>  develop the cas-mine: a context-based framework to extract generalized association rules, providing a high-level abstraction of both, user habits and service characteristics, depending on the context. nam et al.  <cit>  discuss how the context can help classify the face image. although these authors discuss the importance of considering the context in data mining applications while they develop their work according to a context-aware definition, the context involved is intrinsically specific to each working background. hence, their methodologies are not suitable to the molecular docking simulations context explored in this work.

there are many areas of application where a comprehensible model is fundamental to its proper use. in bioinformatics, only a set of data and a set of data mining models may not be enough. the data and the results must represent the context in which they are embedded. bioinformatics is a clear example of where we believe data preprocessing is instrumental. our contribution is within the context of rational drug design . the interactions between biological macromolecules, called receptors, and small molecules, called ligands, constitute the fundamental principle of rdd. in-silico molecular docking simulations, an important phase of rdd, investigate the best bind pose and conformation of a ligand into a receptor. the best ligands are tested by in-vitro and/or in-vivo experiments. if the results are promising, a new drug candidate can be produced  <cit> 

a proper data preprocessing may induce decision-trees models that are able to identify important features of the receptor-ligand interactions from molecular docking simulations. in the present work, we propose and apply a predictive regression decision-tree on the context-based preprocessed data from docking results and show that bioinformaticians can easily understand, explore, and apply the induced models. we apply four preprocessing techniques. firstly, we produce and arrange all attributes based on the domain knowledge. secondly, still based on a context domain, we improve the input by selecting two appropriate features. thirdly, we apply a conventional machine learning feature selection to the initial set of attributes. finally, we combine the feature selection generated using the first and second strategies with those from the third strategy. we assess the results for the model's accuracy and interpretability. then, we demonstrate how a careful and value-added data preprocessing can produce more effective models.

methods
the molecular docking context
interaction between drug candidates  and target proteins , through molecular docking simulations, is the computational basis of rdd. given a receptor, molecular docking simulations sample a large number of orientations and conformations of a ligand inside its biding site. the simulations also evaluate the free energy of binding  and rank the orientations/conformations according to their feb scores <cit> . the majority of molecular docking algorithms only consider the ligand as flexible, whereas the receptor remains rigid, due to the computational cost involved in considering the receptor's explicit flexibility. however, biological macromolecules, like protein receptors, are intrinsically flexible in their cellular environment. the receptor may modify its shape upon ligand binding, moulding itself to be complementary to the ligand  <cit> . this increases favourable contacts and reduces adverse interactions, which, in turn, minimizes the total feb  <cit> . therefore, it is important to consider the receptor's explicit flexibility in molecular docking simulations.

in this work, we model the full receptor explicit flexibility in the molecular docking simulations  <cit>  using a set of different conformations for the receptor, generated by molecular dynamics  simulations  <cit> . this type of representation, named a fully-flexible receptor  model  <cit> , results in the need of executing large numbers of docking simulations and voluminous results to be analysed. actually, one of the current major challenges in bioinformatics is how to handle large amounts of data  <cit> , or big data  <cit> .

data modelling and acquisition
the inha enzyme from mycobacterium tuberculosis   <cit>  is the target protein receptor in this work. it contains  <dig> amino acid residues and  <dig>  atoms. the 3d structure  of the crystal, rigid receptor  <cit> , was retrieved from the protein data bank  <cit> . the ffr model of inha  contains  <dig>  snapshots from a  <dig>  ns md simulation  <cit> . machado et al.  <cit>  performed molecular docking simulations of ffr_inha against each of the four different ligands: tcl  <cit> , pif  <cit> , eth  <cit>  and nadh  <cit> .

all docking results and snapshots of the ffr_inha model were stored into a proper repository  <cit> . we developed this repository to integrate ffr models and docking results, allowing users to query the database from different points of view  <cit> . in fact, queries can traverse relationships between receptors and ligands' atoms and vice-versa, including their conformations and 3d coordinates. this repository enables us to produce effective inputs to use in different data mining tasks with their corresponding algorithms.

attributes arrangements
a major objective of this work is to reduce the number of snapshots used as input in docking simulations of a ffr model against a given ligand. in this sense, by mining the data from the ffr model and its docking results, we expect to select a subset of all available receptor conformations that are most relevant and capable of indicating whether a given ligand is a promising compound. machado et al.  <cit>  <cit>  demonstrated how data mining can address this question. winck et al.  <cit>  obtained encouraging results by applying a context-based preprocessing to data mining of biological text. hence, we focus our efforts on context-based data preprocessing. in our database  <cit>  there are many available features. choosing the most important ones impacts directly the choice of the proper data mining algorithm. predictive data mining task is defined by the target attribute  <cit> . in the following sections we define the target and predictive attributes of the domain-specific knowledge of this work.

target attribute definition
one way to evaluate a molecular docking simulation with autodock <dig> . <dig>  <cit>  is by examining the values of the resulting free energy of binding : the most negative feb values generally indicate the best receptor-ligand binding affinity. autodock <dig> . <dig> predicts the bound conformations of a ligand to a receptor. it combines an algorithm of conformation search with a rapid grid-based method of energy evaluation  <cit> . the autogrid module of autodock <dig> . <dig> pre-calculates a 3d energy-based grid of interactions for various atom types. figure  <dig> shows an example of the grid box used in this work.

we adopt the feb as our target attribute because it discriminates docking results. there is no consensus about what is the reasonable range of feb values. each ligand has to be considered and evaluated individually. analysis of feb values from the docking simulations of the ffr_inha with the four ligands produced different ranges of minimum, maximum and average feb values .

nadh
pif
tcl
- <dig> 
- <dig> 
- <dig> 
- <dig> 
- <dig>  ±  <dig> 
- <dig>  ±  <dig> 
analysis of table  <dig> shows that the difference between the lowest and highest values is very subtle. although we have an absolute difference between these extreme values , there are many instances where the decimal value varies sometimes a difference between two feb values, for instance for eth,  <dig>  and  <dig>  can be significant. in previous work, machado et al.  <cit>  <cit>  using the same four ligands, discretized the feb values using three different procedures: by equal frequency, by equal width and an original method based on the mode and standard deviation of feb values. the authors split the feb into five classes: excellent, good, regular, bad, and very bad. this preprocessing step generated the input data upon which the j <dig> decision tree algorithm was executed. the resulting performance's measures showed that discretization by equal frequency is not satisfactory. that by equal width had good evaluation for two of the four ligands only  <cit> . in these cases, j <dig> did not generate legible trees. discretization by the mode and standard deviation, however, had better performance's measures for two ligands and produced more legible decision trees for all four ligands <cit> . although the j <dig> algorithm produced encouraging results, we found it challenging to discretize feb values whose differences were particularly small. for instance, it was difficult to decide if a feb value of - <dig>  kcal/mol is a good or regular feb since the difference to the next feb value was - <dig>  kcal/mol only. because of the significance of the decimal values we may have an important loss of information when applying this discretization to feb values. therefore, the feb value is taken as real values, which implies the use of a regression predictive task of data mining.

predictive attributes definition
according to jeffrey  <cit>  and da silveira et al.  <cit>  meaningful contact between two atoms can be established on a distance as large as  <dig>  Å. in molecular docking, the feb value is dependent on the shortest distance between atoms of the receptor's residues and ligands. this is because receptor-ligand atoms' pairs within  <dig>  Å engage in favourable hydrogen bonds  and hydrophobic contacts   <cit> . hence, for each receptor  residue, we calculate the euclidean distance  between their atoms and the atoms of the ligand . we define min as the predictive attribute representing the shortest distance between the ligand and the receptor's residues. thus, min with a  <dig>  Å threshold indicates the presence of receptor-ligand favourable contacts . only min is recovered from the repository  <cit> . if we used all receptor-ligand distances the input file would have an enormous amount of attributes, for example, for the pif ligand which has  <dig> atoms, the entry would have more than  <dig>  attributes! this number of predictive attributes would generate model trees with huge amounts of nodes, and, therefore, would not be interpretable. each of the  <dig>  snapshots of the ffr_inha will have  <dig> attributes. we repeat the same procedure for all four ligands. in the end, we have one preprocessed input for each of the four ligands.

data preprocessing strategies
our database does store the ffr_inha which contains  <dig>  snapshots , each with  <dig>  atoms . it totalizes sn × atr =  <dig>   <dig>   <dig> receptor coordinates . because each docking simulation is made of  <dig> runs, we obtain  <dig>  docking results for each ligand. however, some docking simulations runs did not converge or had positive feb values. it occurs when the number of runs and the number of cycles defined as parameter to the algorithm are not enough to find a good position to bind the ligand into the receptor. the docking simulations were performed using the simulated annealing  algorithm, which makes its conformation exploration using the monte carlo approach. since in each step of execution a random movement is applied inside the binding site, sometimes the ligand keeps in a non-favourable position during the number of runs established. if it happens during many runs, the docking result does not converge, that is, it does not present any interaction position/energy in the end of the execution of a given experiment. we considered these data as outliers and did not include them in the preprocessing step. we also defined the parameter valdoc as the total number of valid docking simulations per ligand. since atlig is the number of atoms of each ligand, the sum of the product atlig × valdoc for all four ligands determines the total number of ligand coordinates . in summary, we have:

• coordr =  <dig>   <dig> ×  <dig>   <dig> =  <dig>   <dig>   <dig> records

• ligcoordnadh =  <dig> ×  <dig>   <dig> =  <dig>   <dig> records

• ligcoordtcl =  <dig> ×  <dig>   <dig> =  <dig>   <dig> records

• ligcoordpif =  <dig> ×  <dig>   <dig> =  <dig>   <dig> records

• ligcoordeth =  <dig> ×  <dig>   <dig> =  <dig>   <dig> records

• ligcoord =  <dig>   <dig> +  <dig>   <dig> +  <dig>   <dig> +  <dig>   <dig> =  <dig>   <dig>   <dig> records

data generation
to generate an initial dataset we need to combine the  <dig> , <dig> coordr and the  <dig> , <dig> ligcoord, calculate their interactions, and find their respective min. for that, we developed the dataset algorithm. it executes the first preprocessing step by handling the input data and by producing the best receptor-ligand interactions stored in an output file: the  matrix.  contains valdoc lines and  <dig> columns. the first  <dig> columns contain the  <dig> receptor residues min. to generate a proper dataset for data mining, we aggregated a target attribute in the last column, which is the corresponding feb value. it is important to emphasize that, at this stage, min can have any positive value.

- dataset algorithm -

letr be a receptor

letl be a ligand

lett be a snapshot ofr

letr be a residue ofr

leta be an atom int snapshot

letl be an atom inl

letdist be the distance betweenl andr atoms int

letdistancematrix be a matrix where each line corresponds to a residuer and each cell corresponds to the distance betweena andl

letresult be a matrix that stores for eacht snapshot, all minimum distances betweena andl

letinput be a matrix containingresult and, for eacht, its respective feb value

for eacht intotal_snapshotsr

    * ←null

    for eachr intotal_residuesr

        ← null

        for eacha intotal_atoms_residue_snapshotr,t

            for eachl intotal_atoms_ligandl

                distri,li←2+2+2

                distra,ll ←ed

                ← distra,ll

            endfor

        endfor

        ← min

    endfor

    ← 

endfor

dataset improvement
the initial dataset generated by the dataset algorithm contains  <dig> predictive attributes and a target attribute. to help improve the models induced by the data mining task, we must reduce further the amount of features. jeffrey  <cit>  states that the largest distance value that allows a meaningful contact between receptor and ligand atoms is  <dig>  Å. the feature selection strategy in dataset algorithm includes distances higher than  <dig>  Å. this means that the corresponding receptor residue does not establish a favourable contact with any of the ligand atoms  <cit> . if there is not a contact in any docking results, it is improbable that this attribute can adequately predict the feb value. therefore, we removed all attributes  with shortest distances above the  <dig>  Å threshold. context-fs algorithm generates a new input from the  matrix output produced by dataset algorithm. to compare our context-based feature selection with a well-known machine learning feature selection algorithm, we generated one more dataset seeking to improve the initial one produced by the dataset algorithm. we believe that a subset of representative attributes can improve further the mining results.

- context-fs algorithm -

letr be a receptor

lett be a snapshot ofr

letr be a residue ofr

letinput be a produced by the dataset algorithm

letinputfs be a result after our context-based feature selection

for eachr intotal_residuesr

    ifmin≤ 4

        for eacht ininput

            ← 

        endfor

    endif

endfor

for eacht ininputfs

        ← 

endfor

only a limited number of the existing feature selection algorithms can work effectively on regression predictive tasks. among these, the correlation-based feature selection   <cit>  algorithm implemented in weka  <cit>  can perform feature selection on our datasets. therefore, we applied cfs to each input generated by dataset algorithm, with a different input for each of the four ligands. cfs is based on a filter approach that ranks features according to a correlation-based heuristic evaluation function  <dig>  it looks for a subset that contains features uncorrelated with each other, but highly correlated with the target attribute.

  ms=kbarrcfk+krff- 

where: ms is a heuristic of a subset s that contains k features; barrcf is the mean feature-target correlation  and rff- is the average feature-feature inter-correlation. equation  <dig> forms the core of csf  <cit> . table  <dig> shows the number of attributes selected after applying our feature selection methodology to the original dataset. additionally, we generated one more dataset  which combines the features selected by the context-fs algorithm with those selected by cfs  <cit> .

mining and evaluation the preprocessed data
regression is a data mining task suitable to problems for which the attribute to be predicted is continuous. since our target attribute is numeric, regression is the technique applied to the mining experiments in this study. our models must be understandable and must also represent well the context in which they are inserted. decision trees are algorithms that cover these needs and also can be applied to both classification and regression problems. the results are regression or classification models arranged in a tree structure. decision trees can be applied to predict both continuous and discrete values. for continuous values, there are two main types of trees: regression trees and model trees. in regression trees, each leaf node stores a continuous-valued prediction, which is the average of the target attribute for the training tuples. in model trees, each leaf stores a regression model called linear model , which is a multivariate linear equation for the target attribute  <cit> . our goal is to induce models that indicate residues distances to predict a given feb value. we expect our model to help us discover whether a snapshot, when docked to a given ligand, will lead to favourable estimated feb values. for this, we use the m5p  <cit>  machine learning model tree algorithm.

evaluation of the induced models
there are several measures to verify if the induced models generated are acceptable numerical predictions. they are called predictive measures. in the case of model tree algorithms, the most widespread measures are: root mean-squared error , mean absolute error , and correlation coefficient  <cit> . smaller values of rmse and mae are indicators of better models. all of these measures make use of the predicted values p <dig> . . . pn and the actual values a <dig> . . . an.

  rmse=2+…+2n 

  mae=|+…+||n 

the correlation coefficient  measures the statistical correlation between a and p. the values range from  <dig>  for perfectly correlated results, to  <dig>  when there is no correlation, and to - <dig>  for an inverse perfect correlation. we look for perfectly correlated results or correlation coefficients closer to  <dig> 

  correlation=spaspsa 

where spa=∑in- <dig> sp=∑i2n- <dig>  and sa=∑i2n- <dig>  being that ā and p ¯ are the corresponding a and p averages.

in addition to these measures, some investigations also make use of the model interpretability metric, which is the number of nodes in the model tree. the model tree with the smallest amount of nodes generates the best interpretable models  <cit> .

evaluation based on the context
the measures shown in the previous section were used during the evaluation of the models generated. however, as we are interested in the usefulness of the induced models, we propose a new context-based measure. we also analyze the induced model trees and their contents. figure  <dig> shows a model tree generated upon application of our context-based preprocessing  to nadh. this model contains five non-leaf nodes, each representing a selected amino acid residue, and six lms. equation  <dig> depicts the sixth lm  composed of a selected number of predictive attributes  weighted by their effect in the target attribute  plus a constant value.

  feb=- <dig> ×ser12+ <dig> ×phe22+ <dig> ×thr38+ <dig> ×asp63+ <dig> ×hie92+ <dig> ×thr100- <dig> ×gly101- <dig> ×ala123- <dig> ×asp147+ <dig> ×thr161+ <dig> ×leu167+ <dig> ×gly191+ <dig> ×pro192+ <dig> ×ile193+ <dig> ×ile201- <dig>  

we evaluate the models taking into account the receptor residues present in both the non-leaf nodes and the lms, bearing in mind that the docking software calculates the feb value only for the residues within the grid box around the receptor binding site . consequently, if we are inducing model trees to predict feb values, models that consider residues located outside the grid box have no direct significance. usually, a specialist defines which residues belong to the receptor active site. these residues shape the active site for the complementary ligand binding. for inha, the specialist selected  <dig> residues, here denoted by esr. subsequently, by inspecting each model, we identified which model's residues  appear in the tree or the lms . now we are able to evaluate mr and compare it with esr by calculating the precision , recall  and f-score  measures  <cit> .

  precision=|{relevant}|∩|{retrieved}||{retrieved}| 

  recall=|{relevantes}|∩|{retrieved}||{relevant}| 

  f-score=recall×precision/ <dig> 

in the context of this analysis:

• |{relevant}| ∩ |{retrieved}| can be defined as esr ∩ mr

• |{relevant}| can be defined as esr

• {retrieved}| can be defined as mr

RESULTS
we evaluated the models by means of the predictive and context measures presented. the measures were applied separately to each of the four distinct ligands; nadh, pif, tcl, and eth. for each one of them, we observed the four data preprocessing strategies:

 <dig>  the results obtained by the initial dataset, generated by dataset algorithm;

 <dig>  the results obtained by the context-based feature selection, generated by context-fs algorithm;

 <dig>  the results obtained by the feature selection generated by cfs  <cit> ;

 <dig>  the results obtained by combining both feature selection generated by context-fs algorithm and cfs .

the initial dataset was the first and, possibly, the most important context-based data preprocessing. without the previous knowledge about the context, it would not be possible to generate an input that produces interpretable models as we expected. based on the fact that the initial dataset was constructed considering minimum distances ), our hypothesis is that the context-based data preprocessing we proposed, including feature selection, produces better results than using a worthy feature selection approach, where the context is not observed. hence, we expected that the results from the third strategy would not be better than the others. on the other hand, we expected the context-based feature selection  to give better results than the others. the second strategy was applied considering both, the context already employed in the initial dataset and context to select appropriate features. to evaluate the results in terms of their statistical significance, we applied the friedman test  <cit>  with a significance level of α =  <dig> . for the context measures , we evaluated whether strategy  <dig> was significantly better than the others. in this case, we could assert that it was true since p =  <dig> . we infer that our feature selection approach improves the initial results. therefore, as we are interested in the quality of the induced models, our context-based measure can be considered as the most appropriate.

in table  <dig> we evaluate whether strategy  <dig> is significantly worse than the others. we got p =  <dig>  for mae and p =  <dig>  for rmse as significance levels, indicating that probably strategy  <dig> is really worse than strategies  <dig>   <dig> and  <dig>  however many effort is needed to assess it. with respect of context measures , we evaluate whether strategy  <dig> is significantly better than the other ones. in this case, we can assert it is true because we got p =  <dig> . we infer that our feature selection approach improves the initial results. in doing so, once we are interested in the quality of the induced models, our context-based measure can be considered as the most appropriate.

it is noticeable in tables  <dig> and  <dig> that the results are different for each ligand, despite employing the same strategy in the preprocessing. this is so because different ligands have different sizes, as well as different molecular interaction properties. they bind in different regions of the receptor's binding site. as a result, the target attribute feb has different ranges of values for the distinct ligands  and that is why the models are induced for individual ligands. although they are not interchangeable, we expect them to be used to select ligands that belong to a similar class .

CONCLUSIONS
data preprocessing is a significant step in data mining. in data preprocessing, different techniques are applied to improve data quality such that the mining results are more accurate and better interpretable. there are many techniques available to preprocess data, mainly for model quality measures. however, some applications, like bioinformatics, often demand well-suited models. hence, when the data mining process is based on the context involved, a context-based preprocessing can improve the quality of the induced models.

in this article, we presented a case of mining data from flexible receptor molecular docking simulations results. here the goal was to identify features that could characterize the best fit of ligands into a given receptor. our experiments were conducted considering the inha receptor from the m. tuberculosis and four distinct ligands: nadh, pif, tcl, and eth. we showed that an appropriate context-based data preprocessing could provide improved results.

we concentrated on four main preprocessing steps which: 1) consider the context to choose an initial set of attributes and the proper instances for each ligand input file; 2) perform feature selection on the initial dataset, taking into account the characteristics of the docking results from each ligand; 3) perform feature selection, for each ligand, based on the cfs machine learning algorithm; and 4) combine features selected by our context-based approach  and those selected by the cfs algorithm. we hypothesized that mining the preprocessed data would provide better results, with respect to the original dataset, by using the second strategy.

we performed mining experiments using the m5p model tree algorithm implemented in weka. the values of the rmse error measure, as well as a context-based metric that considers the tree interpretability, suggested that we can obtain better results when using our feature selection approach . statistical analysis of the results, with the friedman test, showed that our context-based approach significantly improves predictive measures while cfs worsens context measures. we concluded that data preprocessing, which considers the context involved, can improve the mining results and produce better interpretable models. as future studies, we plan to use the induced models, generated using the second strategy, to select the most promising subset of snapshots, out of a very large ensemble, for a given ligand.

competing interests
the authors declare that they have no competing interests.

authors' contributions
atw and ksm executed the preprocessing for the data mining experiments, performed all the data mining experiments, evaluated the models results and wrote the first draft of the article. ddar helped to conceive the test cases and to evaluate the models. ons helped to analyze the results and to write the final version of the article. all authors read and approved the final manuscript.

author's information
atw current address:

laca - labortório de computação aplicada, departamento de computação aplicada, universidade federal de santa maria , santa maria, rs, brasil.

ksm current address:

combi-l, grupo de biologia computacional, centro de ciências computacionais, universidade federal do rio grande , rio grande, rs, brasil.

