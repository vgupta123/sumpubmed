BACKGROUND
an ever increasing amount of biological data being produced has predicated the use of databases capable of storing and analyzing such data. in the field of translational research, knowledge management platforms use databases to store a variety of data produced from clinical studies, including patient information, clinical outcomes as well as high-dimensional omics data. for optimal analyzes and meaningful interpretations, such databases also store legacy data taken from public sources, such as the gene expression omnibus   <cit>  and the gene expression atlas  <cit> , alongside new study data. this enables cross-study comparisons and cross-validation to take place. high-throughput transcriptomic data generated by microarray experiments is the most abundant and frequently stored data type currently used in translational studies.

originally developed by johnson and johnson for in-house clinical trial and knowledge management needs in translational studies, transmart is one such knowledge management software platform  <cit>  that has recently been open-sourced. for the needs of various collaborative translational research projects, an instance of transmart is hosted at imperial college london and has been configured to use an oracle relational database for back-end storage. it currently holds over  <dig> million gene expression records. when querying the database simultaneously for hundreds of patient gene expression records, a typical exercise in translational studies, the record retrieval time can currently take up to several minutes. these kinds of response times impede analyzes performed by researchers using this deployed configuration of transmart. anticipating the requirement to store and analyze next generation sequencing data, where the volume of data being produced will be in the terabyte  range, the current performance exhibited by transmart is unacceptably poor.

a typical query involves searching for and retrieving patient gene/probeset values in a study in order to perform data analysis. for each study, the transcriptomic data stored in the database is comprised of individual probeset values for each patient sample , and annotation information further describing the experiment . once retrieved the data is passed to analytical tools, such as genepattern  <cit>  or custom r workflows for further analysis or visualization.

a classic microarray table structure, the deapp schema in transmart.

all records in the table are organized by a b-tree like structure, a tree data structure that allows search, access, and deletions of tree leaves in logarithmic time. b-tree like structures are commonly used in the implementation of databases and file systems and are fully described in  <cit> . the time to fetch the patient probeset data using a relational model, such as that in table  <dig>  is therefore:

 pnps×logmpnps×tr 

where the time to find a next node in the tree is one standard unit, the order of the tree is m, the time to fetch a record in relational database is tr, number of patients is pn, and the number of probesets for each patient is ps. for example, if there are  <dig> patients in a study and each patient has  <dig>  probesets, the tree is of order m, the total theoretical query time to retrieve all patient probesets, ∑tr, is:

 559× <dig> ×logm559× <dig> ×tr 

what this illustrates is that in order to traverse the tree to retrieve a large study, a significant number of records need to be read from the physical disk where such a large number of database operations will cumulatively be slow. a common solution for scalability issues in relational databases is database partitioning  <cit> , which allows for a logical database to be divided into constituent parts and distributed over a number of nodes, for example in a compute cluster. this approach has enabled relational databases to support large-scale genomics datasets through horizontal partitioning , however this still does not solve the data retrieval performance issues such as those observed in translational research studies loaded into transmart.

an alternative solution, and what forms the contribution described in this paper, is to use a non-relational database modality, the key-value pair data model, as implemented in nosql databases such as google bigtable  <cit> , a column-oriented storage system for structured data. bigtable uses a row key and a column key to locate a value. the first advantage of such a data model is that it typically maintains data in lexicographic order by row key. the second advantage is that the column key includes two parts: a family and a qualifier, where database columns  are grouped into sets  and all data stored in a family is usually of the same type and is compressed and stored together. when a key is retrieved, a key-value array, termed a storefile, is loaded into memory and the expected key-value pairs are returned. taking these features of the key-value model into account, we hypothesized that a key-value pair data model would more performant in data retrieval than the current relational model used for microarray data storage in transmart. in this paper we describe an experiment using a new database model for one of transmart's microarray data tables that may be more suitable for high-dimensional data storage and querying than the relational model currently implemented in transmart.

related work
the current version of transmart operates with sql-based databases in a single node mode, typically as single postgresql  <cit>   <dig>  or oracle 11g  <cit>  database. it is possible to migrate the current transmart data model to sql-based database clusters to solve the performance problem mentioned in motivating example. for example, mysql cluster  <cit>  is an open-source high performance database cluster. mysql cluster consists of multiple network database nodes , a single management node  and multiple mysql nodes. the data is horizontally partitioned across the ndbs, where each mysql node is responsible for handling sql operations. the mgm controls all ndb and mysql nodes. mysql cluster can use the same relational data model detailed previously in table  <dig> 

nosql document storage systems, such as mongodb  <cit> , couchdb  <cit> , riak  <cit>  are popular alternatives to using relational database management systems such as mysql, because relational data models can easily map to a document-based key-value model. for example, mongodb has features such as indexing in the form of b-trees, auto-sharding and asynchronous replication of data between servers. mongodb stores data in collections and each collection contains documents. each document is a serialized json  <cit>  object. our mongodb implementation is similar to the relational model where each column is represented with a field in a json object. for example, the relational data record in table  <dig> maps to the json object shown in figure  <dig>  note that the field _id holds a unique key generated for each json object.

all of the data is imported into a single collection with the index built on the keys, where the sharding key uses patient_id to distribute the data into multiple ranges based on patient_id.

nosql systems for structured data, such as bigtable and cassandra  <cit> , have been developed for managing very large amounts of data spread out across many servers. in contrast to document key-value storage, such as mongodb, nosql structured data systems do not easily map from a relational data model to a key-value model. hbase  <cit>  is a popular example of such a system where it is part of the hadoop framework that has been widely adopted for large-scale storage and processing. hbase is an apache software foundation open source project written in the java programming language, and provides bigtable-like capabilities. there are three major components of hbase. the hbase master assigns regions to hregion servers. hregion servers handle client read and write requests. the hbase client discovers hregionservers, which are serving a particular row range of interest. our microarray data model implementation using hbase is described in our results section.

RESULTS
we designed a key-value  <cit>  based data model to support faster queries over large-scale microarray data, and implemented the schema using hbase. we compared the query retrieval time against a traditional relational data model running on a mysql cluster database, and a relational model running on a mongodb key-value document database.

schema design
determining row key and column key of transcriptomic data
in order to speed up queries, the trial_name and patient_id are placed in the row key. this is for two reasons. the first reason is that a family can manage a single data type  of data efficiently. when a family is used in this way, all attributes larger than the data type  are placed in the row key and attributes smaller than the data type  are placed in the qualifier. the second reason is that all records belonging to a patient are stored together in several storefiles. for a typical use case when multiple identical patient_ids are retrieved, the storefiles storing these patients' data will be loaded into bigtable caches. therefore, only retrieving the first record causes a significant delay when loading data from disk to memory, while all other records in the same storefile are fetched directly from memory caches. this design takes full advantage of bigtable caches.

optimizing the row key to speed up data location
the order of the two fields in the row key also has a very significant effect on the query performance. typical user cases always begin with a gene information query of several patients. by placing the trial_name before the patient_id , the composite key becomes two indices on the trial and patient respectively. when multiple identical patients are retrieved, trial_name + patient_id can precisely locate all storefiles expected by using trial_name + patient_id to compare to the start and end keys of a storefile.

optimizing the column key to increase cache hit rate
one way to further increase cache hit rate is to design a new structure by classifying different types of data under column key. the family divides different types of data  into different storefiles and the qualifier  orders probeset values lexicographically. in the motivating example above, the user requests data on several patients that contains only one type of probeset, but the query returns millions of records. such large numbers of probeset records can be loaded with only a limited number of disk loading operations through the corresponding column keys.

key-value data model example
to better explain our design, we can illustrate its application using the relational example in table  <dig> and transforming it into our key-value model shown in table  <dig>  table  <dig> shows how a storefile stores these key-value data on physical disks. key-value pairs are ordered alphabetically by the key in table  <dig> and each pair is an array of java data type byte.

for example, if the raw intensity values of all patients in the trial multmyel are retrieved, the key-value system will locate the key,

trial name  + minimum patient id + family  + gene + probeset

and then load a storefile containing key1

multmyel +  <dig> +  <dig> + raw + ei <dig> + 216396_s_at into memory. this storefile probably also contains key <dig> consisting of

multmyel +  <dig> +  <dig> + raw + ldoc <dig> + 204454_at because key <dig> is typically stored adjacent to key <dig> in the physical storage. this results in a reduction in query response time by increasing the cache hit rate.

as in the relational model, key-value data is organized as a b-tree. each leaf in the tree is a storefile , each of which contains data from more than one patient. considering the maximum probeset count in our database of  <dig>  probesets per sample using the gpl <dig> platform, where the average probeset data size is  <dig> bytes, we can see that,

 300b× <dig> ≈16mb<128mb 

in this case, one type of probeset values for a patient can at most be stored in two storefiles. therefore we assume each patient data in one type is stored within two storefiles. the time to fetch patients data is less than:

 2pn×logmpn×tkv 

where the time to find a next node in the tree is one standard unit, the tree is of order m, the time to fetch a storefile in a key-value system tkv, and patient number is pn. thus, for example, if there are  <dig> patients in a study and each patient has  <dig>  probesets, the total theoretical query time ∑tkv is:

 2×559×logm559×tkv 

in order to make a full comparison of time consumption between key-value data model and relational data model, we assume tr is the time to load  <dig> kb  data from physical disk into memory and tkv is the time to load  <dig> mb data, then in a common physical server with a sata disk , tr is approximately  <dig> milliseconds and tkv is approximately  <dig>  milliseconds. in this case:

 ∑tr∑tkv≈ <dig>  

therefore, if cache and query optimizers are not taken into account, we estimate theoretically that an ideal key-value data model may be up to about  <dig> times faster than an ideal relational data model. the actual observed speedup is much less than our theoretical estimate since real-world performance will be vary according to implementation. however as the basis of our hypothesis, based on our theoretical calculations we propose that a key-value will be significantly faster than a relational model when querying microarray data such as that stored in transmart.

querying transcriptomic data in transmart
our experiment was performed using a dataset loaded in an instance of transmart running on a cloud computing test-bed, ic cloud  <cit> , installed at imperial college london. in our experiment we took a database dump of the deapp schema that transmart uses to store patient microarray data and extracted a public multiple myeloma   <cit>  dataset   <cit> . the reason we chose multmyel as our test dataset was that it is one of the largest datasets loaded into our transmart database instance, consisting of  <dig> samples and  <dig>  probesets for each sample, totalling approximately  <dig>  million records. we took this dataset and transformed and loaded it into two different key-value databases, hbase and mongodb, and a relational database, mysql cluster, each of which was running on ic cloud. hbase was used to implement our key-value model, while mongodb and mysql cluster both re-implement the relational model. each database was configured as follows:

• hbase : one master server node and three slave nodes with hbase configured in fully distributed mode. the master server was configured as a virtual machine  with  <dig> cpu cores and  <dig> gb memory, while each slave node was configured as vms with  <dig> cpu cores and  <dig> gb memory. each vm used a  <dig> gb disk. three copies is the minimum number for the hadoop distributed file system to guarantee data consistency. we ran queries in hbase using two read methods, random read and scan, in order to select the faster method. in the random read method each patient's data is retrieved discretely. in scan, all patients from the first row key to the last row key expected are retrieved sequentially, including unexpected row keys between two expected ones. however we found scans over sequential records are significantly faster than random reads.

• mongodb : four vms were used in our mongodb cluster . each vm used a  <dig> gb disk. vm a and b formed a replication set , with c and d forming another ; rs <dig> and rs <dig> formed a two-shard sharding cluster. vms b, c and d were also deployed as three configuration servers . vm a hosted the load balancer , and handled all the data operation requests. mongodb requires two copies to guarantee data consistency. mongodb indexes the trail_name and patient_id fields within json objects.

• mysql cluster : four vms, with one as a manager node and three data nodes. the manager node consists of a mgm, a mysql and a ndb using a vm with  <dig> cpu cores and  <dig> gb memory. each vm used a  <dig> gb disk. each data node consisted of a ndb. disk-based innodb storage engine was used, where indexed columns and indices are stored in memory and non-indexed columns are stored in disks. the trial_name and patient_id were indexed by btree to speed up the query operations. two data copies were used to guarantee its data consistency.

gene data query using a large transcriptomic dataset
in order to assess hbase, mongodb and mysql cluster performances using a gradient of retrieval size requests, we devised a series of typical marker selection queries based on relevant biological questions  <cit>  and data download requests:

• test cases a <dig> and a2:  <dig> patients who underwent therapy  <dig> and survived longer than  <dig> months are compared to  <dig> patients who undertook the same therapy and survived less than  <dig> months, to discover gene expression patterns affecting the short term response to therapy  <dig> 

• test cases b <dig> and b2:  <dig> patients who took therapy  <dig> and were still alive at the end of the trial were compared to  <dig> patients who took therapy  <dig> and did not survive the duration of the study, to discover gene expression patterns affecting the long term response to therapy  <dig> 

• test cases c <dig> and c2:  <dig> patients who lived longer than  <dig> months were compared to  <dig> patients who survived less than  <dig> months, to discover gene expression patterns affecting patient survival.

• test case d <dig> and d2:  <dig> patients who took therapy  <dig> were compared  <dig> patients who took therapy  <dig>  this comparison is used to see the different effect of the two therapies. d <dig> is comparable to test case a <dig>  as it retrieves a similar amount of patients and thus is not shown on figure  <dig> 

• test case e <dig> and e2:  <dig> patients who survived less than  <dig> months were compared to  <dig> patients who survived more than  <dig> months. e <dig> is comparable to test case a <dig>  as it retrieves a similar amount of patients and thus is not shown on figure  <dig> 

• test case f <dig> and f2:  <dig> patients who lived less than  <dig> months were compared to  <dig> patients who survived beyond  <dig> months.

• test case g: a query over the whole clinical trial consisting of all  <dig> patients. this last case is used to download all patient information to perform further analysis in external tools.

we tested each case three times, where after every test we shut down the entire cluster to clean all caches to remove the influence of residual cached data. the record retrieval times for the hbase, mysql cluster and mongodb configuration are shown in figure  <dig>  in this experiment only raw data was targeted.

with the number of records increasing, most test cases in relational model show retrieval times also increasing.

mysql cluster retrieval times are slow when querying fewer numbers of patient records with query times increasing slightly as the data queries scale up. our key-value based data model demonstrates an average  <dig>  times of increase compared to the relational model implemented on mysql cluster. in 6/ <dig> of cases, the key-value data model is more than 5x faster than relational model on mysql cluster. in 10/ <dig> of cases, the key-value data model is more than 4x faster than the relational model on mysql cluster. in the worst case, a <dig>  the key-value data model is more than  <dig>  times faster than relational model on mysql cluster. as the size of the queried data scales upwards, mysql cluster performs less stable than other databases as show by the greater average result deviations in figure  <dig> 

the relational model on mongodb performs well with smaller queries but slows significantly when the amount of data retrieved is scaled up. our key-value based data model demonstrates an average  <dig>  times increase in query performance compared to the relational model implemented on mongodb. in 6/ <dig> of cases, key-value retrieval is more than 6x faster than that on mongodb, especially in cases with large retrieval data. in 10/ <dig> of cases, the key-value retrieval speed is more than 4x faster than that on mongodb. in the worst case, b <dig>  the key-value data model is more than  <dig>  times faster than the relational model on mongodb.

the data retrieval time in the key-value model varies more widely due to the different read operations, random read and scan, and also the scan range in different cases, as shown in figure  <dig>  in most test cases in our experiment, scans are quicker than random reads. for scans, the worst case is when the expected patient count is small and the patient record distribution within the database is very sparse. in this situation, the scan operation degrades to sequentially read all patient data. for example, in case b <dig>   <dig> patient records are expected and distributed over a range of  <dig> patients. however, a whole dataset scan only needs about half of the time to perform a random read.

the situation is similar when retrieving other types of data, such as log or zscore, as shown in figure  <dig> and  <dig>  in these two figures, the performance trend observed for each family is similar. only the deviation values vary. thus, compared to the other databases, the same conclusion will be generated for every family.

discussion
our results show that in general our key-value implementation of the transmart deapp schema using hbase outperforms the relational model on both mysql cluster and mongodb, as we originally hypothesized. hbase's increased performance in comparison to mysql cluster comes from the new key-value data model. this new key-value data model is however currently not efficient enough for discrete data queries. a possible solution is the use of the hbase feature, ccindex  <cit> . discrete query operations are generated by where conditions in sql queries, where ccindex can index columns that are frequently used in those conditions and transform these discrete queries into large range scans. as future work, we plan to integrate ccindex to further improve the performance in our key-value model for high-dimensional data in transmart.

queries in hbase are optimized manually by choosing to perform either a pure scan or pure random read operation. ideally there should be a query optimizer built into the hbase system, like in mysql cluster or mongodb, to automatically generate query plans for high-dimensional data retrieval. pure scans or random reads may not be the best choice for certain cases, where a mixed, dynamically selected query method may perform better than statically choosing one or the other.

our results also show that performance in different families differ slightly. family log performs best in almost all cases. this phenomenon may result from the log storefile location in hbase system.

we also observed that the relational model implementation consumes a lot of memory. although mysql cluster supports disk based storage, all indexed columns and indices are still stored in memory. this feature may influence its scalability.

CONCLUSIONS
our work is aimed at solving the performance issues faced in translational research data storage databases, due to the increasing volume of data being produced. in this paper we have demonstrated that a key-value pair schema leads to an increase in performance when querying high-dimensional biological data, over the relational model currently implemented in transmart. our results show that, in general, our key-value implementation of the transmart deapp schema using hbase outperforms the relational model on both mysql cluster and mongodb. we aim to further optimise the schema design to achieve a better performance and use this schema as the prototype for developing a next generation sequencing storage data model for the transmart data warehouse.

competing interests
the authors declare that they have no competing interests.

authors' contributions
sw designed the key-value data model, ran the hbase experiment, analyzed the hbase results and drafted the manuscript. ip helped to define the research theme, designed and explained the test cases, and drafted the manuscript. cw designed the mongodb data model, ran the mongodb experiment and analyzed the mongodb results. sh designed the mysql cluster data model, ran the mysql cluster experiment and analyzed the mysql cluster results. dj participated in the key-value data models design and drafted the manuscript. ie participated in the key-value data model design and the test case design. fg participated in mongodb data model design and experiment. yg defined the research theme and participated in all the models design. all authors read and approved the final manuscript.

declarations
this research was partially supported by the innovative r&d team support program of guangdong province, china  and the etriks consortium . sw is also supported by a scholarship awarded by the china scholarship council from 2012- <dig>  the funding agencies had no role in the design, collection, analysis or interpretation of the data, in preparation of the manuscript, or in the decision to submit the manuscript for publication.

this article has been published as part of bmc genomics volume  <dig> supplement  <dig>  2014: selected articles from the 9th international symposium on bioinformatics research and applications : genomics. the full contents of the supplement are available online at http://www.biomedcentral.com/bmcgenomics/supplements/15/s <dig> 
