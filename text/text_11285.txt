BACKGROUND
ontologies have become a crucial component for the analysis, retrieval and integration of the data underpinning modern biomedical science  <cit> . whether structured as controlled vocabularies or expressive description logic-based models, biomedical ontologies have been used to manually and semi-automatically annotate enormous volumes of genomic, clinical and bibliographic information. these annotated datasets support a range of ontology-driven applications such as semantic search, enrichment analysis, data integration and clinical decision support.

of particular importance in the biomedical space are the family of applications, including enrichment analysis  <cit> , semantic similarity clustering  <cit>  and data-based ontology evaluation  <cit> , that quantify the importance of classes in an ontology relative to a collection of domain data. these applications, especially enrichment analysis based on the gene ontology   <cit> , have been widely adopted by the scientific community and have proven effective in distilling large datasets that would otherwise be extremely difficult for researchers to interpret. yet, despite the extensive use and high utility of these applications, the underlying analytical methods remain limited in their ability to successfully detect and synthesize several important types of ontological and dataset complexity, including class overlaps, continuously valued data, inter-instance relationships, non-hierarchical class relationships, semantic distance and sparse data.

to help address these limitations, we have developed a new methodology, markov chain ontology analysis , for analyzing hierarchical models relative to a collection of domain data. our approach represents the combination of an ontology and the instances in an associated dataset as a single finite ergodic markov chain whose adjusted transition probability matrix is used to compute modified eigenvector centralities, or steady-state probabilities, for each class and instance. the negative log of these modified eigenvector centralities, a quantity we call the information rank of the class, represents the importance of each class relative to both the data set and other classes in the ontology.

in the remainder of this paper, we outline the analytical challenges that motivated the development of our methodology, detail the mathematical model of our technique and demonstrate its utility in the context of go enrichment analysis. following a standard benchmarking process, we demonstrate the ability of a mcoa-based enrichment analysis method to outperform existing state-of-the-art enrichment methods on simulated gene enrichment datasets. to evaluate the performance of mcoa on real experimental data, we compare the enrichment results generated by mcoa with other comparable methods using gene expression data from a study of parkinson's disease. finally, we discuss other applications that could benefit from the mcoa approach and our plans for future investigations.

enrichment analysis
although the analysis approach we propose is relevant to any application that quantifies the importance of ontology classes relative to a dataset, we frame the discussion in this paper in the context of enrichment analysis. our focus on enrichment analysis is motivated both because of the widespread use of enrichment analysis in the biomedical field as well as the fact that the technical challenges faced by enrichment analysis methods are directly relevant to many other ontology-based data analysis activities.

enrichment analysis assesses whether classes in an ontology are statistically over or under-represented in a specific dataset based on the semantic annotations of dataset members relative to some baseline distribution. in the biomedical field, enrichment analysis methods are commonly employed to determine the statistical enrichment of go categories for gene expression data by comparing the annotation frequency in a target gene list with the annotation frequency in a background collection of genes. the widespread use of the method in this context has motivated the extensive manual annotation of genomic and proteomic data with go categories and the development of a wide range of enrichment analysis techniques and tools  <cit> . although analysis relative to go is the most common use case, the underlying enrichment analysis techniques are relevant to any biomedical ontology  and correspondingly annotated dataset . recent successes applying enrichment analysis outside the genomic domain include efforts by tirrel et al  <cit>  who performed enrichment analysis of the ontologies contained in bioportal  <cit>  relative to both medline and the collection of biomedical repositories aggregated by the ncbo resource index  <cit> .

whether analyzing genomic data for enrichment of go categories or bibliographic data for enrichment of classes in a clinical ontology, the same set of enrichment methods can be employed. huang et al  <cit>  decomposed the existing diversity of  <dig> different enrichment methods into three broad classes: singular enrichment analysis , gene set enrichment analysis  and modular enrichment analysis . sea represents the traditional linear enrichment analysis strategy and approaches in this category evaluate ontology classes one-at-a-time for enrichment against a fixed list of interesting dataset members using a statistical test like fisher's exact test following the hypergeometric distribution. methods in this class vary according to the statistical test employed, the criteria by which the dataset is selected and any special heuristics or weightings applied during analysis. the gsea class of methods, which includes the original gene set enrichment analysis  technique  <cit> ) as well as the more recent random sets  <cit>  and lr path  <cit>  methods, take advantage of experimentally derived weights to evaluate the entire dataset. methods in the mea category evaluate the enrichment of multiple ontology classes simultaneously by taking into account the full network of ontology and dataset relationships. similar to the methods in the sea category, most mea methods do not consider continuous instance weights and must therefore be run against a fixed list of interesting data set members.

the mea category includes the mcoa-based enrichment analysis approach described in this paper as well as a number of state-of-the-art techniques developed since the publication of the huang et al  <cit>  survey such as noa by wang et al  <cit> , topogsa by glaab et al  <cit> , gengo by lu et al  <cit>  and mgsa by bauer et al  <cit> . noa attempts to capture the functional enrichment of inter-instance relationships through the calculation of link annotations and subsequent application of standard statistical enrichment methods to these annotations . topogsa supports the visualization and analysis of network analytic properties for gene and protein sets mapped to interaction networks. gengo and mgsa, which both adopt a generative probabilistic model of gene activation, are particularly well suited to the challenge of class overlaps in the presence of noise and are among the best methods in terms of benchmarked enrichment performance. gengo uses the generative model and a maximum likelihood approach to identify a small set of go categories that best explains an observed experimental gene list. p-values for this optimal set of go categories are then computed using the standard fisher's exact test, or any other desired test statistic, including optional multi-hypothesis correction. motivated by the gengo approach, bauer et al  <cit>  also adopted a generative model of gene activation for their mgsa method. mgsa uses a bayesian network to model gene activation and represents go enrichment using the marginal posterior probabilities of each go category computed using a markov chain monte carlo algorithm. rather than identifying a fixed set of classes that maximize the objective function based on the generative model, mgsa provides a posterior probability enrichment score for all classes. although not directly comparable against gsea methods, when evaluated against existing sea and mea methods, the gengo and mgsa methods are significantly better, as measured on synthetic data, at correctly identifying enriched go categories while minimizing reported false positives and false negatives  <cit> .

despite the extensive use and high utility of enrichment analysis applications and the important recent advances made in the gsea and mea categories, existing analytical methods remain limited in their ability to successfully analyze the full spectrum of ontological and dataset complexity. challenging structural features include overlaps between ontology classes, continuous instance and annotation weights, relationships between instances, non-hierarchical relationships between classes, semantic distance and sparse data. these analytical challenges, and how current enrichment methods attempt to address them, are discussed in further detail below.

analysis challenges
class overlaps
methods in the sea and gsea categories commonly generate enrichment results comprising long lists of highly correlated classes, leaving users to determine which of multiple, largely redundant, classes are actually relevant. this problem is due to both the overlaps between class members and the fact that sea and gsea methods evaluate each class independently for enrichment and thus fail to take class interdependencies into account. overlaps between the member sets of different classes can result from several structural features:

• inheritance: one class is an ancestor of the other class and therefore all dataset members annotated to the descendant are implicitly annotated to the ancestor.

• multiple parents: both classes share a common descendant and therefore are implicitly annotated with the same dataset members.

• multiple annotations: a dataset member is annotated to both classes .

overlaps between classes are very common in practice with each go term overlapping with an overage of  <dig> other terms based on common human gene annotations . when overlaps between enriched classes exist because of multiple annotations, the results are also skewed in favour of instances associated with a large number of classes. this distortion can be particularly problematic for cases where annotation bias exists  or cases where the total amount of enrichment evidence should be based on the number of instances and their weights rather than on the number of annotations .

the class overlap problem has been explored by several existing enrichment analysis approaches including mgsa, gengo, parent-child union by grossmann et al  <cit>  and elim and weight by alexa et al  <cit> . the parent-child union, elim and weight methods all address overlaps by computing statistical enrichment using the hypergeometric distribution with counts weighted according to the hierarchical structure of the ontology. parent-child union computes enrichment for a specific class in the context of dataset members annotated to the parents of the class. elim removes genes annotated against enriched subclasses when computing enrichment for parent classes and weight generalizes the elim approach by adjusting gene weight to a value between  <dig> and  <dig>  because the weighting heuristics used by parent-child union, elim and weight utilize just the structure of the ontology, these methods only address overlaps due to inheritance or multiple parents. although gengo and mgsa are able to detect all cases of overlaps, the fact that these methods collapse the ontology hierarchy means that they are unable to distinguish between the different cases of overlap, which impacts support for semantic distance and annotation bias.

continuously valued data
a key drawback of methods in the sea category and most methods in the mea category is their inability to model continuously valued data. for most biological data of interest in an enrichment analysis scenario, dataset members have varying levels of experimental significance and continuous weights can be associated directly with each instance  or with each instance-to-class annotation . continuous weights can also be associated directly with classes or with inter-instance and inter-class relationships . analyzing continuously valued datasets using sea or mea methods requires the use of an arbitrary cut-off with all dataset members or annotations above the cut-off given equal weighting in the analysis, potentially leading to significantly skewed enrichment results. addressing this shortcoming is the primary objective of methods in the gsea category including gene set enrichment analysis   <cit> , which computes statistical significance for all genes in all differentially expressed arrays using a weighted kolmogorov-smirov test; lrpath  <cit> , which uses a logistic regression likelihood ratio test compute significance of enrichment for all genes taking expression level into account; random-sets  <cit> , which incorporates quantitative instance scores to compute class enrichment values using an analytical approximation of the statistical distribution and is asymptotically equivalent to the lrpath technique; and probcd  <cit> , which supports probabilistic instance and annotation weights and computes statistical significance using goodman-kruskal gamma and comparison against a null distribution estimated via random permutations.

although the gsea methods avoid a potentially arbitrary dataset "cut-off" through the use of continuous dataset weights, this requirement can be problematic in cases where a single biologically meaningful value for each gene does not exist. gsea methods are further limited by their one-at-a-time analysis of ontology classes and, in practice, have been found to generate enrichment results very similar to those output by sea methods on actual experimental data  <cit> .

inter-instance relationships
meaningful relationships often exist between the members of the datasets targeted for enrichment analysis . network models are particularly well suited for representing the interconnections in real biological systems  <cit> . similar to the links in a social network or hyperlinks between web pages, such instance-level relationships provide evidence of a relative ranking between instances that can be quantified using network analysis metrics such as eigenvector centrality. the use of such network analysis techniques is commonly performed on biomedical networks comprising data instances. although the output from this type of analysis can be used to adjust the weight of genes for subsequent enrichment analysis using gsea category methods capable of handling continuous values, current state-of-the-art methods do not compute or use such metrics for enrichment analysis. while the noa method of wang et al  <cit>  does directly focus on the relationships between dataset members, the goal of this approach is a functional analysis of the gene-to-gene links themselves rather than the use of gene-to-gene links to adjust the functional enrichment of specific genes. analysis of datasets lacking links between dataset members is not possible with noa.

non-hierarchical class relationships
standard enrichment analysis only considers hierarchical relations between classes , however, many relevant biomedical ontologies, including go, include non-hierarchical class relationships . accounting for such inter-class relationships may be even more relevant in scenarios where multiple inter-related ontologies are jointly analyzed and inter-class relationships are used to capture mappings between classes in different ontologies . although the same network analytical methods used to analyze instance-level links can be applied on the ontology graph, the current set of state-of-the-art enrichment methods do not do so, and, for most enrichment approaches, their incorporation is not feasible due to the nature of the underlying statistical tests.

semantic distance
when analyzing data against hierarchical ontologies, it is generally desirable to bias more specific classes over more general classes when both classes are associated with the same number of dataset members. standard sea category methods like fisher's exact test measure significance based solely on annotation frequency and ignore semantic distance. although semantic distance is incorporated into methods such as parent-child union, elim and weight, the state-of-the-art mea methods gengo and mgsa use flattened representations of the ontology and therefore fail to explicitly incorporate semantic distance.

sparse data
real datasets frequently suffer from sparsity due to a variety of data collection and experimental design issues  <cit> . bayesian approaches, which incorporate prior probabilities based on knowledge about the likely statistical distribution of the data, are better able to handle sparse data then frequentist approaches like those based on fisher's exact test, which need to employ some type of smoothing . bayesian methods that perform enrichment analysis using a prior probability distribution include mgsa and the baygo framework  <cit> . although these bayesian methods enable the enrichment analysis of sparse data, their lack of support for inter-instance relationships, non-hierarchical class relationships and semantic distance means that only a limited range of sparse datasets can currently be analyzed.

methods
our approach represents the combination of the classes in an ontology and the instances in an associated dataset as a single finite ergodic markov chain whose adjusted transition probability matrix is used to compute modified eigenvector centralities, or steady-state probabilities, for each class. these modified eigenvector centralities, a quantity we term the information rank, provide a measure of the importance of each class relative to both a dataset and the other classes in the ontology. similar to annotation frequency, the information rank of a class can be used to support applications that compare the importance of a class in a target dataset with a baseline dataset .

ontology model
for defining our approach and discussing other related methods, we follow bade et al  <cit>  and cimiano et al  <cit>  and adopt a simplified formal model of an ontology and its extension as a rooted hierarchy with instance assignments. although both our analysis approach and many related techniques can be generalized to more complex structures, as formalized by the description logic-based models  <cit>  used for popular ontology modelling languages such as owl, this minimal structure contains the essential modelling primitives for evaluating go enrichment analysis and allows the methodology to be developed with minimal descriptive complexity. definitions  <dig> and  <dig> below formally define the ontology model. potential extensions to this model include class weights, weights for inter-class relationships, weights for instance-to-class relationships and weights for inter-instance relationships.

definition  <dig> : an ontology is a directed acyclic graph of classes structured in a hierarchy and represented by the tuple o = 〈c, parent〉

• c is a non-empty set class identifiers

• a strict partial ordering relation parent that maps each class c in c to the set of direct parents of c in the class hierarchy. ∀c ∈ c: parent ⊆ c

definition  <dig> : the extension of an ontology is represented by the tuple e = 〈i, type, rel, weight〉

• a potentially empty set i of instance identifiers

• an instance type relation type that maps each instance in i to a set of one or more classes in c. ∀i ∈ i: type ⊆ c

• an inter-instance relation rel that maps each instance in i to a set of zero or more other related instances in i. ∀i ∈ i: rel ⊆ i

• an instance weight relation weight that maps each instance in i to a normalized weight between  <dig> and  <dig>  ∀i ∈ i: weight ∈  <dig> <cit> 

markov chain model
our proposed methodology for analyzing an ontology relative to a collection of domain data represents the combination of an ontology and its extension as a finite ergodic markov chain. a finite markov chain is a finite stochastic process in which the probability of transitioning from a state i to a state j is only dependent on the state i and not on the path taken through the chain to arrive at state i  <cit> . this property of a markov chain is called the markov property and, for an ergodic markov chain, it enables the state transitions to be represented as a stochastic matrix with the special property of possessing a principal left eigenvector for the maximum eigenvalue of  <dig>  the components of this principal left eigenvector represent the steady-state probabilities for each state in the chain. definition  <dig> below provides the formal specification of a markov chain.

definition  <dig> : a finite ergodic markov chain is a finite stochastic process characterized by:

• a non-empty set of states s of size n

• an n × n transition probability matrix p where each entry pij represents the probability that the state will be j if the current state is i.

• by the markov property, the transition probability values pij are only dependent on the current state i. therefore:

 ∀i,j,pij∈ <cit>  

 ∀i, ∑j=1npij= <dig> 

• the transition probability matrix for a markov chain is a stochastic matrix with a principal left eigenvector, e →, of length n for its largest eigenvalue of  <dig> 

• for a finite ergodic markov chain, the components of this principal left eigenvector are the steady-state probabilities, or eigenvector centralities, of the states of the markov chain.

core mcoa process
at the core of our methodology is a process for computing an eigenvector-based score for each class in an ontology relative to an extension of that ontology . we call this the information rank based on its similarity to the well-known pagerank algorithm for computing the ranks of web pages using a markov model of a random walk with jumps through web page links  <cit> . the mcoa process involves three key steps:

• step 1: model the ontology and extension as a single finite ergodic markov chain.

• step 2: create an adjusted transition probability matrix for the markov chain.

• step 3: use the transition probability matrix to compute the eigenvector-based steady-state probability and information rank for each ontology class.

algorithmic details for each of these steps are outlined below and formalized in definitions  <dig>   <dig>   <dig> and  <dig>  figure  <dig> illustrates these steps for a simple ontology.

step 1: model ontology and extension as markov chain
our approach builds a markov chain model of an ontology and its extension by mapping classes in the ontology and the instances of those classes to states in the markov chain and by mapping all instance-to-class relations and hierarchical relations between classes to state transitions. given the simplified model of an ontology and its extension specified in definitions  <dig> and  <dig> and the model of a finite ergodic markov chain specified in definition  <dig>  the process for building a markov chain from an ontology and its extension is formalized in definition  <dig> below. figure 1b shows an example markov chain for the ontology in figure 1a generated according to this mapping.

definition  <dig> : the mapping between an ontology o and its extension e  and a finite ergodic markov chain is characterized by:

• a partitioning of the set of markov chain states into two disjoint subsets sc, which contains the states corresponding to ontology classes, and si, which contains the states corresponding to ontology instances:

 s=sc∪si,sc⊄si 

• equivalence mapping class ) between the states in subset sc of the markov chain and the classes in set c .

• equivalence mapping inst between the states in subset si of the markov chain and the instances in set i .

step 2: create adjusted transition probability matrix
calculating the transition probability matrix for the markov chain defined above involves three key adjustments:

• a random jump probability α. this is equivalent to the damping factor, d, used in the pagerank algorithm, specifically α = 1-d.

• a parameter, ω, that controls how much of the random jump probability is distributed among class states, sc, vs. instance states, si

• the weights of each individual instance, as specified by the function weight

using these parameters, the creation of the adjusted transition probability matrix can be formalized according to definition  <dig> below. figure 1c contains the adjusted transition probability matrix created for the ontology in figure 1a according to this process.

definition  <dig> : the adjusted transition probability matrix p for the finite ergodic markov chain that represents an ontology and its extension, as specified in definition  <dig> above, is defined by:

• a random jump parameter, α, which determines the probability that the markov chain makes a random jump to one of the other states rather than following the defined transitions from that state.

• a probability distribution weight, ω, that determines how probabilities are distributed between states representing classes, sc, and states representing instances, si, following each random jump. if ω =  <dig>  random jump probability is distributed only among instance states, likewise, if ω =  <dig>  random jump probability is distributed only among class states.

• the instance weight function weight, which is used to compute a potentially non-uniform distribution of random jump probabilities among the instances.

• given the definitions above, the entries pij of the n × n transition probability matrix p are defined as follows :

 pij=si∈sc:sj∈sc:class∈parent):1-α∣parent)∣+αω∣sc∣class∉parent):αω∣sc∣sj∈si:α1-ωweight)∑n∈iweightsi∈si:sj∈sc:class∈type):1-α∣type)+rel)∣+αω∣sc∣class∉type):+αω∣sc∣sj∈si:inst∈rel):1-α∣type)+rel)∣+α1-ωweight)∑n∈iweightinst∉rel):α1-ωweight)∑n∈iweight 

the use of the random jump and non-uniform distribution parameters defined above has several benefits in the context of our method:

• it ensures that the markov chain is ergodic .

• it allows for prior probability smoothing. classes without instances can be assigned a configurable portion of the random jump probability as a form of prior probability smoothing. by varying the ω parameter between  <dig> and  <dig>  the relative weight of a uniform prior probability distribution can be adjusted relative to the analyzed dataset distribution.

• it enables the use of class and instance weighting. similar to the topic-sensitive pagerank approach  <cit> , a non-uniform distribution of random jump probabilities can be used to mirror differential class and instance weights.

• it allows semantic distance to be quantified. the amount of transferred rank naturally decays as one moves up the hierarchy.

step 3: compute information rank
given an adjusted transition probability matrix as specified in definition  <dig> above, the importance of each class relative to the dataset can be quantified using the components of the principal left eigenvector that correspond to classes in the ontology. these eigenvector components represent the steady-state probabilities of the class states in the associated markov chain. normalizing these steady-state probabilities relative to the probabilities for all class states and then taking the negative log of the normalized probabilities generates the information rank. the definitions of steady-state class probability and information rank are formalized in definitions  <dig> and  <dig> below. figure 1d shows the information rank values for the example ontology in figure 1a.

definition  <dig> : given the definitions above, the adjusted steady state probability for a class c in c is defined as the ratio of the principal left eigenvector component for the markov chain state corresponding to that class divided by the sum of all class eigenvector components:

 ∀c∈c:ssp=e →state∑s∈sce →s 

definition  <dig> : the information rank for a class c in c is defined as the negative base- <dig> log of the adjusted steady-state probability:

 ∀c∈c:ir=-log2) 

mcoa enrichment analysis
our initial application of the mcoa method to enrichment analysis adopts the probabilistic generative model of gene activation used by both gengo and mgsa. it specifically extends the gengo maximum likelihood approach by adding mcoa-based terms to the objective function used in the original gengo algorithm. although our initial enrichment analysis method extends gengo, mcoa can be integrated with other enrichment methods or used directly to determine enrichment significance by employing permutation tests to compute a distribution of possible information rank values. our choice of gengo as a base approach was motivated by several factors:

• gengo is one of the best state-of-the-art methods. gengo and mgsa are two state-of-the-art mea approaches shown to provide overwhelmingly superior enrichment performance on simulated data.

• gengo is feasible to extend. integration of mcoa through modification of the objective function was both feasible and straightforward.

• gengo returns intuitive results with flexible statistics. the gengo process outputs p-values, using the statistical test of choice, for the set of categories that maximize the log likelihood objective function. use of p-values, as opposed to the marginal posterior probabilities used by mgsa, make the results of this method more intuitive to researchers and more easily comparable to the results from other enrichment methods. use of multiple hypothesis correction is also optional.

execution of the mcoa enrichment analysis algorithm involves three steps:

• step 1: compute steady state probability scores for the ontology relative to both the reference and target datasets.

• step 2: find the set of ontology classes that maximizes the likelihood of the observed dataset given a probabilistic generative model.

• step 3: compute p-values and apply multi-hypothesis correction.

algorithmic details for each of these steps are outlined below.

step 1: compute steady state probability scores for the ontology relative to both the reference and target datasets
this step follows the core mcoa process outlined above.

step 2: find the set of ontology classes that maximize the likelihood of the observed dataset given a probabilistic generative model
the mcoa approach modifies the gengo objective function by replacing the α|c| term that penalizes the sizes of active go categories by a term computed from the mcoa-based steady state probability scores for each active category. this modification of the gengo objective function to incorporate mcoa steady state probability scores as a regularization parameter is similar to approaches taken for snp selection during gwas analysis in which the objective function for a stochastic wrapper algorithm is modified to include preprocessed attribute quality estimates  <cit> . this replacement term, which is equivalent to a weighted log-odds value, still penalizes large sets of active go categories while also giving a preference to those categories whose steady state probability is larger in the target dataset than in the reference dataset. where the steady-state probability ratios are equal for two categories, the weighting acts to prefer the category with a greater steady state probability in the target dataset. similar to the original gengo method, mcoa optimizes the objective function via a greedy search algorithm. optimization of the p and q values also follows the gengo approach. although originally specified in terms of go categories and genes, this approach can be easily generalized to the generic ontology model outlined earlier in the paper and this generalized description is used in the formal definition of the modified objective function in definition  <dig> below.

definition  <dig> : the mcoa method modifies the gengo log-likelihood function by replacing the α|c| regularization term with β∑c∈clogssptar2sspref. the complete modified objective function is:

 l=∣ag∣logp+∣an∣logq+∣sg∣log+∣sn∣log+β∑c∈clogssptar2sspref 

where:

• c is the set of active ontology classes

• g is the set of active instances

• q is the false positive rate or the percentage of instances not associated with an active ontology class that are activated

•  is the false negative rate or the percentage of instances associated with an active classes that are deactivated

• ag is the set of active instances annotated with at least one active class

• an is the set of active instances not annotated with any active classes

• sg is the set of annotations  between inactive instances and active classes

• sn is the set of annotations  between inactive instances and inactive classes

• sspref is the steady state probability for ontology class c computed using the reference dataset

• ssptar is the steady state probability for ontology class c computed using the target dataset.

• β is a parameter that weights the steady state probability regularization term.

step 3: compute p-values and apply multi-hypothesis correction
for the set of ontology classes that maximizes the objective function, p-values can be computed using any desired statistical test. similar to the original gengo method, the current implementation of mcoa computes p-values using the hypergeometric distribution. if desired, multiple hypothesis correction methods can also be applied to the generated p-values. an important benefit of this approach is that multiple hypothesis correction only needs to consider the subset of classes that maximize the objective function rather than all classes in the ontology.

go enrichment analysis of simulated data
to demonstrate the utility of the mcoa methodology for enrichment analysis of biomedical data, we compared the performance of the mcoa method against gengo , mgsa, alexa et al's weight method  <cit> , grossmann et al's parent-child union and the standard hypergeometric test for gene ontology enrichment of simulated drosophila melanogaster, homo sapiens and escherichia coli data sets. for the gengo, mgsa, weight, parent-child union and hypergeometric methods, we used the implementations and configurations from the ontologizer framework that were employed to generate the benchmarking results in bauer et al  <cit> .

to enable comparison with prior work, our benchmarking process follows the general approach adopted by bauer et al  <cit> , lu et al  <cit> , grossmann et al  <cit>  and alexa et al  <cit> . this process builds a test gene list using a pre-selected set of active go categories, with specific false negative and false positive rates, and then evaluates each enrichment analysis method, using precision/recall metrics, based on its ability to identify the originally selected categories within the noisy dataset. the following parameters control the creation and analysis of the simulated datasets following this approach:

• source of go annotations: creation and analysis of the simulated datasets was performed using the following ontology and species annotation files downloaded from the source control repository links on the gene ontology website  <cit> : gene ontology , drosophila melanogaster annotations from flybase  <cit>  , the homo sapiens annotations from go annotations @ ebi  <cit>   and the escherichia coli annotations from ecocyc  <cit>  & ecolihub  <cit>  .

• selection of active go categories: following prior work  <cit>  we varied the number of active go categories between  <dig> and  <dig> and avoided selecting hierarchically related categories. also following prior work  <cit> , we filtered the set of potential active categories to remove categories with fewer than  <dig> annotations. such a minimum annotation threshold helps ensure that the selected categories are more likely to be biologically meaningful in the context of experimental data analysis . whereas lu et al  <cit>  used categories with  <dig> or more direct or indirect annotations, we have chosen to filter based on just direct annotations. our motivation for using direct as opposed to total annotations is several-fold:

 <dig>  generate datasets using a more accurate distribution of categories. filtering on the total number of annotations results in the disproportionate removal of leaf categories. for the versions of go and the drosophila melanogaster annotations used for our benchmarking,  <dig> % of the  <dig>  directly and indirectly annotated go categories are leaf terms. if all categories with fewer than  <dig> total annotations are removed from this set, the total proportion of leaf categories falls to  <dig> % of the remaining  <dig>  annotated categories. if filtering is instead based on direct annotations, the proportion of leaf categories remains essentially constant at  <dig> % with  <dig>  categories left in the set. both types of filtering effectively maintain the overall distribution of categories by level  with a correlation coefficient of . <dig> between the unfiltered distribution and the direct annotation filtering and . <dig> for total annotation filtering. this pattern is similar for the other evaluated species.

 <dig>  create simulated datasets that are more consistent with a generative model of gene activation. categories with very few or no direct annotations are more likely to be high-level grouping constructs with low analytical value than categories with at least a few direct annotations. a direct annotation for a high-level category provides evidence that the category, rather than one of its subcategories, has been found by curators to provide the best explanation for a specific piece of experimental data. we believe that requiring such evidence for active categories results in datasets that better reflect a generative model of gene activation and represent more biologically meaningful categories.

 <dig>  create simulated datasets that highlight key analytical challenges. filtering based on either direct or total annotations creates a dataset with a high mean annotation level and increased level of class overlaps. filtering by direct annotations has the added benefit of generating datasets with a larger ratio of direct-to-indirect annotations, highlighting the challenge of differentiating between these types of annotations during enrichment analysis, a distinction ignored by most enrichment methods. with no filtering, each go category with drosophila annotations has an average of  <dig> direct and  <dig> total annotations. requiring a minimum of  <dig> direct annotations results in a set of potentially active categories with an average of  <dig> direct and  <dig> total annotations. if a minimum of  <dig> total annotations is required, the set of active categories has an average of  <dig> direct annotations and  <dig> total annotations.

• false positive rate : probability that a gene not associated with an active category is activated. gengo tested with fairly low false positive rates of  <dig>  and  <dig> . mgsa reported results for false positive rates of  <dig>  and  <dig> . the results shown below use a value of  <dig> , which corresponds to one of the mgsa values and is between the two gengo values. simulations were also performed for false positive rates of . <dig> and . <dig> and results can be found in additional files  <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> 

• false negative rate : probability that a gene associated with an active category is deactivated. gengo reported primary results for false negative rates of  <dig>  and  <dig> . mgsa reported results for false negative rates of  <dig>  and  <dig> . the results shown below use a value of  <dig> , which matches one of the gengo settings and is in the between the two mgsa values. simulations were also performed for false negative rates of . <dig> and . <dig> and results can be found in additional files  <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> 

• enrichment threshold for precision/recall calculations : the prior benchmarking work by bauer et al  <cit> , lu et al  <cit>  and others computed precision/recall statistics on the rank ordering of analyzed categories irrespective of the actual enrichment significance assigned by the analysis method. although this is a straightforward evaluation approach that makes comparative evaluation easier, it fails to accurately reflect the performance or actual usage patterns of the underlying enrichment analysis methods. even though a given method may return all active categories  with only a few false positives , if few of the active categories had enrichment p-values that were significant, a user would have ignored most of these valid results, making the reported precision/recall values misleading. similar issues also occur when generation of significant enrichment values for the top set of valid categories also results in significant enrichment values for a much larger set of invalid categories. users analyzing such a result set would need to consider a much larger set of significantly enriched categories despite the high reported precision/recall. given these factors, we also compared enrichment methods using precision/recall numbers generated using only categories with significant enrichment scores after multiple hypothesis correction. we used a threshold of  <dig>  for the mgsa marginal posterior probability, which is the level at which categories are more likely than unlikely according to mgsa . for all other methods, we used a p-value threshold of  <dig>  after multiple-hypothesis correction using the bonferroni method.

go enrichment analysis of parkinson's gene expression data
to demonstrate the utility of the mcoa method on real experimental data, we compared the enrichment results generated by mcoa, gengo, mgsa and the standard hypergeometric test on differentially expressed genes from a study of parkinson's post-mortem brain samples available in the gene expression omnibus   <cit>  as dataset gds <dig>  <cit> .

the r geoquery package  <cit>  was used to retrieve both the raw microarray data and the genes associated with the array platform, which were used as the reference gene list for subsequent enrichment analysis. the set of genes significantly differentially expressed between cases and controls was computed using the r limma  <cit>  package by fitting a linear model, applying empirical bayes shrinkage to compute moderated t-statistics and finally using benjimani-hockberg multiple hypothesis correction. those genes with a false discovery rate below . <dig> were kept for further analysis and, following the recommendation of falcon and gentleman  <cit> , this set was divided based on t-statistic sign into a group whose expression was positively correlated with parkinson's cases and a group whose expression is negatively correlated with parkinson's cases. only the positively correlated group was considered for further analysis with the modified t-statistic values used as a gene weight for the mcoa method. using the modified t-statistic as a weight enabled us to leverage mcoa's ability to support continuously valued data. although the mcoa, gengo and mgsa methods are all able to estimate the false positive rate  and false negative rate  from the data, for this comparison, we ran all methods with fixed false positive and false negative rates of  <dig> . for mcoa, the regularization constant β was set to  <dig> .

implementation
to validate our approach, generate experimental results for this paper and analyze real biomedical data, we have created a prototype implementation of the mcoa core methodology and mcoa enrichment analysis method described above. the core mcoa method was implemented in java™ using jung  <cit>  for the creation of the graphical model and calculation of eigenvector components, apache commons math  <cit>  for basic statistical computations and jena  <cit>  for processing and reasoning over owl ontologies  <cit> .

the mcoa-based enrichment analysis method was implemented in java™ as an extension to the ontologizer  <dig> framework  <cit>  and the ontologizer implementation of the gengo algorithm. we used the ontologizer gengo implementation both to enable comparison with the mgsa benchmarking results and because the original gengo implementation is not accessible for extension. the benchmarking results reported below were computed using a modification of the ontologizer benchmarking framework used by bauer et al  <cit>  for evaluating mgsa with additional data processing and statistical computation performed via r.

the mcoa enrichment analysis application can be accessed at the project homepage  <cit> .

RESULTS
analysis challenge examples
to illustrate the computational behaviour of the mcoa method and the ability of this method to detect complex structural features, we computed information rank and information content values for a set of simple, domain-independent models that represent the analytical challenges outlined in the introduction section above. each model was generated as a synthetic owl ontology with associated instance data and, for all examples, the mcoa method was run with α =  <dig>  and ω =  <dig> . the ontology, dataset and analysis results for each example are shown in figure  <dig> 

• class overlaps. figures 3a and 3b illustrate the two key types of overlaps between non-hierarchically related classes. in figure 3a, the overlap is due to a single instance being associated with both c <dig> and c <dig> . as illustrated by the information content values, analysis based on annotation frequency ignores this overlap and assigns equal weight to c <dig>  c <dig> and c <dig>  the mcoa method, on the other hand, detects the overlap and divides the impact of the shared instance between c <dig> and c <dig> giving these two classes a higher information rank than c <dig>  in figure 3b, the overlap is due to classes c <dig> and c <dig> being associated with multiple parent classes. because classes c <dig>  c <dig> and c <dig> still have equal numbers of instances, they look identical from an information content perspective. the mcoa method also detects this type of overlap and correctly assigns c <dig> and c <dig> lower information rank values than c <dig> 

• continuously valued data. figure 3c contains a variation of the simple ontology from figure 1a in which some of the instances have been assigned continuous weights. as shown in the figure, a binary assessment of annotation frequency results in uniform information content values for classes c <dig>  c <dig> and c <dig>  the mcoa approach, because it generates a score that is sensitive to continuous weights, produces the correct differential ranking of c <dig>  c <dig> and, lastly, c <dig> 

• inter-instance relationships. in figure 3d, the members of the dataset are connected via inter-instance links with the c <dig> instances having a balance of in and out links, the c <dig> instances having net out-links and the c <dig> instances having net in-links. the mcoa methodology is able to directly integrate the impact of these links and, as shown by the information rank scores, correctly identifies a differential ranking of c <dig>  c <dig> followed by c <dig>  from the perspective of information content, all three classes appear identical.

• semantic distance. figure 3e provides a trivial example of semantic distance. because class c <dig> is the only child of class c <dig>  it is indistinguishable from an information content perspective. the information rank measure, through the random jump parameter α, reflects the relative semantic distance between the classes, with more specific classes given a higher weight. in this case, the mcoa method correctly assigned c <dig> a lower information rank than its parent c <dig> 

• sparse data. figure 3f shows a simple example of a sparse dataset in which one of the classes, c <dig>  lacks associated instances. the mcoa approach, when used with a non-zero α and non-zero ω, supports smoothing of sparse datasets through a form of prior probability weighting resulting from the uniform distribution of random jump probability. as shown in the example, this form of smoothing gives c <dig> a low, but non-zero, steady state probability and correspondingly high relative information rank.

results of go enrichment analysis of simulated data
using the benchmarking process outlined above, we tested mcoa enrichment analysis and the other state-of-the-art methods on simulated escherichia coli, drosophila melanogaster and homo sapiens datasets. figures  <dig>   <dig> and  <dig> display performance/recall curves for datasets generated for each of these species using a false positive rate  or  <dig> , a false negative rate  of  <dig> , a β of  <dig>  and a variable enrichment threshold . results for four additional false positive and false negative configurations are contained in additional files  <dig> , <dig> , <dig> , <dig> , <dig> , <dig> and  <dig> and relative execution time statistics are contained in additional file  <dig>  for each species and combination of false negative and false positive rates,  <dig> simulated gene lists were created and the performance of each analysis method was measured using average precision or area under the precision/recall curve.

as the precision/recall curves in figures  <dig>   <dig> and  <dig> show, the performance of the mea methods mcoa, gengo and mgsa dominate the comparable results of the weight, parent-child union and hypergeometric methods for all species and all parameter configurations.

when precision/recall metrics are calculated irrespective of enrichment values, as show in figures 4a, 5a and 6a, the mcoa method performs measurably better than gengo for all species, slightly better than mgsa on e. coli and homo sapiens and on par with mgsa for drosophila . figures 4b, 5b and 6b show these same results with only statistically significantly enriched go categories counted as positives for precision/recall statistics. when enrichment significance is considered during precision/recall calculations, the performance edge of the mgsa method disappears and mcoa becomes the clearly superior approach . although p-value and marginal posterior probability thresholds are not directly comparable and a lower threshold for mgsa could plausibly be selected, which would narrow the average performance delta, any reasonable marginal probability threshold would still give mcoa a measurable performance delta over mgsa.

overall, the mcoa method provides superior enrichment performance across a range of species and experimental parameters. it is important to note that these benchmarking tests, in order to support comparison against other state-of-the-art methods, only reflect performance on data sets that exercise the class overlap and semantic distance challenges. on datasets that incorporate continuous data values, inter-instance relationships, non-hierarchical class relationships or sparse data, the relative advantage of the mcoa method should be even more significant.

results of go enrichment analysis of parkinson's gene expression data
the top ten enriched go terms returned by mcoa, hypergeometric, mgsa and gengo are listed in figure  <dig> . as shown in this figure, all of the top results returned by mcoa are specific, non-overlapping and associated with recently published findings linking the associated biological process, molecular function or cellular component to parkinson's disease. the top result, regulation of osteoclast differentiation, is supported by research linking parkinson's disease with low bone density/osteoporosis  <cit>  as well as the finding of rheumatoid arthritis as a comorbidity  <cit> . the second result, glucose homeostasis, is supported by the link between parkinson's disease and cortical hypometabolism  <cit>  as well as the association between insulin gylcation, glucose homeostasis and parkinson's  <cit> . the third result, lymphocyte mediated immunity, is supported by research that links neurodegeneration in a mouse model of parkinson's with the presence of cd4+ lymphocytes in the brain  <cit> . similar supporting research is present for the other top ten results .

the top go terms returned by the standard hypergeometric method are all at a very high level in the go tree  and a number of terms are redundant due to hierarchical overlap. although both gengo and mgsa generate results that are generally similar in content and specificity to those returned by mcoa, a close inspection reveals important differences impacting result quality and utility to experimental scientists. the second term in the gengo results, carbohydrate homeostasis, receives all relevant experimental annotations from the single child glucose homeostasis. glucose homeostasis should therefore be flagged for enrichment instead of carbohydrate homeostasis. because the mcoa regularization term penalizes semantic distance, it correctly ranks glucose homeostasis above carbohydrate homeostasis. gengo also fails to return go term lymphocyte mediated immunity in the top ten results and instead identifies the nearby, but less significantly enriched and more general, term leukocyte mediated cytotoxicity . in this case, all nine differentially expressed genes annotated to leukocyte medidated cytotoxicity are also annotated to lymphocyte mediated immunity. because mcoa divides the contribution of a gene between all annotated terms, the more granular lymphocyte mediated immunity with some direct gene annotations is preferred over leukocyte mediated cytotoxicity. gengo also includes the overly specific positive regulation of angiogensis rather than parent regulation of angiogensis. the parent is more appropriate since the other two children  vs. positive regulation of angiogenesis ). mcoa correctly identifies regulation of angiogenesis in the top ten results.

the results returned by the mgsa method have similar issues, when compared to mcoa, as the gengo results . in terms of utility for users, however, a more significant difference between mcoa and mgsa relates to mgsa's use of marginal posterior probabilities and the impact these probabilities have on ranking and interpretation of enrichment results. although both mcoa and mgsa identify many similar go terms in the top rankings, the marginal posterior probability rankings of mgsa can differ substantially from what is achieved when hypergeometric p-values are used on the terms that optimize the objective function. we believe that the use of hypergeometric p-values by mcoa and gengo leads to a top set of rankings whose relative order and statistical significance is more easily interpretable by scientists.

discussion
the challenge of biological complexity
ontology-based data analysis methods such as enrichment analysis and semantic similarity clustering have become critical tools for processing the experimental results of modern biomedical science. without the abstract lens of classifications such as go and kegg, the large gene and protein lists generated by molecular biological research would be difficult to analyze manually and almost impossible to compare meaningfully across experimental populations or species. despite the important role that these methods play in interpreting and guiding biomedical research, their utility has been hampered by the limitations of traditional analytical methods to handle the complex interdependencies present in real biomedical data and associated data models. the members of real biological datasets do not cleanly sort into independent classes but instead group into complex collections of nested and overlapping categories, with direct relationships between dataset members and a mixture of continuous and categorical data values.

tackling this complexity requires methods that perform a global, rather than local, analysis of the ontology and dataset to capture the full range of structural interdependencies and data values. although recent methods in the gsea and mea categories have made notable advances in this area, specifically in addressing class overlaps and continuously valued data, the interesting features of many biological datasets remain inaccessible to analytical tools. to help address the challenge of biological complexity, we developed the mcoa method as a network analytic framework capable of addressing the class overlap and continuously valued data challenges targeted by mea and gsea methods as well as supporting continuous relationship values, inter-instance relations, non-hierarchical class relations, semantic distance and sparse data.

advantages of the mcoa markov chain model
underlying the mcoa method's analytical behaviour and its ability to successfully detect structural complexity is the method employed for building a markov chain model and computing steady state probabilities. several features of the mcoa markov chain model are critical to its functionality:

• assignment of probabilistic weight per instance rather than per annotation. under the mcoa markov chain model, the weight for each dataset instance is divided among all of the classes to which the instance is annotated. this weight is initially divided among all direct annotations of the instance and, as it propagates through the markov chain, consolidates in an increasingly smaller number of classes until the entire instance weight is concentrated at the root. the mcoa approach contrasts with the annotation frequency approach in which the full instance weight is assigned to each annotated class with the effect that instances shared by many classes contribute the same weight as instances annotated to only a single class. mcoa uses the differential contribution of instances with a large number of class annotations and those with small number of annotations to help detect class overlaps resulting from multiple annotations and multiple parents.

• flexible relationships. traditional analysis methods only model hierarchical class relationships and class-to-instance annotations. some methods, such as gengo and mgsa, ignore most hierarchical information by analyzing a collapsed representation of the ontology graph. the mcoa method, in contrast, analyzes the full ontology and dataset network and can additionally handle relationships, such as inter-instance relationships and non-hierarchical relationships between classes, that are important for modelling real biomedical data but are not directly supported by existing mea approaches.

• semantic distance computation. the use of a random jump parameter allows semantic distance to be quantified and hierarchical overlaps to be detected, since the amount of transferred rank naturally decays with each transition up the ontology hierarchy. although semantic distance is captured at some level by enrichment methods such as elim and weight, it is ignored by the more recent mea approaches gengo and mgsa as well as by techniques in the gsea category.

• continuous values for instances, classes and relationships. a non-uniform distribution of random jump probabilities can be used in the mcoa method to mirror differential class and instance weights. the markov chain model also enables continuous values to be applied to inter-class, class-to-instance or inter-instance relationships. with existing state-of-the-art analysis methods, support for continuous data values is usually limited to dataset instances.

• prior weighting. the non-uniform distribution of random jump probability also allows the mcoa method to apply any desired prior probability distribution to achieve smoothing of sparse data or to align with a bayesian analysis approach.

mcoa for enrichment analysis
we chose enrichment analysis as the context in which to explore and validate the functionality of the mcoa method. in developing and benchmarking a mcoa-based enrichment analysis approach, we aimed to create an enrichment tool with the best performance among existing state-of-the-art methods on simulated datasets created to highlight the complexities encountered in real biomedical data. we also aimed to create a practical methodology capable of generating enrichment results on real data sets that are specific, non-overlapping and of high utility to experimental biologists. the superior performance achieved by the mcoa enrichment analysis approach can be understood in terms of the kinds of type i and type ii errors encountered by the other generative methods  but avoided by mcoa.

in this context, type i, or false positive, errors represent cases where an enrichment method incorrectly identifies a non-active category as enriched. there were two varieties of type i errors commonly made by the other generative methods that were avoided by mcoa:

• incorrectly flagging non-active categories that are more general than an active category. in these cases, the more general category appears enriched because it is inheriting all of the annotations from the active category along with a significant number of additional annotations enabled due to noise. mcoa is able to correctly ignore these categories because the contributions from the active category are discounted due to both semantic distance and overlaps with other classes. gengo and mgsa, because they collapse the ontology graph and give each annotation equal weight regardless of the number of annotations, do not discount the contributions from the active category and incorrectly flag the more general category as enriched.

• incorrectly flagging non-active categories that are not hierarchically related to an active category, have a small number of associated genes and few or no direct annotations. in these cases, the non-active category appears enriched due to noise. because these categories have few annotated genes and almost no directly annotated genes, mcoa assigns the category a low steady-state probability and does not include it in the set of significantly enriched categories. because the other generative methods assign weight per annotation and ignore semantic distance, they give the category an incorrectly high weight and mark it as enriched.

type ii, or false negative, errors represent cases where an enrichment method fails to identify an active category as enriched. in our experiments, the other generative methods commonly failed to identify as enriched active categories that had a small number of directly annotated genes. when analyzed by mcoa, these categories have a higher relative steady-state probability due to both the lack of a semantic distance discount for the direct annotations and the fact that direct annotations will not have overlaps due to multiple parents. because of this higher relative steady-state probability, mcoa is able to successfully mark these categories as enriched. gengo and mgsa, on the other hand, do not give any special weight to the direct annotations and therefore fail to detect the relatively higher enrichment of these categories.

mcoa limitations
limitations of the mcoa method and mcoa-based enrichment analysis include a comparatively high computational complexity relative to other methods , reliance on the gengo approach for objective function optimization through greedy search and sensitivity to the specified values of the false positive and false negative rates .

other mcoa applications
although the discussion and examples in this paper have primarily focused on the use of the mcoa method for enrichment analysis, the same general approach can be used to support other ontology-based analysis applications, such as:

• semantic similarity clustering: semantic similarity algorithms that use the information content of classes  can be modified to use information rank instead.

• ontology evaluation: similar to the modification of semantic similarity algorithms, existing statistical ontology evaluation approaches that leverage information content  can be modified to use the mcoa-based information rank. the underlying steady state probabilities can also be employed to weight class-specific evaluation metrics when computing overall ontology evaluation scores.

• ontology-driven information retrieval. if the markov chain is created such that state transitions flow from classes in the ontology to instances, instance-level steady-state probability values can be computed that quantify the importance of each instance relative to the classes in the ontology.

• ontology comparative analysis. if state transitions flow from the classes, through a set of associated instances and into the classes in another ontology, it becomes possible to use the mcoa method to quantify the importance of one set of classes relative to another set of classes based on the annotations of a common dataset. comparative analysis of multiple ontologies can also be enabled through non-hierarchical relationships between the classes in one ontology and the classes in another ontology.

CONCLUSIONS
biomedical ontologies have become increasingly critical for the analysis, retrieval and integration of large and complex datasets. of particular importance are applications, such as enrichment analysis, that measure the importance of ontology classes relative to a collection of domain data. current analysis methods, however, remain limited in their ability to detect and accurately quantify a range of complex structural features at the ontological and dataset levels. to help address these challenges, we developed the markov chain ontology analysis  methodology and used this method to create the mcoa extension of the gengo enrichment analysis approach.

the core mcoa method can detect structural features including class overlaps, continuous data values, relationships between data instances, semantic distance and sparse data that are difficult to detect using standard annotation frequency analysis. in benchmarking studies on simulated escherichia coli, drosophila melanogaster and homo sapiens datasets highlighting the complexities of biomedical data, the mcoa enrichment analysis method provides the best performance of comparable state-of the-art gene ontology enrichment methods. on real experimental data, mcoa has been shown to provide specific, non-redundant and scientifically valid results.

as next steps, we plan to conduct benchmarking on datasets that capture a wider range of analytical challenges , use the mcoa enrichment analysis method to analyze and interpret additional experimental data sets, and perform enrichment against ontologies other than the gene ontology. we also plan to explore the use of the mcoa information rank value for applications that have traditionally employed information content, such as ontology evaluation and semantic similarity clustering.

an implementation of the mcoa-based enrichment analysis tool can be accessed at the project homepage  <cit> .

authors' contributions
hrf conceived of the methodology, implemented the mcoa algorithm and mcoa enrichment analysis method, performed the reported data analysis and drafted the manuscript. atm participated in the development of the methodology, selection and analysis of use cases and revision of the manuscript. both hrf and atm have read and approve the final manuscript.

supplementary material
additional file 1
gene ontology term overlap statistics with homo sapiens annotations.

click here for file

 additional file 2
benchmarking results on simulated escherichia coli data sets for false positive rate  of  <dig>  and false negative rate  of  <dig> .

click here for file

 additional file 3
benchmarking results on simulated drosophila melanogaster data sets for false positive rate  of  <dig>  and false negative rate  of  <dig> .

click here for file

 additional file 4
benchmarking results on simulated homo sapiens data sets for false positive rate  of  <dig>  and false negative rate  of  <dig> .

click here for file

 additional file 5
benchmarking results on simulated escherichia coli data sets for false positive rate  of  <dig>  and false negative rate  of  <dig> .

click here for file

 additional file 6
benchmarking results on simulated drosophila melanogaster data sets for false positive rate  of  <dig>  and false negative rate  of  <dig> .

click here for file

 additional file 7
benchmarking results on simulated homo sapiens data sets for false positive rate  of  <dig>  and false negative rate  of  <dig> .

click here for file

 additional file 8
benchmarking results on simulated escherichia coli data sets for false positive rate  of  <dig>  and false negative rate  of  <dig> .

click here for file

 additional file 9
benchmarking results on simulated drosophila melanogaster data sets for false positive rate  of  <dig>  and false negative rate  of  <dig> .

click here for file

 additional file 10
benchmarking results on simulated homo sapiens data sets for false positive rate  of  <dig>  and false negative rate  of  <dig> .

click here for file

 additional file 11
benchmarking results on simulated escherichia coli data sets for false positive rate  of  <dig>  and false negative rate  of  <dig> .

click here for file

 additional file 12
benchmarking results on simulated drosophila melanogaster data sets for false positive rate  of  <dig>  and false negative rate  of  <dig> .

click here for file

 additional file 13
benchmarking results on simulated homo sapiens data sets for false positive rate  of  <dig>  and false negative rate  of  <dig> .

click here for file

 additional file 14
relative execution time statistics on simulated homo sapiens data.

click here for file

 additional file 15
full analysis results for mcoa on geo dataset gds <dig> 

click here for file

 additional file 16
full analysis results for hypergeometric method on geo dataset gds <dig> 

click here for file

 additional file 17
full analysis results for mgsa method on geo dataset gds <dig> 

click here for file

 additional file 18
full analysis results for gengo method on geo dataset gds <dig> 

click here for file

 additional file 19
research linking top ten go terms returned by mcoa on geo dataset gds <dig> and parkinson's disease.

click here for file

 acknowledgements
this work was supported by an anonymous foundation and the harvard catalyst | the harvard clinical and translational science center . we thank the anonymous reviewers for their insightful comments and suggestions.
