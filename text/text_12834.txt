BACKGROUND
in a typical microarray experiment, thousands of genes have their relative expression levels measured in parallel under different biological states  <cit> . to identify differentially-abundant genes, most published methods  <cit>  progress through a similar sequence of elementary steps. first, a normalizing procedure is applied to make data sets comparable. if certain experimental conditions comprise several replicates, methods based either on parametric or nonparametric tests usually reduce the number of values generated by using their means. then, gene variation is quantified by a statistic derived from intensity measurements. knowledge of the null distribution of the gene variation, which is the distribution of its statistic when only random fluctuations occur, allows p-values to be assigned to observed variations and genes to be ranked according to the significance of their variation. as the test is repeated as many times as there are genes, p-values are corrected accordingly, and the false discovery rate  is estimated.

we describe here, in detail, a new analysis method that has been used to analyze the transcriptome in yeast  <cit> . this method is original in several respects. first, rank difference analysis of microarrays  replaces raw signal by its rank , expressed on a 0– <dig> scale, and we show that this simple transformation is a powerful normalizing procedure. also, rdam does not reduce replicated signals to their means, but instead only considers variations, expressed as rank differences , between individual experimental points. an essential step is the standardization of rd observed between two replicates, permitting easy access to the empirical null distribution and allowing accurate and precise p-values to be assigned to observed standardized rd . when dealing with replicated points, rdam uses a random variable, the product of p-values , for which the null distribution is straightforward to compute in a manner that is independent of the experimental conditions. finally, rdam estimates the total number of truly varying genes , assigns a p-value to each gene variation, characterizes the selection of a gene using the fdr and the percentage of truly varying genes included in the selection .

analysis of synthetic data sets allowed us to specify the error distribution of all the estimators used , and to demonstrate the strong predominance that the number of varying genes and the distribution of their variation have on the quality of the results.

we also analysed the transcriptional effects of the tor2-controlled signaling function using a genome-wide microarray approach in yeast. in s. cerevisiae, tor <dig> has two essential signaling functions. one, shared with tor <dig>  is required for translation initiation, transcription, and cell growth in response to the presence of nutrients  <cit> . the second is unique to tor <dig>  and functions in cell-cycle-dependent actin polarization and possibly in transcription  <cit> . a previous genetic screen for mutants defective in the tor-shared and the tor2-unique functions identified several tor <dig> temperature-sensitive alleles  <cit> . in this study, we compared total transcription profiles for strain sh <dig>  which is specifically defective in the tor2-unique function, and its isogenic wild type counterpart sh <dig>  <cit> .

RESULTS
standardization of positive variations
the simplest system to which our method can be applied comprises three experimental points, of which two are replicates, as described by the expression {exp1a, exp1b, exp2a}, where the number refers to the biological condition and the final letter refers to the replicates. to identify significant variations in the comparison exp2a vs. exp1a, we have to first calculate the variation of gene i, vari. from among several possibilities, we tested three different variation units: the fold change , corresponding to the ratio of signals, the signal difference , and the rank difference . the rd uses a standardized signal measure that is independent of the scanner settings, because the signal is replaced by its rank, expressed on a 0– <dig> scale. this normalizing procedure consists of first calculating the absolute rank  of each gene by ordering their signals from  <dig> to n  and then transforming the absolute rank value into a relative one . in this way, all the signals are expressed on the same scale and are directly comparable.

we studied the variation distribution between the two replicates, i.e. exp1a and exp1b, reasoning that the observed empirical variation distribution would be an excellent approximation of the null distribution corresponding to the null hypothesis. the null hypothesis we have in mind states that all observed mrna changes occurring under replicated conditions are due to a combination of biological and technological noise, and are not the result of any biologically significant process.

we first restricted our study to positive variations. as the distribution of positive variations should be the same in both comparisons – exp1b vs exp1a and exp1a vs exp1b – we plotted the positive variations against rank for both comparisons on a single graph. this revealed that the most salient property of variation distribution, common to all tested measures of variation, is its dependence on the signal rank. this is exemplified for the rd in figure 1a, which shows the absolute value of rd against the minimum of the ranks  - ri| vs min{ri, ri} for gene i). this mode of presentation can be interpreted in either of two ways: as a plot of the positive variations of both comparisons, or as the plot of the positive and the negative variations of a single comparison, with both of variation being represented by a positive number. whichever interpretation is prefered, it should be underlined that this presentation allows all the gene variations to be taken in account, and ensures the uniqueness of the resulting variation distribution.

we tried to eliminate dependency of positive variation distribution on the signal rank by standardizing variations according to the general formula:



where var is to be replaced by any one of the variation units tested . using this expression, the sample mean and standard deviation  were calculated for all genes having a rank within a given neighbourhood of ri, the rank of gene i. this notation reflects the fact that the var distribution is not gene specific, but rank dependent.

comparable results are obtained if the standardization is applied to the fc or the sd, but the zrd gives the best results in terms of distribution equalization: figure 2b shows, for example, that qqplots derived from zfc are more erratic than those derived from zrd, which are almost identical to the first diagonal up to the 99th percentile.

the fact that the zrd distribution is independent of rank can be explained by the fact that each gene's zrdi follows the same zrd distribution. therefore, we reasoned that the empirical cumulative frequency distribution, ecfd, approximates the distribution of zrd for any gene i under the null hypothesis, and we used fo =  <dig> - ecfd, based on a comparison between two replicated experiments, to assign a p-value to any zrd calculated in a comparison between two different biological conditions . because of the very large number of genes present on a chip, the null distribution is sampled a great number of times, generating a quasi continuous set of points that spans a wide range of values. this improves the precision of the fo curve and allows accurate p-values to be assigned for even large variations.

the entire procedure can then be applied to the comparison exp2a vs. exp1a. because standardization curves constructed on the basis of the two replicates are used in the standardization process, the calculated zrd can be justifiably compared to the null distribution and interpolated on the fo curve in order to assign a p-value to each gene variation. at this step positive and negative variations can be processed together, although it is necessary to keep track of the actual type of variation, i.e. positive or negative, in order to conduct subsequent analysis.

standardization of negative variations
we also tested to see if it is possible to apply the same standardization techniques to negative variations. in figure 3a, we have plotted the opposite of absolute rd value against the maximum of the ranks  - ri| vs max{ri, ri} for gene i). it is clear that the weak signals are characterized by a truncation of their variation distribution, as evidenced by the clear alignment of points between ranks  <dig> and  <dig>  this explains why the standardization procedure applied in figure 3b fails to equalize the variation distribution, and also why the power of the test is lower for down-regulated genes than it is for up-regulated genes .

false discovery rate , total variation  and sensitivity 
because the test is repeated n times, issues related to multitesting must be considered: the more tests that are performed, the more an outlier outcome becomes probable. in view of this, we first compute the observed distribution of zrd in the comparison exp2a vs. exp1a for increased and decreased genes, giving two curves: finc for the positive variations and fdec for the negative variations ). then we plot finc  and f <dig> on the same graph, corresponding to the observed positive  variation distributions and to the expected variation distribution according to the null hypothesis, respectively . in the following discussion, the rationale is the same for increased and decreased variations, and f stands for either finc or fdec . in most encountered situations, f is on top of f <dig>  f gives the probability, for any variant or invariant gene i, of observing a zrdi that is at least as high as x, and nf gives the corresponding number of genes. if  <dig> <= k <=  <dig> is the fraction of invariant genes, then knf <dig> is the number of invariant genes that have a zrdi at least as high as x. as a consequence, nf - knf <dig> is an estimate of the number of variant genes with zrd equal to or greater than x. we can call k the value of x that gives the maximum number of variant genes, such as nf - knf <dig> = max - knf0), and use this value as an estimate of the total variation , that is, the number of truly varying genes. as these quantities must verify the equation n = kn + tv, i.e., n = kn + nf - knf <dig>  we can deduce that .

for each value of x, we estimate the false discovery rate,



and the sensitivity,



in the context of a transcriptome analysis, p-values reflect how probable it is that a variation reaches or exceeds an observed value. p-values can always be used to rank genes, but the selection of significant variations in the context of multiple testing requires defining significance levels that are far more stringent than  <dig>  or  <dig> , as used in single testing. fdr and s parameters allow this difficulty to be overcome: for each c used as a potential critical value, s reflects the fraction of truly-varying genes that are selected by zrd>c, and fdr estimates the fraction of selected genes that are likely to be invariant genes. it is therefore possible to plot fdr and s against c, and to construct, for positive and negative variations, what we call a selection abacus .

analysis of replicates
the simplest example of a replicated experimental scheme is the system {exp1a, exp1b, exp2a, exp2b}. while it would be tempting to average signals or ranks for each experimental condition and apply the method described above, this is not possible because averaging changes statistics and we have no practical way of obtaining the corresponding empirical null distribution.

in a first round of comparison, we conducted two analyses in parallel by applying rdam to the first comparison exp2a vs. exp1a and to the second comparison exp2b vs. exp1b. based on this first round of comparison, we obtained two p-values for gene i: p1i and p2i. it could occur that gene i is detected as an increasing variation in the first comparison and as a decreasing variation in the second comparison. in this case, we apply a direction rule to decide on the final direction of variation. we consider simply that the lowest p-value is in favor of its corresponding variation direction, and we set the p-value of the discordant comparison to one. once we have calculated and possibly corrected the p-values, we construct a new random variable, the product of p-values, ppvi = p1i × p2i. to obtain an unbiased value for ppv, we apply the same procedure to a second round of comparison by exchanging exp2a and exp2b between the two comparisons, giving a second ppv value. the direction rule is applied to the two ppv before obtaining the final, averaged ppv.

the advantage of the random variable ppv is the ease of constructing its null distribution. in fact, the cfd is a uniform distribution over the interval  <cit> . therefore, for cases in which two independent comparisons between two sets of duplicates were to be considered, we constructed two sets, u <dig> and u <dig>  of  <dig> points uniformly distributed over the interval , to take into account the possibility of increased and decreased variation for each point. these sets were randomized to make them independent in order to model the independence of measurement according to the null hypothesis. this hypothesis states that all variations are due to noise, and that for a particular gene all corresponding p values must be independent. then, we apply the direction rule to the pair u <dig>  u <dig> and calculate ppv for genes that are detected as increased. thus, the f <dig> =  <dig> - cfd) curve allows the significance of any value for ppv to be tested. the significance of ppv combines the significance of variation within each individual comparison and the significance of the correlation between these variations. f curve is, as usual, the observed  <dig> - ecfd), and we get exactly the same kind of selector abacus, as shown in figure  <dig>  simulation of the ppv null distribution used exactly the same steps that the analysis process follows, i.e. application of the direction rule and construction of the product of p-values, and resulted in a null distribution model we found appropriate seeing, both with experimental and synthetic data sets, that the observed distribution of ppv matches the null distribution when no variation occurs .

the system {epx1a, exp1b, exp2a, exp2b} allows the construction of two sets of sandardizing curves, one from exp <dig> replicates and the other from exp <dig> replicates. as these curves are not equivalent, it is necessary to carry out both analyses and then use the more conservative one, whichever has the lower f curve.

the generalization of the entire procedure to more than two replicates is straightforward. for example, with three replicates {exp1a, exp1b, exp1c, exp2a, exp2b, exp2c}, there are  <dig> ×  <dig> ways of arranging the experiments in order to obtain different sets of comparisons. each round of comparison gives three p-values for the gene i - p1i, p2i and p3i – and the direction rule is applied the following way: in case the gene i is detected as an increasing variation in the first comparison and as a decreasing variation in the two other comparisons, we compare p1i to p2i × p3i and determine the variation direction. once we have calculated and possibly corrected the p-values, we obtain the product of p-values for gene i, ppvi = p1i × p2i × p3i. as the number of comparison rounds increases very rapidly with the number n of replicates , we simply apply a circular permutation – the circular permutation of abc consists in the subset of permutations abc, bca and cab – to the replicates inside one of the biological condition which allows the number of rounds of comparison to be restricted to the number of replicates .

generation of synthetic data
in order to test our method, we devised a way of generating synthetic data having similar statistical properties as real biological data. we selected two replicated experiments, exp1a and exp1b, as seeds for generating synthetic data, traced standardization curves, and calculated ecfd for . we randomly selected half of the genes and exchanged the signals for exp1a and exp1b, giving two new data sets exp1a' and exp1b'. random numbers uniformly distributed over interval  <cit>  are generated for each gene. each random number is interpolated on the inverse of the cfd of zrd to assign a random standardized variation zrdi to each gene i. new values ri are obtained by adding or subtracting to the rank r'i of exp1a' the rank difference rdi calculated by applying to zrdi the inverse of the normalization function . rank values are finally back-converted into signal values by interpolation of the rank on the graph of signal vs. rank constructed with one of the original data sets. this procedure allows two data sets to be obtained, exp1c and exp1d, which are statistically indistinguishable from the original data. to obtain a synthetic data set in which a pre-determined subset of genes receives a significant variation value we can possibly add a second step. we selected in exp1c and exp1d a random subset of genes, for example  <dig> increasing genes and  <dig> decreasing genes. to these genes, a second random variation value is applied, but instead of drawing random numbers on the interval  <cit> , we limit the selection to the interval . if we set the limiting p to  <dig> , then the variation applied to the subset will have a p-value <=  <dig> . for the genes receiving an additional variation contribution, the mean magnitude of zrd that is calculated between the synthetic and the original data is proportional to the magnitude of the applied p-value, as shown in figure  <dig> 

this entire procedure, composed of two successive steps, results in synthetic data sets of high quality, because the generation of data mimics the observed variation of genes. we compared the synthetic data sets exp1c and exp1d to the same natural data set exp1a, and plotted the corresponding zrd of one comparison against the other, as shown in figure  <dig>  we observed that these zrd were independent for genes that had not received an additional variation contribution, but were correlated for genes that had been changed. in general, however, this correlation was not absolute, except for some decreased genes. this phenomenon is explained by the high noise that characterizes weak signals: for such genes, there is a high probability that negative variation makes them reach the minimal rank value . for high signals, there also exists a limit for the rank variation, but the noise is very small, and the truncation effect is not visible. we also observed that a small proportion of genes receiving an increased  variation contribution could be detected as increased  in one comparison, but decreased  in another. all of these properties support the realistic nature of the synthetic data generated by our algorithm.

rdam performances
we generated several synthetic data sets from the two experiments  by letting the number of increased and decreased genes equal  <dig>   <dig> or  <dig>  and letting the maximum p-value of extra variation equal  <dig> ,  <dig> ,  <dig>  or  <dig> .

the first question we addressed relates to the effectiveness of our scoring method in discriminating among real variations: does the overall process rank the genes correctly? to address this point, genes were ranked according to their ppv, and the number of hits was computed in a series of sublists of increasing length, selected from the top. from this number of hits, and the number of genes in the sublist, we calculated the real s and fdr. plotting fdr against s allowed us to visualize the respective effects of the number of replicates, the magnitude of the variations, the number of varying genes, and the direction of variation on the performance of our rank difference method. figure  <dig> shows the effect of the first two parameters on the ranking of increasing genes, in this case  <dig> varying genes. the fdr <dig>  defined as the fdr observed when s = 50%, can be used to demonstrate these effects. increasing the number of replicates improves the scoring performance, but this improvement is strongly modulated by the magnitude of the applied variation. for example, when the number of replicates equals, successively,  <dig>   <dig> and  <dig>  the fdr <dig> equals, respectively, 85%, 74% and 41% for small variations  and 3%, 0% and 0% for large variations . for variations that we consider from our experience to be realistic, i.e., p <=  <dig> , the fdr <dig> is equal to 58%, 30% and 5%, respectively. the number of varying genes also has an important impact, since under the same variation conditions, but with  <dig> increased genes instead of  <dig>  we measured fdr <dig> values of 24%, 8% and 1%, respectively, which represents a mean decrease in the fdr <dig> of  <dig> percentage points.

the second question we addressed is the quality of the fdr and s estimators. the genes were ranked according to their estimated fdr , and the number of hits computed in a series of sublists of increasing length, selected from the top. from this number of hits, and from the number of genes in the sublist, we calculated the mean real fdr and s . both fdr and s are overestimated in this case, except for the point at 5% fdr in the groups with two replicates. if we consider individual comparisons, the distribution of errors has a higher variance for small estimator values. despite this dispersion of errors in the low fdr range, the absolute number of genes attributed to a faulty category is always negligible and mainly conservative , as shown in table  <dig>  figure  <dig> shows that the ratio between real and estimated s is rather constant, and we found that this ratio was close to the ratio between the estimated and real tv.

finally, we tested to see whether the independent analysis of positive and negative variations subsequent to standardization was dispensable, or if a one-step procedure could be used instead. in order to illustrate this point, we have constructed, from the experimental replicates exp1a and exp1b , a first group of two synthetic replicates having  <dig> increased and no decreased genes and a second group of two synthetic replicates without changed genes in order to reveal any clear differences that may exist between the one-step and the two-step procedures. table  <dig> shows the number of genes selected at several fdr levels when the two competing methods were applied to the comparison between the two groups of synthetic data we can see that with the one-step analysis the number of true positives is lower and the estimate of fdr is largely biased toward higher values relative to the two-step analysis.

comparison with sam on synthetic data
the generation of synthetic data is also a powerful tool for comparing different methods of analysis. as an example, we conducted a systematic comparison between rdam and sam. we selected this method because it is popular, easy to use , and can be considered as representative of numerous other nonparametric methods, which apply monte carlo procedures to estimate the distribution of the statistics used to quantify the relative difference of gene expression. figure  <dig> shows that for two and three replicates our scoring procedure generates less fdr than sam does across the entire sensitivity scale. for five replicates, the scoring procedure of sam is better only in the low sensitivity  range. in terms of practical gain, and particularly when experimental costs are considered, the improvement obtained with rdam is important because we have the same overall ranking quality as sam but with one replicate less. we also compared the errors made on the estimation of fdr by the two methods, when the nominal fdr equals 20% . we concluded that in this particular case fdr estimation was as good in rdam  than in sam . other comparisons show that this conclusion holds true for all other conditions used to generate synthetic data sets . however, we observed that sam estimator was unable to reach the nominal level of fdr detection in case of few replicates and/or small extra variations . in conclusion the large differences in the number of true and false positives found between the rdam and sam methods  are mainly explained by difference of scoring procedure efficiency between the two methods.

analysis of the tor experiment
when strains sh <dig> and sh <dig> were shifted to 37°c, rdam detected roughly 2300– <dig> genes as being either increased or decreased in each strain . most of these gene variations were caused by the temperature shift and were common to both strains, as shown by the differential analysis which detected only up-regulation of genes as a consequence of tor <dig> temperature inactivation:  <dig> genes at  <dig> h and  <dig> genes at  <dig> h . after  <dig> hours at 37°c,  <dig> annotated genes showed significant induction with a 10% fdr, whereas after  <dig> hours  <dig> genes were induced . however, these two groups of genes do not overlap, i.e. the shift to the nonpermissive temperature leads to a subsequent and transient increase in transcription of a small set of defined genes. we note that among these  <dig> genes,  <dig> are known to be regulated by the amino-acid-responsive transcriptional activator gcn <dig> . with a selection criterion of 20% fdr, five other gcn <dig> regulated genes are detected . therefore, it seems that inhibition of the tor2-unique function leads to an significant increase in the transcription of known gcn <dig> target genes. it is still unclear, however, how the tor2-unique pathway is connected to nutrient sensing or, vice versa, how nutrient sensing interferes with actin polarization.

comparison with sam on the tor experiment
we ran our method in parallel with sam in two situations displaying contrasting transcriptional responses. we tested a first comparison, wt-t <dig> vs wt-t <dig>  which is characterized by a high number of varying genes and a good reproducibility between replicates, facilitating the detection of changes as reflected by the results of rdam analysis which selected  <dig> increased genes at s = 50% and fdr = 6%. as sam does not detect any increased genes at this selection level, we compared results obtained by the two methods at fdr = 10% and observed that among the  <dig> and  <dig> genes selected repectively by rdam and sam, only  <dig> were found in common. these results match what we found with synthetic data sets in case of high strength of variation . we then considered comparisons of biological interest, i.e mu-t <dig> vs. wt-t <dig> and mu-t <dig> vs. wt-t <dig>  and in this situation rdam did not select any decreased genes and found only a few increased genes . on the contrary sam failed to detect any genes, either increased or increased.

discussion
analysis of the tor experiment
rdam is a method for identifying genes with changing expression levels using the user-determined fdr and/or s selection parameters. this method was used to study the effects of a thermosensitive mutation of tor <dig> in yeast. rdam succeeded in identifying the few genes that are differentially regulated by the tor mutation from among the entire mass of genes perturbed by the temperature shift. recently it has been shown that tor controls the translation of gcn <dig> via the eif4alpha kinase gcn <dig>  <cit> . under conditions of tor inactivation by rapamycin, gcn <dig> translation is enhanced, leading to the activation of gcn4-mediated transcription. our data also demonstrate that tor <dig> inactivation leads to enhanced transcription of gcn4-controlled target genes . further experiments may show how the tor2-unique function is integrated into nutrient-  responsive signaling pathways.

normalizing of signal
apart from randomly-distributed noise, microarrays are also prone to systematic effects that can bias the measurement of signal. all analysis methods are sensitive to systematic bias and include a preliminary normalizing step to make chips comparable. this is a limitation of this kind of approach, because the final result depends on the normalizing procedure used. considering that all normalizing procedures rely on monotonous transformations that do not change the rank of raw data, we reasoned that if we used a statistics based on rank there would be no need to optimize the normalizing procedure. the rank unit we describe is similar to quantile normalization  <cit> , but does not depend on the signal values of a particular chip as a reference: it can therefore be considered as an invariant. for example, if we focus specifically on the affymetrix platform, we observe that the signal distribution changes with the different versions of the software: in mas <dig>  the 50th percentile is around  <dig>  as compared to  <dig> in mas <dig>  in our system, the rank of the genes at the same position in the signal distribution would not change, and would always be roughly equal to  <dig>  this rank unit allows the drawing of plots in which all data are evenly distributed alongdimensions representing a signal. in addition, the linear density of points on the corresponding axes is constant, and the skewness of signal distribution has no effect on the graphical representation.

in our system, all values different from  <dig> or  <dig> are assigned to one and only one gene, because ordering of signals always delivers a series of contiguous rank values, even in cases of equivalent signal values.  <dig> is assigned to all unexpressed genes, as long as a robust method is available to detect them, and  <dig> to all genes for which the signal is saturated. in affymetrix technology, especially with the scanner setup presently used, saturation is not a matter of concern and in our analysis, the value  <dig> is simply assigned to the highest signal. it is a complex problem to identify genes that are not expressed in a given experiment, and we decided to consider as absent only genes having a signal less than zero, as they occur in results delivered by mas <dig> 

rank normalization results in transformation of the original signal distribution which is heavily skewed towards low values into a uniform distribution. as a consequence high rank variation could be assigned to small signal variations of weakly expressed genes, and it could be argued that our rank normalization method may bias variation detection towards genes with low signal. by using comparison between synthetic data sets we found no evidence of such a bias .

systematic usage of duplicates and standardization of variation
our method has been developed within the framework of hypothesis testing and requires knowledge of the variation distribution for each gene when the null hypothesis is verified. the rationale of our approach considers replicated experiments as precisely representing a system in which all genes follow this hypothesis. however, it has long been recognized that the variation distribution expressed as a ratio or fold change is dependent upon the level of gene expression  <cit> , and we show here that this property subsists when difference of signals or difference of ranks is used to measure variation. in theory it could be possible to use numerous replicates to obtain the empirical variation distribution of each gene. this is not possible for practical reasons, however, and we found that the classical centered-reduced standardization procedure can render variation distribution totally independent of gene expression level, as demonstrated by the qqplot analysis of figure 2a, and allow us to use duplicates to obtain the null variation distribution.

independent analysis of positive and negative variations
in the algorithmic implementation of our method, we chose to proceed in two steps and deal with increased and decreased variations independently. first, this ensures that symmetrical comparisons  give perfectly symmetrical results. second, even if it were possible to devise another method that would allow one to proceed in one step, it seems more logical to consider increased and decreased variation separately. to clarify this point, a clear distinction must be made between up- or down-regulated mrnas and increased and decreased variation. regardless of the experimental points that are being compared, one always observes increased and decreased variations, but these variations have no absolute meaning because one only has to reverse the comparison to change the direction of variation. on the contrary, we can speak of up- or down- regulated mrnas only when a causal effect exists, such as in a differentiation process or a kinetics or drug assay. in other words, a positive variation observed, for example, between two successive time points in a kinetic can be considered as an up-regulation whatever its mechanism – gene or post-transcriptional regulation – but it is meaningless to invoke any particular form of regulation when comparing, for example, two unrelated cancer tissues. in the case of down-regulated genes, the variation distribution of all weakly-expressed genes is truncated, due to the impossibility of a decreasing signal crossing the zero line. in the case of up-regulated genes, we do not observe the same effect for increasing variation of highly-expressed genes, first because the signal distribution is heavily skewed towards low values, and second because the variance of highly expressed genes is very small . the observation that the reproducibility of variation was lower for down-regulated genes than it was for up-regulated genes  <cit>  is partly explained by this reason, and we were able to demonstrate the statistical difference between up- and down-regulated genes by using synthetic data and observing that all fdr vs. s curves  constructed with down-regulated genes were lower than the corresponding curves for up-regulated genes . moreover, we conducted a test showing that the joint analysis of increased and decreased variations degrades the quality of fdr estimation and reduces the number of true positives detected.

replicates
we did not try to reduce the amount of raw data when using replicates, and devised a two-step method. a first statistics, the standardized rank difference zrd is constructed on each independent comparison, and p-values are assigned by considering an empirical distribution that matches the null hypothesis. then a second statistics, the product of p-values ppv, is calculated and p-values are assigned from the null distribution obtained by simulation. simulation of the null distribution used exactly the same steps that the analysis process follows, i.e. application of the direction rule and construction of the product of p-values, and resulted in a null distribution model we found appropriate seeing, both with experimental and synthetic data sets, that the observed distribution of ppv matches the null distribution when no variation occurs . it turned out that this scheme is very flexible and of general applicability: because the second step is rooted in a rigorous statistical method that uses only p-values as input data, it is possible to adapt or to improve the entire process simply by focusing on the first step of p-value estimation. for example, to apply our method to cdna glass arrays, the only step to be modified would be the variation standardization. alternatively we could use the segmental approach proposed by yang and colleagues  <cit> , which is claimed to equalize log ratio distribution, or the variance stabilization method of huber et al  <cit> , which is efficient in equalizing variation distribution of transformed intensity measurements in both cdna and oligonucleotide platforms.

fdr, total variation and sensitivity estimation
the way in which we estimated fdr is exactly the same as that suggested by b. efron et al. in their demonstration of the equivalence of empirical bayes and frequentist approaches . we did, however, use another heuristic approach to estimate tv because we observed that the estimator proposed by storey et al  <cit>  could be very difficult or impossible to calculate when the expression of a small fraction of genes changes. we demonstrated using synthetic data that our estimator was not prone to this type of instability , and that under realistic conditions  our estimate was 60%, 65% and 80% of the true tv in the case of two, three and five replicates, respectively. the accuracy of this estimator is obviously dependent on the power of the test, which is itself under the control of the number of replicates. we have also shown that estimated sensitivity was biased by a constant factor that was mostly determined by the error made in tv estimation. finally, it must be emphasized that the error made in tv estimation has little effect on fdr estimation, as demonstrated by forcing rdam to use the true tv and k values during the process of synthetic data analysis .

synthetic data sets
using the empirical noise distribution observed between two replicates, we devised a method for constructing synthetic data sets. most published methods add noise to a signal that is supposed to represent the true signal of the gene. we showed here that raw signals without denoising could be used and gave excellent result as judged both by the final distribution of signals and by indirect controls such as the preservation of variation distribution and the possibility of successfully analyzing synthetic data substituted for the original data.

synthetic data sets are well adapted for judging the respective performances of different analysis methods. to characterize the scoring procedure of a particular method, we used a new type of diagram that plots fdr vs. s, two quantities that relate to the subset of selected genes and that seem better adapted than receiver operator characteristic , which relates to both selected and rejected genes . we proposed using fdr <dig>  the fdr at s = 50%, as a comparative index between different methods and showed that rdam has an fdr <dig> that is  <dig> percentage points smaller than sam in the case of three replicates and applied changes with p <=  <dig>  .

CONCLUSIONS
rdam is a new statistical method whose performances have been precisely evaluated through extensive analysis of synthetic data sets. when applied to tor experiment, our method succeeded in finding the few genes of biological interest which were concealed in the mass of varying genes induced by the temperature shift. comparison with sam showed that our method obtained the same  results but with a smaller consumption of chips we conclude that the good quality of the results obtained by rdam is mostly due to the use of replicates to calibrate the noise and to the quasi-perfect equalization of variation distribution, which is related to the standardization procedure used and to the measurement of variation by rank difference.

