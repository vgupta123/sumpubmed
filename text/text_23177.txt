BACKGROUND
predicting the expected time to event from a high-dimensional set of predictor variables has become increasingly important in the last years. a particularly interesting problem in this context is the analysis of studies relating patients' genotypes, for example measured via gene expression levels, to a clinical outcome such as "disease free survival" or "time to progression". survival models of this type share the common problems that are typical for the analysis of gene expression data: sample sizes are small while the number of potential predictors  is extremely large. as a consequence, standard estimation techniques can not be applied any more.

for these reasons, a variety of new methods for obtaining survival predictions from high-dimensional data have been suggested in the literature. most of these methods are focused on the cox proportional hazards model  <cit> , while some other methods have been developed for fitting semiparametric accelerated failure time  models  <cit>  in high-dimensional settings. tibshirani  <cit> , gui and li  <cit> , park and hastie  <cit> , and zou  <cit>  introduced lasso-like algorithms for minimizing l <dig> penalized versions of the cox partial likelihood. due to the structure of the l <dig> penalty, these methods have the advantage that variable selection is carried out simultaneously with parameter estimation. li and luan  <cit>  introduced a technique for maximizing the l <dig> penalized cox partial likelihood, which  does not carry out variable selection but includes all predictors. on the other hand, several authors developed methods for fitting semiparametric aft models in high-dimensional data settings: while huang and harrington  <cit>  and wang et al.  <cit>  considered modifications of the buckley-james method  <cit> , huang et al.  <cit>  and datta et al.  <cit>  developed algorithms based on a censoring-adjusted penalized least squares loss function. in addition to penalized estimation techniques, there are various strategies for reducing the dimensionality of microarray data before building an unpenalized survival model, see schumacher et al.  <cit> , bovelstad et al.  <cit> , and van wieringen et al.  <cit>  for overviews of this topic.

in this paper the focus is on gradient boosting  <cit> , which is an alternative method for fitting survival models in high-dimensional data settings. similar to the lasso, boosting has a built-in variable selection mechanism which is carried out simultaneously with the estimation of the prediction function. although being connected to the lasso , boosting algorithms are not primarily designed for the maximization of penalized  likelihood functions but can rather be interpreted as a method for minimizing convex loss functions via gradient descent techniques. in each step of a boosting algorithm, a so-called base-learner  is fitted to the negative gradient of a pre-specified loss function. the current estimate of the prediction function is then updated with the newly obtained estimate of the gradient. as the base-learner can be modified such that only one covariate is used for estimating the gradient in each step , variable selection is carried out at each iteration.

originally introduced for classification problems by freund and schapire  <cit> , boosting has developed into a computationally efficient technique for fitting many types of regression models in high-dimensional data settings. in principle, the boosting loss function can be any negative log likelihood function of some exponential family. various authors have shown that the method tends to be promising with respect to both variable selection and prediction accuracy  <cit> .

concerning the prediction of survival outcomes from gene expression data, the development of boosting algorithms has so far focused on the same model families as l <dig> and l <dig> penalized estimation techniques, namely the cox proportional hazards model and the semiparametric aft model. ridgeway  <cit> , li and luan  <cit> , and binder and schumacher  <cit>  used boosting algorithms with the negative partial log likelihood loss function while hothorn et al.  <cit>  introduced a boosting algorithm for fitting semiparametric aft models. similar to huang et al.  <cit> , hothorn et al.  <cit>  used censoring weights for adjusting the least squares objective function.

while the proportional hazards model is the most frequently used survival model in biostatistics and while fitting semiparametric aft models is a robust method for survival prediction when there is unknown heterogeneity in the data, application of both types of analyses is still inadequate in a number of situations. instead, it might be advisable to fit a fully parametric aft model with an explicitly specified distribution of the survival outcome, such as the weibull or the log-logistic distribution  <cit> .

as an example we consider a high-dimensional set of microarray data originally analyzed in a classification context by barrier et al.  <cit> . the authors collected a sample of  <dig> patients operated on for a stage ii colon adenocarcinoma.  <dig> patients developed a metachronuous metastasis, whereas the other  <dig> patients remained disease free for at least  <dig> months. for each patient the expressions of  <dig>  genes were obtained. barrier et al.  <cit>  selected a sample consisting of the  <dig> most differentially expressed genes between the disease and the disease-free group. by applying diagonal linear discriminant analysis based on the expressions of the  <dig> genes, the authors achieved a high prediction accuracy  for the two patient groups. in the following we will address the equally important problem of modeling the time to development of metachronuous metastasis.

figs.  <dig> and  <dig> show the results of a preliminary survival analysis of the barrier stage ii colon cancer data: here only the  <dig> most differentially expressed genes between the disease and the disease-free group  were used as predictor variables. the cox-snell residuals shown in fig.  <dig> suggest that a parametric aft model with either a log-logistic or a lognormal distribution fits the data best. the cox proportional hazards model does not seem to be appropriate, which is further confirmed by the results shown in fig. 2: here the parameters of a stratified cox model were estimated, where the strata were defined by splitting the values of the most overexpressed gene in the disease group at their median. the two baseline hazard functions shown in fig.  <dig> cross, so the proportional hazards assumption seems to be violated.

although the preliminary analysis is by no means sufficient for building a survival model from the barrier data, we have nevertheless gained valuable insight into the distribution of the survival times. how should we incorporate this "prior knowledge" into a boosting algorithm? fig.  <dig> suggests that using the cox partial likelihood as a loss function for boosting is at least questionable, since the cox model is known to be sensitive with respect to violations of the proportional hazards assumption . on the other hand, fig.  <dig> suggests that the survival times of the barrier data either follow a log-logistic distribution or a lognormal distribution, so fitting a semiparametric aft model without any distributional assumption might be inefficient . in order to account for these issues, we focus on the development of a boosting algorithm for parametric aft models. an algorithm of this type has not yet been developed in the literature, since aft models include a scale parameter for modeling the variance of the error distribution in the regression equation. with maximum likelihood estimation of aft models, this scale parameter is estimated simultaneously with the regression parameters. boosting algorithms, however, have so far not been able to deal with scale parameters but only with the prediction function, i.e., with the regression parameters. this is the reason why it has not been possible yet to use the negative log likelihood function of an aft model as a loss function for boosting. in fact, boosting cox models and semiparametric aft models was only possible because these methods do not include a scale parameter.

in the following we will introduce a new boosting algorithm that allows for simultaneous estimation of both the prediction function and the scale parameter in parametric aft models. this algorithm uses the negative log likelihood of the aft model as a loss function and works in a stepwise fashion. after starting with some offset values of the regression and scale parameters, a component-wise base-learning procedure is applied to the negative gradient in each iteration, and an update of the prediction function is obtained. afterwards, the scale parameter is re-estimated in each iteration by plugging the current estimate of the prediction function into the loss function and by minimizing the loss function over the scale parameter. as a base-learning procedure we use the classical linear least squares approach, i.e., the final model can be interpreted as a survival fit depending on a linear predictor.

the characteristics of the new algorithm are first investigated by means of a simulation study with artificial data. it is shown that variable selection is carried out efficiently, and that the algorithm has a high predictive power if compared to the "null models" with no covariates at all. also, the regression estimates of boosting in low-dimensional settings turn out to be almost identical to the corresponding maximum likelihood estimates. to show the practical applicability of the new algorithm, we carry out a detailed study on the above introduced data set by barrier et al.  <cit> . evaluating the performance measures of this study suggests that boosting with the negative log likelihood of a parametric aft model is able to outperform boosting with the cox partial log likelihood, at least in case of the barrier data.

methods
estimation problem
consider a set of realizations of i.i.d. random variables ,..., , where t <dig> ..., tn are one-dimensional survival times, c <dig> ..., cn are one-dimensional censoring times, and x <dig> ..., xn are p-dimensional vectors of covariates. it is assumed that some of the survival times t <dig> ..., tn are right-censored, so that only the random variables y˜i := min{ti, ci}, i =  <dig> ..., n, are observable. this implies that the available data consist of realizations of y˜i and of the set of censoring indicators δi ∈ { <dig>  1}, where δi =  <dig> if observation i is censored and δi =  <dig> if the complete survival time ti has been observed. it is further assumed that the ci, i =  <dig> ...,n, are independent of , i =  <dig> ...,n. this corresponds to the classical case of "random censoring". the objective is to fit the accelerated failure time model

  log = f + σw, 

where  follows the same distribution as each of the , i =  <dig> ...,n, and where w is a random noise variable independent of x. the function f is an unknown prediction function of log, and σ is an unknown scale parameter that controls the amount of noise added to the prediction function f. typically, f is a linear or additive function of the covariates x.

the distributional assumption on the noise variable w determines the distributional form of the survival time t. if this distribution is fully specified, we refer to model  as a parametric aft model. for example, if w follows a standard extreme value distribution, t  follows a weibull distribution with parameters λ:= exp and α:= 1/σ . other popular examples of parametric aft models are the log-logistic distribution for t|x  and the lognormal distribution for t|x . it is important to note that the latter two models explicitly allow for non-proportional hazards. the weibull distribution is a parametric example of the well-known cox proportional hazards model  but is instead based on a semiparametric model for the hazard function of t|x).

classically, the estimation of f and σ in a parametric aft model is performed by maximizing the log likelihood function

  ∑i=1nδi+, 

where yi := log is the logarithm of y˜i. the functions fw and sw denote the probability density and survival functions of the noise variable w, respectively .

semiparametric aft models leave the parameter σ and the distribution of the noise variable w in model  unspecified. instead of using maximum likelihood techniques, f is estimated via minimization of a censoring-adjusted least squares criterion  <cit> .

fgd boosting with component-wise linear least squares and scale parameter estimation
in the following we will use boosting techniques for obtaining precise estimates of model  when the number of covariates is large. we start by considering a boosting algorithm known as "component-wise linear functional gradient descent "  <cit> . the objective of fgd is to obtain the real-valued prediction function

  f* := argminfe , 

where the one-dimensional function ρ is a convex loss function that is assumed to be differentiable with respect to f. estimation of f* is performed by minimizing the empirical risk ∑i=1nρ) with respect to f. component-wise fgd works as follows:

 <dig>  initialize the n-dimensional vector f^ <cit>  with an offset value, e.g., f^ <cit> ≡ <dig>  set m =  <dig> 

 <dig>  increase m by  <dig>  compute the negative gradient −∂∂fρ and evaluate at f^, i =  <dig> ...,n. this yields the negative gradient vector

 u=i= <dig> ...,n:=|y=yi,f=f^)i= <dig> ...,n. 

 <dig>  fit the negative gradient u to each of the p components of x  separately by p times using a simple linear regression estimator. this yields p estimates of the negative gradient vector u.

 <dig>  select the component of x which fits u best according to a pre-specified goodness-of-fit criterion. set u^ equal to the fitted values from the corresponding best model fitted in step  <dig> 

 <dig>  update f^=f^+νu^, where  <dig> <ν ≤  <dig> is a real-valued step length factor.

 <dig>  iterate steps 2– <dig> until m = mstop for some stopping iteration mstop.

the above algorithm can be interpreted as a negative gradient descent algorithm in function space. in each step, an estimate of the true negative gradient of the loss function is added to the current estimate of f*. thus, a structural  relationship between y and the covariate vector x is established . moreover, fgd carries out variable selection in each iteration, as only one component of x is selected in step  <dig>  this property makes the algorithm applicable even if p > n. due to the additive structure in step  <dig>  the final boosting estimate at iteration mstop can be interpreted as a linear model fit but will typically depend on only a subset of the p components of x.

as we want to use fgd for the estimation of parametric aft models, we set ρ equal to the negative log likelihood function specified in . however, in this case, the standard fgd algorithm presented above can not be applied, as  includes an additional scale parameter σ that has to be estimated simultaneously with f. we therefore extend the classical fgd algorithm in the following way:

 <dig>  initialize the n-dimensional vector f^ <cit>  with an offset value, e.g., f^ <cit> ≡ <dig>  in addition, initialize the one-dimensional scale parameter σ^ <cit> with an offset value, e.g., σ^ <cit> = <dig>  set m =  <dig> 

 <dig>  increase m by  <dig>  compute the negative gradient −∂∂fρ and evaluate at f^, i =  <dig> ...,n, and at σ^. this yields the negative gradient vector

 u=i= <dig> ...,n:=|y=yi,f=f^,σ=σ^)i= <dig> ...,n. 

 <dig>  fit the negative gradient u to each of the p components of x  separately by p times using a simple linear regression estimator. this yields p estimates of the negative gradient vector u.

 <dig>  select the component of x which fits u best according to a pre-specified goodness-of-fit criterion. set u^ equal to the fitted values from the corresponding best model fitted in step  <dig> 

 <dig>  update f^=f^+νu^, where  <dig> <ν ≤  <dig> is a real-valued step length factor. plug f^into the empirical risk function ∑i=1nρ,σ)and minimize the empirical risk over σ. this yields the scale parameter estimate σ^.

 <dig>  iterate steps 2– <dig> until m = mstop for some stopping iteration mstop.

it is easily seen that the modified fgd algorithm estimates f* and σ in a stepwise fashion: in steps  <dig> and  <dig>  f* is estimated for a given value of σ, while in step  <dig>  σ is estimated given the current estimate of f*. it is also clear from step  <dig> that the built-in variable selection mechanism of fgd is not affected by the additional estimation of the scale parameter σ. the value of the stopping iteration mstop is the main tuning parameter of fgd. in the following we will throughout use five-fold cross-validation for determining the value of mstop, i.e., mstop is the iteration with lowest predictive risk. the choice of the step-length factor ν has been shown to be of minor importance for the predictive performance of a boosting algorithm. the only requirement is that the value of ν is "small", such that a stagewise adaption of the true prediction function f* is possible . we therefore set ν =  <dig> . as a goodness-of-fit criterion in step  <dig> we use the r <dig> measure of explained variation which is known from linear regression.

measures for the predictive power of boosting techniques
after having built a survival model from a set of microarray data, it is essential to evaluate the predictive performance of the model. to this purpose, various measures of predictive accuracy have been proposed in the literature  <cit> . since none of these measures has been adopted as a standard so far, we use the following strategy for measuring the performance of a survival prediction rule:

a) log-likelihood
if the objective is to compare prediction rules obtained from the same model family , we use the predictive log likelihood or partial log likelihood as a measure of predictive accuracy. the predictive log likelihood or partial log likelihood is defined as follows: suppose that the parameter vector θ of a survival model has been estimated from a training sample, and that a test sample , k =  <dig> ...,ntest, is used for evaluating the predictive accuracy of the model. denote the log likelihood or partial log likelihood function of the survival model by lθ. the predictive log likelihood is then given by

  lpred:=∑k=1ntestlθ^, 

where the parameter estimate θ^ has been plugged into the log likelihood or partial log likelihood of the test sample. in case of a parametric aft model we have θ = , whereas in case of a cox model, θ is the prediction function f used for modeling the hazard rate of t|x. in a boosting context it is particularly advisable to use  as a measure of predictive accuracy, since the negative log likelihood or partial log likelihood is used as a loss function for the corresponding boosting algorithms. as a consequence, the loss function is measured on the same scale as the predictive accuracy .

b) brier-score
if the objective is to compare prediction rules obtained from different model families or estimation techniques, it is no longer possible to use the predictive log likelihood as a measure of predictive accuracy. this is because the likelihood and partial likelihood functions of different model families are usually measured on different scales, so they can not be compared directly. moreover, many nonparametric and semiparametric estimation techniques do not involve likelihood functions at all, implying that a predictive likelihood value can not be computed.

in case of the barrier data we will compare the performance of various parametric and semiparametric estimation techniques by using the brier score  <cit>  as a measure of predictive accuracy. the brier score is defined as the time-dependent squared distance between the predicted survival probability and the true state of an observation. since the brier score is not based on any specific model assumptions, the predictive accuracy of various model families and estimation techniques can be measured on the same scale. also, possible misspecifications of a survival model are taken into account  <cit> . in this paper we use the methodology of gerds and schumacher  <cit>  who combined the evaluation of the brier score with the  <dig> + estimator developed by  <cit> . this methodology has been used previously for predicting survival outcomes from microarray data sets  <cit> .

we first draw b bootstrap samples of size n from the data, where the bootstrapped observations constitute the training data sets and the out-of-bootstrap observations constitute the test data sets. define Γi := i, i =  <dig> ...,n, for each time point t >  <dig>  further denote by s^, i =  <dig> ...,n, the survival function of observation i at time point t estimated from the complete data set. we compute the apparent error rate

  err:=1n∑i=1n−s^)2wi, 

where

  wi:=iδig^+ig^ 

are weights that are introduced to account for censoring  <cit> . the expression g^ denotes the kaplan-meier estimate of the distribution of the censoring times ci, i =  <dig> ...,n. for each bootstrap sample and for each time point t we further compute the out-of-bag error rates

  errb:=1nb∑iϵℬbn−s^b)2wi, b= <dig> ...,b, 

where nb is the cardinality of the set ℬb of out-of-bootstrap observations corresponding to bootstrap sample b, b =  <dig> ..., b. the expression s^b is the estimated survival function of the out-of-bootstrap observation i at time point t obtained from the bootstrap training sample b. denote the cross-validated error rate, i.e., the mean or median of the out-of-bag error rates errb, b =  <dig> ...,b, by errℬ.

both  and  are estimators of the true prediction error e <dig>  where s is the true survival function of t|x at time point t. since  is an upward-biased estimator and  is a downward-biased estimator of the true prediction error , we use a linear combination of err and errℬ as the overall measure of accuracy at time point t:

  err:=err+ωerrℬ. 

defining ω :=  <dig> /) with r:=errℬ−errnoinferr−err and

  noinferr:=1n2∑i=1n∑j=1n−s^)2wi 

yields the  <dig> + estimator of the brier score at time point t. for a detailed derivation of  we refer to gerds and schumacher  <cit>  and efron and tibshirani  <cit> . in case of the barrier data, we evaluate  at each time point t >  <dig> up to a survival time of five years . this yields a prediction error curve depending on t, where  <dig> <t ≤ tmax.

RESULTS
in this section we investigate the properties of the new boosting algorithm for parametric aft models. we first conduct a simulation study with artificial data. afterwards, we investigate the ability of boosting to predict survival outcomes from gene expression data. this is done by conducting a benchmark study on the barrier stage ii colon cancer data. all computations were carried out with the r system for statistical computing  using a modification of the glmboost() function in package mboost .

simulation study with artificial data
we carried out a simulation study on linear aft models with five covariates, i.e., we considered the model

  log = β1x <dig> + ... + β1x <dig> + σw, 

where x <dig> ...,x <dig> are jointly normally distributed covariates with parameters e =  <dig>  var =  <dig>  and cov =  <dig> , k, l =  <dig> ...  <dig>  k ≠ l. the data values of the noise variable w were either drawn from a standard extreme value distribution , from a standard logistic distribution , or from a standard normal distribution . for each of the three distributions the scale parameter σ was chosen such that

  var⁡var⁡+var⁡= <dig> , 

i.e., the linear predictor f accounted for 80% of the variance of log. the parameter vector β:= ⊤ was set equal to β = ⊤.

in a first step, we compared the estimates of β obtained from the new boosting algorithm with the corresponding estimates obtained from standard maximum likelihood estimation. for each of the three distributions of w, we generated  <dig> i.i.d. data sets  from model . censoring was introduced to the data by simulating  <dig> additional i.i.d. data sets  from model . the values of the dependent variables in these additional data sets constituted the censoring times of the corresponding first  <dig> data sets. this implied that  the censoring times followed the same distribution as the survival times but were independent from the latter, and that  about 50% of the observations in each data set were censored on average. the new boosting algorithm with the corresponding negative log likelihood loss function was applied to each data set. note that the final boosting estimate can be interpreted as a linear model fit , since we used component-wise linear base-learners. we additionally used standard maximum likelihood techniques for estimating the parameters of model  from the  <dig> data sets.

the boxplots of the parameter estimates of the weibull model are shown in fig.  <dig>  obviously, the parameter estimates obtained from boosting are very similar to the parameter estimates obtained from maximum likelihood estimation. similar results were obtained for the log-logistic and lognormal models. this can also be seen from the spearman correlations between the parameter estimates shown in table  <dig>  we next compared the ability of boosting and of standard maximum likelihood techniques to select a subset of informative covariates out of a larger set of covariates. to this purpose, we extended the set of variables used for the previous simulation study by an additional set of  <dig> independent standard normally distributed covariates with parameters β <dig> = ... = β <dig> =  <dig>  fig.  <dig> shows the resulting parameter estimates obtained from boosting and from maximum likelihood estimation. obviously, all estimates of the "non-informative" parameters β <dig> ...,β <dig> are close to the true value of  <dig>  in  <dig> % of all  <dig> ·  <dig> cases, the boosting parameter estimates of β <dig> ...,β <dig> are exactly  <dig>  which means that non-informative covariates have not been selected. the corresponding percentage rates were  <dig> % and  <dig> % for the log-logistic and the lognormal model, respectively. in addition, the boosting parameter estimates corresponding to β <dig> ...,β <dig> have a smaller variance than the corresponding maximum likelihood estimates. for these reasons, boosting seems to be preferable to likelihood estimation when it comes to variable selection. it can also be seen from fig.  <dig> that the boosting estimates of the non-zero parameters β <dig> ...,β <dig> are  downward-biased. this shrinking effect reflects the well-known bias-variance trade-off which is common to boosting techniques.

we finally investigated the predictive performance of the new boosting algorithm and of maximum likelihood estimation techniques. to this purpose, we used the model estimates obtained from the  <dig> data sets for predicting the likelihood of  <dig> additionally generated i.i.d. test samples of sample size ntest =  <dig> each ). the corresponding predictive log likelihood values, which are shown in fig.  <dig>  suggest that the predictive performance of boosting is better than the predictive performance of maximum likelihood estimation. a wilcoxon paired-sample test for the predictive log likelihood values of boosting and maximum likelihood estimation resulted in a p-value of less than  <dig> . for the log-logistic and lognormal models, the respective p-values were also smaller than  <dig> .

benchmark experiment on stage ii colon cancer data
in order to show the practical applicability of the new boosting algorithm for parametric aft models, we conducted a benchmark study on the barrier stage ii colon cancer data  <cit> . again we used  the negative weibull log likelihood,  the negative log-logistic log likelihood, and  the negative lognormal log likelihood as loss functions for boosting. to compare boosting for parametric aft models with other estimation techniques, we additionally fitted survival models by using  l2boosting for semiparametric aft models  <cit> ,  l <dig> penalized estimation for semiparametric aft models  <cit>   function in r package lars  <cit> ),  gradient boosting with the negative cox partial log likelihood loss  <cit> ,  l <dig> penalized cox partial likelihood estimation  function in r package glmpath  <cit> ), and  nonparametric estimation of the survival function via the kaplan-meier estimator. the latter estimator corresponds to the non-informative "null model" with no covariates at all.

we applied the eight estimation techniques to  <dig> bootstrap training samples generated from the barrier data. prediction errors were obtained by using the  <dig> + methodology, as described in the methods section. for computational reasons we reduced the predictor space for the l <dig> penalized methods  and , i.e., we used the  <dig>  most differentially expressed genes between the disease and the disease-free group instead of all  <dig>  genes. the subset of  <dig>  genes corresponds to those genes whose differences between the disease and the disease-free group are significant at a level of α =  <dig> . this reduction of the predictor space did not have any negative effect on the predictive performance of the two l <dig> penalized techniques . the tuning parameters of all eight methods were determined by using five-fold cross validation.

the survival functions of the eight methods, which are needed for computing the brier score, were estimated as follows: for the "parametric" boosting methods , , and , we estimated the survival functions by plugging the estimated prediction function f^ into the weibull, log-logistic, and lognormal survival functions, respectively. in case of l2boosting and l <dig> penalized estimation for semiparametric aft models  and ) we made use of the following relationship:

  s = p = p + ε > log|x) = p - f|x), 

where ε := σw. we then estimated the probability on the right-hand side of  by applying the kaplan-meier estimator to the residuals ε^i obtained from the bootstrap training samples and by evaluating the kaplan-meier survival functions at "time points" log−fˆ. in case of boosting with the cox partial log likelihood loss and l <dig> penalized cox regression  and ) we used the estimated survival function

  s^=exp⁡, 

where Λ^ <dig> is the breslow estimator of the cumulative baseline hazard of the survival times.

fig.  <dig> shows the median prediction error curves corresponding to the parametric aft boosting techniques , , and . obviously, the prediction error curves cross, so the predictive performance of the models depends on the time point t under consideration. however, for most values of t, boosting with the negative log-logistic or negative lognormal log likelihood loss yields the smallest prediction error. in fig.  <dig> the median prediction error curve corresponding to the log-logistic model is compared to the median prediction error curves obtained from the two techniques for semiparametric aft models  and . here the parametric aft model seems to have the best predictive performance, indicating that the efficiency loss due to the semiparametric estimation of the aft model is relatively large. in fig.  <dig> the median prediction error curve of the log-logistic model is compared to the median prediction error curves obtained from the two partial log likelihood techniques  and , as well as to the median prediction error curve obtained from the "null model" . again we see that boosting with the negative log-logistic log likelihood has the best predictive performance.

we finally computed the cox-snell residuals from the boosting estimates based on the complete data set. the residuals, which are shown in fig.  <dig>  confirm the results obtained from the preliminary analysis of the barrier data presented in the background section: similar to fig.  <dig>  we see that the log-logistic model fits the data well, while the cox proportional hazards model seems to be inadequate here.

CONCLUSIONS
by introducing a boosting algorithm for parametric aft models we have extended the methodology for modeling survival times in high-dimensional data settings. boosting algorithms for survival data have previously been developed for the cox proportional hazards model  <cit>  and for semiparametric aft models  <cit> . an extension of these algorithms to parametric aft models has not been possible so far, since parametric aft models depend on a scale parameter which has to be estimated simultaneously with the prediction function. to overcome this problem we have developed a flexible boosting algorithm which is able to deal with loss functions depending on both the prediction function and a scale parameter. as a consequence, the negative log likelihood function of an aft model can be used as a loss function for the component-wise functional gradient descent boosting algorithm.

the simulation study on various parametric aft models has shown that the favorable properties of boosting with respect to variable selection are left untouched by the additional estimation of the scale parameter. moreover, when sample sizes are small  or when the proportional hazards assumption of the survival times is violated, boosting techniques for parametric aft models seem to lead to an increase in prediction accuracy. this is suggested by the results obtained from the benchmark study on barrier's stage ii colon cancer data. we finally point out that we have so far focused exclusively on boosting with the component-wise linear base-learning procedure. this procedure fits a set of simple linear regression models to the negative gradient in each boosting iteration. we have used component-wise linear least squares mainly because of their computational efficiency. in fact, component-wise linear least squares base-learners are highly efficient even when the number of predictors is extremely large . in principle, however, the new boosting algorithm can also be extended to additive models with smooth components. this can, for example, be done by using component-wise smoothing splines  or p-splines as base-learners. moreover, the boosting algorithm developed in this paper is not exclusively designed for fitting parametric aft models but could also be used in combination with other likelihood-based loss functions depending on a scale parameter. the properties of these extensions still have to be investigated and constitute an issue for future research.

authors' contributions
ms developed the algorithm, conducted the benchmark experiments, and wrote the initial version of the manuscript. th is the primary author of the mboost package, contributed to the design of the benchmark study and revised the manuscript.

