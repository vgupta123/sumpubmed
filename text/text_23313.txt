BACKGROUND
gene expression profiling using microarrays plays an important role in many areas of biology. microarray data however often contains many missing values. among the most commonly used computer analysis tools that require imputation of missing values are data dimensionality reducing algorithms such as principal component analysis  and singular value decomposition, and machine learning algorithms such as support vector machines. advanced imputation methods have therefore been developed, such as knnimpute  <cit> , bayesian pca  <cit>  and llsimpute  <cit> , which all are based on correlations between available measurements in the data matrix . in knnimpute, e.g., a weighted average of the k most similar genes  is used to derive an estimate of a missing value in the gene of interest  <cit> . missing values and the choice of imputation method has also been shown to affect the significance analysis of differentially expressed genes  <cit> .

missing values can occur due to dust or scratches on the slide, spotting problems or hybridization problems. obviously problematic spots are manually flagged as missing and are removed from further analysis. it is customary to subtract background intensities from the spot intensity, and this also produces missing values. negative background-corrected intensities arise if the spot intensity is comparable to the background intensity, either due to contamination of the spot, leakage from neighbouring spots, or from low abundance of dyed cdna in the reference or sample.

obviously, the choice of spot quality assessment and transform of intensities of the resulting values will influence the final result of imputation as well as the analysis as a whole. error models, and transforms other than the logarithm, have been developed, designed to give reliable variance estimates in the absence of replicates  <cit>  or generate a well-defined distribution of expression values  <cit> .

however, more heuristic approaches remain in use. one reason could be that many transforms or weights require careful tuning of parameters, which tend to be platform dependent  <cit> , and perhaps even dataset specific. results from experiments designed to determine such parameters are not always available. another reason is that the biological variation within traits tends to dominate over technical variation. when the study is large enough to get a reliable sample estimate of the total variation within traits, there is less need for information on technical variation alone. furthermore, instead of relying on a specific distribution of expression values, p-values and false discovery rates  <cit>  are often obtained by non-parametric tests  <cit>  or empirical methods such as permutation tests  <cit> .

in heuristic analyses, some spot quality control is still performed, often in terms of threshold values in observables such as spot size, intensity, background variation, or combinations thereof, which are used to flag spots as missing. an undesired feature of this approach is the sharp threshold effects. a spot with an observable, say reference intensity, just below the chosen threshold will be deemed "completely unreliable", while a spot with essentially the same intensity, but just above the threshold will be considered "perfectly reliable".

different smoothings of threshold effects have therefore been developed  <cit> . smoothing introduces continuous weights, ranging from  <dig> for missing or "completely unreliable" measurements, to  <dig> for "perfectly reliable" ones, but also taking on values in between. the chosen weight is related to some commonly used threshold observables and threshold values, typically tuned to be about 1/ <dig> at the otherwise adopted threshold.

weights w associated to the expression values x  can be used to improve imputation  <cit> . for every spot, measured as well as missing, an imputed value ximp is calculated and an adjusted value

 x' = wx + ximp 

is used in the subsequent analysis. thus, missing and "completely unreliable" values x are replaced by ximp, "perfectly reliable" measurements x are kept, and spots with weights between  <dig> and  <dig> will end up with an expression value somewhere between the imputed and the directly measured value.

weighted imputation requires a weight definition which ranges from  <dig> to  <dig>  this property is used in eq.  <dig> and in the selection of the number of nearest neighbours  <cit> . the range constraint on the weights excludes weighting schemes that combine an error model estimate of the variance σ <dig> with a weight motivated by maximum likelihood, w = 1/σ <dig>  instead, weights representing a smoothing of an otherwise adopted threshold filter satisfy the range constraint.

for 2-dye cdna microarrays, it is common to impute missing values using the data matrix of log intensity ratios. in this approach, no information as to why a measurement is missing is included. it is also possible to impute intensities for each channel separately and form the log ratio from them  <cit> . different forms of missing values are then handled differently, but the imputation of a missing intensity is performed without use of the information provided by the other channel of the same spot.

we divided missing values of 2-dye cdna data into three categories; those that are missing due to a missing sample intensity only , a missing reference intensity only , or other reasons. we examined if this categorization can be used to improve imputation of expression values. we wanted to investigate imputation of one-channel depleted spots using the best imputation scheme available. we therefore worked with the weighted version presented in  <cit>  and described in our methods section.

RESULTS
imputation of one-channel depleted spots is biased
the weighted imputation final result x' can be written x' = ximp + w. for one-channel depleted spots, the fact that the opposite channel was measurable provides information which is neglected when we set the weight to  <dig>  for example, sample depleted  spots are those for which the sample channel is missing, while a measurable reference channel indicates a good quality spot, which suggests a low expression. if this qualitative statement "low" is not completely unreliable, warranting a non-zero weight, our final result would be x' = ximp + "some w > 0" × .

to quantify this pseudo-mathematical formula, we asked ourselves if "low" meant "lower than ximp", at least on average, for sd spots. if so, the relation can be written x' = ximp + "some negative correction". we used datasets with duplicate measurements to test this. for some of the duplicates, one measurement is sd while the other is known. we used the known duplicate as the best available estimate of the value that the sd spot should have, and compared this control value with the imputed value.

similarly, reference depleted  spots are those for which the sample channel is measurable, in spite of the poor spot quality indicated by a missing reference, suggesting a high expression. we used the same datasets and compared the imputed values of rd spots with known duplicate controls, to test if the qualitative statements "some w > 0" and "high" motivated a positive correction of the imputed value for rd spots. table  <dig> shows that for one-channel depleted spots, the imputed value has a mean deviation  from the duplicate much larger than expected by random, while the bias for other spots is more comparable to random expectation. furthermore, the md signs are in agreement with the hypotheses that sample- and reference depletion provide useful information about down- and up-regulation, respectively.

thus, though we could not a priori be certain that the pseudo-mathematical expression "some w > 0" ×  implies "some negative correction", we found that it does so, in five different datasets produced on different platforms by different experimental groups, investigating both primary tissue and cell lines.

above, we referred to the weighted scheme to motivate why we looked for an imputation bias, but it should be pointed out that the result, that bias exists, is more general. we also performed imputation with binarized weights as in  <cit> , so that all weights below  <dig>  were set to  <dig> and all others to  <dig>  this corresponds to unweighted knnimpute, and we then found similar biases .

corrections calibrated on training set duplicate controls improves imputation in validation set
the next obvious step was to design and evaluate a correction of the observed bias. in datasets with duplicates, the simplest possible correction is just a constant shift removing the bias found in duplicates where one measurement is sd  while the other is known. the calibrated correction can then be applied to all sd spots.

duplicates used to calibrate the correction are useless as controls. therefore, we tested the constant shift correction in a cross-validation scheme described in methods. the md in the validation set was found to be comparable to random expectation .

a change of md to a new md' will change the mean square deviation  by  <dig> -  <dig>  table  <dig> shows that this corresponds to a reduction of msd varying from  <dig> % to 51%, depending on dataset, with a median of 14%. thus, a constant shift removing the mean deviation  on a subset of samples  reduces the mean square deviation  on another subset of samples . in summary, a correction calibrated on a subset of samples  can be used to essentially remove the bias and reduce the msd in another subset of samples .

corrected imputation increases overall variance without reducing signals for de-regulation
next, we wanted to assess the impact of bias correction for finding differentially expressed reporters. we therefore tested imputation with and without missing value categorization in a more realistic analysis, where duplicates were being merged rather than kept apart for imputation control. we combined merging and imputation as described in methods.

for the lymphoma dataset, we constructed reporter lists for the  <dig> reporters with largest variation across arrays after wenni, with and without the modification. the reporters were ranked on pearson correlation to clinical outcome. in this dataset, the number of sd and rd spots correlated with good and bad clinical outcome, respectively, with an odds ratio of  <dig> . this correlation manifested itself with a shift towards more positive pearson scores with modified imputation, see figure  <dig>  however, the distribution of score magnitudes did not change much, and the false discovery rates  <cit> , estimated from random sample label permutations, gave also very similar results for the two approaches . we also studied how the reporter standard deviation changed when applying the correction, and found that the variation increased for the vast majority of the modified reporters, see figure  <dig>  this implies that more reporters would survive a variation filter after correction, and would be kept for further analysis, thus allowing for a better sensitivity. the same analysis was also performed in the mec tumour dataset, with very similar results.

CONCLUSIONS
we show that conventional imputation of 2-dye cdna data gives biased estimates of expression for missing values where only one channel is missing. we present the simplest possible correction, just a constant shift calibrated on duplicate controls, which can be used to improve imputation for these one-channel depleted spots.

the method does not apply to experiments where the two dyes are used for pairwise comparison of biological traits, but it is applicable to all 2-dye cdna microarray data with a common reference on all arrays, and with at least some duplicates, to allow for calibration.

in principle, common reference experiments with dye swap arrays can be corrected for bias in one-channel depleted spots, as long as the reference channel is properly identified on each array. with the datasets available in this study, a possible benefit from dye-specific corrections could not be investigated.

we speculate that the missing value categorization is more useful in smaller datasets where the knn-based method is less likely to find the correlations needed for a successful imputation. this is supported by table  <dig>  where the three smallest datasets lymphoma, mec cell lines and mec tumours show the greatest improvement. furthermore, datasets with homogeneous classes, such as extensively facs-sorted cell populations, or replicates of transfected cell-lines, are probably more likely to contain those consistently strong down-regulated genes that will be correctly characterized as sample depleted. in fact, we can see that in the mec tumour and cell line datasets where the samples represent distinct gene expression phenotypes, the sd spots outnumber the rd spots in contrast to the other datasets, where rd spots are more common than sd spots.

in these cases, the imputation method proposed here may not only help create complete data matrices for algorithmic purposes, but may also identify strongly down-regulated genes that would not be found to be differentially expressed by conventional imputation.

