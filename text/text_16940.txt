BACKGROUND
because of their ability to link genes that behave similarly across conditions in a gene expression study, pattern recognition and clustering are widely used for data analysis of microarray data . numerous methods have been adopted to cluster genes or samples including hierarchical clustering  <cit> , maximum likelihood clustering  <cit> , the cluster affinity search technique  <cit> , quality threshold clustering  <cit> , and fuzzy k-means clustering  <cit> , among others. some methods specifically aim to identify subsets of behaviors within the data, where genes behave similarly only over a subset of samples. such methods include two-way clustering  <cit>  and biclustering  <cit> . other methods allow genes to belong to multiple patterns, reflecting biological roles where genes function in multiple cellular processes. such methods include bayesian decomposition  <cit> , principal component analysis  <cit> , independent component analysis  <cit> , and nonnegative matrix factorization  <cit> .

by creation of a constrained model , non-negative matrix factorization  shares with bayesian decomposition  the potential to more accurately identify sets of genes that together provide function. both aim to recover two matrices, a and p, that reproduce the data within the uncertainty as in

d = m + ε = a·p + ε.     

in the case of nmf, this constraint is positivity in the a and p matrices, while in bd the constraint is provided through relationships between model points in the form of convolution functions during matrix reconstruction  <cit> . for microarray data, the matrix d provides the estimates of transcriptional levels, such that each column corresponds to the estimate for a single condition, with each matrix element in a column corresponding to the estimate for a single gene  in that condition. a row of d corresponds to the processed intensity for a single gene across all conditions. if d has dimension of i × j, then a has dimensions i × k, and p has dimensions k × j, where k is the dimensionality . there is no accepted method yet to choose k a priori, however many possible k can be tested in order to find an optimal value  <cit> .

the overall simulation in the original nmf algorithm aims to minimize the difference between m and d in equation  <dig>  with every element in d given the same weight in evaluating the difference between the two matrices. however, most microarray measurements are replicated and, in addition, statistical techniques have been developed to estimate uncertainty measurements for all data points in d  <cit> . this uncertainty information is extremely valuable for identifying the best model  <cit> , and it has been used in microarray analysis in other contexts as well  <cit> . inclusion of this information in fitting the a and p matrices should improve the nmf algorithm by allowing it to more precisely fit the reconstructed m to the data, d. such approaches have also been used successfully within supervised methods, such as ls-svm  <cit> .

RESULTS
data sets
we applied the least squares nonnegative matrix factorization  algorithm to two publicly available microarray datasets, one with no individual estimates of uncertainties for each data point and one with such information. the first set is yeast cell cycle data from cultures synchronized with a temperature sensitive cdc28-mutant  <cit> , which has a single affymetrix genechip measurement at each time point. the second set is the yeast deletion mutant compendium from rosetta inpharmatics  <cit> , which comprises microarray measurements of mrna levels from yeast cultures containing either clones of s. cerevisiae with gene deletions or chemical treatments. both data sets were preprocessed as described in methods to create the estimates of mrna levels for the data matrix, d.

algorithm performance
data fitting
to measure the ability of ls-nmf to fit the data, we measured the χ <dig> fit  between the model of the data provided by m and the data in d, as in equation  <dig>  the original nmf algorithm used the root mean square distance  between these two matrices as a measure of fit, however this is incapable of taking into account the uncertainty information used by ls-nmf. essentially, nmf ignores variations in the precision of the measurements. the χ <dig> measure provides a standard approach to determining the fit between a model of a data set and the data set itself. figure  <dig> shows the difference between the d and m matrices using ls-nmf and nmf for dimensionalities of k =  <dig> -  <dig> on the rosetta data set. figure  <dig> demonstrates that the χ <dig> error value decreased with increasing dimensionality for ls-nmf, but not for the nmf simulation. at all dimensions, ls-nmf consistently fits the data better, which is expected as nmf does not optimize m for the known variance. more interestingly, in the ls-nmf simulation, the rmsd error appears to be independent of the dimensionality, while nmf does depend on the dimensionality. this results from the ls-nmf simulation fitting the more reliable data points tightly, while allowing the data points with high variance to be fit more loosely. this can be seen in figure  <dig>  where points with the highest variance  do not improve in rmsd in ls-nmf, while they continue to show improvement in χ <dig>  the χ <dig> measure takes into account the variance providing a better measure of fit in cases of varying variance.

computational complexity
while incorporating uncertainty measurements into nmf update rules  makes sense for fitting microarray data, if the modification adds too much computational complexity, it will not be useful practically. we have measured the additional computational cost when applied to the rosetta data set, and we have found that it is a constant offset compared to the cost of the original nmf implementation, regardless of complexity. this indicates that ls-nmf has the same convergence speed as nmf after a higher set-up cost, so that ls-nmf can be used with a minimal loss of efficiency compared with the original nmf algorithm in most practical applications.

biological insights
roc analysis of metagenes
after normalization as described in the methods, the contribution of each gene to a metagene is represented by a scaled z-score, with a positive z-score indicating that the gene is likely to be associated with that metagene, and a negative z-score indicating that the gene is unlikely to be associated with the metagene. metagenes were introduced to summarize behavior shared by many genes within an experiment that together provided the ability to classify samples  <cit> . this was similar to the concept of an eigengene from singular value decomposition  <cit> , but with regression models using classification data driving the determination of mixing of the genes in a metagene. metagenes have been discussed in relation to nmf analysis of microarray data previously  <cit> , where they summarize behavior across conditions and assign fractions of the overall expression pattern for each gene to these behaviors.

prediction of functional relationships
the other way to evaluate the performance of nmf and ls-nmf is by testing their ability to predict functional relationships between genes. in the reduced rosetta data set,  <dig> conditions are measurements of mrna levels in deletion mutants of s. cerevisiae growing in rich media compared with mrna levels of wildtype s. cerevisiae in similar conditions. mutants showing similar changes in gene expression might be expected to have deletions of functionally related genes, allowing predictions of functional relationships between genes based on links between deletion mutants. these predictions were scored against available database information. in order to demonstrate the gain from including uncertainty measurements, predictions based on p matrices in equation  <dig> from nmf, ls-nmf, and nmf on data scaled by the uncertainty estimates were compared at different dimensions. the dimensions chosen match previous work using estimation by bayesian decomposition and clutrfree  <cit> , where  <dig> dimensions were estimated to cover the data, and nmf  <cit> , where  <dig> dimensions were estimated. in addition, correlations in the original data space  were also calculated as a baseline providing estimates of the ability of the data to predict functional relationships independent of any dimensionality reduction.

predictions of functional relationships were made using pairwise pearson correlations between experiments measured in each of the seven spaces . in all spaces, only the  <dig> deletion mutant conditions were used for analysis. predictions were checked against the mips database , and the results are shown in figure  <dig>  for each of the methods, figure  <dig> shows the percentage of predictions validated by mips as a function of the number of predictions made, which increases as the threshold for correlation is lowered. in general, the methods should exhibit the highest validation for their strongest predictions , but predictions based on the 15-dimensional nmf did not show such a trend, while 50-dimensional nmf and all ls-nmf did behave as expected. nmf applied to the scaled data and ls-nmf performed similarly at  <dig> dimensions, but ls-nmf performed better at  <dig> dimensions. both produced better results than nmf applied to unsealed data and the unreduced original data. the better consistency of ls-nmf across changes in dimension may indicate that including uncertainty information into nmf updating rules improves the robustness of the algorithm. note that for all methods except nmf applied to unsealed data, the methods perform at roughly the same level when only the most reliable predictions are considered .

discussion
in the last several years, many analytical approaches have been used to identify groups of genes related by their similar expression profiles across different conditions, including time series, tumor samples, or different tissues. since evolution has led to the borrowing of genes for use in multiple biological functions, the ability of nmf to estimate an expression profile as a linear combination of metagenes gains power by matching biological behavior. this power is demonstrated by the analysis of the yeast cell cycle data, where the roc analysis shows that nmf is more powerful than hierarchical clustering at recovering coexpression groups. nevertheless, as shown by the  <dig> replicated controls in the rosetta compendium, the mrna levels of individual genes are not equally well controlled in biological systems, leading to potentially large differences in the variance of mrna levels between different genes. the constantly improving quality of microarrays and the ability to replicate conditions, either through repeated experimentation in model systems or through the capture of multiple related samples, provides estimates of this gene and condition specific variance. by using this valuable information in nmf update rules, the least squares non-negative matrix factorization  algorithm improves the ability of this approach to recover biological knowledge as demonstrated by the analysis of the rosetta compendium. here, unlike in the yeast cell cycle data, individual uncertainty estimates are available at each data point. the value of this information is demonstrated in figure 3c, where ls-nmf outperformed nmf in roc analysis, and in figure  <dig>  where at both dimensionalities ls-nmf greatly increased the number of successfully recovered functional relationships.

ls-nmf may also be more stable than nmf in interpreting biological functions of genes based on the metagenes when the dimensionality is poorly estimated. in figure  <dig>  analysis at  <dig> and  <dig> dimensions give similar results in gene function prediction for ls-nmf, but not for nmf, where analysis at  <dig> dimensions failed to give meaningful results. this may result from the lack of uncertainty information in nmf, which makes each data point equally important in feedback to the update rules. a higher dimensionality could then yield a better factorization, while dimensionalities under some threshold would be highly influenced by the noise from mrna levels with high variance. in ls-nmf, data points with low vairance will always influence the update rules more strongly.

ls-nmf gains its power from inclusion of uncertainty information. such information can also be added by scaling the data by the uncertainty estimate, as was done in the original rosetta study  <cit> . as can be seen in figures  <dig> and  <dig>  this gives similar though not identical results. however, the direct inclusion of uncertainty information provides both improved interpretation  and more flexibility. the direct use of uncertainty information in ls-nmf allows extensions, such as treatment of individual data points separately based on additional information during updating , which scaling cannot include. in addition, the approach allows for straightforward addition of methods such as simulated annealing  <cit> , which may be useful in escaping local maxima in the probability distribution.

CONCLUSIONS
we have implemented a new algorithm, ls-nmf, based on nmf, to analyze microarray data. the incorporation of uncertainty information into the analysis of mrna transcript levels significantly improves the recovery of biological information in the form of functional links between genes. in cases where there is no variance information available, ls-nmf reduces back to nmf. ls-nmf will provide the community with a powerful new tool for analysis of high-throughput data. the implementation is straightforward, so analysis of new data types with similar variance estimates should be possible, such as mass spectrometry data. the source code and documentation is available from  by following the open-source link.

