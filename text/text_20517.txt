BACKGROUND
protein structures can be characterized by regular folding patterns. descriptions at the level of local folding pattern  are known as the protein's secondary structure, as opposed to its full tertiary  structure. it is common practise to describe each residue as belonging to either one of eight secondary structure environment classes:

c <dig> = {g, h, i, e, b, t, s, c},

or to one of three classes:

c <dig> = {h, e, c}.

the set c <dig> consists of the eight dssp  <cit>  classes: 310-helix , alpha helix , pi helix , helix-turn , extended beta sheet , beta bridge , bend  and other/loop . in set c <dig>  class h contains the 310-helix and alpha helix classes, class e contains the extended beta sheet and beta bridge classes and class c contains the remaining four dssp classes.

in addition to the dominating covalent polypeptide backbone, the stability of a protein structure is determined by the collective strength of many covalent and ionic bonds, as well as van der waals attractions. however, it is well established that protein structures are not entirely rigid. as the tertiary structure of a protein changes due to thermal motion or outside in influences, a residue may also change secondary structure states. in stark contrast to the typical definition of secondary structures in which a residue can only have a single state, continuum secondary structures allow a residue to be in all states, indicated by a probability distribution over the possible secondary structure states. it has been contended that specifying protein structure this way allows regions of transition in secondary structure  to be characterised more accurately  <cit> . moreover, a probabilistic representation of secondary structures sheds light on the conformational flexibility of proteins  <cit> .

solved by x-ray crystallography, a protein has conformational variation mainly due to different experimental conditions. on the other hand, nmr solution of a protein structure always provides a number of models with structural variation due, at least in part, to intrinsic motions of the protein  <cit> . andersen and colleagues developed a scheme-dsspcont-where the secondary state probability distribution for each residue in a protein is estimated from the variation amongst an ensemble of nmr models of the protein. the dsspcont assignment thus directly distinguishes between less flexible regions and more flexible ones  <cit> .

in this work, we use the same nmr dataset as anderson et al.  <cit>  to develop probabilistic models that are able to predict the continuum secondary structure from the amino acid sequence. we test these models using a dataset of continuum secondary structures developed by us and having very low sequence similarity with the anderson et al. dataset. given a target protein sequence, for each residue, our models are trained to predict the probability distribution over all possible secondary structure environments for that residue. importantly, the probabilistic models are thus directly provided with prediction targets that reflect the variability of conformation.

there are a large number of prediction methods that take as input a protein sequence and predict the secondary structure of each of its residues. current best methods  achieve a 3-class accuracy  of 75–80%  <cit> . these and other previous secondary structure prediction methods implicitly assume that each residue in the protein belongs to a definite secondary structure. the target secondary structure used by most models are categorically determined by dssp  <cit>  or stride  <cit> .

most previous prediction methods provide continuous  outputs, and it is tempting to interpret these as probabilities. what distinguishes our approach is that we train our models with probabilistic data, so it is entirely natural to interpret their predictions as probabilities. previous approaches train models using categorical data, so non-categorical outputs often do not represent probabilities at all. in most cases , non-categorical outputs represent the distance from an internal decision boundary, which may be correlated with the certainty of the prediction, but is not a probability in the strict sense of the word. it is also unclear whether such outputs bear direct physical or biological meanings , or if they merely reflect the confidence that the model has in the prediction. moreover, it is estimated that 5–15% of the current prediction errors can be attributed to the current rigid definition of secondary structure and how it is derived from experimental models  <cit> .

in this work, we study three models of increasing complexity: naive bayes' density predictors , probabilistic neural networks , and cascaded probabilistic neural networks . the first of these methods  is the simplest to implement, but makes the most assumptions about the data. in particular, nbdp assumes that the identities of adjacent residues in the protein are independent given the secondary structure. this is obviously not true in general, but we include results using nbdp because it is often a surprisingly effective method for learning probability distributions  <cit> . the neural network models are known to be effective for categorical secondary structure prediction  <cit>  and are thus explored here, too.

to quantify the accuracy of our models, we measure the divergence between the probabilities derived from high-quality nmr models and the predicted probabilities. in combination with each of the three model types, we also examine the effect of describing the amino acids in the input sequence in two different ways. we refer to the two residue description methods as the amino acid identity and psi-blast profile methods, where the latter is employed in the top performing categorical secondary structure predictors.

our main concern is to establish how well the continuity of structure can be captured by machine learning models from limited datasets. we are specifically interested to see if secondary structure prediction can be improved by training the model with the fine-grained structural data from nmr.

to compare our work with previous studies, we also convert the continuum predictions made by our models into categorical predictions by selecting the class with the highest predicted probability. we compare these results to those of a categorical prediction method trained on the data obtained by a similar conversion of the continuum secondary structure data to categorical. the categorical method we compare to is a cascaded categorical neural network , as employed by the top-performing psipred  <cit>  algorithm.

RESULTS
the probabilistic models studied here are more accurate at predicting continuum secondary structure  than models trained on categorical data. the difference in accuracy is most pronounced for residues with structural ambivalence. furthermore, these probabilistic models can be used to predict categorical secondary  with accuracy comparable to a successful categorical method. these results hold when accuracy is measured by cross-validation on the training data as well as when validated with an test dataset containing only proteins with low sequence similarity to proteins in training set, and for both the 3- and 8-class prediction problems.

probabilistic models
the pnn and cpnn are the most successful models at predicting continuum secondary structure in our study. accuracy is highest when the residues in the sequence are described using the psi-blast profile method. the accuracy of the pnn and cpnn methods is also sensitive to the number of hidden nodes in the model and to the width of the sequence window presented to the model. the accuracy of the naive bayes model is substantially inferior to the that of the pnn and cpnn models.

we use the kullback-leibler  divergence to measure the accuracy of our models at predicting continuum secondary structure . accuracy increases with decreasing kl divergence. the predictive accuracy of the pnn and cpnn models generally improves as the number of hidden nodes increases , although improvement is slight beyond  <dig> hidden nodes. the optimal window size is  <dig> residues for both the 3- and 8-class prediction problems. the kl divergence of the best pnns is  <dig>  and  <dig>  for the 3- and 8-class problems, respectively. the cpnn improves this accuracy to  <dig>  and  <dig> , respectively.

for the 3-class problem, the best naive bayes' model  achieves a kl divergence of  <dig> . for the 8-class problem, kl divergence is  <dig>  for a model with a 7- and 9-residue window. the nbdp model is far inferior to the other models when residues are described using the amino acid identity method , and fails almost totally with the psi-blast profile description method . the optimal window size for nbdp, of approximately  <dig> residues, is smaller than for the pnn model.

to give the reader a better qualitative feeling for the meaning of various kl divergences, we show the output of the most accurate 3-class nbdp and cpnn models on the ras-binding domain of c-raf- <dig>  in figure  <dig>  the average kl divergence of the nbdp prediction on this sequence is  <dig> , and is slightly worse than the average 3-class prediction accuracy for the pnn model , and about average for the nbdp model . the average divergence of the cpnn prediction for this sequence  <dig> , slightly more accurate than the overall average achieved by the cpnn model on all test sequences . the data in the figure is for nbdp using the amino acid identity residue description method, and for cpnn using the psi-blast profile method.

comparing with categorical models
even though our probabilistic models are not explicitly trained to produce categorical output, they perform competitively with a state-of-the-art classification method. we train cascaded categorical neural networks   to predict the categorical targets using a configuration identical to the best cpnn in this study . these ccnns are trained using categorical data derived from the continuum data, as described in the methods section.

the classification accuracy of the probabilistic model  is comparable to that of the categorical model  in the 3-class problem using several popular accuracy metrics including q <dig> and sov . we observe that, with a q <dig> of  <dig> , the cpnn is on par with the ccnn .  to  <dig> .) similarly, the two models have segment overlap-based sov measures  <cit>  of  <dig>   and  <dig>  .  to  <dig> .) compared with the cascaded probabilisitic model , the pnn model has similar q <dig> accuracy but notably inferior sov. the best naive bayes' density predictors with boolean input features manage a q <dig> of  <dig>  and an sov of  <dig> , significantly worse than the other probabilistic models.

as an independent test, we also test the models on the all sequences in the small cafasp <dig> data used to benchmark a range of public predictors  <cit> . none of the sequences in cafasp <dig> are included in our training set . we find that both cpnn and ccnn achieve a q <dig> of  <dig> , only slightly worse than their accuracies measured by cross-validation on the training set . for comparison, q <dig> accuracies reported for other categorical models in the eyrich study  <cit>  ranged from  <dig>  to  <dig>  . similarly, the cpnn and ccnn models have sov measures of  <dig>  and  <dig> , respectively, which are slightly better than their cross-validated accuracies. the classification accuracy of the cpnn is slightly lower than the best results reported in the literature, but this is to be expected because our training set is considerably smaller than those used in many previous studies  <cit> . it also bears noting that the probabilistic model  is not specifically trained to produce categorical targets.

conversely, many models trained on categorical data also offer continuous predictions. for the ccnn fitted with the softmax output function, we can evaluate its ability to directly produce an output close to the continuum secondary structure. on the 3-class problem, the ccnn achieves kl divergence  that is nearly identical to that of the cpnn . the cross-validated kl divergence on all residues is  <dig>   for the cpnn, which is close to that of ccnn . likewise, the ccnn and cpnn have very similar kl divergence on the test dataset :  <dig>  and  <dig> , respectively. on the 8-class problem, the ccnn and cpnn have very similar overall kl divergence on the test dataset , but ccnn appears slightly inferior when cross-validated on the training set:  <dig>   and  <dig> , respectively. we note that the categorical model  is not specifically trained to produce continuous targets.

to investigate the qualitative difference between models that are trained on probabilistic targets and categorical targets, we focus on residues in "fuzzy" regions. in particular, we identified "fuzzy" residues as those with an observed secondary structure state entropy of  <dig>  or above , and "very fuzzy" residues as those with target entropy of  <dig>  or above . we investigate the accuracy of the models created by cross-validation on the full training set on each of these low entropy residue subsets .

both probabilistic models  have significantly lower kl divergence on residues with structural entropy greater than  <dig>  when compared with the categorical model . this conclusion holds when kl divergence is measured using cross-validation on the training set as well as when measured on the test dataset . the probabilistic models have significantly superior  kl divergence than the categorical model for both the 3- and 8-class problems. for example, the cross-validated 3-class kl divergence for residues with entropy at least  <dig>  is  <dig>  for the ccnn, but only  <dig>  for the ccnn. similarly, the average 3-class kl divergence on the test dataset residues with entropy at least  <dig>  is,  <dig>  and  <dig> , respectively for the ccnn and cpnn models. the best probabilistic model  is superior to the best categorical model  for predicting continuum secondary structure for test dataset residues with entropy greater than  <dig>  . this result holds for both the 3- and 8-class problems.

for the 3-class problem, kl divergence on the test dataset is only slightly higher than the cross-validated value . this shows that overfitting is not a serious problem with the 3-class models. on the other hand, the 8-class models show significantly worse kl divergence on the test dataset than during cross-validation. this may be caused by overfitting given the small size of the training dataset  compared to the number of parameters in the models . however, since overfitting does not occur with the  3-class models, it is likely that the more complex output space in the 8-class problem is the true culprit.

for classifying structurally ambivalent residues , the probabilistic networks  are on par with the categorical neural network .

three-class classification accuracy  on residues with entropy at least  <dig>  is  <dig> %  and  <dig> %  for the cpnn and ccnn models, respectively it is worth noting that the probabilistic networks achieve good classification despite not being specifically trained for this task.

discussion
our results support the existence of higher-order dependencies between the residues within the input window as the naive bayes' models and single-layer probabilistic neural networks perform relatively poorly.

in agreement with previous work on both neural networks and support vector machines  <cit> , we note that the psi-blast profile description of the sequence data works much better than the amino acid identity method for all types of probabilistic neural networks. however, the opposite holds for the naive bayes' density predictor. with the psi-blast profile description method, the performance of nbdp drops considerably. on closer inspection, the class conditioned distributions of input values are strongly overlapping and consequently result in poor discrimination.

to investigate whether the input values follow a non-gaussian distribution we also tried dividing each numeric input value into  <dig> and  <dig> bins. the performance of the naive bayes' density predictor then drops even further. the naive assumption of independence among the very large number of random variables  is clearly violated. each residue profile column reflects a single piece of information involving all of the twenty amino acids, and the random variables making up the column are highly dependent. this fact is most likely responsible for the failure of nbdp with the psi-blast profile residue description method.

even though the average kl divergence between the probabilistic target and predicted values are almost the same for both the ccnn and cpnn, they seem to handle structurally ambivalent residues differently. the discrepancy as measured on these challenge subsets indicates that training with continuum data results in more accurate prediction for residues with high structural ambivalence.

our simulations indicate that it is not sufficient to train on the categorical targets if structurally ambivalent states need to be characterised precisely. this precision may be particularly important for identifying conformational flexibility, e.g. young et al.  <cit>  rely on a reliability index of a categorical prediction to identify conformational switches.

CONCLUSIONS
the models we present are adapted to predict a continuum secondary structure, i.e. to predict the probability of a residue belonging to any of the three or eight secondary structure classes. the probabilities derive from nmr models that capture some aspects of protein flexibility-in contrast to most categorical predictors which are trained on categorical data usually derived from x-ray crystal structures.

cascaded probabilistic neural networks using a 15-residue input window  are able to produce 3-class predictions that, on average, measure  <dig>  using the kullback-leibler divergence from target distributions. a similar model produces 8-class secondary structure predictions measuring an average kl divergence of  <dig>  from the observed targets.

to illustrate the performance and utility of probabilistic models, we also convert the probabilistic predictions to categorical classifications and note that the probabilistic models are on par with models that are directly trained on categorical data. in particular, structurally ambivalent residues  are predicted more accurately by the best probabilistic models than by their categorical counterparts. so far, the scarcity of nmr data renders the continuum secondary structure prediction less accurate for classification than categorical models directly trained on much larger sets of crystallographic data.

