BACKGROUND
viral borne diseases exert a significant impact on human health with millions of individuals infected yearly and diseases such as hiv/aids ranking as a leading cause of death worldwide . critical to the design of effective vaccines and therapeutics to combat this burden is a comprehensive map of the genetic composition of viral populations and the characterization of the selective pressures that shape these populations. compiling such a map is a challenge: in viral infections the low fidelity of the genome replication process and various evolutionary pressures can result in a single infected host harboring a heterogeneous population of genetic variants
 <cit> . previous studies
 <cit>  have shown that many viruses including dengue, hcv, hiv, influenza, polio and west nile all maintain diverse populations within a single host. this genetic diversity means that the population may already contain variants that are advantageous in the face of challenges such as host immune responses and drug treatments. as such, understanding the extent and composition of intra-host population diversity, even at low frequencies, can be very important in evaluating disease progression, transmission, and response to changes in therapy.

however, identifying intra-host variation depends on an alignment of the sequence data. in cases where the reads cover identical sequences, multiple alignment can be used to solve this problem
 <cit> , but this is currently not feasible for whole genome sequences as no platform exists to sequence whole genomes as single reads at high throughput. alternatively, the sequencing data can be aligned to a reference, as is commonly done for variant detection in human and other organisms
 <cit> . however, the use of a reference genome that is too genetically distant from the sample population will yield inaccurate read alignments and, as previously reported, substantial data loss; both factors decrease the ability to detect biological variants
 <cit> . because viral consensus can vary substantially between patients it may be difficult to find an existing reference that allows unbiased and complete alignment of reads
 <cit> . one solution to this problem is to start the analysis by de novo assembly of each patient sample, allowing use of the patient consensus as the reference for variant detection
 <cit> . since the sample consensus will be near the centroid of the intra-host variation, it should be optimal for alignment of all sequence data to a single reference.

further, the consensus sequence itself is of value. notably, the majority of publicly available genomic sequences were captured using bulk sanger sequencing strategies and as such a single consensus assembly is the only data available to compare against to derive biological insights. a consensus serves as a single datum that represents the entire underlying population or some subset of the population and thereby enables the identification of dominant genetic mutations that vary between two populations or subsets of the same population
 <cit> . lastly, for samples infected with unknown viruses a reference guided mapping strategy will not be applicable and a de novo approach is required
 <cit> .

there are two major frameworks for de novo genome assembly. overlap-layout-consensus based methods
 <cit>  first identify reads that share good suffix-prefix alignment. this operation divides the input reads into disjoint sets, termed contigs. then, multiple sequence alignment is performed for each contig to derive the consensus sequence, an approximation to a target genome fragment. the relative positions and orientations of these contigs are estimated by paired reads that land in different contigs. the de bruijn graph based methods
 <cit>  first decompose input reads into kmers, denoted as vertices, then create a directed edge between any pair of vertices if the last  bases of the source vertex is identical to the first  bases of the target vertex. the graph is then simplified by chain compaction to shorten paths that have a unique entry and exit, and edited to remove small tips and bubbles that are likely attributed to sequencing errors. finally, contigs are generated and oriented during graph traversal, guided by paired reads and coverage information.

assembly algorithms devised specifically for the sequencing of less diverse haploid and diploid genomes by short reads tend to fare poorly on data derived from variant populations such as viruses, both because of the difficulty of separating continuous variation from error, and because of other process-related challenges
 <cit> . in addition, sample preps of rna viral genomes currently require reverse transcription and usually require amplification. these protocols tend to result in highly variable coverage along the length of the genome and may introduce large amounts of contamination from other rna species present in the starting material. these artifacts of the process tend to further confuse existing assembly algorithms, which rely on depth of coverage to both indicate copy number of sequences and distinguish errors from true variants.

recent work utilizing an overlap-layout-consensus strategy has shown that it is possible to generate high quality de novo assemblies from relatively deep coverage data produced by massively parallel pyrosequencing of diverse populations
 <cit> . however, this method adapts poorly to other platforms such as the illumina and ion torrent, due to a much larger amount of data produced, which increases both the computational complexity, and the difficulty of merging divergent genotypes and handling process error. an alternative strategy was recently reported by iqbal et al. <cit>  these authors demonstrated de novo assembly of diploid genomes or a population consisting of a small number of eukaryotic genomes using a de bruijn graph strategy, but the amount of diversity inherent in viral populations is beyond the target range for this algorithm.

here we present vicuna, a de novo assembler of very high but variable coverage short read data from a population of diverse but non-repetitive genomes. vicuna is an overlap-layout-consensus based assembly algorithm. unlike assemblers optimized for large repetitive genomes, vicuna aggressively merges similar sequences, and has the capacity to retain low frequency single nucleotide and length polymorphisms. we validated vicuna on  <dig> viral population samples. these  <dig> samples were obtained from patients infected with dengue virus , human immunodeficiency virus  and west nile virus , which represent a spectrum of intra-host population variation. vicuna recovers the full target regions with high fidelity in all samples. the algorithm captures low frequency non-dominant length variants, specified by a tunable threshold. in a handful of these samples, we recovered alternate consensus containing large deletions . vicuna runs on workstations or blades, and hence is readily accessible to research labs with limited compute resources. although our immediate target application is the sequencing of rna viruses from infected hosts, we anticipate that our method may also have utility for a range of applications which pose similar challenges, including rrna sequencing and whole genome metagenomic analyses.

RESULTS
vicuna assembly strategy
we assume a typical viral population consists of an unknown number of genomic variants that vary in their frequencies. a subset of these variants dominates the population and minor variants also exist. we assume that the phylogenetic relatedness within variant clusters is greater than that between clusters. notably, the premise of a heterogeneous population comprised of closely and distantly related groups of sequences occurring in varied frequencies is applicable to other biological investigations such as metagenomics. the goal of generating a set of consensus sequences that represents a viral population is confounded by high genomic mutation rates, which results in frequent single nucleotide and length polymorphisms between genomes. this paralyzes the key error correction component of de bruijn graph based assemblers. vicuna applies the basic rule that a contig is produced to represent a spectrum of genomic fragments such that their pairwise genetic distances, defined as one hundred minus the percentage of sequence alignment identity, is bounded by p, a user specified parameter. when p is larger than the pairwise distance between dominant variants in the population, a subset of contigs would represent these while other contigs would capture the more distant ones. with this idea in mind, vicuna outputs a set of contigs that are represented as a multiple sequence alignment  of their constituent reads. each msa contains substitutions and indels, which enable the capture of polymorphism in the data. meanwhile, a single consensus sequence can be readily calculated by taking the dominant nucleotide base in each alignment column with ties broken arbitrarily. notably, the msas are constructed incrementally starting from single reads to the final contigs rather than aligning reads back to the consensus post-assembly. when the viral target is known, as is the case in the examples provided in this study, a final contig representing the population mean can be built by merging contigs spanning the target region, selecting the ones that have the most read support. nonetheless, when the target is unknown such as a microbiome sample or in cases of unknown infections, we can recover a set of contigs that represents the spectrum of the genomes in the population. this enables the identification of samples from patients co-infected with multiple viruses.

vicuna consists of four key steps: read trimming, contig construction & clustering, contig validation, and contig extension & merging . in addition, the algorithm includes two optional modules , one to identify target and non-target reads prior to assembly, and a second to guide contig merging using a relevant reference genome if one is available.

read trimming
primer sequences or adapters may still be attached to the fragments pre-sequencing. end of reads that match any substring of primers are removed. in this step, we also remove low complexity reads that likely result from process artifacts
 <cit> .

contig construction & clustering
identifying similarities among input reads is the ab initio step in building contigs. to avoid the compute-expensive pairwise alignment, existing methods typically identify common kmers  or maximal substrings  among input reads, achieved using efficient data structures such as de bruijn graphs
 <cit> , suffix tree/array
 <cit> , and burrows-wheeler transformation
 <cit> . all use exact string matching strategy as the first pass to identify potential reads with good similarities.

to build contigs at the population level, we need to account for scenarios where diversity varies considerably from one genomic region to another, and the abundance of sequencing errors differs considerably due to highly uneven read coverage. these characteristics of the data pose the following challenges for existing methods. exact string matching strategies may have difficulty balancing specificity vs. sensitivity in identifying similar reads: whereas a shorter kmer may considerably reduce specificity, sensitivity can suffer with a longer kmer. also, the degree of vertices in de bruijn graph increases considerably in both highly diversified and highly covered genomic regions, increasing the required compute resources.

to address these challenges, we adopt a min hash and an inexact string matching based similarity searching methods. the min hash is space and run-time efficient and was originally applied to cluster billions of web documents
 <cit> , and it was recently applied to dna sequences
 <cit> . vicuna decomposes each read into kmers, each hashed uniformly to a large integral space . the min hash value is used to represent each read. based on the proof
 <cit>  that the similarity between any two reads, defined as the jaccard index, is equivalent to the probability that they have the same min hash, similar reads are held to form the same contig via common min hashes without resorting to pairwise comparison. this strategy is particularly effective in reducing read redundancy in the data. in practice, we used a two-phased min hash strategy to increase specificity . however, this method will miss many overlaps that fail to have a common min hash, so we combine it with a seed-based approach. using spaced seeds is proven to be more sensitive in similarity detection while maintaining the same specificity compared to searching by exact strings
 <cit> . to identify good suffix-prefix overlaps among reads, we utilize a sliding window approach, where the window size is w. we then compute the subsequence of k  selected bases, termed spaced-kmers, in each window. sequencing errors or nucleotide polymorphism-incurred differences are neglected if they appear in the remaining  bases. finally, reads that share any common spaced-kmer are held in the same contig. unlike previous methods that store the entire spaced k-spectrum, we developed a strategy that keeps only a small fraction of it in the memory. relying on the observation that a read may concurrently appear in multiple contigs, we devised an efficient partitioning algorithm to merge such contigs .

contig validation
given p, the rate of variation, we would like to capture in the population, define a contig to be consist of a set of reads where each read r differs from the consensus of the contig by at most p×|r| bases where |r| denotes the read length. abiding by this definition, we validate each contig by iteratively aligning each constituent read to the consensus, discard those that do not satisfy the criteria and update the contig consensus base calls accordingly . this novel strategy effectively separates mis-placed reads from each contig generated in the previous step, and creates new contigs when applicable.

contig extension & merging
vicuna identifies contigs that share good suffix-prefix overlaps and iteratively merges them to form longer contigs until no further extension can be made. at each iteration, vicuna starts with the longest contig c. the contig cʹ that shares the maximum number of paired reads with c is examined to identify a good suffix-prefix alignment. however, such an alignment may not be discovered due to polymorphisms captured in either contig. we resolve this issue by editing each contig before alignment. specifically we  convert any consensus base that has nucleotide polymorphisms to the smallest alphabetic base, and  remove non-dominant low frequency insertions. these polymorphisms will be recovered if c and cʹ were to be merged. the contig alignment is carried out by first computing a sequence of non-overlapping common substrings between the consensuses of c and cʹ. we then use the needleman-wunsch algorithm for alignment and require the common substring blocks be directly aligned. to account for length polymorphisms that maintain the coding frame and hence represent true biological variants, any small gap comprising a multiple of three nucleotides is not penalized. once all de novo merges have been completed, contigs can be further merged if mappings to a previously assembled reference genome can validate the merge.

identification of target and non-target reads
viral samples derived from clinical specimens typically contain non-target nucleic acids , which can comprise a large portion of the sequencing output
 <cit> . these “contaminants” can be inherent in the sample or be artifacts that result from a sample prep method. non-target reads increase assembly cost, memory and run-time, and can also negatively impact the quality of the assembly of the target genome. to overcome this issue, we developed an optional component  in vicuna that identifies and flags such contaminant data and removes it from the assembly process. the optional module utilizes a multiple sequence alignment  comprised of available reference genomes for the target. the high degree of polymorphism in some populations compounded by errors introduced via the sample preparation process can impact the ability to identify target reads. as previously reported this situation can result in a failed alignment as a target read r looks too dissimilar to any known target genomic sequence
 <cit> . to address this issue, we employ a profiling method that divides the msa into multiple bins, and compares kmer compositions of r to each bin. if a sufficient number of similar  kmers spanning a sufficient length of r can be identified, r is considered to be target-alike. moreover, we use a less stringent similarity threshold to place paired reads which could not be placed otherwise but obey the distance constraint imposed by the paired read library.

sequence data generation & analysis
using previously described protocols, we amplified four large overlapping amplicons that captured the complete coding regions for denv
 <cit> , hiv
 <cit> , and wnv
 <cit> . amplified products from each sample were acoustically sheared . indexed illumina libraries were prepared as described
 <cit>  except that pfu ultra ii enzyme  was used and  <dig> cycles of pcr enrichment were employed. we pooled all the samples, gel purified them , and generated 225bp paired reads using the illumina miseq platform .

the number of illumina reads generated for each sample ranged from  <dig> m to  <dig> m . to evaluate the general characteristics and coverage profile of these data prior to assembly we aligned the reads to a standard reference genome for each virus . in all cases ≥  <dig> % of reads were uniquely aligned to the chosen reference. as expected, alignments to the standard references were suboptimal as compared to de novo assemblies , with the wnv reference better representing the sample population than the denv and hiv references. the wnv samples in general had a larger percentage of reads that could be uniquely aligned and a considerably smaller rate of discrepancy between the bases in the reference and the ones derived from the read alignment . the average coverage across all the samples ranged from  <dig>  to  <dig>  and in all cases the region of the genome targeted for sequencing was fully covered. coverage varied across the genome within each sample; sequence coverage within an individual sample varied between the minimum and maximum by  <dig> -  <dig> fold . in general, wnv and denv samples had greater uniformity of coverage than hiv samples, likely a result of either the greater diversity in the hiv samples or the high degree of secondary structure in the hiv genome
 <cit> .

asoapdenovo assembly is highly fragmented, a large number of short contigs were merged using the reference genome, leading to the inclusion of many low frequency variants that considerably increased the percentage of non-dominant bases found in the assembly. in the case of sample v <dig>  mosaik failed to report read alignment to the consensus. the number of genes with frame shift is not measured for soapdenovo. for run time and memory, †soapdenovo uses  <dig> threads while the other two use  <dig> thread. av <dig> is run on a subset of the reads .

we evaluate assembly results using the following major metrics
 <cit> : 1) the percentage of the target region captured , 2) the percentage of reads captured by the consensus, an indicator of how closely the consensus captures the population, and 3) the non-dominant base call rate, defined as the percentage of consensus bases that do not represent the majority base at a given position in the read data set. in addition, we report the number of contigs that were used for reference guided merging, which indicates how “fragmented” the assembly results would be if the patient was infected with unknown viruses.

de novo assembly of viral samples with less genetically diverse populations – denv & wnv
clinical samples derived from wnv and denv infected hosts are typically less genetically heterogeneous compared to samples from a host harboring a chronic infection such as hiv; they are acute diseases and either provide limited time for viruses to develop mutations or undergo very rapid selection, quickly removing a majority of early variants before samples are acquired. consensus generation in these samples is similar to haploid genome assembly, and existing assemblers are expected to work reasonably well since variants can be treated as sequencing errors and removed from the data. nonetheless, even high quality haploid genome assemblers like soapdenovo work poorly in dealing with such data .

vicuna fully recovered the target region in all samples, and in  <dig> out of  <dig> samples the target region is captured by a single contig  prior to guided contig merging using a reference. for wnv samples, a comparable number of reads aligned to the vicuna consensus and the standard reference . however, vicuna called the correct dominant consensus base in all cases. the vicuna consensus also better represented the denv samples; we observed an average  <dig> % non-dominant call rate, and up to  <dig> % more reads aligned as compared to the standard reference .

de novo assembly of viral samples with more genetically diverse populations – hiv
hiv/aids is a chronic disease. as such, samples obtained from hiv infected hosts typically harbor greater genetic diversity compared to samples from acute diseases as the virus has had sufficient time to develop mutations. given this greater genetic heterogeneity, existing short read assemblers perform poorly
 <cit> , and the use of reference genomes not derived from the sample population itself often results in a poor backbone on which to map variants.

vicuna successfully assembled the full target region of genetically diverse hiv clinical samples, and captured the genome in one or two contigs  prior to guided contig merging using a reference. compared to the standard reference genome, on average over 4% more reads can be aligned to vicuna consensus; we observed an average  <dig> % non-dominant call rate .

comparing vicuna assembly results to other assemblers
we compare the performance of vicuna with existing de novo genome assemblers soapdenovo <cit>  and av 454
 <cit> ; vicuna outperformed both. we report the result of soapdenovo, which is a representative for de bruijn graph based short read assemblers and its result is comparable to several of other assemblers of the same kind, such as metavelvet
 <cit> , used for metagenomic data sets, and velvet columbus
 <cit> , used for reference assisted assembly. av  <dig> is selected as a representative for overlap-layout-consensus based assemblers, as it was designed specifically for viral population consensus assembly. the av  <dig> algorithm was originally designed for  <dig> read data sets, and for application here we optimized it to assemble paired illumina reads . because av 454’s performance deteriorates as the coverage increases beyond a certain threshold, we down-sampled each dataset by randomly selecting about  <dig>  read pairs for each sample. cortex
 <cit>  was recently developed for assembling diploid genomes or a mixed population using color-coded de bruijn graphs. however, it appears not to be suitable for viral population assembly as when applied to current samples, it generated an excessive number of contigs, which cannot be reasonably handled.

soapdenovo generated a large number of contigs, but only a small percentage of them exceed the minimum length cutoff of 350bp . in total, these contigs account for less than 5% of the target region in each sample. since the algorithm is not able to utilize sufficient reads to form contigs, the dominant base call rate was < 85%. soapdenovo’s performance was not markedly better at assembling acute versus chronic viruses.

av  <dig> outperformed soapdenovo for all viruses, but was inferior to vicuna. vicuna has the capacity to scale to tens of thousands fold coverage, whereas, av  <dig> is limited to a few hundred fold coverage. this may significantly affect the performance of av  <dig> in clinical samples rich in contamination , where the down-sampling required by av  <dig> would result in insufficient coverage of target genome
 <cit> . in all samples, vicuna has a better accuracy in producing intact genes that have no frame shift . for wnv samples, the dominant base accuracy was similar, but on average  <dig> % fewer reads align to av  <dig> consensus than to vicuna consensus . for denv samples v <dig> and v <dig>  the vicuna consensus was significantly more accurate in dominant base calls , and approximately 1-2% more reads aligns to it as compared to the av  <dig> consensus. for hiv samples, where the genetic total diversity is higher, the advantage of vicuna is more pronounced. up to 6% more reads can be aligned to vicuna consensus compared to av  <dig> consensus, and it is over  <dig> times more accurate  in dominant base calling.

vicuna is able to capture low frequency length polymorphisms and large deletions in viral samples
other than nucleotide polymorphisms, length polymorphisms are frequently observed in viral samples, exemplified by a small number of nucleotide insertions at certain genomic loci in a subset  of viral genomes in the population. with the pressure to preserve coding frames, these insertions typically consist of a multiple of three bases. vicuna is capable of capturing non-dominant insertions that occur at the frequency of at least p, a user specified parameter, when compared to the dominant variant at the same loci. in viral samples, we are interested in p ≤ 5%, which are typically treated as sequencing errors in haploid genome assembly. when p=5% , we observed in wnv, denv, and hiv samples,  <dig> %, 0%, and  <dig> % low frequency insertions, respectively, which fits the expectation that hiv genomes have a larger amount of length polymorphisms compared to the other two viruses.

large deletions have been observed previously in viral population studies . since there is no clear definition on the minimum size, we recorded all deletions ≥ 500bp in length . we observed large deletions in two denv and all four hiv samples and, in some of them, we found multiple types of variants. the majority of these deletions occur in the env and pol genes in hiv and in the ns <dig> gene in denv. these large deletions may be real biological mutants or they could be amplification or sequencing artifacts . to eliminate the possibility that our observations are due to illumina specific artifacts, we validated the observed deletions using  <dig> read data sets from the same samples. almost all large deletions in hiv samples are observed with the exact breakpoints in the  <dig> data , while none of the denv deletions were observed. these results suggest that de novo assembly of illumina reads via vicuna provides the ability to observe novel large deletion events.

vicuna is applicable to other data types
many existing sequence data sets that characterize viral populations were generated using the roche  <dig> technology. to demonstrate the applicability of vicuna to such data, we used vicuna to generate a consensus for wnv, denv, and hiv clinical samples sequenced by  <dig>  in all cases, the full target region is recovered with high fidelity . vicuna achieves a 100% dominant call rate. since reads generated by the life sciences ion torrent technology share similar error properties to  <dig> reads, we anticipate that vicuna will be suitable for assembly of these data as well.

CONCLUSIONS
we presented vicuna, a program for de novo consensus assembly of genetically heterogeneous viral populations. for each read data, vicuna outputs consensus sequence and for each consensus sequence, the multiple sequence alignment of its constituent reads. we have demonstrated that vicuna recovers consensus genomes with high fidelity for viral intra-host samples obtained from patients infected with either acute or chronic diseases and that it outperforms other available assemblers in both the accuracy and continuity of the consensus genome. the ability to assemble de novo accurate and contiguous consensus genomes from ultra-deep short read sequence data derived from the illumina platform or other comparable technologies provides the ability to capture a higher resolution map of viral genetic variants in an infected host with greater cost efficiency than previously possible. the availability of a consensus derived from the sample itself provides greater accuracy with respect to aligning reads prior to variant detection and as such vicuna will enable improved detection of low frequency variants and subsequent haplotype identification in genetically heterogeneous populations. while vicuna was developed for the analysis of viral populations its application to other heterogeneous sequence data sets such as metagenomic or tumor cell population samples may prove beneficial in these fields of research.

