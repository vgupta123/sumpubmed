BACKGROUND
an important challenge of computational biology is the reconstruction of large biological networks from high throughput genomic and proteomic data. biological networks are used to represent and model molecular interactions between biological entities, such as genes and proteins in a given biological context.

in this paper we focus on the identification of new transcriptional targets, i.e. coding dna regions directly regulated by transcription-factors. transcription factors are proteins, coded by specific genes, that, alone or with other proteins in a complex, bind the targets cis-regulatory regions and control the target transcriptional activity by promoting or blocking the recruitment of rna polymerase.

in identifying the interactions between transcription-factors and genes from experimental data, two broad classes of computational methods can be distinguished in literature  <cit> : those that rely on the physical interaction between molecules  which relate transcription factors to sequence motifs found in promoter regions; and algorithms based on the influence interaction that try to relate the expression of a gene to the expression of the other genes in the cell . most of the approaches of the second class are basically unsupervised and model the reconstruction of transcriptional relationships as a classification problem, where the basic decision is the presence or absence of a relationship between a given pair of genes  <cit> . those methods can be distinguished in: i) gene relevance network models, which detect gene-gene interactions with a similarity measure and a threshold, such as aracne  <cit> , timedelay-aracne  <cit> , and clr  <cit>  that infer the network structure with a statistical score derived from the mutual information and a set of pruning heuristics; ii) boolean network models, which adopt a binary variable to represent the state of a gene activity and a directed graph, where edges are represented by boolean functions ; iii) differential and difference equation models, which describe gene expression changes as a function of the expression level of other genes with a set of ordinary differential equations   <cit> ; and iv) bayesian models, or more generally graphical models, which adopt bayes rules and consider gene expressions as random variables  <cit> .

the experimental validation of predicted transcriptional regulations is performed with chip-on-chip  <cit> , a technique used to investigate interactions between proteins and dna in vivo by combining chromatin immuno-precipitation  with microarray technology . specifically, it allows the identification of the cistrome, sum of binding sites, for dna-binding proteins on a genome-wide basis. whole-genome analysis can be performed to determine the locations of binding sites for almost any protein of interest, in particular transcription factors. the goal of chip-on-chip is to localize protein binding sites that may help identify functional elements in the genome. for example, in the case of a transcription factor as a protein of interest, one can determine its transcription factor binding sites throughout the genome.

a recent trend in computational biology aims reconstruct large biological networks with supervised approaches  <cit> . supervised methods require a training set, which in our context means a set of transcriptional targets where the information that they are regulated by a transcription factor is known in advance. training targets are used to estimate a function that is able to discriminate whether a new transcriptional interaction exists. the literature of machine learning proposed several supervised algorithms: neural networks, decision tree, logistic models, and support vector machines   <cit> . among all svm gave promising results in the reconstruction of biological networks  <cit> . for example, sirene adopted an svm classifier to reconstruct the regulatory network of escherichia coli, and obtained more accurate results than unsupervised methods based on mutual information   <cit> . compared to unsupervised methods, supervised methods are potentially more accurate, but in fact they need an initial set of known regulatory connections. this is in principle not a restriction as many regulations are progressively discovered and shared among researchers through public regulatory databases. some examples are: regulondb , kegg , trrd , transfac , ipa .

in general a supervised binary classifier needs both positive and negative examples to learn effectively. in the context of gene regulatory networks this condition is not satisfied, as counter negative examples are not available or may be hard to collect. in functional genomics, information about negative examples is in fact not available, as the input is usually a finite list of genes known to have a given function or to be associated to a given disease, and the goal is to identify new genes sharing the same property. thus, under a machine learning perspective, the supervised inference of new transcriptional targets falls into a class of semi-supervised learning problems that consists of learning from positive and unlabeled data. the training set is composed just by known positive examples  and the goal is to predict the unknown positive in examples the unlabeled set.

learning from only positive and unlabeled data is a hot topic in the literature of data mining, where three main families of approaches can be distinguished  <cit> : i) methods that reduce the problem to the traditional two-class learning by selecting reliable negative examples from the unlabeled set  <cit> ; ii) methods that do not need labeled negative examples and basically adjust the probability of being positive estimated by a traditional classifier trained with positive and unlabeled examples  <cit> ; and iii) methods that treat the unlabeled set as noisy negative examples  <cit> .

in this paper we focus on the first class of approaches that rely on a starting selection of negative examples. the main problem is that some of the selected negative examples could in fact be positives embedded in the unlabeled data, reducing the prediction capability of a binary classifier. we empirically evaluate this effect by simulating the positive contamination inside the negative training set showing that the performance of the classifier improves when the positive contamination is low. such a result demands for an approach that is able to generate a sufficiently large negative training set without positive contamination.

we propose, noit , a method to select reliable negative training examples by exploiting the known gene regulatory network topology in the specific context of prediction new transcriptional targets. the method is an extension, to a specific context, of approaches recently published in  <cit>  and  <cit>  where reliable negatives selection benefits from the over presence, in the current known gene regulatory networks, of typical network motifs  <cit> . we introduce a new heuristic that still exploits the known regulatory network topology but not in terms of network motifs as in the specific context of transcriptional target prediction the relationships between transcription-factors and their targets does not exhibit significant network patterns. the noit method gives less importance to indirect targets, i.e. targets of a transcription-factor regulated indirectly through other transcription-factors. the idea is based on the observation that genes controlled directly by a transcription factor or indirectly through other transcription factors are likely to attain for the same family of functions, thus representing unreliable negative candidates. this is supported by the fact that transcription factors evolved in the service of specific biological functions and are usually classified according to their regulatory function  <cit>  and sequence similarity  <cit> . moreover downstream targets activity is usually modulated by regulatory circuits involving small groups of transcription factors organized in typical network motifs.

we compare noit with other negative selection approaches known in literature. for this purpose we adopt the dataset of escherichia coli, where almost all transcriptional regulations are known and a huge amount of experimental data is available for benchmarking . furthermore we provide an example of application in the prediction of bcl <dig> direct targets in normal germinal center human b cells by adopting the results of basso et al.  <cit>  showing that noit predicts  <dig> correct targets in the top  <dig> ranked genes, outperforming other supervised and unsupervised methods that predict less than  <dig> correct targets. the paper is organized as follows. the next section  introduces the noit heuristic, overviews the literature methods that are based on a reliable negative selection procedure, and describes the empirical procedures aimed at evaluating the performance of the negative selection heuristics. section on results reports and discusses the outcomes of the study, and the last section concludes the paper outlining directions for future work.

methods
problem formulation
in a binary classification problem, given a set of training examples, , , ...,  ∈ x × {+ <dig> -1}, the goal is to determine a function f: x → {- <dig> +1} that is able to predict the label y ∈{+ <dig> -1}of a new observation x ∈ x. machine learning algorithms infer an estimate of the function f from the available examples. to distinguish effectively whether a new observation is positive or negative, the training set should contain a sufficient number of both positive and negative examples. such a conventional condition does not hold in the problem we aim to formalize as the training set is composed by only positive examples. in the context of transcriptional target prediction negative counter examples are in principle not available as the nonexistence of a transcriptional activity is hard to be experimentally verified. liu et al.  <cit>  theoretically showed that a statistical classifier may take advantage from unlabeled examples, and that if the sample size is large enough, the classifier could converge to a good classifier by maximizing the number of unlabeled examples classified as negative while constraining the positive examples to be correctly classified. the selection of reliable negatives from the unlabeled set could be crucial for the quality of a positive only classifier. with those examples a classifier could be trained with a traditional two-class set under the control of a convergence condition. the selection of reliable negative training examples may, or may not, exploit the underlying application domain. for example, in the classification of web documents, reliable negative documents are those that do not contain any of the most frequent words extracted from known positive documents  <cit> .

we propose, noit , a negative selection heuristic that exploits the known regulatory network topology by giving less importance to indirect targets, and formalized as follows. let g be the set of all genes in an organism and tf ⊂ g the set of transcription factors. given a transcription factor tfi ∈ tf, the goal is to infer a function, ftfi):g→{- <dig> +1}, from a set of target genes, ptfi={,,...,}⊂, that are known to be regulated directly by tfi . the function should be able to predict the label y of a new gene g∈utfi=g\ from the unlabeled set. the transformation function ϕ describes each gene with an m-dimensional real valued feature vector, ϕ:g→ℝm, such as expression values measured in m different experimental conditions.

the goal of a negative selection heuristic is to select from the unlabeled set utfi a sufficiently large negative training set without positive contamination. our aim is to propose a method based on the assumption that an unlabeled gene g∈utfi is a bad negative candidate if it is indirectly controlled by tfi, through other transcription factors. such information can be extracted from the known gene regulatory network, or in the situation wherein such information is not available, it could be estimated with binding site promoter analysis  <cit>  and/or unsupervised gene regulatory prediction  <cit> .

we introduce a probability mass function pmftfi of negative candidates distribution to estimate the probability that an example g∈utfi is a good negative candidate. we compute pmftfi as:

 pmftfi=1|utfi|k|tf| 

where k ∈  is the minimum number of transcription factors, tfi+ <dig>  tfi+ <dig>  ..., tfi+k, that link tfi to g, i.e. for every j = i, ..., i + k - <dig>  tfj+ <dig> is a known target of tfj . the term 1/|utfi| serves to scale the probability mass function to sum to  <dig>  when a path linking tfi and g through a set of known transcription factors does not exist, we assume that k = |tf|. in that case the probability is maximum, instead it is minimum when at least one tfk exists such that g is regulated by tfk and tfk is regulated by tfi . the hypothesis is that the expression profile of genes regulated by tfi are more correlated with genes selected as bad negatives than those selected as good negatives. this is confirmed with a bootstrapping experiment where we selected  two random genes, g <dig> and g <dig>  belonging to the targets of a transcription factor, and two genes, ggood and gbad, belonging respectively to good and bad negative candidates as selected by the noit procedure. we computed the correlation between g <dig> -g <dig>  g <dig> -ggood, and g <dig> -gbad obtaining the three distributions shown in figure  <dig>  the black curve shows the distribution of correlation between genes within the same targets, the red curve shows the distribution of correlation between targets and bad negative candidates, and the green curve shows the distribution of correlation between targets and bad negative candidates. a two sample mann-withney test between the latter two distributions shows a significant difference  suggesting that the noit procedure is able to select negative that are more distant, in term of correlation, from targets. with a learning scheme similar to sirene  <cit>  we divide the unlabeled set utfi into three random folds. the labels of each fold are predicted with a binary classifier trained with the known positives and a selection of negative examples drawn from the other two folds. sirene adopts a method, known as pu learning , that is strongly affected by the positive contamination of unlabeled examples as all unlabeled examples are considered good negative candidates. we limit such a contamination by selecting the top nc negative candidates scored by the above introduced probability mass function pmftfi. we consider a number of negatives candidates, nc, depending on the number of known positives nc=k*|ptfi|. the parameter k may affect the performance of the classifier. with an experiment performed in the context of escherichia coli we observed on the independent test set that the best performance is obtained with k around  <dig> .

negative selection methods in literature
in this section we briefly review the most important positive only classification methods that include a reliable negative selection step in their classification schema.

spy-svm
spy-svm is a technique proposed in  <cit>  that works as follows. a percentage of known positives, {s <dig>  s <dig>  ..., sk}, randomly selected from ptfi, that act as 'spies', are sent to the unlabeled set utfi. an svm classification algorithm is trained with positive examples  and the unlabeled set  assumed as negatives. the spies should behave identically to the unknown positive examples belonging to utfi, and this allows to reliably infer the behavior of the unknown positive examples. a threshold t is employed to make the decision whether an example in utfi is a reliable negative or not. examples with a probability of being positive, p = +1), lower than t are the most likely negative examples. the threshold is intuitively calculated as the minimum of the probability of being positive of spies, i.e. t = min{p = +1), p = +1), ..., p = +1)}. this means that all the spy examples should be classified as positives.

psol - positive sample only learning
psol selects strong negative example using the euclidean distance measure  <cit> . the algorithm starts with a negative candidate that is the most farthest unlabeled example from ptfi calculated as the maximum of the minimum distance from the elements of ptfi. more negative candidates are selected from the unlabeled set utfi satisfying the constrain that are different from the known positive examples and farthest from the previously selected negative ones. the algorithm assumes that the negative examples in the unlabeled set are located far from positives and from the previous selected negative examples. the last condition assures that the negative set spans the whole negative examples in the unlabeled set. given such initial negative set, the psol method iteratively expands the negative set by using a two-class svm trained with known positives and the current negative selection. negative set expansion is repeated until the size of the remaining unlabeled set goes below a predefined number. at this last step, the unlabeled data points with the largest positive decision function values are declared as the positives.

rocchio-svm
rocchio-svm is based on a technique adopted in information retrieval to improve the recall of pertinent documents through relevance feedback  <cit> . it identifies the set of reliable negatives by adopting two prototypes, one for the positive class, cp, and one for the unlabeled ones, cu, computed as follows:

 cp=α1|ptfi|∑g∈ptfiϕ||ϕ||-β1|utfi|∑g∈utfiϕ||ϕ|| 

 cu=β1|utfi|∑g∈utfiϕ||ϕ||-α1|ptfi|∑g∈ptfiϕ||ϕ|| 

where α and β adjust the relative impact of positive and negative training examples. the unlabeled examples that are more similar to the unlabeled prototype than to the positive one, i.e. sim <sim, are selected as strong negative examples. to compute such a similarity the rocchio technique adopts the cosine similarity. with the known positive examples and the selected negative examples a conventional svm classifier is trained and then used to classify the remaining set of unlabeled examples.

bagging - svm
baggin svm is an ensemble technique that generally improves the performance of individual classifiers when they are unstable or not correlated to each other. positive only learning have a particular structure that leads to instable classifiers due to the positive contamination of the unlabeled set which can be advantageously exploited by a bagging-like procedure  <cit> . the approach collects the outcome of a huge number classification runs , where each classifier, fi, is trained with the known positive examples, ptfi, and a random set of nc negative candidates drawn uniformly from utfi, considered as negative examples. the ensemble classifier, f, scores an unlabeled example g by averaging the scores obtained by that example at each run:

 f=∑i∈tgfi|tg| 

where g is a member drawn from utfi, fi is the i-th classifier, and tg is the set of partial classifiers that were not trained with g, i.e. the unlabeled example g was not drawn by the random selection.

empirical evaluation methods
in this section we introduce the datasets, the basic learning algorithm, and the methods we adopted to empirically evaluate to which extend a negative selection heuristic improves the performance of a classifier trained to infer new transcriptional targets.

datasets
to test our approach we adopt the well known dataset of escherichia coli provided by faith et al.  <cit> , and a dataset that was adopted by basso et al.  <cit>  to predict bcl <dig> direct target genes in normal germinal center human b cells.

the dataset of escherichia coli consists of  <dig> different affymetrix antisense <dig> microarray expression profiles for  <dig> genes. the transcriptional regulatory network of escherichia coli is the most complete annotated network consisting of  <dig> experimentally confirmed relationships between  <dig> transcription factors and  <dig> direct targets extracted from regulondb   <cit> .

the dataset of basso et al. is deposited in the gene expression omnibus database and is accessible through geo series accession number gse <dig>  it consists of  <dig> expression profiles of  <dig> b-cell lymphoma biopsies,  <dig> purified tonsillar geminal center,  <dig> naive and memory b cells,  <dig> follicular lymphoma biopsies, and  <dig> lymphoblastoid cell lines. we normalized the dataset from cel files according to the rma procedure  <cit>  and filtered out probes with low inter experiment variability by means of the varfilter function of the genefilter bioconductor package. the final dataset is composed by  <dig> samples and  <dig> genes. basso et al. identified a group of  <dig> new core targets down-regulated by bcl <dig> with an integrated biochemical-computational-functional approach , validated through chip-on-chip.

we show that those  <dig> new core targets can be predicted with a supervised learning approach starting from a positive training set of  <dig> targets annotated as down-regulated by bcl <dig> in a previous work by ci et al.  <cit> . for the noit negative selection procedure we rely on  <dig> transcription factors known to be regulated by bcl <dig> by transfac sequence motifs analysis which considers those that exhibit a bcl6-bound enrichment in their promoter regions as reported in  <cit> . their targets were predicted preliminary with aracne as reported in the supplemental table  <dig> of reference  <cit> .

basic learning algorithm
we use the support vector machine , with platt scaling  <cit> , to estimate the probability that a target is regulated by a transcription-factor. in particular we use the svm implementation provided by kernlab  <cit> , a package for kernel-based machine learning methods in r. the basic element of an svm algorithm is a kernel function k, where x <dig> and x <dig> are feature vectors of two gene targets. the idea is to construct a separation hyperplane between two classes, + <dig> and - <dig>  such that the distance of the hyperplane to the points closest to it is maximized. the kernel function implicitly maps the original data into some high dimensional feature space, in which the optimal hyperplane can be found. in our experiment we adopt an svm classifier for each transcription-factor tfi ∈ tf trained with the known positive targets and the reliable selection of negative examples performed with a negative selection approach. such a classifier in then used to score the set of genes g ∈ g\tf according to their probability to be regulated by tfi. we used c-support vector classification  which solves the following problem:

 minα12αtyiyjkα-etα 

subject to: yt α =  <dig>  where yi ∈ {+ <dig> -1}is the class of vector xi,  <dig> ≤ αi ≤ c, i =  <dig>  ..., 2n, e is a vector with all elements equal to one, and k is a kernel function. we adopt a radial basis kernel function defined as:

 k=e-γ|xi-xj| <dig> 

where c and γ are parameters that we set empirically inside the training loop  <cit> .

cross validation and performance measures
to estimate the unknown performance of a classifier designed for discrimination we adopt a workflow consisting of  <dig> steps . for each transcription factor tfi ∈ tf we partition the original dataset into  <dig> random folds. alternatively  <dig> folds are used for training, while the other fold is used for testing . each fold contains a density of positives that is almost similar to the density of positives in the original dataset. the known targets regulated by tfi belonging to the current training set is split into a positive set ptfi, assumed to be the known positive training set, and an unknown set qtfi, forming with ntfi the current unlabeled set utfi . the size of ptfi is incremented linearly starting from  <dig> or according to the fraction |ptfi||ptfi∪qtfi|. to limit the selection bias we re-sample ptfi  <dig> times. the negative training set is extracted from the unlabeled set, utfi , and adopted, together with the current known positives, to train an svm classifier . genes belonging to the test set are scored according to the current classifier and the accuracy of classification is evaluated at different ranking levels in terms of precision and recall as follows:

 prn=tpnn;rcn=tpn|targets| 

where tpn is the number of true positives appearing in the top n ranked targets, and targets is the set of tfi targets we want to predict in each test set. instead, true positive rates and false positive rates are computed as:

 tprn=tpn|qtfi|;fprn=n-tpn#truenegatives 

where #true negatives is the number of true negatives in the test set. from those measures we compute also aggregate performance measures, such as: auroc  and aupr . within a selection of known positives performance measures are averaged among all folds, all positive sampling runs, and all transcription factors obtaining an overall performance estimation of the classifier.

RESULTS
effect of positive contamination
the contamination of the training set with positive examples considered wrongly as negatives affects the performance of a classifier. we define the level of positive contamination as the fraction ρq of unknown positives, with respect to the total number of unknown positives , selected wrongly as negatives. figure  <dig> shows the effect, in terms of auroc  and aupr , of positive contamination in two extreme conditions: a training set with full positive contamination  and a training set with no positive contamination . in the first all unknown positives have been selected  as negatives, u = q + n. instead, in the second the training set is composed just by true negatives, u = n , and represents an ideal classifier with a perfect negative selection heuristic. in principle the actual performance of a negative selection heuristic should be within the area delimited by the two curves.

both classifiers have been trained in the context of escherichia coli with the procedure depicted in figure  <dig> at different levels of known positives . the main effect is that the performance of both contaminated and uncontaminated classifiers decreases with the fraction of known positives, although the proportion of that decrement is more rapid for the classifier trained with full positive contamination. when the fraction of known positives is minimum  the difference between contaminated and uncontaminated classifiers is maximum.

effect of the negative selection approach
the performance of a negative selection approach is affected by the proportion of known positives available in the training set. with the evaluation procedure depicted in figure  <dig> we evaluated the performance of a negative selection approach by varying both the relative fraction and the absolute number of known positives. the latter being more in accordance with practical purposes, as users only know the total number of positives which they have. figure  <dig> reports, for each method, the average auroc computed at different fraction of known positives  and at different number of known positives . on average the performance of each method increases with the quantity of known positives. with the exception of rocchio each method reaches the maximum performance  when the training set is completely labeled, i.e. the percentage of known positives is maximum . at low levels of known positives the difference among methods is more significant. up to a percentage of 60% of known positives, or, up to a number of  <dig> known positives, in the training set, the noit procedure outperforms significantly all other methods. at low levels of known positives the worst performance is registered by pu, as in fact does not adopt any negative selection approach. instead, at high levels of known positives the worst performance is registered by rocchio.

the table shows, at different percentage of known positives, the average recalls of negative at 80% and 60% of precision . the p-value column  is the outcome of a t-test performed to check whether the recall of noit is greater than the recall of another negative selection method. a p-value shown in boldface means that the statistical significance of the test is less than  <dig> .

prediction of bcl <dig> core targets in gc human b cells
in order to illustrate an examples of application we predict bcl <dig> core targets in gc human b cells adopting data and results provided by basso et al.  <cit> . figure  <dig> shows the number of true bcl <dig> core targets appearing in the top n genes ranked by an svm classifier trained with different negative selection approaches. each classifier has been trained by using the previously known targets provided by ci et al.  <cit>  and the predicted ranked set of genes has been compared with the bcl <dig> new core targets published by basso et al.  <cit> . for the noit selection procedure we rely on  <dig> transcription-factors, reported in the supplemental table s <dig> of by basso et al.  <cit> , known to be controlled by bcl <dig> by means of transfact sequence motif analysis. the figure includes also the result obtained with aracne  <cit> , an unsupervised method adopted by basso et al.  <cit> , that ranks genes according to their mutual information with bcl <dig>  it is noticeable that supervised reverse engineering methods perform better than unsupervised, a result already confirmed in literature  <cit> . instead, among supervised methods there is a remarkable difference in the top  <dig> ranked genes, where noit predicts  <dig> correct targets  outperforming other methods that predict less than  <dig> correct targets. over the first  <dig> ranked genes the bagging method exhibits the best performance reaching a correct prediction of  <dig> targets in the first  <dig> ranked genes, whereas noit predicts only  <dig> and the others less than  <dig> 

we like to remark that with this experiment we predicted an interesting number of bcl <dig> targets without the integrated approach consisting of wide spectrum genomics experiments adopted by basso et al.  <cit>  . furthermore, among supervised techniques, the noit procedure can take advantage from supplemental transcriptional information, which is aviable in many contexts.

CONCLUSIONS
the availability of only positive examples affects negatively the performance of supervised classifiers. this is particularly manifested in the context of learning transcriptional relationships. we showed that the selection of reliable negative examples, a practice adopted in text mining approaches, could improve the performance of such classifiers opening new perspectives in predicting new transcriptional targets. we introduced a new negative selection heuristic, noit, that promotes, as negative candidates of a transcription-factor, genes that are not regulated indirectly through other transcription-factors. the method has been tested against other negative selection procedures showing that it is able to improve the average performance of almost 10%, in terms of auroc and aupr, especially when the number of known positives is low. we provided an example of application in the context of prediction of bcl <dig> direct core targets in normal germinal center human b cells by adopting the results of basso et al.  <cit> . we showed that in the top  <dig> genes, ranked with the noit method,  <dig> targets out of  <dig> are those experimentally demonstrated by basso et al.  <cit> . this is promising as such targets have been predicted without intersecting the results of chip-on-chip assays, aracne outcomes, and transcriptional repression in gc experiments.

threats to external validity, concerning the possibility to generalize our findings, affect the study as we evaluated the heuristics on a limited number of organisms. the study can be replicated as the tools are available upon request to authors and experimental datasets are publicly available.

competing interests
the authors declare that they have no competing interests.

authors' contributions
lc conceived the negative selection heuristic, designed the empirical evaluation procedure, and drafted the manuscript. vp contributed to the negative selection heuristic definition. pz contributed to assess the prediction of bcl <dig> core targets in gc human b cells. mc participated in the coordination of the study and contributed to draft the manuscript. all authors read and approved the final manuscript.

declarations
the publication costs for this article were supported by a research project funded by miur  under grant prin2008-20085ch22f.

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2013: computational intelligence in bioinformatics and biostatistics: new trends from the cibb conference series. the full contents of the supplement are available online at http://www.biomedcentral.com/bmcbioinformatics/supplements/14/s <dig> 

