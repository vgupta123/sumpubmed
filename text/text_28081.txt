BACKGROUND
stable but not static, the dna of human genome is subject to a variety of heritable changes of different types, which significantly contribute to the phenotypic differences of individuals in human populations. in addition to the single nucleotide polymorphisms , these genetic changes also include the chromosomal structural variations, such as insertions, deletions, duplications, inversions, and translocations, on various genomic scales. recent studies showed that insertions, deletions, and duplications of dna segments of  <dig> kb or longer in the genome-- collectively referred to as the copy number variants --occur at a much higher frequency than previously expected  <cit> . a recent global study of cnvs in the human genome showed that the regions of cnvs covered more nucleotide content per genome than snps  <cit> . it is now widely believed that cnvs are as important as snps and other small genomic changes in their contribution to the phenotypic variation in human populations.

currently, unbalanced structural variants can be experimentally identified by methods based on microarray technology, polymerase chain reaction, or dna sequence comparison. array-based method is a natural, high-throughput extension of the comparative genomic hybridization  analysis, which was originally developed as a method to reveal any regions of allele loss or aneuploidy by fluorescence microscopy  <cit> . high-density oligonucleotide microarrays, which offer high genomic resolution, have been used in several recent array-cgh studies  <cit> . the last several years have seen rapid advancement in the field of sequencing technology. novel methods  <cit>  are being developed to reduce the cost and increase the throughput by generating massive amounts of sequence that can be aligned to the genomic reference. this development has made it possible to resequence whole genomes from multiple individuals.

indeed, a major sequencing project, the  <dig> genomes project, has been launched to resequence the genomes of at least a thousand people from around the world using the new sequencing technologies to produce the most detailed map of human genetic variation for disease studies. as the technologies mature and their uses spread, new sequencing-based methods to detect structural variations have been developed to take advantage of the massively parallel sequencing. in the read-depth approach, after dna fragments are sequenced from one or both ends, the reads are mapped to the genome and then counted in a non-overlapping sliding window. both methods provide a rapid, robust, and comprehensive approach to identify cnvs on the whole-genome scale.

both array-cgh and read-depth sequencing generate genomic copy number  data in a very similar format: they consist of genomic signal output indexed by the genomic locations. the signals are log-ratios of normalized intensities from the test sample to those from the reference sample for array-cgh and sequence read counts after mean subtraction for read-depth sequencing, respectively. the goal of analyzing such data is to detect cnvs by identifying regions with signals that are consistently higher or lower than the normalized baseline. implicitly, there are two distinct and yet closely related estimation problems: one is to estimate the number of cnvs, and the other is to determine the boundaries and the average signal strength of each of them. many statistical and computational methods have been developed to identify cnvs in individual genomes. they include approaches built on hidden markov model  <cit>  or in a bayesian framework  <cit> . recently a method to identify recurrent cnvs within a group of individuals has also been proposed  <cit> . based on their data analysis approaches, algorithms that have been developed to analyze such data can be roughly grouped into three types: some only smooth the raw log-ratio data and the regions with log-ratios higher or lower than a preset threshold are identified as cnvs  <cit> , others estimate the number of cnvs and their boundaries directly using the original log-ratio data  <cit> , and the rest use a combined approach  <cit> . the relative performance of these algorithms has been assessed  <cit> .

here we present a bayesian statistical framework to analyze both array-cgh and read-depth data. treating parameters that define the underlying genomic copy number variation encoded in the data as random variables, our approach derives the posterior distribution of those parameters given observed data. this statistical method models the location of regional changes and their corresponding associated copy number, and estimates the overall noise level in the data at the same time. sampling from the posterior distribution using markov chain monte carlo  simulation is able to give both the best estimate and a corresponding bayesian credible interval for each parameter in the model. we discuss how our model was derived and implemented, and the empirical results from applying our method to both microarray and sequencing data for cnv discovery.

statistical model
in the life sciences we are often faced with the task of making inferences about some object of interest given incomplete and noisy experimental data. in the case of cnv study, we are primarily concerned with inferring the number of the dna copy number variations, their locations, sizes, and corresponding copy numbers-associated amplitude measurements, given the genomic copy number data, which are log-ratios of sample and control intensities measured on microarrays or read depths generated by shot-gun genomic sequencing. to demonstrate the application of our method to the read-depth data, we take a set of sequence reads from the  <dig> genomes project and construct a 'read-depth intensity signal' spectrum by first mapping the reads to the human genome reference sequence and then counting the number of reads in a sliding window, a procedure that transforms sequencing data into array-like intensity signal. we capture these unknown quantities in a probability model that relates them to the observed data. our model is bayesian in essence as we assign prior distributions to parameters and use the posterior distribution function to estimate the underlying data generating process. given the posterior distribution, we then use the markov chain monte carlo method to fit the model. by doing so, we get not only the best estimates for these unknown parameters but also bayesian credible intervals for the estimations at the same time.

given a set of genomic copy number data d = {gk, xk}, k =  <dig>   <dig>  ..., m, in which gk is the sorted genomic location of the kth data point, xk the signal at this location, and m the number of data points, we try to infer the genomic 'spectrum' ℱ, which is the unknown function defined by the cnvs encoded in the data set with the same measurement unit as xk. assuming that the measurements of cnvs are all step functions, the spectrum ℱ can be written as

  fk={ajif sj≤gk <,j= <dig> ,…,n0otherwise, 

where n is the number of 'smoothed' cnvs detectable in the data set, and sj, wj, aj are the start genomic location, the width, and the amplitude of the j-th cnv respectively. thus the 'ideal' data set corresponding to d based on this model is f = {gk, fk}, k =  <dig>   <dig>  ..., m. for simplicity, we assume that x <dig>  x <dig>  ..., xm measured in d are independent random variables each subjected to additive gaussian noise around ℱ with a standard deviation σ.

given the aforementioned model , the set of parameters to be inferred from d is θ = {n, , σ2}, j =  <dig>   <dig>  ..., n. sometime for convenience, instead of reporting the estimate of wj, we report the estimate of ej, the end of the j-th cnv . the conditional probability distribution function p summarizes our inference about θ given the data and our prior knowledge about the cgh spectrum ℱ.

bayes' theorem relates the posterior probability distribution function p to the likelihood probability distribution function p that can be calculated from the data and the prior probability distribution function p that encodes the prior knowledge,

  p=p·pp∝p·p, 

where the normalization constant p is omitted for simplicity.

likelihood. given the simplifying normality assumption stated above, the likelihood function takes the form

  p=e−ψ2/2m/ <dig>  

where

  ψ2=∑k=1m2σ <dig>  

prior. given the discrete nature of cnvs, it is reasonable to assume a priori independence among all the parameters in θ. we choose the following prior distributions:

▫ uniform distributions for n, sj, and wj :

• p = 1/nmax

• p = 1/ = 1/m

• p = 1/m

▫ normal distribution for aj: p ~ n

▫ inverse gamma distribution for σ2:p=βασ−2e−β/σ2/Γ

thus the prior probability distribution function is assigned as

 p=1nmax·1m2n·βαΓσ−2e−β/σ2·∏j=1n12Ũκj2e−2/2κj <dig>  

after rearrangement and removal of the constant nmax, we have

  p∝1m2n·σ2·n/2·∏j=1nκj·eβ/σ2+Σj=1n2/2κj <dig>  

where α, β, τj and κj are the hyperparameters that characterize the prior distribution. see the implementation subsection below for their parameterization.

posterior. substituting the product of the likelihood and the prior of equations  and  into equation , we obtain

  p∝1m2n·σ2+m·n/2·∏j=1nκj·eψ2/2+β/σ2+Σj=1n2/2κj <dig>  

for a given model {ℳ: θ ∈ Θ}, where n is known and thus θ = {, σ2}, j =  <dig>   <dig>  ..., n, the posterior distribution of θ given the data d and the model ℳ can be expressed as

  p∝1σ2+m·eψ2/2+β/σ2+Σj=1n2/2κj <dig>  

informative prior. if we have information on certain parameters in θ, for example sj and wj, from an initial scan of data d, such information can be coded in an informative prior to simplify subsequent parameter estimation. for example, suppose we know n =  <dig>  the cnv starts at a certain place between genomic position a and b, and its length is between c and d bp long. we code such prior information as following:

▫ uniform distributions for s <dig> and w1:

•  p={1b−afor s1∈0otherwise

•  p={1d−cfor w1∈0otherwise

keeping priors on other parameters the same as before, we have

  p{∝1σ2+m·eψ2/2+β/σ2+2/2κ12for s1∈ and w1∈=0otherwise                        . 

with this informative prior, the posterior is the same as , but only none-zero for s <dig> ∈ and w <dig> ∈ . this condition simplifies subsequent parameter estimation, as s <dig> and w <dig> only need to be sampled in these two intervals during mcmc simulation.

in some case, we only know the start and the length of a particular cnv  but still have to estimate n and the parameters of the other cnvs. this is a case that mixes the general and the special ones presented above. it is easy to show the informative prior is a mix of  and :

 p{∝1m2n·σ2·n/2·∏j=1nκj·eβ/σ2+Σj=1n2/2κj2for s1∈  and w1∈ =0otherwise                                      . 

when analyzing clean read-depth data, if the amplitude of the j-th cnv, aj, occurs discretely at several different values , the prior distribution p of aj can be modeled naturally by a multinomial distribution.

algorithm and implementation
parameter estimation by markov chain monte carlo simulation
analytically summarizing the posterior distribution p is difficult. for example, even though in theory the posterior expectation of an arbitrary function of θ, g, can be computed as

  e|d)=∫θgpdθ, 

the calculation is usually impracticable for two reasons. firstly, p is only known up to some multiplicative constant due to the proportionality form of equation . secondly, even if the exact form of p is known, given the number of parameters in θ , the high dimensional integral required in equation  is very difficult to be carried out in practice and soon becomes intractable as the number of parameters increases. however, markov chain monte carlo  provides an alternative whereby the posterior can be directly sampled to obtain sample estimates of the quantities of interest. thus using a random sequence of k draws θ, θ, ..., θ from p, e|d) can be approximated by simply taking the average of these draws. similar methods can be used to compute the posterior standard deviation ζθ^ or quantiles, probabilities that parameters take particular values, and other quantities of interest.

the gibbs sampling algorithm  <cit>  was implemented to sample from the target distribution {p: θ ∈ Θ ⊆ ℝ3n+1}. to do so, the gibbs sampler first constructs an aperiodic and irreducible markov chain whose stationary distribution is p in the state space Θ, and then draws a sequence of random samples from conditional distributions to characterize the joint target distribution. more precisely, it was implemented by  taking some initial values θ;  repeating for each t =  <dig>   <dig>  ..., t, where t is the preset number of iterations, generating θi from p| θ <dig>  ..., θi- <dig>  θi+ <dig>  ..., θ ||θ||,d,ℳ) for i =  <dig>   <dig>  ..., ||θ||;  continuing the previous step for t times after the estimated target distribution p^ converges.

to calculate the conditional probabilities of sj and wj required by the second step of the gibbs sampling stated above, all possible s ∈  and w ∈  are evaluated. given the normality assumption about the data, conjugate prior distributions of aj and σ <dig> can be used to simplify the calculation of their conditional probabilities. if the prior distribution of aj takes the conjugate from p ~ n, the conditional distribution of aj given other parameters, the data d, and the model ℳ is also a normal distribution as n/] τj+ x¯j

, 1/) where x¯j is the average log-ratios of probe intensities in the j-th cnv. given the conjugate prior distribution of σ <dig>  p ~ ℐnvmma, the conditional distribution of σ <dig> given other parameters, the data d, and the model ℳ is also an inverse gamma distribution, ℐnvgamma2/2).

model selection using bayes factor
model selection is required to determine n, the number of cnvs, as different n changes the model parameterization θ. suppose that the data d have arisen under one of the two models, {ℳ1: θ <dig> ∈ Θ1} and {ℳ2: θ <dig> ∈ Θ2}, according to a probability density p or p.

given prior probabilities p and p =  <dig> - p, the data produce posterior probabilities p and p = 1- p. from bayes' theorem, we obtain

 p=pppp+pp, 

so that

  pp=pp⋅pp. 

for a given model {ℳ: θ ∈ Θ}, p can be approximated by the sample harmonic mean likelihoods,

  p^hm=− <dig>  

based on k mcmc draws θ, θ, ..., θ from the posterior distribution p. the harmonic mean estimator is consistent since p^hm→p as k → + ∞. it may, however, have infinite variance across simulations. to solve this problem, newton and raftery  <cit>  proposed an alternative estimator,

  p^4=mδ/+∑i=1kp,ℳ)/mδ/p^4+∑i=1k− <dig>  

which does not display any of the instability of p^hm. we implemented p^ <dig> to calculate the bayes factor for model comparison

implementation
our method, including the markov chain monte carlo simulation and the model comparison, is currently implemented in r  <cit> . to use non-informative priors for our bayesian inference, we set τj =  <dig> and κj =  <dig>  which effectively makes the prior distribution of aj flat around  <dig>  we also assign  <dig> to both α and β for the inverse gamma distribution of σ <dig>  we tested various values of the hyperparameters , and the simulation results showed that the parameter inference was insensitive to the values assigned to these hyperparameters, which is expected given the large number of data points. a 500-iteration mcmc simulation of the posterior distribution  given a data set with m =  <dig> and n =  <dig> took  <dig> seconds of cpu time on a personal computer with one 1400-mhz x <dig> intel processor and  <dig> mb ram. to assess the convergence of the markov chain after  <dig> iterations, we started multiple chains from different values of θ. the simulations showed that after initial dozens of iterations all chains converged to the same solution. based on this observation, we concluded that  <dig> iterations were sufficient for bayesian inference in this case. we used the same convergence diagnostic for all inferences.

because of great computational intensity of the mcmc simulation, to process a large gcn data set, we use a 'divide and conquer' strategy. we first sort array/sequencing features from each chromosome according to their genomic locations and then group  <dig> consecutive features into subsets for parallel processing on a computer cluster.

RESULTS
simulated array-cgh data
we first used simulated array-cgh data sets to test our bayesian model and its implementation. to generate such synthetic data, we first specified values for the parameters in θ = {n, , σ2}, j =  <dig>   <dig>  ..., n, in which n and  define the artificial genomic cnv structure encoded as a step function and σ <dig> determines the overall noise level in the data. the simulated data were then generated by superimposing this predefined step function with random gaussian noise. typical simulated array data with one and multiple cnvs are shown in figures 2a and 3a respectively.

the simulated array data  plotted in figure 2a were generated with θ = {n =  <dig>  , σ <dig> =  <dig> }, which was to be estimated. taking n =  <dig>  we started the markov chain at some random θ = {, σ <dig> =  <dig> } and ran it for  <dig> iterations. the sampling results are shown in figure 2c-f. as the parameter trace plots  show, the markov chain quickly converged to stationarity after approximately ten iterations . to err on the side of caution, we discarded the samples from the first  <dig> iterations as the 'burn-in' samples and estimated the parameter values from the rest  <dig> samples, which gave θ^={,σ^2= <dig> } given n =  <dig> 

remarkably, all these samples have the very similar s <dig> and w <dig>  which are  <dig> and  <dig> respectively. because of this small variation in their estimation, the estimates of s <dig> and w <dig> from the data are of extremely high confidence. the distributions of a <dig> and σ in the  <dig> samples are approximately normal as n and n respectively. based on their normal distributions, we can easily calculate a bayesian credible interval for both a <dig> and σ. for example, a 95% bayesian credible interval for a <dig> is , which suggests that, after observing the data, there is a 95% chance that the average log-ratio of intensities in this cnv falls between  <dig>  and  <dig> .

we also simulated array-cgh data  with multiple cnvs  using θ = {n =  <dig>  , , , , σ <dig> =  <dig> }. to identify the cnvs encoded in this data set, first the model-specific parameters {, σ2}, j =  <dig>   <dig>  ..., n were estimated under different models with n =  <dig>   <dig>  ...,  <dig>  in figure 3a, the scatter plot of the multi-cnv array-cgh data are overlaid with the segmentation found by our algorithm using different models. the figure shows that the most prominent cnv was identified first when the number of cnvs, n, was set to  <dig> and less prominent cnvs were progressively identified as the model became more permissive . to select the most plausible model from which the observed data were generated, each of the models with n =  <dig>   <dig>   <dig>   <dig>   <dig> was then compared with the basal, null model . quantification of these comparisons by the logarithm of the bayes factor, which gives  <dig> ,  <dig> ,  <dig> ,  <dig> , and  <dig>  respectively, clearly indicates that the model with n =  <dig> is the best model among the ones tested . it is noteworthy that since the numbers aforementioned are the logarithms of the bayes factor the actual increase in the marginal evidence p between neighboring models is very substantial. for example, the increase in the marginal evidence from n =  <dig> to n =  <dig> is e <dig> - <dig>  = e <dig>  ≈  <dig>  ×  <dig> fold.

lai et al.  <cit>  examined the performance of  <dig> array-cgh data analysis methods: cghseg, quantreg, clac, glad, cbs, hmm, wavelet, lowess, charm, ga, and ace. to assess the performance of our algorithm in conjunction with these methods, we used the same simulated data as lai et al. used for the assessment in their study to calculate the true positive rates  and the false positive rates  as the threshold for determining a cnv is varied. see lai at al. for the definitions of tpr and fpr and the details of the simulated data sets. we calculated the receiver operation characteristic  curve of our algorithm using the most noisy  data set with the cnv width of  <dig> probes. this roc curve, together with the roc curves of other array-cgh methods based on the same data set, was plotted in figure  <dig>  these curves show that our bayesian algorithm is appreciably more sensitive than all other methods at low  false positive rates. we need to point out that the comparison was conducted in a fair manner, if not to the disadvantage of our method: all the results from lai et al. were used directly without modification and our method has no free parameters to tune.

glioblastoma multiforme array-cgh data
lai et al.  <cit>  compared  <dig> different array-cgh data analysis algorithms that are based on diverse statistical or computational techniques. in addition to testing those methods using simulated data, they also characterized their performance on chromosomal regions of interest in real data sets obtained from patients with glioblastoma multiforme   <cit> . these cdna microarray-based data sets were generated by cgh-profiling copy number alterations across  <dig>  mapped human cdna clones, in a series of  <dig> gliomas of varying histogenesis and tumor grade.

it was observed that the gbm data contain a mixture of larger cnv regions with low amplitude and smaller ones with high amplitude. these two types of array-cgh data are nicely represented by data sets gbm <dig> and gbm <dig> respectively . in sample gbm <dig>  a large region on chromosome  <dig> was lost, and the overall magnitude of this loss is very low due to the low penetrance of this genetic variation in tumor cells in this sample. in sample gbm <dig>  on the other hand, there are three high-amplitude small duplications. to evaluate our bayesian approach in a comparable way, we also used these two gbm data sets processed and utilized by lai et al. to test our method.

figures 5a and 5b show the array-cgh profiles of chromosomes  <dig> and  <dig> in glioblastoma multiforme samples gbm <dig> and gbm <dig>  respectively, overlaid with the segmentation found by our algorithm. as seen in figure 5a, our algorithm detected the single broad proximal deletion of part of chromosome  <dig> in gbm <dig>  spanning from the 59th to the 542nd probe with a log-ration intensity at - <dig>  ,   σ^2= <dig> } with corresponding standard deviations, ζθ^={, ζσ^2= <dig> }, for calculating each bayesian credible interval). the breakpoint e^ <dig> at the probe genomic index  <dig> was also identified by all the programs that detected this deletion in the test conducted by lai et al. the other breakpoint a^ <dig> at  <dig> was again found by clac and ace evaluated in the same test  <cit> . the small sample standard deviations in ζθ^ connote the reliability of the parameter estimation despite a rather low signal to noise ratio of the gbm <dig> data. our algorithm also detected all three high-amplitude amplifications of parts of chromosome  <dig> in gbm <dig> . even though there are only four probes separating the first two amplifications, our method still segmented them clearly. moreover, our method also pinpointed the six breakpoints of these three cnvs , which makes these predictions highly reliable.

β-globin high-density array-cgh data
one recent significant development in the microarray technology is the emergence of the tiling array technology, which can be used to cover large genomic regions or even an entire genome on one or several microarrays in an unbiased fashion by using oligonucleotides  uniformly sampled from presented genomic sequences. the current trend is to migrate from pcr-based arrays to tiling arrays for a much higher resolution and a comprehensive genomic coverage.

in a recent study  <cit> , in order to test the resolution limits of tiling arrays when they are used with cgh for cnv discovery, urban et al. designed microarrays that tile through  <dig> kb of the β-globin locus with overlapping isothermal oligonucleotides spaced  <dig> bp apart alone the tiling path. they compared the test dna from a patient with a known heterozygous deletion of  <dig> bp in the β-globin locus and the reference dna pooled from seven individuals without this chromosomal aberration. figure 5c shows the array-cgh profile of the β-globin locus of the patient overlaid with the segmentation ,  σ^2= <dig> }) found by our algorithm. this deletion in the β-globin locus was detected, and the estimate of its length, w^, corresponding to  <dig> bp in the genomic coordinate system, is highly accurate in comparison with the actual length of the deletion .

read-depth genome resequencing data
the genome of a utah resident with northern and western european ancestry from the ceph collection  has been sequenced by the  <dig> genomes project using both the  <dig> paired-end and the illumina shot-gun sequencing technologies, which produced long  sequence reads with low coverage  and short  ones with high coverage , respectively.

after using these two sequence sets to generate the 'known' genomic deletions in and the read-depth data from this individual, we apply our method to the read-depth data and compare the finding with the 'known' genomic deletions. despite a very low sequencing depth, we are able to use  <dig> reads to detect several large genomic deletions in this individual based on the gapped  alignment of some of these long reads. these deletions are taken as known, and we use a 2653-bp deletion on chromosome  <dig> from  <dig> , <dig> to  <dig> , <dig> to illustrate the application of our read-depth method. after mapping approximately  <dig> -billion 50-bp illumina reads to the human reference genome, we count the number of reads in a 200-bp non-overlapping sliding window to produce the read-depth data. figure 6a shows the read distribution profile based on the illumina short reads surrounding the 2653-bp deletion locus.

our method detected this deletion in the read-depth data and estimated its parameters to be θ^={,  σ^2= <dig> }. to investigate how the sequencing depth affects the estimation of the start and the end positions of a cnv, we simulate a series of sequencing depths by randomly sampling  different numbers of mapped illumina reads and then apply our method to the simulated data. the standard deviation in the estimates of the start and the end positions, s and e, reflects how well these two parameters can be estimated from the read-depth data. in figure 6b we plot the averaged standard deviation in the estimates of the s and e at different sequencing depths. it is clear as the sequencing depth decreases from the original depth  the estimates of the terminal positions become less accurate. in fact, when the coverage is below 1×, it becomes very difficult to find the deletion at all.

discussion and 
CONCLUSIONS
the metropolis-hastings and the gibbs sampling algorithms, two markov chain monte carlo simulation methods, have been widely used for bayesian inference. developed by metropolis et al.  <cit>  and subsequently generalized by hastings  <cit> , the metropolis-hastings algorithm generates, based on the current state and a pre-selected proposal density, candidate states that are accepted  stochastically with a certain acceptance probability but then retains the current value when rejection takes place. gibbs sampling  <cit>  draws a sequence of random samples from conditional distributions of unknown parameters to characterize their joint target distribution. in fact, the gibbs sampling can be regarded as a special case of the metropolis-hastings algorithm as the acceptance probability is always one--i.e., every proposal is automatically accepted.

for our bayesian analysis of genomic copy number data, we implemented both the random walk metropolis-hastings  and the gibbs sampling algorithms and observed that in this application gibbs sampling is much more suitable for parameter inference. rwmh worked well for one-cnv data. however, if the data contain two widely separated cnvs, it can only identify one of them but not both. to investigate this limitation, we plotted the landscape of the posterior probability distribution in a two-dimensional parameter space. a two-cnv data set d  with θ = {n =  <dig>  , , σ <dig> =  <dig> } was first simulated, and then the posterior probability was evaluated with various combinations of s <dig> and s <dig> while all other parameters were kept fixed at their true values.

the surface plot in figure 7a shows a global maximum peak located at s <dig> =  <dig> and s <dig> =  <dig> as expected and an overall very rugged posterior distribution 'terrain': the landscape is full of local maxima with, especially, two prominent 'ridges' of local maxima at s <dig> =  <dig> and s <dig> =  <dig>  respectively. it is clear from figures 7a and 7b that if the markov chain of rwmh gets to a local maximum on the ridge at s <dig> =  <dig> or s <dig> =  <dig> but fortuitously far from the global maximum, it will be trapped on the ridge and practically cannot reach the global peak if the random update interval is small . based on these observations, we chose the gibbs sampling algorithm for our bayesian analysis of the genomic copy number data as the gibbs sampler is well suitable to explore this 'ridged' terrain by using full conditionals to scan the landscape along ridges to find the global maximum.

as the roc curves in figure  <dig> show, our bayesian algorithm is the most sensitive method at low  false positive rates. this means that at a given low fpr our method can identify more true positive probes inside cnvs than other methods. when the fpr is higher, it is less sensitive than several methods, most of which find cnvs through data smoothing. however, this is hardly a disadvantage, as at high false positive rates the list of identified cnvs is awash with false positives, rendering the whole list practically unusable.

in addition to the improved sensitivity, our method also has several distinct advantages innate to its bayesian approach. the confidence on an estimated parameter value can be assessed through its bayesian credible interval. akin to a confidence interval but with an intuitive probabilistic interpretation, a bayesian credible interval is a range of values that contains the true parameter value with a certain probability. through stochastic simulation, it is straightforward to summarize the otherwise analytically intractable joint posterior distribution of the unknown parameters and compute both the best estimate and a corresponding bayesian credible interval for each parameter in the model. the availability of the intervals for sj, ej, and aj--the start and the end genomic locations and the copy number of each cnv--is unique to our bayesian method, and these credible intervals can be especially useful.

recent years have seen fast development of methodologies in different frameworks to detect cnvs in array-cgh data. for example, to detect cnv breakpoints, shah et al. used a modified hidden markov model  that is robust to outlier probes  <cit> , while rueda and d'az-uriarte used a nonhomogeneous hmm fitted via reversible jump mcmc  <cit> . pique-regi et al. used piecewise constant  vectors to represent genome copy numbers and sparse bayesian learning to detect cnv breakpoints  <cit> . other methods for segmenting array-cgh data have also been implemented, including using bayesian change point analysis  <cit> , a spatially correlated mixture model  <cit> , a bayes regression model  <cit> , and wavelet decomposition and thresholding  <cit> .

due to the computational intensiveness of its mcmc simulation, the method that we present here can be most advantageously used to refine cnvs detected by fast point-estimate methods. it could also be seen as a basic genomic copy number data analysis framework, amenable for several possible extensions. firstly, due to the nature of the genomic sequence duplications and deletions, the signal measurements of cnvs will aggregate to certain expected values. such information could be incorporated into the model for better signal detection from background noise. secondly, more complicated likelihood function, such as a truncated gaussian, could be used to handle outliers in genomic copy number data. thirdly, informative priors could be used for better cnv detection. the formation of cnvs in a genome is potentially affected by many local genomic features, such as conservation and repeat content on the sequence level. compared with the aforementioned methods for array-cgh data, our bayesian approach has the advantage to readily incorporate such sequence information through the prior distributions, as it treats the start and the width of cnvs as parameters and thus directly models the genomic cnv state. for this initial bayesian analysis of genomic copy number data, we used flat priors for both the cnv start site and width. however, instead of using such noninformative prior, we can assign a prior for the start site inversely proportional to the conservation level of the probe sequence.  for the width, we can assign the width distribution of known cnvs in the database as a prior. the incorporation of such knowledge through the priors does not need to be done only once: it can be sequential  as more relevant information becomes available. using such informative priors, our method can be seen as a framework that enables integration of genomic copy number data and the cnv-related biological knowledge.

authors' contributions
zdz conceived of the study, implemented the method, performed the analysis, and drafted the manuscript. mbg participated in the design of the study and helped to draft the manuscript. all authors read and approved the final manuscript.

