BACKGROUND
advances in biochemical technologies over the past decades have given rise to next generation sequencing platforms that quickly produce genomic data at much lower costs than ever before. such overwhelmingly large volumes of sequenced dna remain difficult to annotate. as a result, numerous computational methods for genome annotation have emerged, including machine learning and statistical analysis approaches that practically and efficiently analyze and interpret data. supervised machine learning algorithms typically perform well when large amounts of labeled data are available. in bioinformatics and many other data-rich disciplines, the process of labeling instances is costly; however, unlabeled instances are inexpensive and readily available. for a scenario in which the amount of labeled data is relatively small and the amount of unlabeled data is substantially larger, semi-supervised learning represents a cost-effective alternative to manual labeling.

because semi-supervised learning algorithms use both labeled and unlabeled instances in the training process, they can produce classifiers that achieve better performance than completely supervised learning algorithms that have only a small amount of labeled data available for training  <cit> . the principle behind semi-supervised learning is that intrinsic knowledge within unlabeled data can be lever-aged in order to strengthen the prediction capability of a supervised model that only uses labeled instances, thereby providing a potential advantage for semi-supervised learning. model parameters learned by a supervised classifier from a small amount of labeled data may be steered towards a more realistic distribution  by the unlabeled data.

unfortunately, unlabeled data can also drive the model parameters away from the true distribution if misclassification errors reinforce themselves. thus, in practice, semi-supervised learning does not always work as intended  <cit> . moreover, under incorrect assumptions, e.g., regarding the relationship between marginal and conditional distributions of data, semi-supervised learning models risk to perform worse than their supervised counterparts. given that for many prediction problems the assumptions made by learning algorithms cannot be easily verified without considerable domain knowledge  <cit>  or data exploration, semi-supervised learning is not always "safe" to use. advantageous utilization of the unlabeled data is problem-dependent, and more research is needed to identify algorithms that can be used to increase the effectiveness of semi-supervised learning  <cit> , in general, and for bioinformatics problems, in particular. at a high level, we aim to identify semi-supervised algorithms that can be used to learn effective classifiers for genome annotation tasks.

in this context, a specific challenge that we address is the "data imbalance" problem, which is prevalent in many domains, including bioinformatics. the data imbalance phenomenon arises when one of the classes to be predicted is underrepresented in the data because instances belonging to that class are rare  or hard to obtain. ironically, minority classes are typically the most important to learn, because they may be associated with special cases. in general, anomaly or novelty detection problems exhibit highly imbalanced distributions. specific applications outside the bioinformatics area include credit card fraud, cyber intrusions, medical diagnosis, face recognition, defect detection in error-prone software modules, etc. as established in the literature , the existence of a major unevenness between the prior class probabilities leads to impartial learning. as a result, classifiers that produce good classification results under normal circumstances  can be seriously compromised when faced with skewed distributions, as classifiers become strongly biased towards the majority class. in bioinformatics, problems such as promoter recognition, splice site detection, and protein classification are especially difficult because these problems naturally exhibit highly imbalanced distributions.

resampling datasets in order to reach balanced distributions is a common practice that sometimes improves classification performance, as the model encounters an equal number of instances from each class, thereby producing a more appropriate discriminative function as opposed to a function obtained from skewed distributions. however, it is not well understood what is the most appropriate balancing method. context-dependent conclusions are usually driven by empirical observations concerning both the classifier used and the imbalance degree. the most straightforward method is under-sampling, in which instances that belong to the majority class are eliminated until a balanced distribution is reached. as a consequence, information is lost, which is obviously not desirable, given the value of labeled instances, yet this is a good way to speed up the computation. moreover, studies have shown the effectiveness of under-sampling  <cit>  despite its obvious limitations. over-sampling is another popular resampling method in which instances of the minority class are generated artificially to counterbalance majority instances. these synthetic instances can potentially improve the classifier, as it gains access to more labeled data. the trade-off between longer computation times associated with larger datasets and better classification performance is usually worthwhile. however, with oversampling, classifiers are prone to overfitting, due to duplicate instances.

an algorithmic approach to handle imbalanced data distributions is based on ensembles of classifiers. limited amounts of labeled data naturally lead to "weaker" classifiers, but ensembles of "weak" classifiers tend to surpass the performance of any single constituent classifier. moreover, ensembles typically improve the prediction accuracy obtained from a single classifier by a factor that validates the effort and cost associated with learning multiple models. intuitively, "bagging" several classifiers leads to better overfitting control, since averaging the high variability of individual classifiers also averages the classifiers' overfitting. the first effective model ensemble surfaced in the mid 1990s  <cit> , under the name "bootstrap aggregating" , which is a meta-algorithm that performs model averaging over models trained on multiple subsets, i.e., bootstrap replicates of the training set. the predictions of the models are combined by voting  or averaging  in order to output a single final verdict that reflects the ensemble decision. originally applied to decision trees, bagging can be used with any classification or regression model and it is especially effective in conjunction with utilization of unstable nonlinear models . ensembles of classifiers that utilize bagging, boosting, and hybrid-approaches for imbalanced datasets in the supervised framework were reviewed by galar et al.  <cit> .

for a comprehensive survey of data resampling and algorithmic approaches to the imbalanced data problem in the supervised learning framework, the reader is referred to  <cit> . as opposed to supervised learning, fewer efforts have been aimed at the data imbalance problem in the semi-supervised learning framework, with some notable exceptions. in particular, in a previous study  <cit> , we experimented with data resampling and algorithmic solutions and observed that dynamically balancing the classifiers during the semi-supervised iterations of the algorithm is a useful solution that works better than under- and smote  over-sampling for splice site prediction in the context of single semi-supervised classifiers. we also found that ensembles usually tend to perform better than resampling techniques, except for extreme cases when the imbalance degree is 1-to- <dig>  in which case oversampling performs slightly better than the ensemble-based approach. in a subsequent study  <cit> , we empirically evaluated ensembles of self-training semi-supervised classifiers and found that maintaining diversity during the process of semi-supervised learning is an important requirement for the ensemble. in the current study, we experiment with both self-training and co-training, utilizing a different feature representation than the one we used in  <cit> , to accommodate co-training, which requires two views  of the data.

similar to our prior work, the current study is performed on the problem of predicting splice sites, a challenging but important task in genome annotation  <cit> . splice sites are located at the boundaries between exons and introns. at the 3' end of an intron, the "ag" dimer denotes an acceptor splice site; at the 5' end of the intron, the "gt" dimer denotes a donor splice site. other non-consensus splice sites exist, but they are not considered in this work. we formulate the task of predicting acceptor splice sites as a binary classification problem in which the positive class represents true acceptor splice sites and the negative class is comprised by decoy "ag" sites. we use five relatively large datasets from five organisms. the distribution of the data  is very skewed - approximately 1% of "ag" dimers are actually acceptor splice sites.

among others, sonnenburg et al.  <cit>  previously addressed the splice site prediction problem, in the supervised framework, using support vector machines  and specialized kernels. as opposed to prior work, in this work, our goal is to investigate ensemble-based semi-supervised learning as a potential solution for splice site prediction and to study the effects of imbalanced distributions on semi-supervised algorithms when labeled data is sparse. given the large datasets of our case study and the numerous models that needed to be trained to simulate different imbalanced degrees for different ensemble variants, we chose naïve bayes as the base classifier in co-training and self-training, because of its computation speed and to avoid tuning hyper-parameters . although theoretically, the i.i.d. assumption  does not hold for many problems , generative models such as naïve bayes can show superior performance to discriminative models such as svm, especially when small amounts of labeled data are available  <cit> .

the rest of this paper is organized as follows. we continue with a review of related work in the next section, where we also contrast our study with other similar studies. in methods, we describe our approaches, namely the semi-supervised learning ensembles based on self-training and co-training. the data section is dedicated to describing the datasets and the feature representation used with our classifiers. the experimental setting is described in experimental setup, starting with the research questions that motivated the study and continuing with details of the evaluation procedure. we discuss the performance of our approaches in results, and finally, in the conclusion section, we conclude the study and suggest directions for future work.

related work
genome annotation is an ample task that requires machine learning and statistical methods to assist experimental approaches, especially given the large amount of genomic data being generated at unprecedented rates. supervised machine learning approaches have been widely used in bioinformatics for many tasks, including splice site prediction  <cit> . for example, human splice site detection was explored in  <cit>  using svm classifiers with a gaussian kernel, and in  <cit>  using a combination of markov models and svm classifiers with polynomial kernels. the work in  <cit>  proposed a markov model approach for splice site detection in a human dataset with imbalance degrees of 1-to- <dig> for acceptors and 1-to- <dig> for donors.

semi-supervised learning has generally been used in bioinformatics to solve protein classification problems  <cit> , with a few notable exceptions focused on dna classification  <cit> . a small number of studies  <cit>  have explored the data imbalance problem in the semi-supervised context and proposed effective solutions, but the imbalance degrees were moderate. for example, in  <cit> , the authors addressed the problem of molecule activity prediction and experimented with transductive svm classifiers on datasets with relatively small sizes , exhibiting imbalance degrees no higher than 1-to- <dig> 

as opposed to that, we focus on datasets with higher degrees of imbalance  and study the behavior of semi-supervised learning algorithms when the available labeled data is less than 1% of the total amount of training data. in general, such a small amount of labeled data is expected to lead to weak classifiers, but an ensemble of classifiers could help overcome this shortcoming to some extent. galar et al.  <cit>  showed that, in supervised frameworks, ensembles perform better than single learners trained on resampled data. lusa and blagus  <cit>  found that balancing the class prior in the training set via "multiple down-sizing", in other words, training an ensemble of subclassifiers on balanced subsets, is particularly useful for high-dimensional representations. they showed this using a simulated set and a genuine, publicly available dataset from a breast cancer gene expression microarray study. another study by li et al.  <cit>  also concluded that an ensemble of co-training classifiers is suitable for imbalanced datasets.

our objective in this study was to adapt existing semi-supervised learning ensembles to datasets with high degrees of imbalance. towards this goal, we used the approach from  <cit>  as inspiration for two of the methods presented in this work. in  <cit> , the authors proposed that, as the co-training sub-classifiers iterate, the balanced labeled subsets are augmented with the same instances, specifically, the most confidently labeled positive instances and the most confidently labeled negative instances. in our previous work on the problem of splice site prediction  <cit> , we found that adding different instances to each self-training subsets leads to improved prediction because diversity is maintained. however, it was not clear what was the best way to manipulate the original distribution to ensure the largest diversity among ensemble members. motivated by the results of our dynamic balancing technique, where only positive instances are added to the training set during the self-training iterations  <cit> , and also by our preliminary results on ensemble approaches based on self-training classifiers  <cit> , in the current study, we further analyze various combinations of ensembles and dynamic balancing, with focus on how the augmentation of labeled data should be managed during the semi-supervised iterations. we also experiment with co-training, in addition to self-training, and investigate how ensembles of self-training and co-training naïve bayes classifiers behave in the semi-supervised framework when dealing with various imbalance ratios.

a study from wei and dunbrack  <cit>  that explored the effects of various distributions on supervised learning was centered around classification of human missense mutations as deleterious or neutral. by systematically varying the ratio of deleterious to neutral mutations in the training set, the authors concluded that balancing the training dataset improves the performance of svm as evaluated by several accuracy measures, even when the distribution of the data is just mildly imbalanced. the study in  <cit>  was performed under the assumption that the real distribution of deleterious versus neutral mutations is unknown. in the datasets used in our work  <cit> , the proportion of true splice sites was assumed to be approximately 1% of the total number of occurrences of the "ag" dimer throughout the genome, and thus this was the highest imbalance degree that we experimented with . however, we varied the ratio of splice site to non-splice site "ag"s from 1-to- <dig> to 1-to- <dig>  to perform a systematic study of the performance obtained using ensemble-based semi-supervised approaches as a function of the imbalance ratio.

methods
this section describes the algorithms studied. as we focus on ensemble-based semi-supervised learning from imbalanced class distributions, specifically ensembles of self-training and co-training classifiers, we will first provide background on self-training and co-training, and also on ensemble learning. then, we will describe the supervised ensemble approach used as a baseline in our evaluation, and finally, our proposed self-training and co-training ensemble variants.

self-training
self-training, also known as self-teaching or bootstrapping, is an iterative meta-algorithm that can be wrapped around any base classifier. yarowsky  <cit>  originally introduced self-training and applied it to a natural language processing problem, namely word-sense disambiguation. the first step in self-training is to build a classifier using the labeled data. then, the labeled dataset is augmented with the most confidently predicted instances from the unlabeled pool, and the model is rebuilt. the process is repeated until a criterion is met, e.g., until the unlabeled dataset has been fully classified or a fixed number of iterations has been reached. in our work, we classify a sub-sample of unlabeled data at each iteration  in order to increase computation speed. the most confidently classified instances are assigned the predicted class and used to retrain the model. the remaining instances, classified with less confidence, are discarded. the algorithm iterates until the unlabeled dataset has been exhaustively sampled.

co-training
blum and mitchell  <cit>  introduced co-training, also an iterative meta-algorithm, to solve the problem of identifying course pages among other academic web pages. similar to self-training, co-training is applicable to any base classifier. unlike self-training, which is a single view algorithm, co-training requires two independent and sufficient views  of the same data in order to learn two classifiers. at each iteration, both classifiers label the unlabeled instances and the labeled training data of one classifier is augmented with the most confidently labeled instances predicted by the other classifier. similar to self-training, in our work we classify only a sub-sample of unlabeled data at each iteration. instances from the sub-sample classified with small confidence are discarded. the algorithm iterates until the unlabeled dataset has been exhaustively sampled.

ensembles
ensemble learning exploits the idea that combinations of weak learners can lead to better performance. moreover, it is known that diversity among subclassifiers is an important constraint for the success of ensemble learning  <cit> . however, learning naïve bayes classifiers from bootstrap replicates will not always lead to sufficiently "diverse" models, especially for problems with highly imbalanced distributions. in order to ensure sufficient variance between the original training data subsets of our highly imbalanced datasets, we used a technique initially recommended by liu et al.  <cit> , who proposed training each subclassifier of the ensemble on a balanced subset of the data, providing subclassifiers with the opportunity to learn each class equally, while the ensemble continues to reflect the original class distribution. an implementation of this technique by li et al.  <cit>  proved to be successful for the problem of sentiment classification, and was used as inspiration in our work.

supervised lower bound
generally, supervised models trained only on the available labeled data are used as baselines for semi-supervised algorithms. thus, the hypothesis that unlabeled data helps is verified against supervised models that entirely ignore unlabeled instances. because our focus is on ensemble methods and ensembles of classifiers typically outperform single classifiers, the lower bound for our approaches is an ensemble of supervised classifiers. specifically, we train ensembles of naïve bayes classifiers using resampled balanced subsets and use their averaged predictions to classify the test instances. this approach is referred to as the lower bound ensemble .

ensembles inspired by the original approach: cteo and steo
in  <cit> , co-training classifiers were augmented with the topmost confidently labeled positive and negative instances, as found by classifiers trained on balanced labeled subsets. the authors set the number of iterations at  <dig>  and classified all unlabeled instances at each iteration. moreover, the two views of the co-training classifiers were created at each iteration, using "dynamic subspace generation" , in order to ensure diverse subclassifiers.

however, this exact approach did not produce satisfactory results in our case, so we modified the algorithm from  <cit>  in order to better accommodate our problem. we named the resulting approach co-training ensemble inspired by the original approach . we also experimented with a variant where co-training was replaced with self-training, and named this variant self-training ensemble inspired by the original approach . the pseudocode for both cteo and steo variants is illustrated in algorithm  <dig>  as can be seen, steps 7- <dig> are described for co-training  and self-training , separately.

the first modification we made to the original ensemble-based approach, for both self-training and co-training variants, is that we kept the features fixed, i.e., used "static" instead of "dynamic subspace generation." for co-training, we used a nucleotide/position representation as one view, and a 3-nucleotide/position representation as the second view, under the assumption that each view is sufficient to make accurate predictions, and the views are  independent given the class.

the second modification we made is that we did not classify all unlabeled instances at each iteration; instead, we classified only a fixed subsample of the unlabeled data, as proposed in the classical co-training algorithm  <cit> . this alteration speeds up the computation process. the last modification that we made is that once a subsample was labeled and the top most confidently labeled instances were selected to augment the originally labeled dataset, we simply discard the rest of the subsample, thereby differing from the classical co-training approach  <cit>  and from the original co-training ensemble approach  <cit> . this change also leads to faster computation times and, based on our experimentation, reduces the risk of adding mistakenly labeled instances to the labeled set in subsequent iterations. furthermore, the last two adjustments lead to a fixed number of semi-supervised iterations, i.e., as the algorithm ends when the unlabeled data pool is exhausted. we use a subsample size that is dependent on the dataset size, and selected such that the algorithm iterates approximately the same number of times  for each set of experiments, for a certain imbalance degree. after the iterations terminate, the ensemble is used to classify the test set by averaging the predictions of the constituent subclassifiers.

an important observation regarding step  <dig> in algorithm  <dig> is that, in the case of co-training, when the two classifiers based on view <dig> and view <dig>  respectively, make their predictions, an instance is added to the pseudolabeled set p only if  no conflict exists between the classifiers, i.e., both classifiers agree on the label, and  one classifier predicts the label with high confidence, while the other predicts the same label with low confidence. these conditions ensure that the two views inform each other of their best predictions, thereby enhancing each other's learning.

algorithm  <dig> ensembles inspired by the original approach  <cit>  - cteo/steo

1: given: a training set comprised of labeled and unlabeled data d = , |dl| ≪ |du|

2: create u by picking s random instances from du and update du = du - u , s = sample size

3: generate n balanced subsets from dl : dl <dig>  . . ., dln

4: repeat

5:    initialize p = ∅

6:    for i =  <dig> to n do

7:       ct: train subclassifiers ci <dig> on view <dig> and ci <dig> on view <dig> of balanced subset dli

        st: train subclassifier ci on combined views of balanced subset dli

8:       ct: classify instances in u using the classifiers ci <dig> and ci2

        st: classify instances in u using subclassifier ci

9:       ct: use ci <dig> and ci <dig> to select  <dig> positive and  <dig> negative instances and add them to p

        st: use ci to select  <dig> positive and  <dig> negative instances, and add them to p

10:    end for

11:    augment each balanced subset with the instances from p

12:    discard remaining unused instances from u

13:    create a new unlabeled sample u and update du = du - u

14: until u is empty 

as mentioned above, steo differs from the co-training based ensemble, cteo, at steps 7- <dig> in algorithm 1: instead of using two subclassifiers trained on two different views, only one classifier is built using all features , and then this classifier is used to select the best two positive predictions and the best two negative predictions. because each subclassifier in cteo contributes one positive and one negative instance, after one iteration, the set p of pseudo-labeled instances contains 2n positive instances and 2n negative instances. therefore, in steo, we add the top two positives and top two negatives as predicted by the same subclassifier ci in order to maintain an augmentation rate identical to the augmentation rate in cteo. after the semi-supervised iterations terminate, the ensemble is used to predict the labels of the test set. the predictions of every subclassifier in the ensemble on a test instance are combined via averaging, and the resulting probabilities represent the final class distribution of the instance.

ensembles using dynamic balancing with positive: step and ctep
the following two approaches use the dynamic balancing technique proposed in  <cit> , found to be successful for the classical self-training algorithm when the dataset exhibits imbalanced distributions. the dynamic balancing occurs during the semi-supervised iterations of the algorithm and uses only the instances that the classifier  predicted as positive to augment the originally labeled set. in the ensemble context, subclassifiers are used to select the most confidently predicted positive instances. these variants are named co-training ensemble with positive  and self-training ensemble with positive , and illustrated in algorithm  <dig>  as before, the co-training and self-training variants differ at steps 7- <dig>  for ctep, during step  <dig>  the instance classified as positive with topmost confidence in one view and low confidence in the second view is added to p, and vice-versa. for step, the two most confidently labeled positive instances are added to p, such that the augmentation rate is identical to that from ctep.

algorithm  <dig> ensembles using dynamic balancing with positive - step/ctep

1: given: a training set comprised of labeled and unlabeled data d = , |dl| ≪ |du|

2: create u by picking s random instances from du and update du = du - u , s = sample size

3: generate n balanced subsets from dl : dl <dig>  . . ., dln

4: repeat

5:     initialize p = ∅

6:     for i =  <dig> to n do

7:         ct: train subclassifiers ci <dig> on view <dig> and ci <dig> on view <dig> of balanced subset dli

            st: train subclassifier ci on combined views of balanced subset dli

8:         ct: classify instances in u using subclassifiers ci <dig> and ci2

            st: classify instances in u using subclassifier ci

9:         ct: use ci <dig> and ci <dig> to select  <dig> positive instances and add them to p

            st: use ci to select  <dig> positive instances and add them to p

10:     end for

11:     augment each balanced subset with the instances from p

12:     discard remaining unused instances from u

13:     create a new unlabeled sample u and update du = du - u

14: until u is empty 

ensembles that distribute the newly labeled instances: cteod and steod
our next semi-supervised ensemble variants are based on cteo and steo, respectively, and distribute the most confidently labeled instances among the classifiers in the ensemble. they are referred to as co-training ensemble original distributed  and self-training ensemble original distributed , and shown in algorithm  <dig>  in cteod and steod, as opposed to cteo and steo, instances are distributed such that each balanced subset receives two unique instances, one positive and one negative, from each view, instead of adding all instances from p to every balanced subset. the idea that motivated this change was that different instance distributions would ensure a certain level of diversity for the constituent classifiers of the ensemble. in algorithm  <dig>  the co-training and self-training variants differ at steps 6- <dig>  as can be seen, the main difference compared to cteo and steo is at step  <dig>  where classifier ci <dig> trained on view <dig> is augmented with the top positive and top negative instances as predicted by classifier ci <dig> trained on view <dig>  and vice-versa. therefore, each balanced subset is augmented with two positive instances and two negative instances, and the ensemble better conserves its initial diversity.

algorithm  <dig> ensembles that distribute newly labeled instances - cteod/steod

1: given: a training set comprised of labeled and unlabeled data d = , |dl| ≪ |du|

2: create u by picking s random instances from du and update du = du - u , s = sample size

3: generate n balanced subsets from dl : dl <dig>  . . ., dln

4: repeat

5:     for i =  <dig> to n do

6:         ct: train subclassifiers ci <dig> on view <dig> and ci <dig> on view <dig> of balanced subset dli

            st: train subclassifier ci on combined views of balanced subset dli

7:         ct: classify instances in u using subclassifiers ci <dig> and ci2

            st: classify instances in u using subclassifier ci

8:         ct: use ci <dig> and ci <dig> to select  <dig> positive instances and  <dig> negative instances

            st: use ci to select  <dig> positive instances and  <dig> negative instances

9:     augment current balanced subset, dli, with selected positive and negative instances

10:     end for

11:     discard remaining unused instances from u

12:     create a new unlabeled sample u and update du = du - u

13: until u is empty 

ensembles that distribute only positive instances - ctepd and stepd
our last semi-supervised ensemble variants are based on ctep and step. we again use the dynamic balancing technique from  <cit>  that adds only positive instances in the semi-supervised iterations. in addition, instances are distributed among the balanced labeled subsets, such that diversity is maintained and the subclassifiers are trained on diverse enough instance subsets, thus increasing the diversity of the constituent ensemble classifiers. the resulting variants are named co-training ensemble with positive distributed  and self-training ensemble with positive distributed , and shown in algorithm  <dig>  the co-training and self-training variants differ at steps 6- <dig>  overall, at each iteration, 2n unique positive instances augment the ensemble in which n is the imbalance degree since two instances originated from each co-training subclassifier. more specifically, each of the n subclassifier receives two positive instances, different from the instances received by the other subclassifiers.

data and feature representation
for our empirical evaluation, we used five imbalanced and relatively large datasets, originally published in  <cit>  and used for a domain adaptation study. the datasets belong to five organisms, c. elegans, which contains approximately 120k instances, and c. remanei, p. pacificus, d. melanogaster, and a. thaliana, which contain approximately 160k instances each. in each of these datasets, the true acceptor splice sites represent 1% of the total number of instances, hence the datasets exhibit a 1-to- <dig> imbalance ratio. the class label of each instance is either positive to indicate a true acceptor splice site, or negative to indicate a decoy splice site.

algorithm  <dig> ensembles that distribute only positive instances - ctepd/stepd

1: given: a training set comprised of labeled and unlabeled data d = , |dl| ≪ |du|

2: create u by picking s random instances from du and update du = du - u , s = sample size

3: generate n balanced subsets from dl : dl <dig>  . . ., dln

4: repeat

5:     for i =  <dig> to n do

6:         ct: train subclassifiers ci <dig> on view <dig> and ci <dig> on view <dig> of balanced subset dli

            st: train subclassifier ci on combined views of balanced subset dli

7:         ct: classify instances in u using subclassifiers ci <dig> and ci2

            st: classify instances in u using subclassifier ci

8:         ct: use ci <dig> and ci <dig> to select  <dig> positive instances and add them to p

            st: use ci to select  <dig> positive instances and add them to p

9:         augment the current balanced subset with positive and negative instances

10:     end for

11:     discard remaining unused instances from u

12:     create a new unlabeled sample u and update du = du - u

13: until u is empty 

in our previous work  <cit> , we used 141-dimensional feature vectors to represent instances, x=x <dig> x <dig> ...,xn∈ℝn . each dimension corresponds to a position in the original sequences, and takes as values one of the four nucleotides {a, c, g, t }, as shown in figure  <dig>  specifically, feature xi indicates the nucleotide found at the corresponding position i. in the current work, because the co-training algorithm requires two views of the data, we use the nucleotide/position representation as the first view and the 3-nucleotide/position representation from  <cit>  as the second view. as the name suggests, 3-nucleotides are sequences of length  <dig> . intuitively, 3-nucleotides can capture more context information, as compared to single nucleotides. the 3-nucleotide/position representation, thus, captures additional correlations between nucleotides, while maintaining a low number of features , thereby making the two views comparable. given that nucleotide/position and 3-nucleotide/position features have shown to be effective in a domain adaptation scenario  <cit> , we hypothesize that semi-supervised learning could also benefit from these feature representations. for self-training, we used the two views together and trained the classifiers on the complete set of features.

 <dig> experimental setup
 <dig>  research questions
the experiments were designed to answer the following research questions:

 <dig> which ensembles are more affected by imbalanced distributions, supervised ensembles or semi-supervised ensembles?

 <dig> how does the performance of the approaches vary with the imbalance degree?

 <dig> what is the best strategy for utilizing newly labeled instances when using ensembles of semi-supervised classifiers trained on highly imbalanced data?

the five datasets used in this study were labeled, and therefore we were able to create, via resampling, various data subsets with various imbalance degrees , in order to observe the algorithms' performance with respect to the imbalance degree. for example, in the original d. melanogaster dataset, with the imbalance degree of 1-to- <dig>  there are  <dig>  instances,  <dig>  positives and  <dig>  negatives. in order to create the dataset for each experiment, we kept the positive instances and resampled at random n number of negative instances to obtain a new dataset with an imbalance degree of 1-to-n. for example, in the 1-to- <dig> experimental dataset for d. melanogaster, there are  <dig>  instances,  <dig>  positives and  <dig>  negatives. the rest of the datasets, corresponding to higher imbalance degrees, were built incrementally so that the dataset with the imbalance degree of 1-to- <dig> contains all the instances from the 1-to- <dig> dataset, and also contains additional negative instances to reach the desired imbalance.

as can be seen, for each experiment, the number of instances varies, and in the semi-supervised iterations, we used a sample size proportional to the dataset size, such that the experiments iterate roughly the same number of times.

because classifiers are highly susceptible to data variation and prone to sampling bias, we evaluated the models using 10-fold cross validation in which nine folds were used to train the model and the tenth fold was used for testing. data comprising the nine training folds is further divided into labeled and unlabeled. we randomly pick labeled instances such that the ratio of positive to negative is maintained and the total number of instances represents no more than 1%.

evaluation
because of the highly skewed distributions of the datasets, in order to objectively measure the predictive ability of our approaches, we compared their performance in terms of the area under the precision-recall curve , which is a more appropriate assessment measure than the area under the receiver-operating curve   <cit> . in order to evaluate the results, we averaged auprc values for the minority  class across the ten folds for each organism. while the trends are generally maintained for individual organisms, we report averages of auprc values over the five organisms, for easier interpretation. we performed two-tailed paired t-tests, as opposed to one-tailed t-tests, to identify statistically significant differences in either direction, on all semi-supervised algorithms for all variations of imbalance degrees. the test determines if the difference between a semi-supervised ensemble algorithm and its corresponding supervised ensemble baseline  is statistically significant  <cit> .

RESULTS
our experimental results are compiled in table  <dig>  the first column represents the imbalance degree of the experiment, which is varied from 1-to- <dig> to 1-to- <dig>  by randomly discarding negative  instances. the second column, lbe, shows the results of the supervised lower bound, which is also an ensemble, consisting of supervised classifiers. lbe is used as the baseline against which to compare the semi-supervised approaches. from the third column onwards, each method is presented for co-training and self-training. the results are discussed by addressing the research questions. values marked with bold font represent performances of the semi-supervised experiments that outperform the supervised lower bound. the starred  values denote experiments whose variation in comparison to the lower bound was found to be statistically significant by the paired t-test in all five organisms. the values marked with a plus  indicate experiments that the paired t-test found to be statistically significant in four out of five organisms. the values marked with a diamond  indicate experiments that the paired t-test found to be statistically significant in three out of five organisms.

imbal.
the values represent averages of auprc values for the positive class over the five organisms when the class imbalance degree varies from 1-to- <dig> to 1-to- <dig> and the amount of labeled instances represents less than 1% of the training data. lbe is the ensemble-based supervised lower bound. cteo and steo are the co-training-based and self-training-based ensembles inspired by the original approach in  <cit> . ctep and step are the co-training and self-training based ensembles that use the "dynamic balancing" approach introduced in  <cit> , in which only positive instances are used in semi-supervised iterations to augment the originally labeled training data. cteod and steod add positive and negative instances but distribute them among all subclassifiers, such that the balance and diversity of each subclassifier's labeled subset is maintained. ctepd and stepd use "dynamic balancing" but also distribute instances among all subclassifiers. the bold font denotes the semi-supervised experiments that outperform the lower bound. the starred  values denote experiments whose variation in comparison to the lower bound was found to be statistically significant by the paired t-test in all five organisms. the values marked with a plus  indicate experiments that the paired t-test found to be statistically significant in four out of five organisms. the values marked with a diamond  indicate experiments that the paired t-test found to be statistically significant in three out of five organisms.

 <dig> which ensembles are more affected by imbalanced distributions, supervised ensembles or semi-supervised ensembles? the supervised baseline remains somewhat constant irrespective of the imbalance degree, showing that additional labeled data can help alleviate problems caused by extreme cases of imbalance. note that experiments with milder degrees of imbalance contain less instances than experiments with higher degrees of imbalance, given the way we constructed our datasets. when the imbalance degree is the highest, 1-to- <dig>  we used the entire dataset. compared to supervised learning, semi-supervised learning ensembles show a slow decrease in performance as the imbalance degrees become more prominent, most probably due to the fact that additional unlabeled data is more difficult to label correctly.

 <dig> how does the performance of the approaches vary with the imbalance degree? as can be seen from the table, for lower degrees of imbalance , semi-supervised ensembles are considerably surpassing the supervised baselines. as the experiments become increasingly difficult , some semi-supervised ensembles deteriorate as a result of unlabeled data being incorrectly classified with high confidence, and they are surpassed by the supervised baselines.

in the original study  <cit>  that inspired our cteo and steo variants, the ensemble approach was used to predict the sentiment polarity of amazon reviews with imbalance degrees ranging between 1-to- <dig> and 1-to- <dig>  and proved to be superior to supervised baselines. our variants, cteo and steo, also produced good results for experiments with relatively low imbalance degrees, 1-to- <dig> and 1-to- <dig>  from 1-to- <dig> onwards, however, the cteo and steo semi-supervised ensembles performed worse than their supervised baselines, but, surprisingly, the self-training ensembles more effectively utilized the unlabeled data as compared to the co-training ensembles. for approaches that employ the "dynamic balancing" technique  <cit>  in which only positive instances are used, the ensemble based on co-training ctep leveraged the unlabeled data and surpassed the supervised counterpart for experiments with up to 1-to- <dig> imbalance degree, after which point no discernible difference was observed between ctep and the baseline. the ensemble based on self-training, step, is more sensitive and was deteriorated by the unlabeled data beginning with experiment 1-to- <dig>  the "pseudo" positive instances could have been misclassified, thereby misleading the classifiers, which all use the same newly labeled positive instances. in general, the ensembles that do not distribute the instances among their subclassifiers deteriorate and fall below the baseline for moderate and high degrees of imbalance. variants of the algorithms where instances are distributed tend to outperform the other approaches. when both positive and negative instances are used to augment the labeled data, cteod and steod outperformed the not-distributed versions cteo and steo. the self-training based approach steod still falls below the supervised baseline for experiments over 1-to- <dig>  but the co-training based approach cteod is surpassing the baseline for all experiments. the variants ctepd and stepd, which add only positive instances and distribute them, surpassed the baseline for all experiments. no significant difference in performance between cteod and ctepd was observed, but stepd out-performed steod and surpassed the baseline in all experiments. thus, the "dynamic" balancing approach proved to be more useful for the self-training based ensemble.

 <dig> what is the best strategy for utilizing newly labeled instances when using ensembles of semi-supervised classifiers trained on highly imbalanced data? one important observation that can be made based on our results is that the distribution of the newly labeled instances among subclassifiers in order to ensure subclassifier diversity is a useful approach for semi-supervised ensembles. variants that distribute the newly labeled instances  achieved overall better performance than the classifiers that receive all the newly labeled instances . therefore, the conclusion is that diversity in this case is more useful than the addition of substantially more "pseudo"  labeled instances during the semi-supervised iterations.

our results for the paired t-test showed no particular consistency, specifically some experiments and results were statistically significant and others were not.

CONCLUSIONS
in this work, we proposed and studied several ensemble-based variants of two popular semi-supervised learning algorithms, self-training and co-training, and tested their performance on the task of predicting splice sites. the task was formulated as a binary classification problem and the models' performance was tested on five large acceptor splice site datasets from five organisms. we adapted the ensembles to address the highly imbalanced datasets of our case study, and we used various approaches to augment the labeled data during the semi-supervised iterations. our results showed that one important constraint of any ensemble  is to maintain diversity of the ensemble's subclassifiers, by augmenting the labeled subsets of subclassifiers with unique newly labeled instances. maintaining the ensemble diversity by adding less but unique instances to each sub-classifier is a better approach than adding the same  instances to all subclassifiers.

in order to address highly skewed distributions, we found that dynamically balancing of ensembles by utilizing only positive instances during semi-supervised iterations to augment the labeled data and distributing them among constituent subclassifiers is a useful technique that benefits both types of ensembles, but especially the self-training-based approaches. for co-training-based approaches, whether instances from both classes are added  or just positives , the performance variations are negligible. both approaches ctepd and cteod surpass the other semi-supervised ensembles studied.

in general, our results show that ensembles based on self-training are surpassed by the ensembles based on co-training, a trend that has been reported many times in the literature for single classifiers, e.g., in the prediction of alternatively spliced exons  <cit> , or text classification  <cit> .

as part of future work, we consider exploring other base learners  for self-training and co-training algorithms. given that aggregated stacking produced the best results for protein function prediction and genetic interactions prediction in  <cit> , it would be interesting to explore meta-learning and ensemble selection for the splice site prediction problem. transductive approaches demonstrated great potential for protein classification from imbalanced datasets  <cit> , and svm has previously been shown to successfully identify splice sites  <cit> . therefore, the behavior of svm in a transductive context is of interest in relation to splice site prediction.

competing interests
the authors declare that they have no competing interests.

authors' contributions
a.s. and d.c. designed the study. a.s. carried out the computational aspect of the analysis. all authors participated in the writing of the manuscript; all authors read and approved the final manuscript.

