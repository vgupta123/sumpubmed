BACKGROUND
the number of published molecular biology and genomics research articles has been increasing at a fast rate. advancements in computational methods expediting the predictions of thousands of genes have generated high volumes of biological data. in addition, with the advent of microarray technology, it is now possible to observe the expression profiles for thousands of genes simultaneously. consequently, introduction of all these technologies has resulted in remarkable increases in the produced and published data.

currently, biological knowledge recorded in textual documents is not readily available for computerized analysis. and, the current practice of manual curation of text documents requires enormous human resources. hence, there is a need for automated computational tools to extract useful information from textual data.

the computationally extracted knowledge needs to be transformed into a form that can both be analyzed by computers and is readable by humans. to this end, different fields have developed various ontologies in an effort to define a standard vocabulary of each field. in the context of genomics, gene ontology   <cit>  is proposed, continuously maintained, and used as a standardized vocabulary to express the traits of genomic entities, i.e., genes and proteins. go consists of three subontologies, namely molecular function, biological process and cellular component, and contains around  <dig>  concepts which are organized in a hierarchy.

presently, go annotations are either manually curated from the literature or computationally created . most of the current computational annotations are not reliable as they are mostly based on sequence homology, and high sequence similarity does not necessarily indicate go functional coupling. the most reliable literature-based go annotations of genes and gene products are created by biologists manually reading related papers and determining the proper go concepts to be assigned to the corresponding genes. usually, each annotation is accompanied by an article  which is known as an evidence article. an evidence article for an annotation usually discusses or refers to a specific gene trait that leads to the corresponding annotation. for a go concept g, the evidence article set of g contains all the articles that are referenced as evidence articles for the existing annotations of genes with g.

in this work, we focus on information extraction from biomedical publications in terms of go concept annotations. we present a gene annotation system, called geann, that allows for

• automated extraction of knowledge about various traits of genomic entities from unstructured textual data; and

• annotating genes and proteins with appropriate concepts from go, based on the extracted knowledge.

geann utilizes the existing go concept evidence articles to construct textual extraction patterns for each go concept. the extraction patterns are flexible in that geann employs semantic matching of phrases by utilizing wordnet  <cit> . wordnet is a repository of hierarchically organized english words that are related to other words via manually created relations like hyponyms, meronyms, and so on.

the extracted pattern set is further enriched by employing pattern crosswalks which involves the creation of new patterns via combining patterns with overlapping components into larger patterns. geann then searches pubmed publication abstracts for matches to the patterns of genomic entities, and uses the located matches to annotate genomic entities with go concepts.

in this article, we evaluate geann's annotation accuracy over  <dig> go concepts, where geann has reached to 78% precision at 61% recall on the average. geann is being developed as part of pathcase  <cit>  genomic pathways database, a web-accessible bioinformatics tool for storing, visualizing and querying pathways and the associated genomic entities that take role in pathways. geann will be integrated into the pathcase system to provide users with newly discovered annotations and the corresponding pubmed articles leading to those annotations. in general, geann is used in two distinct ways:

 expedite and automate the annotation of genomic entities by go concepts with evidence extracted from scientific articles, and

 enrich existing annotations with additional supporting evidences from the literature.

geann works at the phrase and sentence level over all pubmed abstracts. in comparison, other approaches  <cit>  annotate genes over manually-assigned reference articles of a gene into go concepts. reference article set of a gene consists of articles that discuss various properties of the gene, and is selected by biologists independent of go. note that only well-known genes have a reference article set while majority of genes are not associated with a reference article set, restricting the applicability of other techniques. unlike other systems  <cit> , geann does not require the existence of a reference article set associated with each gene. moreover, it can also provide annotation evidences at a finer phrase-level granularity rather than at the document-level. and, yet it achieves a better or comparable performance in comparison with the other systems.

there are many studies in text mining  <cit> , gene annotation mining <cit> , and question answering  <cit>  literature that can be considered as related .

in our experimental evaluation, we use scientific articles from pubmed database  <cit>  to train and evaluate the performance of geann. pubmed is a digital library of over  <dig> million articles containing article titles and abstracts, and provides links to full texts of articles for some of the entries. note that our approach is directly extendible to full-text of publications. however, most of the full text article access links require subscriptions to publishers' web site, hence, are not readily available for public use. thus, we have designed and tested geann to work on publicly available article abstracts .

a preliminary version  <cit>  of this study was presented at pacific symposium on biocomputing . in this article, we extend the conference version with new content and much more rigorous experimentation on a larger data set. more specifically, with the goal of independent reproducibility, the content has been completely revamped with algorithm sketches and additional explanations at each section. we discuss and evaluate two methods to enhance the recall of the pattern-based approach to provide alternative options for applications that put higher emphasis on getting higher recall values. pattern scoring has been completely revised, and a more intuitive statistical enrichment-based scoring scheme has been incorporated. in addition, we study the application of two different semantic similarity computation schemes, namely, edge distance-based and information content-based measures in the scope of geann. furthermore, the experimental data set is extended to  <dig> go terms in contrast to the previously used smaller data set of  <dig> go terms in the conference version. new experimental studies are carried out:  evaluation of the performance improvement due to the use semantic matching,  a study of how the use of different semantic similarity computation methods influences the performance,  evaluation of the performance at different enrichment threshold values,  assessment of the performance of a probabilistic ordered-pair-based annotation framework. finally, the related work section has been extended, and a discussion section on future directions to improve geann has been added.

the article is organized as follows. in the next "methods" section, we first discuss the pattern construction process, where we present  the significant term discovery,  the pattern construction, and  the scoring scheme used for patterns. next, in the methods section, we present the second phase of geann, namely, the semantic pattern matching framework. then, in the results and discussion section, we present an extensive experimental evaluation of our methodology and discussion of the results. more specifically, we perform a precision/recall analysis to evaluate the overall accuracy of geann as well as its accuracy in three distinct subontologies of go, namely, biological process, molecular function, and cellular location. furthermore, we compare geann to two other related work on a select set of go concepts. in addition, we assess the impact of using semantic matching versus syntactic matching, and compare different semantic similarity algorithms. finally, we discuss two alternative methods to obtain high recall values since, in some application areas, higher recall may be preferred to higher precision. more specifically, we propose a simple probabilistic annotation framework, and evaluate its performance. next, we evaluate the accuracy, and particularly the recall, of geann at different significance thresholds, and then we conclude.

methods
in this section, we present the algorithms that we have developed within the geann annotation framework .

i. pattern discovery
patterns are constructed based on significant terms and phrases that are associated with a go concept. hence, we first describe how we compute significant terms and phrases. then, we elaborate on pattern construction and scoring.

computing significant terms
motivation
some terms or a sequence of terms  may appear frequently throughout the abstracts of a go concept's evidence article set. for instance, rna polymerase ii which is described as the machinery performing elongation of rna in eukaryotes appears in almost all evidence articles associated with the go concept "positive transcription elongation factor activity". hence, intuitively, such kind of frequent occurrences within the evidence article set of a specific go concept should be marked as a stronger indicator for a possible annotation. on the other hand, terms that are common to almost all abstracts  should be excluded from an significant term list.

approach
given  a go concept c,  the set ann of gene annotations by c,  a database of article abstracts , and  a threshold value, geann computes the set of significant terms for c . for each term t that appears in an evidence article of the input go concept c, the algorithm simply computes the number of evidence articles  that are associated with c, and that contain t, as well as the number of articles  that contain t in the whole input article database. then, a simple statistical enrichment score is computed . the terms with enrichment scores below the input threshold are excluded from the significant term list for c. moreover, the terms that constitute the name of a go concept that is being processed are by default considered to be significant terms.

definition . given a go concept g, a term or phrase t, let d be the set of all articles in a database, and e be the set of evidence articles for g where e ⊆ d. then, the statistical enrichment score enrichment_score of term t in e is the ratio of t's frequency in e to the frequency of t in d. that is, enrichmentscore = f/f where f and f are the frequencies of t in t and d, respectively.

one can use various methods to obtain the significant terms of a go concept. for instance, correlation mining can be employed by constructing contingency matrices  <cit>  for each article-term pair, and selecting the terms that are highly correlated with the articles of an input go term c based on a statistical test. alternatively, random-walk networks  <cit>  can be constructed out of the terms appearing in evidence articles, and significant terms can be computed out of frequent word sequences that are obtained through random-walks. since the evidence article sets of go concepts are most of the time very small, to keep the methodology less complicated, we adopted a frequency-based approach.

rather than using the above statistical enrichment score, we could simply use the frequency of a term in the evidence article set  as the basis of determining whether the term is a significant term or not by simply eliminating the term if its support is insufficient . one of the issues that one needs to deal with in this method is setting a threshold to be used for deciding whether a given term should be included into the significant term set. having a strict threshold sometimes results in an empty significant term set for some go concepts, while some other go concepts may have large sets of significant terms, not all of which may be beneficial. the main cause of such occurrences is the large variances in the size of the evidence article set available for each go concept. for instance, assume that the significant term frequency threshold is set to 50%, i.e., for a term to be considered significant, it should appear in half of the evidence articles. for a go concept with  <dig> evidence articles, terms that appear in five of them will be considered significant. on the other hand, for a go concept with  <dig> training articles, terms that appear in  <dig> of them will be excluded from the significant term set. and, intuitively, as the size of the evidence article set increases, the possibility of detecting terms that appear in 50% of the evidence articles will decrease, in comparison to an evidence article set of smaller size. in order to get around this problem, we employ the above statistical enrichment measure to filter out the terms and phrases that are not sufficiently discriminative within the evidence article set. in this way, the problem of missing  terms that appear in relatively low frequencies in both the evidence article set of the input go concept c, and the input article database, but that may still be discriminative as its frequency in the evidence article set is substantially higher than its frequency in the whole article database. the effects of different statistical enrichment scores on the final accuracy are analyzed in the last section.

computing significant phrases
a significant phrase is considered to be an ordered list of significant terms. significant phrases are constructed out of significant terms through a procedure similar to the apriori algorithm  <cit> . given  a go concept c,  the set ann of gene annotations by c,  a database of article abstracts ,  a threshold value, and  the significant term set s for c, geann computes the set of significant phrases for c . the algorithm creates length- candidate phrases out of combinable length-k phrases, where "length" refers to the number of terms in a phrase. in order for two significant phrases spi and spj of length-k to be combinable to produce a length- candidate phrase, the last k- <dig> terms in spi should be the same as the first k- <dig> terms in spj. at each iteration, those candidate phrases with enrichment scores lower than the input threshold are eliminated. this procedure is repeated until no more new phrases can be created. as an example, in the first phase, each pair of terms in the significant term set s are combined  to create length- <dig> candidate phrases. then, the statistical enrichment score is computed for each candidate phrase, and those with enrichment scores below the input threshold are eliminated. next, each combinable pair of length- <dig> significant phrases is merged to obtain a length- <dig> candidate phrase, and similarly, those with insufficient enrichment scores are eliminated.

example 1: consider a set of significant term set s = {polymerase, transcription, rna, factor}. figure  <dig> illustrates the level-wise significant phrase construction. at the first step, each pair of significant terms are combined to produce candidate length- <dig> significant phrases. note that there are  <dig> distinct length- <dig> phrase candidates that can be constructed out of  <dig> significant terms. in order to avoid a complicated illustration, in figure  <dig>  we only show the candidates that have passed the enrichment threshold.

constructing patterns
in geann, the identifying elements  of a go concept are considered as representations of the go concept in textual data. and, the terms surrounding the identifying elements are regarded as auxiliary descriptors of the go concept represented by the pattern. in other words, a pattern represents an abstraction which encompasses the identifying elements and the auxiliary descriptors together in a structured manner. hence, a regular pattern, the most basic form of patterns, is organized as a 3-tuple: {left} <middle> {right} where each element of the 3-tuple corresponds to a set or sequence of words. <middle> element is an ordered sequence of significant terms  whereas {left} and {right} elements correspond to word sets  that occur in training articles around significant terms  where the number of terms to be considered in the elements of {left} and {right} is adjusted by a window size.

each word or phrase in the significant term set leads to the creation of a pattern template. a pattern template can be considered as a blueprint of a pattern family which specifies the middle element shared by all the members of the family. hence, a pattern is an instance of a pattern template, and the pattern template can lead to several patterns with a common middle element, but  different left or right elements. once a pattern template is created from a significant term or a phrase, in the training article abstracts, for each different surrounding word group that occurs around the significant term/phrase, a new instance of the pattern template, i.e., a pattern, is created. we give an example.

example 2: two of the patterns that are created from the pattern template {left} <rna polymerase ii> {right} are listed below, where rna polymerase ii is found to be a significant term for the go concept positive transcription elongation factor. {left} and {right} tuples are instantiated from the surrounding words that appear before or after the significant term in the text, where the window size is set to three.

 {increase catalytic rate}<rna polymerase ii>{transcription suppressing transient} 

 {proteins regulation transcription}<rna polymerase ii>{initiated search proteins} 

note that, to accommodate different forms of the same word, during the preprocessing stage, all the words are stemmed  <cit> , and stopwords are eliminated  <cit> ; but, for readability purposes, the words are shown in their original forms in the above example.

. given  a go concept c for which the patterns will be extracted,  c's annotation set with corresponding evidence articles,  significant terms and phrases that are computed for c in the previous step, and  a windowsize value, the algorithm returns the set of all regular patterns for c. more specifically, for each significant term or phrase tp, first, a pattern template is created. then, for each occurrence of tp in each evidence article of c, an instance of the pattern template is created where the middle tuple consists of the phrase tp, the left tuple is assigned to the first windowsize words preceding tp in c, and the right tuple is assigned to the first windowsize words following tp in c. patterns are not allowed to span multiple sentences. thus, if there are fewer words following or preceding tp in a sentence, then the size of the left and right tuples may be smaller than the windowsize parameter. finally, the constructed patterns are returned as the output.

pattern crosswalks
having constructed patterns based on significant terms/phrases, we extend the initial pattern set by joining regular patterns. the goal here is to create larger patterns that may better characterize a possible go annotation embedded into textual data. extended patterns are constructed by virtually walking from one pattern to another. conceptually, to walk from one pattern to another, a "bridge" connecting these patterns is located. based on the type of bridge connecting a pair of patterns, geann creates two different types of extended patterns, namely side-joined and middle-joined patterns. next, we briefly explain specific extended pattern types in the order of increasing size.

transitive crosswalk
occasionally, created patterns may have overlapping left or right tuples. given a pattern pair p <dig> = {left1}<middle1>{right1}, and p <dig> = {left2}<middle2>{right2}, if the right tuple of the first pattern is the same as the left tuple of the second pattern , then patterns p <dig> and p <dig> are merged into a 5-tuple side-joined pattern p <dig> such that p <dig> = {left1}<middle1>{right1}<middle2>{right2}. side-joined pattern construction process is illustrated in figure  <dig> below.

note that, in comparison to 3-tuple regular patterns, side-joined patterns has five tuples, where <middle1> and <middle2> are sequence of words, and the remaining tuples are bags of words. next, we give an example of a side-joined pattern that is created for the go concept positive transcription elongation factor where  is a tag for a genomic entity name placed by the named entity recognizer.

example 3: consider the two patterns p <dig> and p <dig> below.

 p <dig> = {factor increase}<transcription rate>{rna polymerase ii} 

 p <dig> = {rna polymerase ii}<elongation factor>{} 

then, the side-joined pattern is:

 p <dig> = {factor increase catalytic}<transcription rate>{rna polymerase ii}<elongation factor>{} 

. given a set of regular patterns, the algorithm simply checks each pair of regular patterns to see if the right tuple of the first pattern is the same as the left pattern of the second pattern. if this is the case, a new 5-tuple side-joined pattern is created, and its tuples are initialized as illustrated in figure  <dig> 

side-joined patterns are helpful in detecting consecutive pattern matches that partially overlap in their matches. if there exist two consecutive regular pattern matches, then such a matching should be evaluated differently than two separate matches of regular patterns as it may provide stronger evidence for the existence of a possible go annotation in the matching region. therefore, side-joined patterns are abstractions to capture consecutive matches.

middle crosswalk
the second type of extended patterns are constructed based on partial overlappings between the middle and side  tuples of two patterns. since middle tuples are constructed from significant terms/phrases, a partial overlapping, that is, a subset of a middle tuple, will also be a significant term. a pattern pair p1={left1}<middle1>{right1} and p2={left2}<middle2>{right2} can be merged into a 4-tuple middle-joined pattern as illustrated in figure  <dig> 

a. right middle walk: {right1} ∩ <middle2> ≠ ∅ and <middle1> ∩ {left2} = ∅

b. left middle walk: <middle1> ∩ {left2} ≠ ∅ and {right1} ∩ <middle2> = ∅

c. middle walk: <middle1> ∩ {left2} ≠ ∅ and {right1} ∩ <middle2> ≠ ∅

in comparison to 3-tuple regular patterns, middle-joined patterns have  <dig> tuples: {left}<middle1><middle2>{right} where <middle1> and <middle2> are word sequences, whereas {right} and {left} are bags of words. in case , the first middle tuple is the intersection of {right1} and <middle2> tuples where the intersection is aligned according to the order of words in <middle2>. case  is handled similarly. as for case , the first and the second middle tuples are subsets of <middle1> and <middle2>. middle-joined pattern construction is illustrated in figure  <dig> which is followed by an example middle-joined pattern constructed for the go concept positive transcription elongation factor.

example 4: middle-joined pattern  middle walk). consider the two patterns p <dig> and p <dig> below where window size is three.

 p <dig> = { facilitates chromatin}<transcription>{chromatin-specific elongation factor} 

 p <dig> = {classic inhibitor transcription}<elongation rna polymerase ii>{pol ii} 

then, the resulting middle-joined pattern p <dig> is:

 p <dig> = { facilitates chromatin}<transcription><elongation>{pol ii} 

. for each pair  of patterns in the input pattern set, the algorithm checks for overlaps either between middle tuple of p <dig> and left tuple of p <dig>  or between right tuple of p <dig> and middle tuple of p <dig>  if any overlap is found, then, according to the cases which are enumerated in figure  <dig>  a new 4-tuple middle-joined pattern is created, and its tuples are initialized .

like side-joined patterns, middle-joined patterns capture consecutive pattern matches in textual data. in addition, since we enforce the full matching of middle tuple for a valid match, partial matches to the middle tuple of a regular pattern is missed. however, middle-joined patterns accommodate consecutive partial matches since, by definition, their middle tuples are constructed from the intersection of a middle tuple and a side tuple of different patterns. for instance, in example  <dig>  a partial match to p <dig> followed by a partial match to p <dig> can be accommodated by the middle-joined pattern p <dig>  otherwise, such a match would be missed.

scoring patterns
pattern scores are used to assign a confidence value for a candidate annotation which is created as a result of match to a particular pattern. for scoring patterns, geann uses the statistical enrichment scores of significant terms/phrases as the scores of the patterns. that is, given a go concept g, patternscore of a pattern p with a middle tuple which is constructed from a term/phrase t is

 patternscore = enrichmentscore = f/f 

where e is the evidence article set of g, and d is the set of all articles in the database.

similarly, extended  patterns are also scored based on the statistical enrichment scores of their middle tuples. however, since the extended patterns have two middle tuples, the statistical enrichment is adapted accordingly as follows. given a side-joined or middle-joined pattern exp with middle tuples phrases t <dig> and t <dig>  and go concept g with evidence article set e, patternscore is computed as

 patternscore = enrichmentscore = f/f 

where f is the frequency of articles that contain both t <dig>  t <dig> in e, and f is the frequency of articles that contain both t <dig> and t <dig> in d. for middle-joined patterns t <dig> and t <dig> is required to be consecutive, while for side-joined patterns there may be up to windowsize number of words between t <dig> and t <dig> to compensate for the tuple between t <dig> and t <dig> in a side-joined pattern.

the fact that we design our pattern scoring mechanism completely based on the enrichment scores of the significant phrases is closely related to the pattern construction phase. among the elements of a pattern, the middle tuple constitutes the core of a pattern since only the middle tuple consists of phrases or terms that are determined based on frequency-based enrichment criteria. on the other hand, the remaining elements  of a pattern, are directly taken from the surrounding words of significant phrases in evidence articles without being subject to any statistical selection process. hence, middle tuples are the elements that provide the semantic connection between a pattern and the go concept to which it belongs. alternatively, we could use the support  of the significant phrase in the middle tuple. nevertheless, enrichment score already utilizes support information , and further refines it by considering the global support  so that the influence of the patterns with significant phrases that are common to almost all articles in the database would be relatively smaller.

pattern matching
now that the patterns are obtained, the next step is searching for occurrences of patterns with the goal of predicting new annotations based on pattern matches. given a pattern p and a article pr, we have a match for p in pr if  pr contains the significant phrase in the middle tuple of p, and  left and right tuples of p are semantically similar to the surrounding words around the occurrence of p's middle tuple in pr. we require exact occurrence of p's middle tuple in pr since the middle tuple is the core of a pattern, and it is the only element of a pattern, which is computed based on a statistical measure. and, the motivation for looking for semantic similarity rather than exact one-to-one match for side tuples is that, for instance, given a pattern p <dig> = "{increase catalytic rate}<transcription elongation>{rna polymerase ii}", we want to be able to detect phrases which give the sense that "transcription elongation" is positively affected. hence, phrases like "stimulates rate of transcription elongation" or "facilitates transcription elongation" also constitute "semantic" matches to the above pattern.

. given a pattern pat to be searched in a set of articles articleset, and the go concept that pat belongs to, the algorithm returns a set of gene annotation predictions with their confidence scores. for each occurrence of pat's middle tuple in an article pr in articleset, the corresponding left and right tuples are extracted from the surrounding words around the occurrence in pr. then, pat's left and right tuples are compared for semantic similarity to the left and right tuples that are just extracted from pr. we next describe the implementation of this comparison procedure function.

in order to determine the extent of semantic matching between two given sets of words ws <dig> and ws <dig>  geann employs wordnet to check each word pair , where wi∈ ws <dig> and wj ∈ ws <dig>  if they have similar meanings. to this end, we have implemented a semantic similarity computation framework based on wordnet. given a word pair , many semantic similarity measures are proposed to compute the similarity between the word pair wi and wj in different contexts  <cit> . instead of proposing a new measure, in this study, we have implemented two of the commonly used similarity measures on wordnet. next, we briefly describe these similarity measures and defer their performance evaluations  to the experiments section.

▪ edge distance-based similarity measure
given a taxonomy t and two nodes  t <dig> and t <dig> in t, the most intuitive way to compute the similarity between t <dig> and t <dig> is to measure the distance between t <dig> and t <dig> in t  <cit> . as the path between t <dig> and t <dig> gets shorter, their similarity increases. that is,

 simedge_distance  = 1/distance. 

if there are multiple paths from t <dig> to t <dig>  then the shortest path is selected to compute the similarity. for instance, in figure  <dig>  in a sub-taxonomy in the wordnet, the similarity between "car" and "truck" is simedge_distance  = 1/ <dig> =  <dig>  while the similarity between "car" and "bicycle" is simedge_distance  = 1/ <dig> =  <dig> .

▪ information content-based semantic similarity
resnik  <cit>  proposes a similarity measure which is based on the argument that nodes t <dig> and t <dig> in a taxonomy t are similar relative to the information shared between the two. hence, the information content of an ancestor node t' that subsumes both t <dig> and t <dig>  in t can be used as a measure of similarity between t <dig> and t <dig>  information content of a node t in taxonomy t is computed based on the occurrence probability p of t in t. p is the ratio of the nodes that are subsumed by t to the total number of nodes in t. lesser occurrence probability for a node t implies a higher information content. information content ic of node t is quantified as -log p which decreases as t gets more general in the taxonomy. as an example, the occurrence probability of node "automotive" in figure  <dig> is p = 3/ <dig>  and its information content is -log =  <dig>  whereas the information content of node "car", which is more specific, is -log =  <dig> . if there are multiple shared ancestors of t <dig> and t <dig> then the one with the highest information content is selected for similarity computation.

. in order to compute the overall semantic similarity between sets of words based on the similarities between individual word pairs, we utilize an open source software library  <cit>  which uses the hungarian method  <cit>  to solve the problem as follows. given two word sets, ws <dig> and ws <dig>  let n be the number of words in ws <dig>  and m be the number of words in ws <dig>  first, a semantic similarity matrix, r , containing each pair of words in ws <dig> and ws <dig> is built, where r  is the semantic similarity between the word at position i of ws <dig> and the word at position j of phrase ws <dig>  which can be computed using either of the measures explained above. thus, r  is also the weight of the edge from i to j. the problem of computing the semantic similarity between two sets of words ws <dig> and ws <dig> is considered as the problem of computing the maximum total matching weight of a bipartite graph  <cit> . this makes sense since ws <dig> and ws <dig> are disjoint in the sense that the comparisons are always made between word pairs that belong to different sets. finally, the hungarian method  <cit>  is used to solve problem of computing the maximum total matching weight of a bipartite graph. for instance, consider the bipartite graph g in figure  <dig> where nodes ti are the words from ws <dig> and ws <dig>  and the weight of the edges are the semantic similarity between ti and tj where ti ∈ ws <dig> and tj ∈ ws <dig>  the problem of computing the maximum total matching weight on g is to find a subset e' of edges in g such that no edges in e' share a node, all nodes are incident to an edge in e', and the sum of edge weights in e' is maximum.

then, the match score of pattern p to an occurrence o of p in an article is computed as the average similarity of the semantic matching score computed for the left and the right tuples of p. that is,

  semsim=semsim+semsim <dig> 

where p. lefttuple is the left tuple of pattern p, and o. lefttuple is the word set in o that matches the left tuple of p. similarly, o. righttuple is the word set in o that matches p. righttuple.

the semantic similarity score returned from the wordnet evaluation is used as the base of our confidence for the match between p and o. thus, each individual pattern match between p and o is assigned a score based on  the score of the pattern p, and  the semantic similarity between p and o computed using wordnet . that is,

  matchscore = patternscore*semsim 

associating pattern matches with genes
having located a text occurrence o that matches the pattern p, and evaluated the match score, the next step is to decide about the genomic entity that will be associated with the match, and, hence, will get annotated with the specific go concept the pattern belongs to. we next describe the implementation of this function. in this context, locating the corresponding gene for a candidate annotation, there are two main issues that one needs to deal with:  detecting terms or phrases that are gene or gene product names, and  determining which one of the genes to choose, among possibly many candidates located around the matching region in the text. the first task is a particular version of the problem of developing a named entity tagger, which is an active research area in the natural language processing field. since our focus in this study is not on developing a named entity tagger, we utilized an existing biological named entity recognizer, called abner  <cit> . abner is reported to recognize genomic entities with 68% precision and 77% recall, and it is one of the most accurate entity recognizers in terms of performance, according to experimental comparisons against similar systems  <cit> .

once the gene names in the text are tagged by the named entity tagger, the next task is to decide on the gene to be annotated. this task is not straightforward as there may be several gene products/genes around the matched phrase in the abstract. thus, we need to find a mechanism to correctly recognize the genomic entity the matched occurrence o refers to. our approach is based on a set of heuristics: we first look into the sentence containing the matching m, and choose the gene product that comes first before the matching phrase in the same sentence. if we cannot find one, then we check the part that follows the matching region in the same sentence. if there is no gene name mentioned in the same sentence, we check the previous and the following sentences, respectively.

finally, each predicted annotation is assigned an annotation  score. the final annotation score of a gene g by a pattern p with occurrence o in the text is a function of both the match score of p to o  and the distance of the reference to the gene in the text to o, that is

  annotationscore = matchscore*fdistance  

where fdistance is the distance function, t is the distance of gene reference g to occurrence o in terms of the number of words between them, and n is the minimum number of sentences that span g, o, and the set of words between g and o. as an example, if g and o are in the same sentence, n =  <dig>  and if g reference is in the next sentence that follows the sentence containing o then n =  <dig>  and so on. intuitively, the distance function should generate lower scores as the distance t increases. in addition, being located in different sentences should considerably decay the distance function value. therefore, fdistance should be a monotonically decreasing function as t or n increases. in this article, we use the following heuristic distance function that conforms to the above intuitions concerning t and n.

  fdistan⁡ce={1if0≤tn≺21/logotherwise 

alternative distance functions are possible as long as those alternatives are monotonically decreasing as t or n increases. while designing the above particular function, we choose to incorporate n, for instance, as an exponent of distance t as we have observed in several examples that reliability annotations decay significantly when a pattern match in a sentence is used to annotate a gene in a different sentence. in contrast, it is our observation that the impact of the distance parameter t is less severe in comparison to n. thus, t is incorporated to affect the value of the function in a linearly inverse proportional manner.

RESULTS
experimental setup and dataset
in order to evaluate the performance of geann, we perform experiments on annotating genes in ncbi's genbank  <cit> . during the experiments, we exclude an article abstract from our testing/training dataset if it does not contain a reference to any of the gene products for which it is given as the annotation evidence since it is not possible to extract information from such abstracts. we also exclude those genes from our dataset that could not be located in the article abstract that it refers to as part of its annotation. this way, we aim to clean the noise in the data, which would not be useful to train or test the geann system. these kinds of exclusions usually occur for article abstracts that discuss the sequencing of whole organisms; and, hence, individual gene or protein names belonging to the sequenced organism do not usually appear in such article abstracts. in addition, each annotation in go is accompanied by an evidence code which indicates how the annotation is created, i.e., how reliable it is. the least reliable annotations are the ones that have the evidence code iea  which are computationally created, and not curated. therefore, we exclude such annotations from our training data.

our experiments are based on the precision-recall analysis of the predicted annotation set under several circumstances. to this end, for each case, we adopt the k-fold cross validation scheme  <cit>  as follows: the existing, known annotation set is divided into k parts , and  parts are used to train geann, and the remaining one part is used to test the trained geann system. the same procedure is repeated k  times, each time, using a different partition for testing, and the rest of the annotation set for the training. finally, individual experimental results are normalized, and averaged to obtain a combined result set for a given go concept. next, we formally define the precision and recall as well as f-value which are used as performance measures during interpretation of experimental results.

definition : given a go concept c and the set s of predicted annotations for c, precision for c is the ratio of the number of genes in s that are correctly predicted to the total number of genes in s.

definition : given a go concept c and the set s of predicted annotations for c, recall for c is the ratio of the number of genes in s that are correctly predicted to the number of genes that are known to be annotated with c.

definition : f-value is the harmonic mean of precision and recall, and computed as

 f-value = / 

since we perform 10-fold cross validation, for an accurate analysis, we enforce the requirement that each partition has at least three evidence articles to test during the evaluations. hence, we make sure that each selected go concept for experimental evaluation has at least  <dig> evidence articles and genes. thus, the experimental go concept set consists of  <dig> go concepts  from all three subontologies of go, namely, biological process, cellular component, and molecular function subontologies. distribution of the experimental go concepts by their corresponding go subontology and the level of the go hierarchy are presented in figure  <dig> where the root level of each subontology is considered as level  <dig>   <dig> go concepts had a total of  <dig>  articles referenced as evidences for annotation of a total of  <dig>  genes. in total,  <dig>  articles and  <dig>  genes were removed from the data set, which left us with  <dig>  article abstracts and  <dig>  genes to be used during the evaluation. on the average, each go concept has  <dig> evidence articles and  <dig> gene annotations.

in order to approximate the word frequencies in the actual pubmed database , we used a larger corpus of  <dig>  article abstracts which consist of articles that are referred to in support of an annotation in go . this corpus is only used for the calculation of statistical enrichment scores, and consists of articles that genbank curators list as related reference articles for the genes in the genbank database. reference article set for each gene is part of the genbank database, and it is publicly available  <cit>  to download or browse online. as part of pre-processing, abstracts/titles of articles are tokenized, stopwords are removed, words are stemmed. non-alphanumeric characters are removed, as they are not useful for extracting patterns. each abstract/title is run through the biomedical named entity tagger abner  <cit>  to mark genes and gene products.

geann maps gene name occurrences found in pubmed article abstracts to actual gene records in genbank. one major problem in this type of study is to determine which entities from two different data sources are really referring to the same object. the reconciliation process also known as the entity disambiguation problem  <cit>  by itself is a separate research problem. in this study, we ignore the genomic entities annotated by geann and yet do not have any corresponding entry in genbank. furthermore, a genomic entity symbol that is annotated through the procedure we described so far may match to more than one gene or gene product in genbank because of the shared synonyms between different genes. thus, in this study, as a reconciliation scheme, geann uses three assumptions.

a1: among the genbank genes that match to the symbol being annotated, if at least one of the matched genes has the annotation involving a particular go concept, then this annotation prediction is considered as a correct prediction . on the other hand, if none of the genes sharing the gene symbol of the predicted annotation has a record corresponding to the particular go concept among its go annotations, then such results are considered as incorrect predictions or false positives.

a2: if one of the matche genes in genbank is annotated with a descendant of the given go concept g, then g also annotates the gene  due to the true-path rule of go, which states that if the child concept annotates a gene, then all its ancestor concepts also annotate that gene.

a3: if the predicted annotation is included in genbank, then we consider this prediction as a true positive regardless of  its evidence code, and  whether it has a literature reference.

experiment 1: overall performance
we first evaluate the overall performance of geann. predicted annotations are ordered by their annotation scores. first, precision and recall values for individual go concepts are computed by considering the top-k predictions where k is increased by one at each step until either all the annotation information available for a go concept is located, or all the go candidates in the predicted set are processed. next, the precision/recall values from individual go concept assessments are combined by taking the average of precision/recall values at each k value for top-k results.

from figure  <dig> which presents the average precision/recall values that were computed for different predicted gene set sizes, we have

observation 1: geann yields 78% precision at 48% recall.

note that the accuracy of the tagging gene/gene products in the text influences the association of a pattern to a genomic entity. however, named entity taggers  also negatively affect the accuracy. in the rest of the paper, we consider this negative effect .

experiment 2: annotation accuracy across different subontologies in go
next, we evaluate the accuracy of geann across the three different subontologies of go, namely, biological process, molecular function, and cellular location.

observation 3: in terms of precision, geann provides the best precision for the oncepts from cellular component  and molecular function subontologies where precision is computed as 80% while biological process  subontology yields the highest recall . the fact that mf subontology provides better precision may be due to the fact that biological process concepts refer to biological pathways, and pathways are more general biological abstractions in comparison to the specific functionalities of enzyme proteins/genes, a number of which is included in each pathway.

.

experiment 3: comparative performance analysis with other systems
in this section, we compare our approach to two other studies, namely, raychaudhuri et al.  <cit>  and izumitani et al.  <cit> , that build document classifiers to label the genes with go concepts through the documents associated with them. first, both approaches assume that a gene is associated with several articles. this is a strong assumption in that if the experts are to invest considerable time to read and associate a set of articles with a gene, then they can probably annotate the gene themselves with the appropriate go concepts. under this assumption, these systems are not practically applicable to automate gene annotation. second, since both of the systems work at the document level, no direct evidence phrases are extracted from the text. third, geann can also provide the matching phrases as a source of evidence rather than the whole document. in this experiment, we show that, even though geann does not have the strong assumptions that these systems use, and can work with much smaller sets of training data, geann's performance is still comparable to or better than these systems. furthermore, geann handles the reconciliation of two different genomic databases based on the tagging of genes and proteins by an entity tagger , whereas the studies discussed here have no such considerations as they assume that such a mapping is already provided to their systems with some additional associated articles.

using  <dig> go concepts, izumitani et al. compares its system to raychaudhuri et al.'s study. to provide a comparison, our analysis in this experiment is also confined to this set of go concepts. the following go concepts could not be cross-validated due to their small annotation set size: ion homeostasis go: <dig> , membrane fusion go: <dig> . furthermore, one of the test concepts  has since become obsolete. therefore, here we present comparative results for the remaining nine go concepts in terms of f-values. table  <dig> provides the f values for these systems and geann. table  <dig> provides f-values in terms of the subontologies.

observation 4: geann's performance is comparable to or better than izumitani et al. and raychaudhuri et al. in terms of the average f-value over the test go concept set of size  <dig>  geann outperforms both systems, and for six of the nine test concepts, geann performs the best.

experiment 4: comparing semantic matching against syntactic matching
next, we experimentally measure the improvement provided directly by the use of wordnet as the semantic similarity infrastructure. for comparison purposes, we developed a baseline methodology by replacing the semantic similarity pattern matching part in geann's implementation with a naive syntactical pattern matching method which recognizes only exact  match between a pattern and a textual phrase. all the other scoring and pattern construction mechanisms are kept the same for both the baseline system and the geann in order to focus on the semantic pattern matching infrastructure of geann. then, we run both the baseline approach and geann on our experimental set of  <dig> go concepts. figure  <dig> provides the precision/recall values for both of the systems.

observation 5: geann with semantic matching outperforms the baseline approach by 24% in terms of recall and 15% in terms of the precision.

the improvement in the accuracy is expected since not only exact matches to the side tuples of the patterns, but also approximate matches can be located and scored based on the well-studied taxonomy similarity measures and by utilizing semantic relationships between the concepts of wordnet. table  <dig> lists the top- <dig> go concepts which experience the most dramatic improvements in terms of their annotation accuracy in comparison to the baseline approach.

next, we explore if the semantic pattern matching approach performs better for the go concepts from a specific subontology of go. figure  <dig> shows the distribution of the improvement that geann brings over the baseline approach for each subontology in go.

observation 6: semantic pattern matching approach performs almost equally well for each subontology of go, and the prediction accuracy improvement is more or less uniformly distributed over different subontologies.

this indicates that semantic pattern matching approach is not specific to a particular set of go concepts, but is effective throughout the whole go ontology.

since go is hierarchically organized, go concepts that are closer to the root concept represent more general biological knowledge than those that are closer to the leaf levels. hence, to see how the semantic pattern matching framework performs at different levels of go, next we cluster the concepts by the go level they reside, and analyze the cross-validation accuracy by changing the go level. figure  <dig> shows the change of improvement brought by geann over the baseline method at different go levels.

observation 7: there is no perfect regularity in terms of changes in recall/precision improvement, as the concepts get more specific.

observation 8: there is a general trend of decrease in both precision and recall improvement as the concepts get more specific.

the semantic similarity measures rely on the existence of a path between the synsets  of the words that are compared. intuitively, as the go level gets deeper towards the leaf level, the concepts gets more specific. hence, the sentences describing such concepts would be more likely to include terms that are domain-specific and less likely to be found in wordnet, which narrows down the space that wordnet can be influential. in addition, since wordnet is a general purpose english word taxonomy, and is not specific to the biomedical domain, its capacity to accommodate the terms in the biology domain should not be overestimated. for instance, during our experiments, around 25% of the semantic similarity computations returned the score of zero.

experiment 5: effect of using alternative semantic similarity measures
as there are many alternative measures that have been proposed in the literature to compute semantic similarity over the taxonomies, it is informative to explore the impact of different measures on geann's accuracy. evaluation of alternative measures is by itself the main topic of many research articles  <cit> . in this section, our goal is by no means to provide a comprehensive study of different measures in this article's context, but to present an assessment of how the replacement of an adopted similarity measure may affect the rest of the framework. to this end, we have implemented two well-known semantic similarity measures, namely, the information content -based and edge-counting methods. we ran geann on the experimental set of go concepts twice using a different measure at each run, and then compared average precision and recall of each run.

observation 9: replacement of ic-based semantic similarity computation with edge-counting method does not cause dramatic changes on the overall accuracy of geann . a couple of go concepts had dramatic change  on their either recall or precision, such occurrences were not sufficiently common to influence the overall accuracy significantly.

the above observation is reasonable since the proposed framework here is not primarily based on the type or the nature of the adapted similarity measure. what is crucial to geann's success in using semantic similarity over a traditional syntactical pattern matching system is the adoption of a flexible matching system that takes advantage of semantic relationship of words, which is not always intuitively or readily available in a typical pattern matching system. hence, the above observation confirms that  the adopted similarity measure is only a plug-in tool in the overall framework,  a particular measure is not at the core of our paradigm,  any of the well-known semantic similarity measures that are studied in the literature are likely to be employed by geann.

next, we further examine the small set of go concepts that were affected by the change of the similarity measure. about  <dig> go concepts experienced an f-value change greater than 4%, which are given in table  <dig>  figure  <dig> maps the change in f-values of go concepts to their go levels. from this figure, there is no obvious correlation or trend that one may observe towards the relationship between f-value changes and go-concept levels. in table  <dig>  among the top  <dig> most affected go concepts,  <dig> of them belong to biological process ontology,  <dig> of them to molecular function, and  <dig> of the concepts belongs to the cellular component subontology. from this limited view of distribution, it may be tempting to look for some kind of correlation between biological process concepts and a particular semantic similarity measure. however, this may be biased as  <dig> of the  <dig> go concepts in the experimental set are from the biological process subontology. it is not clear to us why these particular go concepts are negatively or positively affected when the semantic similarity measure is changed from the edge-counting method to the ic-based measure. in previous studies, these two semantic similarity measures are studied in different contexts, and there is no consistent superiority of one measure to another throughout different studies reported in the literature.

enhancing recall
as illustrated through the experimental results, an inherent drawback of pattern-based text mining systems is the fact that their recall performance is frequently low. in this section, we describe and evaluate two different approaches to obtain annotation predictions with high recall:  through a probabilistic annotation framework, and  by adjusting statistical enrichment threshold value .

observation 10: at recall of 61% which is the maximum recall that geann can achieve at its maximum precision level, the probabilistic approach has a precision of 51% while geann has precision of 78%.

observation 11: the probabilistic approach can reach to higher recall values  which is significantly higher than what geann provides .

observation 19: adjusting enrichment threshold to lower values results in higher recall than the maximum recall value provided by the probabilistic approach.

CONCLUSIONS
in this article, we explore a method that automatically infers new go annotations for genes and gene products from pubmed abstracts. to this end, we develop geann that utilizes the existing annotation information to construct textual extraction patterns characterizing an annotation with a specific go concept. during the annotation stage, geann searches for phrases in pubmed abstracts that match the created patterns. matches are scored and associated with the most proper genomic entity or a set of entities around the matching region. as the final output, geann lists the genes that are predicted to be annotated with a given go concept. in our experiments, geann either has outperformed or is comparable to earlier automated annotation work.

.

authors' contributions
ac designed the study, drafted the manuscript and carried out the experimental studies. go participated in its coordination and helped to draft the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
supplementarymaterial. this file contains detailed discussion on sections that are not included . more specifically, the supplementary material document contains an illustration of the overall approach, formal algorithm sketches for procedures described in the main manuscript, some additional experimental results, an elaborate comparative discussion on two alternative approaches to obtain higher recall values, and a detailed discussion on related and future work.

click here for file

 additional file 2
appendix  <dig>  this file contains the experimental go concept set along with overall precision/recall values for each go concept.

click here for file

 acknowledgements
this research is supported in part by the nsf award dbi- <dig>  a grant from the charles b. wang foundation, and microsoft equipment grant.
