BACKGROUND
genotyping of m. tuberculosis complex isolates has enhanced tb control and contact tracing while providing valuable insights on tuberculosis transmission and pathogenesis  <cit> . recently, strain differences were found to affect clinical presentation  <cit> , and unravelling of the genes responsible for these phenotypic differences might lead to the identification of drug- and vaccine targets.

spacer oligonucleotide typing  analysis is the most user-friendly and commonly applied genotyping tool for m. tuberculosis isolates worldwide. global spoligotype databases include 'fingerprints' from thousands of m. tuberculosis complex isolates from diverse regions  <cit> . based on hybridization of the direct repeat region of m. tuberculosis  <cit> , spoligotype analysis generates reproducible binary patterns of  <dig> spacers, which can readily be shared electronically. this  <dig> binary spacer format can be transcribed as a 15-digit code  <cit> , although no international standardization has been established. while spoligotype analysis lacks the resolution of the 'gold standard' genotyping method is <dig> restriction fragment length polymorphism   <cit> , it has several advantages compared with this technique: firstly, it relies on pcr amplification of m. tuberculosis dna, which requires much less dna and can be applied straight to sputum samples. secondly, up to  <dig> isolates can be completed within one day. thirdly, isolates with less than  <dig> bands on is <dig> rflp can be genotyped with a higher resolution by spoligotype analysis. finally, the spoligotype patterns can distinguish between subspecies and clades within the m. tuberculosis complex and are phylogenetically informative  <cit> .

spoligotyping generates arrays of spots and typically data entry and classification is performed manually, which can result in errors  <cit> , or with often expensive electrophoresis gel-type software. spoligotype analysis is a robust method with reproducibility of over 90%  <cit> , but it can be affected by the subjective determination of the hybridization signal, especially after repeated use of the membrane. hybridization detection is not an all-or-nothing process and slight variations in the quality and repeatability of results requires these images to be double checked by experienced staff. alternatively a fully automated multiplex bead-based luminex hybridization assay can be used  <cit> . whereas several papers have described novel data mining methods for spoligotype data  <cit> , to our knowledge none have developed software to facilitate the acquisition of the images and classification of spots on a complete spoligotype film generated by membrane based hybridization. commercial software packages have facilities for the semi-automated capture and classification of spoligo film images. the bionumerics platform has a 'character types' module which can process spot information from tif files. quantity one  allows images to be segmented to quantify spot information via the 'volume tools' function.

to support the spoligotype analysis of a tuberculosis case contact study in the gambia  <cit> , we used two supervised learning algorithms, a neural network  <cit>  and a support vector machine  <cit>  to categorize the hybridization signal as positive or negative. these algorithms were incorporated into a software package that automated the process of gridding, classification, expert verification and transcription of data from the scanned spoligo film directly into a database.

RESULTS
spoligotype analysis was performed according to a standardized method  <cit>  using commercially prepared membranes from isogen biosciences . the assay was repeated for isolates with spacers that were difficult to classify, and for those without any hybridization signal.

the spoligotype film was then scanned using a transparency enabled scanner  at a low resolution of  <dig> pixels per inch, with a  <dig> level greyscale. transparency enabled scanners have a light source in the lid of the scanner as well as the base. the images were scanned to tiff files , appropriately cropped and rotated so that the rows  were true horizontal. the automated processing of spoligo films can be divided into four main processes, automatic grid placement, feature extraction, supervised learning and data storage.

algorithm – automatic grid placement
there are two key problems with automatic gridding. firstly, due to the physical process of developing films the rows and columns are not necessarily orthogonal. secondly, the spoligotype films have small irregularities in the width of the spacer lanes and isolate rows, often with blank or smudged areas. to address these irregularities, we used a  <dig> dimensional autocorrelation of the spoligo image to identify non-orthogonal rows and columns. for an m by n image matrix x, the autocorrelation requires the calculation of:

  

where i = -m to m and j = -n to n and elements of x that lie outside its range are set to zero. image sizes tend to be approximately  <dig> by  <dig> pixels, which make the above summation computationally expensive. the autocorrelation is calculated by taking the inverse fourier transform of the product of the fourier transform of the image and its complex conjugate . figure  <dig> shows the image of a spoligo film with non-orthogonal rows and columns. the non-othogonality can be seen in the image autocorrelation shown in figure  <dig>  where the columns are not aligned with the true vertical yellow line. the angle from the true vertical for the columns can be calculated by finding the arc through the centre with the highest correlation. figure 3a shows an arc between ±  <dig> degrees and figure 3b gives the mean intensity sweeping in  <dig> steps from - <dig> to +  <dig> degrees, where the highest correlation occurs at an angle of  <dig>  degrees. to calculate the mean intensity along an arc, a 'nearest neighbour'  <dig> dimensional interpolation is used as the arcs do not pass exactly through a pixel.

the image could now be sheared to make the rows and columns orthogonal, but this creates an edge effect problem and the displayed image is no longer identical to the film. for these reasons the grid lines were rotated rather than the image.

the number of columns in a film is set at  <dig> and the number of rows has a maximum of  <dig>  given these parameters it is trivial to place an initial grid, allowing for the rotation affect. each grid line is then moved - <dig> to + <dig> pixels  from its initial position and the mean intensity for each position is calculated. white areas in the image have high intensity values and dark areas low values and the grid line is moved to the area of highest intensity.

the films typically have rows missing due to failed isolates or controls and these areas must be detected as noise because they have very few low intensity dark pixels. figure 4a shows the profile for the grid line between two well defined rows in comparison to figure 4b where the second row has very few defined cells. a bump hunting algorithm is used to locate the local maxima and the following threshold is calculated, where c is the number of maxima and mi are the mean intensities for each perturbed grid line.

  

for noisy data with no well defined dark cells the numerator tends to be large and the denominator small, and for the example in figures 4a and 4b, tnoise =  <dig>  and  <dig>  respectively. based on trial and error the threshold was set at  <dig> , which successfully relocated the initial grid lines for the  <dig> lines on the  <dig> films  analysed here, using the following criterion.

if c> <dig> and tnoise <  <dig> 

move grid line to the highest intensity peak

else

do not move grid line

after the optimum position for a grid line was determined the remaining lines were moved to maintain the initial separation and the process is repeated for each grid line in turn. figure  <dig> shows the automatic grid lines for the film in figure  <dig> and individual cells are defined by the four intersections of the relevant rows and columns. the automated software implementation allows any line to be manually moved, but the classification process was not sensitive to small movements in the grid lines.

algorithm – feature extraction and supervised learning
the success of any supervised learning technique depends on the ability of the features extracted to discriminate. there is no prescriptive rule for the determination of these features and the selection difficulty is compounded in this case by the wide range of film quality and the large number of cells that have to be processed. features that discriminate on low noise/high quality films might not be the same as those on films of inferior quality. given that the automated implementation has to run in real time, a small number of features should be extracted from each cell. initially simple summary statistics are extracted from each cell, i.e. the mean and the median pixel intensity for each cell.

the largest dark region within a cell is then selected, as follows. the intensity of the pixels is thresholded, by setting those below the lower quartile intensity for the cell to white and the remaining pixels to black. the largest of the resulting black regions is selected as the region of interest. the vertical and horizontal profiles of the cell pixels through the centre of this region are then extracted and smoothed. the profiles are smoothed using a b-splines smoother  <cit> , with a uniformly high degree of smoothing, to reduce the number of noisy peaks. the b-splines smoother is appropriate for a real time application as it has a fast computational implementation. the amplitude of each smoothed profile and the number of minima are calculated. positive cells typically yield profiles having well defined centrally located minima and large amplitudes, in contrast to negative cells.

due to the non-uniformity of the spot sizes, there can be edge effects caused by overlap from neighbouring cells. these cells can be identified using a symmetry index, calculated by taking the largest central square image from a cell and rotating it by  <dig> degrees and calculating the correlation coefficient of the pixel intensities with the un-rotated square image. cells with large central features tend to have higher correlation than cells with noise or non-symmetrical edge features. figures  <dig> and  <dig> compare the horizontal and vertical profiles for a positive cell with a central spot to those from a negative cell having an edge artefact. for these cells the most effective discriminator is the rotational symmetry coefficient, which is  <dig>  for the image in figure  <dig> and - <dig>  for the image in figure  <dig> 

this gives seven features which are fast to compute  and are able to discriminate between positive and negative cells from a range of different quality films. at the expense of information, the dimensionality of multivariate training sets can be reduced using principal component analysis  <cit> . for this application where the dimensionality of the features is small and a wide range of film qualities may have to be processed, there is little computational advantage in attempting to reduce the dimensionality.

based on a training set of the seven extracted features where the true classification of the cell is known, supervised learning algorithms can classify cells based on unseen sets of cell features. the implementation of two common supervised learning algorithms, a neural network and a support vector machine are described here and compared in the following section.

a feed forward neural network links inputs to outputs by a series of one way connections. the inputs are the seven features for each cell and there are two outputs representing a positive or negative cell. a unary encoding is used for the outputs in the form of a  <dig> element vector , where a perfect prediction is  <cit>  and  <cit>  for a positive and negative cell respectively.

between the input and output layers there is at least one set of neurons, which connects the inputs to the relevant outputs for the training set. more complex relationships can be modelled by increasing the number of neurons and/or increasing the number of layers between the inputs and outputs. there is no prescriptive method for determining the number of neurons or hidden layers and for the films classified here,  <dig> layers with  <dig> neurons in each layer gave good performance across all films. the number of layers and neurons are parameters that can be easily changed. the network was trained using back-propagation and the levenberg-marquardt algorithm was used to optimise the weights. one of the key problems with neural networks is overfitting, where the error on the training set is very small, but the network does not generalise well when used to process unseen data. to overcome this problem the training sets are divided into two, a training and a validation set. the validation set is classified at each iteration  during training, based on the weights obtained from the training set only. if the network over fits the data, the error on the validation set tends to increase. an early stopping rule is applied if this occurs for  <dig> consecutive iterations and the network based on the minimum of the validation error is returned. the requirement for the neural network to converge in a real time implementation restricts the size of the training set. simulation showed little improvement in classification performance for sets of more than 3% of the cells, split evenly between the training and the validation set.

support vector machines are closely related to neural networks. they construct a hyperplane separating the input vectors into two categories and so are ideal for binary classification problems. support vector machines have integral support to minimize overfitting by creating a soft margin between categories that allows some misclassifications. to allow non-linear separations a kernel function is used to transform the data and in contrast to determining the numbers of neurons and hidden layers for a neural network, only the appropriate kernel needs to be chosen for a support vector machine. simulation showed that a linear kernel function gave the most appropriate fit, with non-linear fits showing overfitting problems. in common with the neural networks simulation, a training set of  <dig> % of the cells was adequate. given the integral support for overfitting and the use of a linear kernel no further control for overfitting was necessary and a validation set was not required.

a neural network is initialized with a set of random weights and for repeat training sessions it will converge to different classifications from the same training set. this is in contrast to a support vector machine, which results in a one to one relationship between the training set and the classification.

algorithm – data storage and retrieval
the data from the classified films are automatically updated to an access data base, although any common database standard such as sql server or oracle could be substituted. each film is stored as a separate table, where each row is labelled with a unique isolate identifier and a row position identifier. to enhance data quality and portability, film images are stored directly within the database .

testing
the algorithms above were tested on ten films, where quality ranged from very high, to images with uneven exposure and noisy artefacts. the gold standard for comparison with the supervised learning algorithms was provided by manual classification by an experienced laboratory technician and verification by a senior scientist. visual confirmation by the above staff showed good automatic grid placement for the ten films and a neural network and a support vector machine were applied to each spoligotype film, using identical training sets.

the two classifiers were compared using 10-fold cross-validation. for a real time practical implementation, the training sets of  <dig> cells generated by10-fold cross-validation on a complete film of  <dig> spots are not feasible. to test the dependence of the classification on realistic training sets,  <dig> randomly selected training sets  consisting of  <dig> % of the cells , were generated. figure  <dig> shows  the percentage of correctly identified cells by cross-validation. the box plots in figure  <dig> summarize the distribution of correct classifications from the  <dig> simulations of smaller training sets for each film. the reduction in the accuracy of the classifiers when using small training sets is greatest in the poorer quality films. the median percentage discordance between the two classifiers for each film is shown by the blue line in figure  <dig> 

a rank based analysis of variance on the percentage of correct classifications, allowing for the clustering effect of film  <cit> , shows no significant difference  between the classifiers for the 10-fold cross-validation. for the data simulated from smaller training sets the support vector machine showed a border line significant improvement  over the neural network. the smaller training sets compared to those from the 10-fold cross-validation resulted in a significant  performance degradation for both classifiers. generally for realistic training sets the support vector machine gives slightly better classification, shows less dependence on the training set and is computationally faster to fit. in terms of real time software implementation for small training sets there is little functional difference between the two methods.

the best films have median correct classification rates above 99%. for the poorer quality films there is as expected a lower classification rate and greater dependence on the training set selection. the results in figure  <dig> are conservative as the training sets were chosen randomly and there was no constraint to include training cells having diverse characteristics. in reality decisions about difficult to classify cells may depend on neighbourhood cells and exposure artefacts, which is why the software implementation facilitates user re-classification. the worst performance occurred for the third film, which has a very uneven spot exposure making classification difficult even by manual inspection.

the outputs from either a neural network or a support vector machine can be used to highlight the quality of cell classifications. the output from the neural network is a vector with  <dig> rows and the same number of columns as cells classified. each column is an estimate of the classification, where  <cit>  and  <cit>  are perfect positive and negative predictions and well classified cells are polarized in these directions. the output from a support vector machine is the distance from the hyperplane and the sign of the distance determines the category of a cell. figure  <dig> shows the histogram of the first row of the output vector from a neural network for the third film, representing the poorest classification. there are clearly some values around  <dig> , indicating poor classification. cells within this region can be tagged as likely to be poorly classified. in a similar manner for a support vector machine the distances from the hyperplane clustered around  <dig> could be used to identify difficult to classify cells.

there are many different image processing morphological operations and filters that attempt to improve the quality of image, but these had little affect on the classification error for the poorer quality films. the disadvantage of image transformations is that the film image can be noticeably different from the actual film, which doesn't promote user confidence. the only transformation that was universally applied and accepted was a simple contrast stretch.

implementation
software for the preceding applications was developed in matlab , which offers a rich program development environment and dedicated image processing and neural network toolboxes. matlab is compatible with mac osx, linux and windows operating systems. although it is ideal for developing and testing code, the language is interpreted, which makes some of the more intensive numerical operations relatively slow, affecting the user-friendliness of this interactive software application. code written and compiled in c++, can be incorporated into matlab applications, resulting in faster execution speed, often greater than  <dig> fold. the fast fourier transform, two-dimensional interpolation, and the morphological operations are written in c++.

the code is controlled from a graphical user interface and films are defined by their image name, where the actual image can be in any of the common graphical file types. the code then runs  in the following order :

 <dig>  user selects film for processing from file browser

 <dig>  automatic grid placement 

 <dig>  user can manually move grid lines if required

 <dig>  extracts features from all cells 

 <dig>  user chooses training set and marks any regions excluded from classification

 <dig>  user chooses classifier 

 <dig>  trains and displays classified film 

 <dig>  user verifies and edits classification

 <dig>  writes classification and film image to a microsoft access database 

even though the process is completely automatic, the user can override any of the computer generated results. every line of the grid is a selectable graphics object and the lines can be moved forward, backwards, up or down one pixel at a time. after classification any of the cell classifications can be over-written before updating the results to a database. connection to the access database is via an object linking and embedding database connector and all database actions are automatically written from the matlab code using sql , so no knowledge of access is required. rather than storing a link to the spoligotype film image; for integrity and database portability the actual film image is stored directly in the database. this requires the image to be converted to a blob  and written from matlab to an ole  field in the relevant database table. a typical resolution spoligotype film after resizing by a factor of a quarter consumes  <dig> kb of space in the database table. storage within a single access database is only limited by the product limit of  <dig> gb, which would not affect a server sided database such as an sql server.

the results in figure  <dig> were obtained using randomly selected training sets, but intelligent choice of the training set is likely to give superior performance. the film in figure  <dig> is film  <dig> from figure  <dig> and had a median correct classification of  <dig> %. the support vector machine classification displayed in figure  <dig> correctly classified  <dig> % cells with  <dig> cells having weak classification , including  <dig> incorrectly classified cells.

films can be re-classified at any time and users are warned before updating any existing classifications in the database. an additional graphical tool displays previously classified films and allows cell classifications to be viewed and if necessary edited.

discussion
we developed a method for automating the transcription of spacer classification from spoligo films. with a large throughput of films from the laboratory, the process had to offer a high degree of automation, but allow manual re-classification and provide a systematic method for storing the data. there appear to be no commercial packages exclusively for the processing of spoligo films.

matlab is a high level programming language with excellent graphical capabilities, dedicated image processing, supervised learning tools and sophisticated native support for database connectivity, which make it ideal for processing and managing spoligo film data. c++ can also be incorporated into matlab programs, allowing this application to run within a realistic timeframe.

after scanning, the processing of the spoligofilms was divided into four categories, grid application, classification, manual verification and data storage. the automated processes run in less than  <dig> minute, which is a considerably less than manual processing. the implementation described here uses either a neural network or a support vector machine as classification tools. they provide robust classification, with the support vector machine showing superior performance on realistically sized training sets. in terms of the number of cells correctly identified it is of little practical relevance. given the speed of film specific classifiers there is little advantage in using the same global classifier for multiple films. there are many other types of classification algorithm, which could easily be incorporated into the modular code.

the matlab source code is available for download from the 'statistics and data management' page of the gambia mrc website  and from the corresponding author. it requires the image processing, neural network and bioinformatics  toolboxes. bespoke code or alternative classifiers could be used to replace the reliance on toolboxes. data is currently output to a microsoft access  <dig> database, but there is native support for all the database systems most commonly used in medical research. the modular nature of the code makes it straightforward to add additional output formats. for example the input format for the spoltools software  <cit>  is text with format "name: binary pattern: cluster size" . the matlab code stores output data in a two-dimensional matrix which can easily be written to a text file, from where it could be pasted into the spoltools page. although this paper has concentrated on films with  <dig> spacers, the only limitation on the addition of further spacers  <cit>  is the size of the scanner.

many aspects of the gridding, feature extraction, and supervised learning are generally applicable to other laboratory image-based analyses, and could be adapted to the analysis of microarray or well-plate images. further research will optimise the software implementation, explore the development of platform independent executable code and compare the speed and accuracy of traditional manual data entry against classification with the automated method.

CONCLUSIONS
we developed a user friendly software package that can capture and classify data generated by reverse hybridization methods, such as spoligotype analysis. although fully automated the software allows for manual editing, before uploading the data to an access database. tools are also provided to visualize existing data and make retrospective changes. the software is publicly available, and potential users can contact us for assistance, modification and additions to the software.

availability and requirements
the matlab  source code, user guide and anonymous test film images are available from the 'statistics and data management' page of the gambia mrc website  and the corresponding author. note the source code requires the image processing, neural network and bioinformatics toolboxes.

authors' contributions
djj developed the algorithms and the matlab code. na researched image processing algorithms and advised on the application. bcdj highlighted the issues with manual processing and extensively tested the application. all three authors contributed to the writing of the paper.

