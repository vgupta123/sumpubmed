BACKGROUND
gene expression profiling has been proved to be a valuable resource for classification of complex diseases such as cancer. many studies have showed it is possible to extract compelling information from microarray data to support clinical decisions on cancer diagnosis, prognosis and response to treatment  <cit> . however, like other high-throughput studies, microarray data pose great challenges to accurate prediction in two respects. on one hand, there is a large amount of inherent noise and variability in samples, due to biological variations and experimental conditions, both of which can degrade prediction performance. on the other hand, difficulties also arise from high dimensionality , as compared to a relatively small sample size , which leads to the risk of over-fitting in many machine learning methods. this occurs especially in cases when the few training samples are not good representatives of classes, so that the classifier may learn inherent noise from irrelevant features in training data, leading to poor generalizability. thus algorithmically speaking, there is a need for feature selection in order to improve model performance and avoid over-fitting.

generally, feature selection methods in the context of gene expression studies can be divided into three categories: filter methods, wrapper methods, and embedded methods  <cit> . filter approaches  <cit>  involve calculating feature relevance scores, and selecting a subset of high-scoring features as input to the classifiers after removing low-scoring ones. they are computationally efficient and thus widely used. these approaches are also independent of classifier algorithms. on the contrary, wrapper and embedded approaches interact with classifiers by either wrapping around the classifiers  <cit> , or being built within the classifier construction  <cit> . as a result, those approaches are usually much more computationally intensive than filters, and sometimes become so costly as to be impractical without pre-reduction of the search space with a filter method. whereas filter techniques are frequently univariate in nature, assuming the features are independent and ignoring feature dependencies, wrapper and embedded techniques select features in a multivariate fashion by taking feature correlations into account, an approach which is certainly biologically relevant if we consider, e.g., how genes are co-regulated in pathways.

as more feature selection techniques are explored, computational efficiency and the ability to capture feature interactions remain important considerations. in  <dig>  geman et al introduced the top-scoring pair  classifier  <cit> , which was then refined and extended to k-tsp by tan, et al  <cit> , using k pairs of top ranking genes to build the classifier. there are several things worth noting in the feature ranking algorithm they employ. first, they replace actual expression levels with their ranks in each sample, which reduces inherent noise and is invariant to normalization procedures across platforms. second, the idea of "relative expression reversals", on which the pairs are scored, captures a key mechanism of the disease process, which involves large differential changes of expression levels in up-regulated and down-regulated genes. third, this scoring algorithm is multivariate at different levels, in the sense that bivariate interactions between features are exploited when genes are evaluated in pairs, and higher order interactions are addressed when the final ranking is derived from comparison of scores of all possible pairs. last, by choosing reversed expression pairs, it is guaranteed that features from more than one cluster are collected, and the top-ranked features coming from different clusters can contribute orthogonal information to the classifier. with all the above unique qualities in its feature selection, k-tsp turns out to be a powerful classification method using simple comparisons, and in some cases, it is able to extract a single pair of genes for accurate diagnostic prediction. nonetheless, given all its success in cancer diagnostic prediction, it has been noted that the performance of k-tsp is not always robust for some more difficult types, such as those involving cancer outcome prediction. this could be due to the complexity of data on one hand, and the relatively simple voting scheme of k-tsp classifier after the feature selection step on the other hand.

compared to other complex machine learning methods, the support vector machine  is relatively insensitive to high dimensionality  <cit> , though its performance can still improve considerably after feature selection  <cit> . guyon et al. proposed the recursive feature elimination algorithm   <cit> , using the svm weight vector to rank component features and discarding those with small weights in a recursive manner. this strategy is a multivariate approach embedded in the construction of the svm classifier, and like other embedded methods, is relatively computationally intensive. recently, yoon et al reported their comparison of feature selection methods in combination with svm in a number of cancer diagnostic datasets  <cit> , suggesting that tsp may be an effective filter method candidate for other classifiers, if computational cost is not a concern.

in this paper, we describe a hybrid approach that integrates the tsp scoring algorithm into other machine learning methods such as svm and k-nearest neighbors . a particular focus is placed on assessing this approach in a controlled environment by using simulated datasets with known properties, including correlation structures of informative genes , the variance distribution in all genes, signal strength and sparsity, and sample size in the training set. we also apply svm+tsp to four cancer prognostic datasets, and show that it achieves superior performance to k-tsp, and either outperforms or compares to that using svm alone.

in general, different methods have their respective strengths and weaknesses in dealing with different aspects of data complexity, and a hybrid approach combining one algorithm with another can be beneficial  <cit> . this is also demonstrated in our study, both in real and simulated data, where sophisticated classifiers are enhanced by the feature selection scheme carved out from another learning algorithm.

RESULTS
simulated datasets
simulation process
to investigate how feature selection methods respond to different data structures, we generated two types of data, with variations in aspects such as the strength of differentially expressed genes , the sparseness of signal genes, and covariance structure. the basic model is as follows: each sample contains  <dig> genes, of which  <dig> are signal genes. the signal genes follow the multivariate normal distribution n for class  <dig>  and n for class  <dig>  with μ being a vector of  <dig> distinct values ranging from - <dig>  to  <dig>  with an increment of  <dig>  or  <dig> , each value being the effect size of  <dig> differentially expressed genes . the rest of the genes  follow independent n distributions for both classes. for each simulation experiment,  <dig> independent samples were generated , from which  <dig> were randomly selected as the training set, with the remaining  <dig> used as the test set.

based on this general model, the first type of data  has all of the signal genes in one block Σ. the signal genes are sampled under an independent model or a correlated model. for the correlated model, the block has a compound symmetry  structure with fixed variances of  <dig> and a common correlation coefficient ρ, as shown below:

 ∑=1ρρ…ρρ1ρ…ρ⋮⋮⋱⋱⋮ρρ…ρ <dig> 

in the second type of data , the signal genes come in  <dig> equal blocks, each consisting of  <dig> genes with a distinct value of effect size, and a cs correlation structure Σi = Σ, so each Σi has an equicorrelated structure. the blocks can be correlated among themselves by introducing an inter-block correlation coefficient ρ', which is always smaller than the within-block correlation ρ to ensure the covariance matrix is positive definite.

to mimic various situations in real datasets, we generated different variants of data-i: data-1b, data-ic, and data-id. data-ib is constructed based on the randomized variance model, with the variances drawn from an inverse gamma distribution, instead of taking on a fixed value of  <dig>  data-ic and data-id reduce the proportion of signal genes from 10% to 1%, with data-ic containing the same μ <dig> vector as that in data-i, and data-id containing a μ vector with a larger effect size, which has half of the values in the vector equaling - <dig> , and the other half  <dig>  .

comparison of tsp with fisher and rfe as feature selection methods
we first compared the performance of svm on data-i and data-ii, using tsp, fisher and rfe as feature selection methods. in each experiment, we applied the tsp, fisher and rfe feature ranking algorithms to rank the genes, built svm models with each level of selected genes on the training data, and then tested the models on the test set. experiments were repeated  <dig> times to generate averaged results. two aspects of performance are evaluated in the test data: the classification error rate at each level of gene selection, and the percentage of true signal genes recovered among the  <dig> and  <dig> top-ranked genes.

the error rates  are shown at various selection levels as correlation varies among signal genes, using tsp, fisher and rfe as feature selection methods

when the signal genes are in the multi-block structure of data-ii, we do not observe the differential responses of the three feature selection methods as the signal genes within each block  become more correlated. however, if an inter-block correlation ρ' is introduced among all the blocks, we observe similar pattern of differentiated responses among the three methods. in figure 1b, tsp, fisher and rfe have comparable performance when the blocks are uncorrelated . nevertheless, when the blocks become correlated with one another, the effectiveness of these three methods diverges. the differentiation is more pronounced in the presence of a strong inter-block correlation , where tsp is significantly better than fisher and rfe at most selection levels.

tsp, fisher and rfe are also applied as feature selectors for another benchmark classifier k-nearest neighbors , with k =  <dig> as the number of nearest neighbors. as seen in figure  <dig>  again tsp, fisher and rfe perform comparably when the signal genes in data-i are independent, and all three improve on the performance of knn using the entire set of features. as the correlation among signal genes become increasingly stronger, the performance of tsp starts to set apart from fisher and rfe at ρ =  <dig> , and is superior to the other two at most selection levels at ρ =  <dig> , with the gap further widened at ρ =  <dig> . in parallel, rfe also increases its performance in response to correlation, but is out-raced by tsp.

in parallel with classification performance, the recovery of signal genes among the top-ranked genes by tsp, fisher and rfe displays a similar trend in response to the increased correlation among the signal genes. we examine the percentage of signal genes in  <dig> and  <dig> top-ranked genes, based on the observation that most signal genes are recovered within top <dig> genes, and the recovery is well differentiated among the three methods in the top  <dig> and  <dig> genes. figure  <dig> shows the percentage of the signal genes in the  <dig> and  <dig> top-ranked genes selected by the three ranking algorithms in data-i and data-ii. in single block structure , it can be seen that when the signal genes are uncorrelated, fisher recovers slightly more signal genes than tsp and rfe . as the correlation of signal genes progresses from  <dig>  to  <dig> , tsp gradually out-races fisher to be the one that recovers most signal genes , while the recovery rate for fisher remains unchanged, and that of rfe increases to a smaller extent . a similar trend is observed for multi-block data structures , where the presence of inter-block correlation turns tsp into the leading feature selector for recovering signal genes.

comparison of k-tsp+svm with k-tsp and svm
we compared the classification performance of tsp family classifiers, with our hybrid scheme using k-tsp as feature selection for svm  in data-i and data-ii, as well as some variants constructed based on data-i . in each experiment, the tsp ranking algorithm was used to rank the genes and build the model on training data at each level of selected genes through a standard leave one out cross-validation  procedure. the level that achieved the minimum loocv error rate was chosen as the size of gene subset, with which the classifier is built on the entire training set and then applied to the test data. experiments were repeated  <dig> times to generate averaged test error rates, which were used to evaluate the performance of a classifier.

the classification error rates  of various classifiers as correlation varies among signal genes in a) data-i of fixed variance vs. random variance when signal genes are abundant ; b) data-i of strong signal vs. weaker signal when signal genes are sparse ; and c) data-ii of independent blocks vs. correlated blocks. the lowest error rates for each dataset are indicated in bolded.

to investigate the impact of sparsity of the signal genes on classification, we created data-ic and data-id, which only contain one tenth as many signal genes as in data-i. interestingly, it is shown in table 2b that as the percentage of signal genes is reduced from 10% to 1% in data-ic, the datasets become difficult for all the classifiers and none appears to be effective regardless of the presence of correlation. however, when the signal strength of the signal genes is increased from μ <dig> to μ3b in data-id, k-tsp+svm steps over the others again, showing more robustness in rapidly improving its performance with increased correlation, and outperforming k-tsp and svm at ρ =  <dig>  , and ρ =  <dig>  .

when signal genes are organized in multiple block structures, in which signal genes are correlated within each block , a disparate picture emerges . when the blocks are uncorrelated with one another , the performance of all the classifiers degrade drastically, and k-tsp+svm does not show any advantage. in contrast, when the blocks are correlated , each classifier significantly improves its performance, with k-tsp, and k-tsp +svm achieving comparable best performances .

the effect of sample size in training data
in many microarray studies, sample sizes in training sets are usually limited. it has been suggested that the tsp ranking algorithm is sensitive to the perturbation of training samples  <cit> . to assess this effect by simulation, we generated datasets of data-i with sample sizes of  <dig>   <dig>   <dig> and  <dig> in the training sets, with signal genes moderately correlated . experiments were repeated  <dig> times to generate averaged results. as shown in figure  <dig>  all the classifiers improve their performance as the sample size increases. when the training size is only  <dig>  the performances of all classifiers deteriorate, indicating it almost becomes impossible to train a classifier with such a small training size. as the training set becomes larger, tsp and fisher+svm appear to be significantly less effective than the rest, in which k-tsp+svm is relatively comparable to others. as the sample size reaches  <dig>  k-tsp+svm  rises above all others, significantly outperforming tsp , k-tsp , svm , fisher+svm , and rfe+svm  .

real datasets
cancer prognostic datasets
we applied the above hybrid scheme k-tsp+svm to four cancer prognostic datasets, all of which are available on our project website, and the information of these datasets is summarized in table  <dig>  the first dataset is van't veer's breast cancer dataset  <cit> , obtained from rosetta inpharmatics, which is already partitioned into training and test data. the training data consists of  <dig> patients,  <dig> of whom developed distant metastases or died within  <dig> years , with the rest consisting of those remained healthy for an interval of more than  <dig> years . the test data consists of  <dig> patients,  <dig> with poor prognosis and  <dig> with good prognosis. since this dataset contains many missing values, certain pre-processing was performed. first, two samples  with more than 50% of missing gene values in the training data were removed. next, any genes whose value was missing in at least one sample was discarded, amounting to a total of  <dig> % of all genes. the log-transformed ratio of the two channels was used for analysis.

another breast cancer dataset is derived from wang et al  <cit> , and contains a subset of er-positive, lymph-node-negative patients who had not received adjuvant treatment. we used the raw intensity affymetrix cel files and normalized the data by rma procedures using bioconductor packages http://www.bioconductor.org, obtaining a final expression matrix comprising  <dig> features and  <dig> samples. again patients who developed distant metastases or died within  <dig> years are classified as poor prognosis subjects, and those who remained healthy for more than  <dig> years as good prognosis ones. the dataset consists of  <dig> patients with poor prognosis, and  <dig> with good prognosis

the other two cancer prognostic datasets are obtained from the cancer dataset depository of the broad institute. one is a dataset of  <dig> patients with primary lung adenocarcinoma, which consists of  <dig> patients who were alive, and  <dig> patients who had died  <cit> . the other is a dataset of  <dig> patients with medulloblastomas, which consists of  <dig> survivors and  <dig> treatment failures after radiation and chemotherapy  <cit> . both datasets are pre-processed and contain  <dig> genes.

application to cancer prognostic datasets
the classification performance of k-tsp+svm is compared with k-tsp and svm in the three cancer prognostic datasets. we used the independent test set when it was available from the original dataset ; otherwise we performed 5-fold cross-validation and averaged the results from two 5-fold experiments.

in the van't veer breast cancer dataset where there is an independent test set, the error rate on the test set was obtained at the gene selection level at which the training set achieves its minimum loocv error rate. in the other datasets where there is no separate test set, the error rates  were obtained from two experiments of five-fold cross validation.

meanwhile, we compared tsp for feature selection with fisher and rfe, using both svm and knn as classifiers in some datasets. for the breast cancer dataset where a separate test set is available, the error rate was obtained directly on the test set at selected levels  of genes. for the lung adenocarcinoma and medulablastoma datasets, on the other hand, standard loocv error rate was estimated at selected level of genes. interestingly, different patterns of response are observed in different datasets when feature selectors are combined with a classifier. in the breast cancer dataset , tsp has significantly lower error rates than fisher and rfe at most levels of selected genes, using either svm or knn as the classifier. as compared to  <dig> % achieved by svm without gene selection, the lowest error rate of  <dig> % is achieved for k-tsp+svm on the top  <dig> pairs. in parallel, as compared to  <dig> % achieved by knn without gene selection, an error rate of  <dig> % is achieved for k-tsp+knn using the top  <dig> pairs. on the other hand, in the lung adenocarcinoma dataset , the performance of tsp as a feature selector is set apart from that of fisher and rfe mainly within the top  <dig> pairs. k-tsp+svm achieves its minimum error rate of  <dig> % using the top  <dig> pairs, which is a sizable improvement upon the  <dig> % error rate by svm without gene selection. finally, the medulablastoma dataset presents yet another scenario . none of the feature selection methods appears to be effective, and no improvement is observed at any level of selected genes as compared to the performance by svm without gene selection.

discussion
results from simulated and real datasets show that the k-tsp feature ranking algorithm can be integrated usefully into machine learning classifiers for feature selection. the hybrid algorithm outperforms k-tsp, and in some cases the corresponding classifier using all features in cancer prognostic datasets. simulation studies suggests that with certain data characteristics, this ranking algorithm appears to be a superior feature selector to the univariate fisher method, as well as the multivariate recursive rfe procedures embedded in svm.

in assessing the effectiveness of a new method, simulated and real datasets play complementary roles. simulated datasets with known properties can be used for exploring the robustness and parameter space of a given method, and for studying the influence of data characteristics on its performance. it can also provide insights regarding its strengths and weaknesses in real situations. yet, due to the largely unknowable properties of gene expression data, e.g. the true distribution of expression values across genes in different biological states or the exact correlation structures within and among all the gene networks, simulation is usually an over-simplified representation of real scenarios, and unavoidable biases can be introduced by specified distributions and model assumptions. for that reason, the effectiveness of a method suggested by simulation needs to be validated on real data. on the other hand, the sample size limitation in real datasets may impede the detection of the true data structure, and thereby the demonstration of the advantage of a method in tune with that structure, whereas simulation allows generation of a large number of samples for the full manifestation of data characteristics.

by constructing simulated datasets with various structures, we were able to observe how different feature selection methods interact with different data properties, including the correlation structure in signal genes, signal strength and sparsity, as well as sample size in the training set. indeed, our simulated data sets d fall into a parameter space a whose dimensions consist of  the total sample size n,  the proportion q of signal genes,  the signal strength s of such genes,  the number n of blocks,  inter-gene correlation ρ within blocks, and  inter-block correlation ρ'. within the space a this study can be viewed as a monte-carlo procedure determining which data sets d within a are best tuned to a feature selection method in combination with a classifier. in theory, extension of this procedure to a fuller exploration of the space a may lead to the possibility of taking a biological dataset d' and determining  a point d in a which d' falls closest to, if the above parameters from d' can be empirically measured, and thus estimating which combination of feature selection and classifier is the best match to d'. however, there remain many challenges in mapping a real dataset in the parameter space, one of them being the attempt to extract the true correlation structures within and among all gene networks, especially in cases of small sample sizes.

among the correlation structures considered here, the simplest version is the single-block design with all signal genes in one covariance matrix with uniform inter-gene correlation. in this case we found that tsp, fisher and rfe perform comparably when the signal genes are independent . however, as the signal genes become increasingly correlated, tsp appears to improve increasingly over fisher and rfe, both in terms of classification accuracy and the recovery of signal genes . it is notable that the univariate fisher method seems to be steady regardless of correlation, so that its performance becomes inferior to the other two as the correlation progresses. this indicates that correlated data are more in tune with multivariate methods such as tsp and rfe, which select features based on the joint information from multiple signal genes, rather than the differential expression of individual signal genes. interestingly, between the two multivariate approaches, it is the simple tsp algorithm, which is less computationally costly, that responds to the correlation more robustly and achieves a better performance. a similar trend was also observed in a more complex version involving multi-block design, with signal genes divided into  <dig> covariance blocks. as these blocks become increasingly correlated with one another, tsp seems to become an increasingly superior feature selector to fisher and rfe . it is worth mentioning that in an extensive study comparing univariate and multivariate feature selection methods on seven biological datasets, lai et al. found that most of the multivariate ones do not result in improvement over the univariate ones  <cit> . the above simulation result in fact does reveal an advantage of a multivariate approach in the presence of data correlations, and it is worth investigating why this has not been observed correspondingly in real datasets, where many differentially expressed genes are co-regulated in pathways. one possible reason could be, as the authors stated, the limited sample sizes in real datasets, which makes a correlation structure difficult to extract.

therefore, correlation among signal genes influences the performance of various classifiers using tsp algorithm for feature selection. as shown in table 2a, k-tsp+svm outperforms k-tsp when the signal genes are independent. as the correlation among signal genes rises, the performances of both classifiers are increasingly improved, mainly due to the increasing effectiveness of their shared ranking algorithm. thus the difference between k-tsp+svm and k-tsp tends to taper off as signal genes become more correlated. svm alone, on the other hand, appears to remain constant in response to the increased correlation. as a result, the difference between k-tsp+svm and svm is enlarged as signal genes become more correlated.

an interesting notion was introduced by jin et al.  <cit> , showing that in the parameter space of real data, there exists a region in which successful classification is virtually impossible, this region being jointly determined by the fraction q of discriminating features , the strength s of those features, and the ratio of sample size n to feature number p. this is significant because it establishes a limit on separability for datasets that are difficult to classify. in our simulation experiments, we manipulated the parameter space by varying the fraction and strength of signal genes, the training set sample size, and correlation among signal genes, so as to observe how classifiers respond to the changes in the parameter space. for example, in data-i, where the signal strength is relatively high and signal genes relatively abundant , k-tsp+svm seems to perform significantly better than k-tsp, svm, and other classifiers in most cases, especially when signal genes are correlated . however, when signal genes become very sparse  in data-ic, k-tsp+ svm loses its advantage over the other classifiers in either uncorrelated or correlated data, and its performance deteriorates severely like all of the others. nevertheless, when the sparse signals increase their signal strength in data-id, k-tsp+svm regain it robustness and superiority to other classifiers in correlated data . finally, the sample size in the training sets proves to be crucial. in data-i with signal genes moderately correlated, k-tsp+svm significantly outperforms all the other classifiers when sample size is relatively large , but only slightly outperforms k-tsp and svm when the sample size becomes smaller , and totally losses its advantage when sample size is very small , at which point the performances of all classifiers deteriorate .

the above results suggest that as datasets fall closer to the region of inseparability, in which features are rare and weak, or the sample size is small, k-tsp+svm losses its superiority with respect to other classifiers, and in fact no classifier built from the data themselves is likely to separate the two classes well.

in actual cancer microarray datasets, the data characteristics, and as a result the difficulty of classification, largely depend on the types of data. in general, diagnostic datasets usually contain a set of salient pathophysiological entities that can be easily used to distinguish between cancer and normal tissues with a number of algorithms. prognostic datasets, on the other hand, are more challenging, since the samples with poor and good prognoses often share the same pathophysiological characteristics, and the features that differentiate between the two classes are relatively sparse and not well defined. our observations as well as those of others show that compared to its robustness in cancer diagnostic datasets, k-tsp seems to be less successful in datasets involving cancer outcome prediction. this may partly be due to the relatively simple voting scheme that does the decision-making of the classifier, given that the feature selection algorithm is very effective. thus we believe that in such cases performance can be improved with a hybrid scheme, in which the k-tsp ranking algorithm is combined with a powerful and multivariate machine learning classifier such as svm. this is confirmed in the breast cancer dataset , where the test error is reduced from  <dig> % with k-tsp to  <dig> % with k-tsp+svm. notably svm benefits from the feature reduction as well, since with the entire set of features its error rate is  <dig> %. the performance of k-tsp is also significantly improved with k-tsp+svm in the lung adenocarcinoma and medullablastoma datasets , although in both cases svm alone achieves comparable performances. on the other hand, consistent with what is observed in simulated data with correlated signal genes, tsp is a superior feature selector to fisher and rfe in both the breast cancer and lung adenocarcinoma datasets .

CONCLUSIONS
an effective feature selection method is crucial in classification and prediction of complex diseases through gene expression analysis. we integrated the feature ranking algorithm of k-tsp with multivariate machine learning classifiers, and evaluated this hybrid scheme in both simulated and real cancer prognostic datasets.

we compared the tsp ranking algorithm with a univariate feature selection method fisher, and a multivariate method rfe in simulated data. in the model where the signal genes are uncorrelated, the three feature selectors perform comparably in terms of classification accuracy, with fisher recovering more signal genes. in the models where signal genes are increasingly correlated, however, tsp increasingly outperforms fisher and rfe, both in terms of the classification accuracy and recovery of signal genes. we also observed that as classifiers, k-tsp+svm outperforms k-tsp in most cases, and significantly improves the performance of svm alone when signal genes are correlated.

this hybrid scheme was applied to four cancer prognostic datasets. k-tsp+svm outperforms k-tsp in all datasets, and achieves either comparable or superior performance to that using svm with all features. as observed in simulated data, tsp appears to be a superior feature selector to fisher and rfe in two datasets.

we conclude that the tsp ranking algorithm can be used as a computationally efficient, multivariate filter method for feature selection in machine learning. simulation studies suggest that this algorithm is better tuned to correlated signal genes, the implication of which should be further explored in real datasets, where differentially expressed genes act in concert due to pathway dependencies. moreover, as sexena et al. showed  <cit> , many pathways include both up- and down-regulated components. as a ranking algorithm that is very effective in capturing up-regulated and down-regulated genes simultaneously, tsp ranking can possibly be used as an alternative method to generate the rank list in gene set enrichment analysis, which may reveal a unique profile of enriched sets of genes.

our preliminary work in single enrichment analysis suggests that, among the subsets of top-ranked genes selected by the three feature selectors from lung adenocarcinoma dataset, those by tsp are most relevant to cancer related pathways. we plan to explore this further to see if tsp ranking algorithm has a distinct advantage in revealing important signatures genes in some real datasets.

