BACKGROUND
several large and detailed mathematical models for signal transduction pathways exist in the literature. lipniacki et al.  <cit>  model the nf-κb pathway using  <dig> state variables and  <dig> parameters. yamada et al.  <cit>  introduce a system of ordinary differential equations that describe the jak-stat pathway with  <dig> state variables and  <dig> parameters, and schoeberl et al.  <cit>  describe the egf pathway with a model that comprises  <dig> variables and  <dig> parameters. models of this kind can provide a concise and unambiguous representation even of very complex signaling pathways. however, since usually some of their parameters are not known, these models pose demanding parameter estimation problems. two fundamental problems must be considered in this context: 1) the larger the number of unknown parameters in a model, the larger the amount of quantitative data necessary to determine meaningful values for these parameters. 2) even if appropriate experimental data are available, model parameters may not be uniquely identifiable  <cit> . ultimately, reliable predictive mathematical models can only be created after addressing these two problems.

parameter identifiability has been paid little attention to in the recent systems biology literature. exceptions exist, however. swameye et al.  <cit>  carefully check the identifiability of their jak-stat model. the authors propose a small model  and demonstrate that the model provides quantitatively reliable predictions despite its small size. in the field of hiv/aids modeling differential algebraic techniques have been used to prove identifiability  <cit>  for models of similar size. other examples can be found in  <cit> .

the problem of testing identifiability amounts to answering the following question: given a mathematical model of a system together with system input and output data, are the model parameters uniquely determined? the identifiability tests investigated here can be carried out before experimental data are available. to this end a model is used first to generate simulated data. subsequently, it can be checked whether the model parameters are uniquely defined by the simulated data. only if identifiability can be assured for the model and the simulated data it is reasonable to continue with lab experiments, identifiability tests with experimental data, and, eventually, parameter estimation. in the present paper we exclusively use simulated data.

several notions of identifiability exist. we attempt a brief and informal summary of the essential ideas here. stricter terminology is introduced in subsequent sections. essentially, a model is called globally identifiable if a unique value can be found for each model parameter such that the model reproduces the measured or simulated output data. if, in contrast, a finite number of points in the model parameter space can be found, for which the model reproduces the output data the model is called locally identifiable. finally, if an infinite number of parameter values exists that reproduce the model input-output behavior, the model is considered to be unidentifiable. independently of these three notions we need to distinguish between at-a-point identifiability on the one hand and structural identifiability on the other hand  <cit> . these two concepts distinguish two classes of methods for identifiability testing from one another. methods for at-a-point identifiability testing can only be applied if candidate values for the model parameters are known a priori. this situation arises, for example, when parameter values for a model have already been published in the literature, as is the case for the signaling pathway models investigated here. in contrast, all mathematically and biologically possible parameter values must be considered as candidate values, if no information on the model parameter values is known a priori. in this case we speak of structural identifiability testing.

several techniques exist for analyzing structural identifiability. these methods are based on power-series expansion  <cit> , transfer function analysis  <cit> , differential algebra  <cit> , interval arithmetics  <cit> , state isomorphisms  <cit>  or semi-infinite programming  <cit> . these methods are, however, either restricted to linear models or to models with less than  <dig> states and parameters in the nonlinear case  <cit> .

for large nonlinear models only methods for testing local at-a-point identifiability are feasible. a number of methods to test local at-a-point identifiability have been proposed in the literature. some of these aim at determining the largest subset of identifiable parameters  <cit> , other methods are tailored to finding the unidentifiable parameters  <cit> , or to finding parameters that do not affect the input-output behavior of the model  <cit> . all these methods are based on the sensitivity matrix of the model responses . in contrast to the approaches mentioned so far the method introduced in  <cit>  does not depend on sensitivity information. this method repeatedly estimates parameters with randomly chosen start values and extracts dependencies between parameters with a statistical method. the data used for the parameter estimation steps is created by simulating the model at a nominal parameter point. the approach does not belong to the class of methods for at-a-point identifiability testing, since it is delocalized by using the above mentioned random multistart approach for parameter estimation. consequently, the method is more rigorous than the discussed at-a-point identifiability tests, but not as rigorous as the structural methods.

the examples treated here turn out not to be locally at-a-point identifiable, therefore it is not reasonable to consider stricter concepts of identifiability. in the sequel the term identifiability will refer to local at-a-point identifiability if not noted otherwise.

we compare the methods for identifiability testing introduced by yao et al.  <cit> , jacquez and greif  <cit> , and degenring et al.  <cit> , which for ease of reference we refer to as the orthogonal method, the correlation method, and the principle component analysis  based method, respectively. specifically, we would like to establish which of these approaches is the method of choice for identifiability testing of large signal transduction pathway models. we omit the method published by brun et al.  <cit>  due to its combinatorial complexity. similarly, the method proposed by hengl et al.  <cit>  is omitted, because it proved to be too computationally demanding for the examples treated here. the method was tested with default settings. when applied to the smallest of the models treated here, the wall clock computation time was about  <dig> hours in contrast to wall clock computation times of less than a minute for the methods compared here. when applied to the jak-stat model, the method proposed by hengl et al.  <cit>  did not finish within a week. it was therefore not included in the comparison. finally, we note an interesting approach has only very recently been proposed by chu and hahn  <cit> . since this approach solves a problem that is similar to, but after all different from, the identifiability tests addressed here, it is not included in the comparison. all results are compared to results found with our own method  <cit> , which we refer to as the eigenvalue method for short. the eigenvalue method is an extension of an approach published by vajda et al.  <cit> . we note the eigenvalue method has independently appeared in a recent paper by schittkowski  <cit> .

this paper has three contributions: 1) we reveal that three well established models of signal transduction are not identifiable, and demonstrate how results from identifiability studies can be used to simplify these models. 2) we suggest an efficient numerical method for identifiability testing of large nonlinear systems of ordinary differential equations in general, and 3) we compare our method to three previously published methods.

this paper is structured as follows. we start by reviewing the theory of identifiability and introduce the four methods for local at-a-point identifiability testing compared here. subsequently, the three pathway models used in the case studies are introduced and each of the models is analyzed with each of the methods. insight from this analysis is used to simplify the models.

methods
in this section we will first introduce the mathematical system class and give a more concise definition of identifiability. we focus on local at-a-point identifiability and describe the four methods for testing local at-a-point identifiability compared here. finally, we summarize the three signaling pathways and the corresponding models.

system class
the concept of identifiability applies to a large system class. here we treat models of the form

   

since many biological signaling pathways can be represented by systems of this form. in equation  , ,  and  denote the state variables, the parameters, the inputs, and the outputs of the dynamical system, respectively. a biological parameter, for example a kinetic constant, corresponds to a component pk of the parameter vector. the functions f and h map from an open subset  onto  and , respectively, and are assumed to be smooth. note both a lab experiment and a simulation are uniquely defined by the initial conditions x = x <dig> and the values of the inputs u from t =  <dig> to the final time t = tf. both an experiment and a simulation result in values of the outputs y at successive points  in time

   

where nt denotes the number of measurements or stored simulation results, respectively. in the present paper, output data of the form  is obtained from simulations with models that have been adjusted to experimental data by other authors before  <cit> .

the solution of equation  that results for a particular choice of initial conditions x <dig>  parameters p, and inputs u is denoted by

   

the solution of equation  and its derivatives with respect to the parameters are computed with the integrator ddaspk  <cit> . the output behavior of the model is given by the response function

   

concept of identifiability
here we summarize the notions of identifiability as necessary for the paper . assuming that the inputs u, the initial conditions x <dig>  and the measurement times ti are given, the different notions of identifiability can be defined as follows.

definition 1: the parameter pk of the model  is called globally structurally identifiable if, for all admissible values  and all ,

   

implies

   

definition 2: the parameter pk of the model  is called locally structurally identifiable if, for all , there exists a neighborhood  such that for all , equations  imply equation .

the parameter vector p is called globally structurally or locally structurally identifiable if all its components pk are globally structurally or locally structurally identifiable, respectively. global and local at-a-point identifiability are defined as follows.

definition 3: let  be a point in the parameter space of the model . the parameter pk of the model  is called globally at-a-point identifiable at the point p* if, for all , equations  imply equation .

definition 4: let  be a point in the parameter space of the model . the parameter pk of the model  is called locally at-a-point identifiable at the point p* if there exists a neighborhood  such that for all , equations  imply equation .

the following subsection will introduce the four methods for at-a-point identifiability testing compared in this work.

methods for local at-a-point identifiability testing
the compared numerical methods for local at-a-point identifiability testing are based on the sensitivity of the model outputs at discrete time points tk, k ∈ { <dig> ..., nt}, with respect to the parameters. the sensitivity information is stored in the ny nt × np dimensional sensitivity matrix s. s is a block matrix that consists of time dependent blocks s of size ny × np  <cit> :

   

the entries of s are called sensitivity coefficients. for a nominal parameter vector p*, given x <dig>  and fixed time tk with k ∈ { <dig> ..., nt}, they are defined as

   

essentially, the sensitivity coefficients describe how sensitive the system output is to changes in a single parameter. if the model output is highly sensitive to a perturbation in one parameter we can consider this parameter to be important for the system behavior. in contrast, a parameter that has no influence on the outputs is a candidate for an unidentifiable parameter. linearly dependent columns of the sensitivity matrix imply that a change in the system outputs due to a change in one parameter, say pj, can be compensated by changing some or all of the dependent parameters pk, with k ≠ j. if dependencies exist, parameter estimation will fail or result in non-unique parameter values  <cit> . the correlation and orthogonal method are based on this idea.

for ease of presentation, we first give a detailed introduction to the eigenvalue method proposed here. the remaining methods are summarized briefly only, and the reader is referred to the appendix for more details. since all methods analyze at-a-point identifiability, nominal values for the parameters are required. for the test cases treated here parameter values are available from the literature  <cit>  and denoted by plit.

eigenvalue method
the eigenvalue method is similar to the method published by vajda et al.  <cit> . the eigenvalue method proposed here differs from the approach proposed by vajda and coworkers in that a manual inspection of eigenvalues and eigenvectors is not necessary here. as a result, an automatic analysis of large models becomes feasible. we first describe the eigenvalue method and subsequently describe differences to the method published by vajda et al.  <cit>  in more detail.

we consider the least squares parameter estimation problem that amounts to minimizing cost functions of the form

   

with respect to p, where the yi  are the measured or simulated data introduced in equation . in gaussian approximation, the hessian matrix h of equation  has the entries

   

for k, l ∈ { <dig> ..., np}. as indicated in equation , h can be expressed in terms of the sensitivity matrix s introduced in equation . h is a symmetric matrix and therefore its eigenvalues are real. furthermore, h is positive semi-definite.

in order to study the identifiability of the system in equation  for given x <dig>  u and for nominal parameters  we first solve equation  by numerical integration and subsequently calculate h as given by equation . we set p* to parameter values taken from the literature plit in all calculations. all identifiability tests carried out in the present paper therefore amount to asking whether the investigated signaling models are identifiable at the literature parameter values plit. as pointed out already in the background section, identifiability tests of this type clarify whether a given model is reasonably detailed even before experiments are carried out. in this sense, the identifiability of a model at nominal or literature values for its parameters is considered to be a necessary condition for its practical identifiability with measured data.

let λj and uj denote the jth eigenvalue and the corresponding eigenvector of h, respectively. assume the eigenvalues to be ordered such that , assume the eigenvectors uj to be normalized such that ujt uj =  <dig>  and assume p* to minimize equation . consider the change of ϕ when moving from p* in a direction αuj for some real α. gaussian approximation yields

   

where the linear order is zero, since p* is is a minimum by assumption. since huj = λj uj and ujt uj =  <dig>  the last equality in equation  holds. equation  implies that ϕ does not change when moving from p* to p* + Δp if Δp = αuj for any eigenvector uj with λj =  <dig> and any real α. in words, the directions uj corresponding to λj =  <dig> are those directions in the parameter space along which the least squares cost function is constant. we call these directions degenerate for short. in the particular case of

   

where the entry  <dig> is in position k, the model is not identifiable with respect to the kth component . the approach proposed here will therefore remove this parameter from consecutive calculations by fixing  to . in general however, uj will not be of the special form , but uj will be a vector with more than one non-zero entry, therefore the choice of k is not obvious. in this case, we select a k such that , and remove the kth component of p* from the parameter estimation problem by fixing it to its literature value. in the examples treated below this choice of k turns out to be appropriate in all cases. in general, this might not be the case, however. there might be entries of , l ≠ k, with an absolute value  close or equal to the maximal value . we call such entries  and the corresponding parameters co-dominant, since together with  they dominate the degenerate direction. a simple example for a system with co-dominant parameters is given in appendix a. we will discuss the issue of co-dominant parameters for the examples treated here in the results section.

in practical applications the smallest eigenvalue will typically not be zero but close to zero. in this case an eigenvalue cut-off value ϵ ≈  <dig>  ϵ >  <dig> needs to be specified. an eigenvalue λ <dig> <ϵ  is considered to be small enough to be treated as zero. note λj ≥  <dig> for all j since h is positive semi-definite.

the proposed algorithm splits the set of parameter indices { <dig> ..., np} of a given model into two disjoint subsets which we denote by i and u. upon termination, the sets i and u = { <dig> ..., np} - i contain the indices to the identifiable and the unidentifiable parameters, respectively. we denote the current number of elements in i by ni and the current elements in i by . the algorithm proceeds as follows.

 <dig>  choose Δt, tf, ϵ. set p* = plit. set i = { <dig> ..., np}, ni = np, and u = ∅.

 <dig>  if i is empty, stop. the model is not identifiable with respect to any parameter in this case.

 <dig>  fix the parameters pk, k ∈ u to their literature values and consider only the pk, k ∈ i to be variable. formally, this corresponds to setting p to  and treating all remaining parameters pk, k ∈ u as fixed numbers.

 <dig>  calculate ϕ according to equation . calculate the hessian matrix of ϕ with respect to the parameters pk with k ∈ i and evaluate it at p = p*. calculate the eigenvalues λj and corresponding eigenvectors uj of the hessian matrix. assume the eigenvalues to be ordered such that . assume the eigenvectors to be normalized.

 <dig>  if λ <dig> ≥ ϵ ,stop. the model is identifiable with respect to the parameters pk for all k ∈ i.

 <dig>  if λ <dig> ≥ ϵ ,select k such that . remove k from the set i, add k to the set u, set ni to the current number of elements in i, and return to step  <dig> 

the order in which parameters are removed from i determines the ranking of parameters from least identifiable to most identifiable.

the method is similar to the approach introduced by vajda et al.  <cit> . these authors also analyze the eigenvectors that correspond to small eigenvalues of the hessian matrix, but they focus on the dependencies between eigenvectors that arise due to special parameter combinations of the form p1/p <dig> or p1·p <dig> in the model. in a two step procedure vajda et al. first decide which parameters to lump together  and subsequently recalculate the eigenvalues and eigenvectors for the system with the new lumped parameters. these two steps are repeated until the smallest eigenvalue is sufficiently large. the lumping step requires manual inspection of the eigenvalues and eigenvectors of the hessian matrix. while the approach proposed by vajda et al. worked very well for their example with  <dig> parameters, it becomes infeasible for models with considerably more parameters. the examples treated in the next section demonstrate that our approach, in contrast, can be applied to models with up to at least a hundred parameters. moreover, our approach can be carried out automatically, while the approach suggested by vajda et al. was never intended for this purpose.

correlation method
the correlation method was first introduced by jacquez and greif  <cit>  who compared identifiability results achieved with the correlation method to analytical results obtained with a transfer function method . jacquez and greif  <cit>  consider only small linear compartmental models with up to  <dig> states and  <dig> parameters. in most of the examples the correlation and the transfer function method were in agreement.

in the systems biology literature the correlation method was applied by zak et al.  <cit>  to investigate identifiability of a large genetic regulatory network . more recently, rodriguez-fernandez et al.  <cit>  embedded the correlation method into their framework for robust parameter estimation in order to exclude unidentifiable parameters from parameter estimation.

the central idea of the correlation method is to find unidentifiable parameters by investigating the linear dependence of the columns of s . the correlation method approximates linear dependence by calculating the sample correlation  <cit>  of two columns  of s. the sample correlation is given by

   

where

   

   

   

in equations – corr, cov, σ , and  denote the sample correlation between s.i and s.j, the sample covariance between s.i and s.j, the sample standard deviation of s.i, and the mean of the entries of s.i, respectively.

two linearly dependent columns s.i and s.j of the sensitivity matrix give rise to an absolute value of the correlation corr =  <dig>  in the examples treated here, none of the parameters exhibit a correlation of exactly ±  <dig>  however. in the numerical implementation of the method, two columns of s are considered correlated if the absolute value of their correlation is equal to or larger than  <dig> – ϵc, where ϵc ∈  <cit>  is a parameter of the algorithm. we stress that while ϵc is a tuning parameter, the comparison of the identifiability methods is not affected by the choice of ϵc. as explained below, we systematically vary ϵc over its entire range  <dig> ≤ ϵc ≥  <dig> in the comparison.

when applying the correlation method to the pathway model examples, two problems arise regularly that have not been discussed by jacquez and greif  <cit> . for one, the method detects pairs of correlated parameters, but there is no criterion which one of the parameters from a pair to consider unidentifiable. moreover, if more than one pair of correlated parameters is detected, there is no criterion to choose among the pairs.

in order to mitigate these ambiguities we introduce the total correlation

   

where

   

and l denotes the set of indices of those parameters that have not been found to be unidentifiable in any previous iteration. we select the parameter with the highest total correlation for removal. some cases remain, however, in which two parameters have the same total correlation. in these cases we pick one parameter at random.

in order to be able to compare results to those of the other methods, we use the correlation method to rank model parameters from least identifiable to most identifiable. more precisely, we initially set ϵc =  <dig> and l = { <dig> .. np} and calculate the total correlations  for all parameters in l. if parameters with nonzero total correlations exist we select the parameter with highest total correlation to be unidentifiable and remove its index from l. if no such parameter exists we increase ϵc until nonzero total correlations occur. the order in which parameters are removed from l creates a ranking of parameters from least to most identifiable. the algorithm is described in detail in appendix b. based on the ranking of parameters the method is compared to the other three approaches. the comparison is explained in detail in the subsection entitled method comparison.

principal component analysis  based method
degenring et al.  <cit>  introduce three criteria based on pca that rank the influence of parameters on model output. parameters that do not affect the model outputs are unidentifiable by definition and can be removed from the model equations. degenring et al. successfully use their approach to simplify a complex metabolism model of escherichia coli k <dig>  a complete description how to get a full ranking using the three criteria can be found in  <cit> . a pseudo code description of the method can be found in appendix c.

the pca based method differs from the other methods described here in that it does not use the sensitivity matrix s as defined in equation  but a truncated matrix which we refer to by . this matrix  is defined as

  

where i ∈ { <dig> ..., ny}. while the sensitivity matrix s as defined in equation  describes all responses in one matrix,  is created for each response ri separately.

three pca based criteria are applied to each of the ny matrices . as a result 3ny parameter rankings are created, which we represent by 3ny lists

   

of integers , where j ∈ { <dig>   <dig>  3} and i ∈ { <dig> ..., ny}. the first ranking position  and the last ranking position  contain the index of the parameter with the lowest and the highest influence on the model output ri, respectively. the lower index j ∈ { <dig>   <dig>  3} denotes the criterion by which the ranking is calculated. we will explain one of the criteria in more detail in order to give the reader an idea of the method. the remaining two criteria are summarized in appendix c. all 3ny rankings are integrated into one ranking by applying a strategy described in  <cit> , which we briefly summarize below.

let,  and  denote the jth eigenvalue and corresponding eigenvector of , respectively, and assume the eigenvalues to be ordered such that . for each i ∈ { <dig> ..., np} evaluate the first criterion as follows.

 <dig>  set l <dig> = { <dig> ..., ny}.

 <dig>  for q from  <dig> to np do:

• find rl such that .

• set  = rl.

• set lq+ <dig> = lq - {rl}.

the first criterion loops over the eigenvectors starting with the eigenvector that belongs to the smallest eigenvalue. it identifies the largest absolute entry of each eigenvector and selects the corresponding parameter for removal, if this parameter has not been selected yet. the index of the kth parameter selected for removal is ranked at position k, where k ∈ { <dig> ..., np}.

the remaining two criteria of the pca based method are similar but differ with respect to the process for chosing rl. for brevity we omit details and refer the reader to appendix c. a final ranking j that incorporates all 3ny previously calculated rankings is obtained as follows. set nq = ∅, for all q =  <dig> ..., np. set j to an empty list.

for q from  <dig> to np do:

• for all j ∈ { <dig>   <dig>  3} and all i ∈ { <dig> ..., ny} set .

• set .

• only if nq ≠ ∅ append the nq to the list j.

first note that an iteration q might exist, in which no nq is appended to j. therefore the final number nj of elements in j is not necessarily equal to but might be smaller than np. further note that entries of j do not necessarily correspond to indices of single parameters but to sets of parameter indices. therefore, a ranking position ji might contain several parameter indices an internal ranking of which cannot be obtained. this ambiguity hampers not only the interpretation of the ranking result but also the comparison with the rankings produced by the other methods. we describe a strategy to deal with the latter problem in section entitled method comparison.

orthogonal method
the orthogonal method was developed by yao et al.  <cit>  to analyze parameter identifiability of an ethylene/butene copolymerization model with  <dig> parameters. in the field of systems biology the method has been applied in a framework for model identification  <cit>  and in the identifiability analysis of a nf-κb model  <cit> .

we summarize the concept of the method and refer the reader to appendix d for details. the method iterates over the columns s.k of the sensitivity matrix s, with k ∈ { <dig> ..., np}. those columns of s that correspond to identifiable parameters are collected in a matrix xq where q is the iteration counter. the algorithm starts by selecting the column of s with highest sum of squares in the first iteration. in iteration q +  <dig>  q columns of s have been selected. in the order of their selection these columns form the matrix xq. the next step essentially amounts to selecting the column s.k that exhibits the highest independence to the vector space v spanned by the columns of xq. more precisely, an orthogonal projection  of s.k onto v is calculated and  is interpreted as the shortest connection from v to s.k. the squared length of  is used as measure of independence. if the length of  is near zero, s.k is nearly linearly dependent to the columns of xq. conversely, a large value of  indicates that parameter pk is linearly independent to the columns of xq. in the orthogonal method the parameter pl where  is selected as the next identifiable parameter.

essentially, the orthogonal method finds those columns of s that are as independent as possible. the quantity  can be interpreted as a measure of independence that has been freed from dependencies on previously selected parameters. therefore not only the influence of a parameter on the outputs, but also the dependencies between the parameters are accounted for by using  as a measure. a visualization of the orthogonal method can be found in appendix d.

yao et al.  <cit>  stop the selection process once the length of the maximal  drops below a cut-off value ϵo ≈  <dig>  the choice of which is rather arbitrary. in the present paper we do not apply this cut-off criterion but merely rank all parameters from most to least identifiable where the terms most identifiable and least identifiable are used in the same sense as in the correlation method. a comparison to the results of the other methods is therefore not affected by the choice of ϵo. our approach to comparing the methods is explained in the section entitled method comparison.

method comparison
the central idea behind our comparison of the methods is the equivalence between the local identifiability of a model and the local positive definiteness of the hessian matrix  at a minimum of the least squares parameter cost function  for this model. due to this theoretical result, which has been established by grewal and glover  <cit> , we can test local identifiability at a point in the model parameter space by testing the hessian matrix  for positive definiteness at this point. since the hessian matrix is positive definite if and only if all its eigenvalues are strictly positive, we consider a model to be identifiable if the eigenvalues are bounded below by a small strictly positive number ϵ. note the eigenvalue method proposed here is based on the same idea in that it selects those parameters to be unidentifiable that cause eigenvalues smaller than ϵ.

by virtue of the relation between local at-a-point identifiability and the eigenvalues of the hessian matrix we can compare all four identifiability testing methods to one another with the same criterion and an unique parameter value ϵ. for a given model we first create a ranking of the parameters from least to most identifiable with each method as described in the previous four sections. for each parameter ranking we then fix the parameter considered to be the least identifiable to its literature value, calculate the hessian matrix  with respect to the remaining parameters, and determine the smallest eigenvalue. in all following steps we additionally fix the next least identifiable parameter and recalculate the hessian and smallest eigenvalue. this process is carried out until the smallest eigenvalue exceeds ϵ. the parameters that need to be fixed in order for the smallest eigenvalue to exceed ϵ are considered to be the unidentifiable parameters. clearly, the smaller the set of unidentifiable parameters, the larger the set of identifiable parameters or, equivalently, the larger the number of parameters for which the least squares parameter estimation problem can be solved. in this sense we consider the method to be the best one that results in the smallest number of unidentifiable parameters in our comparison.

note in contrast to the other methods, the ranking created with the pca based method does in general not contain one but several parameters. in such a case we fix all parameters ranked at the same position at once and reevaluate the smallest eigenvalue of the hessian matrix. the last ranking position  created by the pca based method contains the indices of all parameters that have not been ranked previously. fixing of parameters pk, with k ∈ , would therefore not leave any parameter unfixed. for this reason we consider the pca method to have failed, if fixing of all parameters with indices in  does not lead to a value of the smallest eigenvalue larger than or equal to ϵ.

pathway model description
having introduced the methods to be compared, we briefly summarize the three pathways and the corresponding models used for the identifiability studies.

jak-stat pathway
the janus kinase  – signal transducer and activator of transcription  pathway is triggered by cytokines and growth factors. the pathway has an impact on the expression of genes that regulate diverse cellular processes, such as cellular proliferation, differentiation, and apoptosis. a detailed model of the interferon-induced jak-stat pathway is given by yamada et al.  <cit> . this model describes the following signal transducing steps. upon ligand binding the receptor dimerizes thus triggering the activation of receptor-associated jak. activated jak immediately phosphorylates the receptor complex, enabling the binding and subsequent phosphorylation of cytoplasmic stat. phosphorylated stat can dimerize, and finally, dimerized stat is imported into the nucleus. here it activates target genes, one of which is the suppressor of cytokine signaling  <dig> , an important mediator of negative feedback.

egf induced map kinase pathway
mitogen-activated protein  kinases are involved in mitosis and the regulation of cellular proliferation. the epidermal growth factor -induced map kinase pathway is centered on a three kinase cascade that terminates with the dual phosphorylation of the third, so-called map kinase. egf stimulation leads to dimerization of the egf receptor. the dimerized egf receptor triggers the phosphorylation of raf, the first kinase of the cascade. raf, in turn, phosphorylates the mitogen extracellular kinase, mek, the second kinase in the cascade, which finally phosphorylates the map kinase . phosphorylated erk regulates several proteins and nuclear transcription factors and controls the expression of target genes. the model published by schoeberl et al.  <cit>  additionally includes the internalization of egf receptor and two further modules: an sh2-domain containing protein  dependent module and an shc independent one.

nf-κb pathway
nuclear factor-κb  is a transcription factor, which regulates genes involved in inflammation, immune response, cell proliferation, and apoptosis. in the unstimulated cell nf-κb is captured in the cytoplasm by the protein inhibitor of nf-κb . upon stimulation by pathway activating signals such as the tumor necrosis factor , the iκb kinase  is activated. activated ikk phosphorylates iκbα thus inducing its degradation. as a consequence nf-κb is released, translocates into the nucleus and regulates effector genes. nf-κb induces its own downregulation by inducing the production of two proteins: 1) iκbα that inhibits nf-κb by relocating it to the cytoplasm and 2) the zink-finger protein a <dig> that represses ikk and consequently indirectly inhibits nf-κb.

RESULTS
in this section the results of the identifiability analysis are reported. all simulations are carried out with literature values plit for the model parameters, which are taken from the original publications of the models  <cit> . we assume that all state variables are available as outputs, i.e. y = x and ny = nx. clearly, it is far from realistic to assume all state variables could be measured in an actual experiment for any of the signaling pathways. since even under this strong assumption the models turn out not to be identifiable, however, it is not reasonable to investigate the identifiability with fewer outputs. in fact our identifiability results show that the models are overparameterized even under the very strong assumption of all state variables being measurable. the algorithmic parameter ϵ is set to ϵ = 10- <dig>  while this value is motivated by our experience with numerical parameter estimation problems, we admit that it is somewhat arbitrary. note, however, the comparison of the identifiability testing methods is consistent in that the same value ϵ = 10- <dig> is used throughout. output values y are recorded from the simulations at equidistant points in time. the jak-stat model is simulated for  <dig> seconds  and the response function is recorded with a time step of Δt =  <dig> s. the corresponding values for the map kinase and nf-κb models are  <dig> s , Δt =  <dig> s, and  <dig> s , Δt =  <dig> s, respectively. these values are not critical. similar results are obtained with other values of tf and Δt .

jak-stat pathway analysis
some of the results generated by the four methods are worth inspecting more closely. in the first and second iteration of the eigenvalue method there exists more than one dominant contribution to the degenerate direction, cf. table  <dig>  in the first iteration there are two co-dominant entries . the corresponding parameters are kb <dig> and kb <dig>  in the second iteration there exists one co-dominant entry, which corresponds to the parameter kb <dig>  in the last iteration no co-dominant entries exist and the parameter kb <dig> is solely responsible for the degenerate direction. it turns out that selecting the parameter corresponding to the maximum entry of |u1| as done here is sufficient to find a minimal set of identifiable parameters. interestingly, we can see in this example that after three iterations our method finds the parameters corresponding to the co-dominant entry of all previous iterations.

the correlation method indicates that  <dig> parameters need to be fixed in order for the model to be locally identifiable . the sets of parameters removed by the eigenvalue and orthogonal method have only kb <dig> in common with the parameters selected by the correlation method. in the correlation method, the removal of parameter kb <dig> occurs in the last iteration and results in a clear increase of the smallest eigenvalue by a factor of about  <dig> . surprisingly, previous iterations of the correlation method hardly have an effect on the minimum eigenvalue.

the parameters removed by the eigenvalue and orthogonal methods point the way to possible simplifications of the pathway models. the parameter kb <dig> describes the dissociation of phosphorylated stat from the activated receptor complex. this reaction, however, is not the dissociation that immediately follows the phosphorylation of stat by the activated receptor . apart from the association of unphosphorylated stat to the receptor , the phosphorylation of stat by the activated receptor and the subsequent dissociation of phosphorylated stat from the activated receptor , the model also permits the reassociation of already phosphorylated stat from the cytoplasm to the activated receptor . the dissociation described by parameter kb <dig> is the reverse reaction to the reassociation described by kf <dig>  we claim these steps are less important, since the key event is the phosphorylation of cytoplasmic stat by the activated receptor. removing kb <dig> alone creates a sink for phosphorylated stat and activated receptor, since the dissociation of both species does not exist anymore. therefore the model can only be simplified by removing both the reaction kb <dig> is involved in and the reaction kf <dig> is involved in. the main effects of these reactions are 1) to reduce the amount of free phosphorylated stat and 2) to reduce the amount of free activated receptor. after removing both reactions, effect 1) can be imitated by appropriately decreasing those rate constants involved in reactions with free phosphorylated stat. effect 2) can be compensated by appropriately decreasing those rate constants that are involved in reactions with free activated receptor.

the second fixed parameter, kb <dig>  corresponds to the rate of dissociation of stat from the complex formed by socs <dig> and the activated receptor complex. in order to get a simplified model that is identifiable, we may remove this step. this is justified, because the key function of socs <dig> is not its binding to stat, but its inhibition of the activated receptor complex  <cit> . the latter is still ensured in the simplified model.

the third fixed parameter, kb <dig>  describes the dissociation of receptor-bound unphosphorylated stat. assuming that stat binds irreversibly to the activated receptor, the model can be simplified. in this scenario, stat dissociates from the receptor only after stat phosphorylation has occurred. due to the presence of ppx, which can dephosphorylate cytoplasmic stat, this change to the model need not result in an increase of the concentration of phosphorylated stat.

we stress that the fact that kb <dig>  kb <dig>  and kb <dig> render the model unidentifiable does not imply the corresponding reactions do not exist. our results imply, in contrast, that the model is too complex in the sense that these parameters cannot be determined by model based parameter estimation. note that this result holds true even if we assume data for all the state variables to be available.

map kinase pathway analysis
before any parameter of the map kinase model is fixed, the smallest eigenvalue of the hessian matrix  equals  <dig> ·10- <dig>  this implies the model is not identifiable. the eigenvalue and the orthogonal method again outperform the correlation method in that they find a significantly smaller set of parameters to be responsible for the unidentifiability of the model. the pca based method does not lead to an identifiable method.

in the eigenvalue method the maximal components of the respective eigenvectors are near  <dig>  in both iterations. as a result, two parameters, k <dig> and k <dig>  can unambiguously be fixed in the first and second iteration, respectively.

the correlation method needs to fix  <dig> parameters to arrive at an identifiable model. fixing the first  <dig> parameters does not significantly increase the smallest eigenvalue . only when the last parameter is fixed a dramatic increase of the smallest eigenvalue from about  <dig> ·10- <dig> to  <dig> · <dig> results. the parameter k <dig> is also removed by the eigenvalue and the orthogonal method, which indicates the importance of k <dig> for identifiability. the other parameter removed by the eigenvalue method, k <dig>  is selected in the 2nd iteration of the correlation method. therefore, the set of  <dig> parameters found by the correlation method is a superset of the  <dig> parameters selected by the eigenvalue and the orthogonal method. closer inspection of the first iteration in the correlation method reveals a high maximal total correlation, , of  <dig> . this high value is caused by couplings of the selected parameter with eight other parameters. despite this high total correlation value of  <dig> , the removal of the corresponding parameter does not cause a significant increase of the smallest eigenvalue. just as in the previous example, removing parameters with high correlations does not necessarily improve identifiability.

the ranking found by the pca based method has five positions . when the  <dig> parameters that correspond to j <dig>  j <dig>  j <dig>  and j <dig> are fixed, the model remains unidentifiable. fixing the parameters that correspond to j <dig> is not reasonable, since doing so would not leave any parameters unfixed. due to this result, we consider the pca based method to have failed.

in summary, the eigenvalue method and the orthogonal method identify the smallest set of parameters that needs to be fixed in order to create an identifiable model. the first parameter, k <dig>  is involved in internalization processes represented by more than twenty reactions  <cit> . comprehensive changes to this part of the model that are necessary to ensure identifiability cannot be addressed here. the second parameter, k <dig>  describes the binding of egf to its receptor. this essential step cannot be simplified. however, experimental data on this reaction exists in the literature  <cit> . our analysis suggests using this kind of independent information to fix the value of k <dig> before attempting to determine the remaining parameters by parameter estimation.

nf-κb pathway analysis
for the nf-κb pathway model, the smallest eigenvalue λ <dig> =  <dig> ·10- <dig> results if no parameters are fixed . this indicates that the model is not identifiable for the published parameter values  <cit> . the eigenvalue and the orthogonal method again find the same  <dig> unidentifiable parameters, c4a, c <dig>  and t <dig>  the correlation method selects a considerably larger set of  <dig> parameters. the pca based method fails.

in the first three iterations of the eigenvalue method, the largest components of u <dig> have absolute values close to one . the method therefore selects these parameters unambiguously.

the correlation method needs to fix  <dig> parameters until the smallest eigenvalue is greater than ϵ. the last parameter, the fixing of which leads to an identifiable model , has been selected with a value of ϵc =  <dig> . this is not reasonable anymore, since two columns of s with a correlation of  <dig> - ϵc =  <dig>  can hardly be considered correlated. therefore we conclude that the correlation method fails for this example.

* the value of  <dig> – ϵc is given in brackets. see the text for a discussion.

the pca based method creates a ranking j with six positions . fixing the parameters that correspond to j <dig> to j <dig> does not result in an identifiable model. fixing the parameters corresponding to j <dig> will not leave any parameters unfixed. therefore the method has failed in producing an identifiable model. after fixing all parameters from j <dig> to j <dig> the resulting smallest eigenvalue λ <dig> =  <dig> ·10- <dig> is close to ϵ. having a look at the parameters in the ranking j we can see that j <dig> contains parameter t <dig>  and j <dig> contains parameter c <dig>  both parameters are also found by the eigenvalue and the orthogonal method.

the three parameters selected for removal by the eigenvalue and the orthogonal method indicate how to simplify the model. the first two parameters, c4a and c <dig>  describe the only two translation steps in the model: the translation of iκbα mrna and a <dig> mrna, respectively. by lumping together transcription and translation into one protein synthesis step the two unidentifiable parameters c4a and c <dig> can be removed from the model. the third parameter selected by the eigenvalue and orthogonal method, t <dig>  describes the dissociation of activated ikk from iκα. since this reaction is a key step of the model the only option for simplifying this part of the model is to remove ikk entirely. as a consequence a <dig> becomes obsolete, since the only function of a <dig> is the regulation of ikk. if we also remove a <dig>  the resulting simplified model closely resembles the minimal model proposed by krishna et al.  <cit> .

discussion
the eigenvalue, orthogonal, and correlation methods find the desired subset of identifiable parameters and the desired subset of unidentifiable parameters in all cases. the eigenvalue and orthogonal methods give the same results for each of the models. the subsets of unidentifiable parameters that result from the correlation method are larger than those that result from the eigenvalue or orthogonal method in all cases. the pca based method fails for all three examples.

the four identifiability testing methods use different approaches to finding unidentifiable parameters. in order to carry out a meaningful comparison of the methods, we need a single identifiability criterion that can be applied to all methods. the positive definiteness of the hessian matrix  of the least squares parameter estimation problem  is an appropriate and concise criterion for this purpose. ultimately, the use of this criterion is justified by the equivalence between local at-a-point identifiability and the positive definiteness of the hessian matrix . this equivalence was established by grewal and glower  <cit> .

since the hessian matrix is positive definite if and only if all its eigenvalues are strictly positive, we consider a model to be identifiable if the eigenvalues of the hessian matrix  are bounded below by a small strictly positive number ϵ. based on experience with parameter estimation problems we set ϵ = 10- <dig>  while this choice is arguably arbitrary, the comparisons of the methods are meaningful since the same value of ϵ is applied in all comparisons.

the numbers of unidentifiable parameters are summarized in figure  <dig> for the four methods and the three models. more precisely, the figure shows how many parameters are selected by each method to be fixed in order to obtain a hessian matrix with all eigenvalues larger than ϵ = 10- <dig>  from figure  <dig> it is apparent that fewer parameters need to be fixed than those selected by the correlation method to obtain a positive definite hessian matrix or, equivalently, an identifiable model in all examples. in this sense we conclude that the eigenvalue and orthogonal method outperform the correlation method. since the pca based method fails in all three examples, we consider it to be inferior to the other methods.

in the remainder of the section we discuss the differences between the methods.

eigenvalue method vs. orthogonal method
the eigenvalue method can be motivated in two different ways. for one, it can be interpreted as a convexity analysis of the minimization of the least squares cost function ϕ  at a nominal point p*, in parameter space. if ϕ is too flat at p*, the method identifies the parameters that cause this flatness. secondly, the method can be looked upon as an approach to identifying linearly dependent columns of s. if st s has zero eigenvalues it is not invertible, which implies that s has linearly dependent columns. by fixing those parameters that cause st s to be not invertible, the linear independence of columns of s can be ensured. the orthogonal method selects those columns of s that have the largest possible orthogonal distance to previously selected columns. this way linearly independent columns are found. essentially, the eigenvalue and orthogonal method use the same criterion, but the eigenvalue methods discards those parameters that render a model unidentifiable while the orthogonal method selects the identifiable parameters. both methods give the same results for the examples treated here.

a formal analysis of the two methods reveals the computational complexity to depend on the magnitudes of ny nt and np . if , as is the case in the jak-stat and the nf-κb example, the eigenvalue method is an order of magnitude in the number of parameters faster than the orthogonal method. if we assume , as is the case for the map kinase example, the eigenvalue method is slightly faster than the orthogonal method. if ny nt ≈ np, both methods have the same computational complexity. we stress the complexity analysis is carried out under assumptions that are unfavorable for the eigenvalue method in that the eigenvalue method is assumed to require a full ranking to separate the identifiable from the unidentifiable parameters. in the three examples treated here this is clearly not the case, however. in fact the eigenvalue method stops after three, two, and three iterations in the jak-stat, the map kinase, and the nf-κb case, respectively, and therefore turns out to be far less computationally expensive than under the worst case assumption used in appendix e.

technical drawbacks of the correlation method
from a technical point of view, the correlation method suffers from two drawbacks. these drawbacks cause the differences between the number of unidentifiable parameters found by the correlation method as compared to the orthogonal and eigenvalue method. first, the correlation corr is only an approximation of the linear dependency of columns of s. while two columns that are linearly dependent result in a correlation of ±  <dig>  columns that have a correlation of ±  <dig> need not necessarily be linearly dependent . therefore, the correlation method may find false positive unidentifiable parameters. secondly, the correlation corr is only a pair wise measure for any two columns s.i and s.j of s. linear dependence of a set of more than two columns, which does not contain a linearly dependent pair, is not detected.

technical drawbacks of the pca based method
the pca based method differs from the three other methods in several aspects. the pca based method does not use the sensitivity matrix s, but a sensitivity matrix is calculated for each component ri of the response function separately. in contrast to the other methods, couplings between the components ri are therefore not considered. a second important difference is that the pca based method does not use a single criterion, but a combination of criteria which all must hold for a parameter to be considered unidentifiable. due to this combination of criteria, the method does not select a single parameter in each iteration but a set of parameters. finally, some parameters that cause s not to have full rank are not found by the pca based method in all three examples. for this reason, we consider the pca based method to be inappropriate for the identifiability testing of the three treated pathways.

CONCLUSIONS
we proposed a new approach to finding identifiable and unidentifiable parameters of large mechanistic models. the method, which we refer to as the eigenvalue method, was applied to existing models of the jak-stat, the map kinase, and the nf-κb pathways. all three pathway models turned out not to be identifiable. in fact our results show that the models would not be identifiable even if all state variables could be measured precisely at a high frequency. the models must therefore be considered to be severely overparameterized. note state identifiability investigated here is a necessary condition for output identifiability. since the models turn out not even to be state identifiable, we refrained from further investigations of output identifiability.

the identifiability analysis reveals how to simplify the models in order to arrive at identifiable systems. for the jak-stat pathway an identifiable model can be obtained by simplifying only three reactions. for the nf-κb pathway model the results lead to a simplified model that closely resembles the minimal model published by krishna et al.  <cit> .

we presented a detailed comparison of the proposed eigenvalue approach to three established methods for identifiability testing, namely, the correlation method  <cit> , the pca based method  <cit> , and the orthogonal method  <cit> . these three methods and the new approach suggested are of general interest, because they can be applied to a broad class of nonlinear systems of ordinary differential and algebraic equations. essentially, the methods were compared with respect to their ability to find an identifiable subset of model parameters for each of the three pathway models. despite algorithmically different, the eigenvalue and the orthogonal method result in the same subsets of identifiable parameters for each of the examples. moreover, these two methods outperform the other methods in that they find the largest subsets of identifiable parameters.

authors' contributions
tq and mm developed the eigenvalue method. the discussed method were implemented and compared by tq. tq and mm wrote the article.

appendix
a: example with co-dominant parameters
we present an illustrative example to motivate the notion of co-dominant parameters. consider the trivial system  = ·x <dig> with y <dig> = x <dig>  using the notation introduced below equation , the eigenvalue λ <dig> equals  <dig>  with |u1| = t. from the simple model equation it is apparent that p <dig> and p <dig> are not identifiable separately, but only their sum p <dig> + p <dig> can be estimated from data. as a result, p <dig> and p <dig> are co-dominant as indicated by the result . fixing either one of the parameters renders the other one identifiable.

b: correlation method
we first summarize theoretical aspects of the correlation method and subsequently give a detailed description of the algorithm.

theory
let Δp = p - p* describe the deviation of a parameter vector p from the true parameter vector p*. for sufficiently small ||Δp|| <dig> the response function  can be approximated by its linearization at p*:

  

for all i ∈ { <dig> ..., ny} and j ∈ { <dig> ..., nt}. the sum of squared errors of the linearized response functions and the measured outputs yi  reads

   

the last equality holds, because the yi  are assumed to be noise free measurements and p* are the true parameter values by assumption. therefore ri - yi =  <dig> for all i ∈ { <dig> ..., ny} and j ∈ { <dig> ..., nt}.

let  denote the least squares estimates of Δp, i.e. the particular values of Δp that minimize equation . the necessary condition for optimality  is equivalent to the so called normal equation st s Δp =  <dig>  solving the normal equation for Δp therefore results in the least squares estimate .

if st s has full rank, the normal equation only admits the solution Δp =  <dig>  this implies Δp =  <dig>  or equivalently  = p*, is the unique minimizer of the least squares function r. consequently, the model parameters p are locally identifiable by definition. note only local identifiability can be inferred, since an approximation by linearization was introduced. if, on the other hand, st s does not have full rank, there exists a non-trivial solution  ≠  <dig> of the normal equation. as a consequence, a parameter vector  ≠ p* exists that results in the same model response as p*. therefore the model parameters p are not identifiable by definition.

the matrix st s has full rank if and only if the columns of s are linearly dependent  <cit> . the central idea of the correlation method is to detect linear dependence of two columns of s by calculating the correlation of these columns. parameters corresponding to columns of s with a correlation of ±  <dig> are considered unidentifiable.

algorithm for the correlation method
let c denote the matrix that contains those absolute correlation values between columns of s that exceed the threshold  <dig> - ϵc. c is given by:

  

where s.j denotes the jth column of s and corr*  is defined in equation . the algorithm proceeds in the following way:

 <dig>  choose Δt and an initial guess p*. set ϵc =  <dig>  i = { <dig> ..., nt}, and initialize u to an empty list. calculate the sensitivity matrix s.

 <dig>  calculate the matrix c.

 <dig>  for all i ∈ i calculate  as defined in equation .

 <dig>  if i is empty, stop. the list u contains the parameter indices in the order of the least to most identifiable parameter.

 <dig>  find rq such that .

 <dig>  if  =  <dig>  increase ϵc by  <dig>  and return to step  <dig>  otherwise, remove rq from i, append rq to the list u, and return to step  <dig> 

the list u contains the desired result as explained in step  <dig>  note ϵc is incremented in steps of Δϵc =  <dig> . the choice of this increment is not critical as long as it is small enough. if Δϵc is chosen smaller than necessary the same ranking of parameters will be obtained but the algorithm may require more computation time.

c: pca based method
we explain the second and third pca based criterion and present the algorithm for the pca based method. the first criterion is explained in the section entitled methods.

description of the second and third criterion of the pca based method
let  and  denote the jth eigenvalue and corresponding eigenvector of , respectively, and assume the eigenvalues to be ordered such that . for each i ∈ { <dig> ..., np} evaluate the second and third criterion as follows.

• second criterion:

set l <dig> = { <dig> ..., ny}.

for q from  <dig> to np do:

- find rl such that .

- set  = rl.

- set lq+ <dig> = lq - {rl}.

the second criterion is based on the sum of squared eigenvector entries

   

note in this sum, i and j are fixed, therefore the sum is not carried out over components of a particular eigenvector, but the lth entries of all eigenvectors are summed. the parameter with the largest value of  is selected.

• third criterion:

set l <dig> = { <dig> ..., ny}.

for q from  <dig> to np do:

- find rl such that .

- set .

- set lq+ <dig> = lq - {rl}.

the third criterion loops over the eigenvectors but, in contrast to the first and second criterion, starts with the eigenvector that belongs to the largest eigenvalue. as in the first criterion, parameters are selected based on the absolute value of the eigenvector component. the index of the parameter selected first will be placed last in ranking , the index of the parameter selected second will be placed at position np -  <dig>  and so on.

algorithm for the pca based method
 <dig>  choose Δt and an initial guess p*. for all i ∈ { <dig> ..., ny} let , , and  be empty lists. for all q ∈ { <dig> ..., np} set nq = ∅. set j to an empty list.

 <dig>  for all i ∈ { <dig> ..., ny} calculate , its eigenvectors , and the corresponding eigenvalues , j ∈ { <dig> ..., np}.

 <dig>  for i ∈ { <dig> ..., ny} calculate , , and  by applying ranking criterion one to three as described in the method section. let  denote the kth entry of .

 <dig>  for q from  <dig> to np do:

• for j ∈ { <dig>   <dig>  3} and i ∈ { <dig> ..., ny} set .



• only if nq ≠ ∅ append nq to the list j.

the final ranking that combines the pca results of all  and all three criteria is given by j, where the first position in j corresponds to the least and the last position in j corresponds to the most identifiable parameter.

d: orthogonal method
in this part we present a geometric interpretation the orthogonal method, followed by a description of algorithmic details.

visualization of the concept of the orthogonal method
the orthogonal method is visualized in figure  <dig> for q =  <dig>  two columns of s have already been selected and represent the first and the second column of x <dig> denoted by  and , respectively. these two columns span the vector space v <dig>  without restriction this vector space is assumed to be the xy plane and k is the next parameter to be chosen.

algorithm for the orthogonal method
algorithmic details for the orthogonal method are given below.

 <dig>  choose Δt and an initial guess p*. initialize  to a list of zeros.

 <dig>  calculate the sensitivity matrix s.

 <dig>  find r <dig> such that , set , and l <dig> = { <dig> ..., np} - {r1}.

 <dig>  for q from  <dig> to np do:

• calculate pq as given by equation .

• find rq such that , with  corresponding to the kth column of pq.

• set  ≥ ϵo:

set ,

set ,

and set lq+ <dig> = lq - {rq}.

the ranking is given by wortho. the shown implementation ignores the stopping criterion applied in the original literature, to ensure a complete ranking.

e: computational complexity of the orthogonal and the eigenvalue method
the numerical operations of the two methods that dominate the number of necessary numerical operations are 1) matrix inversions and eigenvalue/eigenvector calculations, which both require o operations for n × n matrices, and 2) matrix multiplications, which require n·m·l operations for the multiplication of an n × m by an m × l matrix.

in what follows, we assume the eigenvalue method is used to rank all model parameters. this overestimates the computational cost of the eigenvalue method, since the method stops once an identifiable set of parameters has been found.

computational complexity of the orthogonal method
in compact notation the orthogonal projection in iteration i, where i ∈ { <dig> ..., np} is arbitrary but fixed, can be written as:

   

the major runtime costs result from the computation 1) of y and 2) of y s' in equation . the number of operations necessary to calculate y is:

   

   

where all iterations i =  <dig> ..., np have been accounted for. the first and second term in the sum of equation  account for the multiplication of xi by - <dig>  and the inversion of the matrix xit xi , respectively. the third term accounts for the cost of multiplying xit by xi . this term reduces from i <dig> ny nt to iny nt, the cost or calculating the last row of xit xi, since xi-1t xi- <dig> is available from the previous iteration:

  

here  denotes the ith column of xi. the last row is equal to the last column, since xit xi is symmetric. the cost for i vector multiplications  with  <dig> ≤ k ≤ i amounts to iny nt, which gives rise to the last term in the sum in equation .

the number of operations necessary to calculate y s' is:

   

the first term in the sum accounts for the multiplication of y by s' . the second term accounts for the calculation of s' from the product xit s . the latter reduces to the cost ny nt np for calculating the last row of s', since xi-1t s is vailable from the previous iteration:

  

where  denotes the last column of xi and s.i denotes the ith column of s.

the subtraction of s from y s'  in equation  requires o operations and therefore can be neglected.

in summary the dominant parts in equation  and  result in a computational complexity of order .

computational complexity of the eigenvalue method
for ease of presentation the description of the algorithm in the methods section slightly differs from the actual implementation. in the implemented version the hessian matrix h = st s has to be calculated only once at the beginning of the algorithm, which requires  operations . in each iteration i, the current hessian hi can be obtained by removing the rows and columns from h that correspond to fixed parameters. in each iteration the eigenvalues of the current hessian matrix have to be calculated. the number of operations necessary for the eigenvalue method is as follows.

   

   

the first term accounts for the initial calculation of the hessian matrix. the second term accounts for the eigenvalue calculation.

comparing computational complexity of the eigenvalue and the orthogonal method
the computational complexity of the eigenvalue method is of order  ), whereas the computational complexity of the orthogonal method is of order  ). for a comparison we have to distinguish between the following three cases. in the first case we assume ny nt ≤ np. in this case the complexity of both methods is dominated by the term  and both method are equally efficient. in the examples treated here, this case does not occur. in the second case we assume . here the computational complexity of the eigenvalue and the orthogonal method reduce to  and , respectively. this case applies to the map-kinase example and the eigenvalue method is faster than the orthogonal method. the difference is the more pronounced the larger ny nt - np. in the last case we assume that . this is true in the jak-stat and the nf-κb example. here the complexity of the eigenvalue method and the orthogonal method reduce to  and , respectively. in this case the eigenvalue method is an order of magnitude in the number of parameters faster than the orthogonal method.

f: correlation is not equal to linear dependence
in order to show that a correlation of ±  <dig> between two vectors does not necessarily imply their linear dependence, we present a simple counter-example. consider the vectors x = t and y = t. according to equation  the correlation corr =  <dig> results. clearly, however, x and y are linearly independent. a set of parameters that are found to be unidentifiable with the correlation method may therefore contain false positives.

