BACKGROUND
the inference of population structure from the genotypes of admixed individuals poses a significant problem in population genetics. for example, genome wide association studies  compare the genetic makeup of different individuals in order to extract differences in the genome that may contribute to the development or suppression of disease. of particular interest are single nucleotide polymorphisms  that reveal genetic changes at a single nucleotide in the dna chain. when a particular snp variant is associated with a disease, this may indicate that the gene plays a role in the disease pathway, or that the gene was simply inherited from a population that is more  predisposed to the disease. determining the inherent population structure within a sample removes confounding factors before further analysis and reveals migration patterns and ancestry  <cit> . this paper deals with the problem of inferring the proportion of an individual’s genome originating from multiple ancestral populations and the allele frequencies in these ancestral populations from genotype data.

methods for revealing population structure are divided into fast multivariate analysis techniques and slower discrete admixture models  <cit> . fast multivariate techniques such as principal components analysis   <cit>  reveal subspaces in the genome where large differences between individuals are observed. for case–control studies, the largest differences commonly due to ancestry are removed to reduce false positives  <cit> . although pca provides a fast solution, it does not directly infer the variables of interest: the population allele frequencies and individual admixture proportions. on the other hand, discrete admixture models that estimate these variables typically require much more computation time. following a recent trend toward faster gradient-based methods, we propose a faster simpler least-squares algorithm for estimating both the population allele frequencies and individual admixture proportions.

pritchard et al.  <cit>  originally propose a discrete admixture likelihood model based on the random union of gametes for the purpose of population inference. in particular, their model assumes hardy-weinberg equilibrium within the ancestral populations  and linkage equilibrium between markers within each population . each individual in the current sample is modeled as having some fraction of their genome originating from each of the ancestral populations. the goal of population inference is to estimate the ancestral population allele frequencies, p, and the admixture of each individual, q, from the observed genotypes, g. if the population of origin for every allele, z, is known, then the population allele frequencies and the admixture for each individual have a dirichlet distribution. if, on the other hand, p and q are known, the population of origin for each individual allele has a multinomial distribution. pritchard et al. infer populations by alternately sampling z from a multinomial distribution based on p and q; and p and q from dirichlet distributions based on z. ideally, this markov chain monte carlo sampling method produces independent identically distributed samples  from the posterior distribution p. the inferred parameters are taken as the mean of the posterior. this algorithm is implemented in an open-source software tool called structure <cit> .

the binomial likelihood model proposed by pritchard et al. was originally used for datasets of tens or hundreds of loci. however, as datasets become larger, especially considering genome-wide association studies with thousands or millions of loci, two problems emerge. for one, linkage disequilibrium introduces correlations between markers. although falush et al.  <cit>  extended structure to incorporate loose linkage between loci, larger datasets also pose a computational challenge that has not been met by these sampling-based approaches. this has led to a series of more efficient optimization algorithms for the same likelihood model with uncorrelated loci. this paper focuses on improving computational performance, leaving the treatment of correlated loci to future research.

tang et al.  <cit>  proposed a more efficient expectation maximization  approach. instead of randomly sampling from the posterior distribution, the frappe em algorithm  <cit>  starts with a randomly initialized z, then alternates between updating the values of p and q for fixed z, and maximizing the likelihood of z for fixed p and q. their approach achieves similar accuracy to structure and requires much less computation time. wu et al.  <cit>  specialized the em algorithm in frappe to accommodate the model without admixture, and generalized it to have different mixing proportions at each locus. however, these em algorithms estimate an unnecessary and unobservable variable z, something that more efficient algorithms could avoid.

alexander et al.  <cit>  proposed an even faster approach for inferring p and q using the same binomial likelihood model but bypassing the unobservable variable z. their close-source software, admixture, starts at a random feasible solution for p and q and then alternates between maximizing the likelihood function with respect to p and then maximizing it with respect to q. the likelihood is guaranteed not to decrease at each step eventually converging at a local maximum or saddle point. for a moderate problem of approximately  <dig> loci, admixture achieves comparable accuracy to structure and requires only minutes to execute compared to hours for structure <cit> .

another feature of structure’s binomial likelihood model is that it allowed the user to input prior knowledge about the degree of admixture. the prior distribution for q takes the form of a dirichlet distribution with a degree of admixture parameter, α, for every population. for α =  <dig>  all of an individual’s alleles originate from the same ancestral population; for α >  <dig>  individuals contain a mixture of alleles from different populations; for α =  <dig>  every assignment of alleles to populations is equally likely ; and for α → ∞, all individuals have equal contributions from every ancestral population. alexander et al. replace the population degree of admixture parameter in structure with two parameters, λ and γ, that when increased also decrease the level of admixture of the resulting individuals. however, the authors admit that tuning these parameters is non-trivial  <cit> .

this paper contributes to population inference research by  proposing a novel least-squares simplification of the binomial likelihood model that results in a faster algorithm, and  directly incorporating the prior parameter α that improves estimates without requiring trial-and-error tuning. specifically, we utilize a two block coordinate descent method  <cit>  to alternately minimize the criterion for p and then for q. we adapt a fast non-negative least-squares algorithm  <cit>  to additionally include a sum-to-one constraint for q and an upper-bound for p. we show that the expected value for the estimates of p  across all possible genotype datasets are equal to the true values when q  are known and that the variance of this estimate approaches zero as the problem size increases. compared to admixture, the least-squares approach provides a slightly worse estimate of p or q when the other is known. however, when estimating p and q from only the genotype data, the least-squares approach sometimes provides better estimates, particularly with a large number of populations, small number of samples, or more admixed individuals. the least-squares approximation provides a simpler and faster algorithm, and we provide it as matlab scripts on our website.

RESULTS
first, we motivate a least-squares simplification of the binomial likelihood model by deriving the expected value and covariance of the least-squares estimate across all possible genotype matrices for partially solved problems. second, we compare least-squares to sequential quadratic programming  for these cases. third, we compare admixture, frappe, and least-squares using simulated datasets with a factorial design varying dataset properties in g. fourth, we compare admixture and least-squares using real population allele frequencies from the hapmap phase  <dig> project. finally, we compare the results of applying admixture and least-squares to real data from the hapmap phase  <dig> project where the true population structure is unknown.

the algorithms we discuss accept as input the number of populations, k, and the genotypes, gli∈{ <dig> ,2}, representing the number of copies of the reference allele at locus l for individual i. then, the algorithms attempt to infer the population allele frequencies, plk =  <cit> , for locus l and population k, as well as the individual admixture proportions, qki =  <cit>  where ∑kqki =  <dig>  in all cases,  <dig> ≤ l ≤ m,  <dig> ≤ i ≤ n, and  <dig> ≤ k ≤ k. table  <dig> summarizes the matrix notation.

empirical estimate and upper bound on total variance
to validate our derived bounds on the total variance , we generate simulated genotypes from a known target for p = t. we simulate n individual genotypes using the full matrix q with each column drawn from a dirichlet distribution with shape parameter α. we repeat the experiment  <dig> times producing an independent and identically distributed genotype each time. each trial produces one estimate for p. we then compute the mean and covariance of the estimates of p and compare them to those predicted in the bounds. for α =  <dig> and n =  <dig> 

  mean= <dig> .7002cov= <dig> − <dig> − <dig> .0046trace= <dig>  

the bound using the sample covariance of q in equation  <dig> provides the following:

  qqt= <dig> . <dig> .99trace≤ <dig>  

the bound using the properties of the dirichlet distribution in equation  <dig> provides a bound of  <dig> . as the number of samples increases, the difference between the bound and the asymptotic bound for the dirichlet distributed q will approach zero.

intuitively, the error in the least-squares estimate for p and q decreases as the number of individuals and the number of loci increases, respectively. figure  <dig> supports this notion, suggesting that on very large problems for which the gradient based and expectation maximization algorithms were designed, the error in the least-squares estimate approaches zero.

comparing least-squares approximation to binomial likelihood model
given estimates of the population allele frequencies, early research focused on estimating the individual admixture  <cit> . we also note that the number of iterations and convergence properties confound the comparison of iterative algorithms. to avoid these problems and emulate a practical research scenario, we compare least-squares to sequential quadratic programming  when p or q are known a priori. in this scenario, each algorithm converges in exactly one step making it possible to compare the underlying updates for p and q independently. for n =  <dig>   <dig>  and 10000; and α =  <dig> ,  <dig>  and 2; we consider a grid of two-dimensional points for p, where pi = { <dig> ,  <dig> , …,  <dig> }. for each trial, we first generate a random q such that every column is drawn from a dirichlet distribution with shape parameter, α. then, we randomly generate a genotype using equation  <dig>  we compute the least-squares solution using equation  <dig> and use matlab’s built-in function ‘fmincon’ to minimize the negative of the log-likelihood in equation  <dig>  similar to admixture’s approach. we repeat the process for  <dig> trials and aggregate the results.

simulated experiments to compare least-squares to admixture and frappe
in the previous sections, we consider the best-case scenario where the true value of p or q is known. in a realistic scenario, the algorithms must estimate both p and q from only the genotype information. table  <dig> summarizes the results of a four-way analysis of variance with 2-way interactions among experimental factors. by far the factor with the most impact on performance is the number of individuals, n. the degree of admixture, α, and the number of populations, k, accounts for the second and third most variation, respectively. these three factors and two-way interactions between them account for the vast majority of variation. in particular, the choice of algorithm accounts for less than about 1% of the variation in estimation performance. that is, when estimating population structure from genotype data, the number of samples, the number of populations, and the degree of admixture play a much more important role than the choice between least-squares, admixture, and frappe and least-squares. however, as shown in figure  <dig>  when considering the computation time required by the algorithm, the choice of algorithm contributes about 40% of the variation including interactions. therefore, for the range of population inference problems described in this study, the choice of algorithm plays a very small role in the estimation of p and q but a larger role in computation time.

further exploration reveals that the preferred algorithm depends on k, n, and α. table  <dig> lists the root mean squared error for the estimation of q for all combinations of parameters across n =  <dig> trials. out of the  <dig> scenarios, admixture, least-squares, and frappe perform significantly better than their peers  <dig>  six, and zero times, respectively; they perform insignificantly worse than the best algorithm  <dig>   <dig>  and  <dig> times, respectively. the least-squares algorithm appears to perform well on the more difficult problems with combinations of large k, small n, or large α. table  <dig> lists the root mean squared error for estimating p. for n =  <dig>  the algorithms do not perform significantly differently. for n =  <dig>  all algorithms perform with less than  <dig> % root mean squared error . in all, admixture performs significantly better than its peers  <dig> times out of  <dig>  however, admixture never performs significantly worse than its peers. least-squares and frappe perform insignificantly worse than admixture  <dig> and  <dig> times out of  <dig>  respectively. table  <dig> summarizes the timing results. least square converges significantly faster  <dig> out of  <dig> times with an insignificant difference for the remaining two scenarios. frappe converges significantly slower in all scenarios. with two exceptions, the least-squares algorithm provides a  <dig> - to 5-times speedup.

‘ad’ = admixture with ε = mn×10- <dig>  ‘ls1’ = least-squares with ε = mn×10- <dig> and α =  <dig>  ‘fr’ = frappe with ε =  <dig>  bold values indicate significantly less error than those without bold. ‘<’ indicates significantly less at  <dig> e- <dig> level, and ‘=’ indicates insignificant difference. ‘lsα’ = least-squares with correct α provided only for reference.

‘ad’ = admixture with ε = mn×10- <dig>  ‘ls1’ = least-squares with ε = mn×10- <dig> and α =  <dig>  ‘fr’ = frappe with ε =  <dig>  bold values indicate significantly less error than those without bold. ‘<’ indicates significantly less at  <dig> e- <dig> level, and ‘=’ indicates insignificant difference. ‘lsα’ = least-squares with correct α provided only for reference.

‘ad’ = admixture with ε = mn×10- <dig>  ‘ls1’ = least-squares with ε = mn×10- <dig> and α =  <dig>  ‘fr’ = frappe with ε =  <dig>  bold values indicate significantly less error than those without bold. ‘<’ indicates significantly less at  <dig> e- <dig> level, and ‘=’ indicates insignificant difference. ‘lsα’ = least-squares with correct α provided only for reference.

comparison on admixtures derived from the hapmap <dig> dataset
tables  <dig> and  <dig> lists the performance and computation time for the least-squares approach and admixture using a convergence threshold of ε =  <dig> e- <dig> and ε =  <dig> e- <dig>  respectively. each marker in the illustrations represents one individual. a short black line emanating from each marker indicates the offset from the original  position. for all simulations, the least-squares algorithms perform within  <dig> % of admixture for estimating the true population allele frequencies in p. for well-mixed populations in simulation  <dig> and  <dig>  the least-squares algorithms perform comparably well or even better than admixture. however, for less admixed data in simulations  <dig> –  <dig>  admixture provides better estimates of the true population proportions depicted in the scatter plots. in all cases, the least-squares algorithms perform within  <dig> % of admixture and between about 2- and 3-times faster than admixture.

the apparent advantage of admixture involves individuals on the periphery of the unit simplex defining the space of q. in table  <dig>  this corresponds to individuals on the boundary of the right triangle defined by the x-axis, y-axis, and y =  <dig> – x diagonal line. for simulation  <dig>  the original q contains very few individuals on the boundary, admixture estimates far more on the boundary, and the least-squares was closer to the ground truth. for simulation  <dig> –  <dig>  the ground truth contains more individuals on the boundary, admixture correctly estimates these boundary points but the least-squares algorithms predict fewer points on the boundary. simulation  <dig> provides the most obvious example where admixture estimates individuals exactly on the boundary and least-squares contains a jumble of individuals near but not exactly on the line.

real dataset from the hapmap phase  <dig> project
over  <dig> repeated trials, admixture converged in an average of  <dig>  seconds with standard deviation of  <dig>  seconds, and the least-squares approach converged in  <dig>  seconds with a standard deviation of  <dig>  seconds. figure  <dig> illustrates the inferred population proportions for one run. the relative placement of individuals from each known population is qualitatively similar. the two methods differ at extreme points such as those values of q <dig>  q <dig>  or  <dig> – q <dig> – q <dig> that are near zero. the admixture solution has more individuals on the boundary and the least-squares approach has fewer. although we cannot estimate the error of these estimates because the real world data has no ground truth, we can compare their results quantitatively. the admixture and the least-squares solution differed by an average of  <dig> % root mean squared difference across the  <dig> trials. we estimate α =  <dig>  from the admixture solution’s total variance using equation  <dig>  this roughly corresponds to the simulated experiment with three populations,  <dig> samples, and a degree of admixture of  <dig> . in that case, admixture and least-squares exhibited a very small root mean squared error of  <dig> % and  <dig> %, respectively .

discussion
this work contributes to the population inference literature by providing a novel simplification of the binomial likelihood model that improves the computational efficiency of discrete admixture inference. this approximation results in an inference algorithm based on minimizing the squared distance between the genotype matrix g and twice the product of the population allele frequencies and individual admixture proportions: 2pq. this euclidean distance-based interpretation aligns with previous results employing multivariate statistics. for example, researchers have found success using principal component analysis to reveal and remove stratification  <cit>  or even to reveal clusters of individuals in subpopulations  <cit> . recently, mcvean  <cit>  proposed a genealogical interpretation of principal component analysis and uses it to reveal information about migration, geographic isolation, and admixture. in particular, given two populations, individuals cluster along the first principal component. admixture proportion is the fractional distance between the two population centers. however, these cluster centers must known or inferred in order to estimate ancestral population allele frequencies. the least-squares approach infers these estimates efficiently and directly.

typically, discrete admixture models employ a binomial likelihood function rather than a euclidean distance-based one. pritchard et al. detail one such model and use a slow sampling based approach to infer the admixed ancestral populations for individuals in a sample  <cit> . recognizing the performance advantage of maximizing the likelihood rather than sampling the posterior, tang et al. proposed an expectation maximization algorithm and alexander et al.  <cit>  proposed a sequential quadratic programming  approach using the same likelihood function  <cit> . we take this approach a step further by simplifying the model proposed by pritchard et al. to introduce a least-squares criterion. by justifying the least-squares simplification, we connect the fast and practical multivariate statistical approaches to the theoretically grounded binomial likelihood model. we validate our approach on a variety of simulated and real datasets.

first, we show that if the true value of p  is known, the expected value of the least squares solution for q  across all possible genotype matrices is equal to the true value, and the variance of this estimate decreases with m . in this best-case scenario, we show that sqp provides a slightly better estimate than the least-squares solution for a variety of problem sizes and difficulty. for more common scenarios where the algorithms must estimate p and q using only the genotype information in g, we show that for particularly difficult problems with small n, large k, or large α, the least-squares approach often performs better than its peers. for about one-third of the parameter sets, admixture performs significantly better than least-squares and frappe but all algorithms approach zero error as n becomes very large. in addition, the error introduced by the choice of algorithms was relatively small compared to other characteristics of the experiment such as sample size, number of populations, and the degree of admixture in the sample. that is, improving accuracy has more to do with improving the dataset than with selecting the algorithm, suggesting that algorithm selection may depend on other criteria such as its speed. in nearly all cases, the least-squares method computes its solution faster, typically a  <dig> - to 5-times faster. at the current problem size involving about  <dig> loci, this speed improvement may justify the use of least-squares algorithms. for a single point estimate, researchers may prefer a slightly more accurate algorithm at the cost of seconds or minutes. for researchers testing several values of k and α and using multiple runs to gauge the fitness of each parameter set, or those estimating standard errors  <cit> , the speed improvement could be the difference between hours and days of computation. as the number of loci increase to hundreds of thousands or even millions, speed may be more important. the least-squares approach offers an alternative simpler and faster algorithm for population inference that provides qualitatively similar results.

the key speed advantage of the least-squares approach comes from a single nonnegative least-squares update that minimizes a quadratic criterion for p and then for q per iteration. admixture, on the other hand, minimizes several quadratic criteria sequentially as it fits the true binomial model. although the least-squares algorithm completes each update in less time and is guaranteed to converge to a local minimum or straddle point, predicting the number of iterations to convergence presents a challenge. we provide empirical timing results and note that selecting a suitable stopping criterion for these iterative methods can change the timing and accuracy results. for comparison, we use the same stopping criterion with published thresholds for admixture and frappe <cit> , and a threshold of mn×10- <dig> for least-squares.

this work is motivated in part by the desire to analyze larger genotype datasets. in this paper, we focus on the computational challenges of analyzing very large numbers of markers and individuals. however, linkage disequilibrium introduces correlations between loci that cannot be avoided in very large datasets. large datasets can be pruned to diminish the correlation between loci. for example, alexander et al. prune the hapmap phase  <dig> dataset from millions of snps down to around  <dig> to avoid correlations. in this study, we assume linkage equilibrium and therefore uncorrelated markers and limit our analysis to datasets less than about  <dig> snps. incorporating linkage disequilibrium in gradient-based optimizations of the binomial likelihood model remains an open problem.

estimating the number of populations k from the admixed samples continues to pose a difficult challenge for clustering algorithms in general and population inference in particular. in practice, experiments can be designed to include individual samples that are expected to be distributed close to their ancestors. for example, tang et al.  <cit>  suggested using domain knowledge to collect an appropriate number of pseudo-ancestors that reveal allele frequencies of the ancestral populations. the number of groups considered provides a convenient starting point for k. lacking domain knowledge, computational approaches can be used to try multiple reasonable values for k and evaluating their fitness. for example, pritchard et al.  <cit>  estimated the posterior distribution of k and select the most probable k. another approach is to evaluate the consistency of inference for different values of k. if the same value of k leads to very different inferences of p and q from different random starting points, the inference can be considered inconsistent. brunet et al.  <cit>  proposed this method of model selection called consensus clustering.

for realistic population allele frequencies, p, from the hapmap phase  <dig> dataset and very little admixture in q, admixture provides better estimates of q. the key advantage of admixture appears to be for individuals containing nearly zero contribution from one or more inferred populations, whereas the least-squares approach performs better when the individuals are well-mixed. visually, both approaches reveal population structure. using the two approaches to infer three ancestral populations from four hapmap phase  <dig> sampling populations reveals qualitatively similar results.

we believe the computational advantage of the least-squares approach along with its good estimation performance warrants further research especially for very large datasets. for example, we plan to adapt and apply the least-squares approach to datasets utilizing microsatellite data rather than snps and consider the case of more than two alleles per locus. researchers have incorporated geospatial information into sampling-based  <cit>  and pca-based  <cit>  approaches. multiple other extensions to sampling-based or pca-based algorithms have yet to be incorporated into faster gradient-based approaches.

CONCLUSIONS
this paper explores the utility of a least-squares approach for the inference of population structure in genotype datasets. whereas previous euclidean distance-based approaches received little theoretical justification, we show that a least-squares approach is the result of a first-order approximation of the negative log-likelihood function for the binomial generative model. in addition, we show that the error in this approximation approaches zero as the number of samples  increases. we compare our algorithm to state-of-the-art algorithms, admixture and frappe, for optimizing the binomial likelihood model, and show that our approach requires less time and performs comparably well. we provide both quantitative and visual comparisons that illustrate the advantage of admixture at estimating individuals with little admixture, and show that our approach infers qualitatively similar results. finally, we incorporate a degree of admixture parameter that improves estimates for known levels of admixture without requiring additional parameter tuning as is the case for admixture.

