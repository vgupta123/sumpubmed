BACKGROUND
the volume of biomedical literature available on the world wide web has experienced unprecedented growth in recent years. processing literature automatically by using bioinformatics tools can be invaluable for both the design and interpretation of large-scale experiments. for this reason, many information extraction  systems that incorporate natural language processing  techniques have been developed for use in the biomedical field. a key ie task in this field is the extraction of relations between named entities, such as protein-protein and gene-disease interactions.

many biomedical relation-extraction systems use either cooccurrence statistics or sentence-level methods for relation extraction. cooccurrence-based approaches extract biomedical relations by first tagging biomedical names and verbs in a text using dictionaries, and then identify cooccurrences of specific names and verbs in phrases, sentences, paragraphs, or abstracts. a variety of statistical tests, such as pointwise mutual information , the chi-square , and the log-likelihood ratio   <cit> , have been used to decide whether a relation expressed by cooccurrences between a given pair really exists  <cit> . sentence-level methods, on the other hand, usually consider only pairs of entities mentioned in the same sentence  <cit> . to detect and identify a relation, these systems generally use lexico-semantic clues inferred from the sentence context of the entity targets.

when extracting relations from complex natural language texts, both of the above approaches suffer from the same limitation in that they only consider the main relation targets and the verbs linking them. in other words, they frequently ignore phrases describing location, manner, timing, condition, and extent; however, in the biomedical field, these modifying phrases are especially important. biological processes can be divided into temporal or spatial molecular events, for example activation of a specific protein in a specific cell or inhibition of a gene by a protein at a particular time. having comprehensive information about when, where and how these events occur is essential for identifying the exact functions of proteins and the sequence of biochemical reactions. detecting the extra modifying information in natural language texts requires semantic analysis tools.

semantic role labelling , also called shallow semantic parsing  <cit> , is a popular semantic analysis technique. in srl, sentences are represented by one or more predicate-argument structures , also known as propositions  <cit> . each pas is composed of a predicate  and several arguments  that have different semantic roles, including main arguments such as an agent <dig> and a patient <dig>  as well as adjunct arguments, such as time, manner, and location. here, the term argument refers to a syntactic constituent of the sentence related to the predicate; and the term semantic role refers to the semantic relationship between a predicate  and an argument  of a sentence. for example, in figure  <dig>  the sentence "il <dig> and il <dig> receptors activate stat <dig>  stat <dig>  and stat <dig> proteins in the human b cells" describes a molecular activation process. it can be represented by a pas in which "activate" is the predicate, "il <dig> and il <dig> receptors" comprise the agent, "stat <dig>  stat <dig>  and stat <dig> proteins" comprise the patient, and "in the human b cells" is the location. thus, the agent, patient, and location are the arguments of the predicate.

since srl identifies the predicate and arguments of a pas, it is also called predicate argument recognition  <cit> . in the natural language processing field, srl has been implemented as a fully automatic process that can be operated by computer programs  <cit> . given a sentence, the srl task executes two steps: predicate identification and argument recognition. the first step can be achieved by using a part-of-speech  tagger with some filtering rules. then, the second step recognizes all arguments, including grouping words into arguments and classifying the arguments into semantic role categories. some studies refer to these two sub-steps as argument identification and argument classification, respectively  <cit> . in the second step, it is often difficult to determine the word boundaries and semantic roles of an argument as they depend on many factors, such as the argument's position, the predicate's voice  and the sense .

srl has been applied to information extraction because it can transform different types of surface texts that describe events into pas'. in the newswire domain, morarescu et al.  <cit>  showed that, by incorporating semantic role information into an ie system, the f-score of the system can be improved by 15% . this finding motivated us to investigate whether srl could also facilitate information extraction in the biomedical field. in fact, most of the top open-domain srl systems use machine-learning-based approaches  <cit> . however, at present, there is no large-scale machine-learning-based biomedical srl system because of the lack of a sufficiently large annotated corpus for training.

in this paper, we propose an srl system for the biomedical domain called biosmile . an annotated corpus and a pas standard are essential for the construction of a biomedical srl system. considering our purpose is to train a machine learning srl system, we use propbank  <cit>  and follow its annotation guidelines. since propbank must be annotated on a corpus containing full-parsing information , we use the genia corpus, which includes  <dig> abstracts with full-parsing information. to evaluate srl for use in the biomedical domain, we started with thirty verbs, which were selected because of their high frequency or important usage in describing molecular events. we employed a semi-automatic strategy using our previously created newswire srl system smile   <cit>  to tag a corpus derived from the genia corpus, and then asked human annotators with a background in molecular biology to verify the automatically tagged results. the resulting annotated corpus is called bioprop  <cit> . lastly, we trained a biomedical version of smile on bioprop to construct an srl system called biosmile for the biomedical domain. to improve biosmile's performance on adjunct arguments, which are phrases indicating the time, location, or manner of an event, we further exploit automatically generated patterns.

the corpus construction process is explained in the background section, and the construction of our biomedical srl system is described in the methods section.

corpus selection
to construct bioprop, a biomedical proposition bank, we adopted genia  <cit>  as the underlying corpus. it is a collection of  <dig>  medline abstracts selected from the search results for queries using the keywords "human", "blood cells", or "transcription factors". genia is often used as a biomedical text mining test bed  <cit> . in its officially released version, it is annotated with various levels of linguistic information, such as parts-of-speech, named entities, and conjunctions. in the summer of  <dig>  tateisi  <cit>  published full parsing information for the corpus that basically follows the penn treebank ii  annotation scheme  <cit>  encoded in xml. the genia corpus annotated with full parsing information is called genia treebank . currently, gtb is a beta version containing  <dig> abstracts.

verb selection
as noted earlier, we chose thirty verbs because of their high frequency or important usage in describing molecular events. to select the verbs, we calculated the frequency of each verb based on its occurrence in genia, our underlying corpus, rather than in medline. it is noteworthy that some verbs that occur frequently in medline are rarely found in genia. since we focus on molecular events, only sentences containing protein or gene names are used to calculate a verb's frequency. we listed verbs according to their frequency and removed generally used verbs such as is, have, show, use, do, and suggest. we then selected the verbs with highest frequencies and added some verbs of biological importance. the thirty verbs with their characteristics and frequency of occurrence in bioprop are listed in table  <dig> 

pas standard – proposition bank
to build our srl system, we followed the propbank i  <cit>  standard. propbank i, with more than ten years of development history, has a large verb lexicon, and contains more annotated examples than other standards  <cit> . in propbank i, a pas is annotated on top of a penn-style full parsing tree. figure  <dig> illustrates such a tree with syntactic and semantic role information. the semantic roles arg <dig>  arg <dig>  and argm-loc are annotated on top of the words or phrases labelled as noun phrase subjects , noun phrases , and prepositional phrases , respectively. a proposition bank is a collection of full parsing trees annotated with propositions or pas'. the first annotated propbank-style proposition bank was the wall street journal  newswire corpus, which has  <dig> sections. before being annotated with propositions, it was annotated with penn-style full parsing trees. sections  <dig> to  <dig> are conventionally used as a training set, section  <dig> is used as a development set, and section  <dig> is used as a test set in several nlp tasks  <cit> .

propbank i inherits verb senses from verbnet, but the semantic arguments of individual verbs in propbank i are numbered from  <dig>  for a specific verb, arg <dig> is usually the argument corresponding to the agent  <cit> , while arg <dig> usually corresponds to the patient or theme. for higher-numbered arguments, however, there is no consistent generalization for their roles. in addition to the main arguments, argms refer to adjunct arguments. table  <dig> details all the semantic role categories of arguments and their descriptions. the possible set of roles for a distinct sense of a verb is called a roleset, which can be paired with a set of syntactic frames that show all the acceptable syntactic expressions of those roles. a roleset with its associated frames is called a frameset  <cit> . verbs may have different rolesets and framesets for different senses, which are numbered . <dig>  . <dig>  etc. an example of the frameset is given by the verb activate shown below.

frameset activate. <dig> "make active"

arg0: activator

arg1: thing activated

arg2: activated-from

arg3: attribute

ex1:  activate  .

ex2:  is   activated .

framesets of biomedical verbs
basically, the annotation of bioprop is based on propbank's framesets, which were originally designed for newswire texts. we further customize the framesets of biomedical verbs, since some of them may have different usages in biomedical texts. table  <dig> indicates whether each verb has the same usage in the newswire and biomedical domains.

for verbs with the same usage in both domains, we adopt the newswire definitions and framesets. however, we need to make adjustments for other cases because some verbs have different usages and rarely appear in newswire texts. thus, they are not defined in propbank i. for example, "phosphorylate" is not defined in propbank i, but it has been found increasingly in pubmed abstracts describing the experiment results of phosphorylated events  <cit> . therefore, after analyzing every sentence in our corpus containing such verbs, we added the latter to our list and defined framesets for them. for verbs not found in propbank i, but with similar usages to other verbs in the proposition bank, we borrowed the propbank i definitions and framesets. for instance, "transactivate" is not found in propbank i, but we can apply the frameset of "activate" to it.

some verbs have unique biomedical meanings not defined in propbank i; however, their usage is similar to verbs in propbank i. in most cases, we borrow framesets from synonyms. for example, "modulate" is defined as "change, modify, esp. of music" in the propbank i frame files. however, its usage is very similar to "regulate" in the biomedical domain. thus, we can use the frameset of "regulate" for "modulate". table  <dig> shows the framesets and corresponding examples of "modulate" in the newswire and biomedical domains, as well as those of "regulate" in propbank i.

arg1: music
arg2: from
some other verbs have different primary senses in the newswire and biomedical domains. "bind", for example, is common in the newswire domain and usually means "to tie" or "restrain with bonds". in the biomedical domain, however, its intransitive use, "attach or stick to", is far more common. a google search for the phrase "glue binds to" only returns  <dig> results, while the same search replacing "glue" with "protein" yields  <dig>  hits. for such verbs, we just select the appropriate alternative meanings and corresponding framesets.

annotation of bioprop
once the framesets for the verbs have been defined, we use a semi-automatic strategy to annotate bioprop. we used our newswire srl system smile, which achieved an f-score of over 86% on section  <dig> of propbank i, to annotate the genia treebank automatically. then, we asked three biologists to verify the automatically tagged results. one of the biologists has three years experience in biomedical text mining research, and he managed the task. the other two biologists received three months of linguistic training for this task. after annotating bioprop, we evaluated the performance of smile on bioprop. the f-score was approximately 65%, which is 20% lower than its performance on propbank i. even so, this semi-automatic approach substantially reduces the annotation effort.

inter-annotator agreement
we performed a preliminary consistency test on  <dig>  instances of biomedical propositions by having two of the biologists annotate the results, while the third checked the annotations for consistency. following the procedure used to calculate the inner-annotator agreement of propbank  <cit> , we measured the agreement between the two annotations before the adjudication step using the kappa statistic  <cit> . kappa is defined with respect to the probability of inter-annotator agreement, p, and the agreement expected by chance, p, as follows:

 κ=p−p1−p
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwf6owacqgh9aqpdawcaaqaaiabdcfaqjabcicaoiabdgeabjabcmcapiabgkhitiabdcfaqjabcicaoiabdweafjabcmcapaqaaiabigdaxiabgkhitiabdcfaqjabcicaoiabdweafjabcmcapaaaaaa@3e07@ 

the inter-annotator agreement probability p is the number of nodes that the annotators agree on the annotation divided by the total number of nodes considered. to calculate p, for each category c, let p1c denote the probability of c annotated by annotator  <dig>  and p2c denote the probability annotated by annotator  <dig>  then p is the summation of p1c * p2c over all categories c of the semantic role labels. however, the calculation of p and p is distinguished into two cases that correspond to role identification  and role classification, since the vast majority of arguments are located on a small number of nodes near the verb and we need to avoid inflating the kappa score artificially. for role identification, the denominator of p and p the total number of nodes considered, is given by the number of nodes in each parse tree multiplied by the number of predicates annotated in the sentence, and the numerator is given by the number of nodes that are labeled as arguments . for the role classification kappa, we only consider nodes marked as arguments by both annotators, which yields the denominator of p and p, and compute kappa over the choices of possible argument labels. furthermore, for both role identification and role classification, we compute kappa to process argm labels in two ways. the first  processes argm labels as arguments like any other type of argument, such that argm-tmp, argm-loc and so on are considered as separate labels for the role classification kappa. in the second scenario , we ignore argm labels, treating them as unlabeled nodes, and calculate the agreement for identification and classification of numbered arguments only.

the kappa statistics for the above decisions are shown in table  <dig>  given the large number of obviously irrelevant nodes, agreement on role identification is very high . the kappas for the more difficult role classification task are also high, . <dig> for all types of argm and . <dig> for numbered arguments only.

related work
wattarujeekrit et al.  <cit>  developed pasbio, which has become a standard for annotating predicate-argument structures in the biomedical domain. it contains analyzed pas's for over  <dig> verbs and is publicly available. using predicate argument structures to analyze molecular biology information, pasbio is specifically designed for annotating molecular events and defines a core argument as one that is important for completing the meaning of an event. if a locative argument appears in a specific molecular pas with a frequency greater than 80%, it is considered necessary and is therefore a main argument. to describe molecular events in greater detail, pasbio places biomedical constraints on main arguments. for example, considering the verb "express", its arg <dig>  which is defined as named entity being expressed, is limited to a gene or gene products.

shah et al.  <cit>  successfully applied pasbio in the construction of the lsat system for extracting information about alternative transcripts from the same gene, while cohen et al.  <cit>  showed that the suitability of the pas representational model of representation for biomedical text. they concluded that pas representations work well for biomedical text. kogan et al.  <cit>  built a domain-specific set of pas for the medical domain. their work agrees a bit more with ours in terms of their assessment of the match between propbank's representations and the biomedical domain.

unlike pasbio, bioprop is not a standard for annotating the pas' of biomedical verbs. the main goal of bioprop is to port the proposition bank to the biomedical domain for training a biomedical srl system. thus, bioprop follows propbank guidelines and uses the latter's framesets with further customization for some biomedical verbs. subsequently, we use propbank i as our initial training corpus for the construction of bioprop, and then ask annotators to refine the automatically tagged results. this semi-automatic approach substantially reduces the annotation effort so that bioprop can be used for training srl systems in the biomedical domain.

RESULTS
datasets
we use propbank i and bioprop, which are associated with the general english and biomedical domains, respectively, as the sources of our data. the propbank i corpus contains  <dig>  words,  <dig>  sentences, and  <dig>  pas'. however, only  <dig>  of the pas use the  <dig> biomedical verbs on our list as their predicates. bioprop currently contains  <dig>  pas'. the numbers and ratios of each argument type in the pas' of the selected  <dig> biomedical verbs in propbank i and bioprop are listed in tables  <dig> and  <dig>  respectively.

smile and biosmile
we use two srl systems: smile  <cit>  and biosmile  <cit> . the main difference between them is that smile is trained on propbank i, while biosmile is trained on bioprop. in addition, biosmile has additional biomedical-specific features. details of the features and the statistical models used in the two systems will be introduced in the methods section.

experiment design
we design two experiments: one to compare the performance of smile and biosmile on biomedical applications by testing them on bioprop, and the other to measure the effects of using biomedical-specific features on the system's performance.

experiment 1: improvement by using biomedical proposition bank
since smile and biosmile are trained on the corpora of different domains, in this experiment, we examine the improvement in the performance of the srl system trained on a biomedical proposition bank. since the size of the training corpus affects the performance of an srl system, we need to use corpora of the same size for training smile and biosmile in order to accurately compare the effects of using newswire training data with those of using biomedical data. because propbank and bioprop are of different size, we use limited selections from both.

before testing smile and biosmile on bioprop, we train the two systems on different training sets of  <dig> randomly chosen sets from propbank  and bioprop , respectively. each set contains  <dig>  pas's. after the training process, we test both systems on  <dig> 400-pas test sets from bioprop . we then sum the scores for g1-g <dig> and w1-w <dig>  and calculate the averages for performance comparison. in experiment  <dig>  both smile and biosmile use the baseline features illustrated in the methods section. we denote the systems as smile and biosmilebaseline, respectively.

experiment 2: the effect of using biomedical-specific features
to improve the performance of srl on biomedical literature, we add two domain specific features, ne features and argument-template features  to biosmile. this experiment tests the effectiveness of adding the features to biosmilebaseline and uses the same datasets as biosmilebaseline.

bio-specific ne features are created for each of the following five primary named entity  categories in the genia ontology3: protein, nucleotide, other organic compounds, source, and others. when a constituent  matches an ne exactly, the corresponding ne feature is enabled.

additionally, we integrate argument-template features. usually, each argument type has its own patterns. for example, in the biomedical domain, the regular expression "in * cell" is a locative argument pattern . we automatically generate argument templates, which are composed of words, nes, and pos's, to represent the patterns of each argument. these templates are generated by using the smith and waterman local alignment algorithm  <cit>  to align all instances of a specific argument type. the template feature is enabled if a constituent matches a template exactly. ne features and argument template features are discussed further in the methods section.

evaluation metrics
the results are given as f-scores using the conll- <dig> evaluation script and defined as f = /, where p denotes the precision and r denotes the recall. the formulas for calculating precision and recall are as follows:

 precision=the number of correctly recognized argumentsthe number of recognized argumentsrecall=the number of correctly recognized argumentsthe number of true arguments
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakqaaeeqaaiabbcfaqjabbkhayjabbwgaljabbogajjabbmgapjabbohazjabbmgapjabb+gavjabb6gaujabg2da9maalaaabagaeeidaqnaeeiaagmaeeyzaumaeeiiaaiaeeoba4maeeydaunaeeyba0maeeoyaimaeeyzaumaeeocainaeeiiaaiaee4ba8maeeozaymaeeiiaaiaee4yammaee4ba8maeeocainaeeocainaeeyzaumaee4yammaeeidaqnaeeibawmaeeyeaknaeeiiaaiaeeocainaeeyzaumaee4yammaee4ba8maee4zacmaeeoba4maeeyaakmaeeoeaonaeeyzaumaeeizaqmaeeiiaaiaeeyyaemaeeocainaee4zacmaeeydaunaeeyba0maeeyzaumaeeoba4maeeidaqnaee4camhabagaeeidaqnaeeiaagmaeeyzaumaeeiiaaiaeeoba4maeeydaunaeeyba0maeeoyaimaeeyzaumaeeocainaeeiiaaiaee4ba8maeeozaymaeeiiaaiaeeocainaeeyzaumaee4yammaee4ba8maee4zacmaeeoba4maeeyaakmaeeoeaonaeeyzaumaeeizaqmaeeiiaaiaeeyyaemaeeocainaee4zacmaeeydaunaeeyba0maeeyzaumaeeoba4maeeidaqnaee4camhaaaqaaiabbkfasjabbwgaljabbogajjabbggahjabbygasjabbygasjabg2da9maalaaabagaeeidaqnaeeiaagmaeeyzaumaeeiiaaiaeeoba4maeeydaunaeeyba0maeeoyaimaeeyzaumaeeocainaeeiiaaiaee4ba8maeeozaymaeeiiaaiaee4yammaee4ba8maeeocainaeeocainaeeyzaumaee4yammaeeidaqnaeeibawmaeeyeaknaeeiiaaiaeeocainaeeyzaumaee4yammaee4ba8maee4zacmaeeoba4maeeyaakmaeeoeaonaeeyzaumaeeizaqmaeeiiaaiaeeyyaemaeeocainaee4zacmaeeydaunaeeyba0maeeyzaumaeeoba4maeeidaqnaee4camhabagaeeidaqnaeeiaagmaeeyzaumaeeiiaaiaeeoba4maeeydaunaeeyba0maeeoyaimaeeyzaumaeeocainaeeiiaaiaee4ba8maeeozaymaeeiiaaiaeeidaqnaeeocainaeeydaunaeeyzaumaeeiiaaiaeeyyaemaeeocainaee4zacmaeeydaunaeeyba0maeeyzaumaeeoba4maeeidaqnaee4camhaaaaaaa@05e3@ 

results mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgqbaugaqcaaaa@2de5@, r^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgsbgugaqcaaaa@2de9@, and f^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwggbgrgaqcaaaa@2dd1@, we also list the sample standard deviation of the f-score  for each argument type. we apply a two-sample t test to examine whether one configuration is better than the other with statistical significance. the null hypothesis, which states that there is no difference between the two configurations, is given by

 h0: μa = μb, 

where μa is the true mean f-score of configuration a, μb is the mean of configuration b, and the alternative hypothesis is

 h1: μa > μb. 

a two-sample t-test is applied since we assume the samples are independent. as the number of samples is large and the samples' standard deviations are known, the following two-sample t-statistic is appropriate in this case:

 t=sa2na+sb2nb.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwg0badcqgh9aqpdawcaaqaaiabcicaoiqbdiha4zaarawaasbaasqaaiabdgeabbqabagccqghsislcuwg4baegaqeamaabaaaleaacqwgcbgqaeqaaogaeiykakcabawaaoaaaeaadawcaaqaaiabdofatnaadaaaleaacqwgbbqqaeaacqaiyagmaaaakeaacqwgubgbdawgaawcbagaemyqaeeabeaaaagccqghrawkdawcaaqaaiabdofatnaadaaaleaacqwgcbgqaeaacqaiyagmaaaakeaacqwgubgbdawgaawcbagaemoqaieabeaaaaaabeaaaagccqgguaglaaa@4584@ 

if the resulting t score is equal to or less than  <dig>  with a degree of freedom of  <dig> and a statistical significance level of 95%, the null hypothesis is accepted; otherwise it is rejected. even though we do not list all the argument types in tables  <dig>   <dig>  and  <dig> , we calculate the overall score by checking all argument types.

experiment 1
experiment 2
argument template features, on the other hand, boost the system's performance. table  <dig> compares the performance of argument templates on biosmilebaseline and biosmiletemplate. the overall f-score is only improved slightly . however, we achieve  <dig> %,  <dig> %, and  <dig> % increases in the f-scores of argm-adv, argm-loc, and argm-mnr, respectively. these improvements are statistically significant. although the increase in argm-tmp's f-score is not statistically significant, it is still appreciable. figure  <dig> gives a clearer illustration of the improved performance of these argument types.

discussion
experiment  <dig> demonstrates that biosmilebaseline outperforms smile by more than 20%. experiment  <dig> shows that ne features do not improve biosmile's performance, but template features do. next, we further analyze the results and discuss possible reasons for them.

the influence of sense change on the biomedical and newswire domains
according to the results of experiment  <dig>  verbs with different framesets in the newswire and biomedical domain exhibit larger differences in performance between smile and biosmilebaseline. table  <dig> details the average f-score difference between verbs with different framesets in both domains and verbs with the same framesets in both domains; the verb types are defined in the background section . these performance differences suggest that variations in cross-domain framesets and the usage of specific verbs influence srl performance.

why ne features are not effective
in the newswire domain, ne features have proven effective in improving srl performance. however, the results of experiment  <dig> show that we did not have the same success in the biomedical domain. table  <dig> lists the five ne classes used and the number of nes that occur in the main arguments. though arguments frequently contain nes, according to our findings, the converse does not hold. we find that most nes match the null arguments, i.e., they match the nodes that are not labelled by biosmile and, equivalently, do not correspond to the argument types of interest to us. thus, trained on this data, a machine learning model would give so much weight to the null class that it would render all ne features ineffective for argument classification.

performance gained using template features
only five argument types have a sufficient number of generated templates to be useful: argm-dis, argm-mnr, argm-adv, argm-tmp, and argm-loc. we do not generate templates for arguments like argm-mod and argm-neg as they are usually composed of single words, such as "can"  and "not" . baseline features, such as headword features, can generally recognize these arguments with a high degree of accuracy . templates for arg <dig> and arg <dig> are difficult to implement because a phrase pattern tagged as arg <dig> may be tagged as arg <dig> elsewhere, which results in all generated patterns being filtered out. table  <dig> lists the performance improvement generated by template features on the five specified argument types, based on the improvement of the f-score over the baseline biosmile. it also reports the number of templates, the number of instances, and the template density  for each type.

figures  <dig> and  <dig> further illustrate the positive logarithmic correlation between template density and f-score difference  for each argument type. Δf initially increases with template density, but then appears to taper off. note that the r-squared between Δf and the logarithmic template density is  <dig> . however, after removing argm-adv from the data, the r-squared increases to  <dig> , as shown in figure  <dig>  there are two possible explanations for argm-adv's exceptionally high Δf. first, its baseline f-score value is the lowest  among the five adjunct arguments. thus, increasing its f-score would be the easiest among the five arguments. second, its average length is the longest so that its templates longer and possibly more precise.

here, we give an example to illustrate how template features recognize an argm-tmp. table  <dig> shows the features of each word in the sentence: "nac not only blocks the effect of tpck, but also enhances mitogenesis and cytokine production  upon activation of unsuppressed t cells." the phrase "upon activation of unsuppressed t cells" matches the argm-tmp template "in nn in src src cells". each template slot indicates the allowable real words, ne tags, or pos tags.  however, the baseline biosmile can not recognize this phrase by itself . turning template features on, however, correctly identifies the argm-tmp, as shown in column six.

CONCLUSIONS
to improve the performance of srl in the biomedical domain, we have developed biosmile, a biomedical srl system trained on a biomedical proposition bank called bioprop. the construction of bioprop is based on a semi-automatic strategy. since our experiment results show that the differences in framesets and the usage variations of verbs in the biomedical and newswire proposition banks affect the performance of the underlying srl systems, the necessity of training biosmile on a biomedical proposition bank has been demonstrated. biosmile is capable of processing the pas' of thirty verbs selected according to their frequency and importance in describing molecular events. incorporating automatically generated templates enhances the overall performance of argument classification, especially for locations, manners, and adverbs.

finally, the following related issues remain to be addressed in our future work:  enhancing the system by adding more biomedical verbs to bioprop and integrating an automatic penn-style parser into biosmile;  applying biosmile to other biomedical text mining systems, such as relation extraction and question answering systems;  examining the effectiveness of using biosmile in other biomedical corpora; and  extracting biomedical relations expressed across sentences through srl.

