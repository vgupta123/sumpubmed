BACKGROUND
transcriptome analysis by microarray technology has become a routine tool in many research areas ranging from basic cell biology to clinical research  <cit> . almost as broad as the range of applications is the number of array formats and chip generations available, each with its individual scientific, economic or practical strengths and weaknesses. furthermore, prices continue to decline as the market develops, so that more researchers gain access to microarray technology, generating and banking even more transcriptome data in public databases such as gene expression omnibus   <cit>  or array express  <cit> . there are currently over  <dig> samples  stored in the geo database, which were recorded on more than  <dig> different microarray platforms .

considering the amount of data and platforms already available, we believe it is becoming increasingly important to cross-compare data generated by different research groups. in the past, this has mostly been done via meta-analysis studies, such as the microarray quality control consortium  study i, comparing the outcomes of different microarray projects  <cit> . a direct comparison of raw data from different research groups was hampered by the different data formats of the various array types and by batch effects obscuring meaningful information with systematic non–biological perturbations. these derive for example from differences in sample preparation and hybridization protocols, lot-to-lot variability, limited shelf-life of microarrays, and, most importantly, differences intrinsic to the platforms themselves  <cit> .

to address these problems, a number of algorithms have been designed to reduce batch effects. mean centering, implemented in the “pamr” r package , and standardization, implemented e.g. in the dchip software  <cit>  function at a rather superficial and global level, while cross-platform normalization  and empirical bayes methods  are more sophisticated algorithms that work more flexibly on a smaller per gene or per cluster basis. the ability of these and other algorithms to remove batch effects has been assessed by different groups  <cit> . while batch effects are reduced by all methods, in particular situations and especially in the case of smaller datasets, xpn and ebm have been shown to outperform the others. a downside of all methods mentioned is that they require one consistent dataset and are thus applicable only to cross-batch but single-platform problems. although cross-platform mappings are possible, current implementations only support meta-analysis  <cit> . a straightforward and easy to use tool to combine raw data from different platforms has been lacking.

to fill this gap we have developed the r/bioconductor package virtualarray  <cit> . the package is able to integrate raw data from most microarray platforms available and generates a combined “expressionset” object, allowing unrestricted further manipulation and analysis in r and other software. raw microarray data can be matched by transcript, gene, protein or any identifiers known to r. and most importantly, batch effects are removed by a method of choice . in total there are seven methods directly available in the virtualarray package for multi-platform batch effect removal: quantile discretization , normal discretization normalization , gene quantile normalization , median rank scores , quantile normalization , empirical bayes methods  and mean centering .

implementation
general aspects and design
all parts of the software are written in the r programming language  <cit>  and rely on the bioconductor  <cit>  extension packages. the package has two central functions:

firstly, the "virtualarraycompile" function can integrate the major human microarray platforms in a default mode. it requires minimal user input, but is restricted to the most commonly used platforms. the second function is called "virtualarrayexpressionsets". this function can integrate any kind of raw expression data that can be loaded into an expressionset object in r/bioc. while being highly versatile, the user has to deal with details such as logarithmic transformations, depth of data precision , or assignment of correct annotations.

the data precision in bit can be critical, because the raw data for each microarray can derive from different array scanners. a scanner with a  <dig> bit precision for example uses its analog-digital converter to assign a value between  <dig> and  <dig> to a given point on that array, whereas a  <dig> bit precision would allow assigning values between  <dig> and  <dig>  when comparing the resulting data, it is necessary to take these differences into account.

if no bioconductor annotation package is available for a particular chip type, it is possible to create one using the packages annotationforge and sqlforge  <cit> .

additionally, each of these two approaches can be used with a method of choice to remove multi-platform batch effects. there are seven methods available within the virtualarray package: ebm, gq, qn, qd, mc, mrs, nordi. the default method is ebm, which can be used either in a supervised or in a non-supervised mode  <cit> . the supervised mode allows to “pre-cluster” samples according to their biological or experimental origin by assigning covariates . the grouping has an impact on the results, and should hence be correct and complete for all samples included. last but not least it is possible to use the package to integrate data without batch effect removal, so that other, user-defined, methods of batch effect removal can be employed later. the combined data is presented as a regular bioconductor "expressionset" object, which allows the subsequent implementation of all r/bioconductor functions and packages on the dataset.

detailed stepwise explanations
the procedure that is performed by virtualarray can be split up into several steps. the first two steps are prerequisites involving user input and need to be set up before employing the package. from step  <dig> onwards everything is run without user intervention. steps  <dig> and  <dig> act on one batch/chip type at a time, whereas the last three steps are applied to all batches/chip types simultaneously, resulting ultimately in the creation of a new “expressionset” object. a scheme of all steps is shown in figure  <dig> 

step  <dig> – loading and storage of raw data
the raw data must be provided as expressionsets in bioconductor by means of manufacturer specific packages e.g. "affy"  <cit> , "lumi"  <cit>  or "limma"  <cit> . the "annotation" slot of the expressionset must contain the name of a bioconductor compliant annotation package. this should be checked and adjusted manually, if necessary. this is particularly important when pulling data from ncbi geo  <cit>  or ebi arrayexpress  <cit> .

step  <dig> – transformations of raw data
even samples from the same platform may yield raw data in different formats dependent on the hardware employed or the mode of measurement. thus, each dataset needs to be transformed to one common scale  and one common precision  by using standard r functions on the "exprs" slot of the expressionsets. in the case of personally collected data the precision of the raw data may be known. it is also possible, that this information was deposited along with the data in an ncbi geo database entry. if only information on the scanner used is available, the precision can possibly be obtained from the manufacturer’s website. when the precision is unknown it can be determined empirically .

step  <dig> – annotation of raw data
raw data are comprised of expression levels annotated with manufacturer specific ids that cannot be matched across platforms directly. in order to allow a later matching of corresponding pairs, step  <dig> annotates common identifiers to each single dataset. the default common identifier in "virtualarrayexpressionsets()" is gene symbols . however, any identifier present in the annotation packages, including identifiers for genes, transcripts or proteins can be used.

step  <dig> – collapsing of redundant probesets
in many chips, several probes or probesets target the same gene, transcript or protein, resulting in >  <dig> entry for otherwise unique identifiers. thus, before the annotated common identifiers can be matched, redundant rows need to be collapsed to a single value. this is done by either selecting the "median"  or applying a user supplied function, e.g. “medpolish” or “mean”. this operation reduces the size of the expression matrices .

several major microarray chip platforms have been tested with virtualarray. the collapsing of probes/probesets was based on gene symbols, entrez id, unigene id or ensembl id, resulting in different reduced feature numbers . when two or three platforms are merged, the feature number is further reduced. however, the fraction of overlap in respect to the single chips was always above 80%.

step  <dig> – compilation of the virtual array
in the next step, the software matches common identifiers. a new expression matrix is built, that includes only the rows for identifiers that are present in all datasets. non-matching rows are discarded.

step  <dig> – construction of new expressionset
virtualarray now constructs a new expressionset object using the expression matrix generated in step  <dig> and a "pdata" slot that contains the array and sample names as well as pre-existing “pdata” and the relations between batches and samples. thus, each sample carries its parent batch as an attribute and can be directly linked to it during the process.

step  <dig> – removal of batch effects
the newly generated expressionset can now either be returned without further modifications or directly subjected to batch effect removal using empirical bayes methods as a default. this can be decided by the user with the logical or character vector "removebatcheffects". selecting “removebatcheffects=false” will result in a non-adjusted expressionset. a value of qd, nordi, gq, mrs, qn, eb or mc can be used to remove batch effects on the basis of quantile discretization  <cit> ), normal discretization normalization  <cit> , gene quantile normalization  <cit> ), median rank scores  <cit> , quantile normalization  <cit> ), empirical bayes methods  <cit>  and mean centering  <cit> , respectively.

note, however, that even the contents of a resulting non-adjusted expressionset are not a simple concatenation of the input expression matrices. on the one hand incompatible probes/probesets are excluded during the process. on the other hand expression values targeting the same identifier  are collapsed by the function defined in the first place .

RESULTS
combining three human microarray studies from different platforms using defaults 
in order to demonstrate an application of the package, a consistent dataset is compiled out of three different previously published studies carried out on affymetrics, agilent and illumina platforms, respectively. each study features datasets from human induced pluripotent stem cells , human fibroblasts, and human embryonic stem cells . we selected the studies gse <dig>  <cit> , gse <dig>  <cit>  and gse <dig>  <cit>  for this example. before being able to apply the virtualarray package to these datasets, they need to be prepared to meet the following requirements: raw data must be log2-scaled and all datasets must exhibit the same data precision. a detailed explanation of all steps needed to fulfill these prerequisites can be found in the additional file  <dig> and in the package documentation.

firstly, raw data from the studies were pulled from the ncbi geo database. the raw data of each dataset are imported into r and stored in a regular expressionset by means of the geoquery  <cit>  package:

> gse <dig> <− getgeo

> gse <dig> <− getgeo

> gse <dig> <− getgeo

> gse <dig> <− gse23402

> gse <dig> <− gse26428

> gse <dig> <− gse28688

now the compatibility of all data has to be assured. and all three datasets are transformed into log <dig> space and  <dig> bit precision as follows:

> exprs <− log2)

> exprs <− /20*16)

> exprs <− log <dig> )

a bioconductor compliant annotation is now assigned to the expressionsets. however, this step only hands over the name of the annotation packages, while the packages themselves are fetched automatically later on. note that the spelling of the annotation in quotation marks must be correct, in order to assure bioconductor compliance. expressionsets downloaded from ncbi geo already contain a gpl code annotation. the most commonly used ones can be directly converted into bioconductor compliant ones by virtualarray. this is true in the case of the example datasets used here. however, if a gpl code is not available, or the source of the data is not ncbi geo, an additional step is required to derive correct annotations. an example for this is shown in the additional file  <dig> 

at this point there are three expressionsets present in the current r workspace that have their expression values presented as log2-transformed in  <dig> bit precision with the correct annotation package linked. the virtual array can now be compiled in a very easy way by a single call:

> virtarrays <− list()

> virtarrays] <− virtualarrayexpressionsets()

the default options in this call annotate probes and probesets with gene symbols, then collapse probes and probesets targeting the same gene symbol to their median. a batch effect removal is performed using empirical bayes methods in non-supervised mode, taking only batch contribution of the samples into account.

combining three human microarray studies from different platforms without batch effect removal 
to see the impact of the batch effect, another expressionset without batch effect removal can be compiled as follows:

> virtarrays] <−

virtualarrayexpressionsets

despite omitting batch effect removal the resulting expressionset is not equivalent to the raw data, because redundant values have been collapsed and genes with missing values discarded. thus the reduction of the expression matrix depends on the general overlap of the platforms concerned and the degree of completion of the annotation packages.

impact of batch effect on output expressionsets
the two new expressionsets can be used to illustrate the batch effect. distance matrices were derived from both expressionsets using euclidian distances. these were then used to create hierarchical clusterings based on average linkage .

the examples illustrate that the biggest source of variation in the dataset without batch effect removal  is batch contribution, which prohibits any valuable analysis of the underlying biology. on the other hand, the same data become biologically meaningful after batch effect removal : there are two clusters of fibroblasts and one cluster of pluripotent cells, indicating that biological variance has now become the main source of variation.

improving outcome with user input – supervised mode 
while batch effect removal in the non-supervised mode resulted in a dramatic improvement, the result can be further improved via the assignment of samples into groups by choice . the basis for this, however, is that in addition to the batch information other attributes are made available . this additional information can be provided in a column in the “pdata” slot common to all single expressionsets ”). another way to store this information would be a data.frame or tab delimited text file holding a “sample_info” table . the third option allows the creation of a sample_info.txt file on the fly in the current working directory, prompting the user to modify it with respect to additional sample information. the detailed usage can be found in the package documentation.

the first two columns need to correspond to the sample names used in the expressionsets, respectively. in column  <dig> the contribution of individual samples to batches is tracked. finally, column  <dig> contains user defined group assignments of each sample. group assignments  can include more than one column, for example to include source tissue, sex, age, etc.

in the following example we will hand over the “sampleinfo=’create’” parameter to the “virtualarrayexpressionsets” function to pass on the information:

> virtarrays] <−virtualarrayexpressionsets

during this run, virtualarray will prompt for a modification of the “sample_info.txt” file. this file is automatically created and deposited in the current working directory. for the supervised mode to work as expected, at least column  <dig>  which holds the covariate  <dig>  needs to be modified. if more than one covariate is needed, more columns can be added in order to include more information about the samples . in our example, only column  <dig> is needed. the running numbers are modified and group names such as “fibroblast”, “esc” or “ipsc” are assigned to each sample .

when the hierarchical clusterings of this new dataset  are compared with the non-supervised version from above , there is little obvious difference. however, a principle component analysis of the latter two datasets reveals some improvement upon supervised batch effect removal . all fibroblasts have become clearly distinct from the ipscs and escs, while adult or dermal fibroblasts become distinct from neonatal or foreskin fibroblasts in this setting, indicating an increase in resolution.

discussion
a number of bioinformatics tools can be used to merge raw data from different platforms. however, many of the available programs like arraymining.net  <cit> , crosschip.org  <cit> , webarraydb  <cit>  and conor  <cit>  can handle no more than two batches at once, and are in some cases even restricted to different chip generations of the same platform. other tools, such as anyexpress, are able to integrate several platforms at once, but have no routine to deal with batch effects, which must be removed before meaningful analysis can be derived from cross-platform studies  <cit> . aiming to perform direct cross-platform comparison of raw microarray data, we felt the need to develop a new tool that would facilitate both  the integration of a broad range of different kinds of raw microarray data and  the removal of batch effects in order to provide one consistent dataset that can be subjected directly to further meaningful analysis.

our package virtualarray can integrate raw data generated on most common microarray platforms, including affymetrics, illumina and agilent. by default, batch effects are removed using empirical bayes methods, but the package also offers a variety of other methods for batch effect removal. importantly, and unlike most of the tools named above, virtualarray is entirely based on open source common standards, as it uses r/bioc expressionset objects both as input and output formats. this ensures direct access to public databases such as ncbi geo and ebi arrayexpress independent from platform or manufacturer specific features, as well as an easy route to further analysis of the merged dataset, e.g. in r/bioc or mev  <cit> . virtualarray retains a high number of genes even after multi-platform comparison . it can be used flexibly to build a comparison based on gene, transcript or protein identifiers, and has several tools for batch effect removal already implemented. being open source, virtualarray could be easily extended to integrate next-generation sequencing data in expressionset format, and even allow cross-species comparison if required. the deseq package for example allows for the conversion of next-generation sequencing data into expressionsets using variance-stabilizing transformation  <cit> . the bioconductor homology annotation packages permit mapping between different species. a routine to use multi-core cpus on unix-like systems such as linux or mac os x is built into the package, allowing for the robust computation of large scale analyses comprising several hundred complete datasets using conventional computer hardware.

CONCLUSIONS
vitrualarray is a highly versatile tool that allows the user to combine self generated and publicly available raw datasets according to their biological coherency, but independently of the platform on which the data were recorded. the examples shown here demonstrate the importance of batch effect removal and also show that the integration of data from different platforms can yield biologically meaningful results. we have used virtualarray to compare directly the transcriptional profiles of a range of different adult and pluripotent stem cells, together with mature cell types from different tissues in one consistent principal component analysis  based on >  <dig> individual microarray datasets  <cit> . the resulting pca yielded a hierarchical picture of cellular development, ranging from the most primitive embryonic stem cells, to the most mature differentiated cells types. to the best of our knowledge, this type of analysis has not been possible to date. it is our hope that virtualarray will prove useful also in other areas of research and may complement or even substitute conventional meta-analysis studies in the future.

availability and requirements
the software package virtualarray has been written in the platform independent r programming language. it requires r version  <dig>  or newer to run. a mid to high performance computer is recommended for larger datasets . on systems running mac os x or linux/unix the software can benefit from parallel processing on several cpus via the multicore  <cit>  or biocparallel  <cit>  packages. the examples shown above were run successfully on an intel core  <dig> duo  <dig>  ghz with  <dig> gb of ram running windows xp sp <dig> . the license under which the software is distributed is the general public license version  <dig> . the software can be downloaded for free at http://www.bioconductor.org/packages/ <dig> /bioc/html/virtualarray.html <cit> . it can be installed directly in r by:

source

bioclite

competing interests
the authors declare that they have no competing interests.

authors’ contributions
ah designed and programmed the package, performed the experiments and wrote the manuscript. ra conceived the idea and wrote the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
detailed explanation to set up example data.

click here for file

 acknowledgements
the authors would like to thank michael cross for proof-reading the manuscript. this work has been funded by the german ministry of education and research  and vita <dig> ag, leipzig.
