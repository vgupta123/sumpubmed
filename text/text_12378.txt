BACKGROUND
single nucleotide polymorphisms  are a kind of genomic variations that play an important role in many genetic analysis. most eukaryotic genomes are diploid and it’s both technically difficult and time consuming to experimentally screen the sequence of alleles in contiguous snp sites along each copy of the diploid chromosomes, which is called a haplotype. the two nucleotides/alleles for one locus in a chromosome are usually obtained as an unordered pair, which is called a genotype. a haplotype phasing problem is to infer haplotypes from genotypes. although there are some other methods based on modern sequencing, such as haplotype-resolved sequencing technology and haploseq method, can obtain haplotypes directly rather than computationally infer them, haplotype phasing costs much less money than these methods
 <cit> .

the existing methods for haplotype phasing problem can be classified into two major categories: combinatorial optimization algorithms
 <cit>  and statistical methods
 <cit> . combinatorial optimization algorithms focuses on finding the solution based on reasonable biological assumptions. two models were considered: the perfect phylogeny tree model and the maximum parsimony model. maximum parsimony model assumed that the number of distinct haplotypes in natural populations was really small or the minimum among all feasible solutions
 <cit> . the principle was firstly proposed by wang and xu who also presented a branch and bound algorithm under this principle to speed up the problem resolving
 <cit> . many other algorithms try to solve the problem based on this model using either sat-based formulations or integer linear programming techniques. however, maximum parsimony model has been demonstrated to be np-complete
 <cit>  and apx-hard even in very restricted cases
 <cit> , which means precise solution can only be got with exponential time consuming. therefore many approximate approaches were also proposed based on this model. perfect phylogeny tree model changed the haplotype phasing problem to a graph realization problem
 <cit> . this model assumed that any snp mutation happened just once in the whole evolutionary history. as perfect phylogeny trees are usually not unique for a given genotype set, the minimum perfect phylogeny haplotyping  rule was proposed
 <cit> . mpph is a combination of perfect phylogeny tree model and the maximum parsimony model. it tries to reconstruct a perfect phylogeny tree that consists of minimum number of unique haplotypes. bafna et.al. proved mpph problem to be np-hard
 <cit> . although combinatorial optimization algorithms can do well in small datasets, the strong assumption and high time complexity holds back its application in larger datasets
 <cit> .

compared with combinatorial optimization algorithms, statistical methods focus on estimating the haplotype frequencies according to certain statistical theories. an earlier method was em algorithm
 <cit> . it iteratively computes each haplotype’s frequency and estimate the new solution. then the quality of the solution will be higher and higher. this algorithm worked well on a small data set, but its time cost increased sharply as it should enumerate all feasible solutions. in order to further reduce the computation time requirement, plem
 <cit>  and gerbil
 <cit>  use partition-ligation strategy for speed-up. and bpplem replaces the uniform strategy with non-uniform strategy based on linkage disequilibrium 
 <cit> . in addition, some other statistical theories have also been used to infer the haplotypes, such as bayesian and mcmc
 <cit> . although statistical methods can process larger datasets than combinatorial optimization algorithms, they usually need to consider lots of feasible haplotypes, which require large amount of storage.

with the recent innovations in high-throughput gene chip technologies, huge amount of genotype data was produced, leading to a new challenge of handling the large-scale datasets for the haplotype phasing problem. many methods based on hidden markov model  have been proposed recently, such as beagle
 <cit> , hapi-ur
 <cit> , shapeit1
 <cit>  and shapeit2
 <cit> . this kind of methods firstly yields feasible haplotypes randomly. then it iteratively builds hmm according to current haplotypes and gets new haplotypes based on this hmm. our experiments have shown that while they can get accurate haplotype results for datasets with a large number of homologous sequences, they can’t do well for datasets with small number of long genotype sequences which are very common. that’s because statistical methods need more information to refer to compared with other methods. in addition, some methods combine statistical theories and combinatorial optimization rules. although it’s impossible to get exact solution based on combinatorial optimization rules for large scale datasets, they are used in some steps of algorithms to help to improve the precision of approximate solutions. for example, 2snp algorithm finds the most relevant allele for a specific allele by building a phylogeny tree
 <cit> .

we proposed the winhap algorithm
 <cit>  by combining probability statistic and combinatorial optimization
 <cit> . winhap significantly improved the speed of haplotype phasing, while achieving similar or better overall accuracy compared with the other existing programs. but days are still needed for winhap to screening the millions of snps in the human genome. we further improve the program's running speed and memory efficiency by using the following two strategies. firstly, a divide-and-conquer strategy is utilized to solve the challenge of huge computer memory required by the existing algorithms. the basic idea is to screen the long chromosomes for haplotypes within the consecutive  <dig> -snp segments. thus, the memory need of the algorithm is only related with one segment and no longer increases as length of sequences. secondly, the openmp parallel computing mode is implemented to utilize all the computing power in a multi-core computer cluster. the haplotype phasing performance of winhap version  <dig>  is discussed in the following sections.

winhap <dig>  software package is available at http://staff.ustc.edu.cn/~xuyun/winhap/index.htm. we have also uploaded the source code, manual, materials and example datasets onto it. as of now, winhap software packages have been downloaded for at least dozens of times and used by biologists from different organizations and institutes around the world, such as max planck institute for molecular genetics, dahlem centre for genome research and medical systems biology, alacris theranostics gmbh in germany, university of medical sciences in poland, jiangxi agricultural university and university of science and technology of china. particularly, some users from huazhong agricultural university want to process very long sequences with more than  <dig> million snps on personal computers which cannot be done by winhap <dig>  and nearly all the existing tools except for winhap <dig> .

implementation
the input to winhap <dig> consists of n genotype vectors, each with m coordinates corresponding to m snps. each genotype vector can be phased to two haplotype vectors with value in { <dig>  1}, where ‘0’ represents the major allele and ‘1’ the minor allele. a haplotype vector is represented as a string of the alphabet { <dig>  1}. a genotype vector is represented as a string of alphabet { <dig>   <dig>   <dig>  ?}, where ‘0’ and ‘1’ represent the homozygous snp { <dig>  0} and { <dig>  1}, respectively, ‘2’ is a heterozygous snp { <dig>  1}, and ‘?’ is a missing snp.

divide-and-conquer strategy
winhap <dig> utilizes the divide-and-conquer strategy to phase long sequences. the winhap algorithm infers haplotypes from the neighboring snps, and the comparison with the other programs suggests that little association exists between a pair of remote snps. so a segment with fixed number of consecutive snps  is sufficient for inferring haplotypes.with divide-and-conquer strategy, winhap <dig> consists of three phases. in first step, the genotypes are partitioned into segments. in second step, all segments are phased by winhap respectively. in last step, the results of all segments are merged into a whole result. figure 
 <dig> shows the overall framework.

in the first step, we partition genotype datasets sequentially into segments, each of which has the same sizes. it should be mentioned here that the value of s must be neither too large nor too small. larger sis, more memory is cost and then partition will be less meaningful. on the other hand, if sis too small, precision of algorithm will be affected. because it’s possible that there’s no ‘2’ in one segment, which will make it difficult to merge the haplotype result of that segment with others’. based on our experiments, s should be larger than  <dig>  sites, while the upper limit of sis related with the memory of computer used. for the last segment, as we know, the size of it is less than or equal to s. if it is smaller than  <dig>  sites, we merge it to the last but one segment.

in the second step, all segments are phased by winhap respectively. winhap has three phases: in the first phase which is called simplified 2snp algorithm, the initial haplotype results are obtained. in the second phase, scalable sliding windows are used to correct some errors in first phase. in the third phase, maximum parsimony principle is used to improve the quality of results further. two points should be mentioned here. firstly, in the second phase, precision of the sites near edge is lower than others’ because they can only be covered by sliding windows from one side. number of this kind of sites becomes much larger in winhap <dig> because each segment has edges. this problem is solved in the third step of winhap <dig>  secondly, the third phase can get better results in winhap <dig>  because maximum parsimony principle is not suitable for very long sequences and segmenting makes the sequences shorter.

in the final step, the results of all segments are merged into a whole result. to ensure the precision of the snps at the edge of each segment, a merging strategy is proposed. it’s described in detail in section method.

parallelization
due to its nature of local calculation, and its large data size, an openmp parallel computing mode is adopted for the time-consuming step of haplotype phasing including step  <dig> and step  <dig>  as phasing in each segment has no relationship with other segments, parallelization of step  <dig> is comparatively easy. we just distribute one or several segment to one thread and each thread gets the result respectively. however, parallelization of step <dig> is harder. for merging is a process involving all segments, how to distribute the assignment to each thread is the key point. to eliminate the relationship between segments, we divide the process into  <dig> phases. firstly, we cut the right edge of each segment except last one and save it in file, which can be done by each thread respectively. secondly, each thread merge the right edge of the former segment and later segment and save the merged each segment. finally, one thread put all merged segments together, which is hardly time-consuming.

time complexity of winhap2
now, let’s analyze the time complexity of winhap <dig>  in the first step, winhap2partitions genotypes into segments, which takes  <dig> time since we have n genotypes each with m snps. in the second step, each segment is phased respectively by winhap. according to the analysis in the previous paper
 <cit> , winhap takes  <dig>  so if we use p computing cores, it takes  <dig>  in the last phase, our algorithm merges the results of all segments into a whole result using scalable sliding window. for each two adjacent segments, lmax- <dig> windows are needed. and the computation in one window takes  <dig> time. so the last phase takes  <dig>  where k denotes the number of segments and p denotes the number of computing cores. thus, our algorithm takes 0~  <dig> in total time.

RESULTS
datasets
as the performance of winhap processing comparatively small datasets  like ace
 <cit> , 5q31
 <cit>  and cftr
 <cit>  has been showed in the previous paper
 <cit> , only large scale datasets with more than  <dig>  snps are tested in this paper. however, public real datasets of this scale are very few. so we employ one real dataset, one simulated dataset which is generated by randomly pairing real haplotypes, and simulated datasets of different scale from ‘ms’ software.

hapmap real dataset
firstly, we compared the performances of winhap <dig> and the other haplotype phasing programs on the real dataset from international hapmap project which aims to develop a haplotype map ofthe human genome
 <cit> . this dataset consists of  <dig> pedigrees , each genotyped at  <dig>  snps in the 20th chromosome of human. we choose the genotypes of  <dig> fathers and  <dig> mothers to get  <dig> unrelated genotype sequences. as shapeit <dig> is a newest algorithm for large scale datasets, we want to compare winhap2with it. the input of shapeit <dig> includes recombination rate of each snp, and hapmap shows a table which includes recombination rate of most snp in human chromosome. however, there are still some snps of this real datasets are not in this table. so we have to choose  <dig>  snps from original dataset. so it’s a dataset comprising of  <dig> unrelated genotype sequences, each of which has  <dig>  sites.

hapmap simulated dataset
we further tested the algorithms with another dataset from hapmap international hapmap project. it’s a dataset of  <dig> real haplotype sequences, each of which has  <dig>  snps in the 20th chromosome of ceu . we generated the genotype datasets by randomly pairing two haplotypes. to let the simulated genotype dataset be similar to real ones, we only generate  <dig> sequences. so it’s a dataset comprising of  <dig> unrelated genotype sequences, each of which has  <dig>  sites.

“ms” dataset
we use well-known hudson’s software “ms”
 <cit>  to generate simulated genotype sets with n =  <dig>  n =  <dig>  n =  <dig>  n =  <dig> and m =  <dig> , m =  <dig> , m =  <dig> , m =  <dig> , m =  <dig> , <dig>  here n means the number of sequences and m means the length of sequences. the parameter“θ” is set to  <dig> . the recombination parameters “ρ” and “nsites” are set to  <dig> and  <dig> respectively.

measurement criteria of phasing accuracy
usually, the individual error rate 
 <cit>  and the switch error rate 
 <cit>  are used to evaluate the performance of phasing algorithms
 <cit> . ier is defined as the percentage of individuals whose genotypes are incorrectly resolved and ser is defined as the ratio between the numbers of switch errors and all the heterozygous loci. the value of ier usually increases along with the increase of genotype length. when the number of snps is large enough, the ier value of almost all haplotype phasing approaches is close to 100%. in our experiments, the number of snps in all datasets is larger than  <dig> , and ier is meaningless for datasets of this scale. so we just use switch error rate  to evaluate the performance of winhap <dig> in this paper.

we compared our algorithm with three existing programs including shapeit2
 <cit> , beagle
 <cit>  and 2snp
 <cit> . other algorithms are not tested for either of the following three reasons:  can’t produce the results within reasonable time;  cannot handle the missing snps;  can’t process such long sequence.

validation on hapmap real dataset
we run the 2snp, beagle, shapeit <dig> and winhap <dig> on a hapmap real dataset averaged over  <dig> independent runs. the dataset comprises of  <dig> unrelated genotype sequences, each of which has  <dig>  snps. 2snp and shapeit <dig> were both run with the default settings. the parameter “nsample” of beagle was set to  <dig> and we randomly generated the parameter “seed” in every independent running.

the performance of various phasing programs on this hapmap real dataset was shown in table 
 <dig>  from the result, we can see that although the error rate of winhap <dig> is not the best, it’s similar to others’. but the running time and memory consumption of winhap <dig> are both much lower than others’. while shapeit2gets the lowest error rate, its running time is about  <dig> times and its memory consumption is  <dig> times than winhap <dig>  in addition to that, the use of shapeit <dig> has some limit which means that it needs the recombination rate of each snp, but not all snps’ recombination rate can be got. beagle’s running time is  <dig> times and memory consumption is more than  <dig> times than our method. the precision and memory consumption of 2snp are both the worst among the programs, and its running time is nearly  <dig> times than our algorithm’s.

bold data means the best result in terms of the corresponding evaluation standard.

validation on hapmap simulated dataset
to test the performance of winhap <dig> on datasets with longer sequences, we run 2snp, beagle and winhap <dig> on hapmap simulated dataset. as shapeit <dig> need the information like recombination rate of each snp and sex of each individual, it cannot be run on simulated datasets. we constructed a dataset of  <dig> haplotypes with no missing data from  <dig> experimentally identified disease haplotypes. each haplotype has  <dig>  snps. to ensure the objection of the test, we repeat the sampling and test for  <dig> times. the parameter “nsample” of beagle was set to  <dig>  all the other parameters were set to the default values.

table 
 <dig> gives the accuracies, running times and memory consumption of the algorithms. through winhap2has higher ser than beagle, the running speed is about  <dig> times than beagle’s and memory consumption is only about one thirtieth of beagle’s. the precision, running time and memory consumption of 2snp are all the worst among the algorithms.

bold data means the best result in terms of the corresponding evaluation standard.

validation on ‘ms’ simulated datasets
to test the performance of winhap <dig> on different size of datasets, we run 2snp, beagle and winhap <dig> on ‘ms’ simulated datasets.

the performance of various phasing algorithms on this ‘ms’ simulated dataset was shown in table 
 <dig>  from the result, we can see that sers of winhap <dig> are similar with and sometimes the same with beagle’s. the running speed of our algorithm is  <dig> to  <dig> times than beagle’s. in addition, beagle cannot process  <dig> sequences with more than  <dig>  snps or  <dig> sequences with more than  <dig>  snps on our machine because of memory overflow. 2snp cannot process  <dig> sequences with more than  <dig>  snps for the similar reason. winhap <dig> can process sequences with  <dig> , <dig> snps using only  <dig>  mb memory.

bold data means the best result in terms of the corresponding evaluation standard.

figure 
 <dig> shows that the memory requirement of winhap <dig> received minor changes for test datasets with different numbers of snps. by analyzing the winhap <dig> algorithm, we may hypothesize that its memory requirement per genotype should be a constant, and actually the values / are  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> , and  <dig>  for the six datasets in tables 
 <dig> 
 <dig> and
 <dig>  respectively. most of current large-scale genotyping studies sampled fewer than tens of thousands of individuals
 <cit> , and their memory requirement using winhap <dig> is estimated to be ~330 mb. so winhap <dig> can handle a large-scale genotyping study with any number of snps in the current literature and at least in the near future.

performance of winhap <dig> using different sizes of segments
to test how the different sizes of segments affect the accuracy and computation load, we run winhap on the hapmap real dataset using different sizes of segments.figure 
 <dig> shows that the accuracy will reduce extremely when the segments are too short. at the same time, the longer the segments are, the more running memory is used. therefore, the moderate size of segments should be chosen.

performance of parallelization
we further tested the running speed of parallelized winhap <dig> on a server with  <dig> computing cores. the program was run on a linux server with  <dig> 800 mhz computing cores and  <dig> gb memory. the dataset is a ms dataset which has  <dig> genotypes with  <dig> , <dig> snps. figure 
 <dig> shows that winhap <dig> using  <dig> computing cores was more than  <dig> times faster than the single threaded winhap <dig>  considering the running speed of winhap <dig> with single processor has already been several or dozens of times than other high performance algorithms which has been above showed, the parallelized winhap <dig> with  <dig> computing cores is over  <dig> times faster than beagle, and even  <dig> to  <dig> times faster than shapeit <dig> and 2snp.

CONCLUSIONS
with the development of large-scale sequencing technologies, a large amount of genotype data is being generated. algorithms for large-scale haplotype phasing are needed. most of existing programs cannot process extremely large datasets because of either space limit or time consumption.

in this article, we introduced a computer program, winhap <dig>  which achieves significant improvements in running speed and memory requirement, with better or comparable precision, for the haplotype phasing problem. winhap <dig> can handle a large-scale genotyping study with over  <dig> , <dig> snp sites, which is beyond the capability of the other existing programs.

