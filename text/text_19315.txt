BACKGROUND
the importance of the database  integration problem to bioinformatics has been recognized for many years  <cit> . many questions in bioinformatics can be addressed only by combining data from multiple dbs, and db integration also permits cross-checking of individual dbs for validation. in  <dig> a summary of a u.s. department of energy workshop on genome informatics noted that "achieving coordination and interoperability among genome dbs and other informatics systems must be of the highest priority. ... we must begin to think of the computational infrastructure of genome research as a federated information infrastructure of interlocking pieces..."  <cit> .

one approach has involved mediator-based solutions that transmit multidatabase queries to multiple source dbs across the internet. although some progress has been made in developing mediator technology, we argue that these systems face several practical limitations , including that  few source dbs accept complex queries via the internet ,  the user lacks control over which version of the data is queried, and over the hardware that provides query processing power,  the speed of the internet limits transmission of query results, and  users cannot cleanse the source dbs that they query of potentially erroneous, incomplete or redundant data – that is, they cannot alter the source dbs in any way. the db warehouse approach overcomes all of these limitations.

this paper describes an evolving open source toolkit for constructing db warehouses that combines different collections of bioinformatics dbs within a single physical db management system  to facilitate queries that span multiple dbs. we emphasize that biowarehouse is a flexible toolkit that supports multiple alternative warehouses: our goal is to enable different investigators to create different warehouse instances that combine collections of dbs relevant to their interests. the warehouse also facilitates integration of locally produced data with other public bioinformatics dbs in pursuit of goals such as capture of experimental data, sharing experimental data with collaborators, internet publishing of data to the scientific community, and data mining and global integrative studies across multiple dbs. the data sources supported by biowarehouse are particularly well suited to integration of pathway information, although pathways are only one of many datatypes supported by biowarehouse.

sri operates a publicly accessible biowarehouse server called publichouse that contains four of the dbs supported by biowarehouse. see url  <cit>  to obtain an account.

motivations
the  <dig> online compilation of molecular biology dbs prepared by nucleic acids research lists approximately  <dig> dbs  <cit> . these dbs represent a significant investment whose full potential has not been realized due to integration barriers. this is due to the need of different scientific projects to access information from multiple different dbs to meet their objectives. as bioinformatics dbs grow in size, and as biological questions grow in scope, the point-and-click style of db mining becomes less and less practical in such an environment. instead, computational biologists seek discoveries by using programs and complex queries to mine dbs. these programs seek new patterns and generalizations by issuing queries to one or more dbs to select, combine, and compute over millions of data records. unfortunately, many barriers exist to the querying of multiple dbs, including mismatches in query language, access mechanisms, data models, and semantics.

combining information from multiple dbs is important for two principal reasons. first, information about a given biological entity is often scattered across many different dbs: the nucleotide sequence of a gene may be stored in one db, the three-dimensional structure of its product may be stored in a second db, information about the expression of the gene may be stored in a third db, and data regarding interactions of the gene product with other proteins may be stored in a fourth db. bioinformatics data tends to be organized around a given type of experiment , yet many types of scientific investigations require combining data from different experiment types. for example, one way to partially validate whether an observed protein-protein interaction is of physiological relevance is to ask whether the genes for both proteins are expressed in the same cell type; answering this question requires combining information from multiple dbs.

second, db integration is important because different dbs often contain redundant or overlapping information. db integration allows cross-validation and verification of the dbs to identify such information.

in summary, advances in biology are hindered not by lack of data, but by the diversity of technologies used for storing the data. here we propose a solution, as shown in figure  <dig>  whereby the dbs required for a project  are collected into a single high-performance environment operating from a local server so that scientists can control the set of dbs, the version of each of these dbs, and the performance of the overall system. this approach also enables scientists to combine data they have produced locally with data from public bioinformatics dbs.

comparison of the warehouse and multidatabase approaches
we define the multidatabase approach  to db integration as the approach whereby users employ a special-purpose query language to formulate a single query that spans multiple dbs  <cit> . a mediator software system then dissects the query into subqueries relevant to individual dbs, transmits the subqueries via the internet to each db, combines the results of the subqueries, and returns the result to the user. systems that employ the multidatabase approach include k <dig>  <cit>  developed at the university of pennsylvania, biokris  <cit> , opm  <cit> , tambis  <cit> , and biomediator  <cit> .

this approach has two potential advantages over the warehouse approach. first, it allows users to avoid investing in the hardware required to replicate the source dbs locally. the significance of this barrier depends, of course, on the number and size of the dbs to be integrated. second, it provides users with immediate access to the freshest version of each source db. however, warehouses can provide fresh data if they are updated frequently. yet, refreshing large dbs can involve frequent downloads that are costly in network bandwidth, unless the db offers downloads of changed entries only . very few public bioinformatics offer delta downloads, which we argue should be an area for future research, to decrease the costs of warehouse updating.

however, several limitations are associated with the mediator approach:

• a key limitation of the multidatabase approach is that it rests on a faulty assumption, namely, that all the bioinformatics dbs that users want to query are queryable via the internet by using a network query api with the expressive power of a language such as sql.

that is, the assumption is that the multidatabase approach will allow us to integrate dbs that are ready and waiting in a queryable form. however, the vast majority of bioinformatics dbs have not been made queryable in this manner by their developers, and this situation has changed little in the past decade. the discoverylink system  <cit>  does provide technology for wrapping into an sql-queryable from data sources that are available as files. we expect that writing such a wrapper would be similar in complexity to writing a biowarehouse db loader. furthermore, once wrapped in this manner, some advantages of the multidatabase approach  are lost.

• the performance of the internet limits query speed because potentially large intermediate results must be sent across relatively slow internet links.

• every source db server that is accessed by a user query must be available during execution of that query; the more sources a query accesses, the higher the probability that at least one source will be unavailable.

• the fact that users do not control the hardware on which queries submitted to a mediator are run can be problematical for users whose applications produce large numbers of queries and/or produce large query results. it is unrealistic to expect a single internet- accessible server for a given public db to be able to satisfy the computational demands of the entire biomedical research community, and mediator users have no control over the hardware supporting the dbms servers to which their queries are sent. this problem can be solved by installing a local warehouse whose hardware is configured to satisfy the query processing demands of the user's application .

• although access to up-to-date data can be an advantage of the multidatabase approach, it can also be a disadvantage. some users need control over what dataset version they are querying, and do not want that version changing out from under them. for example, a user who is running a software package that functions with version  <dig>  of a db, but that breaks under version  <dig>  of that db, wants control over when to migrate to version  <dig> . similarly, a user who is training and evaluating a machine learning program may want to perform experiments with respect to a constant version of the db for consistency in evaluating program performance.

• mediator users are unable to perform cleansing of remotely accessed dbs. in contrast, dbs loaded into a local warehouse can be cleansed of what a user considers to be erroneous or noncompliant data. if desired, this extracted data can then be stored as a distinct dataset within the warehouse.

in addition, users who generate their own local experimental or computationally derived datasets need a dbms in which to house them. the warehouse fulfills that need, whereas the multidatabase approach is unable to store locally produced data.

taking full advantage of the biowarehouse and its preceding benefits does not preclude integration of databases not yet supported by the biowarehouse. biowarehouse may be used in a complementary fashion with a mediator system in that a multidatabase query engine can sit above biowarehouse and enable access to additional dbs. this software can then send subqueries to biowarehouse when it contains the relevant data, whereas other subqueries can be sent to external internet-accessible dbs.

in summary, our intention is not to rule out use of the mediator approach, but to present what we see as a systematic discussion of the strengths and weaknesses of both approaches, many of which have not been discussed previously.

definitions
a data object is an element of a description of a biological system at a particular granularity and with respect to a particular ontology. for example, a data object might correspond to a gene, or to a control region within a gene, or to a protein-protein interaction. a database  is a collection of data objects. for example, swiss-prot is a db. a dataset is a specific version of a db. for example, swiss-prot version  <dig>  is a dataset.

a database management system  is a software system for storing and querying collections of data. a data model is the primitive data structuring mechanism used by a dbms, such as the relational data model used by relational dbmss. a schema is the set of all tables used to represent data objects. a subschema is a set of related db tables within the warehouse schema, such as all tables involved in defining the representation of a data object.

a biowarehouse instance results from integrating a specific set of datasets into the biowarehouse schema within a single dbms. an instance could contain multiple versions of the same db. a warehouse object is the representation of a data object in a warehouse. a warehouse identifier  is an integer unique identifier assigned within a biowarehouse instance to a warehouse object.

implementation
design requirements of biowarehouse reflect goals of simplicity, accessibility, efficiency, and scientific utility. this section discusses these requirements, and the corresponding design decisions that were made to satisfy them.

overall requirements
the warehouse must scale to allow effective operation with terabytes of data. each dataset is expected to be gigabytes in size, and available datasets typically increase significantly in size each year. decision: relational dbms technology is in much more prevalent use in the bioinformatics community than is object-oriented dbms technology, and has better scalability; therefore, biowarehouse uses relational technology.

the warehouse should be compatible with standard freeware dbmss and commercial-grade dbmss that are most commonly used in the bioinformatics community, to allow users who cannot afford the cost of commercial dbmss to employ freeware dbmss, and to allow users who cannot afford the limitations of freeware dbmss to employ commercial dbmss. decision: biowarehouse supports the oracle and mysql dbmss.

the biowarehouse architecture should be scalable to support the integration of a growing number of data sources. decisions: the project is open source to permit contributions of loaders from many groups. in addition, the complexity of the schema must be constrained to facilitate extensibility .

multiple access mechanisms must eventually be provided including sql, xml-based methods, open agent architecture   <cit> , corba, and web-based query interfaces. decision: initial support is provided for sql  and for oaa .

for installation, given an operating dbms the requirements for the person configuring a warehouse instance are limited to basic db administrator  expertise.

schema requirements
the biowarehouse schema should be as simple as possible. if its schema grows too large, that complexity could be a significant barrier to widespread adoption of the warehouse technology, because users will find the schema so difficult to understand that they will be unable to write queries or application programs. furthermore, because its schema must evolve as biowarehouse grows to support new datatypes, a complex schema will be a significant barrier to the further development of biowarehouse, both at sri and by collaborators at other institutions, thus limiting the scalability of biowarehouse to more data sources.

decisions: we define single common tables for information that is common to many warehouse datatypes, to decrease the schema complexity. for example, comments and citations are common to many warehouse datatypes , and each is implemented through a single table. we must, of course, define an association between a comment and a warehouse object. if different biowarehouse object types used different spaces of object identifiers, uniquely defining that association would require the schema to encode both the object id and the object type, since objects of different types could have the same id. thus, we use a simpler approach whereby all biowarehouse objects share a single space of object identifiers within a warehouse instance. the identifiers are known as warehouse identifiers , and are integers that are assigned at db load time. the use of a single space of wids allows associations between, for example, a comment and an object, to specify a wid only, and not the object type also.

the warehouse schema must support the concurrent presence, accessibility, and addressability of multiple datasets  within a biowarehouse instance.

the warehouse schema should facilitate coercion of different sources of the same type of data into a common semantic framework. for example, a warehouse might be created that contains both the swiss-prot and pir protein dbs . once loaded, the two dbs would exist side by side within the warehouse, without having been merged, but within the common warehouse schema. a separate project within the same biowarehouse instance could create yet a third dataset within the warehouse consisting of a nonredundant merging of swiss-prot and pir. this approach implies that the warehouse user need learn only the schema of the warehouse, not of each data source, whereas some mediator systems lack a global schema and require the user to know the schema of each db that they query.

loader requirements
because bioinformatics dbs are often large and may have a relatively poorly defined syntax, load failures are frequently observed and without precautions could result in crashing the loader. for this reason, db loaders should be able to recover gracefully from errors encountered during parsing of their input files. decision: the loaders are designed to keep loading even in the presence of an error. if partial data has been inserted into the warehouse, a loaderror flag maintained on the related objects is updated to indicate that an error occurred while parsing the object, and that the warehouse entry for the object may therefore be incomplete or contain errors.

biowarehouse schema design
we designed the biowarehouse schema by first studying the schemas of each db to be integrated, as well as the schema of other dbs that use the same datatype. our experience in developing the pathway tools ontology  <cit> , which spans many warehouse datatypes, was also helpful.

the development of the biowarehouse schema was guided by several principles, illustrated in this example involving swiss-prot, trembl, pir, and ecocyc. although the exact fields present in these dbs vary, all of them contain information about proteins; therefore, we consider the protein subsets of these dbs to be a single datatype.

since dbs typically conceptualize proteins in different ways, any kind of cross-db operation faces the problem of semantic heterogeneity, whereby information is partitioned in different fields that use different definitions . one possible approach to supporting protein dbs within the warehouse would be to create different schema definitions for each of the conceptualizations of proteins used by the source dbs. however, this approach would perpetuate the semantic heterogeneity among these dbs, and would complicate the resulting schema of biowarehouse. at the extreme, biowarehouse would have to contain a different protein schema for every protein db that it loads. therefore, the warehouse schema would be larger and more complex, and users would have more difficulty understanding the schema, making it more difficult for a user to query the db. under this approach, a user who wanted to query all proteins in the db would have to study the subschema for each different protein db in the warehouse, and essentially write a separate subquery for each subschema.

instead, the warehouse approach uses a single set of schema definitions to cover a given datatype, even if that datatype is conceptualized differently in different dbs. for example, we create a single set of schema definitions to span all attributes for proteins. the db loaders are responsible for translating from the conceptualization used in each db within the family to the conceptualization used by the biowarehouse schema. this approach eliminates the semantic heterogeneity of these dbs, allowing users to query all protein sequence dbs using the same schema.

another important element of our approach is to explicitly encode the dataset from which each data object within the warehouse is derived. for example, since entries from any protein dbs are loaded into the same set of tables, it is critical for user queries to be able to distinguish swiss-prot entries from trembl entries. thus, queries can request the retrieval of all warehouse protein sequences that were loaded from the swiss-prot dataset.

our db loaders do not attempt to remove redundancy during the loading of multiple dbs, so if swiss-prot and pir were loaded, and contained entries describing the same protein, two distinct entries would be created in the warehouse. this is important for four reasons:  scientifically, we believe that the warehouse should respect and maintain the integrity of individual datasets, that is, the warehouse should preserve information about the source  of warehouse entries so that users can determine exactly what proteins are part of the swiss-prot dataset or the pir dataset.  swiss-prot and pir might provide different information about the same protein, either because they disagree about the biological facts, or because they provide different commentary.  it simplifies the loaders, which need not be concerned with detecting and removing redundancy between different datasets.  it allows later execution of algorithms for computing nonredundant protein datasets, where different algorithms can be appropriate for different applications. a nonredundant protein db could be created as a separate dataset within the warehouse that resulted from applying a redundancy-reduction algorithm to the swiss-prot and pir datasets. this approach satisfies those users who will want to study swiss-prot per se, and those users who will want to work with a large nonredundant protein db.

each dataset is described by a row within the dataset table, and contains attributes such as the dataset name, version number, release date, and reference urls.

our schema design also allows multiple versions of a given dataset to be loaded simultaneously, thus supporting queries that study the relationships among different versions of a dataset . having access to multiple versions of a db simultaneously is also important when a newer db version has changed in a way that interferes with important application software, or when one user is in the middle of a computational study that he wants to complete on the older db version to maintain a consistent set of results, and another user of the same biowarehouse instance wants to access the newest version of swiss-prot. our approach gives warehouse administrators the option of deleting the old version of a db and loading its new version, or loading the new version alongside the older one, allowing both to exist simultaneously.

this ability to maintain distinct datasets also enables users to capture locally produced data  in the warehouse by defining a separate warehouse dataset to hold these data.

consider the fact that proteins, pathways, and other bioinformatics datatypes all need to cite literature references. we do not want the protein and pathway datatypes within bioware-house to refer to different schema definitions of literature references, as would be done in warehousing approaches that create separate data-source-specific subschemas for every data source they include. we also do not want them to contain duplicate copies of the same definitions of literature references, which can happen with normalization schemes that blindly create new tables for every multivalued attribute of an entity. therefore, we encode citations from all biowarehouse datatypes and datasets within a single citation subschema within biowarehouse. db links are treated in a similar fashion – the subschema for each warehouse datatype encodes links to other db objects in the same way.

note that the current version of the warehouse schema is more oriented toward prokaryotes than eukaryotes; over time better support for eukaryotes is being added. this orientation applies to representation of genes, for example, the schema does not currently represent introns or alternative splicing.

biowarehouse schema implementation
biowarehouse supports several bioinformatics datatypes, each of which is implemented as one or more tables in the schema. the full schema is provided as supplementary material to this article as an er diagram  to provide an overview of its organization, and as an sql definition for readers interested in its details . figure  <dig> shows the main datatypes in the biowarehouse schema, and the relationships between them. from left to right within this figure:

• taxon: describes taxonomic groups such as genus or species. taxa are related to one another to represent the various hierarchical levels of taxonomic classification.

• biosource: represents the source of a biological material, whether nucleic acid molecule, protein, or gene. in addition to specifying the taxon of the material by cross-referencing to entries in the taxon table, biosource also stores source descriptors such as cell type, tissue, and sex of the organism. the loaders provided with the biowarehouse all reference the ncbi taxonomic db  <cit>  when creating entries in biosource to provide a normalized set of taxonomic classifications.

• nucleic acid: defines a dna or rna molecule, or a fragment thereof. a single contiguously sequenced fragment of a larger nucleic acid is represented by an entry in table subsequence.

• gene: biowarehouse uses the prokaryotic notion of gene – a region of dna that codes for a protein or rna product, beginning with the transcriptional start site. genes are related to entries in the nucleicacid and subsequence tables that define their sequence, and to their protein products. a gene may also be directly related to a biosource.

• protein: defines proteins, including their amino acid sequences. proteins are related to the genes that encode them, to the reactions they catalyze, and to features defined on their sequence. a protein may also be directly related to a biosource. the schema can capture the subunit composition of a multimer, and does not require that a protein sequence be present.

• feature: a subsequence of interest on an amino acid or nucleic acid sequence, such as a catalytic site, a phosphorylation site, a promoter region, or a transcription-factor binding site.

• reaction: a spontaneous or catalyzed chemical reaction. a reaction is related to the protein that catalyzes it; to pathways that contain it; and to its chemical substrates, which can be small molecules or macromolecules such as proteins.

• chemical: a small-molecular-weight chemical compound.

• pathway: an ordered collection of biochemical reactions.

the bottom of figure  <dig> lists some additional tables within the schema. every entity within the warehouse  has a row in the entry table that defines metadata such as the time it was inserted in the warehouse, and its time of last update. every warehouse entity is also associated with the dataset  from which it was derived; datasets themselves are defined in the dataset table.

the term table implements storage of external controlled vocabularies such as those provided by an ontology. each entry in the term table is a single term in a controlled vocabulary. the enumeration table implements controlled vocabularies internal to biowarehouse itself. for example, the biowarehouse table biosource contains a column "sex" that is allowed to take on several possible controlled values including "male" and "female." the enumeration table specifies the allowed controlled values for specified biowarehouse tables and columns, making biowarehouse self-documenting.

in addition to the preceding bioinformatics datatypes, the biowarehouse represents many relationships among these datatypes. if the relationship is one-to-one, a column in the corresponding table simply contains the wid of the associated object. for example, a gene is associated with its nucleic acid in this manner. relationships of higher multiplicity, such as many-to-many relationships, are implemented as linking tables that associate two or more primitives, such as a gene and a protein. a linking table contains wids for each of the related objects. this permits efficient, normalized representation of these relationships.

the warehouse also contains tables that implement auxiliary information, including descriptions of the source dataset of each warehouse entry, literature citations, human- and software-generated comments, db cross-references , synonyms for named objects, and controlled vocabularies.

the biowarehouse schema will evolve as new loaders are added to support new datatypes. sri encourages other groups to submit new loader implementations, java library extensions, and schema extensions to this open-source project. we do feel it is necessary for sri to be the final arbiter of such extensions to ensure that the schema implementation remains compatible with its design principles to ensure continued maintainability and scalability of biowarehouse. new versions of the schema will sometimes be incompatible with old versions, which will require loader modifications, and data reloading. since public data sources must in any event be reloaded at regular intervals, the requirement of data reloading is not a large burden.

to support multiple dbmss and their different flavors of sql, each dbms-specific schema is generated from a common schema template. the schema template consists of a framework using sql syntax that is common among all supported dbmss, interspersed with variables. macro substitution converts the common schema to one that is conformant to the dbms. the most common substitution is for primitive data types, which differ significantly across dbmss.

db loaders
it is the responsibility of each loader to translate the flat file representation of its source db into the warehouse schema. typically, many of the source db attributes are copied into the warehouse either verbatim or with minor transformations . the few source attributes not represented in the warehouse are generally ignored, although some attributes are added to the warehouse commenttable. an example of warehouse translation semantics is shown in table  <dig> for one file from the biocyc collection of dbs.

loaders have been implemented in both the c and java languages. c-based mysql loaders interface with mysql using the c api provided as part of mysql. c-based oracle loaders interface with oracle using the oracle pro-c precompiler. java-based loaders use the java database connectivity  api to interface with the dbms. each of these apis allows sql to be embedded and/or generated within its source language.

loaders have been implemented for these bioinformatics dbs: uniprot , enzyme  <cit> , kyoto encyclopedia of genes and genomes   <cit> , the biocyc collection of pathway/genome dbs , the ncbi taxonomy db  <cit> , genbank  <cit> , the comprehensive microbial resource   <cit> , and gene ontology  <cit> .

the architecture required to load datasets into a warehouse instance is akin to the process of compilation, but with the source code being a dataset and the object code being sql insert statements to add the contents of the dataset to the warehouse. standard parser generation tools  are used throughout to specify the syntax of the input files, and associated loader actions.

a set of support routines is provided to create and manipulate an internal representation of the warehouse objects, including assignment of wids to objects, and the resolution of intra-dataset cross-references into wids. loaders may make sql queries to the local warehouse. sql generation is performed by translating the internal representation into sql statements.

each loader has an associated manual describing its operations and any limitations in the loader. for example, the genbank loader is currently recommended for use on prokaryotic sequences only.

biowarehouse java utility classes
the biowarehouse implementation provides a set of java classes with general utilities for interacting with biowarehouse. these classes are useful for developers who want to construct new biowarehouse loaders or applications. the classes provide methods for connecting to the database and for loading data into a biowarehouse instance. these classes are packaged as a single module  that can be used by any java-based loader or application. to abstract underlying db specifics from the developer, a client application uses a factory class to obtain an instance of a java db class . the db class provides methods for connecting to the database, performing queries, or doing biowarehouse-specific tasks, such as obtaining a wid in an object-oriented, dbms-independent way.

similarly, we provide java classes enabling a client to interact with the biowarehouse tables in an object-oriented manner. the table classes are defined by an extensible class hierarchy that defines one class per table. the hierarchy greatly simplifies implementation of new table classes. similar table types  have their common functionality factored out into a base class. each class provides accessors with methods for each property  of the table. these classes enable developers to create instances of the table classes , set properties of the table object , and store the data into the database. methods are also provided to retrieve a row from a table given its primary key , and to update or delete that row. table  <dig> compares the processes for the sql versus the java method. in these examples, the protein.datasetwid is  <dig> and the protein. wid is  <dig> 

in addition to providing a convenient object-oriented interface to the biowarehouse schema, the java schema classes leverage the metadata capabilities of jdbc to perform data checking before inserting data. for example, we can detect if a text length exceeds the allowed maximum for a column before attempting an insert . another benefit is that we can perform unit tests on the classes to ensure compatibility with the current implementation of the biowarehouse schema, and to confirm that all functions work as expected with each type of dbms.

publichouse: publicly queryable biowarehouse server
as a convenience to users who do not want to maintain their own local biowarehouse instance, sri provides a publicly queryable biowarehouse instance called publichouse, which stores the open biocyc, ncbi taxonomy, enzyme, and cmr dbs. users can query publichouse via internet sql queries. because publichouse stores only those dbs that their creators make openly available, users wanting to query other biowarehouse-supported dbs must load those dbs into their own local biowarehouse instance. that is, sri cannot typically redistribute dbs that are not openly available, but most users will be able to download and install those dbs for their own local use.

currently, publichouse contains biocyc dbs for ten organisms. however, in the near future we expect that number to increase to more than  <dig> organisms due to a joint effort between sri and the computational genomics group at the european bioinformatics institute to generate pathway/genome dbs for every completely sequenced bacterial and eukaryotic genome.

user support and documentation
extensive documentation is available for biowarehouse within the software distribution. the available documentation is listed in a table of contents within the distribution at . the biowarehouse documentation set includes release notes, a quick start guide, environment setup documentation, schema description and dbms setup instructions, a description of the integration with the dashboard for the february  <dig> bio-spice demonstration, and descriptions of perl utilities and perl demo scripts.

the table of contents also has a table listing statistics about each loader  for each loader, there are two pieces of documentation: how to build and run the loader, and a manual for developers describing the details of the loader implementation and mappings from the source db schema to the biowarehouse schema.

bug reports and requests for assistance should be sent to support@biowarehouse.org.

RESULTS
here we present results obtained by biowarehouse in its use by several bioinformatics projects, and a performance analysis of biowarehouse.

an sri project is developing algorithms for predicting which genes within a sequenced genome code for missing enzymes within metabolic pathways predicted for that genome  <cit> . biowarehouse fills several roles within that project: it is used to construct a complete and nonredundant dataset of sequenced enzymes by combining protein sequences from the uniprot and pir dbs, and by removing from the resulting dataset those sequences that share a specified level of sequence similarity. our current research involves extending the pathway hole filling algorithm with information from genome-context methods such as phylogenetic signatures, which is obtained from biowarehouse thanks to the large all-against-all blast results stored within cmr.

another sri project is comparing the data content of the ecocyc and kegg dbs using biowarehouse to access the kegg data in a computable form. jeremy zucker of harvard is using biowarehouse as a component of an automated pipeline that will construct metabolic flux models from annotated genomes  <cit>  as part of the bio-spice project  <cit> . zucker is also using biowarehouse to develop a translator from kegg to the biopax pathway exchange standard .

enzyme sequence completeness
an sri project is using biowarehouse to determine the completeness with which biochemically characterized enzymes have been sequenced. specifically, we answered the question: in the swiss-prot, trembl, pir, cmr, or biocyc dbs, what fraction of enzymes that have been assigned ec numbers are associated with at least one protein sequence  <cit> ? illustrated here is a type of problem that can be solved with biowarehouse. this problem is not a systematic or formal evaluation of biowarehouse.

this question is significant since the identification of an enzyme in a newly sequenced genome cannot be made if no sequence is known for an enzyme with that activity. unrecognizable enzymes therefore limit the completeness of genome annotations and of metabolic pathway predictions. furthermore, we cannot genetically engineer an unsequenced enzyme into a new organism to accomplish a metabolic engineering goal, because we do not know which gene to insert to provide the needed enzyme activity.

ec numbers  constitute a classification system for enzyme function developed over the course of many years by the nomenclature committee of the international union of biochemistry and molecular biology   <cit>  . the classification system is four levels deep, and each enzyme function at a leaf of the classification tree is assigned a unique tuple of four numbers. the enzyme functions are classified according to the chemical transformation of the enzymatic reaction. for example, the enzyme "tryptophan synthase" is assigned the ec number  <dig> . <dig> , with class "4" indicating that this enzyme belongs to the class of lyase enzymes.

the enzyme db is an electronic version of the ec system. version  <dig>  of enzyme  contains  <dig> distinct ec numbers, of which  <dig> have been deleted or transferred to new numbers; it therefore lists  <dig>  different biochemically characterized enzyme activities. warehouse queries allowed us to determine the distinct ec number content of each db as shown in table  <dig>  we provide execution times to give the reader a sense of how fast these queries execute on biowarehouse.

in total, these dbs reference  <dig> distinct ec numbers, or 64% of all known ec numbers. therefore, for  <dig> ec numbers , no sequence is known. we refer to such ec numbers as "orphan activities."

two qualifications to the preceding analysis should be stated. first, the ec system is incomplete in that it does not yet include a number of enzymes whose biochemical activities have been characterized. the metacyc db alone describes  <dig> enzyme activities that have no associated ec number. the true number of biochemically characterized enzymes is probably between  <dig> and  <dig> 

second, there could be entries in uniprot that omit ec number annotations, which, if properly annotated, would provide sequences for some of these enzymes. we have performed manual literature searches and db searches in swiss-prot and trembl for  <dig> orphan activities, and have found sequences for 18% of them. additional information about this survey can be found at  <cit> .

we conclude from this analysis that an enzyme genomics project should be initiated with the goal of obtaining at least one sequence for each unsequenced enzyme activity. such a project would boost the accuracy of genome annotation at the level of both proteins and metabolic pathways, and would remove barriers to metabolic engineering.

one sql query that implements the analysis in table  <dig> is shown below. it queries a single column of the reaction table for a set of ec number values. care is taken to filter out partial ec numbers and multiple ec numbers by using query expressions containing wildcards . partial ec numbers contain a hyphen in place of one or more of its numbers . occasionally, multiple ec numbers will be associated with a reaction. these are separated by forward slashes.

select distinct ecnumber from reaction

where datasetwid = 

and ecnumber not like '%-%'

performance analysis
we present the run times of the following queries to illustrate the performance of bioware-house using the hardware described in table  <dig>  all queries were run against an oracle biowarehouse instance running on a dual-processor  <dig>  ghz pentium  <dig> processor machine with  <dig> gb of memory. the instance contained all seven dbs loaded, which occupies approximately  <dig> gb. additional queries are provided in supplementary material . our intention is not to provide a thorough performance analysis, but simply to show that even with a number of large datasets loaded, simple queries are evaluated quickly.

query 1: "select * from protein where name = 'zyxin' and datasetwid = "

query  <dig> retrieves three proteins  from swiss-prot by name in  <dig> milliseconds. the protein table contains  <dig> rows.

query 2: "select aasequence from protein where wid =  and datasetwid = "

query  <dig> retrieves the sequence of one protein from swiss-prot given its swiss-prot id in  <dig> milliseconds. the protein table contains  <dig> rows. the dbid table contains  <dig> rows.

related work
biowarehouse is distinguished from other bioinformatics db warehouse efforts in the following respects. biowarehouse supports a unique collection of bioinformatics data sources that are not supported by any other warehousing system, with a unique focus on integrating metabolic pathway and enzyme databases, and on integrating multiple sources of information on completely sequenced microbial genomes. biowarehouse offers a unique approach to scalability that will allow it to scale to a large number of data sources; that approach combines a methodology for limiting the growth of schema size and complexity, which are critical attributes of warehousing systems; with an open-source development model that will allow other groups to contribute new loaders to biowarehouse; and a java library that simplifies the process of writing new loaders. biowarehouse runs on two industry standard dbmss – oracle and mysql – whereas some other warehouses run on non-standard dbmss, or on only one dbms.

ritter created a warehouse of several bioinformatics dbs called igd  <cit>  using acedb  <cit>  as the underlying dbms. acedb did not have adequate scalability for the handful of dbs that ritter integrated. the acedb dbms has other limitations as well, such as its lack of a well-crafted and standardized query language.

srs  <cit>  uses a variant of the warehouse approach that is highly text oriented rather than oriented toward structured data values, meaning that the ability of srs to compute with its data is constrained. furthermore, srs has no integrated schema, meaning that it does not attempt to unify the disparate semantics of the dbs that it integrates. srs is also a read-only system – it does not have standard dbms update operations based on atomic transactions. therefore, users cannot insert and update data from their own laboratories into an srs db by using transactions. finally, srs is not an industry-standard dbms, and does not support a full-featured complex-query language. the proprietary nature of srs makes its workings very difficult to discern.

the gus  <cit>  project at the university of pennsylvania has somewhat different goals than biowarehouse. gus is not designed for integration of public bioinformatics dbs, but is oriented toward implementation of bioinformatics applications that require integration of custom local data collections. therefore, gus does not provide loader tools for public bioinformatics databases. gus emphasizes issues of data provenance and of detecting changes in the underlying data sources. gus is implemented for the oracle and postgresql dbmss. gus has an extremely large schema  that is likely to limit its understandability by other groups and therefore their ability to query and extend gus. gus provides a publicly queryable dbms server.

the atlas warehouse system  <cit>  is remarkably similar to biowarehouse in its design. like biowarehouse, atlas uses a set of loaders to transform data from source-db files into a relational dbms schema that models several bioinformatics datatypes. atlas focuses integrating data on biological sequences, species taxonomies, molecular interactions, gene annotations, and ontologies. atlas and biowarehouse support four dbs in common: genbank, uniprot, gene ontology, and ncbi taxonomy. atlas runs on mysql only, and has a medium-sized schema of approximately  <dig> tables, which according to the atlas schema diagram in  <cit>  are not shared among different biological datatypes.

ensmart  <cit>  is another warehouse-based system that is distinguished by its user-friendly query front end that allows users to compose complex queries interactively. ensmart runs on oracle and mysql. datatypes currently supported by ensmart are genes, snp data, and controlled vocabularies. the size and full design principles of the schema underlying ensmart  are unclear. because the number of datatypes supported is small, so is the schema, and it is unclear how well the schema will scale as more datatypes are added. one schema design principle stated is the use of the star-schema approach, which is known to produce schemas that have large numbers of tables for each datatype, and thus could prove troublesome for scalability.

the biozon system  is also warehouse-based, and also provides a user-friendly web interface for constructing complex queries.

CONCLUSIONS
we have presented the design and implementation of the open source biowarehouse toolkit for constructing bioinformatics db warehouses. biowarehouse consists of a global relational db schema for important bioinformatics datatypes, and a set of loader tools that parse public bioinformatics dbs and load their contents into that schema. biowarehouse has been implemented for both the oracle and mysql relational dbmss. the toolkit can be downloaded and installed by users who want to configure their own warehouses on local hardware. in addition, sri operates a biowarehouse instance called publichouse that contains the bio-cyc, ncbi taxonomy, enzyme, and cmr dbs, and can be queried by the public. the utility of biowarehouse has been proven as a result of its use in several research projects at sri and elsewhere.

availability and requirements
• project name: biowarehouse

• project home page: 

• operating system: linux

• programming languages: c and java

• license: mozilla

• any restrictions to use by non-academics: none

authors' contributions
tl, yp, vw, pg, dsc, and jt were involved in implementing one or more loaders. all authors contributed to the system design of biowarehouse, and to the design of its schema. pk supervised the project. pk drafted the manuscript; other authors wrote sections of the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
biowarehouse schema er diagram; an entity-relationship diagram for the biowarehouse schema.

click here for file

 additional file 2
biowarehouse schema definition; an sql definition of the biowarehouse schema.

click here for file

 additional file 3
additional queries and timings; additional example queries to bio warehouse and the time they required to execute.

click here for file

 acknowledgements
this work was supported by defense advanced research projects agency contract f30602-01-c- <dig> and by the department of energy under grant de-fg03-01er <dig>  we extend our thanks to eric sparling, rich giuli, john pedersen, and mark johnson for their assistance in the design and development of the biowarehouse. we thank michelle green for the ec number analysis of pir.
