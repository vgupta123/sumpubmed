BACKGROUND
high-performance computing  for the life sciences is undergoing a fundamental reshaping  <cit> . the reliance on processor-intensive resources through which ever-enlarging genomics workflows are funneled is giving way to distributed data-intensive infrastructures like tcga and icgc  <cit> . accordingly, the immovable volumes that are flooding data centers demand “beyond the data deluge” solutions  <cit>  that invert the traditional transfer model so that computations travel to the data and not vice versa. the emphasis, then, is to maximize the availability of the data and the portability of the application. the increasing use of cloud computing infrastructure for biomedical applications reflects the realignment of hpc, as exemplified by the recent partnership between the national institute for health  and amazon on the  <dig> genomes project  <cit> .

at the same time, hpc projects such as seti@home  <cit> , folding@home  <cit> , and boinc  <cit>  have constructed distributed platforms that aggregate commodity hardware and volunteer compute cycles in order to power computationally intensive scientific workflows. in fact, the folding@home project currently utilizes the central and/or graphics processing units from more than  <dig>  personal computers and video game consoles  <cit> . to orchestrate concentrated efforts across such large numbers of physical machines and hardware platforms, researchers provide client applications that they must persuade volunteers to download and install permanently on their machines. these applications range in invasiveness from programs that run only when a machine is idle, such as the pioneering seti@home, to always-on background services like condor  <cit>  that may tangibly impact a machine’s performance.

the world wide web provides a different avenue for hpc, and this is what we explore with qm – a novel direction. the temptation to optimize qm for a particular problem domain was overcome by the greater challenge of creating a system not only to distribute computation across the web, but also to be “of the web” itself. a careful study of the web as a platform reveals that the necessary components are indeed ready for assembly.

the javascript  language is not only a “real language”  <cit>  but also a “lisp in c’s clothing”  <cit>  with support for functional and object-oriented programming styles. unlike lisp, however, js is widely used outside of academia and has ranked among the top twelve most popular languages for more than thirteen years  <cit> . scientific libraries in js are relatively scarce, although a number of specialized libraries such as ebi’s biojs  <cit> , nih/nhgri jbrowse  <cit> , and the recent genome maps  <cit>  have emerged to capitalize on the widespread availability of those computational resources, particularly in the genome browsing application domain.

web browsers execute js in sandboxed environments that rigorously control access to machine resources, and now those sandboxes implement standardized apis that provide native capabilities like hardware-accelerated 3d graphics. all modern browsers and even a few browser plugins include just-in-time  compilers to boost performance  <cit> . regular expressions in js, for example, perform at levels that are no longer matched by perl  <cit> , the language most often associated with string processing in bioinformatics applications. moreover, these high-performance js environments come pre-installed on every personal computer sold today, as well as on smartphones, tablets, gaming consoles, and even televisions. thus, web browsers represent a modern route for high-performance computing that is well-suited for the “crowdsourcing” model  <cit> . indeed, the current fast proliferation of bioinformatics libraries in js also reflect the advent of web-based “social coding” environments which present entirely novel opportunities for large-scale collaboration  <cit> . furthermore, the networking capabilities of the browser platform allow it to import code and data dynamically and thereby orchestrate distributed workflows across multiple browsers on distinct machines, a feature at the core of social computing  <cit> . therefore, what is described in this report could be construed as social computing for machines  <cit> , extending the reach of loose distribution models such as mgrid  <cit> .

the emergence of big data in the biomedical sciences has been associated with the proliferation of reference databases such as those reviewed yearly by nucleic acids research <cit> . the aggregation of the web of linked data resources independent of the institutions that host them has been approached by comprehensive data models such as the distributed annotation system  <cit> , which we have also explored as a backbone for workflow assembly  <cit> . it is now amply clear, however, that the linking of data resources, regardless of the domain, is itself domain-neutral and best described by dyadic predicates of w3c’s resource description framework  that underlies the third generation of web technologies  <cit> .

the rdf approach has expanded the basic reliance of unique resource identifiers  both to identify and locate data  which require only a web browser to be put to good use by any researcher, regardless of his expertise or domain of interest. the current extent of its use is dramatically illustrated by the adoption of the rdf framework across all data services of the european bioinformatics institute  <cit> . as also illustrated by some of our work  <cit>  developing sparql endpoints for tcga, the volume of the server-side hosted data is not a significant obstacle to developing web applications  that consume those data. on the other hand, mechanisms to assemble workflows for data analysis have not yet matured as user-friendly commodities, despite the availability of excellent frameworks like taverna  <cit>  and share  <cit> . one possibility is that the underlying web services themselves need to be amenable to assembly at a moment’s notice – even for deprecated or outdated versions of a procedure. this is an absolute requirement of the modern focus on reproducibility of workflow results  <cit> . we have explored the use of modular browser-based web apps to deliver this functionality in standard bioinformatics applications such as image analysis  <cit>  and sequence analysis  <cit> . the success in those two efforts strengthens the claim that script tag loading, the same mechanism web browsers use to load web apps, can orchestrate and distribute the execution of bioinformatics workflows across multiple physical machines. the illustrative, and validating, example detailed in the results section below will extend the same example of sequence analysis approached in the second of those two reports by analyzing twenty different genomes of streptococcus pneumoniae in parallel.

methods
qm provides a distributed computing platform as a web service . its architecture  combines the general pattern of web  <dig>  technologies with the model used by modern social networking sites by decoupling the presentation/analytical layer from the persistent representation layer so that the former runs on the client side as a web app that consumes an application programming interface  presented by the latter on the server side. qm also decouples the presentation and analytical layers of the web app so that third parties may embed the qm web service as part of their own web apps.

to provide this functionality, qm contains three main components: an api server, a web server, and a website. the api and web servers are written completely in js, and the website is written in html <dig>  css, and js. nothing about qm’s design or interface binds it to a particular development stack, but our desire to construct the project as a true web computing “device” motivated us to implement as much of the code in js as possible. the strategy paid unexpected dividends, as well; the server-side components are free from assumptions about the hardware and operating systems on which they run, which vastly simplifies deployment to the cloud via platform-as-a-service   <cit> .

this report also presents code examples  which can be run from any website that embeds the qm web service. the examples are all written in js, but some of them also make use of coffeescript, “a little language that compiles into javascript”  <cit> . many common scientific languages can be translated to and from js, and a comprehensive list of projects for this purpose is available at http://bit.ly/altjsorg..

api server
the api server is a program which responds to requests sent by clients over the standard hypertext transfer protocol . the program then interprets the requests according to their methods, target urls, and embedded data. qm’s api presents three operations, as shown in table  <dig>  data sent as part of a post should be formatted in javascript object notation  format; response data from qm are json-formatted, too. note that clients need not be browsers – any software package that can communicate over http and manipulate json-formatted data can use qm directly.

this table specifies the application programming interface  understood by qm’s api server. request and response data use javascript object notation  format, but data may be omitted where values are left blank. the “{ }” denotes a json object, and the “” denotes a json array.

the api server is implemented as a simple node.js  <cit>  program that loads and executes all of its application logic from qm’s own publicly available module, “qm”, using the node package manager   <cit> . the module supports five different databases as targets for persistent storage: apache couchdb  <cit> , mongodb  <cit> , postgresql  <cit> , redis  <cit> , and sqlite  <cit> . these five open-source databases were chosen for support based on their high performance and popularity, and their differences in design help to guide the development of qm as an hpc solution for a heterogeneous database landscape. the relative merits of the alternative implementations to the default use of mongodb are as follows. couchdb and mongodb are both document-centric nosql databases with mapreduce apis that understand js, but their designs are very different. couchdb is more than just a database – it is nearly sufficient to implement qm by itself because it bundles a web server and an http-accessible api. mongodb, by way of contrast, has an api that mimics the traditional relational style used by postgresql and sqlite, with a much stronger focus on clustering and “sharding”  across nodes. postgresql represents relational database management systems , the workhorses that traditionally power enterprise applications and data warehouses, while sqlite represents embedded  databases. redis is an in-memory key-value store that is often referred to as a “data structure server” because its keys can contain strings, hashes, lists, sets, and sorted sets. the ability to map qm’s persistent representation layer across such a wide variety of storage systems simplifies deployment and maintenance significantly. the service that backs this report’s illustrative examples, available at https://v <dig> qmachine.org, uses mongodb.

qm’s api server supports cross-origin resource sharing   <cit>  so that any webpage can embed qm to distribute workflows across web browsers without violating the same-origin policy  <cit> . there is wide support for cors in web browsers  <cit> .

web server
the web server, like the api server, is implemented as a node.js program, and its logic is contained in the same npm module, “qm”. that is, the installation of all of qmachine’s base libraries can be achieved simply by running node’s built-in module management system: npm install qm. it is worth recalling the minimal role played by the server-side components of qm . the web server exists only to provide the presentation/analytical layer’s resources to client machines. because these resources are static, the web server can be replaced by off-the-shelf web servers like apache  <cit>  and nginx  <cit> .

website
the website functions as the presentation/analytical layer of qm. it was developed as a browser client for the qm api, and thus it is implemented in html <dig>  css, and js, as can be verified by inspecting its source code at https://github.com/wilkinson/qmachine. the website consists of a single webpage that communicates with the api server periodically via xmlhttprequest  using a technique known as asynchronous javascript and xml . the “ajax” name is a bit misleading, however, because xhr is not limited to handling xml data; all of the browser client’s communications with the api server use javascript object notation .

when a browser loads the webpage, it initially loads only the presentation layer, comprised of the html, css, and js resources necessary to render the graphical user interface . immediately after the gui loads, the browser retrieves qm’s analytical layer, which is written entirely in js. this design improves the user experience by loading the gui faster, and it isolates the presentation layer’s code from the analytical layer’s code. thus, third parties can embed qm’s analytical layer and thereby use qm’s persistent representation layer without loading qm’s presentation layer, as shown by the examples at https://v <dig> qmachine.org/barebones.html and http://q.cgr.googlecode.com/hg/index.html.

qm’s browser client models a workflow as a set of transforms that should be applied to input data in a specific order to produce output data. a “task description” is an object that contains the transform f, the data x, and any information needed to prepare the environment prior to execution.

as described above, the client-side application that is distributed when a browser is pointed to https://v <dig> qmachine.org was developed using only web technologies: html <dig>  js, and css. in order to stay within the core js syntax that is supported by all browsers and all platforms – including mobile devices – code development was assisted by jslint  <cit> . jslint is also used directly within the analytical layer as a static analysis tool to identify tasks which can be serialized faithfully into json for distribution to volunteer machines. a generic library, quanah  <cit> , was also developed to solve the numerous concurrency challenges faced in asynchronous data transfer by qm; it is therefore a key component of the prototype described here and is accordingly also made publicly available with open source. the presentation layer uses jquery  <cit>  and twitter bootstrap  <cit>  to ensure consistent look-and-feel across a variety of mobile and desktop browsers. the gui additionally attempts to support outdated browsers through optional integration with google chrome frame  <cit> , html <dig> shiv  <cit> , and json <dig> js  <cit> , but it does so only as a courtesy.

demonstration program
the demonstration program is written in pure javascript so that it can be run in an ordinary web browser without dependencies on any native applications, plugins, or add-ons. it extends a demonstration from a previous study  <cit>  in which a mapreduce decomposition of a sequence analysis procedure was used to find the longest similar segment between a user-given sequence and a full bacterial genome. the demonstration in this study will not only reproduce the previous results using remote execution on another machine, but it will do so in parallel for all of the twenty strains of streptococcus pneumoniae that are currently available from the national center for biotechnology information . it uses an updated version of the same universal sequence maps   <cit>  library used by the previous study, as referenced directly by exact version from its online repository.

RESULTS
the architecture of a qmachine detailed in figure  <dig> follows the general pattern of web  <dig>  technologies by using the server side exclusively for persistent representation and leaving the rest of the program logic to run on the client side. qm uses a key-value architecture to orchestrate volunteering client machines in a manner that maximizes the distribution of the computational resources required for data transfer and subsequent data processing. this orchestration is highlighted in figure 1: qm distributes not only the compute cycles needed to execute the n different procedures , but also the bandwidth needed the retrieve the corresponding input data  being processed from their respective urls . this design is motivated by the constraints of biological applications such as next generation sequencing in which the limiting factor is more often the available memory than the processor speed.

the operation of qm relies on the creation of unique identifiers to define “boxes” that are then shared with the volunteering browsers in a manner resembling traditional api keys. this operation will be described in a series of four examples that increase in complexity, beginning with  the remote execution of a simple algebraic operation, followed by  distribution of the same operation as a parallel  transformation of the elements of an array and  distribution again as part of a mapreduce procedure; finally, the  parallel execution of a real-world genomic sequence analysis in which both the code and the data needed to perform the analysis are invoked by a single submitter but then entirely resolved and executed asynchronously by multiple volunteer browsers. the final, real-world example distributes both the processing and networking loads, as described in figure  <dig>  it illustrates the ability of volunteer nodes to call code and data from multiple sources which are independently developed and maintained. this illustrative series is also available as a youtube webcast at http://goo.gl/tnpmiq.

loading the client-side library
qm’s analytical layer is provided by a js library that can be loaded by any web browser automatically as part of any webpage that contains the following code:   

 once loaded, the js environment will contain a global object named qm with convenient high-level methods that can be used to reproduce the results of the four examples that follow.


 simple algebraic operation

for the first illustrative example, let f be a function that increments a given number x by  <dig>  and let x =  <dig>  to compute the result, f, on a volunteer machine, we could use the qm.submit method:   

 as in the rest of the illustrative series, this example is described and demonstrated in the accompanying screencast . note also that this simple operation is easily expressed in other languages such as coffeescript  <cit> ):   

 as discussed in “methods”, qm’s architecture does not impose the use of a specific programming language, as long as a compiler to js, the web’s “assembler language”  <cit> , is distributed with the remote call. to support this claim, the qm client library delegates to a compiler – written in js – for the coffeescript language. for a list of compilers that can translate programs written in other languages into js so that they can be interpreted by volunteering browsers, see http://bit.ly/altjsorg.


 simple distributed map

because each qm.submit operation is an asynchronous call, multiple calls can run simultaneously. thus, it is straightforward to distribute the execution of a “map” function, a higher-order functional pattern that applies the same operation to each element of an array. this pattern is so ubiquitous in scientific computing that it warrants a dedicated method, qm.map, that can be used as follows:   


 simple distributed mapreduce

just as in the “map” function shown above, it is straightforward to distribute the execution of a “reduce” function, a higher-order functional pattern which combines elements of an array two at a time until only one value remains. as recently surveyed by zou et al.  <cit> , the mapreduce programming template is at the very core of modern computationally intensive bioinformatics applications. this third illustration demonstrates the mapreduce pattern as an extension of the second example by subsequently summing the results of the distributed “map” using a “reduce” also distributes across qm’s volunteers:   


 real-world genomic analysis

the fourth illustrative example assesses qm’s ability to scale the asynchronous operations demonstrated above for use in a real-world bioinformatics workflow. the example is a fractal mapreduce decomposition of sequence alignment  <cit>  which distributes both the processing and networking loads across qm’s volunteers, as described in figure  <dig>  it also demonstrates that libraries of any complexity or elaboration can be distributed to the volunteers along with the commands that invoke those libraries. specifically, both the data and the library encoding for the sequence analysis procedure are invoked by qm but entirely resolved and executed by the volunteer browsers. it also illustrates the ability of a volunteer node to call code and data from multiple sources which are independently developed and maintained.

consider, as in the first example, that we have some x that we wish to transform via some function f, so that x is now an array of urls that reference fasta files hosted by ncbi:   

 we want to perform a particular sequence analysis on each fasta file, namely a fractal mapreduce decomposition of the chaos game representation  <cit> . thus, we define a function f for use with the qm.map method that will take a url as input and return the results of the sequence analysis as output:   

 there is a key challenge, however, in that our function f depends on a usm function that exists only after an external library has been loaded. therefore, to specify the task completely, we will need either to include usm as part of f or else to pass a reference to the library in the form of a url. we chose the latter strategy in this case so that the library can be downloaded in parallel by each volunteer simultaneously without burdening the api server. each external function may have multiple dependencies, and thus qm.map accepts an optional env parameter so that the dependencies for each external function can be specified as an array of urls to be loaded sequentially:   

 finally, we will specify the box parameter explicitly for demonstration purposes. the box parameter takes the place of an api key and allows volunteers to execute tasks in a particular queue. this mechanism allows submitters to direct tasks into different queues and further enables the use of abstractions like mapreduce:   

 putting these definitions together, we now launch twenty individual genomic sequence analyses for simultaneous execution via   

 a full version of these examples can be found online at http://q.cgr.googlecode.com/hg/index.html. the version there includes the full urls to all twenty streptococcus pneumoniae genomes and also to the versioned libraries specified by env. an accompanying screencast for these examples is also provided in that page.

usage statistics
the dissemination of browser-based tools in social coding environments like github  <cit>  is characterized by the same expansive dynamics as social media. for example, although this is our first report describing it, qm can be – and has been – discovered by the community at large. during the  <dig> months period beginning in april  <dig>  qm received more than  <dig>  million api calls from  <dig>  ip addresses in  <dig> countries to more than  <dig>  qm “boxes” , with  <dig> boxes receiving more than  <dig>  calls each and  <dig> boxes receiving  <dig>  calls or more. the statistics of qm usage are described in figure  <dig>  and the geographic distribution of its users is described in figure  <dig>  it is unclear exactly how much of qm’s usage is associated with the distributed computational genomics web apps that motivated its development, but the wide geographic distribution of its users suggests an appeal driven by a more general interest in distributed computing. this interpretation is reinforced by unsolicited reports about qm in hpc media such as hpcwire  and insidehpc . finally, as noted in methods, all of the server- and client-side software are open-source and permissively licensed. the browser client requires nothing more than script tag loading to be included in a web app, and the server is just as accessible through npm  <cit> . it is therefore conceivable that other qm deployments are in use at other addresses, perhaps even within the firewall of medical centers, as was the specific intention of qm’s development.

the server load associated with orchestrating this initial heavy use of qm is very modest because of the reliance on code distribution rather than on code execution. in fact, the deployment supporting the usage statistics described above  was never overwhelmed by traffic spikes even though it was running on a shared-tenant virtual machine with just  <dig> mb of ram, 2× <dig> mb mongodb databases, and no hard drive. furthermore, the authors do not incur any maintenance costs for the public tool dissemination, either from github or from npm’s package repository. we are therefore committed not to collect any data beyond the broad statistics described in figures  <dig> and  <dig> for the reference deployment discussed here. particularly relevant for the biomedical usage scenario that motivated this work, we are also committed not to collect any data at all from private deployments of qm; in other words, no part of qm’s software ever sends data back to our server from other deployments. this design allows administrators to deploy their own qm servers through npm and fully configure their own security as needed for clinical and/or biomedical research usage. these assurances can, of course, be verified through inspection of qm’s source code.

discussion
qmachine is a web service for executing distributed workflows that can use ordinary web browsers as the ephemeral compute nodes of a crowdsourced supercomputer. the idea here is simple: commodity computers equipped with web browsers join an abstract machine by visiting a website, and they unjoin by navigating to a different site or by closing the browser. while a browser remains on the site, it reacts to input from the user and from the site’s backend infrastructure by executing js, which provides the abstract machine with some potential to perform computations. at any instant, the net computational potential available to a high-traffic website falls well within the hpc range, as shown in figure  <dig>  qm enables this potential to be harnessed with no nominal cost through volunteer computing.

mapreduce
many researchers with access to large-scale computational resources still find those resources inaccessible because “everyday” workflows often require more than just fast computers – they require programming skills that are harder to acquire. bioinformatics workflows increasingly rely on mapreduce as an abstraction, but available mapreduce resources still expose researchers to programming environments with strict procedural requirements and steep learning curves. qm is much simpler to set up and operate than apache hadoop  <cit> , for example. it allows users to run mapreduce jobs on multiple physical machines and to crowdsource elastic computing resources with the simplicity of writing and loading a webpage – skills performed every day by millions of people worldwide. we argue that using the web computing architecture explored by qm – that is, without installing a dedicated application – is a natural evolution of current cloud-based mapreduce services, just as hadoop was a step up from one-off compile-and-run workflows.

distributed computing
qm’s web service provides a message passing interface for distributed computing. this statement may at first sound paradoxical, but js’s single-threaded programming model does not limit js programs to single-threaded execution; external execution contexts can be used to support concurrency via event-driven programming. qm leverages browsers’ asynchronous  network communications layers to connect multiple machines’ execution contexts, but browsers that support web workers  <cit>  can execute concurrent programs within the same physical machine.

cloud browsers
an interesting new twist in the development of web computing architectures is the emergence of the “cloud browser”  <cit> . in these systems, a mobile browser behaves as a thin client when a webpage’s scripts demand heavy computation. cloud browsers therefore demonstrate browser scaling in the vertical direction, whereas qm demonstrates browser scaling in the horizontal direction. because qm makes no assumptions about its volunteers’ underlying resources, cloud browsers can volunteer for qm alongside ordinary browsers without loss of generality. in other words, cloud browsers represent enhancements of present-day browsers, while qm presents a solution for hpc that advances the underlying architecture of the web towards that of a global computer  <cit> .

biomedical applications
in clinical environments, it can be difficult or even impossible to distribute workflows due to privacy concerns that prevent sensitive data from leaving the hospital environment, where conventional hpc is typically absent. qm satisfies this preoccupation without requiring additional resources. as shown in figure  <dig>  the median computational power of the top <dig> hpc in november  <dig>  was roughly  <dig>  times faster than our lab’s standard-issue desktop machine. this is a much smaller factor than the number of machines in a typical medical center. thus, even if restricted to a single hospital environment, volunteer computing can still rival the total capacity of very substantial hpc resources.

qm can also be used to power workflows inside of a single workstation. in such a scenario, the workstation would run qm’s api server locally and use multiple browser tabs to execute the workflow in parallel. such a workflow might also incorporate existing bioinformatics tools such as the basic local alignment search tool   <cit>  by using traditional server-side scripting languages like perl  <cit>  or python  <cit>  to connect to qm’s api or even directly to the persistent storage layer.

security
the security of workflows that use qm is handled orthogonally to qm by the selection of volunteers and by access control to code and data. a number of considerations should nevertheless be made to assist in the configuration of its distributed operation. it is important to recall that the web browser executes js within a sandboxed environment, which, among other protections, prevents programmatic access to the filesystem of the volunteer machine. as a result, qm’s security is configured around two firewalls.

the first and most basic protection is associated with the uniqueness of the “box”  issued by the submitter, which should be shared only with trusted volunteers. an additional layer of security can be added through the use of open authentication such as oauth  <dig>   <cit>  to verify that only trusted volunteers are involved. this second layer of protection is particularly useful in creating audit trails. these two mechanisms can be combined in many ways, as appropriate for a particular workflow. for example, different steps of a workflow could be assigned to distinct cohorts of volunteers depending on the sensitivity of the code and data and/or the trustworthiness of the volunteers. the resulting granularity could also be used to build redundancy – and therefore robustness – into the distributed qm operation.

in short, the weakest link in qm’s architecture – and where the opportunities for abuse lie – derive from the sharing of the “box” by members of a group of volunteers. in this regard, the key feature of qm’s design is that the abuse can target the submitters but not the volunteers, because qm’s operations take place within the sandbox of the web browser.

CONCLUSIONS
qmachine was developed to respond to the challenges of – and to capitalize on the opportunities of – bioinformatics applications encountered in biomedical environments. for more than a decade, volunteer computing has enticed computational biology as a scalable and cost-effective high-performance computing solution. qm essentially ports that solution to the modern computational landscape, which is increasingly dominated by mobile hardware platforms and the use of the web browser as the universal software platform. the features of the modern web browser go beyond those that make it a high-performance computational environment with advanced communication layers; they also include the transformative feature that computations run in a robust sandbox that prevents access to the underlying machine’s potentially sensitive filesystem. qm also responds to another modern trend towards engaging hpc resources through the use of the mapreduce programming pattern, rather than through direct interactions with compute nodes. the sequence analysis application that illustrates the use of qm in this report offers the sort of immediate utility that would benefit bioinformatics applications in medical genomics. it is argued, however, that qm, as an “of the web” distributed computing system, may be just as useful in the identification of the fundamental features of pervasive web computing.

availability of supporting data
the streptococcus pneumoniae genome data are used directly from the publicly available online repository at http://ftp.ncbi.nlm.nih.gov/genomes/bacteria/, and the relevant fasta files have also been archived to http://q.cgr.googlecode.com/hg/data/, a version-controlled repository. the original data used to produce figure  <dig> were taken from http://s.top <dig> org/static/lists/xml/top500_201311_all.xml and are archived to http://q.cgr.googlecode.com/hg/data/.

source code
all source code for this paper is version-controlled and open-sourced. the primary source for qmachine’s code is located in a git  <cit>  repository at https://github.com/wilkinson/qmachine. the code and data for the illustrative examples shown in the results section are available in a mercurial  <cit>  repository at http://q.cgr.googlecode.com/hg/. quanah’s source code repository is available at https://github.com/wilkinson/quanah, and the usm repository is available at https://github.com/usm/usm.github.com.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
the original concept is due to jsa, and both authors made substantial contributions to the design interfaces of qm. srw designed quanah and implemented both qm and quanah, and jsa designed and implemented usm. both authors developed the report and tested the illustrative examples. both authors read and approved the final manuscript.

