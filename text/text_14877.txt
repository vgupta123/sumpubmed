BACKGROUND
the advent of large scale approaches investigating biological systems has generated a requirement for standard data formats that has been recognised by the bioinformatics community for several years. it is a major challenge to create standards that are stable and "future proof" for considerable lengths of time. in this document, we review the models associated with standard data formats for microarrays, proteomics and metabolomics . the experimental techniques in these areas are evolving rapidly, different laboratories use different instruments and software, and a single experiment can produce a wide range of heterogeneous data types. this causes problems because data produced in one laboratory often cannot be interpreted by other groups or compared with other data sets produced in a different setting. proposals have been made for data standards for microarrays , protein-protein interactions , mass spectrometry , and protein separation proteomics . there have also been proposed extensions to mage-ml to accommodate other types of experiment . data standards for metabolomics are at an early stage, but there are three models that could contribute to a data standard: sysbio-om models metabolome data arising from nmr  and mass spectrometry; ccpn  <cit>  is a comprehensive model of nmr data for macromolecules; and armet  <cit>  covers data arising from metabolomic studies on plants.

each proposal has been developed using various modelling strategies that enable unanticipated types of data to be encoded or to allow the model to be extended in the future. this extensibility of the models is an essential component because new functional genomics techniques are frequently developed, new instruments and software have parameters and data types that must be stored, and there are no limits on the types of biological samples that can be tested. sufficient annotation must be captured to allow data sets to be interpreted, queried and analysed, the context must be unambiguous, and the format should capture sufficient detail about the provenance of data.

in this paper, we first describe the modelling structures that allow for extensions and the tasks that may be carried out over biological data. we analyse how well each task can be supported if information is captured within one of the extensible structures. the following section examines the extensible structures employed in the current models and highlights potential problems, in terms of tasks that may not be adequately supported. we then make recommendations as to the modelling structures that best support the most important tasks for common parts of a functional genomics workflow, and discuss the relevance of such structures to the development of new data standards.

modelling constructs for extensibility
there are several structures that can be incorporated into models that allow additional types of data to be encoded without affecting the core schema. in this section, we first describe the modelling constructs that allow for extensibility, and then describe the kinds of tasks that may be performed over experimental data and its associated annotations, with a view to clarifying which extensibility features support which tasks.

external ontologies
external ontologies are widely used for making extensible models. ontologies are structured controlled vocabularies containing defined terms. each term may be associated with a set of rules or relationships to other terms that allow logical questions to be asked of the ontology. terms from an ontology can be imported into a model, which is advantageous because the term has a meaning beyond the scope of the source system. furthermore, where there is a standard ontology, data produced by different laboratories will use the same terms, promoting greater uniformity across different systems. the following example demonstrates the use of ontologies within mage-ml:

<biosource identifier="biosource:drosophila:oregonr" name="drosophila strain, oregon r">

   <materialtype>

      <ontologyentry category="mged:materialtype" value="organism"/>

   </materialtype>

   <characteristic_assnlist>

      <ontologyentry category="ncbi:taxonomy" value="drosophila melanogaster"/>

      <ontologyentry category="flybase:genotype" value="wild type"/>

      <ontologyentry category="flybase:strain" value="oregon r"/>

   </characteristic_assnlist>

</biosource>

this example demonstrates the specification of a source of material  of a particular strain. the element <characteristic_assnlist> contains a set of characteristics of the biological material using terms obtained from two different controlled vocabularies. the flybase ontology  <cit>  has a definition of the "wild type" genotype and the "oregon r" strain of flies. the ncbi taxonomy  <cit>  is used to specify which species is being studied. the definitions can be retrieved if required to ensure that the species, strain and genotype are unambiguously described.

name-value-type triples
many of the data standards listed in the introduction have name, value, type  triples that allow additional parameters or data types to be added by the user which do not exist in a publicly available controlled vocabulary or ontology. "name" stores the item that must be captured, "value" is the data value and "type" is a qualifier or unit. the following example is taken from the mzxml format:

<namevalue name ='heatedcapillarytemperature' value=' <dig> ' type='celsius'>

in this example, there is an additional property  that must be encoded in the data format but was not incorporated in the core schema. the parameter has a parent element that corresponds to the mass spectrometry device, demonstrating that nvt is usually context-sensitive.

external files
additional information not covered in the data schema can be captured in separate files that are referenced from the source document. many data formats are encoded with extensible markup language , which is a fairly verbose format. in some instances, information is captured in separate tab-delimited files, spreadsheets, word processing documents or image files. for example, both mage-om and pedro specify that image data should be stored in a separate file and referenced by a uri .

inheritance
inheritance is used in software engineering to reduce the size of a model and make explicit areas of overlap by re-using certain components. models are often represented in the unified modeling language  <cit>  , which facilitates the design of software systems in a platform independent manner. the example in figure  <dig> demonstrates how inheritance has been used in mage-om, the object model that is part of the microarray data standard. the classes labeledextract, biosource and biosample are all subclasses of the general class biomaterial. the associations between biomaterial and other classes are inherited by labeledextract, biosource and biosample. these three classes have additional properties that make them more specific than biomaterial.

inheritance could be used to make a model extensible by designing a set of generic classes that describe components shared across all possible domains that use the format. such a data model could grow over time by the addition of new subclasses containing attributes that are specific to a particular domain or to a newly emerging technology. this would have the effect that previous versions of the standard should still be supported by software, and that the standard can continuously evolve. an example of a standard developed in this way is the geography markup language  <cit>  , which contains a modular structure allowing developers to use the subsets of the model that apply to their domain of interest. inheritance has also been used extensively in mage-om, the microarray object model, and the model has been extended by the addition of new subclasses in fge-om and sysbio-om that do not affect classes defined in mage-om. while there is no current proposal in functional genomics for the development of an evolving standard, the release of the fge-om and sysbio-om models raises interesting questions as to whether this may be a feasible methodology for defining an extensible data standard. in the results section, we define this kind of extensibility as extend model inheritance .

tasks
we have identified a set of tasks that a data model must support for users. in some instances it is assumed that the data model has been implemented in a system, such as a database. the tasks are as follows:

• search: performing of simple searches over the attributes coded in extensible structures to retrieve particular data sets.

• share: sharing of data sets between different research groups.

• read: manual reading of data files  to understand the intention and execution of an experiment.

• repeat experiment: the provision of sufficient detail on methods and protocols to allow an experiment to be repeated.

• compare experiments manually: manually determine how similar different experiments are.

• compare experiments automatically: automatically determine using a software system if two experiments are sufficiently similar to allow results to be directly compared. although there are many other issues that may prevent automatic comparison of results, such as the use of incompatible accession numbers from different databases to identify the same objects.

• query: querying the parts of an experiment that have been encoded in extensible structures to retrieve particular subsets of data or to ask more complex questions about the structure of the data.

• analyse: performing of statistical or analytical processes over the data set.

• browse: manually browsing the contents of a set of data files to find relevant experiments.

• populate: creating data sets conforming to the standard.

there are also several tasks that fall into a different category, relating to the development and management of the model.

• modelling: the ease with which the model can be created.

• data capture interface: the cost to develop the user interface for populating the data format.

• query or browse interface: the cost to develop the user interface for browsing or querying the data format.

• data management: the cost of data management in terms of time for developers to implement changes to the database schema or additional software required for parsing.

• quality assurance: the ability of the representation to prevent inclusion of an incorrect or imprecise value.

support for tasks
this section presents an analysis of how well each task is supported by the different extensible structures. the support for each task by each extension is described in table  <dig> and table  <dig>  for many of the tasks, we differentiate between performing within an organisation  and the task being performed by a user from a different organisation from where the data are produced .

extensibility in biological models
mage-om
a standard has been developed for microarray data, of which one part is an object model, called mage-om , which is expressed in uml. the developers of mage-om recognised that microarray technology was still evolving, that the types of experiments were fairly diverse, and that the biological samples on which experiments could be performed are practically infinite, yet all the information should be captured in a structure that would support many of the tasks described above. therefore, several modelling constructs have been used in mage-om to create a highly extensible object model.

ontologies in mage-om
mage-om has many specified places in which parts of external ontologies can be imported. examples include the characteristics of biological samples, types of biological material or compounds, and taxonomic classifications of organisms. a term can be obtained from any ontology as long as the source of the term is specified. this allows the object model to be stable but the external ontologies can grow over time with contributions from domain experts to increase the coverage of the data standard. changes to the ontology are unlikely to cause software to fail whereas most software is dependent on the structure of the object model.

nvt triples in mage
the extendable class in mage-om has a relationship to a class  that has the attributes name, value and type . all other classes in mage-om are subclasses of extendable and inherit this relationship, allowing additional properties to be captured in nvt triples with no restrictions. in the various data repositories that support mage, there have been few, if any, reported uses of general nvt triples because there are usually specific classes that have been used to capture a particular concept. the inclusion of the nvt triple class could cause problems as experimental parameters encoded in this way could not be automatically compared with other experiments that have modelled parameters correctly and the values may not be capable of being queried.

external files
mage-om represents processed data, resulting from image analysis, in external files containing tab-delimited data. the model captures metadata to describe what each column refers to, which is essential to ensure that when the data files are re-analysed there should be no misinterpretation of what is contained within external files. this design is advantageous because tab-delimited data files are more compact than xml. mage-om also allows external image files to be specified , as image files tend to be in standard formats that can be interpreted by widely available software.

extension to mage-om through inheritance
there has been no formal attempt to evolve the mage-om standard by the addition of new classes that inherit from parts of the core model, but there have been two proposals that have extended mage into other areas of functional genomics, called fge-om and sysbio-om. both models cover microarrays and proteomics, and sysbio-om additionally covers metabolomics. in several places the two models have extended mage-om through the use of inheritance. for example, both proposals include new subclasses of biomaterial  to model substances specific to proteome studies, such as spots on a two-dimensional gel and fractions from a column separation. the two models also create new subclasses of classes modelling a generic laboratory treatment, the inputs to the treatment and the output. a similar design is used in pedro . it is interesting to note that several different designs have arrived at a similar method for specifying laboratory treatments, raising the possibility that mage-om could become a standard that grows over time through the addition of new subclasses modelling inputs, treatments and outputs.

pedro
overview
the pedro  model was released in early  <dig> to stimulate community involvement in the development of a data standard for proteomics. pedro consists of an object model expressed in uml, which covers protein separation techniques, such as gel electrophoresis and liquid chromatography, and protein identification using mass spectrometry. around the same time the proteomics standards initiative  was founded by the human proteome organisation  to develop data standards for proteomics in the context of protein-protein interactions and mass spectrometry . pedro has been accepted as the working model of psi for protein separation based experiments. pedro is divided into four sections capturing  the design of the experiment and source of material,  protein separation,  the experimental setup for ms, and  database identification of proteins with ms data. the design methodology of pedro is significantly different from mage-om. pedro has detailed classes containing attributes that specify exactly what data type should be stored in which position. the model is very tightly specified and it is unlikely that experimental annotation encoded in pedro would be open to widespread misinterpretation. however, the model is relatively rigid and cannot easily be extended to cover unanticipated data types. pedro does not utilise extensible structures to describe biological samples, and therefore cannot store a structured description of all types of sample that may be used in proteomics.

ontologies, nvt and inheritance in pedro
pedro uses ontologies in a small number of positions, such as additional parameters for database searches or unanticipated types of laboratory treatment. there are no instances of ontology usage in the database implementation , due to the lack of controlled vocabularies in the proteomics field at present. there are no positions at which nvt triples are employed in pedro. pedro uses inheritance by including superclasses that capture  the concept of a substance  used in a proteomics experiment and  the type of processing or technique used . an analyteprocessingstep takes instances of analyte as input and output. the specific details of each processing step or substance are captured in subclasses. this design could in theory be extended by adding new subclasses of analyteprocessingstep and analyte. an evolving standard may be possible, although the overhead of vetting, discussing and finalising additions to the model may be prohibitively costly.

external files
images of electrophoresis gels are represented in separate files in pedro, which is an acceptable solution because most users will have software that can view the majority of image file formats. pedro also specifies that a file containing the input parameters for ms instruments or database searches can be specified. this could cause problems if the file is not in a standard format because it will support very few of the important tasks for the user, such as query or compare experiments automatically. if the file is a proprietary format, the information may not be readable or accessible to some users at all.

models for mass spectrometry
there have been several proposals in the past covering general ms data formats, including spectroml  <cit>  and andi  <cit> . we focus on three recent proposals for ms data standards: mzxml produced by the institute for systems biology, mzdata developed by psi and animl developed by astm  <cit>  . the mzxml format is a superset of the data formats produced by different instrument manufacturers, and software has been developed to convert many of the vendor specific data formats to mzxml. it is planned for future versions that controlled vocabularies will be used for vendor specific details, such as the name and type of instrument used. however, the current version of the mzxml schema does not use ontologies to capture additional information; instead, the options for terms are included within the schema. this design means that the schema can be used immediately with no additional resources required but that it cannot be extended to cover new types of technology without releasing a new schema. additional information can be captured in the format using an nvt element that has no restrictions.

the mzdata format has a similar goal to mzxml, namely to provide a single encoding of information from the different output formats produced by ms instruments. controlled vocabularies will be used to populate many parts of the format including lists of instrument parameters, the detection mechanism, and the type of ms analysis. supplementary information can also be captured for several objects in an element that captures the name of the object, the value and the simple data type . this might cause problems because, as stated above, nvt triples may not be open to automated analysis. the source file from which the mzdata file is created can be referenced using a uri. source files are usually proprietary formats that cannot be processed by other groups. as such, there will be limited benefit in relating the mzdata file back to its source, except for the purposes of local laboratory management.

the mzxml and mzdata formats have very limited descriptions of the biological samples used in the experiment because it is intended that they will be used in conjunction with another data standard, such as the psi-om  <cit>  model of proteome data. various instrument parameters can be captured in both models using nvt triples, which could cause problems for querying or comparison of different data files. however, instrument parameters are unlikely to be used for searching or querying and rarely for analysis; therefore, it is possible that nvt triples are an adequate structure for encoding such information.

animl is a model for analytical chemistry data, including the output from mass spectrometry, nmr and chromatography. animl consists of a flexible core defined by an xml schema. there are extensions for different experimental techniques, which are xml instance documents, rather than xml schemas, defining the allowable values. this approach could be viewed as a combination between using inheritance and ontologies because specific terms are defined that should be used in particular places in the format. in this context, this is an extension of the core schema by providing more strict requirements in the form of controlled vocabularies. however, the controlled vocabularies are effectively hard-coded in the extensions.

convergence of mass spectrometry formats
it is essential that the three formats converge to some extent to allow standardisation of mass spectrometry data files. one of the main differences is the method in which controlled vocabularies are referenced. the mzdata format can include references to an external list of terms with accession numbers. in contrast, mzxml includes the terms hard-coded within the schema, although it is planned for mzxml version  <dig> that external cv terms will be used. animl has specific terms in the technology specific extensions. the advantage of placing the terms outside of the schema, as in mzdata and in the animl technology instance documents, is that changes can be made to the list of terms without releasing a new schema. this has the disadvantage that additional software is required to verify that external terms have been used correctly. the mzxml format can be validated using only a standard xml schema parser but if new terms are required, a new version of the schema must be released.

there has recently been an agreement that the same terms will ultimately be used by mzdata, mzxml and animl. furthermore, future versions of mzxml will include references to external vocabularies, hence becoming closer to mzdata in structure. it should be possible to write software that converts data between the different formats, although it is unlikely that all formats will have exactly the same coverage. it is hoped that the different organisations continue to collaborate to bring about the unification of the formats.

molecular interaction format
a standard data format for protein interaction experiments, such as yeast two-hybrid  <cit> , has been developed by psi called the molecular interaction format , which is defined by an xml schema. the first release of the format  covers the data that is available in most of the publicly accessible databases. psi has developed a controlled vocabulary of terms which are used at specific places in mif. an example term is the name of the experimental method but the format does not have a detailed description of the experimental protocols or the biological samples used. descriptions of experimental protocols will be required in future versions because the results of protein interaction experiments are highly dependent on the technique used  <cit> . the gene ontology  <cit>   will be used for describing genes and proteins and the ncbi taxonomy will be used to standardise the names of species. extensible structures may be less important for mif because its primary use is the transfer of data between pre-existing databases. as such, the format's requirements are known in advance to some extent. if extensions are required, they can be accommodated in the next release of the standard.

metabolomics
there are three data models that have relevance for the metabolomics community: sysbio-om, armet and ccpn. sysbio-om is an extension of mage-om with the addition of new classes to model nmr data that may arise in a metabolome investigation. armet is a proposal from the plant metabolomics community to capture the large volumes of data that are being produced as a result of gc-ms  experiments on plants. ccpn is a data model produced by the nmr community to capture details of the starting sample, the input parameters and the output from the instrument. ccpn could be used to capture metabolome data because nmr is a commonly used technique for analysing the metabolites present in a sample.

sysbio-om has a close correspondence with mage-om, and shares the same kinds of extensible modelling structures. therefore, the comments about nvt, ontologies and external files for mage-om are also relevant for sysbio-om. ccpn contains a fairly detailed object model, and many classes have a large number of attributes that specify exactly the data types that can be captured. it is similar to the design of pedro in that it uses few extensible structures, although references to external databases for molecules or chemical compounds are allowed. data files produced from ccpn are likely to be consistent and open to querying, although the format may need constant updates if there are changes in technology.

the armet proposal specifies that controlled vocabularies can be used for describing biological samples and chemical compounds but uses few of the extensible structures described above. the developers of armet suggest that the format may evolve and it could be extended through inheritance. if extensions are developed to the model, it is vital that they are widely publicised to prevent the development of different dialects of the format that cannot be compared.

RESULTS
in this section we examine various parts of a generic functional genomics experiment , and determine the relative importance of each task listed above. we have identified the following areas that have highly similar annotation requirements across all types of experiment: the experimental hypothesis, the source of biological material, experimental protocols, numerical data and machine or software parameters.

experimental hypothesis
the purpose of a functional genomics experiment  is typically to discover the genes, proteins or metabolites that are present or expressed in a sample of interest, or those that are altered in one set of conditions compared with another. the critical difference between the conditions must be open to searching and querying. the hypothesis is often the first text that will be viewed by someone accessing the data set to determine its relevance, and it is therefore of primary importance that it can be read and browsed.

the relative importance of each task is summarised in table  <dig>  we believe that querying, searching, browsing and sharing are of greatest importance for experimental hypotheses. if these components are to be captured in extensible structures, ontologies are the only option that allow all these tasks to be well supported. nvt triples should not be used because querying or searching this information would be hindered.

source of biological material
the source of material is a critical part of the experimental annotation because the results of a functional genomics investigation only have any validity within the context of the sample from which they were generated. it is important that biological samples can be queried or searched to enable users to retrieve relevant data sets, and samples must be described in a manner that allows automated comparison of experiments . we stated that querying and searching are best supported over ontology terms and that nvt triples or external files will cause problems. it seems appropriate that efforts are focussed on designing ontologies that contain terms to describe samples, for instance as extensions to the mged ontology  <cit> .

experimental protocols
the basic protocols employed in an experiment have fairly similar semantics across all functional genomics experiments, and similar representations of experimental protocols are present in several models. it is unlikely that fine details of protocols will often be searched or queried but the protocol text must be easily readable to allow manual comparison of results . a well structured description of protocols may allow results from different experiments to be compared automatically. if nvt is used to express protocols, reading and manual comparison of experiments will be fairly well supported but automatic comparison will not be possible. the use of ontologies to capture protocols would improve facilities for automated comparison of experimental results, but the cost to model all types of protocol with controlled terms may be prohibitively high. there would be limited benefit storing protocols in external files, such as a word processing documents, compared with storing plain text within the core data format .

numerical data
the importance of the provision of support for tasks over numerical data is presented in table  <dig>  examples in this context could be raw or processed data, such as the ratios of fluorescence from a microarray scan, or quantification data from a proteomics experiment. the important tasks over numerical data are analyse, share and query. there may be metadata that describes the semantics of the values, which should be described using ontology terms if possible to allow queries over the data. the actual values could be stored in an external file, such as tab-delimited text or a spreadsheet over which standard analyses can usually be performed.

machine parameters
many types of instrument and software have a set of input parameters. the most important uses for the parameters are to allow the experiment to be repeated, and to underpin automated comparison of results between two or more experiments. in many cases the equivalence of results can only be established if all the parameters are equal. tasks such as query, search, read or browse are much less relevant . non-local repetition of experiments requires encodings using ontologies, and nvt should only be used to allow local repetition. however, it is unlikely that controlled vocabularies, containing parameters from all types of instrument, will exist. in this case, the use of nvt is preferable to storage in external files or in extensions to the model, because nvt should allow experiments to be compared manually, and the parameters can be accessed more easily if encoded in nvt rather than in a proprietary format.

discussion
it is important that data standards are created that allow flexibility in the data types that can be captured. this issue is particularly important for experiments such as proteomics, in which large volumes of data are created but the experimental methodology is frequently changing. data models must allow for extensions that cover new technologies, otherwise a "data standard" will only cover a subset of experiment types that exist. new proposals for standards would continue to arise, or intrusive changes would be required on a regular basis. developers should also be wary of creating models that are overly general or give users too many options for how information can be encoded. in these cases, dialects of models could arise where information can be encoded sufficiently but interpretation by different groups is difficult. we have examined data structures that allow for extensibility, identifying how well a set of tasks can be supported by data encoded in each type of structure . the general findings are as follows. for most parts of experimental annotation, ontologies give significant advantages to users because the standardisation of terms allows for improved searching and querying, and reduces the chance of terms being misinterpreted. ontologies also allow software to perform automated analysis to determine the similarity between different experiments. the disadvantage is that ontologies are expensive for developers to create, and present some additional costs to the user in data management . furthermore, ontologies will never be able to cover all the terms required by all users, because ontology development will always lag behind the creation of new experimental techniques, software or instruments. there are also issues of consistency and maintenance of ontologies which are unlikely to be resolved by official standards organisations due to the costs involved.

nvt triples give considerable flexibility to the user and they are preferable to the storage of parameters in proprietary formats because nvt encodings can be read and browsed. nvt triples are difficult to search or query though and they should not be used for data types that will be used frequently to retrieve data sets. it is important that data formats are well documented to ensure that there are guidelines for "reasonable" usage of nvt triples. in several of the data formats supplementary information about objects can be provided using ontology terms where they exist or nvt for user-defined terms. if the same user-defined terms are used frequently by different groups, this can be a mechanism for discovering new terms that should be updated in the ontology.

external files are an acceptable solution for images and for tab-delimited data if there are facilities within the core schema for capturing metadata describing the data type in each column in the file. external files should not contain information that is required for querying and searching, and they should be in a standard format that all users can process easily.

we have suggested that a data model could be developed incrementally using inheritance to add new classes that capture technology specific details. a model developed using this strategy must include generic classes that capture the concept of a laboratory technique, biological substances, raw and processed data. the core of the model should not contain details that are specific to a particular technology. any extensions that are developed must be carefully managed to ensure that parallel development of different models covering a single domain is avoided. there must be strict guidelines for the kinds of extensions that are allowed and good documentation describing the intended usage of the core model.

we briefly touched on the issue of data quality. data quality is a very broad concept that can be measured in a variety of ways relating to the consistency and credibility of a record  <cit> . consistency can be classified into format and value consistency. format consistency comprises rules for how the data should be parsed in terms of simple data type usage , cardinality and so on. format consistency could be verified over ontologies  and extensions to the model, but only to a limited extent on nvt and not at all over external files. the use of an ontology is the only extensible solution which allows verification of whether values meet semantic rules , for instance whether a genuine taxonomic name has been given for a "species" data type. other aspects of data quality, such as credibility, often depend upon domain specific knowledge and they are difficult to control at the level of model development. the result is that sufficient annotation must be stored in structures that can be browsed, searched or queried to allow users to make judgements about data quality, and that data are open to statistical analyses.

CONCLUSIONS
we have presented a classification of structures that allow for extensibility within models that are used to create standard file formats for functional genomics. we hope that the classification will help to guide the development of new data models and standards. the first version of a protein separation standard will be released by psi within the next year, the second version of the microarray standard mage-om is also under development, and metabolomics standards are being discussed. the guidelines we have presented should maximise the potential use of data sets, while allowing good expression of the data semantics as required by the users of each format.

