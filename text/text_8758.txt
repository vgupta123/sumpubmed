BACKGROUND
the onslaught of new genome sequences has begun to outpace the local computing infrastructures used to calculate and store comparative genomic information. for example, because the number of genomes has increased approximately  <dig> fold over the last  <dig> years, algorithms that detect orthologs and assemble phylogenetic profiles are faced with an increasing computational demand.

one such computationally intensive comparative genomics method, the reciprocal smallest distance algorithm , is particularly representative of the scaling problems faced by comparative genomics applications. rsd is a whole-genomic comparative tool designed to detect orthologous sequences between pairs of genomes. the algorithm  <cit>   employs blast  <cit>  as a first step, starting with a subject genome, j, and a protein query sequence, i, belonging to genome i. a set of hits, h, exceeding a predefined significance threshold  is obtained. then, using clustalw  <cit> , each protein sequence in h is aligned separately with the original query sequence i. if the alignable region of the two sequences exceeds a threshold fraction of the alignment's total length , the codeml program of paml  <cit>  is used to obtain a maximum likelihood estimate of the number of amino acid substitutions separating the two protein sequences, given an empirical amino acid substitution rate matrix  <cit> . the model under which a maximum likelihood estimate is obtained in rsd may include variation in evolutionary rate among protein sites, by assuming a gamma distribution of rate across sites and setting the shape parameter of this distribution, α, to a level appropriate for the phylogenetic distance of the species being compared  <cit> . of all sequences in h for which an evolutionary distance is estimated, only j, the sequence yielding the shortest distance, is retained. this sequence j is then used for a reciprocal blast against genome i, retrieving a set of high scoring hits, l. if any hit from l is the original query sequence, i, the distance between i and j is retrieved from the set of smallest distances calculated previously. the remaining hits from l are then separately aligned with j and maximum likelihood distance estimates are calculated for these pairs as described above. if the protein sequence from l producing the shortest distance to j is the original query sequence, i, it is assumed that a true orthologous pair has been found and their evolutionary distance is retained .

the algorithm is a multi-step process that composes several applications  into a straightforward workflow. the workflow involves the use of blast for initial sequence comparison, clustalw for sequence alignment, codeml for estimation of distance calculation, as well as various intervening conversion programs to ensure proper formatting of input keeping the tunable parameters of the algorithm constant, rsd scales quadratically with the number of genome sequences. however, to enable more flexibility for ortholog detection among distantly related organisms and also to enable the creation of clusters of recent paralogs, rsd should ideally be run over a range of values for both the divergence and evalue parameters, spanning from conservative to relaxed. thus, the total number of processes that must be run for n is /2)*m, where m represents the number of different parameter settings for evalue and divergence.

assuming that the current number of genomes sequences, n, is  <dig>  and the number of different parameter settings, m, is  <dig>  the total number of processes required for a full complement of results would be  <dig> , <dig>  further assuming that each individual process takes on average  <dig> hours , and constant access to  <dig> cores of computer processing power, the total time to complete this task would be  <dig>  hours, or  <dig>  years. therefore, the cost of operation of the rsd algorithm can be quite extensive and magnified not only by the influx of new genome sequences, especially as sequencing technologies continue to improve in speed, efficiency and price, but also by the rate at which genomic sequences are updated and revised. in fact, to keep pace with genome additions and revisions, ensuring all-versus-all for both new and existing genomes, the number of processes rises as: f =  + /2)) × m, where n is the number of genomes awaiting computation of orthologs,  <dig> is number of genomes previously processed, and m is the number of different parameter settings.

elastic cloud architectures, for example amazon's elastic computing cloud   <cit> , represent a possible solution for rapid, real-time delivery of cross-genomic data as the availability of genomic information continues to climb at a rapid pace. typical uses of the cloud have been in the areas of economics, health, and the entertainment industry, but so far this computing solution has had limited impact on the field of comparative genomics. only a handful of projects have been launched, for example, the sanger institutes fast matching and alignment algorithm to assemble full human genome  <cit> , cloud burst to map next generation sequencing data  <cit> , cloud blast a "clouded" implementation of ncbi blast  <cit> , a virtual laboratory for protein sequence analysis on cloud established at indiana university  <cit> , and an algorithm to search for single nucleotide polymorphisms  <cit> . yet the number of cloud resources is on the rise, with service-based cloud environments from microsoft  <cit> , google  <cit> , amazon  <cit> , sgi  <cit> , and more, lending an unprecedented opportunity to evaluate the capabilities of the cloud for sustainable and large-scale comparative genomics.

in the present study, we elected to test the capabilities of ec <dig> for all-against-all ortholog calculation using the reciprocal smallest distance algorithm across a wide array of recently sequenced genomes. our study examines the efficacy of the cloud in general, and the best practices for optimal setup and operation within the ec <dig> in particular. we expect that the protocols developed as a consequence of our research will be readily scalable to other problems within the space of comparative genomics as well as to other fields employing similar workflows for large-scale computation.

RESULTS
cloud testing and configuration
prior to the successful operation of the cloud, it was necessary to choose optimal settings for various parameters used by the elastic mapreduce framework , the framework that enables parallel processing within the elastic compute cloud . the complete configuration of the cloud for both the blast and ortholog estimation steps required that  <dig> parameters be set . the argument "--jobconf mapred.map.tasks" was used to specify a priori the number of map tasks for both the blast step and ortholog computation step of the rsd cloud algorithm. in our case, the number of map tasks was the number of blast comparisons and number of ortholog computations, respectively. in cases similar to ours, for example, situations where a user is only running blast or clustalw, this setting would still need to be used, but adjusted appropriately to equal the number of blast comparisons or clustal alignments required. since our process flows did not need a reducer step, the output of the mapper task was the final output of each job, and the number of output files  generated was equivalent to the total number of mapper tasks.

specific commands passed through the ruby command line client to the elastic mapreduce program  from amazon web services. the inputs specified correspond to  the blast step and  the ortholog computation step of the rsd cloud algorithm. these configurations settings correspond to both the emr and hadoop frameworks, with two exceptions: in emr, a --j parameter can be used to provide an identifier for the entire cluster, useful only in cases where more than one cloud cluster is needed simultaneously. in hadoop, these commands are passed directly to the streaming.jar program, obviating the need for the --stream argument.

certain parameters including "--jobconf mapred.task.timeout" required tests to identify the best value for optimal performance and cost effectiveness of the compute cloud. this parameter was used to specify the maximum number of hours needed to execute the rsd cloud algorithm on a single pair of genomes. if the value for this parameter was set to be too low, ortholog comparisons exceeding this setting were marked as failed by the emr program causing after  <dig> consecutive tries the node to be blacklisted by emr and no longer available for further computational processes. on the other hand, if the value for this parameter was set to be too high, jobs that had failed due to transient filesystem errors or other reasons were left running in a zombie state, thereby burning time and resources. in either case, the size of the compute cloud and the speed of the calculations were negatively impacted. therefore, we empirically determined an optimal setting for this parameter by benchmarking the time period needed to complete the largest pairs of genomes available in our roundup data repository  <cit> . we determined the best "goldilocks" setting to be  <dig> seconds . this ensured that the emr process would distinguish between long-running and failed jobs without impacting the availability of nodes within the cluster.

in addition, the allocation of the heap space was of critical importance to ensure proper function of the compute cloud. through various test runs we discovered that the jobtracker daemon would frequently run out of memory and require manual restarts. because this occurred on the master node, the entire cluster would be negatively impacted. to avoid this, we used a bash script that would automatically reconfigure the memory allotted to the daemon at launch time. we placed the script on s <dig> and passed it to the emr program using the "--info" option. the script accepted a simple argument designed to reallocate the memory assigned to the jobtracker daemon from the default setting of 1gb to 3gb for blast processes and 5gb for rsd processes. these values represented upper bounds and successfully avoided memory-related compute cloud failures.

compute cloud processing
we selected  <dig> small bacterial genomes that had not already been incorporated into the existing roundup repository  <cit> . to provide a comprehensive test of the capabilities of the ec <dig>  we computed orthologs for all pairs of these  <dig> new genomes, plus the number of processes needed to compare these  <dig> with the existing set of genomes included in the roundup repository,  <dig> at the time of writing, bringing the total number of genomes compared to  <dig>  as such, the total number of computational jobs run on the cloud was  <dig>  computed as +)* <dig> +ccn*n-l/2)+)*x, where n is the number of new genomes and x represents the number of parameter combinations typically calculated by the roundup tool, in this case  <dig>  the first part of the formula corresponds to the blast procedure and the second corresponds to the ortholog estimation step. although the  <dig> new genomes used for our study were relatively small, the genomes contained in the roundup repository against which these were compared spanned a wide range of large eukaryotic and smaller bacterial genomes. the smallest genome contained  <dig> sequences and the largest contained  <dig>  and the time for completion of any genome comparison ranged from approximately  <dig> minutes to  <dig> hours. table  <dig> provides a detailed summary of the process time and cost per step.

these cost estimates are based on the use of the high-cpu extra large instance at  <dig>  per hour and use of emr at  <dig>  per hour. these costs assume constant processing without node failures. total costs = $ <dig> 

throughout the execution of both the blast and ortholog estimation steps, we were able to monitor the health of our compute cloud through the user interface for the jobtracker daemon on the master node . this ui enabled us to see that our map tasks executed properly and to monitor their status as they completed. we were also able to monitor individual running blast and ortholog estimation jobs in more detail using the job summary user interface .

our decision to use high-cpu extra large instances proved both cost and time effective. although alternatives such as the standard small instances were cheaper per hour, our calculations demonstrated that these alternatives would have required substantially more compute time to complete, ultimately resulting in the same cost totals .

amazon's elastic compute cloud  can be accessed via a number of differet instance types. for the purposes of our comparative genomics problem, we elected to utilize the extra-large high-cpu instance. note that the total cost for a small instance is equal to the total of the extra large, despite the large difference in computing time.

discussion
comparative genomics will continue to demand high performance computing solutions. this is especially true as new genome sequencing technologies continue to drive down costs and ramp up volume. the work we present here represents one of the first successful deployments of a standard comparative genomics tool, the reciprocal smallest distance algorithm , to amazon's elastic compute cloud  via the web service elastic mapreduce .

to date, most use cases on the cloud have fit the paradigm embodied by the hadoop and emr frameworks. the applications are written in java and are generally "pleasingly parallel" compute problems, such as text or image processing. as such, they conform well to the configuration expected. our tool, which is likely to be similar to many other comparative genomics algorithms, deviated sharply from these hadoop and emr standard use cases. the largest deviation was that the rsd algorithm involves a pipeline of programs written in languages other than java, including python, perl, and c. at first glance, the streaming functionality provided by emr appeared to be a viable out-of-the-box solution. however, this function also was not designed to handle complex operations like that inherent to rsd. the original intent of the streaming function was to pass input via standard-in to the mapper for processing, the results of which would be passed via standard-out to the reducer for summation. as such, the object of the mapper was expected to reside within an input directory on the hadoop distributed file system used by emr. given the complex stream of operations needed by rsd including the need to run a host of programs across whole genomic sequence databases, this straightforward design proved too inflexible. therefore, we elected to generate, prior to compute cloud configuration, a single input file containing the rsd command-line arguments needed for the set of genomes to be compared. this input file became the object of the mapper, enabling the mapper to read the rsd commands line-by-line and to launch them to compute nodes for processing. this solution provided the flexibility necessary to accommodate a host of programs written in alternative languages aside from java while retaining the capabilities of the emr program, most importantly including fault tolerance and job tracking. because the endpoint of every map task was a file containing the orthologs and evolutionary distances for a specific pair of genomes, a reducer step was not required. however, going forward one could be used for meta-analysis of the results from individual map tasks, such as selecting the most conserved orthologs among a set of genomes, or for assembly and analysis of phylogenetic profiles.

with this solution, we were able to take full advantage of the compute cloud to run rsd in the same way as it would be run on a local linux compute farm, for a manageable cost. we ran over  <dig>  processes in total, computing results for  <dig> fully sequenced genomes, including  <dig> new genomes not previously incorporated into the roundup online genomics resource that employs the rsd algorithm. this successful application demonstrated that the cloud represents an ideal platform for either augmentation of existing local computing hardware, or for complete replacement. we anticipate that other comparative genomics tools that have similar workflows and that are not written entirely in java will be able to take advantage of the solutions we present here. in particular, the instantiation of the compute cloud, the run configuration steps via the ruby clc , and our use of the streaming function of emr should be immediately scalable to other similar problems.

in sum, based on our successful deployment of rsd on amazon's ec <dig>  we believe that cloud architectures represent an exciting alternative to standard approaches to high performance computing for comparative genomics. it remains to be seen how much of an impact cloud architectures and the "pay-as-you-go" model of computing provided by vendors like amazon will have on the future of comparative genomics and other fields requiring heavy computation. our prediction is that the impact will be significant and that within  <dig> years, a majority of applications like the one studied here will be ported to cloud architectures.

CONCLUSIONS
cloud computing architectures are rapidly emerging as robust and economical solutions to high performance computing of all kinds. to date, these architectures have had a limited impact on comparative genomics. here we describe the successful deployment of a commonly used comparative genomics tool, the reciprocal smallest distance algorithm, to the elastic compute cloud  of amazon web services using the elastic mapreduce .

one of the most important components of our deployment was the configuration and use of the streaming function provided by both emr and hadoop. by using this function, we were able to capitalize on the technical advantages conferred by emr/hadoop, without having to recode our own sequence analysis workflows into java, and without having to design our own solutions for job queuing, tracking and maintenance. these steps are applicable to virtually any sequences analysis workflow with little or no changes to the configuration settings that we describe. in addition, the procedures we have outlines can be ported to any cloud environment that accommodates standard linux distributions running hadoop. thus, we expect that more and more applications like ours will be running on cloud environments in the near future.

