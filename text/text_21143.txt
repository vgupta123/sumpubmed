BACKGROUND
the electronic health record  contains valuable information entered by clinicians. besides its immediate clinical use at the point of care, the ehr, when treated as a repository of medical information across many patients, provides rich data waiting to be analyzed and mined for clinical discovery. patient notes, in particular, convey an abundance of information about the patient’s medical history and treatments, as well as signs and symptoms, which, often, are not captured in the structured part of the ehr. the information in notes can be found in the form of narrative and semi-structured format through lists or templates with free-text fields. as such, much research has been devoted to parsing and information extraction of clinical notes  <cit>  with the goal of improving both health care and clinical research.

two promising areas of research in mining the ehr concern phenotype extraction, or more generally the modeling of disease based on clinical documentation  <cit>  and drug-related discovery  <cit> . with these goals in mind, one might want to identify concepts that are associated by looking for frequently co-occurring pairs of concepts or phrases in patient notes, or cluster concepts across patients to identify latent variables corresponding to clinical models. in these types of scenarios, standard text-mining methods can be applied to large-scale corpora of patient notes. collocation discovery can help identify lexical variants of medical concepts that are specific to the genre of clinical notes and are not covered by existing terminologies. topic modeling, another text-mining technique, can help cluster terms often mentioned in the same documents across many patients. this technique can bring us one step closer to identifying a set of terms representative of a particular condition, be it symptoms, drugs, comorbidities or even lexical variants of a given condition.

ehr corpora, however, exhibit specific characteristics when compared with corpora in the biomedical literature domain or the general english domain. this paper is concerned with the inherent characteristics of corpora composed of longitudinal records in particular and their impact on text-mining techniques. each patient is represented by a set of notes. there is a wide variation in the number of notes per patient, either because of their health status, or because some patients go to different health providers while others have all their visits in the same institution. furthermore, clinicians typically copy and paste information from previous notes when documenting a current patient encounter. as a consequence, for a given longitudinal patient record, one expects to observe heavy redundancy. in this paper, we ask three research questions:  how can redundancy be quantified in large-scale text corpora?  conventional wisdom is that larger corpora yield better results in text mining. but how does the observed text redundancy in ehr affect text mining? does the observed redundancy introduce a bias that distorts learned models? or does the redundancy introduce benefits by highlighting stable and important subsets of the corpus?  how can one mitigate the impact of redundancy on text mining?

before presenting results of our experiments and methods, we first review previous work in assessing redundancy in the ehr, two standard text-mining techniques of interest for data-driven disease modeling, and current work in how to mitigate presence of information redundancy.

redundancy in the ehr
along with the advent of ehr comes the ability to copy and paste from one note to another. while this functionality has definite benefits for clinicians, among them more efficient documentation, it has been noted that it might impact the quality of documentation as well as introduce errors in the documentation process  <cit> .

wrenn et al. <cit>  examined  <dig>  patient notes of four types  and assessed the amount of redundancy in these notes through time. redundancy was defined through alignment of information in notes at the line level, using the levenshtein edit distance. they showed redundancy of 78% within sign-out notes and 54% within progress notes of the same patient. admission notes showed a redundancy of 30% compared to the progress, discharge and sign-out notes of the same patient. more recently, zhang et al.  <cit>  experimented with different metrics to assess redundancy in outpatient notes. they analyzed a corpus of notes from  <dig> patients. they confirm that in outpatient notes, like for inpatient notes, there is a large amount of redundancy.

different metrics for quantifying redundancy exist for text. sequence alignment methods such as the one proposed by zhang et al.  <cit>  are accurate yet expensive due to high complexity of string alignment even when optimized. less stringent metrics include: amount of shared words, amount of shared concepts or amount of overlapping bi-grams  <cit> . while these methods have been shown to identify semantic similarity of texts, they do not specifically capture instances of copy-paste operations, which reproduce whole paragraphs.

blast  <cit> , the most popular sequence similarity algorithm in bioinformatics, is based on hashing of short sub-strings within the genetic sequence and then using the slower optimized dynamic programming alignment for sequences found to share enough sub-sequences.

the algorithm we present in this paper for building a sub-corpus with reduced redundancy is based on a finger-printing method similar to blast. we show that this algorithm does not require the slower alignment stage of blast and that it accurately identifies instances of copy-paste operations.

text mining techniques
we review two established text-mining techniques: collocation identification and topic modeling. both techniques have been used in many different domains and do not require any supervision. they both rely on patterns of co-occurrence of words.

collocations are word sequences that co-occur more often than expected by chance. collocations, such as “heart attack” and “mineral water,” carry more information than the individual words comprising them. extraction of collocation is a basic nlp method  <cit>  and is particularly useful for extracting salient phrases in a corpus. the nsp package we use in our experiments is widely used for collocation and n-gram extraction in the clinical domain  <cit> .

collocations in a corpus of clinical notes are prime candidates to be mapped to meaningful phenotypes  <cit> . collocations can also help uncover multi-word terms that are not covered by medical terminologies. for instance, the phrase “hip rplc” is a common phrase used to refer to the hip replacement procedure, which does not match any concept on its own in the umls. when gathering counts or co-occurrence patterns for association studies with the goal of high-level applications, like detection of adverse drug events or disease modeling, augmenting existing terminologies with such collocations can be beneficial.

collocations and n-grams are also used for various nlp applications such as domain adaptation of syntactic parsers  <cit> , translation of medical summaries  <cit> , semantic classification  <cit> or automatically labeling topics extracted using topic modeling  <cit> .

state of the art articles  and libraries  do not include any form of redundancy control or noise reduction. redundancy mitigation is currently not a standard practice within the field of collocation extraction.

topic modeling aims to identify common topics of discussion in a collection of documents . latent dirichlet allocation , introduced by blei et al.  <cit> , is an unsupervised generative probabilistic graphical model for topic modeling. documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. the words in a document are generated one after the other by repeatedly sampling a topic according to the topic distribution and selecting a word given the chosen topic. as such, the lda topics group words that tend to co-occur. from the viewpoint of disease modeling, lda topics are an attractive data modeling and corpus exploration tool. as illustrative examples, we show the top- <dig> tokens corresponding to three topics acquired from a corpus of patient notes in table  <dig>  the corpus consists of records of patients with chronic kidney disease.

topic modeling has been leveraged in a wide range of text-based applications, including document classification, summarization and search  <cit> . in the clinical domain, arnold et al. <cit>  used lda for comparing patient notes based on topics. a topic model was learned for different cohorts, with the number of topics derived experimentally based on log-likelihood fit of the created model to a test set. to improve results, only umls terms were used as words. more recently, perotte et al. leveraged topic models in a supervised framework for the task of assigning icd- <dig> codes to discharge summaries  <cit> . there, the input consisted of the words in the discharge summaries and the hierarchy of icd- <dig> codes. bisgin et al. <cit>  applied lda topic modeling to fda drug side effects labels, their results demonstrated that the acquired topics properly clustered drugs by safety concerns and therapeutic uses.

as observed for the field of collocation extraction, redundancy mitigation is not mentioned as standard practice in the case of topic modeling.

impact of corpus characteristics and redundancy on mining techniques
conventional wisdom is that larger corpora yield better results in text mining. in fact, it is well established empirically that larger datasets yield more accurate models of text processing . naturally the corpus must be controlled so that all texts come from a similar domain and genre. many studies have indeed shown that cross-domain learned corpora yield poor language models  <cit> . the field of domain adaptation attempts to compensate for the poor quality of cross-domain data, by adding carefully picked text from other domains  <cit>  or other statistical mitigation techniques. in the field of machine translation, for instance, moore and lewis  <cit>  suggested for the task of obtaining an in-domain n-gram model, choosing only a subset of documents from the general corpora based on the domain's n-gram model can improve language model while trained on less data.

in this paper, we address the opposite problem: our original corpus is large, but it does not represent a natural sample of texts because of the way it was constructed. high redundancy and copy-and-paste operations in the notes create a biased sample of the “patient note” genre. from a practical perspective, redundant data in a corpus lead to waste of cpu time in corpus analysis and waste of i/o and storage space especially in long pipelines, where each stage of data processing yields an enriched set of the data.

downey et al. <cit>  suggested a model for unsupervised information extraction which takes redundancy into account when extracting information from the web. they showed that the popular information extraction method, pointwise mutual information , is less accurate by an order of magnitude compared to a method with redundancy handling. they present a model for unsupervised information extraction which takes redundancy into account when extracting information from the web.

methods for identifying redundancy in large string-based databases exist in both bioinformatics and plagiarism detection  <cit> . a similar problem has been addressed in the creation of sequence databases for bioinformatics: holm and sander  <cit>  advocated the creation of non-redundant protein sequence databases and suggested that databases limit the level of redundancy. redundancy avoidance results in smaller size, reduced cpu and improved annotation consistency. pfam  <cit>  is a non-redundant protein sequence database manually built using representatives from each protein family. this database is used for construction of hidden-markov-model classifiers widely used in bioinformatics.

when constructing a corpus of patient notes for statistical purposes, we encounter patients with many records. high redundancy in those documents may skew statistical methods applied to the corpus. this phenomenon also hampers the use of machine learning methods by preventing a good division of the data to non-overlapping test and train sets. in the clinical realm, redundancy of information has been noted and its impact on clinical practice is discussed, but there has not been any work on the impact of redundancy in the ehr from a data mining perspective, nor any solution suggested for how to mitigate the impact of within-patient information redundancy within an ehr-mining framework.

RESULTS
quantifying redundancy in a large-scale ehr corpus
word sequence redundancy at the patient level
the first task we address is to define metrics to measure the level of redundancy in a text corpus. redundancy across two documents may be measured in different manners: shared words, shared concepts or overlapping word sequences. the most stringent method examines word sequences, and allows for some variation in the sequences . for example the two sentences: “pt developed abd pain and acute cholecystitis” and “pt developed acute abd pain and cholecystitis” would score 100% identity on shared words but only 73% identity of sequence alignment.

our ehr corpus can be organized by patient identifier. we can, therefore, quantify the amount of redundancy within a patient record. on average, our corpus contains  <dig> notes per patient, with standard deviation of  <dig>  minimum of  <dig> and  <dig> maximum notes per patient. there are also several note types in the patient record such as imaging reports or admission notes. we expect redundancy to be high across notes of the same patient and low across notes of distinct patients. furthermore, within a single patient record, we expect heavy redundancy across notes from the same note types. we report redundancy on same patient / similar note type .

within this scope, we observe in our corpus average sequence redundancy  of 29%: that is, on average one third the words of any informative note from a given patient are aligned with a similar sequence of words in another informative note from the same patient. in contrast, the figure drops to an average of  <dig> %  when comparing the same note types across two distinct patients.

the results of high redundancy in patient notes are consistent with wrenn et al. <cit>  observations on a similar ehr dataset. the contrast between same-patient and across-patient redundancy, however, is surprising given that the whole corpus is sampled from a population with at least one shared chronic condition. our interpretation is that the observed redundancy is most likely not due to clinical content but to the process of copy and paste.


the detailed distribution supports the distinction into  <dig> groups of notes: those with heavy repetition  and those with no repetition . a possible interpretation is that a group of patient files include many notes and tend to exhibit heavy redundancy while others are shorter with less natural redundancy. the level of overall redundancy is significant and spread over many documents .

concept redundancy at the corpus level
since free-text notes exhibit high level of variability in their language, the redundancy measures may be different when we examine terms normalized against a standard terminology. we now focus on the pre-processed ehr corpus, where named entities are mapped to umls concept unique identifiers  . we investigate whether a redundant corpus exhibits a different distribution of concepts than a less redundant one.

we expect that different subsets of the ehr corpus exhibit different levels of redundancy. the all informative notes corpus, which contains several notes per patient, but only the ones of types: “primary-provider”, “clinical-note” and “follow-up-note”, is assumed to be highly redundant, since it is homogeneous in style and clinical content. by contrast, the last informative note corpus, which contains only the most recent note per patient, is hypothesized to be the least redundant corpus. the all ehr corpus, which contains all notes of all types, fits between these two extremes, since we expect less redundancy across note types, even for a single patient.

one standard way of characterizing large corpora is to plot the histogram of terms and their raw frequencies in the corpus. according to zipf’s law, the frequency of a word is inversely proportional to its rank in the frequency table across the corpus, that is, term frequencies follow a power law. figure  <dig> shows the distribution of umls concepts  frequencies in the three corpora with expected decreasing levels of redundancy: the all informative notes corpus, the all notes corpus, and the last informative note corpus. we observe that the profile in the non-redundant last informative note corpus differs markedly from the ones of the redundant corpora . the non-redundant corpus follows a traditional power law  <cit> , while the redundant ones exhibit a secondary frequency peak for concepts which appear between  <dig> and  <dig> times in the corpus. in the highly-redundant all informative notes corpus, the peak is the most pronounced, with more concepts occurring four to eight times in the corpus than once.


the difference in shapes of distributions confirms in a qualitative fashion our hypothesis about the three corpora and their varying levels of redundancy. the observed contrast in distribution profiles indicates that more concepts are repeated more often than expected in the redundant corpora, and gives us a first clue that statistical metrics that rely on the regular long-tailed, power-like distributions will show bias when applied on the redundant ehr corpus. a similar pattern is observed at the bi-gram level .

impact of redundancy on text mining
we have observed that redundant corpora exhibit different statistical profiles than non-redundant ones, according to their word occurrence distributions. we now investigate whether these differences impact the performance of standard text mining techniques: collocation identification and topic modeling.

we compare the performance of standard algorithms for collocation identification and topic modeling inference on a variety of corpora with different redundancy levels. we introduce synthetic corpora where we can control the level of redundancy. these synthetic corpora are derived from the wall street journal  standard corpus. the original wsj corpus is naturally occurring and does not exhibit the copy and paste redundancy inherent to the ehr corpus. we artificially introduce redundancy by randomly sampling documents and repeating them until a controlled level of redundancy is achieved.

collocation identification
we expect that in a redundant corpus, the word sequences  which are copied often will be over-represented. our objective is to establish whether the collocation algorithm will detect the same n-grams on a non-redundant corpus or on a version of the same corpus where parts of the documents have been copied.

two implications of noise are possible. the first is false positive identification, i.e., extracting collocations which are the result of mere chance. the second implication is loss of significant collocations due to noise .

we apply two mutual information collocation identification algorithms  to the all informative notes corpus  and to the last informative note corpus . in this scenario, we control for vocabulary: only word types that appear in the smaller corpus  are considered for collocations. to measure the impact of redundancy on the extracted collocations, for each collocation, we count the number of patients whose notes contain this collocation. a collocation that is supported by evidence from less than three patients is likely to be a false positive signal due to the effect of redundancy .

we observe that the lists of extracted collocations on these two corpora differ markedly . the pmi algorithm identified  <dig>  collocations in the all informative notes corpus, and  <dig>  in the last informative notes corpus. when comparing the collocations extracted from the two corpora, we find that 36% of the collocations identified in the all informative notes corpus were supported by  <dig> patients or less, compared to only 6% in the last informative note corpus. see table  <dig>  for example, a note replicated  <dig> times signed by “john doe np”  was enough to gain a high pmi of  <dig>  for the “doe np” bigram .

collocations were extracted using a stringent cutoff of  <dig>  pmi.

the second type of error, loss of signal can also be observed. when comparing all collocations using the same tmi cutoff, the all informative notes corpus produces  <dig> times as many collocations as the last informative notes corpus , but we find that only 54% of the collocations found in the non-redundant corpus are represented in the bigger list.

another method for selecting the significant collocations is using a top-n cutoff instead of a pmi cutoff. comparing the top  <dig>  collocations with tmi for all informative notes and last informative notes, we find a marked difference of  <dig> collocations.

to control for size, we repeated the same experiment on a standard large-scale corpus, the wsj dataset, on which collocation identification algorithms have been heavily tested in the past .

comparison of extracted collocations on synthetic redundant corpora and non-redundant corpora . collocations were extracted using using true mutual information and pointwise mutual information .

consider a scenario where a corpus is fed twice or thrice in sequence to pmi , then the list of extracted collocations will be identical to that of the original corpus. this is expected based on the definition of pmi, and we confirm this prediction on wsjx <dig> and wsjx <dig> which produce exactly the same list of collocations as wsj- <dig> .

we observe a different behavior on wsjs <dig> : in this corpus, original sentences from wsj- <dig> are sampled between  <dig> and  <dig> times in a uniform manner . on this synthetic corpus, we obtain a different list of collocations when using the pmi algorithm:  <dig>  instead of  <dig> . the growth in number of extracted collocations is expected since wsjs <dig> is  <dig>  times larger than wsj- <dig>  but this growth is less than expected when comparing the trend  with a growth of  extracted collocations.

on the other hand, the collocations acquired on the redundant wsjs <dig> corpus have much weaker support than those obtained on wsj- <dig> . the differences we observe in this experiment are caused by the fact that some sentences only are copied, in a variable number of times . thus, pmi  does not behave similarly when fed with our different corpora.

in the case of this synthetic dataset, the newly acquired collocations are all due to the synthetic copy-paste process and are likely a false positive signal. one may ask, however, whether the fact that the sentences are repeated in ehr corpora reflects on their semantic importance from a clinical standpoint, and therefore, whether the collocations extracted from the full ehr corpus contain more clinically relevant collocations. this hypothesis is rejected by the comparison of the number of “patient-specific” collocations in the redundant corpus and non-redundant one: the collocations acquired on the redundant corpus cannot serve as general reusable terms in the domain, but rather correspond to patient-specific, accidental word co-occurrences such as  pairs. in other words, the pmi algorithm does not behave as desired because of the observed redundancy. for example, through qualitative inspection of the extracted collocations, we observed that within the top- <dig> extracted collocations from the full ehr redundant corpus,  <dig> appear only in a single cluster of redundant documents . the fact that redundancy never occurs across patients, but within same-patient notes only, seems to create unintended biases in the extracted collocations.

the results on the wsj and its synthetic variants confirm our results on the ehr corpora: collocations extracted on a redundant corpus differ significantly from those extracted on a corpus of similar size without redundancy. slightly weaker, though consistent, results were encountered when using an alternative algorithm for collocation identification on the ehr and wsj corpora .

topic modeling
the algorithm for topic modeling that we analyze, lda, is a complex inference process which captures patterns of word co-occurrences within documents. to investigate the behavior of lda on corpora with varying levels of redundancy, we rely on two standard evaluation criteria: log-likelihood fit on withheld data and the number of topics required in order to obtain the best fit on the withheld data. the higher the log-likelihood on withheld data, the more successful the topic model is at modeling the document structure of the input corpus. the number of topics is a free parameter of lda – given two lda models with the same log-likelihood on withheld data, the one with the lower number of topics has better explanatory power .

we apply lda to the same two ehr corpora  as in the collocation identification task, and obtained the results shown in figure  <dig>  the redundant corpus, though  <dig>  times larger, produces the same fit as the non-redundant corpus .


when applied to the synthetic wsj corpora, we get a finer picture of the behavior of lda under various corpora sizes and redundancy levels . the wsj- <dig>  wsj- <dig> and wsj- <dig> corpora are non-redundant and have increasing size. we observe that the log-likelihood graphs for them have the same shape, with the larger corpora achieving higher log-likelihood, and the best fits obtained with topic numbers between  <dig> and  <dig> . the behavior is different for the redundant corpora. wsjx <dig>  wsjx <dig>  and wsjs <dig> are all larger in size than wsj- <dig>  we therefore would expect them to reach higher log-likelihood, but this does not occur. instead, their log-likelihood graphs keep increasing as the number of topics increases, all the while remaining consistently inferior to the wsj- <dig> corpus, from which they are derived. the higher the redundancy level , the worse the fit.


furthermore, when comparing wsjx <dig> and wsjs <dig> corpora , which have roughly the same size, we note that the more redundant corpus  has consistently lower fit to withheld data than wsjx <dig> . this confirms that redundancy hurts the performance of topic modeling, even when the size of the input corpus is controlled.

even more striking, when examining the behavior of wsjs <dig>  up to  <dig> topics, we observe it reaches the same fit as wsj- <dig>  that is, redundancy “confuses” the lda algorithm twice: it performs worse than the original wsj- <dig> corpus although it contains the same documents, and the fit is the same as if the algorithm had roughly five times less documents .

we have seen that for the naturally occurring wsj corpus training on more data produces better fit to held out data . in contrast, we observe that the redundant all informative notes corpus, while  <dig> times larger than the non-redundant subset, does not increase log-likelihood fit to held out data.

to understand this discrepancy, we examine the topics obtained on the redundant corpora qualitatively. topics are generated by lda as ranked lists of words. once a topic model is applied on a document, we can compute the topic assignment for each word in the document. we observe in the topics learned on the highly redundant corpora that the same word may be assigned to different topics in different copies of the same document. this lack of consistency explains the confusion and consequently low performance achieved by lda on redundant corpora.

mitigation strategies for handling redundancy
given a corpus with inherent redundancy, like the ehr corpus, the basic goal of redundancy mitigation is to choose the largest possible subset of the corpus with an upper bound on the amount of redundancy in that subset.

we compare two mitigation strategies to detect and handle redundancy in a corpus – a baseline relying on document metadata and one based on document content . we focus on the all informative notes corpus. the metadata-based baseline produces the last informative note corpus. the content-based mitigation strategy, which relies on fingerprinting, can produce corpora with varying levels of redundancy. we report results for similarity thresholds of  <dig> ,  <dig>  and  <dig> . we expect that the lower the similarity threshold, the lower the actual redundancy level of the resulting corpus .

descriptive statistics of reduced corpora
all informative, input corpus, the corpus obtained by the redundancy reduction baseline , and the corpora produced by the fingerprinting redundancy reduction strategy at different level.

computation time for constructing a redundancy-reduced corpus at a given similarity threshold using the selective fingerprinting is  <dig> minutes .

to confirm that fingerprinting similarity effectively controls the redundancy level of the resulting corpora, we align a random sample of the notes included in the corpus for a sample of patients using different methods and different similarity cutoffs . the average amount of redundancy in removed note pairs is sampled as well. redundancy is computed in the same way as in section  <dig> . <dig>  we randomly sampled  <dig>  same-patient pairs of notes and aligned them using smith-waterman alignment.

amount of redundancy in a random sample of  <dig>  same-patient note pairs within the corpora using different similarity thresholds.

to investigate whether the corpora whose redundancy is reduced through our fingerprinting method are robust with respect to text mining methods, we focused on the following corpora: the inherently redundant all informative notes corpus, the baseline non-redundant corpus last informative notes , and “reduced redundancy informative notes”, a corpus created by selective fingerprinting with maximum similarity of 25%. the reduced redundancy informative notes corpus contains  <dig>  patient notes,  <dig>  as many notes as the last informative notes corpus while having same-patient redundancy of only  <dig> % compared to 29% in the all informative notes corpus.

performance of text mining tasks on reduced corpora
for collocations detection, in reduced redundancy informative notes,  <dig>  collocations were extracted, on average each collocation is supported by  <dig> distinct patients and collocations supported by  <dig> patients or less make 6% of the extracted collocation. we see a significant reduction in the number of collocations based on very few patients from 36% to 6% .

for topic modeling, figure  <dig> shows the log-likelihood fit on the ehr withheld dataset graphed against the number of topics for the lda topic modeling for three corpora. we see that the significantly smaller last informative note performs as well as all informative notes  while reduced redundancy informative notes  outperforms both. as we showed in figure 4a, we would expect a larger corpus to yield a better fit on the model: all informative notes is more than  <dig> times larger than last informative, still it yields the same fit on held out data. this is explained by the non-uniform redundancy of all informative as shown in figure 4b. in contrast, the reduced redundancy informative notes improves the fit compared to the non-redundant last informative notes in the same manner as wsj- <dig> improves on wsj- <dig> . this healthy behavior strongly indicates that reduced redundancy informative notes indeed behaved as a non-redundant corpus with respect to the lda algorithm.


CONCLUSIONS
training and improvement of nlp tools for medical-informatics tasks on public available data will continue growing as more ehrs are incorporated into health care givers worldwide. the nature of epidemiological research demands looking at cohorts of patients, such as our kidney patient notes. such cohort studies require application of text mining and statistical learning methods for: collocation detection , topic modeling with lda and methods for learning association between conditions, medication and more.

this paper identifies a characteristic of ehr text corpora: their inherent high level of redundancy, caused by the process of cut and paste involved in the creation and editing of patient notes by health providers. we empirically measure this level of redundancy on a large patient note corpus, and verify that such redundancy introduces unwanted bias when applying standard text mining algorithms. existing text mining algorithms rely on statistical assumptions about the distribution of words and semantic concepts which are not verified on highly redundant corpora. we empirically measure the damage caused by redundancy on the tasks of collocation extraction and topic modeling through a series of controlled experiments. preliminary qualitative inspection of the results suggests that idiosyncrasies of each patient  explain the observed bias.

this result indicates the need to examine the effect of redundancy on statistical learning methods before applying any other text mining algorithm to such data. in this paper, we focused on intrinsic, quantitative evaluations to assess the impact of redundancy on two text-mining techniques. qualitative analysis as well as task-based evaluations are needed to get a full understanding of the role of redundancy in clinical notes on text-mining methods.

we presented a novel corpus subset construction method which efficiently limits the amount of redundancy in the created subset. our method can produce corpora with different redundancy amounts quickly, without alignment of documents and without any prior knowledge of the documents. we confirmed that the parameter of our selective fingerprinting method is a good predictor of document alignment and can be used as the sole method for removing redundancy.

while methods such as our selective fingerprinting algorithm that extract a non-redundant / less-redundant subset of the corpus prevent bias, they still lead to lost information of the non-redundant parts of eliminated documents. an alternative route to text mining in the presence of high levels of redundancy consists of keeping all the existing redundant data, but designing redundancy immune statistical learning algorithms. this is a promising route of future research.

