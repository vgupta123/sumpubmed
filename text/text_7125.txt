BACKGROUND
various gene expression signatures have been extracted from breast cancer microarray data  <cit>  to predict outcome. subsequently, these signatures have been validated in larger cohorts  <cit> , the datasets were re-analyzed to assess their robustness  <cit> , or other data mining techniques were tested  <cit> . depending on the technique used, different signatures are extracted from the same dataset. often, these signatures have varying degrees of overlap in genes that compose the signature  <cit> . this has led to the insight that there is no unique signature  <cit> .

from a clinical point of view, accuracy of a signature is very important  <cit> . to extract such an accurate gene expression signature, which is predictive of outcome, a variety of methodologies have been proposed  <cit> . typically, the group of patients is dichotomized based on a five year survival threshold, resulting in a poor and a good outcome group. this assignment then serves as a class label, and the aim of this work is to predict this label. most protocols apply a cross validation scheme, with which the generalization error and optimal number of genes are estimated. a final classifier is obtained by selecting the optimal number of genes and training the final classifier on all samples.

michiels et al.  <cit>  introduced the concept of repeated random resampling to estimate the robustness  of proposed signatures. the strategy relies on repeatedly analyzing a subset of patients in a dataset , and then combining the results of the separate resamplings into a final list of genes. this strategy has been employed to investigate the stability of proposed signatures, and produces, by repeated resampling, a gene list which is employed as a type of 'gold standard'. this 'gold standard' has been shown to be different from signatures previously reported on the same data  <cit> .

however, the repeated random resampling approach has several drawbacks. first, resampling selects a subset of the available data, thereby reducing the number of samples and intensifying the small sample size problem. second, the actual ranks within each resampling are not used , which implies that the available information is not fully exploited. lastly, the true rank of genes is unknown, making it hard to show that this strategy indeed provides a better ranking.

we set up an experiment using artificial datasets to test the resampling strategy in a controlled environment. this allowed us to compare the resampling based ranking to the true ranking. in this comparison, we included two variants of the resampling strategy, and a non-resampling based strategy. on a higher level, datasets from different institutions consisting of samples from the same disease type, can be viewed as resamplings from the same underlying population. concurrent analysis of these datasets aimed at, for example, producing a classifier, can be performed using a range of integration strategies. from a statistical point of view, a larger dataset implies more statistical power, advocating an early integration approach where  datasets are pooled prior to the analysis  <cit> . a classifier derived from such a pooled dataset, should, in theory, be superior to those derived from a single dataset. of course, in order to minimize inter-dataset variability, one of a variety of normalization techniques should be employed  prior to pooling the datasets. alternatively, a rank-based classification methodology has been put forward  <cit> , which is independent of dataset normalization. moreover, heterogeneity in the different datasets might be a hurdle  <cit> , which might not be overcome with simple normalization approaches. in such cases, performing a meta-analysis may be advantageous, which implies combining statistics from each dataset. lastly, 'late integration'  involves training a classifier on the separate datasets and then merging the separate decisions to reach a final result. in fact, this approach possesses the least statistical power, since each of the individual classifiers have no benefit from the other datasets.

a setting using artificial datasets, allows for a controlled analysis of the effects observed when pooling these artificial datasets. if synergy exists between the artificial datasets, we hypothesize that the classifier performance should improve when more artificial datasets are pooled. therefore, we extended our analysis on artificial data to multiple artificial datasets. first of all, we start pooling pairs of artificial datasets, and thereafter increase the number of pooled datasets pooled until all six artificial datasets are pooled. we explored the effect of pooling artificial datasets on the classification performance , independent validation, and signature size.

currently, multiple breast cancer datasets are publicly available  <cit> . teschendorff et al.  <cit>  created a consensus er positive classifier from three breast cancer datasets employing a meta-analysis strategy.

these datasets were measured on different platforms, thereby introducing difficulties with respect to probe matching, and the fact that different reference pools may have been used. if direct pooling of the datasets would be allowed, such a meta-analysis would not yield the optimal result. moreover, true synergy of the meta-analysis on multiple datasets wasn't shown. similarly, xu et al.  <cit>  have created a consensus rank-based classifier from three different datasets, which was validated on a fourth independent dataset. to validate the type of effects observed when pooling artificial datasets, we applied the same analysis to a compendium of six breast cancer datasets . this compendium of breast cancer datasets is particularly suited for such an analysis, since they were all measured on the same platform . in this setup, we inspected the effect of pooling breast cancer datasets on the classification performance , independent validation , signature size, and functional enrichment.

lastly, the analysis on such a compendium of six breast cancer datasets, might provide answers to an important unsolved question: why is there only limited overlap between existing signatures?  <cit>  as potential explanations, several explanations have been postulated, none of which have up to now been rigorously tested. we employ the results obtained when progressively pooling more breast cancer datasets, to provide insight into the correctness of the five explanations, and identify sample size as the most likely cause of limited signature overlap.

RESULTS
resampling on a single breast cancer dataset
we applied the repeated random resampling strategy  on the veer et al.  <cit>  dataset, see figure  <dig>  when comparing the frequency-based ranking to the set of genes in the published 70-gene and 231-gene signatures, there appear to be additional genes with high frequencies of occurrence that were not part of the published signatures. both michiels et al.  <cit> , and roepman et al.  <cit>  point out that the genes with high frequencies of occurrence, which are not part of the original signature, are of high interest. they hypothesize that these genes were probably not picked up in the signatures due to sampling specific biases in the dataset. this suggests a beneficial effect due to the resampling which does pick them up. however, since the true ranking of genes underlying breast cancer outcome is unknown, a plausible alternative hypothesis could be that those genes are false positives.

roepman et al.  <cit>  considers all genes with a non-zero frequency of occurrence to represent a robust signature. this seems rather optimistic, since genes will be expected to have a positive frequency by chance. therefore, we propose to set a threshold to prune the set of genes to only those with a frequency higher than expected by chance . this threshold is indicated by the vertical red line in figure  <dig>  the set of genes retained after this pruning step still contains all genes from the original signatures, i.e. all light-blue bars  reside on the left of the threshold. of course, the ranking itself remains unchanged.

in the current experimental setting, it is impossible to conclude which hypothesis correctly describes the appearance of genes with a high  frequency of occurrence, that were not in the original signature.

resampling on a single artificial dataset
we set up an experiment on artificial data to be able to compare the ranking after resampling to a known ground truth . the artificial dataset mimics a microarray dataset, and consists of  <dig> genes, of which either  <dig>   <dig> or  <dig> genes were informative. the experiments were done using artificial datasets consisting of  <dig> to  <dig> samples, with equal priors for the two classes. the data was sampled from a class-conditional gaussian distribution, without any covariance structure. since genes are assumed to be independent, these datasets are more ideal than real microarray datasets. nevertheless, this controlled, artificial setting can be a useful tool to gain insight into the relationship between sample size and gene selection.

on these artificial datasets we compare the ranking resulting from the resampling strategy with the ranking based on all samples. additionally, we also consider two variants of the resampling strategy. instead of only storing the top n genes in each of the resamplings, we sum the statistics ) of the genes across all the resamplings, and base the final ranking on this cumulative statistic. summing the statistics amounts to a 'meta-analysis', and the top n method is a 'combining decisions' approach. we then compare the ranking obtained by each method to the ground truth . since a particular initialization of the artificial data might influence the results, we repeated the entire experiment  <dig> times, and present statistics across all repeats . figure  <dig> shows the spearman rank correlation between the known ranks of the artificial data and those obtained by the top n resampling approaches, the two resampling variants, and the ranking over all samples. for n we considered values of  <dig>   <dig> and  <dig>  similar to michiels et al.  <cit> , and roepman et al.  <cit> .

for smaller artificial datasets the obtained spearman rank correlations are lower compared to larger artificial datasets. the reduced sample size is the most plausible explanation for this effect. for larger artificial datasets, the gap in performance between the top n resampling approaches and the sum of ranks and sum of snrs widens, indicating that the latter two methods profit more from an increase in artificial dataset size.

taken together, these results clearly show that the random resampling strategy produces a suboptimal ranking of the prognostic genes, and that it is preferable to employ a ranking based on all samples in the dataset. in addition, these differences become more pronounced as the sample size increases. given that these effects are already evident on an artificial dataset, we hypothesize that the sample size effects will be even stronger on data with a complex covariance structure. we therefore conclude that it is unwise to use the repeated random resampling approach on real data for ranking features.

pooling artificial datasets
the experiment on a single artificial dataset is easily extendible to a scenario where multiple artificial datasets are available. such a setup would emulate the availability of several microarray datasets from the same biological context. we hypothesize that a significant association exists between the classification error and the number of datasets pooled.

we set up an experiment using six artificial datasets, each with  <dig> samples and  <dig> genes, of which  <dig> are informative. each of these six artificial datasets were generated using the same model, and no additional noise or heterogeneity was introduced. this represents the ideal case, where each of the six datasets are truly relevant for the same underlying problem. we applied the double loop cross validation  protocol on each artificial dataset separately , and each possible pooled subset of artificial datasets. first of all, we inspected the behavior of pooling pairs of artificial datasets relative to the performance on the individual artificial datasets. a dlcv error which is lower for the pooled pair compared to the individual errors implies a synergetic effect on the classification performance. when the pooled dlcv error is between the two individual errors, and lower/higher than the weighted mean  we label the effect as marginal synergetic/marginal anti-synergetic. conversely, if the dlcv error is strictly higher than the individual errors it is labeled as anti-synergetic. figure  <dig> shows a network indicating the type of synergy that was observed for the six artificial datasets, labeled art <dig> to art <dig> 

next, we evaluated the dlcv error for every potential pooling of one to six artificial datasets. in addition, we evaluated the performance of each of the classifiers derived from the pooled data on an independent large validation set of  <dig> artificial samples, see figure 4a and 4b. figure 4a and 4b show that a high degree of synergy is obtained by pooling artificial datasets. both the dlcv error as well as the error on the validation set show a highly significant correlation with the number of artificial datasets that is pooled, pearson correlation of - <dig>   and - <dig>  , respectively.

to test whether the observed trends are classifier specific, we repeated the experiment using non-linear classifiers . thus, we repeated the experiment using a k-nearest neighbor classifier  as well as a support vector machine with a radial basis function as kernel . using these classifiers, we observed the same trends for the dlcv and validation error. that is, in all cases the performance gets better after pooling the artificial datasets . however, the errors obtained are higher than those from the nmc. the plots are shown in additional files  <dig> and  <dig> 

in addition to the classification performance, we looked at the optimal number of genes that is selected by the classifiers, see figure 4c. it turns out that there is a highly significant positive trend  towards selecting more genes when artificial datasets are pooled.

in summary, pooling artificial datasets shows that larger training sets result in larger gene sets being selected and, most importantly, in better classification performance. since this effect is already clearly present in the simple model, we believe that this effect will be even stronger in more complex models which include, for example, a covariance structure between genes. this stems from the fact that these more complex models will require more data  to accurately estimate the required complex classifiers.

pooling breast cancer datasets
we used a compendium of six affymetrix breast cancer datasets to evaluate the effect of pooling datasets on the performance of classifiers . our aim is to classify patients into the poor/good outcome groups as well as possible. to this end, we followed the same strategy as on the artificial datasets. first, we inspected the synergy effect on the dlcv error achieved on pairs of datasets relative to the dlcv errors on the individual datasets . figure  <dig> shows a network indicating the synergy among the six real datasets.

the survival column indicates the type of survival data that was used; dmfs distant metastasis free survival; sos breast cancer specific overall survival. lastly, for each dataset the total number of samples is indicated along with the number of poor/good samples per er subgroup.

as a next step we applied the dlcv framework on each potential pooling of the datasets, and assessed the dlcv error. in addition, we evaluated the performance of each classifier on a seventh independent dataset . figure 6a and 6b show the dlcv error and validation error as a function of the number of datasets pooled.

there is a clear trend indicating that the dlcv error decreases when more datasets are pooled . the observed trend has a pearson correlation of - <dig> , which is significant: p =  <dig> e -  <dig>  of course, since the different datasets have different numbers of samples, an alternative would be to evaluate the correlation of the error and the actual number of samples in the pooled datasets. doing this unveils a similar significant correlation .

the results on the seventh independent validation set  confirm the synergetic effect obtained by pooling datasets . the trend is even more pronounced at a pearson correlation of - <dig> , with p =  <dig> e -  <dig>  in addition, in a multivariate cox proportional hazards model the signature is an independent predictor of outcome in the presence of the standard clinico-pathological variables . similarly, plotting the results in a kaplan-meier plot indicates a highly significant separation .

the nearest mean classifier that was used in the above experiment is a relatively simple linear classifier, which is known to perform well on this type of data  <cit> . to test whether the observed trend is classifier specific, we repeated the experiment using non-linear classifiers. thus, we repeated the experiment using a k-nearest neighbor classifier  as well as a support vector machine with a radial basis function as kernel . the results that were obtained for the dlcv error and the validation error on the vijver dataset are shown in additional files  <dig> and  <dig>  for the k-nn and svm classifiers, respectively.

overall, the average efpfn across all  <dig> different pooling combinations when using the k-nn classifier  is slightly worse compared to the nearest mean classifier . in addition, the error obtained using the svm  is higher than the k-nn. similarly, the average error on the independent validation set  is also higher: . this corresponds to the previous observation that simpler classifier often perform better on this type of data. at the same time, the trend that classifiers that are derived from pooled datasets are more accurate is also observed.

in conclusion, not only simple linear classifiers, but also more complex non-linear classifiers show a clear benefit by training them on pooled datasets. moreover, these results confirm that the linear nearest mean classifier is the best candidate for this type of classification problem.

previously reported classifiers have been using varying numbers of genes. signatures that have been derived using a cross validation setup that is similar to the one used here, use  <dig>  <cit>  to around  <dig> genes  <cit> . we have investigated the number of genes selected by the dlcv method, for each of our pooled datasets, see figure 6c. the classifiers trained on a single dataset use around  <dig> genes, whereas pooled datasets select progressively more genes. especially the classifiers from five or six datasets select more than  <dig> genes. these results indicate a positive trend in the number of datasets pooled and the number of genes selected in the signatures .

in summary, the effects observed when pooling artificial datasets are also observed when pooling breast cancer datasets. this implies that datasets should be analyzed jointly rather than in isolation.

enrichment of the signatures
functional enrichment of prognostic signatures has unveiled a multitude of pathways that are associated with tumor progression. we set out to inspect the effect of pooling datasets on the enrichment. we used a collection of  <dig> genes sets consisting of at least five annotated genes . for each pooled combination, we used the hypergeometric test to assess the significance of the overlap, followed by bonferroni correction .

the most highly enriched categories were proliferation associated, which are well-known to be associated with tumorigenesis. figure  <dig> shows the enrichment for three of the gene sets. figure  <dig> depicts a complete heatmap of enrichments.

as seen in figure 7a, the mitosis go category  shows a significant enrichment for practically every pooled combination of datasets. this is an example of a very strong signal, which gets even stronger when pooling datasets. another proliferation related category , the cell cycle pathway  becomes significantly enriched when roughly three or more datasets are combined. thus it's association is weaker than the mitosis gene set, but gets detected when pooling a few datasets.

lastly, a gene set associated to microtubule activity  is non-significant for the range of pooling one to three datasets . when combining four or five it is sometimes picked up, and is definitely picked up when pooling six datasets. this is a clear example of a relatively weak signal, which benefits significantly from pooling datasets.

conversely, we also observe that many gene sets which are significant in some of the single/pairs of datasets are no longer significant when pooling . for instance, glycolysis related pathways are only picked up in the signature extracted from the pawitan et al.  <cit>  dataset. this is evidence that those categories are specific to this dataset, but represent false positives within the context of the global breast cancer population.

overall, the enrichment analysis provides further proof that pooling datasets significantly increases the statistical power to detect weak signals and filter out false positive associations.

heterogeneity amongst the breast cancer datasets
pooling datasets with a high degree of heterogeneity might have a detrimental effect on subsequent analysis. a potential way to limit the heterogeneity would be to restrict the analysis to known clinical subgroups. to test this, we split each dataset based on er status, and repeated the analysis on each of these subgroups. of course, these splits reduce the number of samples and intensify small sample size related problems. for this reason it becomes impractical to exhaustively explore all possible stratifications of the dataset. we chose the er status since it is known to have a profound effect on disease progression and gene expression.

on the subgroup of er positive samples, we observed the same trends as on the complete datasets. we observed a synergetic effect for  <dig> of the  <dig> of the pooled pairs of datasets . moreover, the trends observed on all samples are also significant on the er positive subset . enrichment analysis of these signatures revealed the same set of categories that were found on the complete set of samples .

similarly, we also repeated all analyses on the er negative subgroup of samples. we observe the same trends as on all samples or the er positive group . however, the classification accuracy on this subgroup is much lower. a potential explanation for this observation is that this group of samples is inherently more difficult to classify, or that the small sample size related problems play a big role .

enrichment analysis of the signatures from the er negative group, leads to a very limited set of enriched gene sets . the most highly enriched category is 'go:0006955: immune response'. this confirms previous reports on the association of this category with outcome in the er negative group  <cit> . however, in the largest dataset , not a single category is enriched. in both of the er groups, we observed the same synergetic trends when pooling datasets on cross validation, independent validation, and the number of genes selected. therefore, heterogeneity among the datasets due to er status is only limited.

overlap amongst signatures
it has been previously observed that existing signatures have limited overlap in terms of genes. several potential explanations have been proposed for these differences:

 <dig>  heterogeneity in expression due to different platform technologies and references;

 <dig>  differences in supervised protocols with which signatures are extracted;

 <dig>  although the genes are not exactly the same, they represent the same set of pathways;

 <dig>  differences in clinical composition between datasets  and

 <dig>  small sample size problems that cause inaccurate signatures.

here we will critically evaluate whether we observe support for each of these explanations in the context of the six breast cancer datasets we studied.

explanations  <dig> and 2
in our analysis, we have restricted ourselves to the same platform for training our classifiers, and use the same supervised protocol on all datasets. this allowed us to check whether the first two explanations apply. we evaluated the relative overlap of every combination of two signatures that were extracted from a single dataset, see figure 9a. for these  <dig> comparisons, the majority of genes are exclusively part of one signature . this clearly indicates that, within the context of the datasets studied here, explanations  <dig> and  <dig> are not the likely causes of the limited overlap between the signatures from these six datasets.

explanation 3
using our functional enrichment analysis we checked explanation  <dig>  the enrichment patterns of the signatures from single datasets are largely disparate . two of the six signatures are not enriched for any functional category. there is only one gene set , which is enriched in three signatures, namely those derived from desmedt et al.  <cit> , miller et al.  <cit> , and loi et al.  <cit> . when considering the set of enriched gene sets that overlap between every pair of signatures , we observed a mean overlap of less than 1%. taken together, this indicates that there is very little support for the third explanation in these six datasets.

explanation 4
breast cancer is well known to be a complex disease which manifests itself in a range of subtypes. as a result, differences in the clinical composition of different datasets might explain the different signatures. however, when creating a histogram of signature overlap within the er positive/negative subgroups we also found that the majority of genes are only part of one signature. thus, heterogeneity with respect to er status does not explain the limited degree of overlap among the signatures from these six datasets. of course, it could still be that differences in the composition with respect to another  clinical parameter partly explain the lack of overlap.

explanation 5
signatures that are derived from pooled datasets have been derived using more samples, thereby easing small sample size related difficulties. therefore, we inspected the relative overlap between signatures derived from one, two and three datasets. to avoid any bias, we only consider pairs of pooled datasets that do not overlap in terms of samples. the histogram and p-values shown in figure 9aâ€“c represent the mean over all these combinations. the overlap becomes progressively larger for signatures that are extracted from larger datasets . at the same time this overlap becomes more significant  =  <dig> , two pooled datasets: -log <dig> =  <dig> , three pooled datasets: -log <dig> =  <dig> ). the same trends were observed on the artificial data, and the er positive subgroup . next, we repeated this analysis by looking for overlap in terms of enriched gene sets . similarly, signatures derived from the pooled datasets show a higher degree of enriched gene sets that overlap. the distinguishing property of these signatures is the fact that they were derived from a larger dataset, i.e. more samples. thus, we conclude that explanation  <dig> is the most relevant explanation for the low degree of overlap in genes from signatures derived from these six datasets.

consensus signature across six datasets
the signature from the six pooled datasets will be substantially different from those derived from each individual dataset. to gain insight into the difference, we created a chart indicating the rank position of all  <dig> genes selected in the classifier trained on all six datasets, along with rank positions from these genes in the rankings obtained from the individual datasets . additional file  <dig> contains detailed information on the  <dig> genes, and the centroids of the classifier. most of the  <dig> signature genes, had a much lower rank  in the individual signatures, implying that they're not part of those signatures. the difference in ranking of the  <dig> genes is especially large compared to the signature derived from the minn et al.  <cit>  and chin et al.  <cit>  data. this implies that those rankings are dominated by genes which are not generally relevant for breast cancer.

we compared our signature to signatures derived from different breast cancer datasets. we compared to the 70-gene signature from wang et al.  <cit> , 76-gene signature from vijver et al.  <cit> , 70-gene signature from naderi et al.  <cit> , and the 52-gene signature from teschendorff et al.  <cit> . the latter of which was derived from the first three datasets. none of these three datasets were employed to derive the  <dig> gene signature. the overlap between the  <dig> genes and the teschendorff signature is more significant  than with naderi , vijver  or wang . this provides additional evidence that the limited signature overlap is most likely caused by small sample size difficulties.

the set of genes in the signature from all six datasets contains many well known and described genes. for instance, it confirms the significance of hmmr  which was only recently associated with breast cancer  <cit> . this gene was not picked up in any of the individual signatures. in the same study, pujana et al.  <cit>  derived a set of  <dig> genes that are associated with brca <dig>  brca <dig>  atm, and chek <dig>  this set of  <dig> genes is significantly overrepresented in the  <dig> signature genes .

biological relevance of the consensus signature
cell cycle and mitosis pathways are the common background of most prognostic gene expression classifiers. not surprisingly, many of the  <dig> genes have been previously described as markers of proliferation and poor prognosis in different types of cancer. these include: bub1b, bub <dig>  mad2l <dig>  c16orf <dig> , cct <dig>  eif4ebp <dig>  melk, mybl <dig>  ccna <dig>  ccnb <dig> and ccnb <dig>  more specifically, eif4ebp <dig>  is a eif4e-binding protein that plays a critical role in the control of protein synthesis, survival, and cell growth. rojo et al.  <cit>  showed that 4ebp <dig> is activated in a high percentage of breast tumors, and is associated with higher malignant grade, tumor size, and local recurrence regardless of her2/neu status . lin et al.  <cit>  showed that melk , is overexpressed in both breast cancer specimens and cancer cell lines, and that its kinase activity possibly plays a significant role in breast cancer cell proliferation. they show that down-regulation of melk by treatment with sirna significantly suppressed the cell growth of breast cancer, indicating its crucial role in the proliferation of breast cancer.

c16orf <dig>  and cct <dig> are examples of genes associated with therapy resistance. more specifically, c16orf <dig>  was previously found to be involved in multidrug resistance in breast cancer  <cit> , and mrna levels of cct <dig> were significantly associated with resistance to docetaxel  <cit> .

cellular movement and cytokinesis pathways are significantly over-represented in our prognostic signature mainly through genes  linked to the microtubule motor activity  and the microtubule-based movement  go categories. it is of major interest that these go categories are non-significant for the range of pooling one to three datasets and are only picked up when pooling more datasets.

the genes that are associated with the microtubule categories are all part of the kinesin family. this family is composed of more than  <dig> genes involved in spindle assembly and function, chromosome segregation, mitotic checkpoint control and cytokinesis. they are motor proteins that generate movement along microtubules. kif <dig> is a prognostic factor in breast and lung cancer . kif20a is over-expressed in pancreatic ductal adenocarcinoma  and sirna knockdown of kif20a expression in pdac cell lines attenuated growth of those cells . kif2c is also known to be over-expressed in breast cancer cells. treatment of breast cancer cells  by sirna against kif2c suppressed kif2c expression and inhibited the growth of the breast cancer cell lines . immunoprecipitation assay showed an interaction between prc <dig> and kif2c in breast cancer cells . kif4a is overexpressed in cervical cancer . treatment of lung cancer cells with sirna for kif4a suppressed the growth of these cells and induction of exogenous expression conferred cellular invasive activity on mammalians cells . tubb <dig> over-expression is a marker of poor prognostic in pancreatic, non-small cell lung carcinoma and ovarian cancer  and is a potential marker of resistance to treatment by paclitaxel or vinorelbine-based chemotherapy. kinesin related genes are currently vigorously pursued as therapeutic targets. taxanes and vinca alkaloids are major drugs in the clinical routine and are designed to bind and inhibit the motor function of the microtubules. the high frequency of side-effect of these drugs is due to the simultaneous alteration of microtubule cell-resting and differentiation function.

drug companies are now putting an effort into the development of new molecules specifically targeting the kinesin family. four other genes also present in the signature, tenc <dig>  sema3g, cxcl <dig>  and cx3cr <dig> also play a role in regulating cell motility and migration independently of the microtubule pathways .

taken together, this shows that many of the genes in the consensus signature have been characterized within a breast cancer con- text, and some have even been considered as drug targets. nevertheless, not all of them have been covered, opening up possibilities for follow-up analysis. the fact that these genes have been derived from a compendium of six pooled datasets boosts confidence in their relevance within a clinical context.

CONCLUSIONS
in our analysis we aimed to extract a robust classifier from multiple datasets. we explored the possibility of pooling datasets, which should alleviate small sample size related problems, and increase robustness. for a single dataset, we revisited the resampling strategy  <cit>  which has been employed as a 'gold standard' for evaluating gene sets and has been applied to extract robust signatures, e.g. yu et al.  <cit> . our analysis provides proof that the resampling strategy does not alleviate potential problems due to sampling biases. instead, by only taking the top n genes in every resampling into account, the obtained feature ranking is inferior to the ranking obtained based on all samples. combining the statistics from each of the individual resamplings , provides a ranking which is equivalent to the ranking based on all samples. therefore, we propose to use the ranking over all samples in order to train a final classifier. 

concurrent analysis of multiple datasets is an established way to increase statistical power, and thereby robustness. pooling the datasets prior to any analysis would enable the largest gain in statistical power. we presented an example of pooling artificial datasets, which unveiled a significant negative correlation between the number of datasets which is pooled, and the dlcv error/validation error. we also observed a significant positive correlation between the degree of pooling and the number of genes which is selected. thus, on the artificial data there is a clear synergetic effect associated with pooling of datasets.

similarly, pooling breast cancer datasets represents a way to improve classification performance, since the pooled datasets will provide a better resemblance to the true underlying breast cancer population. for instance, breast cancer subtypes that are not present in one dataset might be present in another. a classifier trained on a pooled dataset was hypothesized to be more robust when applied to unseen, independent data. to test this hypothesis, we used a collection of six independent datasets, all measured on the same platform. indeed, we observed a lower classification error for classifiers trained on pooled datasets. this significant trend is supported by a validation on a seventh independent dataset. the trends observed for pooling datasets strongly confirm the belief that there is a synergetic effect between pooling datasets and classification performance. recently, another group  <cit>  has come to a similar conclusion, albeit using only three datasets for training, and one for testing. a shortcoming in their analysis is that they do not switch the roles of which datasets are used for training and testing. moreover, they mention the issue of small overlap among signatures, but do not provide any clear answers to that issue. pooling in itself might lead to unwanted effects . our results do not rule out the existence of detrimental effects due to pooling heterogeneous datasets, but imply that the synergetic effect of pooling is stronger than the detrimental effect of heterogeneity.

signatures that have been extracted from independent cohorts of patients share only a limited number of genes. a variety of explanations have been proposed, only some of which have been thoroughly tested. we have checked whether there is any support for five of these explanations. on a collection of six breast cancer datasets, which were all measured on the same array, we applied the same supervised protocol. this lead to signatures that still lack overlap in terms of genes contained in the signatures. similarly, our results point out that the overlap in terms of enriched pathways is just as limited, and reducing heterogeneity by stratifying based on er status does not resolve the issue either. of course, er status might not be the only important source of heterogeneity. therefore, we cannot completely rule out the effect of data heterogeneity on signature overlap. we believe that the limited signature overlap is most likely due to small sample size problems . more specifically, that the variability between classifiers stems from the fact that the individual datasets are too small to capture the important sources of variation in the data. moreover, pooling datasets might average-out adverse effects from heterogeneity that is only present in a subset of the samples/datasets. explanation  <dig> is supported by a much larger overlap between signatures from pooled datasets than between signatures derived from single datasets. nevertheless, we would like to point out that these conclusions might be specific to the six breast cancer datasets analyzed here and the specific techniques we employed to perform these analyses. that is, this conclusion may not extrapolate to different datasets/other tumor types.

further evidence for the synergetic effect of pooling datasets comes from the functional enrichment analysis. we observed both low signal associations that become stronger, as well as categories that we can label as false positive associations since they're only found in one dataset and disappear when the complete set is analyzed. moreover, we pick up relevant breast cancer associated genes in the signature derived from six pooled dataset, that are not picked up in any of the signatures from the separate datasets. many of the genes in the signature have been previously characterized as drug targets and since our analysis is based on a large compendium it boosts our confidence in their potential relevance for clinical practice.

overall, we hypothesize that the  <dig> gene signature that was derived from the six pooled datasets is one of the most robust signatures currently available. our results indicate that new breast cancer datasets should not be analyzed in isolation, but should be analyzed in the context of the available compendium of breast cancer samples

