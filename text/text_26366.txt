BACKGROUND
the ability of an antibody to respond to an antigen, such as a virus capsid protein fragment, depends on the antibody’s specific recognition of an epitope, which is the antigenic site to which an antibody binds. based on their structure and interaction with antibodies, epitopes can be divided into two categories: linear and conformational. a linear epitope is formed by a continuous sequence of amino acids, whereas a conformational epitope is composed of discontinuous primary sequences, which are close in three-dimensional space.

several different approaches exist for predicting linear and conformational epitopes. previous studies relied on the varying physicochemical properties of amino acids to predict linear epitopes . a study on  <dig> amino acid scales revealed that predictions based on the best-performing scales poorly correlated with experimentally confirmed epitopes  <cit> . this result prompted the development of machine-learning methods to improve prediction. bepipred combines amino acid propensity scales with a hidden markov model to achieve marginal improvement over methods based on physicochemical properties  <cit> . abcpred uses artificial neural networks  for predicting linear b-cell epitopes  <cit> . chen et al. proposed the novel amino acid pair  antigenicity scale  <cit> , for which the authors trained a support vector machine  classifier, using the aap propensity scale to distinguish epitopes and nonepitopes. bcpreds uses svm combined with a variety of kernel methods, including string kernels, radial basis kernels, and subsequence kernels, to predict linear b-cell epitopes  <cit> .

an increase in the availability of protein structures has enabled the identification of conformational epitopes by using various computational methods. for example, discotope  <dig>  uses a combination of amino acid composition information, spatial neighborhood information, and a surface measure for predicting epitopes  <cit> . ellipro uses thornton’s propensities and applies residue clustering to identify epitopes  <cit> . seppa  <dig>  predicts conformational epitopes based on the unit patches of residue triangles, and the clustering coefficient for describing local spatial context and compactness with two new parameters appended, asa  propensity, and consolidated amino acid index  <cit> . epitopia combines structural and physiochemical features, and adopts a bayesian classifier to predict epitopes  <cit> . epsvr uses a support vector regression method to predict conformational epitopes. the meta learner epmeta incorporates consensus results from multiple prediction servers by using a voting mechanism  <cit> .

in this study, we propose combining multiple predictions to improve epitope prediction based on two meta-learning strategies: stacked generalization   <cit>  and cascade generalization   <cit> . these strategies work in a hierarchical architecture of meta learners and base learners, in which the input space for meta learners is extended by the predictions of the base learners. we selected several linear and conformational epitope predictors as the base learners, and evaluated four inductive learning algorithms as the meta learners. to evaluate performance, we tested the combinatorial method on an independent set of antigen proteins that were not used previously to train the epitope prediction tools according to the documents on the tools and their publications. our results indicate the potential of meta learning for epitope prediction.

RESULTS
prediction correlations between base learners
for a meta-learning method to perform effectively, the base learners must have complementary predictive capabilities, which can be reflected by relatively low correlation among their predictions. we selected four conformational and four linear epitope predictors as our base learners. the conformational predictors were discotope  <dig>   <cit> , ellipro  <cit> , seppa  <dig>   <cit> , and bpredictor  <cit> , and the linear epitope predictors were bepipred  <cit> , abcpred  <cit> , aap  <cit> , and bcpreds  <cit> . we calculated the pearson’s correlation coefficients for the prediction scores produced by the base prediction tools. to further analyze the correlations among predictions based on the score rankings, we sorted the prediction scores of all protein residues provided by each base learner and then conducted a spearman’s rank correlation analysis. tables  <dig> and  <dig> list the pearson’s correlation coefficients and spearman's rank correlation coefficients of all pairs of linear and conformational predictors, respectively. the average correlation coefficients of the linear and conformational prediction tools were  <dig>  vs.  <dig>  and  <dig>  vs.  <dig>  in the pearson’s and spearman’s correlation analyses, respectively, which indicate a relatively weak correlation among the epitope predictions of the base learners.table  <dig> 
correlation analysis of linear epitope predictors



linear
aap
abcpred
bcpreds

pearson
spearman
pearson
spearman
pearson
spearman



conformational
seppa  <dig> 
discotope  <dig> 
bpredictor

pearson
spearman
pearson
spearman
pearson
spearman


in the independent test data set,  <dig> epitope residues and  <dig> nonepitope residues exist on  <dig> protein antigens. a base predictor can classify a protein residue as epitopic or nonepitopic. for each of the  <dig> epitope residues, we counted the number of base tools that correctly classified the residue as epitopic. similarly, for each of the  <dig> nonepitope residues, we counted the number of base tools that correctly classified the residue as nonepitopic. figure  <dig> shows the distributions of epitope and nonepitope residues for the independent test proteins, based on the number of base tools with the same prediction, to indicate the degree of agreement in classification among the base tools. for example, in figure  <dig>  we observed  <dig> epitope residues, each of which was classified correctly by two of the base predictors. overall, we observed that none of the epitope residues were classified correctly by only one of the base tools, or escaped the detection of all of the base predictors, and 11% and  <dig> % of the epitope residues were classified correctly by seven and all of the base tools, respectively. by contrast, >85% of the epitope residues were classified correctly by between three and six base predictors. we observed similar trends for the nonepitope residues. these results indicate that base learners do not always agree when predicting epitopes, and may have complementary strengths, suggesting that a meta learner built upon these learners can demonstrate synergy in their predictive capabilities.figure  <dig> 
pie charts showing the degree of agreement among the base tools for epitope prediction.  distribution of the counts of epitope residues based on the number of base tools with the same prediction, and  distribution of the counts of nonepitope residues based on the number of base tools with the same classification.



performances of meta classifiers and base learners
the multilevel architecture for stacked or cascade generalization can vary with the arrangement of the meta learners in the hierarchy. for example, we can place svm  <cit>  at the top level in a stacked generalization architecture, or we can substitute c <dig>   <cit>  for svm. for cascade generalization, we can place the k-nearest-neighbor   <cit>  prior to the ann  <cit> , or vice versa, in the cascading sequence. we conducted two stratified five-fold cross-validations  to evaluate the performances of different architectures. we randomly divided a data set of  <dig> antigens into five disjoint folds , each of approximately equal size. we stratified the folds to maintain the same distribution of epitopes and nonepitopes as in the original data set. we used one fold of data for testing prediction performance, and used the remaining four folds for training. we repeated the same training–testing process on each fold iteratively. each run produced a result based on the fold selected for testing. the overall performance was used as the average of the results obtained from all iterations of the two 5-fold cvs.

first, we tested c <dig> , k-nn , ann, and svm in a two-level  stacking architecture . table  <dig> shows the average performances of the two 5-fold cvs. the results indicated that svm was the best-performing meta learner when compared with c <dig> , k-nn, and ann. therefore, to build a three-level  stacked generalization architecture we placed svm above the other three classifiers to arbitrate their predictions. figure  <dig> shows the three-level stacked architecture. cascade generalization performs a sequential composition of meta learners in a hierarchy in which only one meta learner exists at each level. we tested all  <dig> possible sequential arrangements of the meta learners svm, c <dig> , k-nn, and ann by using cv. figure  <dig> shows the best-performing cascade generalization architecture.figure  <dig> 
two-level stacking architecture. the conformational epitope predictors and linear epitope predictors were all placed at level  <dig>  one of the learners svm, c <dig> , k-nn, or ann served as a meta learner to integrate the output from the base predictors, and produced the meta classification as the final result.
five-fold cross-validations of meta classifiers



classifier
tpr
fpr
precision
accuracy
f-score
mcc
auc
2-level a
2-level a
2-level a
2-level a
3-level stackingb
cascadec

atwo-level stacking meta classifiers with ann, c <dig> , k-nn, or svm as the top-level meta learner.


bthree-level stacking meta classifier .


ccascade meta classifier .


dpaired t test showed no significant difference.
three-level stacking architecture. the conformational epitope predictors and linear epitope predictors were all placed at level  <dig>  we selected c <dig> , k-nn, and ann as the level- <dig> meta learners that transformed the output of the base predictors into meta features, and passed them to the successive level. we designated svm as the top meta learner that learned from the base features and the meta features to produce the meta classification as the final result.
cascade generalization architecture. the conformational epitope predictors and linear epitope predictors all served at level  <dig> as the base predictors. we placed k-nn, c <dig> , ann, and svm sequentially from levels  <dig> to  <dig> as meta learners. each meta learner generalized the output from the previous level to meta knowledge in the form of meta features. the meta features and base features propagated sequentially to the successive level as input to the subsequent meta learner. the top-level meta learner, svm, produced the final meta classification.



table  <dig> shows the average results of the two 5-fold cvs for the three-level stacked and cascade generalizations. table  <dig> presents the performances of each base learner based on the same cv. we optimized all of the parameters of the base predictors or meta learners by using a systematic search  within a range of parameter values in the cvs. we selected the optimum parameter values and used them in subsequent independent tests and ablation studies. table  <dig> lists the parameter values for the base epitope predictors. tables  <dig> and  <dig> show that stacking and cascade markedly outperformed all of the base prediction tools for accuracy, f-score, matthews correlation coefficient , and area under the curve . the differences among the meta-learning models were nonsignificant in a paired t test. these results demonstrate the advantages of exploiting the complementary capabilities of the base prediction tools.table  <dig> 
five-fold cross-validations of base epitope predictors



classifier
tpr
fpr
precision
accuracy
f-score
mcc
auc



base learner
parameter
range of parameter values
selected value


subsequently, we conducted an independent test using the test data set of  <dig> antigens that were not used previously to train the base learners, and a training data set of  <dig> antigens that were known from the literature or the websites to train the base learners. to ensure a fair comparison, we used the training data to train a meta classifier, and compared its performance with the performances of the base learners for the same test data. this independent test provided consistent and unbiased comparisons among the proposed meta-learning approach and the eight base learners. we also included two recent epitope prediction methods, cbtope  <cit> , lbtope  <cit> , for comparison. we selected the parameter values of their best-performing models for the training data set, respectively, and used them in the independent test to ensure a fair comparison. table  <dig> shows the results, which indicate marked differences in accuracy, f-score, mcc, and auc among the meta models and the base learners. figure  <dig> shows the roc curves. the roc curves indicate the trade-off between the amounts of true positives  and false positives  produced by the classifiers.table  <dig> 
results of the independent test data



classifier
tpr
fpr
precision
accuracy
f-score
mcc
auc
cbtopea
lbtopeb
epmetac

awe selected the parameter value  of the best-performing cbtope on the training data set of  <dig> antigens, and used the value in the independent test. cbtope’s performances were markedly lower for acc, f-score, and mcc , using the default value .


bwe selected the parameter value  of the best-performing lbtope on the training data set of  <dig> antigens, and used the value in the independent test. by contrast, lbtope’s performances were markedly higher for acc, f-score, and mcc , using the default value .


cwe selected the parameter value  of the best-performing epmeta on the training data set of  <dig> antigens, and used the value in the independent test. epmeta did not provide the default parameter value.
the roc curves of epitope predictors and meta classifiers based on the independent test data. the curves show the amounts of tp and fp produced by the classifiers in different parameter settings. the results show that both stacking and cascade outperformed all other epitope prediction tools, including the meta server, epmeta, in the independent test.



from the results of the 5-fold cvs and the independent test, we observed that most of the base tools produced high true positive rates of prediction, nevertheless they also suffered high false positive rates. in contrast to most of the base tools, the proposed meta-learning approach  showed lower false positive rates. although bpredictor demonstrated the lowest false positive rate in the independent test, unfortunately its true positive rate was also the lowest. among the eight base prediction tools, seppa  <dig>  obtained the best balance between true and false positive rates as indicated by the highest f-score, mcc, and auc. when comparing seppa  <dig>  with stacking and cascade, we observed that both stacking and cascade outperformed seppa  <dig>  for all the performance measures except the true positive rate in the independent test. overall, these observations suggest that the performance of an ensemble approach based on meta learning is superior to that of a single prediction tool for b-cell epitope prediction.

the applicability of an epitope predictor is limited by the protein properties it explores, and constrained by the search strategies it adopts. one method for achieving improvement is combining the results from an ensemble of various predictors. different ensemble approaches are distinguished by the manner in which they integrate the results from a set of predictors with different characteristics. we compared the proposed meta-learning approach with a meta server, epmeta, which incorporates the consensus results from multiple discontinuous epitope predictors by using a multistage voting scheme  <cit> . although epmeta and the other base learners performed comparably, both the proposed stacking and cascade meta classifiers markedly outperformed epmeta for f-score, mcc, and auc.

in addition to the comparison between the meta classifiers and the base epitope predictors for the same independent test data set of  <dig> antigens, we also compared the meta classifiers with the epitope predictors separately, using different data sets. we conducted the experiments on several representative epitope predictors released in 2008–2014: seppa  <dig>  , discotope  <dig>  , bpredictor , cbtope , and ellipro . each of them had been trained and tested by different data sets . in each experiment, we selected one epitope predictor for comparison. if it was one of the eight base learners, we built the meta classifier upon the remaining seven base learners. we only trained and tested the meta classifier on the same data sets that had been used specifically to train and test the predictor selected for comparison. other predictors, such as abcpred, bcpreds, and lbtope, though their data sets were available, were excluded from the experiments because their data sets either contained only sequence segments, or lacked the information of pdb files required to calculate the structure-based feature values, such as asa, for the meta classifiers. table  <dig> shows that stacking and cascade outperformed seppa  <dig> , discotope  <dig> , bpredictor, cbtope, and ellipro markedly in the individual tests. the results demonstrate that the synergy in the effects of multiple epitope predictors can achieve superior performance compared with that produced by a single epitope predictor.table  <dig> 
results of individual comparisons



classifier
tpr
fpr
precision
accuracy
f-score
mcc
auc
cbtoped
3-level stackingd
cascaded

ameta classifiers were trained and tested using the data sets that were used specifically to train and test seppa  <dig>  , excluding the antigens with missing feature values. all the classifiers, including the base predictor , were tested on the same test data to conduct a consistent comparison.


bmeta classifiers were trained on the data used specifically to train bpredictor, excluding the antigens with missing feature values. though bpredictor provided the test data set of its own, the data lacked the epitope residues annotated in the iedb. alternatively, we used the independent test data set of  <dig> antigens  to test all the classifiers, including bpredictor, to conduct a consistent comparison.


cellipro only provided the test data set, but no training data. meta classifiers were consequently trained on the training data set of  <dig> antigens , and tested on the test data of ellipro, excluding the antigens with missing feature values. all the classifiers, including ellipro, were tested on the same test data to conduct a consistent comparison.


dmeta classifiers were trained and tested using the non-redundant  benchmark dataset previously used to evaluate cbtope, excluding the antigens with missing feature values. following cbtope, we adopted 5-fold cv to compare the performances. all the classifiers, including cbtope, were tested on the same test data to conduct a consistent comparison. the parameter value  we used for cbtope was the same as used previously to evaluate cbtope in  <cit> .



ablation analysis
in a meta-learning architecture, base learners interact and contribute differently to the meta decision. an ablation study provides insight into the effects of base learners on the prediction performance of a meta classifier. however, the time required for a complete ablation analysis increases exponentially with the number of base learners. to avoid computational explosion, we adopted a greedy approach for the ablation study. although the greedy ablation analysis does not consider all possible combinations of base learners in a meta-learning architecture, it provides a significant estimate for the base learners in meta classification.

we used conformational  and linear  epitope predictors as the base learners for our meta classifiers. we conducted three ablation analyses for the stacking and cascade meta classifiers. the first analysis investigated a tool-based meta classifier and a feature-based meta classifier. the tool-based meta classifier only used the eight epitope prediction tools as the base learners in the meta learning hierarchy, whereas the feature-based meta classifier only adopted the structure-related features, such as secondary structures  <cit> , and the features that were relevant to the protein sequences, such as amino acid hydrophilicity  <cit> . tables  <dig> and  <dig> show the performances of the two types of meta classifier. the results indicated a marked reduction in prediction performance for f-score, mcc, and auc when the meta classifier used only the base tools, or only the base features. prior to ablation, the performances of the stacking meta classifier for f-score, mcc, and auc were  <dig> ,  <dig> , and  <dig> , respectively. after ablation, they reduced to  <dig> ,  <dig> , and  <dig> , respectively, for the tool-based stacking meta classifier, and to  <dig> ,  <dig> , and  <dig> , respectively, for the feature-based stacking meta classifier. we observed similar trends for the cascade meta classifier. collectively, the base predictors and features exerted great influence on the meta classification, as suggested by the result that the removal of the predictors or the features induced a substantial reduction in prediction performance. in addition, we observed approximately equal amount of reduction in performance after the removal of the base tools or the base features, which indicated the comparable influence of the predictors and the features on the learned meta model. extending the input space for the meta learners with both the predictions of the base tools and the protein features improved the prediction of epitopes . overall, these results demonstrate synergy in the effects of the conformational and linear epitope predictors, and protein features in meta learning.table  <dig> 
ablation analysis of 3-level stacking meta classifiers



classifier
tpr
fpr
precision
accuracy
f-score
mcc
auc



classifier
tpr
fpr
precision
accuracy
f-score
mcc
auc


the second analysis focused on the interactions of the eight epitope predictors, and their individual influence on the meta decision. we applied a greedy iterative backward elimination approach for ablation analysis to avoid exponential computational time, using mcc as the performance measure. we adopted mcc in the ablation study because it considers all four numbers , and provides a more balanced evaluation of prediction than some measures, such as tpr or precision  <cit> . in each iteration, we removed the base learner from the meta classifier if its removal caused a maximal decrease in mcc. table  <dig> shows the order of the base learners removed sequentially from the stacking meta classifier. seppa  <dig> , ellipro, and bepipred were the first three base tools to be removed from the stacking meta classifier. this observation agreed with our expectation that their removal would induce a maximal reduction in performance because seppa  <dig> , ellipro, and bepipred were the top three base predictors in the independent test according to mcc . the rest of the order varied because of the removal of the base tools. the remaining two conformational predictors, bpredictor and discotope  <dig> , were the sixth and seventh to be removed after the selection of bcpreds and aap for removal. the final base tool to be removed was the linear epitope predictor abcpred.table  <dig> 
ablation analysis of base learner interactions in stacking meta classifiers



classifier
*
tpr
fpr
precision
accuracy
f-score
mcc
auc

*classifiers tested in the ablation analysis. the first classifier in the first row is the stacking meta classifier that employs all of the  <dig> base learners . the remaining classifiers are listed in the order in which they were selected to be removed iteratively from the stacking meta classifier for the ablation study. ‘\’ indicates “removed”. for example, the second classifier is the stacking meta classifier after seppa  <dig>  was removed, and the third classifier is the stacking meta classifier after seppa  <dig>  and ellipro were removed from the meta model. the meta classifier in the final row did not apply any base learner after the final prediction tool abcpred was removed.



in contrast to the results of the stacking meta classifier, although the linear epitope predictor aap did not perform the best for mcc in the independent test, it was the first to be removed from the cascade meta classifier . seppa  <dig>  outperformed the other base predictors in the independent test, however it was the fourth to be removed after aap, ellipro and bpredictor had been selected for removal. prior to the remaining conformational prediction tool discotope  <dig> , we removed the linear epitope predictor bcpreds, and the rest of the linear epitope predictors, bepipred and abcpred, were the last two base tools to be removed.table  <dig> 
ablation analysis of base learner interactions in cascade meta classifiers



classifier
*
tpr
fpr
precision
accuracy
f-score
mcc
auc

*classifiers tested in the ablation analysis. the first classifier in the first row is the cascade meta classifier that employs all of the  <dig> base learners . the remaining classifiers are listed in the order in which they were selected to be removed iteratively from the cascade meta classifier for the ablation study. ‘\’ indicates “removed”. for example, the second classifier is the cascade meta classifier after aap was removed, and the third classifier is the cascade meta classifier after aap and ellipro were removed from the meta model. the meta classifier in the final row did not apply any base learner after the final prediction tool abcpred was removed.



the third ablation analysis evaluated the individual influence of the linear base prediction tools on the meta classifiers. we applied a greedy iterative forward selection approach starting with a meta classifier built upon all the conformational base learners. in each iteration, we added one linear base learner if its addition caused a maximal increase in mcc. tables  <dig> and  <dig> show the order of the linear tools added sequentially to the stacking and the cascade meta classifiers, respectively. in both analyses, bcpreds was the first selected linear base tool added to the meta classifiers. though it was not the top linear tool in the 5-fold cvs and the independent test according to mcc, it had the highest tpr compared with the other linear tools. the addition of bcpreds to the conformational meta classifiers increased the tprs markedly for stacking and cascade, from  <dig>  and  <dig>  to  <dig>  and  <dig> , respectively. in addition, after the inclusion of bcpreds, both stacking and cascade achieved their maximal mccs,  <dig>  and  <dig> , respectively. though the rest of the order varied between stacking and cascade because of the addition of other linear base learners, the performances of the meta classifiers for mcc remained above   <dig> .table  <dig> 
ablation analysis of influence of linear base learners on stacking meta classifiers



classifier
*
tpr
fpr
precision
accuracy
f-score
mcc
auc

*classifiers tested in the ablation analysis. the first classifier in the first row is the stacking meta classifier that only employs the  <dig> conformational base learners. the remaining classifiers are listed in the order in which they were selected to be added iteratively to the stacking meta classifier for the ablation study. ‘+’ indicates “added.” for example, the second classifier is the stacking meta classifier after bcpreds was added, and the third classifier is the stacking meta classifier after bcpreds and aap were added to the meta model. the meta classifier in the final row applied all the base learners after the final prediction tool abcpred was added.
ablation analysis of influence of linear base learners on cascade meta classifiers



classifier
*
tpr
fpr
precision
accuracy
f-score
mcc
auc

*classifiers tested in the ablation analysis. the first classifier in the first row is the cascade meta classifier that only employs the  <dig> conformational base learners. the remaining classifiers are listed in the order in which they were selected to be added iteratively to the cascade meta classifier for the ablation study. ‘+’ indicates “added”. for example, the second classifier is the cascade meta classifier after bcpreds was added, and the third classifier is the cascade meta classifier after bcpreds and abcpred were added to the meta model. the meta classifier in the final row applied all the base learners after the final prediction tool bepipred was added.



these observations together suggest that in the meta-learning framework, all conformational and linear epitope predictors interact, and the degree of interaction among them differs. an ensemble of complementary base learners incorporated in a hierarchy of appropriately arranged meta learners can produce a meta classification performance that is comparable to, or superior to, the performances of the base predictors.

CONCLUSIONS
understanding of the interactions between antibodies and epitopes provides the basis for the rational design of preventive vaccines. following the increased availability of protein sequences and structures, several various computational tools have been developed for epitope prediction. our analytical and experimental results reveal the complementary performances of various epitope prediction methods, suggesting synergy among these computational tools. unlike previous ensemble approaches, we propose a meta-learning approach for predicting b-cell epitopes, which combines base epitope prediction tools with other meta learners in a hierarchical architecture to integrate multiple predictions into meta knowledge for epitope classification. we conducted a consistent and unbiased independent test on our method, and compared the results with those from other prediction tools. our results demonstrate that the proposed meta-learning approach outperforms the single base tools and other recently developed epitope predictors.

