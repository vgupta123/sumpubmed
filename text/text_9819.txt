BACKGROUND
systems biological research is frequently carried out within collaborations connecting multiple labs each conducting a specific type of experimental work. the ultimate goal of these research collaborations is the integrated analysis of the data generated within the consortium. data integration involves the storage and cross-linking of initially independent and heterogeneous data sets. this allows for the simultaneous analysis of data sets and therefore enhances the overall functional interpretation, which provides additional information compared to the sequential analysis of single data sets  <cit> . an important prerequisite for data integration is the standardization of storage and exchange formats, both within data domains  and across different data domains , since such data typically show a lack of coherence  <cit> .

in this article we describe a data integration platform that provides a flexible representation of collaborative data based on xml. it is designed for research collaborations, typically involving heterogeneous 'omics' data along with functional data from validation experiments, genetic and phenotypic data. the introduction of new data types or the modification of existing data types can be easily accomplished, thus providing high format extensibility. this data representation approach takes advantage of a growing number of xml data formats in biotechnology  <cit> .

the system is built upon three components: a) the web-server , providing a convenient user interface; b) the search index , which can be accessed through the user interface, providing a fast full-text search engine; and c) helper applications , providing interactive, data specific analysis functionality.

all components of the system are platform-independent, open-source developments, and thus can be easily adopted by researchers. an example installation of the collaboration platform with proto-typical public data sets is provided at http://dipsbc.molgen.mpg.de.

implementation
general functionality
the functionality of the data integration system is realized by a combination of four components: xml, solr/lucene, foswiki, and java applets. in the following we describe the implementation and interplay of these components.

integration of existing and user-defined xml formats
in recent years, several initiatives have specified and developed xml-based representations of primary data in domains such as proteomics, genomics, molecular interactions, cellular assays and mathematical models, amongst others  <cit> . xml features high format extensibility and can be used to represent virtually any kind of data structure, thus making it easy to integrate new data types and, importantly, modify existing ones. in addition, many tools in bioinformatics and systems biology use xml as their default exchange format. besides the formats for primary data, xml offers the possibility of defining a structured representation of data analysis results. these results are typically adapted to a particular data analysis workflow and are highly user-specific, thus requiring a flexible data integration solution. an example for such a format is the xml schema that we defined for study results . because of the advantages given above, dipsbc uses xml for data representation, and a list of supported formats is given in table  <dig> 

the example installation of dipsbc incorporates many of these data and allows integrated indexing of primary and secondary data.

data normalization and indexing
in a first data integration step in dipsbc, all experimental data are converted to xml. the conversion can be done either by publicly available or by custom parsers. we use xml schema definition  in order to syntactically define the structure of the xml files and to ensure their data integrity. if available, community compliant xsds like mzdata , mage-ml , or psi-mi  are used. this is an ultimate benefit since it ensures wide acceptance and compatibility of the data formats. for more specific data sets that lack community standards, custom schemas can be easily developed.

in order to add the data to the index and make them available for searching, all xml files are normalized, i.e. generalized versions of the files are created. the normalized files contain only the search-relevant content, stored in unified variables . the normalization is done with xslt , and the output can be directly fed into the solr search index .

the search index can be created, updated, optimized, or queried through http calls over the network. to increase the overall performance, the index server and the foswiki web server can be installed on different computers. additionally, solr offers advanced query syntax and fast search routines. usually response times remain below one second when querying the dipsbc index, which contains about  <dig> million records . moreover, solr/lucene supports detailed configuration capabilities including custom document scoring functionality. as an example, we make use of this functionality when indexing gene expression results by using the fold-change and p-value of each gene as index score boost coefficients. thus, the more differentially expressed a gene was in a study, the higher rank it will reach in the index result page.

the index contains a large collection of different data types, including protein mass spectra, dna microarray experiments, molecular interactions, protein sequences and pubmed abstracts, amongst others. in total about  <dig> million records are indexed and thus searchable.

besides possible boost values, the search result page orders the documents by contextual relevance. the sources file types  and additional links to further information are shown . depending on the file type, either a report page or specific helper applications can be opened allowing a closer inspection and analysis of the data set. consequently, all data sets can be searched simultaneously and be evaluated in context.

foswiki collaboration platform and incorporation of helper applications
dipsbc uses the foswiki content management software as a browser-independent user interface due to its advanced features for managing collaborations. for example, users can create or edit web-pages within their browser and directly upload and share data. all modifications made to the website or attached files are tracked by a built-in revision control system. therefore, different document versions can be compared; moreover, e-mail notifications that automatically inform users about document changes can be enabled. additionally, the foswiki technology provides a fine-grained user management system, which can be used to define rights for viewing or editing web pages for different users or user groups. for more general data privacy, password-protection or ip-range checks at the web server level can be applied as well.

an important feature of the proposed platform is the possibility of a straightforward incorporation of helper applications associated with the different data types. for this purpose we take advantage of the foswiki plug-in interface to integrate specialized programs as java applets, resulting in minimal installation efforts on the client side, as the applets are automatically started within the user's web browser. currently the system includes three different applets: the argo genome browser  <cit>  and two custom developed applets: an mzdata viewer, which provides a graphical representation of peptide spectra, and a graph browser, which reads molecular interaction data stored in psi-mi files and dynamically visualizes the underlying protein-protein interaction networks .

RESULTS
here, we illustrate the usage of the system with several archetypical use cases that incorporate different levels of integrated primary data.

integration of experimental results from proteomic and transcriptomic data
nowadays large-scale profiling on mrna and proteome levels has become routine and increasing numbers of large-scale data sets have become available. a combination of these different experimental approaches will help to gain a more comprehensive view of biological processes and molecular networks  <cit> . observing evidence of genes  in different heterogeneous data sets might lead to better disease markers. data integration systems give a first glance in searching through these data sets. as an example, we used our data integration system to screen a prostate cancer gene expression study together with a mass spectrometry study of the human plasma proteome project ii. in general, plasma proteome data sets could be used to identify biomarkers for certain disease states, as proteins up-regulated in diseased tissues may enter the blood stream in higher concentrations than usually  <cit> . outside of the platform, we analyzed mrna expression differences between primary and metastatic prostate cancer cells of the transcriptomic study  <cit> , geo accession gse <dig> statistically with r and identified  <dig>  differentially expressed genes .

we then added these study results to the solr index. an overview of the results, including download links to the test result tables, can be found by entering 'vindex:studies study 4' in the index search field. likewise, genes differentially expressed in the study can most easily be found by entering 'study 4' in the index search field. because the genes' search score is boosted according to the respective log2ratio and p-value, the most significant genes will be listed on top of the result page.

several of the differentially expressed genes are known to play a critical role in prostate cancer, such as apc <cit> , mapk <dig> <cit> , or zeb <dig> <cit> . therefore, we correlated these genes with their identification in the human plasma proteome by querying the index for significant genes and checking the result list for mass spectrometry hits. in the case of the above mentioned three examples, these are found in the blood plasma sample as well. clicking on the result link of a mass spectrum opens the 'mzdata viewer' applet, which can be used to view and zoom into the spectrum and display associated annotation . additionally, protein identifications can be re-analyzed by uploading the respective xml file containing the spectrum's peak list to the public mascot  <cit>  web search interface.

characterization of candidate genes for an animal genome with sparsely known functional information
many research projects in functional genomics are focusing on organisms whose genomes are still partly unknown. as a use case we investigated how functional information for porcine genes can be extrapolated with the architecture. the pig genome was only partly sequenced until recently, which made it difficult to identify genes which influence specific traits and the same holds still true for many other animal genomes. therefore homology is a valuable concept which can help to find out more about possible functions of yet poorly characterized pig genes  <cit> . for example, based on an f <dig> resource population consisting of  <dig> animals, a qtl for the phenotypic trait ‘drip loss’ was identified on chromosome  <dig>  <cit>  with the respective orthologous region in human located on chromosome  <dig>  <cit> . by alignment of porcine affymetrix probes to the human genome we identified  <dig> pig genes that match to this region  <cit> . we carried out a gene expression analysis with animals of different genotypes with respect to this qtl and stored the statistical results in the dipsbc index. as an example, we explored the role of one of these genes , which was found to vary significantly across different genotypes . because the respective genomic region had been sequenced already, the 'argo genome browser' applet could be used to inspect this region indicating that the gene lies within the mentioned qtl on porcine chromosome  <dig> . most importantly, human exons that match the pig gene have also been incorporated in the system. these exons belong to the human gene tomm <dig> which is a central receptor component of the mitochondrial translocase . to reveal molecular interactions of this gene, we searched the index and used the link to the consensuspathdb  <cit> . the resulting network shows a highly conserved interaction of tomm <dig> with vdac <dig> . this protein forms a channel through the mitochondrial and plasma membrane, respectively, and is involved in molecule diffusion and cell volume regulation. therefore, it probably also influences the specific trait under analysis and provides an interesting candidate for further validation.

querying protein-protein interactions based on two different network datasets
we integrated two datasets of human protein-protein interactions , both constituting a representative part of the human interactome  <cit> . in each case, the authors performed systematic yeast two-hybrid  screens that resulted in  <dig>  and  <dig>  interactions, respectively. the datasets are publicly available and we downloaded them as psi-mi xml files from the intact database  <cit> . after normalization and indexing, an index query for a protein of interest lists all its interactions existent in the two datasets. by clicking on one of the interactions, the 'graph browser' applet is launched, which parses the underlying original xml file and visualizes a sub-network around the focus protein. in case the queried protein is available in both interaction datasets, two sub-networks are generated side-by-side. this allows users to compare the two network topologies and to detect differences or overlaps. as an example, the gene pin <dig>  which is known to play a critical role in prostate cancer  <cit> , has no overlapping direct neighbors in the two networks . therefore the two graphs complement each other and by the analysis of both networks in parallel, additional interactions can be found. furthermore, network visualizations can be inspected in greater detail and modified dynamically. nodes and edges can be moved, added, and hidden. additionally, each node can be expanded or collapsed, provided it is connected to other nodes; the radius of nodes shown in relation to the central protein can be defined as well. by clicking on a node, the solr index is queried for the respective protein and the results are shown at the right of the application window.

linking genes to interaction networks and computational models
systems biology studies specifically aim at interpreting biological data at the network level. thus, a data integration system should be able to cross-reference primary data with interaction resources and computational models. this is demonstrated with the apc gene found significantly differentially expressed in the prostate cancer study mentioned in the first use case  <cit> . the index search results for the apc gene are displayed in figure 7a; these results cover primary data records such as mass spectra, data analysis results from microarray studies, sequencing results as well as information on associated networks. as far as the latter are concerned, we indexed the biomodels database  <cit>  and the consensuspathdb  <cit> . indexing has been done with sbml documents in the former and with a specific xml format in the latter case . the first sbml hit of the apc index search directs the user to a computational model of wnt/erk signalling  <cit>  stored in biomodels. the respective consensuspathdb entry links the user to the list of known molecular interactions of apc recorded in the database. using the visualization function of consensuspathdb the interaction neighborhood of apc is displayed . thus, the architecture can be used to directly link primary data with biological networks.

comparison to related data integration systems
there are several related software tools for the integration of heterogeneous genome data, such as the isa infrastructure  <cit>  and the biomart system  <cit> .

isa infrastructure consists of several java desktop applications and a relational database, built around the isa-tab format. amongst others, the system provides tools for metadata structure definition  and data input and processing by collaborating experimentalists . experiment metadata is stored in the generic isa-tab format and can be exported to xml-based, community compliant formats to meet the standards of public repositories like arrayexpress, pride or european nucleotide archive . the system is well suited for the production of standardized, richly annotated experimental data and its formal validation. however, in comparison to dipsbc, the system's data analysis and visualization options are rather small yet. also, it has a less strong focus on the collaboration platform as has been realized in dipsbc by incorporating the foswiki system and its features.

biomart is a data management system aiming at the integration of disparate, geographically distributed data sources. typically the latter are relational databases, each maintained independently and with its own data structure. biomart provides a consistent graphical user interface for the unified query of all contained sources. these can be filtered by different attributes, e.g. genomic region or gene ontology term. biomart is used by several large-scale research consortia, e.g. the international cancer genome consortium   <cit> . in general, the system is best suited for readily processed, i.e. finished data and its decentralized structure leads to a lightweight installation. however, it is less well suited to integrate complex, evolving data types that change frequently as is the case in particular at the start of new collaborative projects. furthermore, it neither features a document sharing option nor the possibility of an index based full-text search.

overall, both isa infrastructure and biomart are systems that are well suited for rather large collaboration projects, at the cost of increased time consumption and man-power. in comparison, our system has the advantage of being very flexible and extensible. in contrast to systems based on relational databases, our xml index based platform offers a straightforward way of integrating data sets via common keywords, supported by a very fast full-text search.

discussion
we presented a data integration system that is utilizing and indexing xml-based data representation formats. thus, the basic unit of data stored within the dipsbc platform is 'xml document'. this unit is very generic and can range from genes and pathways to whole genome microarray experiment results, implicating a very high variability in data granularity. we use xml as central data format in order to capture this granularity and to make heterogeneous data compatible, a prerequisite for the coordinated integration of the various data sets.

as a result, the document management of our system is highly flexible, community compliant and well suited for data collaborations. on the one hand, the adoption of community standards enables cross-referencing proprietary data with publicly available data sets and applets for data visualization such as genome browser etc. as was demonstrated in the use cases. on the other hand, in particular with data types that are not yet standardized or that are so heterogeneous that they can not be standardized, for example the very specific data analysis results, the system offers full format flexibility and has basically no restrictions as was demonstrated by introducing a custom standard for data analysis results .

currently the procedure of adding new data to the system involves two steps: first, the member of the consortium who generated the data set  transfers it to the administrator. second, the administrator checks the data for integrity by xsd schema validation and then adds the normalized xml to the index. although this procedure ensures improved data integrity by manual curation, it would still be favourable to automate the procedure of xml transformation, validation, normalization and indexing, for example by implementing custom perl plugins. these plugins could provide data upload interfaces, enabling members of a given collaboration to directly add their experimental data to the system. a corresponding interface is currently under development and will be provided in a future version of dipsbc.

in the age of 'omics'-data, researchers are faced with ever growing data set sizes. while the proposed xml structure is feasible for most of the functional genomic data types, it can not be applied to high-throughput sequencing experiments. the usage of xml for the representation of such data might be counterproductive here, because xml is a human-readable format which adds lots of redundant text to the actual data. therefore, in practice we do not transform such data sets to xml, but rather create metadata xml files for the search index that store processed data. the raw files  are stored in the file system and are only referenced by the indexed metadata xml file.

one important issue within collaborative research groups is data security. experimentalists need to be able to maintain in control of their raw data and study results need to be dealt with confidentially before they are published in a research journal. this can best be accomplished by securing the system with password protection and possibly also ip range restrictions at the web server configuration.

also a more fine-grained user management can be realized by using the foswiki user group functionality. then, certain pages of the web site can be restricted to certain users or groups. additionally, this concept could easily be extended to the central solr index search so that particular search results would be restricted to specific users. for this purpose, the solr-search-plugin would need to read the current user id via the respective foswiki variable and then filter the index results according to the logged in user. an overview of corresponding current and planned developments can be found at the dipsbc homepage under the section 'roadmap'.

another advantage of the foswiki collaboration platform worth mentioning is its intuitive data exchange function. at each page, users can upload files by clicking the 'attach' button. other users can then download the respective files. this has two important advantages compared to data sharing via e-mail: first, files that are too large for e-mail transmission can be shared; second, the reference file is stored only once at a central location, and if the file is changed, it can be downloaded again from the same location.

an important part of the proposed data integration system is the incorporation of data analysis results that add additional value to the raw experimental data and aid in the interpretation of these data. currently, data analyses which lie beyond the capabilities of the java applets need to be generated outside of the platform . however, for future development steps it might be worth considering the integration of an r interface that could enable the direct statistical processing of experimental data.

our data integration system was already applied within several research projects, typically involving between  <dig> and  <dig> collaboration partners located at different sites. these small to medium sized projects likely represent the typical size for the majority of research projects. however, the system might as well be suited for larger collaborations, because the web server and foswiki collaboration platform can still handle a lot more simultaneous accessions than would be generated by tens or even hundreds of participating users. this is proved by the fact that many companies use foswiki as their intranet system, sometimes including thousands of web pages and high access rates.

as for scalability of the index machine, of course its search and index performance decreases with increasing numbers of stored documents. nevertheless, the solr/lucene software library is optimized for very fast text queries on large amounts of data. e.g., the current index size of our data integration system amounts to almost  <dig> million indexed documents or  <dig>  gb of physical storage, with pubmed and uniprot records representing the major part. while indices of smaller size typically can be queried within split seconds, query times of this rather large index lie in the range of below one second for general queries and up to a few seconds for very complex queries. therefore the system can be conveniently used to handle quite large amounts of documents. however, if larger index sizes are needed, as might be the case e.g. with meta-data of next-generation sequencing experiments, solr/lucene offers native support of distributed searches. for this purpose, a large index is split into several smaller indices on different machines, and thereby fast response times can be maintained.

all parts of the introduced system can be straightforwardly implemented. the basic system setup with the foswiki user interface and the solr backend can be achieved in less than one day by an experienced programmer. also, an important advantage of the system is the fact that its components are open source. therefore it can be modified and adjusted for specific functions.

because of its flexibility, the system can easily incorporate additional or new data types like patient data, high-throughput sequencing data, or any other data types that will occur during future developments of experimental techniques. adequate helper applications that make use of the underlying xml files can be developed or adapted efficiently in order to support the analysis of such new data. therefore, the combination of a fast indexing machine with a web-based collaboration platform makes this system highly flexible, evolvable, scalable and easy to use for research collaborations.

CONCLUSIONS
we developed dipsbc, a systems biology data integration platform that utilizes a large number of xml-based exchange formats and connects primary data with higher-level data. the combination of a fast indexing machine with an online content management platform makes this system highly flexible and easy to use for research collaborations. furthermore, the incorporation of helper applications is a powerful feature of the system, which distinguishes it from a mere data repository. since all parts of the platform are open source, it can easily be modified and adjusted for specific functions.

availability and requirements
project name: dipsbc

project home page:http://dipsbc.molgen.mpg.de

operating system: platform independent

programming language: perl, java

other requirements: perl  <dig>  or higher, java  <dig>  or higher, foswiki web server, solr/lucene index

license: gnu gpl

any restrictions to use by non-academics: none.

competing interests
none.

authors' contributions
fd and tk developed the system. ch, ry and ak adapted the system for specific collaboration projects. ks, hl and bml provided data sets for testing data integration and data collaboration. rh led the development and design of the system. fd and rh drafted the manuscript. all authors read and approved the final manuscript.

funding
this work was supported by the german research foundation , the bmbf under its ngfn-plus , ngfn-transfer  and medsys programs , and by the max planck society.

