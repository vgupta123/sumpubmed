BACKGROUND
speech perception is an intricate process exceptionally resilient to distortions of almost any kind, whether occurring in natural environments or caused by manipulation of particular stimulus features in laboratory conditions. this extraordinary robustness enables successful communication under acoustically adverse conditions. despite decades of research and development, attempts to create artificial speech recognition systems have demonstrated that the human brain is far superior to machines in interpreting speech signals in noisy or otherwise degraded conditions  <cit> . how the brain achieves this computational feat, however, has remained unsolved.

the basis of the robustness of human speech perception lies in the fact that there does not seem to be a single indispensable property within the acoustic signal upon which the entire recognition process relies. this view is supported by a wealth of behavioral research aimed at delineating the boundary conditions of speech intelligibility. drullman  <cit> , for example, demonstrated that speech perception does not depend on the extraction of fine structure cues present in the temporal envelope by showing that an intact temporal speech envelope with random fine structure retains perfect intelligibility. further, loizou et al.  <cit>  established that when there are enough spectral channels available, a relatively small spectral or temporal amplitude resolution is sufficient for successful recognition of speech. then again, also the spectral information in speech contains significant redundancies, as documented by shannon et al.  <cit> , who showed that speech recognition is possible even with very few spectral cues as long as temporal envelope cues are present in a few contiguous spectral regions. moreover, saberi and perrott  <cit>  demonstrated that continuous speech is highly resistant to time reversal of local segments up to a duration of  <dig> ms, which indicates that a detailed analysis of the short-term acoustic spectrum is not essential for speech comprehensibility.

in natural auditory environments, speech signals can be subjected to various kinds of external, "additive" distortions. these distortions can be caused, for example, by different types of environmental noise or informational masking by several concurrent speakers. although the amount and quality of temporal and spectral cues of speech can be reduced considerably without a significant decrease in recognition performance, the inclusion of external noise poses an additional challenge to the perceptual mechanisms  <cit> . based on behavioral results, it appears that the spectral structure of external masking noise has a profound effect on the efficiency of the noise as a speech masker. for example, studebaker et al.  <cit>  found that noise which was spectrally matched to the speaker's voice spectrum was a significantly more effective masker than uncorrelated white or high-pass noise.

the neural processing of intact as well as distorted speech has recently aroused interest within the cognitive neurosciences. given that both the perceptually significant alterations in acoustic speech signals and the ensuing cognitive processes in the brain take place on a timescale of milliseconds, the high temporal resolution of electro-  and magnetoencephalography  is ideal for measuring the rapid changes in brain activation during speech perception. the prominent n1m wave of the auditory evoked field , in particular, has been shown to exhibit sensitivity to a variety of acoustic attributes of the speech signal  <cit> .

a majority of earlier studies aimed at investigating cortical responses to degraded speech have used continuous stochastic noise to distort the speech stimuli. stochastic noise has a random frequency spectrum which does not correlate with the spectral structure of the speech signal. in these studies, the noise was presented continuously in the background, resulting in an elevated recognition threshold for the speech stimuli as the signal-to-noise ratio was decreased. using continuous low- and high-pass stochastic noise, martin et al.  <cit>  demonstrated that the n <dig>  n <dig>  and p <dig> responses elicited by speech sounds were delayed and reduced in amplitude as the bandwidth of the masking noise was increased. in a related study, whiting et al.  <cit>  showed that when speech sounds were distorted with broadband stochastic noise, the auditory evoked potential  amplitudes were attenuated and latencies increased as the signal-to-noise ratio was reduced. corroborating results were obtained by muller-gass et al.  <cit> , who showed that the aeps were decreased in amplitude and increased in latency as the intensity of the speech-masking noise was raised. furthermore, kozou et al.  <cit>  studied the effects of different background noise types on brain responses to speech and non-speech stimuli and discovered similar decrements in aep amplitudes with decreasing signal-to-noise ratios. while the various noise types yielded somewhat different results, their effect was qualitatively the same, i.e., a decrease in the amplitude of the brain response. further evidence that stochastic noise results in delayed and attenuated evoked brain responses to speech sounds was demonstrated also with noise that initiated one second prior to the speech stimulus instead of being continuous throughout the measurement  <cit> . similar increases in aep latencies with decreasing signal-to-noise ratio were found. the effect of noise on the aep amplitudes was less systematic, although some of the stimuli elicited a decreasing trend of the n <dig> amplitude with the reduction of the signal-to-noise ratio  <cit> . a number of meg studies have also revealed hemispheric asymmetries using white-noise maskers. for example, shtyrov et al.  <cit>  found that white noise depresses left-hemispheric aefs to speech sounds while right-hemispheric aefs remain constant or even increase in amplitude. taken together, most of the neurocognitive studies investigating the effects of external distortions of speech have employed only maskers whose spectra do not correlate with the intact speech spectrum. however, given that the strength of spectral correlation between the original signal and the masker considerably affects speech intelligibility on the perceptual level , it seems possible that this effect could be observed in brain dynamics as well.

in addition to external noise, speech intelligibility can be compromised by directly manipulating the acoustic structure of the speech signal. a recent study of ours  <cit>  demonstrated that degrading speech by using a method that results in spectrally correlated, signal-dependent distortion yields different effects on brain activation than those reported in previous studies using additive, uncorrelated stochastic masking. the signal-dependent distortion was generated with uniform scalar quantization , a method for degrading auditory signals based on time-domain sampling and representing the amplitude values of the temporal waveform on a finite scale . essentially, the quantization decreases the amplitude resolution of the signal waveform, yielding a significant reduction in speech intelligibility. the results showed that activation in the right-hemispheric auditory cortex as measured through the n1m amplitude increased when the amount of signal-dependent distortion was raised. furthermore, the depression of left-hemispheric aefs with increasing noise level reported by shtyrov et al.  <cit>  was not observed. these results, which are at odds with the findings discussed above  <cit> , are interesting considering that the distortion we used reduces the comprehensibility of speech very effectively as well. the diverging results are further accentuated by the fact that both quantization and uncorrelated white noise flatten the spectrum of speech by adding high frequencies to the original signal. moreover, both distortions increase the spectral bandwidth of spectrally simple, narrowband sounds. consequently, the most conspicuous spectral difference between quantized and white noise -masked sounds is that in the former the degradation process generates new, noisy spectral components that are located mainly at the harmonics of the original, intact sound spectrum. in other words, the spectral structure of the quantized sound, in comparison to its white-noise corrupted counterpart, correlates more closely with the spectrum of the original signal. as the differential results are presumably due to these distinctions between signal-dependent distortion and uncorrelated noise utilized in the studies, it seems that the effects of stimulus degradation on brain responses are highly dependent on the characteristics of the distortion.

one might speculate that the previously demonstrated increase in the n1m amplitude is related to processing of acoustic properties of speech in particular  <cit> . alternatively, because speech processing is classically thought of as being lateralized to the left hemisphere, the observed right-hemispheric sensitivity could be related to processing of basic acoustic features of all sounds instead of speech-specific processes. given that the quantization procedure substantially modifies the spectral structure of speech, the latter interpretation gains support from studies showing that the right-hemispheric auditory cortex is more sensitive to spectral variation of sounds than the left  <cit> . in the current study, our aim was to determine whether the increase of activation in the right-hemispheric auditory cortex caused by the signal-dependent distortion can be observed also with spectrally simpler non-speech sounds. in order to link brain measures with behavioral data, an identification experiment was conducted to find out whether the non-speech sounds are perceptually separable from the speech sounds when the amount of degradation is increased.

methods
participants
sixteen right-handed volunteers  took part in the study with informed consent. all were native finnish speakers with normal hearing. the experiment was approved by the ethical committee of the helsinki university central hospital.

stimuli
the stimuli comprised the finnish vowel /a/, a complex non-speech sound, and a sine-wave tone. the vowel was created by using the semi-synthetic speech generation method, which enables the production of fully controlled natural-sounding speech stimuli  <cit> . firstly, a waveform of the vowel /a/ was created from a natural utterance of a finnish male speaker  by estimating the glottal excitation pulseform produced by the vibrating vocal folds. secondly, this waveform was used as input to an artificial vocal tract modeled by a digital all-pole filter. the complex non-speech sound was a composite of sinusoids created by combining five sine-wave tones whose frequencies were matched to the major spectral harmonics of the vowel /a/: the first frequency  was adjusted to be equal to the fundamental of the speech spectrum, the other four tone frequencies  were selected to coincide with the strongest harmonics in the vicinity of the lowest four formants of the vowel spectrum. the frequency of the simple sine-wave tone  coincided with the strongest individual harmonic in the entire vowel spectrum, that is, the fifth harmonic in the vicinity of the first formant.

in order to degrade the synthesized sounds in a controlled manner, each signal was modified with uniform scalar quantization. degradation was implemented stepwise by reducing the number of bits in the sample quantization from  <dig> to  <dig> to  <dig>  the differences between the 16-bit and 1-bit temporal waveforms and power spectra of the stimuli are shown in fig.  <dig>  a 16-bit signal, having  <dig> levels to quantify sound amplitude, is perceptually equivalent to an analog signal of the same bandwidth. decreasing the bit mode reduces the amplitude resolution of the temporal waveform, and this is manifested in the frequency domain as an increase in quantization noise . the generation of distortion by the usq method is, importantly, very different from that based on merely adding  noise to an undistorted speech signal. namely, in quantizing the original sound sample, the usq method acts as a non-linear process. hence, as described by the theory of signals and systems  <cit> , it creates new frequency components into the input signal, a feature which is not possible with linear filtering. in addition, since the usq method reduces the amplitude resolution of the input signal, the flatness of the output spectrum will be increased. taken together, these two factors imply that if the input signal comprises only a single frequency component, the output spectrum will be flat but will consist of noisy harmonics of the input frequency . if the input has several harmonics, as in the case of a tone composite, each component will generate new harmonics in the usq process, and the output spectrum will have a flat overall trend with a dense structure of noisy harmonics . in the case that the input signal is a vowel comprising a large number of harmonics over a wide frequency range, the output spectrum will again be flat comprising noisy harmonics that are located at the multiples of the f <dig> of the input . we note that the spectral structure of the degraded vowel  resembles that of the undistorted speech sound  whereas a much larger relative spectral change is caused by signal degradation in the case of, for example, sinusoidal composites . in summary, by utilizing three types of periodic input signals , all of which are spectrally unique but share the feature of including spectral harmonics, the usq process can be used to study processing of sound degradation which is signal-dependent.

all the stimuli were  <dig> ms in duration and matched in rise/fall times . the stimuli were presented binaurally through a pair of plastic tubes and ear pieces. sound intensity was measured at the ear pieces using a sound level meter  and this value was adjusted to  <dig> db for each stimulus. this intensity calibration was conducted using the standardized a-frequency weighting together with standardized f-time weighting in which the spl is integrated as the root mean square  value within a  <dig> ms time window. the stimuli were arranged in separate sequences presented in a randomized order counterbalanced across the participants. the onset-to-onset inter-stimulus interval was  <dig> ms.

meg registrations
auditory evoked fields  were recorded in a magnetically shielded room with a 306-channel whole-head meg device . cortical activation was sampled at  <dig>  khz and band-pass filtered at  <dig> - <dig> hz before online averaging in a 600-ms window including a 100-ms pre-stimulus baseline. epochs in which the magnetic signal exceeded an absolute amplitude variation of  <dig> ft/cm were discarded online. eye-movement artefacts were monitored by two electrode pairs and automatically excluded during the registration . a minimum of  <dig> artefact-free epochs were sampled for each stimulus. before the acquisition, the locations of the head-position indicator coils were determined using a three-dimensional digitizer. the head-based coordinate system was defined by the x-axis passing through the preauricular points , the y-axis passing through the nasion, and the z-axis as the vector cross product of the x- and y- unit vectors. the head-position indicator coil locations were determined before each stimulus block. during the acquisition, the participant was seated in a reclining chair and was under instruction not to pay attention to the auditory stimuli and to watch a self-selected silent movie.

meg data analysis
the auditory n1m was studied for effects in response amplitude, latency and source location. a 100-ms pre-stimulus baseline and a filter passband of 1- <dig> hz were used in calculating the aef gradients. to quantify the cortical activity in the left and right hemispheres, unrestricted equivalent current dipoles  were determined using a set of  <dig> planar gradiometers over each temporal region. the ecds were fitted to a single time point defined as the maximum dipole moment in the interval 85- <dig> ms. the left-hemispheric data of five subjects were discarded from the ecd analysis due to poor signal-to-noise ratio, with goodness-of-fit values <60% or with anomalous generator locations. the average goodness-of-fit values of the ecds accepted for further analyses was  <dig> %.

in addition to ecd analyses, the peak amplitudes and latencies of the p1m, n1m and p2m waves of the aef were quantified from the pair of gradiometer channels exhibiting maximal vector sum amplitudes. this analysis, carried out separately for each hemisphere, allowed us to investigate the effects of stimulus distortion specifically on the p1m and p2m responses, for which ecd source modeling could not be reliably performed.

behavioral experiment
we tested the subject's ability to identify the usq-degraded stimuli as either a particular vowel or as a non-vowel. the stimuli consisted of five finnish vowels  , created with the technique described in section  <dig> , as well as sine-wave composites and sine-wave tones, each quantized to 16-bit, 4-bit and 1-bit modes. in a forced-choice task, the subject was instructed to respond to each stimulus with a keyboard stroke. the response alternatives comprised all eight of the finnish vowels and a "not-a-vowel" category. since the 16-bit mode is perceptually equivalent to an analog signal, it was expected that all of the 16-bit stimuli would be easily identifiable. the stimuli were presented through headphones in a randomized order, with  <dig> repeats per stimulus. identification accuracy and reaction times for each stimulus were determined by calculating the average accuracy and reaction time of the responses.

statistical analyses
in the meg experiment, the amplitude, latency and source location of the n1m were analyzed with repeated-measures analyses of variance  for effects of hemisphere, stimulus type, and bit mode. in the behavioral experiment, the reaction times and identification accuracy were analyzed with anova for effects of stimulus type and bit mode. the newman-keuls test was used in all post-hoc comparisons.

RESULTS
n1m ecd modeling
all stimulus types elicited prominent aef responses in both hemispheres, as depicted in fig.  <dig>  the n1m amplitudes and latencies obtained through ecd modeling are shown in fig.  <dig> 

n1m amplitude
the grand-averaged n1m responses were larger in the right hemisphere  than in the left  . importantly, the degradation of the stimuli  resulted in a significant increase of n1m amplitude for all stimulus types, ranging from  <dig>  nam in the 16-bit mode to  <dig>  nam in the 1-bit mode . the overall amplitude of the n1m was dependent on stimulus type, with the sine-wave composite eliciting, on average, a stronger response  than the sine-wave tone  and the vowel stimuli  . no statistically significant interactions between any of the factors  were observed.

when the ecd amplitudes of the n1m were analyzed separately for each hemisphere, an unexpected effect of bit mode was found in the left hemisphere, where a decrease in the bit level amplified the n1m response . the left-hemispheric n1m amplitude was also dependent on stimulus type , the sine-wave composite eliciting a stronger response  than the other stimulus types . the stimulus type × bit mode interaction was not statistically significant for the left-hemispheric n1m amplitude. in the right hemisphere, similar results were obtained: the sine-wave composite elicited a stronger n1m  than the other stimuli   and the reduction in bit mode resulted in an amplification of the n1m response  . no significant stimulus type × bit mode interaction was observed.

n1m latency
no differences between the left and right hemisphere were observed in the n1m latency . the sine-wave tones elicited, on the average, n1m responses  <dig> ms earlier than spectrotemporally more complex stimuli  . furthermore, the reduction of bit level resulted in a monotonic decrease of n1m latency . post-hoc tests revealed that this effect was caused by sine-wave tones eliciting the n1m response approximately  <dig> ms earlier for the 1-bit mode  than the higher bit modes . separate analyses for each hemisphere yielded a stimulus type - bit mode interaction on n1m latency in the left hemisphere . compared to the n1m elicited by the sine-wave composite and speech stimuli, the latency of the n1m for the sine-wave stimuli was  <dig> ms  and  <dig> ms  earlier in both the left  and right  hemisphere.

n1m source location
for all stimulus categories, the n1m ecds were situated in the vicinity of the left and right auditory cortices. as depicted in fig.  <dig>  the right-hemispheric ecd locations were more anterior than the left-hemispheric ones, which is in line with previous observations  <cit> . in the left hemisphere, the sources of the n1m elicited by the sine-wave composite stimuli were approximately  <dig> mm medial compared to those of the n1m to the vowel and sine-wave stimuli . in addition, a significant effect of stimulus degradation was observed on the anterior-posterior axis in the left hemisphere, the sources of the n1m elicited by the 1-bit stimuli being  <dig> mm anterior to those of the n1m to the 16-bit stimuli . in the right hemisphere, the sine-wave composite n1m ecd locations were roughly  <dig> mm anterior to the sources for vowels . in the superior-inferior-dimension, no differences between the ecd locations were observed.

gradiometer data
p1m amplitude and latency
the peak amplitudes and latencies of the p1m, n1m and p2m responses are shown in fig.  <dig>  the main effect of stimulus degradation on the amplitude of the p1m was very significant  and clearly observable in both hemispheres, bit reduction yielding a monotonic increase in the p1m amplitude . furthermore, a significant main effect of stimulus type on the p1m amplitude was observed, the sine-wave composite stimuli eliciting a stronger response  than the other stimulus types  . no hemispheric asymmetries for the p1m amplitude were found . however, significant interactions for hemisphere × stimulus type , stimulus type × bit mode  and hemisphere × stimulus type × bit mode  were observed for the p1m amplitude. the average latency of p1m was approximately  <dig> ms earlier for the sine-wave stimuli  than for the other stimulus types  . moreover, the sine-wave stimuli yielded a bit mode effect on the p1m latency in the left hemisphere  =  <dig> , p <  <dig> ]), with the reduction of bit mode resulting in the response peaking earlier .

n1m amplitude and latency
the gradiometer data confirmed the results on the n1m response obtained through ecd modeling, including main effects of hemisphere , stimulus type  and bit mode  on the n1m amplitude. however, diverging from the ecd data, additional interactions of hemisphere and stimulus type  and stimulus type and bit mode  on the n1m amplitude were found. post-hoc analysis indicated that the n1m amplitude for the vowel stimuli was less sensitive to the distortion than that for the other stimulus types. these discrepancies between the n1m ecd and gradiometer data are most likely due to the fact that less data was available for the ecd data analyses as a result of inadequate signal-to-noise ratio in five subjects. for the n1m latency, the sine-wave stimuli yielded responses  <dig> ms earlier  than the other stimuli  . in addition, the reduction of stimulus quality resulted in a significant decrease in n1m latency . post-hoc analyses revealed that this effect was evident only for the sine-wave stimuli  =  <dig> , p <  <dig> ]), with the peak latency of the n1m ranging from  <dig> ms  to  <dig> ms .

p2m amplitude and latency
the p2m response was sensitive to bit mode reduction, its amplitude also increasing with the degree of stimulus degradation  . the peak amplitudes of p2m occurred, on the average,  <dig> ms earlier for the sine-wave tones  than those for the sine-wave composite  and speech stimuli  . the reduction of bit mode resulted in a significant decrease of p2m latency from  <dig> ms  to  <dig> ms  . furthermore, an interaction of hemisphere × stimulus type × bit mode  was observed for the p2m latency.

behavioral measurements
identification accuracy
the identification accuracy and reaction times in the behavioral recognition experiment are shown in fig.  <dig>  the vowels and the non-vowel  stimuli were identified well beyond the chance level of 11% in the 4-bit  and 16-bit  conditions. however, in the 1-bit condition, identification accuracy decreased significantly  , except for the sine-wave stimuli  . despite this decrement, the accuracy for the 1-bit condition was still above chance level, although the most difficult case, the 1-bit /e/, was identified at a relatively low rate of  <dig> %.

reaction times
reaction times  for the 1-bit condition,  <dig> ms on the average, were significantly increased for all vowel stimuli in relation to the 4- and 16-bit modes  . moreover, rts were not dependent on vowel identity . interestingly, it took  <dig> ms longer, on the average, to identify the stimuli as a non-vowel than as a specific vowel in the 16-bit and 4-bit conditions  . there were no statistically significant differences in rts between the vowel and non-vowel stimuli in the 1-bit condition. furthermore, for the sine-wave stimuli, the reduction of bit mode resulted in a 130-ms increase in the rts from  <dig> ms  to  <dig> ms  , but no comparable effect was observed for the sine-wave composite stimuli .

discussion
the present study investigated the cortical and behavioral processing of speech and non-speech sounds in conditions of decreased signal quality. unlike in previous research, we distorted the sounds using uniform scalar quantization, which directly reduces the amplitude resolution of the signal waveform thereby introducing signal-dependent, spectrally correlated distortion to the original sound. the results show that degradation of both speech and non-speech sounds amplifies the aefs bilaterally, indicating that the auditory cortices of both hemispheres are highly sensitive to distortion. the amplitude of the n1m response exhibited hemispheric asymmetry in that the response was significantly stronger and more sensitive to degradation in the right hemisphere than in the left. in addition to the n1m response, we also examined the p1m and p2m responses to investigate the temporal evolution of cortical activation. we found that the degradation amplified these responses as well, the p1m in particular. in contrast to the n1m, the p1m and p2m amplitudes showed no hemispheric asymmetry. the aef latencies were not systematically affected by the distortion, apart from a small decrease of p1m/n1m/p2m latency for the sine-wave stimuli as the quality of the stimuli was decreased. in addition, the aef latencies were earlier for the sine-wave than for the speech sound and sine-wave composite stimuli, which corroborates previous findings  <cit> . in the behavioral experiment, stimulus distortion was severe enough to interfere with identification accuracy and reaction times only in the most degraded  condition. these findings are congruent with the results of loizou et al.  <cit>  who demonstrated that for vowel stimuli, an amplitude resolution of four levels  is required for successful performance in a recognition task, assuming that spectral resolution is sufficiently high.

the current findings on the processing of speech sounds are in accord with previous results  <cit> , which show that the n1m amplitude is increased in the right hemisphere when acoustical cues of vowels are degraded. the present study indicates, importantly, that this amplifying effect of stimulus distortion is not related to processing of acoustic features of speech in particular, since it was clearly observed with all stimulus types. in fact, the augmentation was somewhat more pronounced with the spectrally simpler, non-speech stimuli than with the speech stimuli. thus, the increased activation may reflect processing of auditory stimulus features common to both speech and non-speech stimuli. the hemispheric asymmetry observed in  <cit>  was not as distinct in the current data, given that the distortion caused an amplification of the n1m also in the left hemisphere. the hemispheric differences in the n1m amplitude might be related to the changes in spectral structure associated with the stimulus degradation. the larger distortion-related increase of the n1m amplitude in the right hemisphere might reflect an increase in spectral processing caused by the addition of noisy harmonic frequencies to the signal spectrum. this view is corroborated by haemodynamic studies according to which the right-hemispheric auditory cortex is sensitive to spectral variation  <cit> , as well as intracortical  <cit>  and aef  <cit>  findings showing that the right hemisphere is more responsive to the spectral composition of complex tones than the left.

the present results differ from previous observations using additive, uncorrelated noise to degrade speech  <cit>  in which the increase of masking noise resulted in decreased and delayed aep/aef responses. these differences are interesting, given that both distortion methods render the speech stimuli less intelligible. it seems likely that the increased activation brought about by the usq manipulation is related to the manner in which this signal distortion technique changes the spectrum of the stimulus. the usq procedure adds correlated, signal-dependent distortion to the signal over the entire bandwidth, causing an alteration in the distribution and dynamic range of spectral energy. as a result, the balance of spectral energy is shifted towards high frequencies, resulting in a flattening of the spectrum . further, the quantization process adds new harmonic components to the signal. this is in contrast with the additive  noise masking, in which the original spectral harmonics of the intact sound are inundated by an aperiodic masker and due to the linearity of the masking process, no new frequencies are created. therefore, since both the addition of white noise and the use of the usq flatten the signal spectrum , the most conspicuous spectral difference between the resulting signal spectra is, indeed, the amount of harmonic components. consequently, one could speculate that the presence of a regular, harmonic spectral structure could be the most relevant feature which induces stronger activation in the brain compared to that elicited by sounds with a random spectral structure. this explanation gains support from recent studies showing that the auditory cortex is sensitive to the periodicity of speech  <cit> , with periodic stimuli yielding stronger responses than aperiodic ones. similar results have also been obtained using non-speech sounds, with the presence of a periodic structure leading to enhanced activation  <cit> . the above observations are in accord with the current findings, considering that the quantization adds periodic components to the signal over the whole spectral bandwidth. taken together, it seems plausible that the auditory cortex is sensitive to harmonic regularities in the sound spectrum, extending even to noisy harmonics.

it could be argued, though, that in the case of the spectrally simpler non-speech stimuli, the increase in spectral bandwidth caused by the distortion could explain the observed aef augmentation effects regardless of the amount of harmonic components in the signal spectrum. indeed, the quantization procedure increases the spectral bandwidth of narrowband sounds, for example the sine-wave, by creating new frequency components to the signal. previous studies have indicated that an increase in spectral bandwidth of tonal, periodic sounds amplifies the n1m response  <cit> . however, a number of observations speak against an explanation of the n1m enhancement being based on an increase of spectral bandwidth independently of the spectral structure of the stimulus. for instance, in the current study, an increase in the amount of noisy harmonics caused by stimulus distortion amplified the n1m even when the spectral bandwidth remained unchanged . secondly, in the present observations, the undistorted sine-wave composite stimuli elicited a stronger n1m response than the undistorted vowel, despite having a narrower spectral bandwidth. thirdly, a number of previous studies suggest that increasing the bandwidth of sounds with a flat, random spectral structure can decrease the amplitudes of auditory evoked responses. soeta et al.  <cit> , for example, demonstrated that the amplitude of the n1m decreased with increasing bandwidth of stochastic noise. furthermore, lütkenhöner et al.  <cit>  found that a band-pass random noise elicited a weaker n1m response than a pure tone of narrower bandwidth. in addition, seither-preisler et al.  <cit>  did not find any systematic change in the n1m amplitude accompanied with variation in spectral bandwidth of stochastic noise. similar effects on aeps have also been documented using continuous masking noise  <cit> , the increase in noise bandwidth resulting in diminished aep responses to speech sounds. taken together, it seems plausible that the spectral structure of the stimulus has an effect on the related brain responses when the spectral bandwidth of the stimulus is modified. more specifically, it appears that a random frequency spectrum yields decreased responses when the spectral bandwidth is increased while an increase in spectral bandwidth of harmonic components results in amplified aep/aefs  <cit> . thus, we propose that the increase in noisy harmonic components over a wider bandwidth as a result of the quantization procedure could be the main determinant of the observed amplification of the aef responses.

it might be suggested also that the flattening of the signal spectrum and the relative amplification of higher frequency components would account for the aef enhancement observed in the present experiment. while it is possible that spectral flattening does have effects on aef amplitudes, it seems that these effects are to some extent dependent on the degree of harmonic regularity in the signal spectrum. for instance, palomäki et al.  <cit>  found that wide-band noise bursts elicited substantially smaller n1m responses than vowels and pseudo-vowels even though the noise stimuli had a flat spectrum and thus the high frequencies were significantly emphasized compared to the other stimuli. hence, it might well turn out that a random increase in high spectral frequencies does not enlarge aefs whereas an increase in high harmonics yields enhanced aefs .

the sine-wave stimuli elicited the aef responses  <dig> ms earlier than the sine-wave composite and the vowel stimuli in the current experiment. as the sine-wave composite and speech sound contained energy at high frequencies, cochlear travelling wave delays which result in a faster auditory nerve response to high than low frequencies  <cit>  would not seem to account for the cortical latency variations. rather, these variations may reflect a non-linear relationship between the activity of the cochlea and that of auditory cortex: roberts and poeppel  <cit>  demonstrated using pure tones that the n1m latency is shortest to mid-range frequencies and significantly delayed for low and high frequencies. the presence of high frequencies in the speech stimuli may thus have contributed to the observed 10-ms delay in response latency.

the present findings raise interesting questions for future research. for instance, in order to find out why earlier studies have demonstrated decreased and delayed aep/aef responses to masked speech stimuli  <cit> , the effects of continuous and transient masking of speech should be examined. transient masking refers to a situation where the distortion is only present simultaneously with the stimulus. it might turn out that the aep/aef decrements observed in the above studies arise from the use of continuous masking noise instead of the transient acoustic distortion employed in the present study. continuous noise causes neuronal adaptation in the auditory cortex, which leads to attenuated and delayed aep/aef responses for the masked stimuli: when continuous uncorrelated masking noise is introduced, the intensity of the masked stimuli has to be elevated to produce n1m response amplitudes and latencies equal to those measured in the case of unmasked stimuli  <cit> . the adaptation effects of random noise masking on the n1-p <dig> response can be clearly observed also when an ongoing noise stimulus becomes periodic and vice versa  <cit> . to obtain an overall picture of the brain dynamics related to the perception of degraded speech, the effects of spectrally correlated distortion and uncorrelated additive noise using both continuous and transient masking on aef responses should be investigated in the future. these studies might reveal why the introduction of spectrally correlated, signal-dependent distortion to a sound results in a substantial increase in cortical activity while additive, uncorrelated random noise does not, considering that both maskers interfere similarly with the accurate identification of the signal.

CONCLUSIONS
the data presented here suggest that the activation of human auditory cortex as indexed by the amplitude of the aefs is highly dependent on acoustic degradation of speech and non-speech sounds. more specifically, distorting sounds by reducing the amplitude resolution of the stimulus waveform resulted in amplified aefs with no systematic changes in response latencies. the n1m amplitudes were asymmetric in that the responses were stronger and more sensitive to distortion in the right hemisphere than the left. we propose that the observed enhancement of cortical activity is related to the distortion caused by the decrease of the amplitude resolution of the signal, in particular the resulting addition of noisy harmonics over the entire bandwidth of the signal spectrum. taken together, the present results demonstrate that differences in the spectral structure of signal distortion can have substantial effects on brain responses while the perceptual identification of the signal is compromised in a similar manner.

abbreviations
aef: auditory evoked field; aep: auditory evoked potential; anova: analysis of variance; ecd: equivalent current dipole; eeg: electroencephalography; f0: fundamental frequency; meg: magnetoencephalography; rt: reaction time; usq: uniform scalar quantization.

authors' contributions
ht, pa, pm and im designed the experimental setup of the study, and pa prepared the auditory stimuli. im acquired the data, performed the statistical analysis and prepared the manuscript. all authors participated in the writing process, and have approved the final version of the manuscript.

