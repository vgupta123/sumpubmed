BACKGROUND
a major objective of genomic studies is to characterize genetic variations. the types of variants include single nucleotide polymorphisms , micro-insertions/deletions  and large, structural variations of deletions, insertions, translocations and inversions  <cit> .

traditionally, deletions at the megabase and submegabase levels are characterized by positional cloning and microarray technologies  <cit> . with the rapid progress of next-generation sequencing  technology  <cit> , a strategy of paired-read sequencing has been developed  <cit> . several methods have been developed to characterize the breakpoints of structural variants, including analysis of the so-called ‘split reads’ that map to different loci of the reference sequence  <cit>  and comparison of the consensus sequence from assembly-based approaches to a reference sequence  <cit> . while most of these methods are comprehensive, their detection ability is limited on medium-sized deletions at low sequencing coverage. furthermore, many analyses do not require a comprehensive genomic survey, but identify certain specific markers. for example, in comparative genomic studies, a small subset of deletions is sufficient to serve as molecular markers to trace the evolution of genomes. during the past five years, several restriction enzyme based methods have been developed for this purpose, including rrl  <cit> , rad-seq  <cit> , crops  <cit>  and gbs  <cit> . most of them aimed at identifying single nucleotide polymorphism  in restriction regions. however, few methods were developed to detect proportional deletions at the genomic level.

chen et al. proposed a method for examining genomic structural variations based on paired-end restriction tags   <cit> . however, its application was limited due to complex experimental procedures requiring laborious library cloning and single-end sequencing. this method also lacked a computational program for variant discovery. taking the advantage of new ngs developments, we greatly simplified the library construction process by adapting this method to a mate-paired library construction system for sequencing and validation . we also developed a computational program to identify genomic deletions and an experimental protocol to verify the mapped deletions. we used this system to analyze a liver cancer genome. the results demonstrated the power of the system.

RESULTS
the major improvements in our method
compared to the previous method  <cit> , we made four major improvements:

1) we use paired-end sequencing instead of single-end sequencing.

2) we adopted a mate-paired system for library construction to eliminate the cloning process.

3) we extended the sequence length beyond the original 17 bp limit.

4) we designed a specific computational program for ditag analysis and deletion detection.

therefore, our improved method is more simple, specific and systematic.

choice of restriction enzyme determines the detection resolution
the goal of our method is to detect medium-sized deletions, specifically those in the range of 100 bp to 10 kb.

the detection resolution is correlated with the cutting frequencies of the selected restriction enzymes . each enzyme recognizes fixed restriction sites and produces fixed restriction tags; thus, it targets a fixed proportion of the deletions. for normal fragments, the paired tags are located at two adjacent restriction sites, whereas deletions that include restriction sites result in consecutive skipping of those sites. skipping a single restriction site can either be attributed to a point mutation that has caused its inactivation or to an undigested site . to increase the specificity, we only detected deletions that skipped at least two consecutive restriction sites.

we simulated the restriction digestion of  <dig> unique restriction sites to test their detection resolutions . the sites are recognized by more than  <dig>  type ii restriction enzymes in rebase . the results show that the detection resolutions of two known deletion datasets  are strongly correlated with each enzyme’s cutting frequency. furthermore, up to 80% of the deletions can be targeted by the most frequent cutter.

taqi, which recognizes the sequence tcga, produces  <dig>  million fragments with a median size of  <dig>  kb. tests with this enzyme show that 10% and 28% of the medium-sized deletions can be target detected in the yh genome and the dgv database, respectively . an overview of the analysis pipeline is shown in figure 1b.

consistency of restriction ditags from the two libraries
we analyzed a liver cancer genome using a taqi restriction digestion. we constructed two separate libraries, producing  <dig>  million read pairs of 33× <dig> bases in library  <dig> and  <dig>  million read pairs of 48× <dig> bases in library  <dig> .

1 m: counts of the reads, pairs or ditags in millions.

2gb: gigabases.

3genomic regions identified in the insert regions of the experimental ditags that correspond to the reference genome.

overall,  <dig> million sequences  were mapped to the expected restriction sites in the hg <dig> reference genome sequence , while the other 15% of the reads failed to map to the expected regions. the hg <dig> reference contains  <dig> , <dig> unique ditags that are defined by the taqi sites . approximately 68% of the ref-ditags were mapped by experimental ditags with an average depth of  <dig> × . the  <dig> million experimental ditags cover 45% of the genome.

the ditags from the two libraries were highly consistent, covering 53% and 65% of the ref-ditags. while the coverage percentages were not high, the proportions of their covered genomic regions are highly correlated across different chromosomes . furthermore, 50% of the ref-ditags were covered by both datasets with a 73% rate of overlap between the datasets . analysis of the insert size distribution showed that the overlapping ditags tended to be small fragments . both libraries showed correlated coverage-enrichment curves for their ditags , which could be attributed to the fact that smaller fragments were more likely to be circularized than larger fragments  <cit> . this graph also explains the presence of uncovered ref-ditags, which have significantly larger insert sizes than covered ref-ditags . in effect, this feature enhances the reproducibility of the restriction-based method by targeting fragments of a given size.

deletion identification
the following four conditions were used to identify the candidate deletions.

1) at least two consecutive restriction sites were skipped by the ditags  and the two restriction sites should include at least one site that is excluded from the snp database . this criterion should prevent false positive results raised by random point mutations that inactivate restriction sites.

2) a candidate deletion should be supported by at least two ditags in order to eliminate artifact from the randomness during both experimental and computational process.

3) ditags should pass the in silico pcr test . this test simulates the real pcr process by searching all possible genomic alignments within an expected distance and allows multiple mismatches. we used ispcr to examine the accuracy of the ditag sequences as sense/antisense primers. ditags remained if they produced a single electronic pcr product at the alignment position . this step ensures the reliability of the mapping result as well as the success rate of the downstream pcr validation.

4) a candidate deletion should be supported by more than 10-40% of the ditags mapped to its locus. candidate deletions with low proportion of the ditags mapped to the locus were eliminated. these candidates could represent deletions from duplicate regions, making them difficult to validate. however, setting it too high would have a danger of losing real heterozygous deletions. in principle, the threshold mainly depends on the complexity, especially the repetitive content, of the in-analysis genome. in our analysis, we set this value to 33% to ensure a high specificity.

using these conditions, we identified  <dig> and  <dig> deletions from the two libraries, with a total of  <dig> deletions in the combined dataset . approximately 76% of the deletions identified by the lower-coverage library were also identified by the higher-coverage library .

validation of the candidate deletions
we validated the candidates by pcr . of the  <dig> candidates randomly selected for validation,  <dig> were validated as real homozygous or heterozygous deletions . the false positive one was due to the reason that one of the restriction sites was inactivated by a point mutation while the other site was also a snp site.

 <dig> an id of a ditag from which the local structural information is inferred .

 <dig> deletions that overlap with structural changes in the database of genomic variants <cit> .

 <dig> double point variants inactivating both restriction sites.

of the  <dig> validated deletions,  <dig> deletions overlapped with existing data in the database of genomic variants <cit> . the data indicate the involvement of the genes lrp <dig>  adipor <dig> and rph3al , which have a reported role in developmental disorders and tumorigenesis  <cit> .

according to our validation rate, the total number of actual deletions that were identified by taqi restriction fragments was estimated to be 175×18/ <dig> =  <dig> 

our simulation showed that taqi ditag sequencing may detect up to 10% of the deletions across the entire genome . approximately 45% of the genome has been examined with the experimental ditags . we can calculate that the lower bound on the total number of  <dig> -10 kb deletions is  <dig> / 10% / 45% =  <dig> .

discussion
next-generation sequencing has been a powerful tool for deletion identification  <cit> . a variety of computational algorithms have been developed to use ngs sequence data to search for deletions. notably, these methods attempt to collect comprehensive details about genomic structural variants. for example, a recent study surveyed structural changes  in two personal genomes using the de novo assembly of short reads  <cit> . however, comprehensive methods require high sequence coverage , which drives up costs, requires a large amount of data storage space, necessitates long analysis time and creates heavy computational demands. in select studies such as evolutionary genomics, it is not necessary to achieve comprehensiveness; instead, a limited amount of information is sufficient. in recent years, several restriction-based ngs methods have been developed to sequence partial genomes  <cit> . most of these methods aim for snp discovery, not the detection of structural changes. we modified the method of chen et al. <cit>  by simplifying its experimental procedures and developing a computational program. in this study, we showed that sequencing both ends of the restriction fragments generated by a medium-frequency enzyme can be an accurate method for identifying medium-sized deletions, even with sequence coverage as low as one-fold genome size.

the deletion resolution can be controlled by selecting restriction enzymes with different cutting frequencies depending on the research objectives. the selected restriction enzyme determines what target regions will be sequenced, as well as the length distribution of the restriction fragments . in this study, very low sequencing coverage  could be concentrated within the tag regions to reach a sufficient depth for deletion identification. the high rate of overlap between the two separate datasets used in our study, both of which had low genomic coverage, demonstrates the reproducibility of this method .

the number of detected deletions can also be adjusted by the coverage. library  <dig> contained  <dig> × paired reads and detected  <dig> deletions, whereas library  <dig> had  <dig> × read coverage and detected  <dig> deletions . importantly, most of the deletions found with library  <dig> were also identified with library  <dig> .

in addition to high flexibility and efficiency, this method also displayed high accuracy. the use of in silico pcr significantly increased the specificity of the detected deletions by eliminating the noisy sequences that were produced by experimental errors, such as randomly broken fragment ends, star activity of the restriction enzyme, sequencing errors and false mapping.

the population of target deletions can be fixed once the restriction enzyme is determined, and the size of the deletion population can be adjusted by selecting different enzymes and coverage according specific needs . the flexible choice of the fixed target enabled comparative genomic studies on a subset of deletions across different samples because these deletions were randomly distributed across the genomes  and could be accessed repetitively without heavy sequencing input. thus, our method is applicable to a variety of fields, including:

1) detecting the deletions across multiple genomes, especially for the species with large, difficult-to-sequence genomes in population or comparative genomic studies. for example, a recent survey of the structural variants in an individual gorilla genome required a  <dig> gb sequence input  <cit> . at this scale, our method can examine the genetic diversity of deletions in a population of 5– <dig> gorillas. although the genome strip can also examine deletions in multiple large genomes, as it did with  <dig> genome data  <cit> , it cannot deal with single genome data nor identify singletons from pooled data as we did in this study.

2) detecting the deletions in paired samples. for example, rapid identification of residual alleles of cancer cells which usually exist in trace amount in circulating dna  <cit> . by sequencing the ditags of original tumor dna and normal dna in the same individual, several somatically-acquired, tumor specific deletions could be identified. pcr primers could be designed based on these deletions to amplify the tumor dna specifically.

3) massive validation of deletions found by other comprehensive methods or massive genotyping of known deletions.

CONCLUSIONS
we developed a simplified experimental protocol and computational pipeline to detect genomic deletions at low genomic coverage. the library construction procedure can be adapted to other ngs platforms. the method is cost-effective, flexible and accurate. our method may be potentially useful for the identification of representative markers.

