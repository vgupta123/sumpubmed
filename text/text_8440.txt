BACKGROUND
fingerprint mapping continues to play an important role in large-scale dna sequencing efforts  <cit> . the procedure is challenging in terms of both its laboratory and computational demands. indeed, most of the computational steps involve non-trivial algorithmic aspects. while reasonable solutions have been found for many of these, one task that remains particularly problematic is assessing postulated clone overlaps based on their fingerprint similarity.

the "overlap problem", as this is often referred to, basically involves examining all pairwise clone comparisons in order to identify overlaps. for a map consisting of λ clones, there are cλ,  <dig> = λ / <dig> such comparisons. in each one, the number of matching fragment lengths between the two associated fragment lists is established. a case having μ >  <dig> matches indicates a possible overlap because the mutual length may represent the same dna. lengths are not unique, so such matches are not conclusive indicators of overlap. instead, the problem is largely one of probabilistic classification. one or more quantitative metrics are used to evaluate the authenticity of each such case. for example, an apparent overlap might be judged against its likelihood α of arising by chance. methodologies of varying degrees of rigor have been proposed for this task  <cit> . however, the so-called sulston score, or sulston probability ps has emerged as a de facto standard  <cit> , in part because of its integration in the widely-used fpc program  <cit> . a liability of a number of these methodologies, including ps, is they assume fragment length comparisons are independent when, in fact, they are not  <cit> .

recently, the exact distribution characterizing the overlap problem was determined  <cit> . comparisons reveal that the assumption of independence is usually a poor one and that the sulston score systematically over-predicts actual overlap probabilities, often by orders of magnitude. consequently, a bias arises in projects that utilize ps . one chooses the significance threshold α to minimize erroneous decisions according to what is presumed to be the actual probabilistic description of the problem, pe. the alternative result using the sulston score is an overall increase of false-negatives . clones having significant overlap will still be correctly detected . moreover, false-positives would not be increased because ps errs on the conservative side with respect to non-overlapping clones . miscalls can obviously be expected when poor values of α are chosen . however, if α is set too high, there will still be circumstantial cases where the correct decision is made . these will presumably be more than offset by a higher rate of false-positives . in summary, ps is not an especially good discriminant for the overlap assessment problem.

the drawback of pe is that it is rather difficult to compute and cannot be used directly in most cases. for example, current resources are not sufficient to evaluate it for most bac comparisons or for capillary-based fingerprinting  <cit> . a suitable method of approximating pe is therefore required. here, we propose a straightforward correlation-based approach that derives correction factors for the sulston score. this procedure dramatically increases accuracy without incurring much additional computational effort.

RESULTS
the overlap problem is formally cast in terms of two clones having m and n "bands", respectively, where m ≥ n. each band represents an individual clone fragment, with its position on a gel image providing an estimate of the fragment's length. multiple bands of roughly the same length often appear. finite measurement resolution ± r allows an image of length l to be subdivided into t =  <dig>  l/r discrete bins. the sulston score ps = ps  is taken as a provisional estimate of the probability that at least μ fragment matches between the two clones arise by chance. note here that the variables  correspond to , respectively, in notations used by the fpc program  <cit> . the corresponding exact probability is pe = pe , as given in refs.  <cit> . we formulate a corrected value, pc, that can be both efficiently calculated and that gives substantially better estimates of pe than the sulston score, i.e. |pe - pc| ≪ |pe - ps|.

the simple log-log plot in fig.  <dig> shows good correlation , suggesting that standard regression might be a reasonable basis for correction. note the characteristic over-prediction of ps.  these particular data are computed for t =  <dig>  which describes traditional settings for fragment length measurements and comparisons, i.e. ±  <dig> pixels over a  <dig> pixel gel image  <cit> . considerations of coverage usually dictate a large number of clones in a map  <cit> , so that values substantially above 10- <dig> are not usually of interest  <cit> . the data range over  <dig> ≤ μ ≤ n for a number of different fingerprint comparison sizes:  <dig> ≤ n ≤ m for  <dig> ≤ m ≤  <dig>   <dig> ≤ n ≤  <dig> for m of  <dig> and  <dig>   <dig> ≤ n ≤  <dig> for m of  <dig> and  <dig>   <dig> ≤ n ≤  <dig> for m of  <dig> and  <dig>  and finally  <dig> ≤ n ≤  <dig> for  <dig> ≤ m ≤  <dig>  the exact solution becomes difficult to evaluate beyond these ranges using readily-available resources. specifically, the computational effort increases according to a factor that exceeds m!/!  <cit> .

correlation in fig.  <dig> is clearly not perfect. specifically, the points show some amount of lateral scatter. accuracy of the correction can be further enhanced to the degree that dispersion within the window can be minimized. here, we can apply a simple power-law data reduction model to obtain a transformed sulston score

 pt=psνμξmηnζ.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgqbaudawgaawcbagaemivaqfabeaakiabg2da9iabdcfaqnaadaaaleaacqwgtbwuaeaaiigacqwf9ogbaagccqwf8oqbdaahaawcbeqaaiab=57a4baakiabd2gatnaacaaaleqabagae83tdggaaogaemoba42aawbaasqabeaacqwf2ogeaagccqgguaglaaa@3faf@ 

the four power values can be chosen empirically such that the data locally collapse into a more highly correlated set. for example, selecting  =  in eq.  <dig> leads to the curve-fit

 pc≈ <dig>  pt <dig> 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgqbaudawgaawcbagaem4qameabeaakiabgiki7kabimda5iabc6cauiabiida4iabiwda1iabiwda1iabbccagiabdcfaqnaadaaaleaacqwgubavaeaacqaixaqmcqgguaglcqaixaqmcqai3awncqaixaqmaaaaaa@3d9b@ 

and the associated pearson's coefficient ρ ≈  <dig> .

discussion
we propose eqs.  <dig> and  <dig> as a correction to the standard sulston score for typical fingerprint mapping conditions  <cit> . although shown as two separate equations so as to clarify the concept, these can clearly be combined into a single equation for actual computations. pearson's coefficient is not especially sensitive to the parameters in eq.  <dig> and there are many combinations of  that elevate ρ into the ~ <dig>  range. other methods for reducing the data do not perform as well as the model in eq.  <dig>  for example, standard dimensional analysis  <cit> , which involves correlating variables such as pe/ps, m/n, and μ/n, cannot adequately resolve the fact that values of the individual variables relative to one another remain important.

accuracy assessment
eqs.  <dig> and  <dig> are obviously straightforward to compute, leaving the question of just how much error reduction is actually realized over the un-adjusted sulston score. this can be quantified with a simple metric. for the sulston score, the error is taken as es = |pe - ps|/pe. error for the corrected result, ec, is calculated similarly.

a size-selection step is part of most library-construction protocols, meaning that the variance of clone sizes will be limited to some degree. consequently, many clone-clone comparisons will involve similar, though not necessarily equal numbers of fragments. fig.  <dig> shows a comparison of error rates for the raw sulston score and the corrected score in eq.  <dig> for m/n ≤  <dig> . that is, we compare clones whose numbers of fragments in their respective fingerprints are within 30% of one another. the figure also shows the error rate for the un-reduced data, i.e. for a regression equation that does not use the preliminary processing given by eq.  <dig> 

the sulston score shows an increasing error as the acceptance threshold is tightened . maximum values for the threshold are typically in the neighborhood of 10- <dig>  <cit> , for which ps over-predicts by about one order of magnitude. for threshold parameters around 10- <dig>  sulston over-prediction is about  <dig> orders of magnitude. while eq.  <dig> shows significant local variation, the overall trend is much more constant and its error is appreciably smaller. correction on un-reduced data also shows better accuracy than the raw sulston score, being roughly as good as eq.  <dig> up to about 10- <dig>  it diverges beyond this point and eventually shows about the same level of error as the raw sulston score. the combined correction procedure of eqs.  <dig> and  <dig> appears to provide the best fidelity over the widest range.

comments on uncertainty
a simple correction of the type we propose here obviously cannot capture all the complexities inherent in the exact distribution. this results in a scatter of the data that cannot be completely eliminated, for example as illustrated in fig.  <dig>  this scatter is a primarily function of m/n, rather than individual values of m and n. for instance, log-log regression of data restricted exclusively to m = n returns a pearson's coefficient of ρ ≈  <dig>  without any sort of preliminary data reduction. of course, such a correlation would not be generally applicable to realistic clone libraries and maps.

eq.  <dig> is based on the limited set of data described above. applying it outside this set necessarily involves a degree of extrapolation, which raises two types of uncertainty. first, large m/n ratios contribute to scatter, but such extrema only emerge for cases involving sufficiently large differences between m and n. eq.  <dig> accounts for data up to m =  <dig> with a maximum ratio of m/n of about  <dig>  in the context of averages, this implies a comparison of two clones whose sizes differ by a factor of four. while there is the possibility of even greater disparities, such cases will be comparatively rare in general because of size-selection steps executed during the library-construction phase. for example, in the human genome project rpci- <dig> library, about two-thirds of the bac clones were concentrated between  <dig> and  <dig> kb  <cit> , for which the maximum m/n would be roughly  <dig> . most comparisons would be somewhat closer to one. only about  <dig> % of the library resided in each of the <  <dig> kb and >  <dig> kb ranges. this means that fewer than  <dig> % of the comparisons will involve uncharacteristically large m/n ratios. consequently, we do not view this type of uncertainty as being particularly significant.

the larger issue in our opinion arises for comparisons that extend beyond  the 10- <dig> threshold tolerance. while minor extrapolation of a few orders of magnitude is probably not worrisome, some projects utilize substantially lower tolerances. for example, luo et al.  <cit>  and nelson et al.  <cit>  report values on the order of 10- <dig> and 10- <dig>  respectively, when using capillary electrophoresis. other techniques, such as the traditional double-digest, can also generate higher numbers of fragments, which may require reduced thresholds. the fidelity of eq.  <dig> for such cases is not clear. for example, in the data set shown in fig.  <dig>  larger m/n values are under-represented at the lowest scores. because loci for larger m/n values do not slope as steeply as those for smaller ones, the trend shown in the figure may not continue in the exact same manner for values well below 10- <dig>  we can only observe that the corrected score will still be the significantly more accurate choice as compared to the raw sulston score because the assumption of independent fragment comparisons is increasingly untenable. characterizing the exact solution in this range requires computations considerably larger than what can readily be made at present.

CONCLUSIONS
we have calibrated eq.  <dig> according to the traditional parameters used in the fpc mapping program  <cit> . similar corrections can readily be constructed for different parameters. for example, protocols and software sizing methods now allow for band resolutions higher than the customary value of t =  <dig>  table  <dig> shows correction parameters for several such cases. similarity of the correlation coefficients suggests that results would be comparable to that shown in fig.  <dig>  although the accuracies derived from this approach are probably acceptable in the correlation range, they could, in principle, be further increased by using multiple corrections calibrated for specific "bins" of the m/n parameter.

clone overlap assessment is sometimes framed as a statistical testing problem  <cit> . here, α is the probability of erroneously concluding that two clones overlap, when in fact they do not.  consequently, corrections are most immediately relevant in the neighborhood surrounding α . the overlaps here are the most valuable to detect in the sense that they are the smallest, and consequently contribute most effectively to a minimum tiling path  <cit> . a large fraction of the comparisons will be either far above or below the threshold, so their assessments will not ultimately be affected. however, correction is still important for these cases. for example, branscomb et al.  <cit>  have pointed out that the ability to accurately rank all overlaps according to their associated probabilities is useful in the assembly phase of mapping.

ascertaining the degree to which a particular mapping project would actually be improved by using sulston score correction is difficult. aside from the usual factors that complicate comparisons  <cit> , there are special considerations for this kind of evaluation. for example, established sulston-based mapping projects may have obtained their best results using threshold values that would not necessarily be considered "correct" from the standpoint of the exact probability distribution . biologists have historically viewed selection of the sulston threshold to be a non-trivial, library-dependent problem and often resort to empirical sampling and iteration  <cit> . consequently, one probably cannot obtain an objective comparison by just replacing ps with pc for these cases. another avenue, perhaps more pragmatic, would be to assess corrections on a simulated project. for example, digesting finished sequences in silico  <cit>  enables one to use the resulting simulated fingerprints to see how well a map could be reconstructed. several variations on this method are possible  <cit> . of course, use of correction for new projects is certainly recommended.

other issues remain unresolved. with the exception of the conditional nature of match trials, the correction in eq.  <dig> is based on the same set of assumptions as the sulston score. neither consider, for example, possible non-iid distribution of fragment lengths or length-dependent measurement accuracy. consequently, we feel that the simple correction procedure proposed here represents a reasonable, though admittedly provisional advance in dna mapping methodology.

