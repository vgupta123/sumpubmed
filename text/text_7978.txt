BACKGROUND
dna microarrays have become common tools for monitoring genome-wide expression in biological samples harvested under different physiological, pathological or pharmacological conditions. one of the most challenging problems in microarray data analysis is probably the identification of differentially expressed genes  when comparing distinct experimental conditions. in spite of its biological relevance, there is still no commonly accepted way to answer this question.

an ideal deg identification method should limit both false positives, i.e. genes wrongly called significant , and false negatives, i.e. genes wrongly called not significant . to this end, understanding how gene expression values measured in replicated experiments are spread around the true expression level of each gene, would help to distinguish biologically relevant gene expression changes from fluctuations due to different sources of variability that are unrelated to the biological phenomenon under investigation. measurement error estimates can be obtained in two ways: either by empirically inferring noise from highly replicated data or by deducing noise from a theoretical error model  <cit> . especially when the experimental design requires the investigation of a high number of conditions, the former strategy is not always feasible, because of the high cost of these experiments or due to the availability of biological material. in addition, there is still a lack of consensus on how gene expression values from replicated experiments should be theoretically distributed, which restricts the application also of the latter strategy.

the most widely used methods for identifying deg range from purely empirical filtering techniques  to more sophisticated statistical tests such as the signal-to-noise ratio described by golub et al.  <cit>  or the significance analysis of microarrays  method by tusher et al.  <cit> . while empirical filtering techniques rely on arbitrarily chosen thresholds and are unable to provide any type of control on the significance of the results, the more sophisticated statistical tests usually need a high degree of replication in the data to accurately measure gene-specific variability.

in the past years various authors have proposed competing error models for microarray data from which discordant implications for the variance versus mean dependence can be deduced. chen et al.  <cit>  first proposed a simple gaussian model, more recently ideker et al.  <cit>  and li and wong  <cit>  introduced two-component models containing a multiplicative and an additive error term. all of these models implicitly or explicitly assume a constant coefficient of variation , implying that standard deviation should vary proportionally with the mean. more recently, rocke and durbin  <cit>  proposed a variation of the two-component model from which they derived that variance of repeated microarray measures is a quadratic function of the mean. dealing specifically with spotted cdna microarray technology baggerly et al.  <cit>  proposed a beta-binomial model, from which it can be derived that variance is a second-order polynomial function of the mean. unfortunately, most of these models are based on theoretical assumptions that have been verified on simulated data or on data sets consisting of small numbers of replicates. more recently, tu et al.  <cit>  empirically modeled the variance versus mean dependence from a data set consisting of ten replicated oligonucleotide microarray experiments. according to the authors, the variance of the genes should decay exponentially with the mean, but only for moderately expression values. taken together, all these aspects could limit the applicability of these error models.

independently from the choice of the error model, another point that remains to be faced is on how to estimate residual error. a discussed by wright et al.  <cit>  the possibilities range between two extremes: either obtaining a single variance estimate across all genes or obtaining a gene-specific residual variance. in the same paper a hybrid approach is proposed in which information from all genes is used to fit a single linear model from which the gene-specific variance estimates can be deduced. in the present work we chose to follow an approach similar to the latter.

the aim of this study was to use highly replicated microarray data to empirically determine the true variance versus mean dependence that exists in this type of data. this knowledge enabled the proposal of plgem as a simple but powerful error model. we fitted the proposed model on various data sets without pre-filtering the data, deriving an improved test statistics and identifying deg even in data sets with very little number of replicates.

RESULTS
variance versus mean dependence
the relationship between measurement variability and average expression values was investigated by means of scatter plots where different measures of spread were displayed against different measures of location. for each gene absolute or relative standard deviation was plotted against the mean expression value in either linear or log-log plots using data from the 16idc data set . independently from the choice of standard deviation or inter-quartile range as the estimate of spread and of mean or median as the location estimate we obtained qualitatively similar plots . log-log plots of both absolute and relative spread estimates revealed a strikingly linear dependency, indicating that measurement spread could depend on signal location following a power law.

the power law global error model
based on the previous observation, we chose to empirically model measurement noise through linear regressions:



where s and  respectively represent standard deviation and mean of repeated measures. error term ε is the realization of a random variable e that we will show later to be normally distributed as assumed when fitting a linear model. inspired by the previous experimental observations we propose the following power law global error model :



model parameters α and β can be estimated from linear regression coefficients in  <dig> in a straightforward way:

α = ec     eqn. 3

β = k     eqn. 4

plgem fitting method
instead of performing a simple linear fit through the whole set of points, we preferred to implement a method that could provide improved model robustness by partitioning the data to gain local estimates of spread as in mutch et al.  <cit> . most importantly, this method should also provide the possibility to choose different levels of confidence when modeling the spread of the data. note that mutch et al.  <cit>  proposed to model within-replicates fold changes as a function of average expression using a model that was very different from plgem. therefore, the following algorithm was applied:

• rank genes according to their ln() value and subdivide the overall expression range into a given number p of partitions containing an equal number of ranked genes.

• choose a "modeling quantile" q and determine for all the genes contained in each partition a single "modeling point" with median of ln() values as the x-coordinate and q-th quantile of ln values as the y-coordinate.

• finally, find a linear fit through the set of p modeling points using the least-squares method and obtain a slope k and an intercept c of the resulting regression function.

thus, for all possible combinations of p and q a slope kp,q, an intercept cp,q and a correlation coefficient r2p,q can be obtained. performance of this modeling method was tested also using different combinations of partitions, in the range between  <dig> and  <dig>  and quantiles, ranging from  <dig>  to  <dig>  . for all  <dig> analyzed combinations of p and q regression lines gave good fit with the modeling points, with an adjusted r <dig> that was always very close to  <dig> . in addition, all regression lines were strikingly parallel as judged by their slopes:  <dig>  ±  <dig>  . the reason for not considering p >  <dig> was that above this number we empirically revealed a poorer modeling quality in terms of correlation coefficients , most likely due to the decrease of the number of data points contained in each partition.

eight data sets with different percentages of deg were constructed from the latin square data set by keeping the  <dig> known spiked-in probe sets, but randomly removing increasing amounts of the remaining probe sets reaching the total indicated in the first column. the null distributions of the plgem-stn statistics were evaluated through the classic permutation strategy either including or excluding deg. a wide range of significance levels was used to select deg and correlation between the fpr and the significance level or between estimated and observed fdr was evaluated through linear regressions in log-log plots. table reports slopes, intercepts and adjusted r <dig> of linear models. see text for details on estimation of fdr.

as a straightforward application of this modeling method, plgem could be fitted at the 50th-percentile to obtain a central tendency of standard deviation to be used for improving test statistics . another application of this method could be to fit plgem at the 5th- and at the 95th-percentile of standard deviation, to consequently find the limits of the corresponding 90% empirical confidence interval of standard deviation.

in order to verify the feasibility of the former application, fitting of plgem on real-life data as well as distribution properties of the random variable e were investigated by analyzing the residuals of the model, i.e. differences between observed and expected values:



improved test-statistics for detecting differential expression
in order to identify deg, we implemented the following general algorithm derived from the framework of statistical hypothesis testing, in which we test against the null hypothesis of non-differential expression. first of all, we chose to implement as the test statistic the signal-to-noise ratio  already used by golub et al.  <cit> , because it explicitly takes unequal variances into account and because it penalizes genes that have higher variance in each class more than those genes that have a high variance in one class and a low variance in another  <cit> :



where in the original version  and  represent, respectively, the mean of the replicated expression measures of gene g in condition  <dig> and  <dig>  whereas  and  are the corresponding standard deviations. instead, we propose to use model-derived standard deviation estimates predicted by plgem in equation  <dig> for the corresponding signal mean, rather than data-derived standard deviation values calculated independently from the few data points that are usually available for every single gene.

the improvement of the test statistic in ranking deg was evaluated as done by broberg  <cit>  through receiver operator characteristic  plots on the hg-u133a latin square data set, where there is an a priori knowledge on the truly differentially expressed transcripts. roc plots investigate the relationship between false positive rates  and false negative rates  at different significance levels; in this way the performance of the plgem-derived stn statistic  has been compared with the original stn statistic  and the statistic implemented in the commonly accepted significance analysis of microarrays  deg identification method . to this purpose exp <dig> of the latin square was taken as the baseline to which the remaining  <dig> experiments were compared. for each comparison absolute values of each statistic were ranked in decreasing order and first n genes selected . figure  <dig> summarizes results only for the most informative comparisons, but in each tested comparison analysis plgem-stn was at least as good as the other two statistics for each tested value of n . in addition, the roc curve of plgem-stn always had the shortest distance from origin, indicating that it resulted in the best trade-off between sensitivity and specificity. interestingly, improved sensitivity was observed especially when the nominal fold change was particularly low .

apart from discriminating between significant and not significant gene expression changes, an optimal test-statistic should additionally provide an accurate quantification of the actual degree of differential expression. figure  <dig> shows that plgem-stn outperforms the competing statistics in correlating the value of the statistic with the nominal concentration variation of the known latin square deg; this was particularly true for the most extreme variations.

identification of differentially expressed genes
a resampling-based method for estimating the null distribution
though ranking of genes based on the absolute value of their test-statistic has been proven to be an effective method for selecting deg, an even more useful way would be to compare the observed statistic with its null distribution , in order to control the fpr.

a classic approach to empirically obtain the null distribution of a test-statistic is running a series of random permutations of the chip indexes of the full data set and re-computing the test-statistics at each permutation. permutated test-statistics can then be pooled and significance thresholds  are found as specific quantiles of the null distribution.

nevertheless, we can foresee that the classic permutation strategy may not be optimal for estimating the actual fpr when the test-statistic makes use of a global error model such as plgem. we can in fact hypothesize that measurement spread of deg may not be accurately described by means of a global error model that was designed to describe signal variability in absence of differential expression. to test this hypothesis we compared the correlation between the expected significance level and the observed fpr using plgem-stn and the classic permutation strategy either including or excluding deg during the permutation step. to this end, data sets containing different percentages of deg were obtained by merging the  <dig> known deg of the latin square data set with differently sized random samples of not deg extracted from the same data set. as predicted, the presence of deg during the permutation step caused the significance level to be less correlated with the observed fpr and this correlation worsened with increasing percentages of deg . this lack of correlation was dramatically amplified when expected and observed numbers of false positives were divided by the number of selected genes to obtain an oversimplified estimate of the false discovery rate  and the observed fdr. conversely, when deg were omitted during the permutation step the correlation between estimated and observed fpr or estimated and observed fdr was sensibly higher for each tested percentage of deg. we hereby by no means claim that this fdr estimate is the most accurate. a more appropriate relationship between fpr and fdr can be found in the paper by storey and tibshirani  <cit> . nevertheless, the explicit control of the fdr goes beyond the scope of the present paper.

since in real-life data sets true deg are unknown in advance, we propose the following resampling-based method to obtain the null distribution of not deg when comparing n <dig> replicates of condition a with n <dig> replicates of condition b:

• artificial condition a* is obtained by randomly sampling with replacement n <dig> indexes corresponding to the replicates of only one experimental condition. if available, chose the condition with the highest number of replicates;

• similarly sample n <dig> values from the same set to obtain indexes of artificial condition b*;

• compute resampled test-statistics between a* and b* at each cycle.

the previous resampling should be repeated a sufficiently large number of times – as large as possible compared to the total number of possible combinations and compatibly with available computational resources – and the resampled test-statistics finally pooled. in our opinion resampling the expression values from only one experimental condition, rather than permutating indexes of both conditions, makes more sense with this particular statistic, because in this way we avoid merging true and false null hypothesis. note that when more than one condition  are to be compared to a common baseline, the distribution of resampled test-statistics needs to be determined only once, obviously providing a computational advantage. as a test of substantial equivalence between this resampling method and the classic permutation strategy , we compared the distribution of the permutated and of the resampled plgem-stn test-statistics in q-q plots. the distribution of the plgem-stn resampled from exp <dig> of the latin square data set was almost identical with the distributions of permutated plgem-stn obtained with the classic strategy from each comparison with the remaining  <dig> experimental conditions . figure  <dig> shows that the quantiles of the resampled plgem-stn values have a good concordance with the mean quantiles of the classically permutated statistics averaged over the  <dig> comparisons, implying that no differences are expected also in the gene selection step.

in accordance with the previous observations, the roc curve of the resampling method applied to the plgem-stn statistic was not significantly different from the roc curve of the classic permutation strategy  applied to the same statistic on the latin square data set . conversely, roc curves of the classic permutation strategy  applied to the classic-stn statistic and of the sam method gave poorer performance similarly to the results in figure  <dig> .

increased robustness to varying number of replicates
another appealing feature of an optimal deg identification method is that it should provide consistent results when different replicates of a same data set or different numbers thereof are analyzed. we therefore compared the performance of our resampling approach applied to the plgem-stn statistic  with sam  and with the classic permutation strategy applied to the classic-stn statistic . the number of available replicates for each experimental condition in the latin square data set was unfortunately too small to investigate this particular task. we therefore took advantage of the 16idc+lps data set, where the first sixteen columns can be considered as the baseline condition for the remaining four experimental replicates. we then constructed a series of reduced data sets in which the baseline columns were kept constant while all possible combinations of  <dig>   <dig> or  <dig> replicates of lps-stimulated dc were systematically deleted from the 16idc+lps data set, reaching a total of fifteen distinct data sets including the original one. since methods  <dig> and  <dig> are not applicable on the four reduced data sets containing single samples for the lps experimental condition, only the eleven data sets with at least two replicates were used for comparison purposes. since the sixteen baseline columns are identical in each reduced data set, plgem parameters were determined only once on this common baseline condition. significance levels used by each method in all eleven data sets were empirically selected in order to achieve a similar number of significant genes  in the full data set, i.e. the one containing all available replicates. thus, for each method eleven lists of identified deg were obtained and the consistency between these lists was evaluated by counting the number of times each probe set was selected, giving a probes set count between  <dig> and  <dig>  in figure  <dig> we compared the three distinct cumulative frequency curves for each method, which show the percentage of identified deg that were selected at least a given number times. while method  <dig> and  <dig> gave similar results, the method proposed in the present work identified a larger number of probe sets in a larger number of lists.

we finally evaluated the possibility of applying our method also to data sets where one of the experimental conditions was investigated only with a single sample without replication. to this end, we used the remaining four reduced data sets that could not be used in the previous comparison. in this case, the same plgem parameters derived from the sixteen baseline columns were applied to each of the single lps-treated dc sample to obtain an estimate of standard deviation associated to each gene expression value, treated here as if it was a mean value from a larger group of values. interestingly, when results obtained through this procedure were compared to the previously described results a comparable number of deg was identified and only one probe set was newly detected in comparison to the previously identified ones , arguing for a good consistency of results.

discussion
plgem accurately describes genechip data variability
in the present work we described a new global error model for microarray gene expression data that describes measurement variability with the same degree of accuracy over the whole dynamic range of values and that can be fitted at any desired quantile of spread. plgem has proven to correctly model signal standard deviation, in spite of the presence of different sources of variability, e.g. biological variability as well as the use of different target preparation protocols or of different chips. moreover, plgem has shown to be able to deal with the great variability that exists at low expression levels while at the same time considering the significant relative reproducibility of highly expressed genes. previously proposed error models assumed that measurement spread depended on signal location following different mathematical relationships, but none of them was based on a power law thus far. analysis of the residuals showed a good fit of plgem to a number of high-density oligonucleotide microarray data sets, with model parameters being very similar to each other even when dealing with rna samples coming from completely different biological sources and analyzed on different array layouts. this suggests that plgem could represent a general affymetrix genechip measurement noise model. even though scaled mas <dig> signals gave satisfactory modeling results, a further improvement could be achieved by using other emerging gene expression indices  <cit>  or more sophisticated normalization techniques, e.g. quantile normalization  <cit> . interestingly, if the same evaluation of sensitivity vs. specificity using roc plots on the latin square data set was done using gcrma expression values  <cit> , the results were even more striking than using mas <dig> signals . further studies will be needed to assess if plgem is also able to deal with data coming from microarray technologies others than affymetrix genechips.

interestingly, model parameter β was found to be quite stable and comprised between  <dig> and  <dig> in all analyzed data sets. it is noteworthy that for β ∈  absolute variability increases with growing expression values, while relative variability decreases . on the other hand, none of the models mentioned in the background section seem to agree with these experimental observations. formal statistical reasoning could unravel the underlying theoretical error model that leads to the power law relationship that was observed to be at the basis of the variance versus mean dependence in replicated microarray data.

a plgem-based method successfully detects differential expression
in spite of the lack of a theoretical statistical model, the empirical model presented here has proven its applicability in the identification of deg, providing improved results under a wide range of different testing conditions. in comparison to other commonly used deg identification methods, the proposed approach demonstrated improved specificity and sensitivity on the latin square data set and robustness to decreasing number of replicates on the 16idc+lps data set. the good performance of our proposed method is reasonably due to the fact that it relies on a global error model. as an example, when the classic permutation strategy is applied to the classic-stn statistic or when the sam method is used, the selected genes are apparently more dependent on the number and identity of the replicates than when our proposed approach is used. we hypothesize that, when no error model is assumed and a small number of replicates is present in the data set, the probability of observing for some genes coincidently very similar  values increases, thus leading to an underestimation  of the standard deviation and a consequent overestimation  of the test statistic, finally leading to false positives .

interestingly, when the performance of our method was compared on a data set of dc stimulated for  <dig> hours with lps, sam showed a decreased sensitivity in identifying down-regulated genes when the number of lps replicates was low . under these experimental conditions dc undergo a process known as maturation, which is a specialized form of cellular differentiation, for which both up- and down-regulation of gene expression is expected  <cit> . we speculate that sam did not select these genes, because of the combination of two effects. first of all, down-regulated genes are expected to have lower and therefore intrinsically more variable expression values in the four lps replicates than in the sixteen replicates of immature dc. when, in addition, the number of lps replicates becomes too low, sam filters these genes out to control the fdr. in agreement with this hypothesis sam was perfectly able to identify down-regulation when the full data set was used .

the gene selection method proposed in the present work does not provide a direct control on the fdr, but the significance level has been proven to be a direct estimate of the fpr. thus, if a significance level of  <dig>  is used and  <dig> probe sets are displayed on the mg-u74av <dig> chip, 12– <dig> genes are expected to be selected by chance in cases where all genes are in fact not differentially expressed. therefore, a researcher can test how many genes would be selected over a range of different significance levels and chose the one that results in the most acceptable compromise between number of selected genes and estimated fpr.

CONCLUSIONS
the proposed deg identification method provides a direct control of the fpr and an indirect control of the fdr. moreover, as tested on the latin square data set, our method improved the specificity vs. sensitivity trade-off in comparison to other commonly applied deg selection techniques. it finally showed an increased robustness when different replicates or numbers thereof are analyzed, giving consistent results even in data sets containing single samples. in conclusion, the global error model presented here may facilitate the analysis of microarray gene expression data by discriminating information from noise, and thus possibly helping the formulation of new hypothesis concerning gene functions.

