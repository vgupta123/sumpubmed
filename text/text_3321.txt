BACKGROUND
optical recordings with calcium-sensitive, fluorescent dyes can be used to measure insect brain activity. in particular, we address questions regarding the olfactory system of honeybees. across organisms, odours are encoded by activity patterns  <cit>  in dedicated brain regions, such as the olfactory bulb in mammals, or the insect antennal lobe . the al of the honeybee apis mellifera provides a 160-dimensional coding space, where a particular odour elicits a characteristic response pattern across the  <dig> glomeruli, the functional units of the al  <cit> . figure 1a shows an anatomical model of the honeybee al.

recording many odour response patterns gives rise to the species' olfactome, the entire odour coding space that defines odour similarity and dissimilarity according to the species' sensory input . apart from recording odour response patterns, observing the honeybee al allows us to answer questions about odour learning and memory. honeybees can be conditioned to associate an odour with a  reward, and this can lead to changes in the odour response pattern after learning the association with the reward  <cit> . even in the absence of odour stimulation, reverberations of past odour response patterns can be detected  <cit> , suggesting a role in short-term memory.

glomerular odour response patterns can be recorded by imaging with calcium-sensitive fluorescent dyes  <cit>  <cit>  <cit> . for this work, we used the dye fura2-dextran to record from the honeybee al. after exciting the dye with light at a frequencies  <dig> nm and  <dig> nm, a ccd camera on top of a confocal microscope recorded sample fluorescence. fluorescence changes are proportional to the changes in intracellular calcium, a proxy signal for neuron firing rate  <cit> . for an example, see figure 1b: the input for data processing consists of calcium imaging movies of brain activity recorded with a temporal resolution of 4- <dig> hz. in the movies, a subset of the  <dig> glomeruli  can be observed.

during the experiments used for this work, several movies recorded from the same bee were concatentated. movies were either recordings of odour responses or recordings of periods without odour stimulation. as glomeruli have characteristic odour responses, stimulation with different odours decorrelates the glomerular signals. as glomeruli also have individual spontaneous background activity in the idle state, also recordings without odour stimulation contribute to decorrelating glomerular signals. correlations between pixels  from the same glomerulus, and the fact that pixels from different glomeruli are  uncorrelated, is the basis for detecting glomerular signals and for extracting them from the imaging movies.

motivation and outline
imaging with calcium-sensitive dyes allows us to record the signals of many glomeruli simultaneously, gaining access to the glomerular odour response patterns. before evaluating the response patterns, the challenge for data analysis is to detect glomerulus positions in the movies and to accurately estimate their signals.

in order to provide a solid foundation for data analysis and to enable automatic processing of imaging movies, we have developed a plugin, imagebee, for the data analysis framework knime  <cit> . the graphical user interface of knime facilitates construction of data pipelines or workflows, where algorithms are embedded in a chain of pre- and postprocessing steps. each step is implemented by a module  in the knime workflow. imagebee is a flexible platform for the analysis of imaging data from the honeybee brain. due to the modular principle of knime, it can be easily extended or adapted to new kinds of data from emerging technologies such as two-photon microsopy  <cit> .

analysis of calcium imaging movies has traditionally been performed semi-automatically, using software to preprocess data, to visualise correlations between time series, and then to select regions of interest, that correspond to the glomeruli, by visual inspection  <cit>  <cit> . in contrast, we present an algorithm, along with an implementation for knime/imagebee, that automatically extracts glomerular signals from calcium imaging movies of the honeybee al.

similar to concepts from remote sensing  <cit>  <cit> , an imaging movie can be described by a non-negative mixture model. pixels  are assumed to contain either pure glomerular signals, or, in regions of contact, the additive mixture of one or more glomerulus signals. this is motivation for a convex analysis approach that aims to select the generating extreme vectors of a convex cone containing the data. these correspond to the pure glomerulus signals that can be combined, with non-negative coefficients, to reconstruct also the mixtures.

in the following, we first review related work and then introduce the algorithm , followed by a description of the imagebee plugin, and evaluation of the algorithm on biological, as well as on artificial data .

relationship to the conference version
this article is an extended journal version of the conference paper that appeared in the proceedings of iccabs  <dig>  <cit> . in the conference version, we developed the main algorithm for processing imaging movies. the journal version introduces the imagebee plugin for knime that contains implementations of the methods described in this work, as well as a variety of tools for handling, processing and visualising imaging data. furthermore, the journal version provides additional details on the algorithm and points out relationships to other methods.

related work
data analysis for calcium imaging movies is still dominated by semi-manual methods that involve user interaction to identify glomerulus positions .

algorithmic solutions can be classified as synthetic or analytic. in a synthetic approach, stetter et al.  <cit>  defined nonlinear model functions, e.g. functions describing odour response and dye bleaching. they reconstructed honeybee imaging data by estimating the contribution of each of the model functions to time series from the movie. this approach does, however, not separate glomeruli.

analytic approaches attempt to decompose the movie matrix into components that correspond to signals or latent factors underlying the data. reidl et al.  <cit>  and strauch&galizia  <cit>  applied independent component analysis  to imaging movies. while ica is able to decompose imaging movies into glomerular signals, these approaches are adaptations of a general paradigm to imaging data. they rely on statistical model assumptions - independence and non-gaussianity for all but one of the source signals, and do not consider non-negativity.

in contrast, we propose a data-specific mixture model that avoids these strict statistical model assumptions and incorporates a non-negativity constraint that proves beneficial with respect to interpretability of the factors. based on the non-negative mixture model, we then develop an algorithm that identifies the pure glomerular signals as the extreme vectors of a convex cone that contains the data .

the convex analysis approach has previously found application in remote sensing and the analysis of hyperspectral data  <cit> . in particular, ifarraguerri&chang  <cit>  and gruninger et al.  <cit>  have proposed related algorithms that also aim at finding a convex cone containing the data. for remote sensing applications, the goal is to identify so-called endmembers, the signals of pure materials or soil types, that can be used to unmix the signal of a given pixel, decomposing it into contributions by the pure materials. our algorithm utilises a convex cone approach to find and to select the pure glomerular signals, which are then postprocessed to remove residual noise. mixed-signal pixels are not unmixed, but discarded, as only the pure glomerular signals are relevant features for the analysis of glomerular odour response patterns. another strategy to distinguish glomeruli based on their differential activity would be to select pixels that are cluster centers. however, clustering algorithms may group pure and mixed-signal pixels from the same glomerulus into one cluster, or they may result in overclustering, creating additional clusters for the signal mixtures. employing the mixture model and the convex cone approach is what renders our algorithm robust against selecting mixed-signal features.

our algorithm can be seen as a feature selection approach that selects a subset of pixels . the approach is, however, unsupervised, and the term "feature selection" is often used in a supervised context, where features are sought that improve classification success  <cit> . our algorithm utilises signals selected from the movie matrix as basis vectors in a matrix factorisation framework. in this respect, it can be understood as performing a column-based matrix factorisation of the kind a ≈ cx, where the m × n matrix a is approximated with a subset of its columns in c  that are combined by  coefficients in x . column-based matrix factorisation approaches  <cit>  focus on selecting the most relevant vectors from a matrix to use them as interpretable basis vectors, as opposed to generating features by linear combination, such as in pca and ica.

methods
matrix factorisation framework for imaging movies
an imaging movie can be represented as a m × n matrix a, where m is the number of time points and n the number of pixels. the movies used for evaluation in this paper have  <dig> ×  <dig> pixels and about  <dig>  time points, where both numbers could be higher in theory. they depend on the chosen resolution of the recording and e.g. on the number of odour stimulations.

we can factorise a as follows, using only k factors, giving rise to the approximated ak:

  am×n:ak=tm×ksk×n= ∑r=1ktirsrj 

while being an approximation, akshould still be similar to the original matrix a in the sense that the frobenius norm error ∥a-ak∥fr is small. in equation  <dig>  matrix t has a temporal interpretation as it contains k time series, and matrix s has a spatial interpretation as it contains k images.

regarding notation, aijis the jth column of a, i.e. the jth pixel or pixel time series. the rows of s are denoted as s and the columns of t as t.

with respect to minimising ∥a-ak∥fr, the optimal solution is given by the principal components of a. for the sake of interpretability, we will introduce further constraints and demand that t should be restricted to columns selected from a , and that s should be non-negative. the ideal factorisation should yield the glomerular signals in the columns of t, and images describing the position of the glomeruli in the rows of s. as we will deal with both cases, a principal component solution, and a column-based solution, we reserve k for the number of principal components and denote the principal component solution as ak. we use c to refer to the number of columns selected from a, and acfor the column-based solution.

in the following, we describe an algorithm for computing a matrix factorisation with the desired properties. the algorithm can be structured into three main steps:

 <dig>  preprocessing by z-score normalisation and pca.

 <dig>  convex cone fitting .

 <dig>  postprocessing to remove residual noise.

depending on the nature of the data, additional preprocessing steps may be added, such as spatial filtering of the images or correction for animal movement.

preprocessing with pca
for each aij, we computed the mean µjand the standard deviation σj, and then performed z-score normalisation as aij:= /σj.

we then performed principal component analysis   <cit> , computing the top-k principal components of a. in the notation from equation , matrix s contains the k principal component images, and the corresponding loadings are in matrix t. similarly, computing pca on at would give rise to principal component time series.

fixing k and denoting as pkthe matrix of the top-k principal components, pca minimises ∥pktpk-ata∥fr. thus, pca preserves the covariance structure in the movie, such as the covariance between pixels that belong to the same glomerulus. optimal preservation of the covariance structure and optimal reduction of the frobenius norm error ensure a good approximation to a by pca. dimensionality reduction by pca allows any subsequent algorithm to be carried out on a much smaller matrix. the principal component images in s  illustrate another important aspect of pca: signals are accumulated in the top principal components. the  principal component is the variance-maximising projection  <cit> . transient signals, such as glomerular odour responses or spontaneous activity, contribute to the variance. glomerular signals are thus concentrated in the top principal components with high eigenvalues, allowing to discard components with lower eigenvalues that contain mostly noise .

both preprocessing steps, z-score normalisation and pca, help to increase the prominence of glomerular signals in the movies. due to its optimality properties, pca is of general value. z-score normalisation proved beneficial on the fura2-dextran recordings used in this work, but it may not be as useful for recordings with other dyes that exhibit stronger bleaching when exposed to light.

convex cone fitting
the movie matrix a can be described by the following data model:

  a=ts0++n 

a set of time series in the columns of t can be combined with non-negative coefficients in s0+ to reconstruct a up to the residual noise n. each time series from a is either represented by one of the basis time series in t, or it can be modelled as a combination of several time series in t with non-negative coefficients. the model assumption is that we can observe pure signal sources in the middle of the glomeruli, whereas at the fringes of the glomeruli light scatter from neighbouring glomeruli can lead to additive signal mixtures. ideally, the pure signal sources should be selected into t, and mixtures can then be modelled with the help of the coefficients in s0+.

for the moment, we omit the noise term n, which is dealt with in postprocessing . we assume that the pure signal sources are present in a, and we will use concepts from convex analysis to find them.

we first introduce definitions: a set of vectors v is called a convex cone if α1v <dig> + α2v <dig> ∈ v, where α <dig>  α <dig> are non-negative and v <dig>  v <dig> ∈ v. linear combinations with non-negative coefficients are also called "conic combinations". by definition, the extreme vectors of a convex cone are those that cannot be reconstructed by conic combination. however, perfect reconstruction of all other vectors in v is possible by conic combination of the extreme vectors <cit>  <cit> .

returning to equation  <dig>  the columns of matrix t span a convex cone that is pointed at the origin. the cone encloses a part of a that can be reconstructed perfectly by conic combination of the columns of t, and the remaining data points are reconstructed  by projection to the closest point on the boundary of the cone. thus, if we choose the set of extreme vectors of a into t, the cone defined by t encloses all data points and we achieve perfect reconstruction. interestingly, the extreme vectors do not only guarantee us perfect reconstruction, but they also identify the pure signal sources, as, by the definition, the extreme vectors cannot be reconstructed by conic combination.

we thus propose cone_fitting  as a heuristic to find extreme vectors. the strategy is a greedy forward heuristic, selecting at each iteration the column that is least explained by conic combination of the columns selected so far. thereby, we make a locally optimal choice with respect to selecting an extreme vector.

during c iterations, r =  <dig>  . . . , c −  <dig> columns t are selected into t, and the current version of matrix a, a{r}, is downdated by removing the influence of t. in particular, we compute the corresponding spatial mapping s = att and then compute s0+ by projecting negative entries in s to zero. then, we downdate a by setting a{r+1}=a{r}-ts0+. the next column is chosen as the column with the highest euclidean norm in a{r+1}. this stepwise removal of the chosen column, and everything that can explained by it with conic combinations, works towards selecting a non-redundant set of vectors. as the iteration proceeds, the norm of mixed signal columns is reduced by the downdating, rendering them less likely to stand out as the highest norm column that is the candidate for the next extreme vector. the first vector t needs to be initialised. for example, we could choose the vector with the largest euclidean norm in a. for this work, we chose the column with the largest distance to a randomly selected column in order to obtain a vector from the hull, more explicitly avoiding to start the iteration with a mixed-signal column.

postprocessing to remove n
so far, we have not yet dealt with the noise term n . furthermore, the induced clustering does not yet distinguish pure signals from mixed signals in regions of contact with other glomeruli. in figure 4a, mixed signal pixels are discretised to the source signal with the strongest contribution. we employ a postprocessing step  to remove the residual noise, as well as the mixed signals. this

algorithm  <dig> = cone_fitting , c)

    for r =  <dig> to c −  <dig> do

        if  then

            initialisation of p: see main text

        end if

        t=aip{r}/∥aip{r}∥

        s=a{r}tt

        s0+=negative_to_zeros

        tir=t;srj=s0+

        a{r+1}=a{r}-ts0+ //form residual matrix

        p=argmaxp||aip{r+1}|| //index of next column

    end for

leads to refined signals, and, with respect to the induced clustering, to a map of pure glomerular signals .

algorithm  <dig> selects time series from the movie matrix, that are  pure source signals and not mixed with other source signals. the residual noise n can be averaged out. the more time series from the same glomerulus we can use for averaging, the better. however, we should avoid to include time series that are more similar to another source signal.

we project a onto t and then average over all time series that are closer to a given t ∈ t than to any other column of t. this is visualised in figure 4b: a is projected onto the first two column vectors t and t ∈ t. each dot corresponds to one time series, where t and t end up at the extremes of the two "arms". the postprocessing step consists of averaging over the coloured dots, those that are closer to t or t, respectively, than to any of the other t ∈ t. the averaged signals are denoted as t^ and t^, respectively.

for the row vectors s^ , this means that all pixel coefficients are set to zero that do not contribute to the average t^. the effects of the postprocessing step can then be visualised by the new induced clustering in figure 4c. white pixels do not participate in any average: they contain mixed signals that are not close enough to any of the pure signal sources. the coloured areas indicate the pixels which contribute to the average signal of the respective glomerulus. spatially contiguous clusters appear as a property of the data: neighbouring pixels from the same glomerulus are correlated over time. however, at no point does spatial contiguity enter as a criterion into the algorithm.

comparison to other methods
manual approaches to process imaging data  rely on the evaluation of correlations between time series: correlations between neighbouring pixels  are visualised, and then feature selection is performed manually after inspection of these visualisations. our method automatically extracts pure signal vectors, that are typically found in the middle of the glomeruli, and the corresponding images in matrix x  can be interpreted as the spatial distribution of correlation with the basis signals in t. while pca preserves the correlation structure in the data, different glomerular signals are not split up into different principal components, as it is accomplished by algorithm  <dig>  when using pca to factorise the movie a, the images in s  are dense, with many non-zero pixels, whereas the convex cone fitting in algorithm  <dig> results in sparse images with only few non-zero pixels . this illustrates the different concepts behind the two approaches: principal components are means , whereas we select extreme vectors. while pca is optimal with respect to reducing the frobenius norm error, it can be argued that the extreme vector solution is more interpretable: the time series vectors corresponding to the images in figure  <dig> can be interpreted as the signals of individual glomeruli, whereas this is not the case for the pca solution .

implementation for knime
the central part of our method is the cone_fitting procedure  that selects glomerular time series from an imaging movie. yet, image analysis often requires that data be channelled through a pipeline of consecutive processing steps. in this work, we employ z-score normalisation as a preprocessing step and pca for denoising and dimensionality reduction before performing cone_fitting. then, different steps may be taken: the user might wish to view a glomerulus map, the glomerular time series or a low-rank reconstruction of the movie matrix. ultimately, novel methods might be developed that improve a certain part of the pipeline, or a preprocessing step that proved beneficial for the data at hand might not be as useful for other data types, e.g. when a different recording technique is employed.

providing a user interface that is as flexible as it is easy to operate, we implemented our method for the open-access platform knime  <cit> . in a knime workflow, data processing pipelines can be arranged based on a modular principle where individual processing steps, such as pca, are available as a nodes. each node contains a java program and individual nodes can be connected to create a data pipeline . for implementation, we relied on the libraries provided by knime, as well as on the image processing plugin  for handling image data within knime. matrix operations were implemented using the cern colt  and parallel colt  <cit>  libraries.

we implemented knime nodes for all the preprocessing steps described in this paper and the cone_fitting algorithm, along with several other nodes for data visualisation, e.g. for creating a low-rank reconstruction of the original movie matrix and for visualising it using a false-colour scale. the modular architecture of knime ensures that the toolkit can be easily extended.

at any step in the pipeline, data can be written to files or taken over by other nodes available for knime that also features connections to imagej  <cit>  and r  <cit> . fixed processing pipelines can be summarised in meta nodes and loop nodes allow for repeated execution of the pipeline, e.g. by iterating over a list of files. we have bundled our knime nodes in a plugin, imagebee, that is available online. imagebee extends knime by the following nodes .

• vwsreader: reader for the till vision format.

• stabiliser: correction for animal movement/shifts between movies by cross correlation.

• normalisation: zscore, foldchange, histogramnormalisation, imagearithmetic: subtract/add other movies, e.g. a control measurement

• filters: spatialfilter, temporalfilter

• pca: samplingpca , ccipca 

• convexcone: algorithm  <dig> from this paper

• visualisation: colortable, applycolortable, showroom , overlaymap 

• matrix operations: matrixmultiplication, backproject , timeseriesprojector 

• helper nodes: imageextractor, splitmovies, mergemovies

example pipeline in knime
for illustration of the data pipelining concept, refer to figure  <dig>  where we show a knime pipeline implementing the method described in this paper. imaging movies are read by a reader node . as the data is a concatentation of movies recorded intermittently in the same bee , the individual movies might be slightly shifted due to animal movement or changes in experimental setup. if necessary, the stabiliser node aligns subsequent movies optimally by cross correlation.

then, the data flow is split into two pipelines, where the upper pipeline computes a glomerular map and the lower pipeline computes glomerular time series. for computing time series, the movie is only treated with foldchange that is configured to normalise time series by subtracting the mean during the interval before odour stimulation, thus highlighting changes due to the stimulation.

as algorithm  <dig> is performed in pca space where the time dimension is reduced to k, the output of convexcone is the spatial matrix s , and not the time series matrix t . for the knime implementation  we thus employ the node backproject that computes the full-length matrix t by projecting the movie matrix a onto s. time series are then written to a text file for further analysis, e.g. with a statistics software. by default, convexcone performs signal refinement to remove residual noise, i.e. the procedure from the knime pipeline in figure  <dig> is equivalent to computing matrices t ^ and  Ŝ .

in the upper pipeline, a glomerular map is constructed as described above by first z-score normalising the movie , applying pca and algorithm  <dig> . here, the samplingpca node is configured to compute exact pca. approximations to pca that can lead to considerable speedups are discussed below.

availability
the imagebee plugin for knime  is available online via the update site for knime community contributions: http://tech.knime.org/update/community-contributions/nightly. please install imagebee and the knime image processing plugin . detailed installation instructions, downloadable knime pipelines , and an example dataset are available at http://tech.knime.org/imagebee-analysing-imaging-data-from-the-honeybee-brain.

RESULTS
computational complexity
computational complexity of algorithm  <dig> is dominated by forming the m × n residual matrix c times. after dimensionality reduction with pca, the computational effort for performing algorithm  <dig> on a small matrix is negligible. the computational load of the entire method depends mainly on preprocessing, and in particular on pca.

in our knime plugin, we provide three algorithms for computing pca. the samplingpca node implements an iterative pca approach .

optionally, a speedup can be achieved by pixel importance sampling. for example, by sampling 5% of the pixels based on a biologically motivated importance criterion  <cit> , running time can be reduced to about 5% of the running time for exact pca. we have previously shown  <cit>  that pca with pixel sampling leads to high quality approximations to the principal components on imaging data.

another variant is an incremental pca approach, ccipca  <cit> . the ccipca algorithm finds an approximate solution to pca by incrementally processing a movie stream. it is a memory-efficient way of computing pca, as, at any given time point, only the current image from the movie stream and the principal components are kept in memory.

iterative pca has a complexity of o where m and n are the movie dimensions, k is the number of principal components and i the number of iterations until convergence. the pixel sampling approach can substantially decrease the number of pixels n and thereby complexity of iterative pca. incremental pca with the ccipca algorithm does not reduce n, but it saves the factor i as no convergence is involved, leading to a complexity of o <cit> .

parameter settings
two parameters need to be set, the number of principal components k and the number of time series/basis signals c for cone_fitting . in both cases, default values  can be employed for typical calcium imaging recordings of the honeybee al .

for very large k we would sacrifice the dimensionality reduction aspect of pca, while information would be lost by choosing a very small k. from the typical dataset in figure  <dig> it is apparent that values of k around  <dig> are a good compromise in this respect.

regarding c it should be noted that cone_fitting returns nested basis vectors, i.e. given a fixed starting point, the nth basis vector remains the same, regardless of the size of c. this is different from a common behaviour for clustering algorithms where cluster centers move when the number of clusters is changed. as we know that there are between  <dig> and  <dig> glomeruli that can be visible at the most , setting c to a value in this range is a reasonable choice. choosing a slightly larger c accounts for the presence of non-glomerular objects in the movies that also have their own signals.

while, in principle, c is an uncritical parameter, it can become relevant for signal refinement in postprocessing . for large c, the distance to the closest basis signal tends to decrease, simply because there are more basis signals. this leads to averages over a smaller amount of time series  and thus to smaller clusters . thus, especially for recordings with a small number of glomeruli it can be helpful to lower c in order to increase cluster size.

evaluation on artificial data
for evaluation, we constructed artificial datasets with  <dig> known source signals each. the biological datasets used in this work contain both recordings of odour responses and of spontaneous background activity in the idle state. generally, odour response recordings have high peak amplitudes directly after odour presentation, while spontaneous activity has lower amplitudes.

to check for possible performance differences between the data types, we used two artificial datasets, "odours" and "idle". for both datasets, source signals were derived from real measurements , i.e. signals were distinct, but not perfectly uncorrelated. for both artificial datasets, source signals  were assigned to spatially contiguous, partially overlapping clusters . in regions of overlap, signals were additive mixtures. finally, gaussian noise  was added .

we then measured how well the implanted source signals could be recovered from the artificial datasets for varying noise levels σ. source recovery is expressed by a correlation score based on the pearson correlation coefficient ρ between time series vectors x and y. for a recovered signal t⌢ and known source signal u, the correlation score is defined as:

  corr=1n ∑i=1nargmaxjρ,u) 

we performed algorithm  <dig>  on both artificial datasets. across a range of noise levels from σ =  <dig>  to σ =  <dig>  sources could be successfully recovered on both datasets . a sharp decline in source recovery occurred for σ >  <dig>  which, however, could be remedied by spatial filtering. in order to reduce noise, we smoothed images with a gaussian kernel . with the additional spatial filtering, source recovery was possible even for higher noise levels up to σ =  <dig> 

source recovery was similar on both data types. for high noise levels, in particular on spatially filtered data , performance was slightly better for the "odours" dataset than it was for the "idle" dataset. this can be explained by the clearer signals during odour responses, as opposed to low amplitude fluctuations in spontaneous activity that are more susceptible to high noise levels.

evaluation on biological data
finally we tested the imagebee plugin on imaging movies from the honeybee al  that contained measurements of both odour responses and spontaneous background activity. for each imaging movie, we performed algorithm  <dig> with pca preprocessing and followed by spatial filtering. as a result, we obtained for each movie a time series matrix t ^, an image matrix  Ŝ, and a glomerular map. figure  <dig> shows low-rank reconstructions of imaging movies, computed as ac=t ^Ŝ and displayed in a false-colour scale. this visualises the response pattern sequence after odour stimulation. for raw images, see figure 7a. low-rank reconstructed images are shown in figure 7b  and figure 7c . in both cases, the same odour response has been measured twice, giving an impression of inter-trial variability. while there is variability in the glomerular odour responses, both within and between animals, stable odour representations can be obtained by averaging over many responses. in fact, odour responses in the honeybee are sufficiently characteristic to speak of a species-specific olfactory code  <cit> .

based on the glomerular maps, we could determine the identity of landmark glomeruli using an anatomical model of the honeybee al  <cit>  . figure 8a shows glomerular maps from two different bees. the number of glomeruli in a map depends on several experimental parameters, e.g. the focal plane of the recording. despite experimental and also biological variation in individual bees, al anatomy is partly conserved between bees .

this allows for comparing odour response time series  for glomeruli that have been identified based on their position in the glomerular maps. figure 8b shows time series for three different glomerulus types in response to the odour nonanol. time series in figure 8b are pooled data from several animals: two odour responses were measured in each of three different bees. despite some variability, response amplitude and temporal dynamics of the glomerular odour responses are conserved, both within and between animals.

CONCLUSIONS
we have introduced the imagebee plugin for knime as a platform for analysing and visualising imaging data from the insect al. previously, data analysis in this field required manual selection of regions of interest . the imagebee plugin enables automatic detection of glomerulus positions in calcium imaging movies , it automatically extracts glomerular time series , and it can be used to produce denoised versions of the imaging movies by low-rank reconstruction . at the core of the image processing pipelines that can be constructed with imagebee lies an algorithm  that is based on a data-specific mixture model and that leads to a factorisation of the movie matrix with interpretable basis vectors. these are either time series selected from the movie matrix, or, after postprocessing, combinations, with non-negative coefficients, of a limited number of similar time series vectors. this interpretability aspect is what distinguishes our method from more general approaches, such as pca, where basis vectors can be linear combinations of a large number of vectors with mixed-sign coefficients.

while the software has been designed for and tested on honeybee recordings with fura2-dextran, a large number of knime nodes for data pre- and postprocessing are available, such that it can be adapted to other kinds of data with similar properties, e.g. imaging movies from other insect species, recordings that rely on other calcium dyes or on other techniques, such as multi-photon microscopy. in all these cases, a so-called functional segmentation of the image plane can be achieved, a segmentation into units  with similar signals over time.

reproducible results by automated and deterministic processing, accurate estimation of glomerular time series, along with visualisation of the spatial aspect of odour response patterns by denoised movies, are the basis for analysing data on the physiology of olfactory coding, odour learning and memory.

competing interests
the authors declare that they have no competing interests.

authors' contributions
ms developed methods and wrote the manuscript. ms and cl performed programming. jr performed biological experiments. cgg supervised research and edited the manuscript. all authors read and approved the final manuscript.

