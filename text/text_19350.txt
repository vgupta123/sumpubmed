BACKGROUND
recent advances in the next-generation sequencing  methods allow for analyzing the unprecedented number of viral variants from infected patients and present a novel opportunity for understanding viral evolution, drug resistance and immune escape  <cit> . however, the increase in quantity of data had a detrimental effect on quality of reads. in the case of  <dig> gs-flx titanium pyrosequencing, the mean error rate is  <dig> % and the error-free haplotypes represent  <dig> % -  <dig> % of the total number of reads, depending on the read length  <cit> . originally, the emphasis was on obtaining the consensus sequence, provided that the depth of coverage easily allowed for retrieving the main true sequence and its most common polymorphisms irrespective of the suboptimal quality of numerous individual reads. however, analysis of viral amplicons is usually applied to biological tasks requiring in-depth characterization of viral populations and entails the examination of individual error-free reads rather than consensus sequences.

the main purpose of an error correction algorithm for viral amplicons is to discriminate between artifacts and actual sequences. this task becomes especially challenging when applied to recognizing and preserving low-frequency natural variants in viral population. currently, the most used error correction algorithms involve the clustering of reads  <cit> . pyronoise clusters the flowgrams using a distance measure that models sequencing noise  <cit> , whereas shorah clusters the reads in bayesian fashion using the dirichlet process mixture  <cit> . other approaches to error correction are based on the use of multiple sequence alignments  <cit>  and k-mers, or substrings of reads of a fixed length k  <cit> . k-mer based algorithms are efficient but rather time and memory consuming. additionally, these algorithms are prone to introduction of errors during the correction phase  <cit> . to overcome these disadvantages, the authors of edar algorithm  <cit>  developed an approach for the detection and deletion of sequence regions containing errors. however, although this error deletion approach is efficient for shotgun sequencing, it is unacceptable for treatment of short amplicon reads commonly analyzed in viral samples due to their lower k-mer diversity.

the aforementioned methods are optimized for shotgun analysis and assume that the errors introduced by sequencing are randomly distributed. however, a recent assessment of the accuracy and quality of amplicon reads obtained using 454-sequencing showed that the error rate is not randomly distributed; rather, it is strongly affected by the presence of homopolymers, position in the sequence, size of the sequence and spatial localization in pt plates  <cit> . these findings indicate that many of these sequencing errors are sequence specific and may variably contribute to accuracy of reads generated from amplicons of different sequences. more importantly, the accuracy of amplicon sequencing should be improved by incorporating the factors affecting the error rate into calibration of the error correction algorithms.

in this paper, we present two new efficient error correction algorithms optimized for viral amplicons. the first algorithm  includes a calibration step using sequence reads from single-clone samples. et estimates an empirical frequency threshold for indels and haplotypes calculated from experimentally obtained clonal sequences, and also corrects homopolymer errors using sequence alignment. the second algorithm  does not need a calibration. it is based on the k-mer error correction. kec optimizes the edar algorithm  <cit>  for the detection of error regions in amplicons and adds a novel algorithm for error correction. performance of both algorithms was compared to the clustering algorithm shorah using  <dig> experimental amplicon datasets obtained by 454- sequencing.

methods
experimental sequence samples
a set of  <dig> plasmid clones with different hcv hvr <dig> sequences was obtained. all the clones contained the modified hcv jfh <dig> sequence . each of  <dig> hvr <dig> sequences was introduced into plasmid pjfh <dig> by two-step recombinant pcr using oligonucleotides encoding different hvr <dig> sequences, followed by digestion with a set of restriction endonuclease and ligation. plasmids were then electroporated into e. coli and purified with the qiagen miniprep kit. all constructs were verified by dna sequencing  using an automated sequencer . the average number of nucleotide differences among the  <dig> plasmid clones was  <dig>  .

a total of  <dig> samples of the plasmids were created, with  <dig> containing a single clone and  <dig> containing a mixture of clones in different concentrations . the junction e1/e <dig> region  was amplified using a nested pcr protocol  <cit> . briefly, all samples were amplified using the perfecta sybr fastmix chemistry  and a set of external primers. the amplicons generated during the first round pcr were used as templates for a nested pcr using hybrid primer composed of the  <dig> primer adaptors, multiple identifiers and specific sequences complementary to the hcv genome. this allowed for multiplexing and downstream pyrosequencing procedure. resulting amplicons were quantified using the picogreen kit . integrity of each fragment was evaluated using bioanalyzer  <dig> . pcr products were pooled and subjected to pyrosequencing using the gs flx titanium series amplicon kit in a 454/roche gs flx instrument. sequencing of the reverse strand was conducted using the amp primer b. the initial reads were processed by matching to the corresponding identifier. low quality reads were removed using the gs run processor v <dig>  . the  <dig> run was divided in  <dig> sectors, two of which were used in the current experiment, one sector with a pool of the mid-separated single-clone samples and one sector with a pool of the mid-separated mixture samples .

et algorithm
the main purpose of the procedure is to calculate the frequency of erroneous haplotypes in amplicon samples where a single haplotype is expected. the calculation of an accurate threshold is dependent on high-quality pairwise sequence alignments and proper correction of homopolymers. the procedure was carried out with matlab  <cit>  and involved the following steps:

 amplicon size limits: all reads smaller than 90% of the expected amplicon length are deleted and all reads bigger than 110% of the expected amplicon length are clipped. all different haplotypes and their frequencies are calculated, which saves considerable time and memory at the following steps.

 alignment to external references: each haplotype is aligned against a set of external references of all known genotypes. for each haplotype the best match of the external set is chosen. the aligned sequence is clipped to the size of the chosen external reference.

 alignment to internal references: the  <dig> most frequent haplotypes that do not create insertions or deletions in regard to the external reference are selected as the internal reference set. each haplotype in the dataset is aligned against each member of internal references set. for each haplotype the best match of the internal set is chosen.

 homopolymer correction: all homopolymers of  <dig> or more nucleotides are identified. if the homopolymer region includes an insertion, the nucleotide is removed. if the homopolymer includes a deletion, the gap is replaced by the missing nucleotide. then all different haplotypes and their frequencies are recalculated.

 outlier removal: all reads containing obvious pcr or sequencing artifacts are removed. using the internal reference, the number of indels in each haplotype is found. an outlier threshold is defined for each file, calculated as the weighted average of the number of indels +  <dig> standard deviations. if a haplotype contains a number of indels higher than the outlier threshold, the haplotype is removed.

 indel threshold: the read aligned to its reference is used to calculate the frequency of erroneous indels over all the  <dig> samples containing a single clone. an indel threshold is defined as the maximum frequency of erroneous indels . if a haplotype contains an indel with a frequency lower than the indel threshold, the haplotype is removed.

 haplotype error threshold: the frequency of erroneous haplotypes and its standard deviation is calculated over the  <dig> samples containing a single clone. a haplotype threshold was defined as the weighted average frequency of erroneous haplotypes +  <dig> standard deviations . all haplotypes with a frequency lower than the haplotype threshold are removed.

 removal of reads with ns: all haplotypes with ns are removed from the final file. this step was performed at the end rather than at the beginning to take advantage of the information that these reads provided regarding nucleotide frequencies at positions other than those with n.

kec algorithm
the scheme of kec includes  <dig> steps:

 calculate k-mers s and their frequencies kc . we assume that k-mers with high k-counts  are correct, while k-mers with low k-counts  contain errors.

 determine the threshold k-count , which distinguishes solid k-mers from weak k-mers.

 find error regions. the error region of the read is the segment  such that for every p ∈  the k-mer starting at the position p is considered weak.

 correct the errors in error regions.

let r = , ri∈{a, t, g, c} be the fixed read. denote by sk the k-mer of r starting at the position i and by kck the k-count of this k-mer. for an arbitrary sequence s let prefj  be the prefix of length j of s.

 calculating k-mers and k-counts
the unique reads r were stored together with their frequencies fr. the straightforward calculation of k-mers and k-counts is inefficient due to the usually large size of the data set. we use hash map, where each key is a k-mer s and the corresponding value is the array v = : s = sk in the read r). the hash map can be rapidly constructed even for very large data sets.

 finding the error threshold
the idea proposed in  <cit>  is used to find the error threshold. consider the distribution of frequencies of k-count values. let f be the frequency of the k-count value v. it is assumed, that k-counts of erroneous k-mers and correct k-mers follow different distributions . it was observed in  <cit> , that it is not necessary to explicitly consider the model for the distribution, because the first minimum of f satisfactorily separates different distributions, and therefore can be used as the error threshold. however, this approach often is not applicable to the amplicon data, because of the rather discrete distribution of k-count values than in the shotgun experiments. the first minimum of f is usually equal to  <dig> and corresponds to the gap in the distribution . we define the end of the first sufficiently long segment of the consecutive 0's of f as the error threshold ter. the length of the segment is the parameter of the algorithm.

 finding error regions
the error regions in every read are calculated as follows. we first sequentially find isolated segments  such that for every p ∈  kck ≤ ter. then the k-mers of the read are clustered according to their k-counts using clustering by the variable bandwidth mean-shift method  <cit> , as was proposed in  <cit> . we use the fast implementation fams  <cit>  of the variable bandwidth mean-shift method. finally, every segment is extended in both directions by adding consecutive positions q by the following rule: q is added if and only if there exists p ∈  such, that k-mers sk and sk belong to the same cluster. overlapping segments are joined, and the obtained segments are error regions.

 error correction
this stage consists of  <dig> steps:

 error correction in "short" error regions .

 error correction in "long" error regions .

 haplotypes reconstruction and postprocessing.

steps  and  could be used for any sequencing data and could be considered as the separate algorithm. stage  is designed for amplicon data.

 error correction in "short" error regions
 is based on the following principle: correct the error in the read r by finding the minimum-size set of insertions, deletions and replacements, which transform the r into read with all k-mers being solid. this problem could be precisely solved by the dynamic programming  <cit> , but this approach has two disadvantages: it is slow and the additional errors could be introduced. however, taking into account the homopolymer nature of errors and using the found error regions, the efficient and fast heuristics for the problem can be proposed.

we call error region x =  of a read r =  a tail, if either b =  <dig> or e = n-k+ <dig>  let l be the length of x, and hi denotes a sequence of i identical nucleotides w∈{a, t, g, c}  is a homopolymer).

let us introduce the following notation for different types of single-nucleotide errors: rep denotes replacement; ins <dig> -- insertion, which does not create a homopolymer; insp, p ≥  <dig>  -- insertion, which creates a homopolymer of length p; del <dig> -- deletion of an isolated nucleotide; delm -- deletion from the homopolymer of length m+ <dig>  m ≥  <dig>  the straightforward verification shows, that the following proposition holds:

lemma  <dig>  suppose, that the non-tail error region x =  of the read r was caused by a one-nucleotide error e. let w = re.

1) if e = rep, then l = k.

2) if e = insp,  <dig> ≤ p ≤ k, then l = k-p+ <dig> and if p ≥  <dig>  then x is followed by hp- <dig> 

3) if e = delm,  <dig> ≤ m ≤ k, then l = k-m- <dig> and if m ≥  <dig>  then x is followed by hm, where c≠w.

according to lemma  <dig>  errors corresponding to non-tail error regions with lengths ≤ k could be identified and corrected. if the error region x =  of a read r =  is a tail, then we delete from the read the suffix starting at the position b+k- <dig>  or the prefix ending in the position e . this is the tail cutting operation. so, the basic scheme of the first stage of the error correction algorithm is the following.

algorithm  <dig> 

1) consider every non-tail error region x =  with length not exceeding k of every read r = . we assume, that x was caused by single isolated error at the position e. taking into account the length of x and the sequence of nucleotides following it, identify the type of error using lemma  <dig>  if l < k- <dig>  then the type of error and its correction can be determined unambiguously. in the case of insertion remove re; in the case of deletion duplicate re, if it will introduce a solid k-mer. according to lemma  <dig> the cases l = k and l = k- <dig> contains ambiguities. if l = k, then x could be caused either by nucleotide replacement with  <dig> possible corrections or by simple nucleotide insertion. if l = k- <dig> and re = re+ <dig>  then x could be caused either by the insertion of the nucleotide re or by the deletion of the nucleotide c≠re between re and re+ <dig>  consider all possible corrections of the error and choose the correction, which introduce solid k-mer with the highest k-count.

2) cut tails, delete short reads, recalculate k-mers and error regions, delete reads covered for more than 40% with error regions. storing k-mers in the hash map allows to perform both steps 1) and 2) very fast.

3) repeat steps 1) and 2) until there are no error regions in the data set or the fixed number of iterations is reached.

algorithm  <dig> is heuristic. it assumes that errors in two consecutive nucleotides are extremely unlikely. there are elements of greedy strategy at different stages of the algorithm. nevertheless, the disadvantages of greedy algorithms are less detrimental in real data. for example, during the consideration of error regions with lengths k and finding the possible replacement of the nucleotide re, the existence of several different strong k-mers s with prefk- <dig> =   is possible. to find the replacement of re algorithm  <dig> will choose s with highest k-count, which is a greedy approach. however, the tests of algorithm  <dig> on the selection of  <dig> different data sets with hcv sequencing data shows, that for the overwhelming majority of error regions of length k the strong k-mer s satisfying  is unique. the percentage of such error regions varies from  <dig> to  <dig> % with average of  <dig> %.

 error correction in "long" error regions
the error regions with lengths greater than k likely correspond to the situations when two or more errors are separated by less than k positions. this situation is significantly less probable than the presence of a single error. in figure  <dig> the typical distribution of error regions lengths frequencies is illustrated. the number of "long" error regions is regulated by the parameter k. it should not be too small in order to obtain the more accurate boundary between strong and weak k-mers, and at the same time it should not be too large in order to better separate errors from each other. in our tests we used k =  <dig> 

however, a certain number of "long" error regions is inevitable. we can locate the possible error bases for the error region x = , len > k - it is the positions b+k- <dig> and e. however, for such error regions we lose the opportunity for prediction of the error type by combining their length and nucleotide sequence following it, because the length of error regions corresponding to the individual errors could not be determined.

there two ways to treat "long" error regions. one is to discard all reads with errors uncorrected by algorithm  <dig>  in this case, the error threshold and error regions are recalculated after finishing algorithm  <dig>  and reads containing error regions are discarded. the other way is to correct errors in "long" error regions. all possible errors at positions b+k- <dig> and e are considered to choose the correction procedure causing the introduction of k-mer with the highest k-count. since these corrections are less reliable than for "short" error regions, correction of "long" error regions is conducted at the end of the algorithm after correcting "short" error regions, and algorithm  <dig> is applied again before generating the final output of corrected reads. both approaches are implemented in kec, allowing users to exercise their preferences.

 haplotypes reconstruction and postprocessing
in the error-free data set of amplicon reads the collection of unique reads should be identical to the set of haplotypes, and the frequencies of unique reads should be proportional to the concentrations of haplotypes. errors result in the increasing number of unique reads and divergence between the frequencies and concentrations.

the steps  and  dramatically reduce the numbers of unique reads in the data set. the corresponding statistics is presented in table  <dig> for the data sets m1-m <dig>  a set of corrected reads usually contains many error-free reads, which are subsequences of real haplotypes. although such reads are not useful for finding true haplotypes, they are important for identifying haplotype frequencies. therefore, the row "after , " in table  <dig> presents the number of unique reads and unique maximal reads .

as illustrated in table  <dig>  some errors persist in the dataset after steps , . the small number of unique reads allows for using pairwise and multiple alignments to correct these errors. this idea is implemented in the following heuristic algorithm.

let r = {r <dig>  ..., rn} be a set of unique reads obtained after steps ,,  be the frequencies of these reads and rmax ⊆ r be a set of maximal reads of r. let ai, j =  <dig>  if the read rj is a subsequence of the read ri, and ai, j =  <dig>  otherwise, i, j =  <dig>  ..., n . let also dj be the number of reads, which contain rj as the subsequence. the basic scheme of the error correction and haplotypes reconstruction algorithm is the following.

algorithm  <dig> 

1) for every ri∈r set its initial concentration ci:=fi/∑i=1nfi

2) calculate set rmax. for every ri∈rmax recalculate its concentration by the following formula: ci:=∑j=1nai,jcjdj

3) calculate the multiple alignment of reads of rmax. identify homopolymer regions in the alignment. for every position of each homopolymer region calculate the total concentrations of reads with and without gap at this position. if both of these concentrations are non-zero and one of them is α times greater than the other, correct the position accordingly.

4) put r: = rmax and repeat 1) and 2). calculate neighbor joining tree t of reads from rmax based on their pairwise alignment score. let p be the set of pairs of reads having common parents in t. for every  ∈p consider pairwise alignment of ri and rj. identify homopolymer regions as in 3). correct the position of the homopolymer region if and only if either the nucleotide difference between ri and rj is  <dig> or their concentrations differ more than α times.

5) put r: = rmax and repeat 1)-4) until no corrections can be made.

for the datasets described in this paper, α =  <dig> was used. clustalw  <cit>  was used for calculation of alignments and neighbor joining trees.

algorithm comparison
et was implemented in matlab and kec in java. each sequence file was analyzed using et, kec and shorah error correction algorithms. shorah was applied several times using different parameters and the best attained results are reported here.

to evaluate performance of the three algorithms, nucleotide identity and frequency of the observed and expected true haplotypes were compared. before doing this, the outputs of the three algorithms were post-processed to assure a fair comparison. the true haplotypes expected in each sample were aligned with the observed haplotypes using needleman-wunsch global alignment. the true haplotype with the best score was considered to be the match for each haplotype and the gapped ends were clipped in both sequences. for each method and sample, the following measures were calculated:  missing true haplotypes, the number of expected haplotypes which were not found among the observed haplotypes;  false haplotypes, the number of observed haplotypes with indels or nucleotide substitutions;  root mean square error, the disparity between the expected and observed frequencies of haplotypes;  average hamming distance, the distance between an observed false haplotype and its closest match, averaged over all false haplotypes.

RESULTS
error profile of single-clone samples
the errors in reads of every single-clone sample were calculated by aligning each read with the corresponding clone sequence. the average percentage of error-free reads  in the single-clone samples was  <dig> % . the most common false haplotype was found with an average frequency of  <dig> % but could be as high as  <dig> % .

a minimum spanning tree  of a distance graph gdist of the dataset s <dig> illustrates the degree of sequence errors generated during 454-sequencing. this tree shows sequence relatedness of all unique haplotypes observed among the 454-reads of a single-clone sample. gdist is a complete weighted graph, the vertices of gdist are unique haplotypes, the weight of each edge h1h <dig> is the edit distance between h <dig> and h <dig>  most errors are found in low frequency haplotypes but homopolymer errors are usually found in high-frequency haplotypes.

although the total number of nucleotide errors is high, they occur in different positions, making the frequency of individual reads with a particular error very low. the case with homopolymers errors is different, the longer a homopolymer is, the higher its fraction of errors . small homopolymers  have high prevalence but low fraction of errors, whereas big homopolymers  have low prevalence but high fraction of errors.

algorithms comparison
mt: missing true haplotypes; fs: false haplotypes; rmse: root mean square error; hd: average hamming distance, averaged over all false haplotypes. shorah_all shows the raw results and shorah_1% the results based only on those haplotypes with a frequency > 1%.

all methods found the correct sequence in each single-clone sample . when mixtures were tested, all three algorithms were successful in identifying most of the true haplotypes, with et being the most sensitive. the major difference among algorithms is in the reported number of false haplotypes. et and kec reported a lower number of false haplotypes than shorah in every mixed sample .

the low root mean square error  between observed and expected frequencies of true haplotypes indicates that et and kec have a greater accuracy than shorah in single-clone samples. all three algorithms show equivalent results in the mixture samples . shorah is less accurate in identifying haplotype frequency owing to the large number of reported false haplotypes, presence of which reduces the observed frequency of the true haplotypes.

analysis of the hamming distance between false haplotypes and their closest match shows that false haplotypes retained by kec and et are genetically closer to true haplotypes than the ones retained by shorah .

the test results of all samples are summarized in table  <dig> 

discussion
hepatitis c virus  is a single-stranded rna virus belonging to the flaviviridae family  <cit> . hcv infects  <dig> % of the world's population and is a major cause of liver disease worldwide  <cit> . hcv is genetically very heterogeneous and classified into  <dig> genotypes and numerous subgenotypes  <cit> . the most studied hcv region is the hypervariable region  <dig>  located at amino acid  positions 384- <dig> in the structural protein e <dig>  sequence variation in hvr <dig> correlates with neutralization escape and is associated with viral persistence during chronic infection  <cit> . ngs methods allow for analyzing the unprecedented number of hvr <dig> sequence variants from infected patients and present a novel opportunity for understanding hcv evolution, drug resistance and immune escape  <cit> . most current methods are optimized for shotgun analysis and assume that the errors are randomly distributed. this assumption does not compromise the accuracy of shotgun sequencing as much as accuracy of amplicon sequencing. the sequencing error rate for amplicons is not randomly distributed  <cit>  and should vary among amplicons of different primary structure.

in addition, current error-correction algorithms report performance measures related to their ability of finding true sequences, rather than the number of false haplotypes  <cit> . however, the biological applications of viral amplicons necessitate the use of error-free individual reads. all three methods studied here could find the correct sequences in both single-clone and mixture samples but showed marked differences in detecting the frequencies of the true haplotypes and the number of false haplotypes. we found that both et and kec are suitable for rapid recovery of high quality haplotypes from reads obtained by 454-sequencing.

the highly non-random nature of 454-sequencing errors calls for internal controls tailored to the amplicon of interest. the error distribution of single-clone samples helped us to calibrate the et algorithm, thus facilitating its high accuracy in detection of true sequences in the hvr <dig> amplicons. et was successful in finding the correct set of haplotypes in all  <dig> mixtures and in  <dig> of  <dig> single-clone samples, while found one false haplotype in  <dig> single-clone samples. kec was correct for  <dig> of  <dig> single-clone samples  and for  <dig> of  <dig> mixtures , having also the advantage that it does not need an experimental calibration step. shorah found all correct haplotypes for all single-clone samples and for  <dig> of  <dig> mixtures, having a very large number of false haplotypes and a significant divergence of expected and found frequencies. introduction of a frequency cutoff for shorah results in loss of true haplotypes. shorah with frequency cutoff 1% was correct for  <dig> single-clone samples and for  <dig> mixtures, having both missing true and false haplotypes for other samples.

we highly encourage the sequencing of single-clone samples of the desired amplicon in order to understand the nature and distribution of the errors and to measure the performance of the algorithm in this particular amplicon.

most algorithms are successful in identifying and removing low-frequency errors. however, reads with high-frequency homopolymer errors should not be removed but rather corrected, allowing for preservation of valuable data. all three algorithms correct reads with homopolymers in a different way. kec uses the k-mer distribution to discern between erroneous and correct k-mers and then fixes homopolymers using a heuristic algorithm. et fixes the homopolymers based on pairwise alignments with high-quality internal haplotypes. shorah clusters reads with a similar sequence, effectively creating a consensus haplotype. sample s <dig> is particularly interesting because it included a false haplotype with a raw frequency of  <dig> %. this false haplotype contained a deletion in a long homopolymer . both kec and et fixed this haplotype, but the clustering algorithm shorah preserved this false haplotype because of its high frequency and made it a center for the cluster of other reads, achieving a final frequency of  <dig> %. the same situation occurs in most single-clone samples: in samples s <dig>  s <dig>  s <dig>  s <dig>  s <dig>  s <dig>  s <dig>  s <dig> the second-frequent haplotypes with frequencies  <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> %,  <dig> %, respectively differs from most frequent haplotype by one indel in a long homopolymer. the main assumption of clustering algorithms is that the observed set of reads represents a statistical sample of the underlying population and that probabilistic models can be used to assign observed reads to unobserved haplotypes in the presence of sequencing errors  <cit> . however, these algorithms assume that errors rates are low and randomly distributed, which is not true for the 454-sequencing of amplicons. some homopolymer errors achieve very high frequencies, making very difficult to separate these false haplotypes from true ones using a clustering model.

CONCLUSIONS
shorah, et and kec are equally accurate in finding true haplotypes. however, new algorithms, kec and et, are more efficient than shorah in removing false haplotypes and estimating the frequency of true ones. both algorithms are highly suitable for rapid recovery of high quality haplotypes from reads obtained by ngs of amplicons from heterogeneous viruses such as hcv and hiv.

competing interests
the authors declare that they have no competing interests.

authors' contributions
ps developed and implemented the kec algorithm. zd and dsc developed the et algorithm. zd implemented the et algorithm. yk and gv designed the experimental section. lr and jy created the plasmid clones. gv and jcf prepared the samples for sequencing. az helped to run the shorah software. ps, zd and dsc analyzed all data. dsc, ps and yk wrote the manuscript. all authors read and approved the final manuscript.

