BACKGROUND
as more and more high-dimensional molecular data is amassed, the importance of biomarker research increases. specifically, predictive biomarkers are usually wanted in order to predict risks associated with diseases. when building multivariate risk prediction models for finding such biomarkers, it is desirable to produce sparse models. the sparsity of the resulting models facilitates the biological and statistical interpretation  <cit> . approaches such as componentwise boosting  <cit>  or the lasso  <cit>  achieve sparsity by performing variable selection and parameter estimation simultaneously. there are two frequently occurring problems in this context: first, lack of reproducibility of variable selections across different studies, for example concerning gene expression data  <cit> ; second, no established approaches to account for interactions. the latter deficit can lead to selection of wrong variables or biased parameter estimations. the first problem, i.e. the inability to confirm most of the published gene related signatures, has led to doubts whether signatures should be produced at all. however, the failure of finding stable signatures could to some extent be ascribed to inadequate modeling. approaches that are more comprehensive are necessary, for example, combining molecular data with annotation and clinical information  <cit> . one ingredient should be to incorporate promising interactions in the model. many tools for modeling interactions exist, but, as far as we know, no systematic investigations of potential building blocks are available.

examples for promising modeling strategies that can account for interactions are penalized regression models  <cit> , logic regression  <cit> , multifactor-dimensionality reduction  <cit>  or random forests  <cit> . for a comprehensive review regarding interaction  selection approaches, we refer to  <cit> . logic regression and multifactor-dimensionality reduction are primarily destined for discrete marker data, e.g., for single nucleotide polymorphism data. in contrast to that, penalized or regularized regression models cover more general types of data. their main property is to put a penalty on the model parameters, which correspond to marker effects, for estimation. the usage, for example, of an l1-penalization forces most of the estimated parameters to be zero, i.e., the values of the corresponding covariates do not influence predictions obtained from the fitted model. even though these models are primarily used for main effect selections, there is an increasing interest in incorporating interactions  <cit> . when there is no a priori knowledge, such approaches either require the interactions to be formed by variables that represent main effects or that interaction terms are created by combining the covariates in a certain way, e.g., by producing all distinct two-way interactions . the first route can lead to false negatives even if the true interactions have relevant marginal effects, and the second one neglects the fact that it is frequently either not feasible or computationally too expensive to consider all possible interactions. altogether, this means that a screening method for promising interaction terms is in most cases necessary, especially for higher order interactions. the potential of random forests to provide non-parametrical means for handling various kinds of interaction structures makes them attractive as an interaction screening method for penalized regression models. however, apart from some interesting theoretical results  and positive empirical findings regarding prediction performances , the ability to extract information from random forests is considered problematic. the main objection is that established variable importance measures seem to be unable to detect relevant interaction effects in the absence of strong marginal components  <cit> .

variable importance measures  for random forests are meant to extract the information contained in forests. established vims are the gini, the permutation accuracy, or the minimal depth importance . the first measure uses the mean improvements in the gini index in a forest related to the investigated variable. permutation accuracy importance measures the change in prediction accuracy of the forest when the values of a variable are permutated randomly, and minimal depth importance is roughly related to the mean minimum distance  from the root node to the investigated variable. these measures can also be used for finding interactions in the forest. for example, the permutation accuracy importance can easily be extended such that the values of two variables are permutated randomly  <cit> . these variable importance measures lead to a ranking of variables, in which interaction information is assumed to enter in some way. whether these interactions are statistically relevant can be evaluated by penalized regression models. hence, a comprehensive evaluation can consist of two parts: extracting interaction terms based on random forest information and estimating a statistical regression model based on all available variables and identified interaction terms.

in this paper, we show building blocks for evaluating and incorporating interactions terms in high-dimensional time-to-event settings, in particular for settings in which it is very computationally expensive to check all possible interactions with an exhaustive search algorithm. the main ingredients are random survival forests , a specific adaptation of random forests to time-to-event settings, and an incremental stagewise forward regression technique, called coxboost  <cit> . coxboost is a boosting technique based on the cox proportional hazards model and combines variable selection with model estimation. for this purpose, it uses a penalized version of the partial log-likelihood and applies componentwise boosting. we investigate the effect of a combination of these approaches, the additional contribution of resampling, and the advantage of a special data pre-processing step. this work is a feasibility study; hence, we are first of all interested in investigating how several components can contribute to the solution of the interaction finding problem. the specific choice of the investigated tools is justified by their specific properties; however, there are alternatives to our decisions . we are interested in predicting risks within time-to-event settings and we use methods established in these settings. in this context, we rely on some assumptions, such as the proportional hazard assumption.

in the next section, we present details of coxboost and rsf together with corresponding vims. after presenting evaluation tools and our interaction detection strategy, we outline a simulation design for the evaluation. in the results section, the findings of the simulation study are shown, and we illustrate our approach on two real-world applications. finally, we describe limitations of the study and summarize our findings in the conclusion section.

methods
time-to-event or survival data for n investigated entities is typically given as a set of triples z
i
=,i= <dig> …,n. the first component is the observed time for each entity i and is given by t
i
= min, where t
i
 is the event time and c
i
 is the censoring time from which on the entity is no longer observed. the second component is the event indicator δ
i
, which takes the value  <dig> if an event has occurred at the observed time  and  <dig> if the event time is censored . the third element, x
i
, is the vector of values of the p covariates observed at baseline.

coxboost
as a forward stagewise regression technique in the time-to-event setting, we use a likelihood-based boosting variant, called coxboost  <cit> . this technique is based on the cox proportional hazards model, which relates the hazard λ, i.e. the instantaneous risk of having an event at time t, given the covariate information in x
i
, for entity i, in the following way:

 λ=λ0exp, 

where the baseline hazard λ <dig> is left unspecified. usually, the parameter vector β=
t
 is estimated by maximizing the partial log-likelihood :

 pll=∑i=1nδixitβ−log∑j=1niexp 

with indicator function i . however, such a procedure is not feasible for p>n. therefore, coxboost uses a penalized version of the pll and applies componentwise likelihood-based boosting  <cit> . conventional coxboost starts with parameter estimates β^=t. in each boosting step k= <dig> …,b, only one coefficient is updated. in order to determine which component j∗ should be updated in step k, the penalized univariate pll with argument θj, j∈{ <dig> …,p}, is considered:

 pllpenθj=pllθj−ρ2θj <dig>  

with fixed penalty parameter ρ> <dig> and the variable parameter θj. in pllθj, all parameter components with indices unequal to j are set to the corresponding components of β^. the parameter vector component j∗ is the one that leads to the maximum value of pllpenθj. instead of maximizing the penalized pll for each candidate j, using the standard newton-raphson algorithm, the penalized score statistic can be used as a criterion

 ujij+ρ−1uj, 

where uj is the value of the score function u=∂pll/∂θ for θ=θj= <dig>  and ij is the value of the fisher information i=∂2pll/∂θ <dig>  again for θ=θj= <dig>  the covariate j∗ with the largest value of the score statistic is selected for an update of the form:

 β^j∗=β^j∗+θ^j∗ 

while β^j=β^j for all covariates j≠j∗. the tuning parameter ρ is typically set to ∑iδi·, with ν∈. in addition to that, coxboost has many extensions. it is, for example, possible to force the inclusion of a number of covariates into the model by suspending penalization for them  <cit> . this is relevant for settings with few clinical covariates and a large number of molecular variables. in this case, the coefficient estimates of the mandatory covariates are updated before the other covariates. further, more than one coefficient can be updated in each boosting step, or the penalization parameter can vary from step to step. coxboost and all these features are implemented as an r-package, correspondingly called coxboost <cit> .

random forests
random forests are ensembles of – usually binary – classification or regression trees  <cit> . usually unpruned trees are generated based on resamples of the original data and a random component in the splitting procedure, which implies that in every knot splitting is based on the number mtry of randomly selected variables. unpruned trees in the context of random forests are rather unproblematic in terms of overfitting on training data; however, they can have detriment effects on the consistency of the response estimations  <cit> . each path in such generated trees represents a sequence of splits that leads to the response of cases corresponding to that path. the final model response is determined by aggregation, e.g. averaging the responses of a case over all trees.

random forests can detect and deal with small effects, interactions and non-linear associations, making no assumptions about the corresponding functional form  <cit> . all of these characteristics are also valid for trees. however, one important rationale behind random forests is the de-correlation of information that is represented in single trees, which reduces the corresponding variances – a bagging phenomenon  <cit>  – and the grouping property of trees. the latter property relates to the fact that a split on a variable from a cluster of correlated variables is frequently followed by splits of other members of that group  <cit> . a further advantage of forests over trees is that they can approximate smooth functions without the necessity of having a large number of leaves in a tree, due to the smoothing effect of the bagging phenomenon  <cit> . random forests perform relatively well off the shelf  <cit>  with the default-values for mtry  and for the number of trees in a forest .

as one specific adaptation of random forests to right-censored time-to-event data, we consider random survival forests   <cit> . for a – computationally expensive – alternative, see party. the response for rsf is the cumulative hazard function , defining an ensemble predicted value with respect to ’mortality’. for splitting, typically the logrank test is used  <cit> . hence, the homogeneity of nodes in the tree is a result of maximizing the difference of event probabilities between daughter nodes. for each entity in the data set, the ensemble chf is calculated by averaging the nelson-aalen estimator of all leaves, into which the entity drops  <cit> . for a terminal node h with n distinct event times t <dig> h
<t <dig> h
<…<t
n,h
, this estimator is given as

 Ĥh=∑l:tl,h≤tdl,hyl,h, 

where d
l,h
 and y
l,h
 are the number of deaths and entities at risks at time t
l,h
. rsf are implemented in the r-package randomsurvivalforest <cit> .

variable importance measures for random forests
various variable importance measures  can be used for selecting variables. there are two well-known vims: gini importance and permutation accuracy importance . another vim is the mean minimal-depth measure, which has been proposed recently. roughly, it measures the shortest distance  from the root node to the parent node of the maximal subtree . for further details, we refer to  <cit> . different vims can produce different rankings; for example, the gini importance was found to be highly affected by selection bias, e.g., continuous variables are preferred to categorical variables with only few categories  <cit> . in the following, we focus on pam, because it is widely accepted and relates to the concept of simulating a null distribution , even though we are aware of potential problems  <cit> . for further information regarding vims, we refer to  <cit>  and  <cit> .

there are two versions of pam. in its common version it is computed with respect to random permutations of the components of x
j
=
t
, which breaks the association of x
j
 with the response and all variables. in a more sophisticated variant, which is unique to rsf with respect to survival data  <cit> , the vector x
i
= related to entity i is dropped down in all trees, in which it was out of bag in the training process; whenever a split node for an investigated variable is encountered, the corresponding vector x
i
 is randomly assigned to one of the daughter nodes. in both variants, the variable importance results from the prediction error of the altered forest minus the prediction error of the non-altered forest. the larger the importance values of a variable, the higher its value for prediction. it is important to notice that pam is tied to the error measure used. one frequently used error measure for rsf, which we use here as well, is harrell’s concordance index, which measures the discrimination ability of a model  <cit> .

tools for finding effects in time-to-event data
in high-dimensional settings, the problems of extracting relevant information by regression models are aggravated compared to the low-dimensional counterparts. for example, even if stepwise regression introduces biases related to multiple test problems , it nevertheless provides a means for tackling variable selection issues in a comprehensive manner. it is therefore crucial to investigate mechanisms and measures for an adequate model selection on high-dimensional data. three issues have to be addressed simultaneously:  a sparse variable selection,  representing the relevant structure in the data, and  good prediction performance. we try to tackle these issues and in particular concentrate on integrating substantial interactions into the model.

the likelihood-based boosting algorithm promises sparse and stable variable selection, which is a consequence of simultaneous selection and estimation in a multivariable model. naturally, variable selection stability also depends on the quality of the data , and for obtaining high-quality molecular data frequently appropriate pre-processing steps are necessary, e.g., background correction and normalization. concerning the other two issues  yang  <cit>  strikingly demonstrates that best predictive models usually contain irrelevant features and important features often do not lead to best prediction performances . whenever we encounter the trade-off between relevance and usefulness for prediction, we prioritize ’finding relevant variables’ over prediction performance.

the models are evaluated within a resample procedure for estimating sensitivity and stability. as a performance measure adapted for time-to-event endpoints, we use the brier score  <cit> . the brier score is a strictly proper scoring rule, i.e. it is optimal only at the true probability model . for example, the area under the curve  is not a strictly proper rule, because it can lead to optimal values for different probability models . two common resampling techniques are cross-validation  and bootstrapping. cross-validation partitions the data into folds and evaluates prediction performance on every single fold with models fitted to the data from the remaining folds; a more precise characterization for cv is therefore ’subsample technique’. both techniques can cause problems , and we decided to use subsampling with splits of relative size  <dig>  to , because this seems to work well in many settings  <cit> . such a subsampling procedure is roughly comparable to a 3-fold cv .

the brier score quantifies the squared deviation between predicted survival probability and observed survival status and is independent from the assumed survival model. when Ĥ <dig> is the estimated cumulative baseline hazard at baseline and β^ denotes the estimated coefficients, the predicted survival probability is given by

 π^=1−exp−Ĥ0exp 

and the expected brier score tracked over time  has the form

 err:=exδ−π^ <dig>  

where δ is the true survival status at time t. typically the survival status at time t will be right censored for some observations. thus, inverse probability of censoring weights  were proposed to avoid the related bias  <cit> . the ipcw for individual i is defined as

 wi=iδip^+ip^, 

where p^ is a consistent estimate of probability that the censoring time is larger than s, given x
i
. i is again the indicator function. the cross-validation estimate of the brier score tracked over time is then

 err^boot:=1b∑b=1b1|i∖ib|∑i∉ibδi−π^b2wi. 

here, b is the number of resamples, n the number of rows, and ib the indices of those cases that are included in the resample b.

assembling of building blocks into an interaction detection strategy
our comprehensive strategy consists of three parts:  first main effect detection,  pre-selection of interactions terms, final model selection. parts  and  use coxboost and are fixed. here, we rely on the ability of coxboost to produce sparse models and to include important variables. in part , we consider the following building blocks:  subsampling,  random forests, and  orthogonalization as a data pre-processing step. different decisions concerning the building blocks lead to flexibility in part . when combining building blocks into comprehensive strategies, over-fitting to the data at hand and over-optimism could occur  <cit> . one way to account for that – besides the usage of independent validation data sets – is to evaluate the contribution of the building blocks to the results.

the use of an outer subsampling for interaction finding has the aim of enhancing the credibility of interaction information. specifically, we use the variable inclusion frequency , i.e. the proportion of times that the variable appeared in the model, for assessing the relevance of an interaction term. for example, when using random forests, this means that the number of random forests in which interaction terms are deemed relevant is the basis for a pre-selection of interactions. here, an interaction term is assessed as relevant if both underlying variables have pam values larger than zero in a random forest . in other words, variables have to be simultaneously important for a random forest.

when all building blocks are used for the pre-selection of interactions terms, random forests are applied to the data in a subsampling context and orthogonalization is used as a data-pre-processing step. orthogonalization means that all variables not considered as main effects are made orthogonal to those that are indicated as main effects by coxboost in the first step. this leads to disentanglement of information, which might allow to determine variables and related interactions that contain information that was originally masked by main effects . the strategy using all building blocks is described by the following pseudo-algorithm : 

 <dig>  specify: indices  of clinical covariates or other known main effect variables, number s of subsamples for pre-selecting interactions, and number r of pre-selected interaction terms. in case of identical vif values for the rth and th found interaction, all interactions with that vif value are included as well.

 <dig>  subsample the original data set z in relation  <dig>  to , leading to the data sets z
b
 and zb′. 

 first pass main effects detection: run coxboost on z
b
, possibly incorporating clinical covariates {xk|k∈k} without penalization. this leads to the model coxboostm and a list of main effects, given by the index set . main effects and clinical covariates are used for orthogonalization  in the pre-selection step and as unpenalized variables in the final coxboost model.

 pre-selection of interaction terms: if ℳ∪k is non-empty, regress all covariates with indices { <dig> …,p}∖ on the variables in . subsequently, compute the corresponding residuals of the covariates, which leads to the data matrix z~b ). subsample s times data from z~b – from z
b
, when  is empty – in relation of  <dig>  to  and generate rsf on each larger subsample  and ). construct interaction terms by all pairs of variables with pam values greater  <dig> on every subsample and compute vifs of the interactions terms at the end of the subsampling process. select the r most frequent pairs.

 final model: covariates are xk,k∈k, xi,i∈ℳ, and the r selected cross product terms of . run coxboost on z
b
 with these covariates without penalization for covariates with indices in , leading to model cbfin.

 compute prediction error: apply cbfin on zb′ and compute the brier score.

for assessing the contribution of building blocks, we successively remove one of them in the pre-selection step, leading to following alternatives to step : 

b <dig> do the same as in rsf-vif-res, but without orthogonalization. 

b <dig> replace rsf in rsf-vif by coxboost: subsample s times data from z
b
 in relation  <dig>  to  and run coxboost on each of the larger data sets. finally, compute vifs related to the variables selected by coxboost in each subsample, and create r pairs, i.e. interaction terms, related to the variables with the highest vifs. 

b <dig> omit subsampling in cb-vif: compute all distinct cross product terms of covariates with indices in . here, s is superfluous and r is not needed, if ||ℳ||≤r; otherwise, select randomly r interactions from all cross product terms. 

there are many more alternatives, which are not considered due to a limited space and for reasons of clarity.

simulation design
for a systematic analysis of the building blocks and the corresponding interaction detection strategies, a time-to-event simulation study was conducted. here, we define interactions as effects based on multiplicative combinations of variables. the main interest concerns the ability of the strategies to find relevant interactions and especially those that might be difficult to detect, i.e., variables in interactions are not members of the set of true main effects. the secondary focus is on the prediction performance, which highly depends on the effect sizes of main effect variables and interaction terms.

the simulation scenarios are designed to mimic simple yet realistic settings, e.g. microarray studies. we simulate independent as well as correlated data for a time-to-event end point. table  <dig> summarizes the scenarios and shows the effect sizes of the main effects and interactions. the number of covariates is fixed as  <dig> , the sample size is fixed as  <dig> , and all covariates are from a standard normal distribution . the covariates not indicated in the table have zero effect sizes. for each simulation scenario,  <dig> datasets are generated. survival times and censoring times are generated from an exponential distribution with baseline hazard λ= <dig> .

me: main effect. int: interaction. corr: correlation. in all scenarios, the samples size is  <dig>  and the number of covariates is  <dig> . the effect sizes are given in the form ‘’. for the scenarios with correlations, p divided by the block size  gives the dimension of the normal distribution from which the values of the variables in a block are sampled, and the correlation value is the value at the off-diagonals of the corresponding covariance matrix. in scenarios sim <dig> and sim22_ <dig> , all interaction detection strategies are covered. all other scenarios are used for investigating rsf-vif-res.

sim <dig> refers to the simulation case with  <dig> main effects and  <dig> interactions that are composed of the main effects and sim22_x to the cases with  <dig> main effects and  <dig> interactions that are not related to these main effects; in other words: they are composed of variables that have zero effect sizes. if x is numeric, it gives the uniform effect size; x= "bin" denotes the case of interactions composed of binary variables, and x beginning with "corr" relates to cases with variables that are block-correlated with a uniform correlation coefficient c, c∈{ <dig> , <dig> , <dig> , <dig> }, across the  <dig> fiver-blocks, i.e., values of variables are sampled from a 5-dimensional normal distribution with the same variance matrix  over all blocks. here, main effects and variables in interactions terms stem from different blocks.

in scenarios sim <dig> and sim22_ <dig> , all interaction detection strategies are considered for evaluating the effect of the building blocks. the other scenarios are used to investigate the behavior and the limits of rsf-vif-res. the simple scenario sim <dig> is used for ascertaining that the strategies are capable of finding the relevant main effects. scenario sim22_ <dig>  is the reference scenario for the scenarios with non-smooth interactions, i.e. interactions incorporating binary covariates, and correlated variables.

the performance of a strategy is measured by the number of correct non-zero variables in the models, i.e. the variable selection sensitivity with respect to the main effects and the interaction terms, and by the prediction error . specificity values or predictive values are not separately listed in the result tables. however, these measures can be deduced from the sensitivity values and the number of selected variables.

in order to obtain one interpretable measure for the prediction performance, the brier scores tracked over time are aggregated by computing the integrated prediction error curves  for each model. furthermore, the ipecs of the estimated models  are considered relative to the ipec of the corresponding kaplan-meier estimator :

 ripeci:=ipeckm−ipecsiipeckm. 

in other words, ripec gives the relative improvement of prediction performance of strategy s
i
 compared to the prediction performance of the kaplan-meier.

RESULTS
simulation study
the simulation was conducted in r- <dig> . <dig> with following main settings for the model implementations used in our strategies. parameters not listed are considered secondary and were set to their default values:

coxboostpenalty:  ·. penalty value for the updates in each boosting step. standardize: true. covariates are standardized. stepno: as computed by cv.coxboost. number of boosting steps.

rsfmtry: square root of the number of variables . ntree:  <dig>  number of trees grown .

the parameter values of the strategies described in the methods section were chosen in the following way: no clinical covariates, hence k={}; the number of subsamples  for pre-selecting interactions was  <dig>  and the number of pre-selected interaction terms  was  <dig>  for all scenarios, data were are randomly generated  <dig> times. the results for scenarios sim <dig> and sim22_ <dig>  are given in table  <dig>  the relevant columns are: the number of selected interactions by the corresponding screening method , the number of total variables in the final model , the sensitivity with respect to the inclusion of true main effects , the sensitivity with respect to the availability of true interactions from the screening step , the sensitivity with respect to the inclusion of true interactions in the final model , and the ripec values of coxboostm and the final model. for simplifying the discussion of the results, we will abbreviate the phrase ‘random forests together with pam’ by ‘random forests’ or ‘rsf’.

intscreen  is the number of selected interactions by the corresponding screening method; intsensia ’) is the sensitivity related to the availability of true interactions; varstotal ’) is the number of total variables in the final model; mainsensi ’) is the sensitivity related to the inclusion of true main effects; and intsensi ’) is the sensitivity related to the inclusion of true interactions. the ripec values ’) are shown for coxboostm and the final model. the scenarios were repeated  <dig> times. additional file 1: figure s <dig> provides boxplots for further insights into the nature of the variability in ripec.

in scenario sim <dig>  use of random forests generate models with more than  <dig> variables in the mean , whereas the other two strategies result in less than  <dig> variables on average. this means that there are many false positive findings when using rsf, which has a negative impact on the ripec compared to the cb-strategies. on the other hand, rsf-vif-res leads to the largest sensitivity values for main effects and interactions. hence, there is a trade-off between sensitivity and prediction performance. for all pre-selection variants it seem that when true interaction are pre-selected , then almost all of them are selected in the final model. comparing cb-crossp with cb-vif, we see that subsampling can increase intsensi without decreasing mainsensi, and this leads to the best ripec value in scenario sim <dig>  use of random forest instead of coxboost for interaction pre-selection  increases intsensi but leads to a slight reduction of mainsensi. the mainsensi and intsensi values of rsf-vif-res indicate that orthogonalization is not only important for further increasing intsensi but also for a higher mainsensi value compared to rsf-vif. overall, in this scenario, subsampling is important and random forests should be applied on orthogonalized data for achieving the largest sensitivity values but even then, prediction performance cannot be improved compared to interaction pre-selection with coxboost.

scenario sim22_ <dig>  exhibits some differences to sim <dig>  first, all strategies lead to similar and moderate numbers of total variables in the final model. hence, high intscreen values in rsf strategies do not result in more false positives than interaction pre-selection variants that use coxboost. coxboost is not able to pre-select true interactions, with or without subsampling. subsampling even reduces mainsensi values. use of random forest further decreases mainsensi with a little compensation of increased intsensi but at an interchange rate that makes a further reduction of ripec possible. again, rsf has to be applied to the pre-processed data  for increasing mainsensi and intsensi compared to rsf-vif. now, the increase in intsensi is drastic, which leads to the best ripec value in this scenario. the intsensi value is still moderate; however, one should bear in mind that the interactions are built by variables that do not represent main effects. this might be particularly relevant for real world applications: even a moderate variable inclusion frequency of an interaction term could indicate an important interaction if the underlying variables are irrelevant as main effects.

both scenarios show that all building blocks are important, and in particular orthogonalization is important before applying rsf, i.e. disentangling information beforehand is crucial for pre-selecting interactions. coxboost is unlikely to benefit from such a pre-processing because it already applies some sort of orthogonalization during fitting .

that intsensia is often similar to intsensi in both scenarios means that one can rely on the ability of coxboost to choose the right interaction terms out of those presented, regardless of intscreen. hence, it seems that the parameter r  can be quite high. for assessing the effect of r, we additionally investigated the same scenarios rsf-svif-res with r= <dig> . with this reduced r-value, sensitivities, varstotal, and ripec decreased; the latter two measures were in particular reduced in scenario sim22_ <dig> . thus, in case of doubt, r should be set to a larger value.

we also investigated the behavior of the parameter estimates of the main effects and the interaction terms. in no case did a true effect receive a wrong sign. in the mean, shrinkage of the coefficients was stronger in scenario sim <dig> than in sim22_ <dig> . this has two reasons: higher absolute values of the true coefficients and increased number of non-zero coefficients. as the results show, this increased shrinkage is not relevant for the sensitivity of the detection strategies. one can try to reduce shrinkage by reducing the value of the penalty parameter or manually increasing the number of step sizes; however, we would not recommend such intervention in a high-dimensional setting without good reasons .

intscreen  is the number of selected interactions by the corresponding screening method; intsensia ’) is the sensitivity related to the availability of true interactions; varstotal ’) is the number of total variables in the final model; mainsensi ’) is the sensitivity related to the inclusion of true main effects; and intsensi ’) is the sensitivity related to the inclusion of true interactions. the ripec values ’) are shown for coxboostm and the final model. the scenarios were repeated  <dig> times. additional file 1: figure s <dig> provides boxplots for further insights into the nature of the variability in ripec.

in table  <dig> results of the scenarios with correlations are given. these scenarios are challenging, because random forests can have problems in distinguishing between correlation and interactions. the problem is dealt with extensively in  <cit> . there is a debate whether correlations are pointing to relevant associations or not . our focus here is on the effect of non-informative correlations on mainsensi and intsensi. the results show that even small correlation values lead to decreased sensitivities compared to sim22_ <dig> . both, coxboost and random forest are negatively but not overly affected by correlations. however, with correlations ≥ <dig>  intsensi and mainsensi decrease excessively. intersensia values of  <dig>  and intersensi values of about  <dig>  suggest that most of this deterioration can be ascribed to coxboost and not to the random forests. the ripec does more or less reflect the tendency of reduced sensitivities. in summary, correlations do pose a problem for rsf-vif-res, but mainly because of the inability of coxboost to select the true effects and not because of the random forests component. this is corroborated by the fact  that true interactions were almost never replaced by interaction terms built by variables that correlate with variables in the true interactions.

intscreen  is the number of selected interactions by the corresponding screening method; intsensia ’) is the sensitivity related to the availability of true interactions; varstotal ’) is the number of total variables in the final model; mainsensi ’) is the sensitivity related to the inclusion of true main effects; and intsensi ’) is the sensitivity related to the inclusion of true interactions. the ripec values ’) are shown for coxboostm and the final model. the scenarios were repeated  <dig> times.

real data illustrations
diffuse large-b-cell lymphoma data
in order to illustrate how rsf-vif-res can be applied on real data, we first analyzed the well-known rosenwald data  <cit> . this data set was used to link  <dig>  gene expression features of  <dig> patients with diffuse large-b-cell lymphoma  to the time of their death. dlbcl is an aggressive malignancy of mature b lymphocytes with a high rate of remissions. the objective of the rosenwald study was to devise a molecular profile that accounts for the underlying heterogeneity, predicts survival and can be used for assessing the effect of the related therapies. overall,  <dig> deaths were observed, with a five year overall survival of 48%. the  <dig> features measured at baseline represent  <dig> genes. an established clinical predictor, the international prognostic index , is available for n= <dig> patients, which will be considered for the analysis in the following, i.e. k={indices}. for further details and an overview with respect to various strategies for analyzing this and related data sets, we refer to  <cit> .

we were interested in gaining new insights by incorporating interactions together with main effects. in almost all previous analyses of the rosenwald data, at least four genes exhibited strong main effects. our assumption was that there should be relevant interactions as well. even though, it is frequently reasonable to assume complex and non-linear interactions, using cross product terms should be a first step in enriching the molecular profile. rsf-vif-res was applied with r= <dig>  and on  <dig> subsamples of the original data set. the prediction error curves of coxboostm and of rsf-vif-res are given in figure  <dig>  the mean prediction errors  show that rsf-vif-res performs slightly worse than coxboostm. based on our assumption that there should be relevant interactions, the simulation study suggests that slight reduction of the prediction performance might still point to interaction effect sizes that are moderate, i.e., below the effect sizes of the main effects but not negligible .

the three main effects and gene-gene-interactions, given in unigene cluster notation, related to the largest relative vifs  are: 

- hs. <dig> , hs. <dig> , hs. <dig>  and

- hs.76807:hs. <dig> , hs.79428:hs. <dig> , hs.20191:hs. <dig> 

the underlying genes of the interactions represent no relevant main effects for coxboostm, and the vifs of these interactions are low. in order to increase certainty concerning the interactions, we manually increased the step size of the final model to  <dig>  there, the same main effects are associated with slightly higher vifs . the changes for the interactions are more interesting: two new interaction terms are among the interactions with the largest vifs and the relative vif values increased to  <dig> ,  <dig> , and  <dig>  for hs.20191:hs. <dig>  hs.20191:hs. <dig>  and hs.76807:hs. <dig>  respectively. from the considerable increase of relative vifs, we concluded that these interactions might be more reliable. figure  <dig> shows the connections between the genes in selected interaction terms with relative vifs ≥3/ <dig> . our observations in the simulation study  indicate that the most frequent interaction term hs.20191:hs. <dig> could be relevant, although its frequency is moderate. however, mean model size increased drastically from  <dig> to  <dig> and led to an ripec value of  <dig>  so, in order to corroborate our conclusion further, we went back to molecular biological information. from kegg , we retrieved the pathways of the genes in the interaction term hs.20191:hs. <dig>  the proteins of these genes  are elements in the pathway related to  apoptosis  <cit> . in addition to that, they have a role in the cellular response to hypoxia  <cit> . the expression values of both genes lead us to the assumption that the corresponding proteins might interact complementarily: common down /up-regulation with respect to the apoptotic function and to hypoxial-induced reactions might have an impact on tumor genesis and growth .

neuroblastoma data
a further real-world example is related to the microarray data set of oberthuer et al  <cit> . it consists of n= <dig> patients suffering from neuroblastoma. overall,  <dig> deaths were observed and the median survival time is  <dig> days. for each patient, p= <dig>  microarray features are available, and we concentrate on the relationship between survival and these microarray features. the same parameter values as for the rosenwald data are used but with no clinical covariates, i.e. k={}. the prediction error curves of coxboostm and of rsf-vif-res are given in figure  <dig>  again, the mean prediction errors  indicate that rsf-vif-res performs slightly worse than coxboostm.

the three main effects and gene-gene-interactions with the largest relative vifs are: 

- hs. <dig> , hs. <dig> , hs. <dig> and

- hs.496658:hs. <dig> , hs.496658:hs. <dig> , hs.496658:hs. <dig> 

vifs are higher than for the rosenwald data. based on the simulation study results, the vifs might be considered large enough for indicating important interactions. hs. <dig> is the most relevant gene entity: it contributes the largest main effect vif and is involved in interactions with the largest vifs. the corresponding gene name is slc25a <dig>  and the product of this gene functions as a gated pore that translocates adp from the mitochondrial matrix into the cytoplasm. suppressed expression of this gene has been shown to induce apoptosis and inhibit tumor growth . figure  <dig> shows the connections between the genes from interactions with relative vifs ≥3/ <dig>  the graph is more complex than figure  <dig>  which translates into an increased uncertainty with respect to the relevance of the interactions. for example, hs. <dig> is gene cgnl <dig>  which encodes a protein that localizes to both adherens and tight cell-cell junctions and mediates junction assembly and maintenance . there could be a real interaction between both genes , but further biological validation would definitely be necessary.

discussion
from the results of the simulation study, we conclude that random forests can provide relevant interaction information. if the interaction is strong enough, the marginal effects of the underlying variables are at a level such that they are frequently selected as split variables in the random forest generation process. further, the results indicate that disentangling information also is important for achieving good results. the reason behind this might be that variables associated to main effects can mask interactions in random forests, which affects the split variable selection process. disentanglement of information specifically means to transform variables to be orthogonal to those with indices in . when the number of estimated main effects in coxboostm is too large , the corresponding regressions can be unreliable. in this case, we would recommend focusing on those main effects with the largest absolute coefficient estimates in coxboostm. another possibility is to use the linear predictor lp=∑i∈ℳβixi for the regression xjk=α·lpk+εjk,j∉ℳ,k= <dig> …,n. in both cases, the orthogonalization is imperfect and results  based on the latter variant indicated that sensitivities related to interactions are considerably lower than with the strategy for computing residuals proposed here. however, an alternative should be taken into consideration, when the number of cases is small .

the scenarios with correlation and non-smooth interactions show that the pre-selection of interactions is less affected than the final coxboost model. for non-smooth interactions, this was expected due to the non-smooth nature of individual trees in random forests, but the effects on correlated data indicate that the pre-selection of interactions in rsf-vif-res is quite robust. one further interpretation of the simulation study is that moderate variable inclusion frequency of an interaction term  still could indicate an important interaction. the real data example showed that uncertainty related to the reliability of the findings can make it necessary to consider and contextualize as much information as possible. specifically, increasing the step size of coxboost from its optimal value to  <dig> in the rosenwald data was an attempt to reduce the uncertainty. due to the considerable deterioration of prediction performance, the decision on the importance of the identified interactions was based on additional biological knowledge. there is no absolute threshold with respect to a decrease in prediction performance that makes the results definitely unreliable. the results showed that detection of true interaction and main effects can be accompanied by deteriorated or bad prediction performances due to the increase in false positives. it always depends on the subject-matter question whether a certain level of prediction performance is deemed necessary. if biology can help sorting out the true effects, concerns related to prediction performance even might be considered secondary.

the results showed that the number of pre-selected interactions r must be large enough  for guaranteeing that the screening process is able to pre-select relevant effects. coxboost was frequently able to select the right variables out of ten thousands of variables. this is a feature of many other  regularized regression techniques such as the lasso , which  also are consistent for variable selection, even when the number of variables p is as large as exp for some 0<α< <dig>  <cit> . empirically determining an optimal r is nevertheless difficult. this issue certainly needs further scientific investigations.

limitations
due to the focus of this paper and the limited space, our study has several limitations. first, only two-way interactions were considered in the interaction screening process. in real-world data, all kind of multifactor and non-linear interactions can be expected. second, the simulation scenarios are limited in their scope, because we focused on one critical issue: the effects of the building blocks when interactions are built from variables that do not represent main effects. although, we also investigated simulations scenarios with correlations, further investigations of informative correlations and more complex correlations structures are relevant.

third, the real-data applications showed that the strategies cannot be used in an automatic way. decisions related to the choice of some parameter values , interpretation of the results, and further processing of these results have to be based on subject-matter knowledge and the specific application. such requirements could discourage a user from using rsf-vif-res. nevertheless, it should be clear that assessing the necessity of considering interactions is not trivial, even for the simplest case of gene-gene interactions and therefore informed decisions are crucial.

fourth, there are open questions such as the specific value for r or alternatives to the building blocks presented in the paper. there are several routes for extending our proposal or replacing components in it. fifth, we only considered proportional hazard models and simulated data from such models. it is important to consider departures from the related assumptions in future studies, for example by considering time-dependent effects. finally, the real data examples only considered microarray data. recent sequencing approaches, such as the rna-seq technology, are gaining more and more ground and should be targeted as well.

CONCLUSIONS
our aim in this study was to build a strategy for incorporating two-way interactions into multivariate risk prediction models that are built on high-dimensional molecular data. when it is either not feasible or computationally too expensive to consider all possible interactions, screening is necessary in case of no a priori knowledge. we presented three important building blocks for such a screening strategy: subsampling, random forests, and orthogonalization of the data, and concluded that all building blocks are important. our decision for using random forests for screening interactions has one main reason: the promise of random forests to capture various kinds of relevant interaction structures. coxboost was used, because it usually produces sparse risk prediction models. we assumed that a combination of these two approaches could be fruitful due to their complementary character. however, components can be separately replaced by other ones, for example random forests by multifactor-dimensionality reduction, and such flexibility seems necessary, because no specific combination of building blocks will perform well on every kind of data.

the results show that screening interactions through random forests is feasible and useful, when one is interested in finding relevant two-way interactions. effect sizes of the interactions should be large enough in order to guarantee useful results. when the underlying variables do not represent main effects, sensitivities related to variable and interaction selection are moderate . the results of the simulation study indicates that making all variables orthogonal to those with indices in  could enable random forests to pre-select relevant interaction effects even in the absence of strong marginal components.

the real data applications showed that not only pre-processing and a combination of different tools are important for interaction detection but also an intelligent post-processing. our final conclusion is that in addition to focusing on establishing new methods, it is important to make full use of existing ones.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
ms and hb developed the approach and the design. ih implemented the pseudo-algorithm, conducted the simulation study, applied the approach to the real data, and contributed to design decisions. the work of ih fulfills part of the requirements of her phd. ms supervised the implementation process and wrote most of the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
supplementary tables and figures. one table with respect to different values of r in the simulation study. two boxplots for further insights into the nature of the variability in ripec.

click here for file

 acknowledgements
we thank the editor and two anonymous reviewers for their constructive comments, which helped us to improve the manuscript significantly. grateful acknowledgement goes also to dr. johanna mazur for proofreading.
