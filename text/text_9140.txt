BACKGROUND
simplicity and flexibility
simple classification rules with few variables and parameters are preferable to complicated classification rules for the following two reasons  <cit> . first, classification performance is primarily a function of the first few variables selected, with only slight improvements when additional variables are included. second, only those variables that strongly predict class in the study data, namely the first few selected, are also likely to moderately or strongly predict class in new data. a popular simple classification rule for analyzing gene expression microarrays is diagonal discriminant analysis, which is discriminant analysis with a diagonal variance-covariance matrix  <cit> . diagonal discriminant analysis yields linear or curved classification boundaries, given the name ripples. although ripples are optimal boundaries for normally distributed expression levels in each class with the appropriate variance  <cit> , they can perform poorly with other distributions of gene expression levels  <cit> . a simple modification of diagonal discriminant analysis with two classes yields a smooth highly nonlinear classification boundary, given the name swirls. swirls can outperform ripples under certain scenarios. the proposed simple, yet flexible, classification rule for two classes selects either swirls or ripples after parsimoniously selecting the number of genes and the distance measure.

classification rules generally have two objectives: prediction and understanding  <cit> , which correspond to goals  <dig> and  <dig>  respectively, which are described below.

goal 1: rule discovery and testing
rule discovery and testing involves splitting the data once into training and test samples, selecting the classification rule in the training sample, and evaluating the performance of this classification rule in the test sample. a univariate measure of performance when the training sample is "fixed", as in this case, is called the conditional error  <cit> . two measures of performance that are more informative than the conditional error are the receiver operating characteristic  curve and the relative utility  curve. the roc curve plots true versus false positive rates. the ru curve plots the maximum expected utility of prediction as a fraction of the utility of perfect prediction versus the risk threshold, which is the risk corresponding to indifference between harms and benefits  <cit> .

goal  <dig>  gene discovery
gene discovery involves identifying those genes that contribute most to good classification by repeatedly randomly splitting the data into training and test samples, computing a distribution of roc curves in the test samples to ascertain classification performance, and tabulating the most frequently selected genes in the training sample  <cit> .

data sets
applications of the proposed methodology with both goals involve the following five publicly available data sets for gene expression microarrays:

colorectal cancer:  <dig> genes,  <dig> normal and  <dig> tumor specimens  <cit> ,

leukemia 1:  <dig> genes,  <dig> all, and  <dig> aml specimens  <cit> ,

medulloblastoma:  <dig> genes,  <dig> survivor and  <dig> non-survivor specimens  <cit> ,

prostate cancer:  <dig>  genes,  <dig> tumor and  <dig> non-tumor specimens  <cit> ,

leukemia 2:  <dig> genes,  <dig> t-all specimens and  <dig> tel-aml specimens  <cit> .

RESULTS
classification rule
let j index gene, and k =  <dig> and  <dig> index class. the following quantities specify the classification rule:

  f=classification components = , wherec=centroid set= {cjk,vjk,nk}, cjk=centroid= mean expression level for gene j in class k, vjk=estimated variance of expression level for gene j in class k, nk=number of specimens in class k,g = gene set,d = distance measure fiom specimen to centroid,s= score formula for combining distance measures. 

let zhj denote the set of expression level of gene j in new specimen h, and let zh = {zhj} denote the set of expression levels for specimen h. the distance from specimen h to the centroid of class k, based on gene set g, is

  distance={∑j∈g2/vjp,if d= <dig> ∑j∈g2/vjk,if d= <dig>  

where vjp = { vj <dig> +  vj1}/ is the pooled variance over the two classes. thus d is the distance measure with

d =  <dig> = euclidean distance divided by the pooled variance,

d =  <dig> = euclidean distance divided by the class-specific variance.

division by the variance ensures that the distance measure is not inappropriately weighted by genes with high average levels of expression. the score for combining distance measures is

  score={distance 2 −distance  <dig> if s = <dig> distancedistance+distance,if s = <dig>  

thus s indicates the score formula, either a difference of squared distances or a fraction of the total of the distances. the classification rule for specimen h, which is based on the cutpoint u of the score, is

  rule={assign zh to class  <dig> if score≥u,assign zh to class  <dig> otherwise. 

diagonal linear and quadratic discriminant analysis correspond to s =  <dig> with d =  <dig> and d =  <dig>  respectively  <cit> . a classification rule with s =  <dig> and euclidean distance was previously used to analyze microarrays  <cit>  but without a discussion of its implications.

swirls and ripples
by setting equation  equal to various constants and plotting the solution, one can see that the score formula s =  <dig> yields a classification boundary that encircles a centroid and the score formula s =  <dig> yields a boundary of lines and curves . these boundary shapes motivate the following terminology for the score formula,

s =  <dig> = ripples,

s =  <dig> = swirls.

if the data in each class are normally distributed, ripples is the optimal classification boundary if the correct variance  is specified  <cit> . however if the data in each class are normally distributed with class-specific variances, and one specifies a pooled variance to avoid adding parameters, then swirls can dramatically outperform ripples . the proposed classification rule selects either swirls or ripples, which increases flexibility without adding parameters.

implementation
the proposed classification rule involves randomly splitting 70% of the data into a training sample and 30% into a test sample. classification performance in the training sample is measured via the area under the roc curve, denoted auc, computed assuming a normal distribution of scores in each class. details are provided in the methods section.

for each score formula and distance measure d, the classification rule selects a gene set g by first identifying the  <dig> highest ranking genes in terms of auc of the score and, after starting with the highest ranking gene, successively including genes from these  <dig> highest ranking genes until there is little improvement classification performance. a greedy algorithm, which is sometimes called forward stepwise selection, successively adds the gene that most improves classification performance given the previously selected genes in the classification rule. for the greedy algorithm, the classification rule adds a gene only if the increase in auc of the score is at least  <dig> . a wrapper algorithm selects features by "wrapping around"  the full method of selecting a classification rule that uses both training and test samples, when these samples are nested within the training sample  <cit> . the wrapper algorithm randomly splits half the training sample data into training-training and training-test samples, which is repeated five times. on each random split, a greedy algorithm within the wrapper algorithm formulates a classification rule based on centroid set in the training-training sample and the gene expression levels in the training-test sample, successively adding a gene to the classification rule only if the increase in auc of the score is at least  <dig> . the wrapper algorithm selects the best performing classification rule among classification rules obtained in the five random splits. . after the classification rule computes the gene set g as described above, for each score formula s the classification rule selects d =  <dig> if the increase in auc with d =  <dig> is less than  <dig> , and selects d =  <dig> otherwise. finally the classification rule selects the score formula, s =  <dig> =ripples or s =  <dig> = swirls, that has the highest auc.

computations for goal  <dig> are based on a distribution of roc curves in the test sample computed from  <dig> bootstrap iterations. the ru curve is derived from the concave envelope of the mean roc curve over the bootstrap iterations. computations for goal  <dig> are based on  <dig> random splits into training and test samples.

simulation
simulations are useful for investigating some aspects of classification rules, but one should not overly rely on their results because little is known about the true distributions of gene expression levels  <cit> . here a simulation was used to investigate the ability to identify informative genes in a simple setting. the simulation involved  <dig> genes with independent normal distributions including   <dig> non-informative genes with mean  <dig> and standard deviation  <dig> in each class and   <dig> informative genes, used for figure  <dig>  each with mean  <dig> and standard deviation  <dig> in class  <dig> and mean  <dig> and standard deviation  <dig> in class  <dig>  sample sizes were  <dig> and  <dig> per class. for goal  <dig>  the classification rule under the wrapper algorithm included both informative genes and performed well for both sample sizes in contrast, the classification rule under the greedy algorithm included only one informative gene and performed poorly with a sample size of  <dig> per class and performed well with a sample size of  <dig> per class . for goal  <dig>  the two informative genes were selected much more frequently than the non-informative genes , as anticipated.

data analysis
for goal  <dig>  the classification rules under both greedy and wrapper algorithms performed well in all data sets except for medulloblastoma . for goal  <dig>  there was good classification in test samples obtained by random splits in all data sets except for medulloblastoma . the most frequently occurring genes among random splits of the training sample associated with good classification were desmin, zyxin, hepsin, and hla class ii. see table  <dig> 

genes listed in bold occur most frequently and are discussed in the text.

discussion
the reason for using auc to measure classification performance in the training sample is that it can be computed quickly under a binormal assumption and is familiar to many researchers. the threshold increase in auc to add a gene to the classification rule of  <dig>  for the greedy algorithm and  <dig>  for the wrapper algorithm represents a small improvement in performance relative to the range of auc from  <dig>  to  <dig> . the specified threshold increase in auc is smaller with the wrapper than with the greedy algorithm because splitting of the training sample into a training-training sample and training-test sample with the wrapper avoids overfitting, unlike the case with the greedy algorithm in which the entire training sample is used both gene selection and evaluation. investigating various values for the threshold increase in auc to determine an optimal threshold increase in auc is not desirable in this setting because it would require the use of the test sample for both rule selection and evaluation, which could bias the results.

some centroid-based classifications of microarray data shrink centroids to the mean of the centroids and select genes based on soft thresholding  <cit> . however this procedure is not desirable for our goals because it "makes the class centroids look more similar to each other"  <cit>  and typically selects many more genes than with our approach. some classification rules are based on the connectivity of each gene in a network  <cit> . however this approach is not desirable for our goal of identifying the few genes most directly predictive of class as some highly connected genes may be selected due to multiple associations with many moderately predictive genes and not because they are highly predictive themselves.

with goal  <dig> of rule discovery and testing for purposes of prediction, one should consider baseline clinical variables as well as microarray data when formulating a classification rule. binary variables can be coded as  <dig> or  <dig>  ordered variables created from continuous variables, such as age categories, can be assigned the midpoint of each category. an ordered variable of low, medium, and high can be treated as two binary variables, one comparing low versus medium and high, and one comparing low and medium versus high. to evaluate the use of classification rules to stratify patients for treatment, in a new sample one could select patients with the highest class probabilities based on the classification rule and randomize them to treatment.

with goal  <dig> of gene discovery, the most frequently occurring genes  among random splits of the training sample in the four data sets with good classification performance in the test samples have an interesting connection to the tissue organization field theory of carcinogenesis. tissue organization field theory postulates that a disruption of intercellular communication between the microenvironment and the cells in which cancer arises is the proximal cause of cancer  <cit> . in contrast the somatic mutation theory postulates that genetic alterations in the cells in which cancer arises are the proximal cause of cancer. desimin is associated with pericytes, cells in the blood vessel walls, that have been implicated in foreign-body carcinogenesis  <cit> , a phenomenon that likely involves disruption of intercellular communication  <cit> . zxyin is associated with morphogenesis  <cit> . hepsin mediates the digestion of extracellular matrix components in initial tumor growth  <cit> . lastly hla class ii is a marker for tumor-infiltrating dendritic cells  <cit> . these genes involve changes in the tumor microenvironment, which is important to the development of cancer under the tissue organization field theory.

CONCLUSIONS
the proposed simple and flexible classification rule that select swirls or ripples after parsimoniously selecting genes and a distance measure is a good basis for either rule discovery and testing or gene discovery.

