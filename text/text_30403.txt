BACKGROUND
in recent years, modern high throughput sequencing technologies have become one of the essential tools in measuring the number of transcripts of each gene in a cell population or even in individual cells. such information could be used to detect differential gene expression due to different treatment or phenotype. in our case we are interested in using gene-expression measurements to classify phenotypes into one of two classes. the accuracy of classification will depend on the manner in which the phenomena are transformed into data by the measurement technology. we consider the effects of next-generation sequencing  and serial analysis of gene expression  on gene-expression classification using currently accepted measurement modeling. the accuracy of classification problem has previously been addressed for the lc-ms proteomics pipeline, where state-of-the-art modeling is more refined, the purpose being to characterize the effect of various noise sources on classification accuracy  <cit> .

ngs technology provides a discrete counting measurement for gene-expression levels. in particular, rna-seq sequences small rna fragments  to measure gene expression. when a gene is expressed, it produces mrnas. the rna-seq experiment randomly shears and converts the rna fragments to cdnas, sequences them, and finally outputs the results in the form of short reads  <cit> . after obtaining those reads, a typical part of a processing pipeline is to map them back to a reference genome to determine the gene-expression levels. the number of reads mapped to a gene on the reference genome defines the count data, which is a discrete measure of the gene-expression levels. two popular models for statistical representation of the discrete ngs data are the negative binomial  <cit>  and poisson  <cit> . the negative binomial model is more general because it can mitigate over-dispersion issues associated with the poisson model; however, with the relatively small number of samples available in most current ngs experiments, it is difficult to accurately estimate the dispersion parameter of the negative binomial model. therefore, in this study we choose to model the ngs data processing pipeline through the transformation via a poisson model, for the purposes of phenotype classification.

sage technology produces short continuous sequences of nucleotides, called tags. after a sage experiment is done, one can measure the expression level of a particular region/gene of interest on the genome by counting the number of tags that map to it. sage is very similar to rna-seq in nature and in terms of statistical modeling. the sage data processing pipeline is traditionally modeled as a poisson random vector  <cit> . we follow the same approach for synthetically generated sage-like data sets.

our overall methodology is to generate three different types of synthetic data:  actual gene expression concentration, called mvn-gc, from a multivariate normal  model formulated to model various aspects of gene expression concentration  <cit> ;  poisson transformed mvn-gc data, called ngs-reads, with specifications that resemble ngs reads; and  poisson transformed mvn-gc data, called sage-tags, where the characteristics of the data model sage data. the classification results related to these three different types of data sets indicate that mvn-gc misclassification errors are lower compared to data subjected to transformations that produce either ngs-reads or sage-tags data. moreover, classification using rna-seq synthetic data outperforms classification using sage data when the number of reads is in an acceptable range for an rna-seq experiment. the better performance is attributed to the significantly higher genome coverage associated with the rna-seq technology.

next-generation sequencing technologies
next-generation sequencing refers to a class of technologies that sequence millions of short dna fragments in parallel, with a relatively low cost. the length and number of the reads differ based on the technology. currently, there are three major commercially available platforms/technologies for ngs:  illumina,  roche, and  life technologies  <cit> . the underlying chemistry and technique used in each platform is unique and affects the output. in this paper, we focus on illumina sequencers and use the ngs term to refer to this platform. high-throughput sequencing and ngs are used interchangeably.

the specific application of ngs for rna sequencing is called rna-seq  <cit> , which is a high-throughput measurement of gene-expression levels of thousands of genes simultaneously as represented by discrete expression values for regions of interest on the genome . ngs has many advantages when compared to the available microarray expression platforms. ngs does not depend on prior knowledge about regions of expression to measure a gene  <cit> , whereas the microarray probes are designed based on known sequences  <cit> . the background correction, probe design and spot filtering, which are typical for microarray-based technology, are no longer problematic due to the different nature of ngs technology. rna-seq enables scientists to discover new splice junctions  <cit> , due to its flexibility and independence of the pre-designed probes. the prediction of absolute copy-number variation  is another great advantage of rna-seq, which allows scientists to identify large segments of deletions/duplications in a genome with respect to another  genome  <cit> . detecting single-nucleotide polymorphisms  is yet another application of rna-seq  <cit> . furthermore, it has been shown that rna-seq can detect spike-in rna controls with good accuracy and reproducibility  <cit> .

the rna-seq process starts with samples that are randomly sheared to generate millions of small rna fragments. these fragments are then converted to cdna and the adapter sequences are ligated to their ends. the length of a fragment can vary between  <dig> bp -  <dig> bp, approximately. the illumina system provides flowcells for sequencing by synthesis and its reversible terminator chemistry <cit> . a flowcell is an eight-channel glass and each channel is commonly referred to as a lane. the size selected fragments with the adapters attach to the flowcell surface inside the lanes and generate clusters of the same fragment through bridge amplification. following the bending and attaching of both sides of the fragment to the surface, the strand duplicates. this process is repeated many times and results in a cluster of fragments. after the cluster generation step, a pool of floating nucleotides is added to the flowcell along with dna polymerase to incorporate to the single strand fragments in each cluster. each nucleotide incorporation makes a unique fluorescent label and the images are captured after the addition. finally, image processing and base calling determine the base at each cycle of the sequencing. each cluster produces a read whose length equals the number of cycles. each rna-seq experiment produces millions of reads depending on the input rna material, length of the reads, desired coverage for the reference genome, number of samples per lane, etc. following the sequencing experiment, the expression levels of the genes are estimated by mapping the reads to the reference genome. there are many algorithms developed for this task, including: eland  <cit> , bowtie  <cit> , bwa  <cit> , maq  <cit> , shrimp  <cit> , mrfast  <cit> , mrsfast  <cit> , soap  <cit> , etc. after the gene expressions are determined, they can be used in further analysis, such as snp detection or detecting differentially expressed genes.

the entire rna-seq sample processing pipeline from generating reads to calling gene expressions can involve different sources of error, e.g. the basecalling is a probabilistic procedure and the quality scores assigned to each base of the reads are prone to small likelihoods of being wrong. the certainty of reference genomes and mapping algorithms are additional issues that need attention. thus, the entire rna-seq sample processing pipeline should be considered from a probabilistic point of view. in this study, we model the above mentioned errors as a noise term in the model of the sample processing pipeline.

discussion
in this section, we consider three specific models for “actual” gene expression data, rna-seq count data and sage tags data. the models are used to synthetically generate data for the simulation experiments described in this paper. the performances of different classification schemes are analyzed and compared across these three synthetically generated types of data.

a common assumption for modeling of the original mrna expressions is that they follow a multivariate gaussian distribution  <cit> . starting with this assumption, we model and generate the rna-seq and sage data by applying a specific nonlinear poisson transformation to the mrna expression model. all data are synthetically generated according to a biologically relevant model to emulate the real experimental situations where the number of features/genes is very large, usually tens of thousands, and only a small number of sample points is available. knowing the full distributional characteristics of the synthetic data makes it possible to measure the classification performance as described by the respective error rates.

mvn-gc model
the model proposed in  <cit>  uses a block-based structure on the covariance matrix, which is a standard tool to model groups of interacting variables where there is negligible interaction between the groups. in genomics, genes within a block represent a distinct pathway and are correlated, whereas genes not in the same group are uncorrelated  <cit> . sample points are drawn randomly and independently from two equally likely classes,  <dig> and  <dig>  each sharing the same d features. there are also c equally likely subclasses in class  <dig> with different parameters for the probability distribution of the features. the subclasses model scenarios typically seen as different stages or subtypes of a cancer. each sample point in class  <dig> belongs to one and only one of these subclasses. features are categorized into two major groups: markers and non-markers. markers resemble genes associated with a disease or condition related to the disease and they have different class-conditional distributions for the two classes.

markers can be further categorized into two different groups: global and heterogeneous markers. global markers take on values from dgm-dimensional gaussian distributions with parameters  for the sample points from class  <dig> and  for the points from class  <dig>  heterogeneous markers, on the other hand, are divided into c subgroups of size dhm, each associated with one of c mutually exclusive subclasses within class  <dig>  therefore, a sample point that belongs to a specific subclass has dhm heterogeneous markers distributed as a dhm-dimensional gaussian with parameters . the same dhm heterogeneous markers for the sample points belonging to other subclasses, as well as points in class  <dig>  follow a different dhm -dimensional gaussian distribution with parameters . we assume that the global and heterogeneous markers have similar structure for the covariance matrices. therefore, we represent the covariance matrices of these two types of markers by Σ0=σ02Σ and Σ1=σ12Σ for class  <dig> and class  <dig>  respectively, where σ <dig> and σ <dig> can be different. for this structure, we assume that Σ has the following block structure:

 Σ=Σρ00…00Σρ0…0⋮⋮⋱⋮⋮00…0Σρ, 

where Σ
ρ
 is an l×l matrix, with  <dig> on the diagonal and ρ off the diagonal. note that the dimension of Σ is different for the global and heterogeneous markers. furthermore, we assume that the mean vectors for the global markers and the heterogeneous markers possess the same structure denoted by μ0=m0× and μ1=m1× for class  <dig> and class  <dig>  respectively, where m <dig> and m <dig> are scalars. the non-markers are also divided into two groups: high-variance and low-variance non-markers. the dhv non-markers belonging to the high-variance group are uncorrelated and their distributions are described by pn+n, where m <dig>  m <dig>  σ <dig> and σ <dig> take values equal to the means and variances of the markers, respectively, and p is a random value uniformly distributed over  <cit> . the dlv low-variance non-markers are uncorrelated and have identical one-dimensional gaussian distributions with parameters  <cit> . figure  <dig> represents the block-based structure of the model.

ngs-reads and sage-tags models
in ngs-type experiments, the gene-expression levels are measured by discrete values providing the number of reads that map to the respective gene. several statistical models have been proposed for representing ngs data. those models are based on either negative binomial or poisson distributions  <cit> . in what follows, we denote the read count for gene j for sample point i by x
i,j
, where it is assumed that in an ngs experiment each lane has a single biological specimen belonging to either class  <dig> or class  <dig>  furthermore, we select a model where the number of reads for each gene is generated from a poisson distribution with a known parameter. we calculate the expected number of reads  from the generalized linear model  <cit> :

  log=logsi+λi,j+θi,j, 

where λ
i,j
 is the jth gene-expression level in lane i. the term θ
i,j
 represents technical effects that might be associated with an experiment. the term logs
i
 is an offset where s
i
, referred to as “sequencing depth” in the statistical community  <cit> , denotes a major factor in the transformation from expression levels to read data. it accounts for different total numbers of reads produced by each lane and plays an important role in normalizing the specimens across the flowcell. the trimmed mean of m values   <cit> , quantile normalization  <cit> , and median count ratio  <cit>  are three commonly used methods for estimating the sequencing depth. equation  represents the expected value of reads, conditioned on the sequencing depth, based on the linear combination of the factors that affect its value: the depth of sequencing, the gene-expression level and a general noise term. therefore, it can be used to model the expected number of reads, as the mean of the poisson distribution in our synthetic data generation pipelines. rewriting equation  yields

  e=siexp, 

indicating that if λ
i,j
 and θ
i,j
 are normally distributed, then exp will have a log-normal distribution. therefore, for a given s
i
 the mean of x
i,j
 is log-normally distributed. this phenomenon has been previously reported for microarray studies where the means of expression levels are shown to have log-normal distributions  <cit> . furthermore, we assume that the offset s
i
 is random and uniformly distributed  <cit> . because the term θ
i,j
 represents the unknown technical effects associated with the experimentation, we assume that it follows a gaussian distribution with zero mean and a variance set by the coefficient of variation :

  θi,j∼n, 

cov aims to model the unknown errors that can occur during an/a ngs/sage experiment, including basecalling, mapping reads to the reference genome, etc. the term e[ x
i,j
|s
i
] serves as the single parameter of a poisson distribution that models the ngs/sage processes by generating random non-negative integers, as the read counts or tag numbers data, having expected value equal to the right-hand side of equation .

to generate synthetic datasets for the purposes of our simulation study we proceed as follows: for a sample point i in the experiment, we first randomly generate a number s
i
 from a uniform distribution, u , where α >  <dig> and β > α. then, a random sample point, λ
i,j
, is drawn from the mvn-gc model and its value is perturbed with θ
i,j
, which is drawn randomly according to its distribution defined in . using , the mean of the poisson distribution is calculated for each sample point i and gene j, and a single random realization of x
i,j
 is generated. the processes of generating count data for rna-seq reads and sage tag numbers are very similar, but the total number of reads per ngs run is significantly more than tags for a sage experiment. therefore, we only change α and β to get the desired number of read counts or tags. we also assume that the sage experiments always have a fixed range for the total number of tags, whereas rna-seq has a variety of ranges for the total read counts. by having different ranges for the read counts, we can compare the performance of classification resulting from ngs-reads and sage-tags models under different experimental settings.

classification schemes
the setup for the classification problem is determined by a joint feature-label probability distribution f
x
y
, where x∈rd is a random feature vector and y ∈ { <dig> } is the unknown class label of x. in the context of genomics, the feature vector is usually the expression levels of many genes and the class labels are different types or stages of disease to which sample points belong to. a classifier rule model is defined as a pair , where Ψ is a classification rule, possibly including feature selection, and Ξ is a training-data error estimation rule. in a typical classification task, a random training set sn={,,…,} is drawn from f
x
y
 and the goal is to design a classifier ψn=Ψ, which takes x as the input and outputs a label y. the true classification error of ψ
n
 is given by ε
n
 = p  ≠ y). the error estimation rule Ξ provides an error estimate, ε^n=Ξ, for ψ
n
.

in this study, we consider linear discriminant analysis , three nearest neighbors  and radial basis function support vector machine  as the classification rules, and report the true error of the classifiers. we implement t-test feature selection  before the classifier design procedure to select d ≤ d features with highest t-scores. the training set with d features is then used to design the classifier. the same d features are also used for finding the true error of the designed classifier.

lda is the plug-in rule for the bayes classifier when the class-conditional densities are multivariate gaussian with a common covariance matrix  <cit> . the sample means and pooled sample covariance matrix are estimated from the training data sn and plugged into the discriminant function. if the classes are equally likely, lda assigns x to class  <dig> if and only if

  tΣ^−1≤tΣ^− <dig>  

where μ¯y is the sample mean for class y ∈ { <dig> }, and Σ^ is the pooled sample covariance matrix.

3nn is a special case of knn rule , which is a nonparametric classification rule based on the training data. the knn classifier assigns a label,  <dig> or  <dig>  to a point x according to the majority of the labels of the k nearest training data points to it. to avoid tied votes in binary classification problems, an odd number is usually chosen for k.

a support vector machine finds a maximal margin hyperplane for a given set of training sample points. if it is not possible to linearly separate the data, one can introduce some slack variables in the optimization procedure that allow the mislabeled sample points and solve the dual problem. alternatively, one can transform the data and project it into a higher-dimensional space, where it becomes linearly separable. the equivalent classifier back in the original feature space will generally be non-linear  <cit> . if a gaussian radial basis function used as the kernel function, the corresponding classifier is referred to as rbf-svm.

simulation setup and parameters
to emulate real-world scenarios involving thousands of genes with only a few sample points, we choose d =  <dig>  as the number of features and n ∈ { <dig>   <dig>  180} as the number of sample points available for each synthetic feature-label distribution. because there is no closed form expression to calculate the true error of the designed classifiers, we generate a large independent test sample of size n
t
 =  <dig>  with samples divided equally between the two classes. because the rms between the true and estimated error when using independent test data is bounded above by 1/2nt, this test sample size provides an error-estimate rms of less than  <dig> . once the training and test data are generated, we normalize the training data so that each feature is zero-mean unit-variance across all sample points in both classes. we also apply the same normalization coefficients from the training data to the test set. the normalized data are then used in the feature selection, classifier design and error calculation. the parameter settings for the mvn-gc model, sage-tags and ngs-reads are provided in table  <dig> 

test sample size n
t
as explained in the previous section, the datasets generated from the mvn-gc model are transformed into the ngs-reads and sage-tags datasets through equation  and poisson processes. we only need to properly set the parameters cov, α, and β to get the desired number of read counts or tags. we assume that the parameter cov can take on two values,  <dig>  and  <dig> , representing two different levels of noise and unknown errors in the experiment. in its current state, rna-seq technology can provide different numbers of reads per sample, depending on many factors, such as quality of the sample, the desired coverage, sample multiplexing, etc. in this study, we examine a variety of ranges for rna-seq experiments. we start with a very low total number of rna-seq reads, which may not match the real-world experiments, however, they are necessary for comparing the sage-tags and ngs-reads models with similar coverage. furthermore, demonstrating the classification results for the ngs-reads model with wide ranges introduces an extra internal variability, which makes interpretations of the results rather difficult. table  <dig> lists the ngs-reads ranges we have considered in this study. in a typical sage experiment, one expects 50k to 100k tags  <cit> . using trial and error, we have found that by choosing the parameters α =  <dig>  and β =  <dig>  in the distribution of s
i
, the observed number of tags usually falls within this range. similarly, the parameters α and β are chosen to meet the range requirements in the ngs-reads model.

our goal is to study the performance of the traditional classification schemes on different sources of random samples; thus, we take a monte-carlo approach and generate  <dig>  random training sets of size n from the mvn-gc model, transform them to the corresponding ngs-reads and sage-tags samples, and apply the classification schemes to each training sample. by taking such an approach, we aim to compare the classification performance of the three pipelines, in terms of the true error of the deigned classifiers. we also study the effect of ngs-reads and sage-tags transformations on the performance of a simple t-test biomarker discovery method, where we report the probability that global markers are recovered when d ≪ d features are selected after the feature-selection step.

RESULTS
the probability of recovering a certain number of global markers after a t-test feature selection can be approximated empirically by the percentage of experiments  that detect such a number of markers. this probability depends on the size of the training data sample, quality of features, and underlying joint probability distribution of the features. here, we only show the results for the mvn-gc model, with d =  <dig>  in table  <dig>  tables  <dig>   <dig>   <dig> and  <dig> represent the corresponding results for ngs-reads for different combinations of cov and ρ: { <dig> ,  <dig> }, { <dig> ,  <dig> }, { <dig> ,  <dig> }, { <dig> ,  <dig> }, respectively. in each table, we also report the corresponding results for the sage-tags model in a row with the ngs-reads range of . a successful feature selection should identify as many global markers as possible, however the situation is worsened because with small sample sizes the noisy features sometimes may appear to be good among the  <dig>  features. as the number of sample points increases, we expect to get better results for the feature selection and this is exactly what we see in tables  <dig>   <dig>   <dig>   <dig> and  <dig>  another important observation in tables  <dig>   <dig>   <dig> and  <dig> is the effect of the total number of reads and cov for the ngs-reads models. as the number of reads increases, it is more likely to pick up more global markers, until the number of reads reaches a threshold, where no further improvement is observed. similarly, for smaller cov the probability of selecting more global markers also increases.

=0
.
4

=0
.
8

=0
.
4

=0
.
8

the true error of a designed classifier measures the generalization capability of the classifier on a future sample point. given a set of training sample points and a classification rule, one needs the full feature-label probability distribution to calculate the true error of the classifier designed on the training set. in this study, we find the true error of a classifier with a large independent sample. because the training sample is random, the true error is a random variable with its own probability distribution. therefore, to demonstrate the actual performance of the designed classifiers, the estimated probability density function  of the true error for each classification rule, distribution model and all data generation models  is presented.

we report the true errors of the designed classifiers for two different feature-selection schemes and classification rules. in the first scheme, no feature selection is performed and the first d global markers are directly used for classification. in the second scheme, t-test feature selection is done before a classifier is designed to select the best d features. figures  <dig>   <dig> and  <dig> show the salient finding of this study. we present the pdf of the true error for different classification rules trained on  <dig> and  <dig> sample points when the correlation among features in the same block is high  and cov =  <dig> . distributions with higher and tighter densities around lower true errors indicate better classifier performance. if the pdfs can be approximated as univariate gaussians, then a good classification performance amounts to smaller mean and variance. in all cases, for similar sample sizes and similar settings for ρ and cov, mvn-gc outperforms the two other data types. this is not surprising since it is considered as the ground truth. also, it is evident that a larger sample size gives better classifiers with smaller variance as illustrated by the distribution of the true error.
t
-test feature selection ,
ρ
=0
.
 <dig>  cov =0
.
 <dig> ∑jxi,j∈ for sage-tags and the following total number of reads for ngs-reads: ∑jxi,j∈; ∑jxi,j∈; ∑jxi,j∈; ∑jxi,j∈.
t
-test feature selection ,
ρ
= <dig> , cov = <dig> ,∑jxi,j∈ for sage-tags and the following total number of reads for ngs-reads: ∑jxi,j∈; ∑jxi,j∈; ∑jxi,j∈; ∑jxi,j∈.
t
-test feature selection ,
ρ
= <dig> , cov = <dig> ,∑jxi,j∈ for sage-tags and the following total number of reads for ngs-reads: ∑jxi,j∈; ∑jxi,j∈; ∑jxi,j∈; ∑jxi,j∈.

figures  <dig>   <dig> and  <dig> show that utilizing the best d features  in the model, sage-tags and ngs-reads for small ranges of the total reads yield similar results  in terms of the classification performance, and both are inferior when compared to the ground truth. this may lead to a conclusion that one should not expect improvements in classification performance when gene-expression levels are processed through an ngs-reads pipeline. however, our main objective here is to show that as one increases the total number of reads for the ngs-reads model, improvement can be achieved and the error rates decrease. this conclusion confirms the intuition provided by a simple calculation about the increase of the separability of two poisson random variables with means proportional to the number of reads. however, notice that the modeling of the sequencing pipeline introduces randomness in the means of the respective poisson parameters describing the individual gene reads. moreover, our focus is not that much on the separability of the two classes/phenotypes but rather on the classification performance as measured by the classification error and its dependence on ground truth  sample size, sequencing depth, and feature vectors. our goal for modeling ngs-reads with small ranges is to demonstrate the performance of sage and rna-seq, when their data is similar. this result suggests that having a larger number of reads for the rna-seq experiments could compensate for the errors that can occur during the ngs-reads pipeline sample processing. here we have shown the results only for cov =  <dig> , since in our observations, it appears that changing cov has little effect on the distribution of the true error for both sage-tags and ngs-reads models, except that it slightly increases the variance of the pdfs. the effect of feature selection  on the performance is best shown when the sample size is small. in this case, the variance of the true error is so large that drawing any conclusion regarding the performance on a small sample would be futile. another interesting observation is that, on average, 3nn and rbf-svm classification rules outperform lda for both ngs-reads and sage-tags data, supporting the idea that using linear classifiers for these types of data is not the best way to proceed in a highly non-gaussian model – as is the case after the data have gone through the pipeline. the best rates for the expected true error across all settings are achieved by the rbf-svm classification rule, even for small samples, especially with small variance.

CONCLUSIONS
in this paper, we model gene-expression levels as a multivariate gaussian distribution that statistically captures the real mrna levels within the cells. the newly developed technologies of sequencing dna/rna, referred to as ngs, and their ascendant sage technology generate discrete measures for gene expressions. the count data from these technologies can be modeled with a poisson distribution. the multivariate gaussian gene expressions are transformed through a poisson filter to model ngs and sage technologies. the three categories of data are subjected to feature selection and classification. the objective is to evaluate the performance of the ngs technologies in classification. our simulations show that when the gene expressions are directly used in classification, the best performance in terms of the classification error is achieved. the ngs-reads model generates considerably higher coverage for genes and can outperform sage in classification, when the experiment generates large number of reads. even though ngs still has a variety of error sources involved in its process, its high volume of reads for a specific gene can lower the chance of misclassification. thus, it is important to use the highest possible coverage for the entire genome while performing a rna-seq analysis if the goal of the study is to distinguish the samples of interest. nevertheless, one must recognize that, as is typical with nonlinear transformations, the ngs pipeline transforms the original gaussian data in such a way as to increase classification difficulty. as more refined modeling becomes available for the ngs pipeline, further study needs to performed, as has been done in the case of the lc-ms proteomics pipeline  <cit> , to determine which segments of the pipeline are most detrimental to classification and what, if anything, can be done to mitigate classification degradation.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
ng proposed the main idea, planned/structured the study, and worked on the simulations and manuscript. mry contributed to the main idea, simulation, structure of the study, and the manuscript. cdj and ii helped in conceiving the study and revised the manuscript. erd contributed to the formulation of main idea and also to the steps of the study, and revised the manuscript. all authors read and approved the final manuscript.
