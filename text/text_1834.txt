BACKGROUND
segmental structure of various scales exists in genomic sequences. many evolutionary and genetic mechanisms leading to variation in dna operate on segments of the genome . furthermore, eukaryotic chromosomes consist of alternating regions of gene-rich and gene-poor regions. a gene-rich region can be further decomposed into non-coding segments, segments that contain regulatory information, and genes, which in turn consist of introns and exons. also, remnants of viral or microbial inserts in a genome form a type of segmental structure.

there are many types of features with which one can segment the sequences. for any given technique, there may exist alternative biological features to segment on. for example, if the goal is to identify coding and noncoding segments in a sequence, one may study the distribution of three-letter words  along the sequence to determine where to set the segment boundaries. an alternative would be, for example, to segment on the frequency of nucleotides in each third position, as done in  <cit> .

there are also many possible segmentation techniques for discovering the segmental structure in dna sequences. the techniques include recursive segmentation methods  <cit> , bayesian methods  <cit> , hidden markov models  <cit> , and wavelet analysis  <cit> , among others. for a review of various approaches, see  <cit> .

if a reliable annotation of the underlying segmental structure exists, it is of interest to find out which feature set and segmentation method give a result closest to the true segmental structure. this can give insight into the biological process that is responsible for creating and maintaining the segmental structure. furthermore, the segmentation technique and feature set that is the best on an annotated sequence may also yield results close to true segmental structure on unannotated data.

in order to find differences between segmentations one has to define a notion of similarity or distance between them. in this paper we describe the distance between two segmentations p and q by two numbers – conditional entropy of p given q, and vice versa. for a   similar approach to comparing clusterings see meilă  <cit> . conditional   entropy is an information theoretic measure that quantifies the amount   of information that one segmentation gives about the other. the sum of   these two conditional entropies also defines the entropy distance   between the two segmentations. ideally, we want both terms in the sum   to be small, instead of only requiring their sum to be small.  we give   an example of how this makes a difference when comparing segmentations.   by using this measure we can easily rank a set of segmentations with   respect to their distance from the underlying true segmentation, if one   is known.

the sum of these two conditional entropies also defines the entropy distance between the two segmentations. ideally, we want both terms in the sum to be small, instead of only requiring their sum to be small. we give an example of how this makes a difference when comparing segmentations. by using this measure we can easily rank a set of segmentations with respect to their distance from the underlying true segmentation, if one is known.

knowing the best segmentation technique and feature set for a given sequence is still not enough. namely, one candidate segmentation could be better than the other candidates, but all could still be quite far from the true one. that is, we want to find out if the best result is in some sense significant. the problem of deciding segmentation significance has been addressed before in the case where the sequence data itself is known  <cit> . our approach does not rely on the sequence data, since it takes as input only the set of segmentations we want to evaluate. our technique is therefore more general, and also applies to cases where the segmentations are obtained by using alternative biological features. to our knowledge, the issue of significance of segmentation similarity has not been considered before.

we test the significance of segmentation similarity by generating random segmentations and computing the distances between the underlying true segmentation and the randomized segmentations. if a random segmentation is about as close to the true one as our candidate segmentation is, then the agreement between the true segmentation and our candidate one is not very interesting. note that in our randomization approach we do not make any assumptions regarding the properties of a good segmentation, but only consider the values of conditional entropies summarizing the similarity between two segmentations. to define a randomization procedure, we have to specify the class of segmentations from which we sample random elements. in this paper we use two classes of segmentation:  segmentations that have a given k number of segments, and  segmentations that have the same number of segments and the same segment-length distribution as the true segmentation.

we apply this randomization technique to examples on coding-noncoding structure and to isochore detection. the results show that the small distances obtained by some segmentation techniques and biological features are indeed significant, while for others, the obtained segmentations are only as similar to the ground truth as a majority of the randomly generated segmentations.

RESULTS
in this section we show the results of our randomization techniques for evaluating the discovered segmentations in two examples of genomic sequences for which a segmental structure is already known.

example 1: coding-noncoding structure
discovering the locations of genes in a dna sequence is an important task to which computational methods give different predictions. in this example, we evaluate the closeness of different segmentation results to known gene boundaries.

we used a dataset consisting of a  <dig> kb region of bacterium rickettsia prowazekii  <cit>  , containing  <dig> coding segments. the correct number of segments in the data is  <dig> . we denote the correct underlying segmentation into coding and noncoding regions by t.

we applied the encoding scheme described in  <cit>  to transform the dna sequence into a 12-dimensional signal. this encoding of the sequence captures the codon usage, see  <cit>  and references therein. the segmentation techniques we applied are the entropic segmentation  by bernaola-galván et al.  <cit>  and the least squares segmentation method . in the entropic segmentation the input is split recursively until no more significant splits can be found. the optimal least squares segmentation with k segments is computed with a dynamic-programming algorithm  <cit> . note that the entropic segmentation decides on the number of segments, while the least-squares method takes the number of segments as input. the entropic segmentation method found  <dig> segments  for this dataset, outputting segmentation ecd. the least squares method for k =  <dig> output segmentation lcd.

we additionally applied the least squares method on six other features: frequencies of 2-letter words , 1-letter words , and each nucleotide separately . we denote these features by { <dig> , a, c, g, t} respectively. the sequences we constructed in such a way correspond to a 16-dimensional, a 4-dimensional and four 1-dimensional signals. each point in these sequences corresponds to the frequency of a feature in an  <dig> bp window. we divided the sequence into  <dig> non-overlapping windows.

the obtained segmentations are shown in figure  <dig>  along with the known boundaries t. we use ef and lf to denote the output of the entropic and the dynamic-programming algorithm on a sequence obtained using feature f. from the figure its hard to conclude which one of the output segmentations is the closest to t and whether the same measure of agreement could arise by chance.

to evaluate the significance of our findings we applied the randomization tests described in the methods section. we generated random segmentations from classes cn,k
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarbqabaaaaa@348b@ and cn,k,ℓ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarjab+xcasiablorisbqabaaaaa@3698@. the distributions of conditional entropies h  and h  for r ∈ cn,k
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarbqabaaaaa@348b@ for and r ∈ cn,k,ℓ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarjab+xcasiablorisbqabaaaaa@3698@ in  <dig>  randomizations are shown in figure  <dig>  the conditional entropies and the empirical p-values are given in table  <dig>  the values pℓ and pk correspond to the p-values obtained by ℓ and k-randomizations respectively. from the figure and the table we see that segmentations lcd, ecd, l <dig> and l <dig> and lt are all closer to the ground truth segmentation than would be expected by chance. for any of the above segmentations, the values of the two conditional entropies  are much lower than those appearing in the two randomization tests. on the other hand, segmentations la and lc should not be considered similar to t. although h  and h  are small, the values of h  and h  do occur often in the randomization. thus segmentations la and lc  fail one of the two randomization tests and are not considered to be close to the ground truth. in this example, the choice between cn,k
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarbqabaaaaa@348b@ and cn,k,ℓ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarjab+xcasiablorisbqabaaaaa@3698@ does not make a difference in deciding the significance.

conditional entropies and their significances for segmentations on coding-noncoding data. ecd: entropic segmentation with codon features. lf: least-squares segmentation with features f; f ∈ {cd,  <dig>   <dig>  a, c, g, t} indicate the codon feature, frequencies of 2-letter words, 1-letter words, and frequency of a, c, g, or t, respectively, pℓ: the fraction of segmentations from cn,k,ℓ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarjab+xcasiablorisbqabaaaaa@3698@ with a smaller value of the conditional entropy in  <dig>  randomizations; pk: the fraction of segmentations from cn,k
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarbqabaaaaa@348b@ with a smaller value of the conditional entropy in  <dig>  randomizations.

example 2: isochore structure
isochores are large  dna segments fairly homogeneous in their guanine and cytosine  content. exactly defining isochore borders in the human genome remains an open problem, but different computational approaches exist. isochores are discussed in biological literature already since  <cit> . in our experiments we used a dataset consisting of the  <dig> mb short arm telomeric region of human chromosome  <dig> . unlike in the previous example, there is no well defined biological annotation of the segmental  structure of this genomic region. we consider as the ground truth segmentation t results by costantini et al.  <cit>  and alternatively results by isofinder  <cit> . these results differ considerably in their segment number , reflecting genomic structures at different granularities. we also study segmentations on the major histocompatibility  region in chromosome  <dig>  for which some biologically validated isochore boundaries are known. to generate candidate segmentations, we use the least squares dynamic programming algorithm  described in the previous example, with the same number of segments as the in respective t.

first, we consider the segmentation results by costantini et al.  <cit>  as the ground truth segmentation t, in which there are  <dig> segments. for generating candidate segmentations, we aggregated the data into  <dig> kb windows . we use lgc to denote the output segmentation of the least squares method on the signal summarizing the g+c content of the sequence. besides the g+c content we also applied the least-squares method on the six features we considered in the previous section. therefore, we again constructed the sequences on features { <dig>   <dig>  a, c, g, t}, that correspond to the frequencies of 2-letter words , 1-letter words , and each nucleotide separately . the segmentations are shown in figure  <dig>  along with the chosen reference segmentation t. again, it is not immediately clear which one of the segmentations is closest to t. the conditional entropies and the corresponding p-values for ℓ and k-randomizations are shown in table  <dig>  figure  <dig> shows the distributions of conditional entropies h  and h  for r ∈ cn,k
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarbqabaaaaa@348b@ for and r ∈ cn,k,ℓ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarjab+xcasiablorisbqabaaaaa@3698@ in  <dig>  randomizations. segmentations lgc, l <dig>  l <dig>  lc and lg are significantly close to t. this is somehow expected given the importance of g and c concentration in the isochore structure. note that segmentation ifgc, as a high entropy segmentation, has a significantly small value of h , but the value of h  is by far larger that the entropy of h  for any random segmentation r.

conditional entropies and entropy distances for segmentations on isochore structure, for the ground truth segmentation from  <cit>  with k =  <dig>  lf: least-squares segmentation with features f; f ∈ {gc,  <dig>   <dig>  a, c, g, t} indicate frequencies of g+c, 2-letter words, 1-letter words, and frequency of a, c, g, or t, respectively, pℓ: the fraction of segmentations from cn,k,ℓ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarjab+xcasiablorisbqabaaaaa@3698@ with a smaller value of the conditional entropy in  <dig>  randomizations; pk: the fraction of segmentations from cn,k
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarbqabaaaaa@348b@ with a smaller value of the conditional entropy in  <dig>  randomizations.

next we consider isofinder  <cit>  results as the ground truth segmentation t, with k =  <dig>  we generated least squares candidate segmentations lf with this k and the same features f as in the previous case, the only difference being that here the sequence was aggregated into  <dig> kb windows . the segmentations are shown in additional file  <dig>  along with the chosen reference segmentation t. it is not clear from the figure which of the segmentations, if any, are close to t. the distributions of conditional entropies h  and h  for r ∈ cn,k
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarbqabaaaaa@348b@ for and r ∈ cn,k,ℓ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaruwrpxgbtfmbzbaceagae83qam0aasbaasqaagqaciab+5eaohqaaiab9xcasiab+tgarjab+xcasiablorisbqabaaaaa@3698@ in  <dig>  randomizations are also shown in the additional file  <dig>  in this case, the significance results depend on the choice of k-versus ℓ-randomization: if the former is chosen, all the candidate segmentations are significantly close to t. this means that the least squares segmentations are indeed significantly closer to t than segmentations with randomly assigned k boundaries. in the case of ℓ-randomization, we find that h  < h  for all our candidate segmentations lf. thus all segmentations lf are far from t. the reason is that the segment length distribution of t is far from the distribution for any lf, and this prevents the candidate segmentations from being significantly close to t.

we also study the major histocompatibility  region in chromosome  <dig> . the  <dig>  mb region contains two experimentally validated isochore structures  <cit> . the known mhc isochore boundaries are around  <dig>  mb,  <dig>  mb and  <dig>  mb. we studied predictions from wavelet multiresolution analysis by wen and zhang  <cit>  , results by costantini et al.  <cit>  , and predictions by the tool isofinder  <cit>  , choosing in turn each one of these as the ground truth and thus performing three randomization experiments. we used least squares segmentations  as alternative candidate segmentations . the segmentations and results from the randomization tests are shown in additional file  <dig>  the randomization tests show that all segmentations, except in some cases those for features a and t, are significantly close to the chosen ground truth segmentation. in particular, all the ground truth segmentations are found to be significantly close to each other.

CONCLUSIONS
in biological sequence analysis, there exist situations where many alternative segmentations for the underlying biological structure are proposed. we give a framework for evaluating the quality of results produced by different segmentation methods. our approach also applies to cases where the segmentations are obtained by using alternative biological features, as we base our analysis only on the segment boundaries. applicable segmentation distance measures and randomization tests are discussed, and results are shown for two applications of segmenting genomic data.

