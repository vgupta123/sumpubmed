BACKGROUND
protein loop structure modeling is important in structural biology for its wide applications, including determining the surface loop regions in homology modeling  <cit> , defining segments in nmr spectroscopy experiments  <cit> , designing antibodies  <cit> , and modeling ion channels  <cit> . typically, the protein loop structure modeling procedure involves the following steps  <cit> . first of all, the structural conformation space is sampled to produce a large ensemble of backbone models satisfying certain conditions such as loop closure, clash-free, and low score . secondly, clustering algorithms are applied to select representative models from these backbone models. thirdly, side chains are added to the representative models to build all-atom models and their structures are further optimized by score minimization. finally, the models are assessed and the "best" ones will be selected as the predicted conformations.

in many loop modeling methods  <cit> , sample loop conformations are constructed by dihedral angle buildup or fragment library search  <cit> . recently, mandell et al.  <cit>  developed a kinematic closure approach, which can construct loop conformations within a 1a resolution. nevertheless, scoring functions used to guide loop modeling vary widely. rohl et al.  <cit>  optimized the rosetta score using fragment buildup. fiser et al.  <cit>  used a hybrid scoring function by summing up charmm force field terms and statistically derived terms. xiang et al.  <cit>  developed a combined energy function with force-field energy and rmsd  dependent terms. they also developed the concept of "colony energy" that has been used by fogolari and tosatto  <cit>  as well, for considering the loop entropy  as part of the total free energy. olson et al.  <cit>  used a multiscale approach based on physical potentials. an efficient grid-based force field has been employed by cui et al.  <cit> . jacobson et al.  <cit> , zhu et al.  <cit> , rapp and friesner  <cit> , de bakker et al.  <cit> , felts et al.  <cit> , and rapp et al.  <cit>  employed physics-based energy schemes with various solvent models. soto et al.  <cit>  found that using the statistical potential dfire  <cit>  as a filter prior to all-atom physics-based energy minimization can improve prediction accuracy and reduce computation time. dfire has previously proven to be successful by itself for loop selection  <cit> . all these methods have led to recent significant progress in generating high-resolution loop models and several loop prediction servers are now available .

in practice, the value of computer-generated protein loop models in biological research relies critically on their accuracy. while efficiently sampling the protein loop conformation space to produce sufficient number of low-energy models to cover conformations with good structures remains a challenging issue, another critical problem is the insensitivity of the existing protein scoring functions. these scoring functions are developed to estimate the energy of the protein molecule. the insensitivity of the scoring functions leads to difficulty in distinguishing the native or native-like conformations from the erroneous models, and thus restricts the loop structure prediction accuracy. therefore, selecting the highest quality loop models from a number of other models is a critical step in solving the protein loop structure prediction problem.

the scoring functions play a significant role in protein structure assessment and selection. although a number of scoring functions are currently available for protein loop model evaluation, there is no generally reliable one that can always distinguish the native or near native models. every existing scoring function has its own pros and cons. recently, the strategy of using multiple scoring functions to estimate the quality of models and improve selection was proposed in protein folding and protein-ligand docking  <cit> . multiple, carefully selected scoring functions are integrated and selection improvements can be achieved by tolerating the insensitivity and deficiency of every individual scoring function. thus, the multiple scoring functions method can usually lead to a better performance than an individual scoring function.

similar to structure prediction in an overall protein, the scoring functions that have been used in loop modeling can be categorized into knowledge-based  <cit>  and physics-based  <cit> . the knowledge-based scoring functions are typically derived from protein structural databases such as the pdb and thus incorporate empirical criteria to distinguish the native structure from the misfolds. by contrast, the physics-based scoring functions are developed based on first principle concepts, where electrostatic, van der waals, hydrogen bonds, solvation, and covalent interactions are taken into account.

there are problems in theoretical justification of both the physics- and knowledge-based scoring functions for protein structure modeling. ideally, a physics-based scoring function would be evaluated with quantum mechanics, in which case the score could reflect the true energy. in computation practice, quantum mechanics is wildly intractable due to the size of protein molecule. as a compromise, the physics-based scoring functions  are developed mainly based on classical physics to approximate the true energy of a protein molecule. on the other hand, the knowledge-based functions derive their rules from the existing experimental structure data, typically by applying the inverse boltzmann law. however, because compared to the unknown structures, the known structures are in an extremely small fraction, the data used to develop knowledge-based functions are potentially undersampled  <cit> . moreover, studies have shown that inter-residue interactions may not be considered as independent factors  <cit> , which violates the assumption of inverse boltzmann law. in consequence, all these aspects led to inaccuracy or insensitivity factors in the existing scoring functions for protein loop modeling, as is true in overall protein structure modeling. that is, in practice, the native conformation usually does not exhibit the lowest score when it is put among the models generated by the computer simulation program  <cit> . moreover, in the low score regions, a conformation with a relatively higher score may in fact be a more reasonable structure than the one with a lower score. the score-rmsd plots in figure  <dig> show that in the decoy set of 1onc, the best model  never yields the lowest score in dfire  <cit> , triplet backbone dihedral potential  <cit> , opls-aa/sgb  <cit> , rosetta  <cit> , or dope  <cit> , which strongly indicates insensitivity in each individual scoring function.

in this paper, we present a pareto optimality consensus  method based on the pareto optimality  <cit>  and fuzzy dominance theory  <cit>  to take advantage of multiple scoring functions for ranking protein loop models. the rationale is to identify the models at the pareto optimal front of the function space of a set of carefully selected scoring functions and then to rank them based on the fuzzy dominance relationship relative to the other models. for protein loop structure ranking, we employ five knowledge- or physics-based scoring  functions: dfire  <cit> , our triplet backbone dihedral potential  <cit> , opls-aa/sgb  <cit> , all-atom rosetta  <cit> , and dope  <cit> . all of these scoring functions have shown efficiency in loop modeling in the literature  <cit> . we apply our approach to the loop decoy sets generated by jacobson et al.  <cit> . the loops in jacobson's decoy sets are regarded as "difficult" targets  <cit> . there are frequent pro and gly occurrences in these loops. cys are treated separately in both reduced and oxidized forms to take the formation of disulfide bridges into account. the loop positions are random to make possible encountering of all sorts of situations. jacobson's decoy sets have been frequently used as a benchmark for loop prediction and effectiveness of scoring functions  <cit> . the original loop decoy sets include targets whose native protein structures have certain exceptional features such as high or low ph values when crystallized, explicit interactions between the target loops and heteroatoms, and low resolution crystal structures in target loop regions with large measured b-factors  <cit> . jacobson et al. also provide a filtered list of decoy sets by eliminating targets with the above exceptional features. since none of the scoring functions we used makes assumptions of these exceptional features, we only consider the filtered decoy sets in this paper. in addition to jacobson's decoy sets, we apply our method to more recent decoy sets for  <dig> loops chosen from  <dig> chains in  <dig> membrane proteins  <cit> . we also compared the poc method with the hydrophobic potential of mean force  approach for loop model selection as well as other multiple scoring functions ranking strategies  <cit> , including rank-by-number, rank-by-rank, rank-by-vote, and regression-based methods.

methods
the consensus strategy
although each scoring function may have certain insensitivity and inaccuracy, combining multiple, carefully selected scoring functions may effectively tolerate the deficiencies existent in the single scoring functions. for example, as shown in figure  <dig>  models yielding lowest score in one individual scoring function have higher scores than the best model in other scoring functions. however, the multiple coordinate plot in figure  <dig> shows that the decoys commonly yielding low scores in all scoring functions are the best decoys in the 1onc decoy set. as a result, efficiently integrating multiple good scoring functions may lead to resolution improvement in selecting the best decoys in the decoy set.

the pareto optimality consensus method
the rationale of the poc method is to rank a model according to its pareto-dominance relationship to the other models in the model set. the first step of the poc method is to identify models with pareto-optimality. the definition of the pareto-optimality  <cit>  is based on the dominance relationship among models in the model set. a model u is said to dominate another model v  if both conditions i) and ii) are satisfied:

i) for each scoring function fi, fi ≤ fi holds for all i;

ii) there is at least one scoring function fj where fj <fj is satisfied.

by definition, the models which are not dominated by any other models in the model set form the pareto-optimal solution set. a pareto-optimal model possesses certain optimality compared to the other ones in the model set.

once the pareto-optimal models are identified, the next step in the poc method is to rank these models, so that the model that exhibits most optimality over other models in the model set will have the best rank. a simple solution, which is used in several evolutionary algorithms for multi-objective optimization  <cit> , is to count the number of models in the model set that each pareto-optimal model dominates. then the pareto-optimal model that dominates most other models is top-ranked. the major disadvantage of this simple solution is that it cannot accurately measure the significance of the dominance relationship between two decoys. figure  <dig> shows an example of  <dig> models in a two-dimensional functional space, where a dominates both b and c while b and c do not dominate each other. the simple solution is not able to distinguish between the dominance relationships a ≺ b and a ≺ c, although a seems to have a "stronger" degree of dominance to c than to b.

to more accurately measure the dominance relationship, we adopt a fuzzy scheme  <cit>  for model ranking. first of all, a function g):  →  <cit>  is used to normalize each scoring function fi. then, a bounded division operation, ÷, is defined as  

finally, the fuzzy pareto dominance relation between two models u and v can be written as follows: model u dominates model v by degree μa where  

for all normalized scoring functions g). similarly, model u is dominated by model v by degree μp where  

for all normalized scoring functions g). in our current poc method, we use a linear membership function, min/y, as suggested in  <cit> , and the fuzzy scheme does not bias to any individual scoring functions.

for the example shown in figure  <dig>  μa =  <dig> , μp =  <dig> , μa =  <dig> , and μp =  <dig> . as a result, a shows a more significant dominance to c than to b in the fuzzy dominance scheme.

the ranking value for model xi, r, is computed as  

which will be used to rank the pareto-optimal models. for ranking of the whole model set, we firstly identify the pareto-optimal models and rank them according to fuzzy pareto dominance relationship. then, we remove the pareto-optimal models, identify the pareto-optimal models for the rest of the models, and assign ranks to them. the procedure is repeated until there are no more models left in the model set.

RESULTS
effectiveness of the pareto optimal models
because in the poc method, selection and ranking are based on pareto optimality, the quality of the pareto-optimal models is critical. the pareto-optimal models include not only those optimums in individual scoring functions, but also the non-dominated ones yielding certain optimality in the  combination of various scoring functions. in our computational experiment, five scoring functions, including rosetta, ddfire, dope, triplet backbone dihedral, and opls-aa/sgb, are selected to form the function space. figure  <dig> shows that the average number of the pareto optimal decoys is around 20% or less of the total number of decoys in the jacobson's decoy sets for 4- to 12-residue targets. as shown in figure  <dig>  the pareto optimal decoys have efficient coverage of the best decoy or one close to the best decoy in a target's decoy set. in more than 82% of the loop targets, the pareto-optimal decoys include the best decoy of the target, whereas in more than 97% of the loop targets, the pareto-optimal decoys include decoys within  <dig> a rmsd to the best one. moreover,  <dig> out of  <dig> targets include decoys within  <dig> a rmsd cutoff to the best decoy. figure  <dig> shows the rmsd distribution of the decoys in the sets corresponding to the 9-residue loop targets as well as the coverage of the parento optimal decoys. one can find that in most of the 9-residue targets, the very best decoy is in the pareto-optimal decoy set, which typically contains only 5%~20% of the decoys from the original decoy set. this indicates that the selected scoring functions can efficiently identify a much smaller set of decoys that contains the best decoy or one very close to the best.

efficiency in identifying near-native structures
we applied the poc method to the decoy sets generated by jacobson et al. the decoy set for each target contains very good models  derived from the native structure by optimizing the opls-aa/sgb force field as well as other models generated by hierarchical comparative modeling  <cit> .

by considering a decoy with rmsd less than  <dig> a as a near-native one, a false positive is a non-near-native decoy with a high rank. figure  <dig> shows the overall percentages of the targets in which the top-ranked decoy is a false positive and the top-5-ranked decoys are all false positives in the  <dig> loop targets as scored by poc and the individual scoring functions. one can find that each individual scoring function we employed has rather high accuracy, yielding less than 50% false positive rate in ranking a near-native decoy as the top decoy. however, by integrating these scoring functions using fuzzy dominance, the poc method leads to  <dig> % less false positives than the best individual scoring function in identifying the top-ranked decoy as a near-native. more significantly, the poc method has  <dig> % less cases where the top-5-ranked decoys do not include a near-native model than those of the best individual scoring function.

we use the receiver operating characteristic  curves to evaluate the ranking performance of each individual scoring function as well as the poc method for each loop target, according to the method described in  <cit>  for ranked data. roc curves display the true positive rate versus the false positive rate. the area under the roc curve  is determined from these roc curves. an auc of  <dig>  indicates perfect ranking of the top n decoys whereas an auc of  <dig>  is representative of a random ranking. the higher an auc value, the better the ranking performance. figure  <dig> shows the roc curves for evaluating the top-10-ranking of decoys in 1ivd and  <dig> l. one can find that the poc method yields larger roc auc than individual scoring functions. moreover, table  <dig> shows the average roc auc values of individual scoring functions and poc in jacobson's decoy sets and the membrane protein loop decoy sets, where rosetta and dfire are the most effective individual scoring functions, respectively. poc yields even higher auc value than rosetta and dfire, as well as other scoring functions, in both cases. the oplsaa score is not evaluated in membrane protein loop decoy sets because hydrogen atoms in the decoys are not available.

respectively, figures  <dig> and  <dig> show the false positive rates using different rmsd cutoffs and the percentage of targets with a top-ranked decoy within 1a rmsd from the native in the membrane protein loop decoy sets. one can find that dfire yields the best overall performance compared to the other individual scoring functions. similar to our results in jacobson's decoy sets, poc yields lower false positive selections than the best individual scoring function in the membrane protein loop decoy sets.

we also applied the poc method with the native structure mixed in the decoy sets generated by jacobson et al  <cit> . similar effectiveness of the poc method can be found in .

discussion
comparison to regression-based consensus method
a popular approach to take advantage of multiple scoring functions is to build a consensus scoring function by combining the individual scores using linear regression  <cit> . however, the disadvantage of the regression-based consensus scoring function method is that it will overlook some models with certain optimality when the pareto optimal front of the scoring function space is concave. figure  <dig> shows a schematic illustration of a concave search space of two scoring functions s <dig> and s <dig>  when a set of weights are determined by regression, a contour line is formed and the minimum solution of the consensus function corresponds to a model on the pareto optimal front, which is the tangent point of the contour line and the model solution space. however, there exists no contour line that can produce a tangent point with the feasible solution space in the region bc in the pareto optimal front. this is because before a tangent point is reached in bc, the contour line becomes a tangent at another point at ab or cd zones, which yields a lower overall consensus function value. in other words, models in the concave region bc will never be selected in a consensus scoring function method, although these models show certain pareto-optimality relative to others in the model set. some regions in the pareto optimal front may still be unreachable even if nonlinear regression is used to combine various terms.

another major drawback of the regression-based consensus method is its dependence on the size, composition and generality of the training set used to derive the weights. similar to the vote-based or rank-based consensus methods, poc does not require a training procedure. the selection and ranking solely depend on evaluation of the dominance relationship among the decoys.

comparison to rank-by-number, rank-by-rank, and rank-by-vote methods
the vote-based consensus method is another strategy of multiple scoring functions selection method, which takes advantage of the observation that similar models voted by more scoring functions tend to be more accurate than those having fewer votes. however, the disadvantage of vote-based consensus methods is that it is very sensitive to the artificially-set vote threshold value  <cit> . also, the vote-based consensus method has difficulties in situations when the scoring functions strongly disagree with each other. as a result, the vote-based consensus methods are usually inferior to the consensus score methods and are generally not recommended  <cit> .

comparison to another selection method
lin and head-gordon recently presented a new physics-based energy function with an implicit solvent model, so-called hpmf  <cit> , which has shown improved native-like loop discrimination capability from non-native decoys compared to ddfire and opls-aa/sgb, particularly in the long loop targets. table  <dig> presents the average rmsd of the top-ranked decoys in loop targets with lengths ranging from  <dig> to  <dig> residues. the top-ranked decoys selected by poc method yield better average rmsd than those selected by hpmf in short, medium, and long loop targets.

result analysis
in this section, we analyze, from the biological perspective, the results obtained for several loop targets. these targets include 1fus, 1aac, and 1hbq.

for the test case of 1fus loop target, rosetta and the triplet scoring functions select, as best scored, decoys with rmsd > 3a from the native structure, albeit the other scoring functions as well as poc can correctly identify the decoy with best resolution. the decoy selected by triplet has a better scoring torsion angle combination, allowing for some favorable near residue neighbor interactions depicted as black dashed lines in figure  <dig>  these are either local backbone to backbone or side-chain to backbone hydrogen bonds. our triplet scoring function evaluates a loop only by its internal local interactions. the cause this decoy is highly deviated from the native loop structure is that it makes few tertiary contacts with the rest of the protein, being rather detached from it , it is solvated and unfolded, and therefore, cannot be stable. this is the reason for which the triplet scoring function should be used in conjunction with other distance-based potentials, a task that is accomplished here by our consensus poc method, which performs well for this target despite the triplet and rosetta failures.

on the other hand, rosetta's best scored decoy has the opposite problem: it makes some good contacts with the protein frame but has a poor choice of backbone torsion angle combinations. for example, the thr <dig> residue has the following backbone torsion angle combination: phi = 80°, psi = -45°, which falls on a region of the threonine's ramachandran map that is disallowed due to local steric clashes. the success of the poc method in this case is justified by selectively relying on the other scoring functions that have good performances.

a somewhat opposite example is provided by the 1aac target, where only the triplet scoring function selects decoys close to the native structure. all the other scoring functions select decoys with inferior torsion angle combinations. it seems that the distance-based scoring functions cannot accurately evaluate the local backbone interactions that are well described by our triplet torsion angle scoring function. despite scoring a loop by its internal interactions only, our triplet scoring function proves itself as a valuable tool in the poc scheme. our poc method heavily relies on the triplet scoring function to identify the near-native conformation in this case.

the only case, from all the  <dig> loop targets studied here, where poc fails to capture a native-like structure  on the parento optimal front, is the 1hbq loop. this also means that none of the individual scoring functions can correctly identify a decoy with native-like conformation as the top-ranked one. 1hbq is a special case where the native loop energy is a compromise between a poor loop internal energy and a very favorable energy of interaction with the rest of the protein. the loop internal energy is compromised by the leu <dig> residue with an unfavorable backbone conformation  that scores poorly in every potential function. on the other hand, this residue participates, together with its loop neighbor phe <dig>  in a network of favorable hydrophobic interactions involving many atoms from the protein frame. in figure  <dig>  the two loop residues are shown enclosed by their external surface, together with the protein frame atoms that are closest to their side-chains. with the exception of a sulfur atom, all the other surrounding atoms are hydrophobic carbons. the two loop side-chains are hydrophobic themselves and form a hydrophobic core that is very favorable for the protein stability.

the best decoy selected by poc for this loop shows many favorable contacts, including the hydrophobic interaction between phe <dig> and leu <dig> side-chains. but they are not buried in a protein hydrophobic core in this case. also, this decoy's surrounding surface, shown in figure  <dig>  forms a central internal cavity that is not filled with other protein atoms and is energetically unfavorable for this reason. none of the scoring functions is able to capture these important protein features, involving hydrophobic cores and internal cavities, because they are based on multiple-body cooperative interactions. as a result, our poc method is misguided in constructing the pareto optimal front.

limitations of the poc method
similar to the other consensus methods, a limitation of the poc method depends on the accuracy of the scoring functions involved in the consensus scheme. if the large majority of the scoring functions have poor accuracy, the consensus scheme is unlikely to select decoys with high resolution. the effectiveness of the poc method also depends on the quality of the decoys generated. poc is a selection and ranking scheme and thus it is unable to generate better decoys than the best one in a decoy set.

another minor disadvantage of the poc method is the decoy selection and ranking time when the decoy set is large. for a set of n decoys, the pareto-optimal decoys selection and ranking time scaling is o because of the requirement of evaluating pair-wise decoy dominance relationship, whereas the ranking time scaling in regression-based, rank-based, or vote-based consensus methods is o. however, compared to the training time in regression-based method and the evaluation time for the scoring functions, the decoy selection and ranking time in the poc method is still rather small for a reasonable size of the decoy set.

CONCLUSIONS
the poc method is shown to be effective in distinguishing the best models from the other ones within jacobson's loop decoy sets and the membrane protein loop decoy sets. it is clear that a combination of multiple, carefully-selected physics- and knowledge-based scoring functions can significantly reduce the number of false positives compared to using an individual scoring function only. moreover, identifying the decoys at the pareto optimal front and ranking these decoys based on the fuzzy dominance relationship against the other decoys in the set have led to higher model selection accuracy in the poc method than in the other consensus strategies including rank-by-vote, rank-by-number, rank-by-rank, and regression-based methods. in addition to protein loop structure prediction, the poc approach may also be used in applications of protein folding, protein-protein interaction, and protein-ligand docking.

our current poc implementation does not bias to any individual scoring function. however, there may still be improvement space for the poc method. for example, the poc may couple with a training algorithm to measure the efficiency of a scoring function and then certain bias to some scoring functions can be incorporated in evaluating the fuzzy pareto dominance relation. this will be one of our future research directions.

authors' contributions
yl conceived and implemented the method and carried out the computation. ir performed the biological analysis. sc designed the computational experiment using physics-based energy function. ej coordinated the study. yl, ir, sc, and ej performed the result analysis. all authors read and approved the final manuscript.

supplementary material
additional file 1
efficiency of poc in identifying the native structures. the supplementary file describes the efficiency of the poc method with the native structure mixed in the decoy sets generated by jacobson et al. the poc method also leads to less false positives compared to individual scoring functions.

click here for file

 acknowledgements
we acknowledge support from nih grants 5pn2ey016570- <dig> and 5r01ns063405- <dig> and from nsf grants  <dig>   <dig>  and  <dig> 
