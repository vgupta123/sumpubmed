BACKGROUND
introduction
recently, bayesian networks  have become a very popular tool for biological network reconstruction  <cit> , for genotype-to-phenotype relationship studies  <cit>  and for clinical and microarray data aggregation  <cit> . bn are directed acyclic graphs  that model the probabilistic dependencies underlying the data. these graphical models are highly attractive for their ability to describe complex probabilistic interactions between variables. they offer a coherent and intuitive representation of uncertain domains of knowledge. the graphical part of bn reflects the structure of a problem, while local interactions among neighboring variables are quantified by conditional probability distributions. learning a bn from data requires identifying both the model structure g and the corresponding set of model parameter values. given a fixed structure, however, it is straightforward to estimate the parameter values. the task can be efficiently solved according to the maximum likelihood  or maximum a posteriori  criterion under the assumption that the learning data contain no missing values  <cit> . as a result, research on the problem of learning bn from data is focused on methods for identifying the structure that best fits the data. despite significant recent progress in algorithm development, the computational inference of network structure is currently still very much an open challenge in computational statistics  <cit> . to appreciate the complexity of learning a dag, we note that the number of dags is super-exponential in the number of nodes  <cit> .

broadly speaking, there are two main approaches to bn structure learning. both approaches have advantages and disadvantages. score-and-search methods search over the space of structures  employing a scoring function to guide the search. another approach for learning bn structures, known as the constraint-based  approach, follows more closely the definition of bn as encoders of conditional independence relationships. according to this approach, some judgments are made about the  dependencies that follow from the data and use them as constraints to construct a partially oriented dag  representative of a bn equivalence class. there are many excellent treatments of bn which surveys the learning methods  <cit> . when data sets are small, the relative benefits of the two approaches are still unclear. while none has been proven to be superior, considerable advances have been made in the past years in the design of highly scalable divide-and-conquer cb methods  <cit>  in order to improve the network reconstruction accuracy when the number of samples is small.

in this study, we apply one of these cb algorithms, named recursive hybrid parents and children , for representing the statistical dependencies between  <dig> clinical variables among  <dig> women with various degrees of obesity. obesity is recognized as a disease in the u.s. and internationally by governments, health organizations, researchers and medical professionals. it is a complex multifactorial condition that needs to be studied by the means of multidisciplinary approaches involving biological expertise and new statistical and data mining tools. features affecting obesity are of high current interest. clinical data, such as patient history, lifestyle parameters and basic or even more elaborate laboratory analytes  form a complex set of inter-related variables that may help better understand the pathophysiology of visceral obesity and provide guidance for its clinical management. gregori et al.  <cit>  performed a meta-analytic framework for the analysis of obesity and its determinants in children using bayesian networks. only seven lifestyle risk factors were considered as being potentially related to obesity in this population. to the best of our knowledge, our study is the first attempt to use bns in the context of modeling the complex relationships between lifestyle and metabolic correlates of visceral obesity among women.

we use the bootstrapping method to generate more robust network structures as discussed in  <cit> . statistical significance of edge strengths are evaluated using this approach. if an edge has a confidence above the threshold, it is included in the consensus network. thus, if dependencies have enough support in the bootstrapping process they are captured and represented in the final consensus network. the confidence estimate assigned to each network edge is represented graphically on the final network. such network represents a powerful computational tool for identifying putative causal interactions among variables from observational data. the consensus network graphically represents the possibly causal independence relationships that may exist in a very parsimonious manner  <cit> . in this study, special emphasis was placed on integrating physiological knowledge into the graph structure. once the consensus pdag was constructed from data, the remaining undirected edges were then directed according to our causal interpretation and additional latent variables were added to the graph for the sake of clarity, coherence and conciseness. the graphical representation provides a statistical profile of this sample of obese women, and meanwhile helps identifying the most important predictors of visceral obesity. using the concept of a markov blanket we can identify all the variables that shield off the class variable from the influence of the remaining network. therefore, bns automatically perform feature selection by identifying the dependency relationships with the class variable. we compare our findings with the results obtained using the same data and more traditional regression models.

bayesian networks
formally, a bn is a tuple <g, p > where g = <u, e > is a directed acyclic graph  with nodes representing the variables in the domain u, and edges representing direct probabilistic dependencies between them. p denotes the joint probability distribution on u. the bn structure encodes a set of conditional independence assumptions: that each node xi is conditionally independent of all of its nondescendants in g given its parents paig. these independence assumptions, in turn, imply many other conditional independence statements, which can be extracted from the network using a simple graphical criterion called d-separation  <cit> .

we denote by x ⊥p y|z the conditional independence between x and y given the set of variables z where p is the underlying probability distribution. note that an exhaustive search of z such that x ⊥p y|z is a combinatorial problem and can be intractable for high dimension data sets. we use x⊥gy|z to denote the assertion that x is d-separated from y given z in g. we denote by dsep, a set that d-separates x from y. if <g, p > is a bn, x ⊥p y|z if x⊥gy|z. the converse does not necessarily hold. we say that <g, p > satisfies the faithfulness condition if the d-separations in g identify all and only the conditional independencies in p, i.e., x ⊥p y|z if and only if  x⊥gy|z. two graphs are said equivalent iff they encode the same set of conditional independencies via the d-separation criterion. the equivalence class of a dag g is a set of dags that are equivalent to g.  <cit>  established that two dags are equivalent iff they have the same underlying undirected graph and the same set of v-structures . so we define an essential graph  for a markov equivalence class to be the partially directed acyclic graph , that has the same links as the dags in the equivalence class and has oriented all and only the edges common to all of the dags in the equivalence class. the directed links in the essential graph are called the compelled edges  <cit> .

an important concept of bn is the markov blanket of a variable, which is the set of variables that completely shields off this variable from the others. in other words, a markov blanket mt of t is any set of variables such that t is conditionally independent of all the remaining variables given mt . a markov boundary, mbt , of t is any markov blanket such that none of its proper subsets is a markov blanket of t. suppose <g, p > satisfies the faithfulness condition. then, for all x, the set of parents, children of x, and parents of children of x is the unique markov boundary of x. a proof can be found for instance in  <cit> . we denote by pctg, the set of parents and children of t in g, and by sptg, the set of spouses of t in g, i.e., the variables that have common children with t. these sets are unique for all g, such that <g, p > satisfies the faithfulness condition and so we will drop the superscript g.

bayesian network structure learning
automatically learning the graph structure of a bn is a challenging topic of pattern recognition that has attracted much attention over the last few years. cb methods systematically check the data for conditional independence relationships and try to construct a partially directed graphical structure  that encodes perfectly the set of independencies. typically, these algorithms run a χ <dig> independence test when the dataset is discrete and a fisher's z test when it is continuous in order to decide on dependence or independence, that is, upon the rejection or acceptance of the null hypothesis of conditional independence. therefore, conditional independencies that are read off from the bn structure are in total agreement with the conditional independencies that are obtained by the statistical tests. very powerful, correct, scalable and data-efficient cb algorithms have been recently proposed  <cit> . they are correct  in the sense that they return the correct essential graph under the assumptions that the independence tests are reliable and that the learning database is a sample from a distribution p faithful to a dag g. the  assumption that the independence tests are reliable means that they decide dependence iff the dependence holds in p. in this paper we adopt one of these cb approaches  <cit> . the essential graph is obtained by running an algorithm called recursive hpc , where hpc stands for hybrid parents and children.

RESULTS
simulation experiments on artificial data
as rhpc relies on hpc to build the whole network structure, we conducted several experiments on synthetic data to assess the comparative performance of hpc, and two algorithm proposals that appeared recently in the literature, namely mmpc  <cit>  and getpc  <cit> . the source code  of hpc as well as all data sets used for the empirical tests are available at http://www <dig> univ-lyon <dig> fr/~aaussem/software.html. the authors' implementation of mmpc and getpc can be found respectively at http://discover.mc.vanderbilt.edu/discover/public and http://www.ida.liu.se/~jospe. mmpc was deemed one of the best cb algorithms in  <cit>  and getpc was used recently in  <cit>  for modeling gene networks. we also report the performance of our weak learner inter-iapc for comparison. for getpc and mmpc, we used the softwares proposed by the respective authors . the confidence threshold of the independence test was fixed to α =  <dig>  for all algorithms. all the data sets used for the empirical experiments presented in this section were sampled from a bio-realistic network that has been previously used as benchmark for bn learning algorithms, namely insulin . the insulin network  <cit>  was chosen purposely as it consists of the same number of nodes as our dataset. four sample sizes have been considered:  <dig>   <dig>   <dig> and  <dig>  for each sample size,  <dig> data sets were sampled. we do not claim that this benchmark resembles our real-world problem, however, it makes it possible to compare the outputs of the algorithms.

all four algorithms were run on the target node having the largest degree  in the insulin bn to increase the difficulty of the task. the variables in the output of the algorithms were compared against the true neighbors. to evaluate the accuracy, we combined precision  and recall  as 2+ <dig>  to measure the euclidean distance from perfect precision and recall, as proposed in  <cit> . figure  <dig> summarizes the variability of the euclidean distance over  <dig> data sets in the form of quadruplets of boxplots, one for each algorithm . the advantage of hpc against the other three algorithms is clearly noticeable. hpc outperforms the other algorithms in terms of euclidean distance from perfect precision and recall.

simulation experiments on the sample of women
the consensus pdag obtained by running rhpc on the present sample of women is shown in figure  <dig>  line thickness corresponds to the relative confidence of the edges. the edges that appeared more than 25% in the networks were included in the aggregate pdag. the threshold was tuned on the previous insulin benchmark samples to maximize accuracy. as may be seen, the directionality of the arrows was partially identifiable:  <dig> edges out of  <dig> were directed, indicating the presence of several robust uncoupled head-to-head meetings .

physiological knowledge integration into the model
several interconnected groups of variables were identified, e.g., beer consumption, wine consumption and spirit consumption; cigarettes per day and low exercise; om and sc fat cell sizes. in each of these densely connected subgraphs, the variables were highly interdependent and a common cause is likely to explain the observed correlations. hence, we added some extra nodes and directed some of the links according to physiological knowledge available in the literature. the result is the partially directed acyclic graph  that is shown in figure  <dig>  dashed nodes and arrows are the latent variables that were added for sake of clarity and coherence. by definition, these latent variables are not observed, nor recorded in our data set. for example, the variable high alcohol intake was added as a common "cause" to beer consumption, wine consumption and spirit consumption; the variable unhealthy lifestyle was added as a common cause to cigarettes per day, high alcohol intake and low exercise; the latent variables fat storage and prevailing hormonal conditions were added as two distinct common causes to sc fat cell size and om fat cell size.

almost all the undirected edges were oriented based on current literature as follows. edges directed from the age variable were oriented based on the well-documented impact of ageing on visceral adipose tissue accumulation, blood pressure and plasma ldl-cholesterol levels  <cit> . the edge between age and tea consumption is based on the  <dig> canadian community health survey, which showed a steady increase in tea consumption from  <dig> to more than  <dig> years of age  <cit> . the edge between tea consumption and blood pressure was oriented based on literature showing lower cardiovascular disease risk in tea consumers  <cit>  and a direct effect of black tea consumption on peripheral blood flow and arterial stiffness  <cit> . the edge between age and the number of live children was attributed to the slight decrease in canadian birth rates observed between 1961- <dig> and 1981- <dig>  <cit> , which corresponds approximately to the period in which women of the study had their children. accordingly, older women of the sample were more likely to have delivered slightly more children. orientation of the edge between the number of pregnancies and the number of live children is self-explanatory.

the edge between the number of live children and om fat cell size was derived from literature supporting that post-pregnancy weight retention is an important risk factor for obesity  <cit> . the finding of a specific association between the number of children and om fat cell size was novel and warrants further investigation. the edges between om and sc fat cell sizes and the variables obesity or visceral fat is self explanatory since the excess adipose tissue mass of obese or abdominal obese individuals is constituted of larger fat cells. associations between fat cell size and obesity have been previously observed  <cit> . the edges between visceral fat or large om fat cells and metabolic variables such as ldl-cholesterol, triglycerides and blood pressure was oriented based on the 'portal vein hypothesis', which states that visceral fat is a causal agent for metabolic disturbances  <cit> . however, this hypothesis has not yet been fully proven as operative and has been challenged by a number of investigators. further studies are required to firmly establish causality. however, the fact that the association between visceral fat and metabolic disturbances is independent from overall obesity is well-accepted  <cit> . the edges between the various components of body composition  were logical but it was difficult to provide causal direction between these variables. indeed, many genetic, epigenetic, developmental and environmental factors can contribute to determine body built of a given individual. moreover, the sizes of all compartments generally evolve in a more or less coordinated manner throughout the individual's existence  <cit> . it was expected that the variable 5-yr maximal weight would be a strong correlate of the level of obesity and lean body mass since these variables are the main components of body composition  <cit>  and that most patients reported a stable weight in the five years preceding their inclusion in the study.

the edges around the number of hours of work and the number of meals out per week were oriented based on the demonstration that increased working time was associated with food choice coping strategies  <cit> , which we suggest is reflected by the edges to number of meals out per week, beer, wine and coffee consumption. on the other hand, the number of meals out per week was related to obesity. accordingly, the frequency of restaurant food consumption was previously found to be positively related to body fatness  <cit> . wine consumption was related directly with plasma levels of hdl-cholesterol. this edge was oriented based on epidemiological data showing a protective effect of moderate wine consumption on hdl-cholesterol levels  <cit> . low leisure time physical activity was linked together with smoking habits under a latent causal variable that we termed unhealthy lifestyle. these variables were also linked with coffee and beer consumption, but had no direct link with the level of obesity. we were unable to provide orientation for these edges. moreover, we were not able to readily explain a small number of edges. for example, the link between age at menarche, which reflects timing of puberty, and dietary supplement use is not intuitive. further analyses and other samples will be required to clarify this apparent association.

statistical validation
we noticed from the pdag that om fat cell size, visceral fat, blood pressure, tea consuption and age belonged to the triglycerides markov boundary, though the edge between om fat cell size and triglycerides was only moderate in strength. the influence of om fat cell size on triglycerides was mostly mediated by visceral fat. we observed that age and triglycerides were marginally independent according to the d-separation rule. however, they became dependent conditioned on visceral fat. the pdag was consistent with multivariate linear regression analyzes performed a posteriori on the sample . in model  <dig>  plasma triglyceride levels were predicted using computed tomography-measured visceral adipose tissue area  and total body fat mass . visceral fat explained  <dig> % of the variance in triglyceride levels whereas overall obesity was not a significant predictor of triglyceride levels. a similar analysis in which plasma triglyceride levels were predicted by om and sc fat cell size was also performed . om fat cell size explained  <dig> % of the variance in triglyceride levels, whereas sc fat cell size was not a significant predictor of triglyceride levels in the model.

multivariate regression models for the prediction of plasma triglyceride levels with adiposity measures ; or fat cell size in the omental  and subcutaneous  compartment . variables with non-normal distributions  were log-10- or box cox-transformed for the analysis.

discussion
the purpose of this paper was to introduce the bn methodology in the context of clinical studies, specifically obesity, and to show its effectiveness, as a component of general data mining/knowledge discovery approaches in epidemiology research. we have evaluated a consensus bn learning approach based on boot-strapping techniques on synthetic data with satisfactory results. although our approach did not use any prior information, it was successful in uncovering biologically relevant dependencies and conditional independencies. once the most interesting dependencies are ascertained, traditional statistical methods  can be used to rigorously scrutinize the resulting smaller subnetworks.

in this study, special emphasis was put on integrating physiological expertise and statistical data analysis together. it is well beyond the scope and purpose of this paper to delve deeper into the problem of inferring causalities from observational data. however, the usefulness of bn stems partly from their causal interpretation. as we have seen, the graphical representation is useful as it allows tighter collaboration between the modeler and the biologist. the integration of medical knowledge into data-driven models is not only desirable, but it is also far easier and less subjective than constructing the whole bn with a priori knowledge. in this spirit, most edges were directed according to plausible causal inference although interpretation of edges as carriers of information does not necessarily imply causation.

CONCLUSIONS
thirty-four predictors related to lifestyle, adiposity, body fat distribution, blood lipids and adipocyte sizes have been considered as potential correlates of visceral obesity in women. the analysis was performed with a novel scalable and effective constraint-based bayesian network structure learning algorithm called rhpc.

from a biological point of view, the present study confirms, among other interesting findings, that visceral fat is the predominant predictor of triglyceride levels in obese individuals. it is reassuring that an unsupervised bn analysis uncovered previously established relationships between visceral fat, blood pressure, aging and triglyceride levels. the advantage of bn method is not that it will identify the "true causes", but rather that it will perform initial data exploration to unearth new knowledge in a semi-automated and rapid fashion.

in conclusion, we suggest that bns are valuable data mining tools for the analysis of clinical data. in addition, bns can explicitly combine both expert knowledge from the field and information studied from the data. a need for such multi-step processes  is essential. finally, an extension to our existing framework would be to consider bayesian model averaging as an alternative to a single consensus model selection. this extension is currently underway.

