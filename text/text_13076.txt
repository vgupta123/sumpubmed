BACKGROUND
this paper introduces the figg  tool, which is designed to be of use to computational researchers who require high volumes of artificially generated genomes that mimic the variation seen in the natural population. figg is designed to use high performance computing to rapidly generate artificial genomes, and can be used to generate large numbers of similar whole genome sequences by iteratively seeding each run with new parent genomes.

in the last few years high-throughput sequencing  has allowed researchers to sequence genomes for species that range from bacteria and plants, to insects and vertebrates. in the context of biomedicine hts is being used to: characterize complex ecologies such as the human gut microbiome
 <cit> ; understand parasitic diseases such as malaria
 <cit> ; identify genomic variations that may be responsible for virulence in diseases such as tuberculosis
 <cit> ; and search for the mutations that drive genomic diseases such as cancer
 <cit> .

a result of this wide-ranging use of sequence information is petabytes worth of genomic data across multiple species, populations and diseases. new tools are constantly being required to enable the management and analysis of this information. the figg tool is meant to be of use to different computational researchers working in the area of large-scale genomics. in particular it is designed to be used by those who are struggling to keep pace with the scale and diversity of data in large-scale genomic projects. using figg to generate artificial data has a number of advantages over downloading and storing publically available whole genome sequences as it: has known characteristics, so can be used for consistent benchmarking; can be used to generate mixed populations of heterogeneous genomes for algorithm testing; has no security requirements, so can be shared and used more easily; and does not place undue load on local resources, as genomes can be generated on the fly.

figg is designed to generate large volumes of potentially related sequences that can be used by computational researchers in testing their models, analysis pipelines and informatics solutions. simulating experimental data is a common step in the development and evaluation of new analysis tools
 <cit> , computational methods, and the support infrastructure for managing such sequences. many different genomic simulators are available  and have been described elsewhere
 <cit> , however these are not designed to provide the high volumes of complete genome sequences which are required for software testing and algorithm development. they range in application from instrument-specific sequence read simulation , to genotype simulation for case–control studies based on linkage disequilibrium patterns , to evaluating a population over time to determine how genomic hotspots or population bottlenecks affect a genome  or protein sequence .

art
metasim
genome
gwasimulator
fregene
genomesimla
alf
example simulators used in various types of genome investigations. many use the wright-fisher model of population genetics theory
 <cit>  in order to generate populations that vary over time given some set of event frequencies such as ld, hotpots, population bottlenecks , others provide a set of sequences that could be generated by a given sequencing technology with an error model . the specific simulator used is based on the type of investigation. in planning new gwas studies for instance, a simulator that uses ld patterns and can provide predicted genomic regions for disease related mutations would be selected. however, such a simulator would not be of use in the planning of a metagenomic study for an organism which may not yet be fully sequenced, or is highly variable. none of these simulators provides whole genome fasta as outputs.

figg generates whole genome sequence files, in fasta format, by directly sampling from populations of observed variations. each artificial genome includes sequence mutations that range from single nucleotide variations  to small and large-scale structural variations . it has been designed to use a distributed computing framework to enable rapid generation of large numbers of genomes while tracking the mutations that are applied to each. below we provide details of the figg methods that enable the creation of diverse whole genomes which accurately model experimentally derived real sequence data. the following sections describe the methods used for analysis of background genomic variation, generation of the sequences, and validation of the models through the use of standard sequence analysis tools. finally we discuss applications for figg within the sequencing community.

methods
figg requires two inputs in order to create a genome: 1) all fasta files representing the chromosomes to be simulated , and 2) a database that is the result of the frequency analysis as described in the next section . the resulting output from figg is set of fasta formatted sequence files  that can be used by any tools which use fasta as an input, including sequence-read simulators and genome alignment software.

variation frequency analysis
the public availability of large datasets that characterize human genomic variability provide a wealth of data on population and individual variations. in order to develop an accurate estimate of the range of "normal" variation we used ensembl
 <cit> . this data was mined for all variants validated in the 1000genomes
 <cit>  and hapmap
 <cit>  projects, as these are generally considered representative of normal populations. several other sources representing disease variations were downloaded for comparison, including those from the catalogue of somatic mutations in cancer 
 <cit>  and small structural variants in the database of genomic variants archive 
 <cit> .

in order to characterize the variant frequency across the genome for different classes of mutations each chromosome was first fragmented into base-pair lengths that were manageable for processing. for each fragment a profile of unique variants was developed. these profiles were then analyzed to determine the frequency of each variant class: single point mutations being the most common, followed by sequence alterations , and then insertions. based on these frequencies structural elements in the sequence fragment were identified that can be directly observed and which could explain the variation frequencies including: a higher incidence of coding/non-coding regions; predicted cpg methylation sites; and high/low gc content. a weak correlation with snvs was observed in segments with high/low gc content
 <cit> , but no other genome-wide structural correlation was found. when the same analysis on "disease" variations was run  as a comparison, gc content continued to be the only clear structural correlation for variation frequency .

based on this analysis the observed sequence fragments were separated into bins by gc content, with variant counts per segment recorded for each chromosome . the result is a set of tables that can be easily sampled for fragments based on a gc profile. additionally, base pair size probabilities were calculated for all size-dependent variants , and nucleotide mutation rates were determined for snvs .

implementation
the general architecture of figg is shown in figure 
 <dig>  it has been designed to take advantage of distributed computing by both breaking down the processing of the data into a distributed model, and by separating the functionality required into distinct steps, called "jobs", that can be added or altered for downstream analysis or testing needs. figg is separated into three distinct jobs. the additional file
 <dig> document provided describes how to set up and run these jobs on an amazon web services cluster.

the first job fragments a reference genome and persists it to a distributed database, which ensures that the background genomic information is highly accessible, and only needs to be run once per reference .

the second job mutates each of the segments from a parent genome, using information pulled from a variation frequency database. this database provides the information necessary to determine which variations should be applied to a given fragment  and how often these occur.

the third job assembles the mutated fragments into a whole genome, and generates the corresponding fasta files. the second and third jobs are run in parallel to each other, allowing for a means to generate large numbers of artificial genomes in a highly scalable manner.

mutation rules
the generation of new, mutated sequences is achieved through application of a ruleset based on the frequency analysis described above. each input chromosome is split into fragments of the same size as those used for the frequency analysis . each fragment is then processed stepwise :

 <dig>  determine the gc content of the fragment then fit to the identified bins in the frequency database based on the fragment chromosome. this provides a set of observed fragments to sample.

 <dig>  randomly sample an observed fragment from the set of fragments that fit the gc bin. this fragment will include  <dig> .n counts for each variation type .

 <dig>  apply each variant type to the fragment sequentially . this is achieved through sampling without replacement random sites within the fragment for each mutation, applying size-dependent or snv probabilities for that mutation to the site, and repeating until all variants have been applied to the sequence.
 <dig>  in step  <dig> the gc content of the fragment is calculated then fit to the pre-determined bins, all observed fragments within that bin are then available to sample. step  <dig> samples one of these observed fragments to get the counts of specific variants. in this case the observed fragment had a single deletion and three snvs. in step  <dig> these observed variant counts are applied in stages. sites for each variation are selected randomly , and the mutation applied. for a size-dependent variant such as the deletion, a size is determined from a probability table, for snvs the probability of the point mutation is determined based on the nucleotide present at that site. the resulting fragment will not replicate the sampled fragment  in specific mutations, but only in the number of mutations applied.

the resulting fragment may vary significantly from, or be nearly identical to, the original sequence depending on the selected variant frequencies. use of random site selection for applying the mutations ensures that no specific population bias  is introduced into the bank of resulting sequences. the final fasta sequence then provides a unique variation profile.

mapreduce for multiple genomes
applying this process to the human genome to create a single genome is slow and inefficient on a single machine, even when each chromosome can be processed in parallel. in fact, a basic version of parallelization took more than 36 hours to produce a single genome. producing banks of such genomes this way is therefore computationally limited. however, mutating the genome in independent fragments makes this a good use case for highly distributed software frameworks such as apache hadoop mapreduce
 <cit>  backed by distributed file systems to create and store tens, hundreds, or more, of simulated genomes. in addition, use of hbase
 <cit>  allows for highly distributed column-based storage of generated sequences and mutations. this enables rapid scale-up for management, ensures that all variations to a given genome can be identified, and allows for the simple regeneration of simulated fasta files on an as-needed basis.

mapreduce has been used effectively by us and others in various large-scale genomics toolsets to decrease computation times, and increase the scale of data that can be processed
 <cit> . figg uses this framework in order to allow the rapid generation of new genomes or regeneration of previous mutation models. it is designed to run in three discrete jobs: 1) breakdown input fasta files into fragments and save to a hbase database for use in subsequent jobs; 2) mutate all of the fragments from the first job and persist these to hbase; and 3) reassemble all mutated fragments as new fasta formatted sequences.

mapreduce accomplishes these tasks by breaking each job into two separate computational phases . the map phase partitions data into discrete chunks and sends this to mappers, which process the data in parallel and emits key-value pairs. in each of the separate jobs for figg the mappers deal with fasta sequences, either directly from a fasta file or from hbase. each mapper performs a computation on these sequences, and produces a sequence  with a key that provides information about that sequence . these key-value pairs are "shuffle-sorted" and picked up by the reduce phase. the framework guarantees that a single reducer will handle all values for a given key and that the values will be ordered.

it is worth noting that not all jobs will require the use of a reducer. in figg the first job which breaks down fasta files into fragments and saves them to hbase  is a "map-only" job, because we cannot further reduce these fragments without losing the data they represent. therefore, the mappers output directly to hbase rather than to the reducers. in the mutation job  the map phase performs multiple tasks including applying variations to a sequence fragment, and writing new sequences and specific variation information directly to hbase. whereas in job  <dig> , the map phase only does a single task, tagging a sequence with metadata that enables it to be ordered for the reduce phase, which actually outputs the file. as each mapper is processing a subset of the data in parallel to all other mappers the compute time required will scale directly with the number of mappers available to the task, limited in figgs case only to the organization of the data in hbase.

RESULTS
our primary interest in developing this tool was to provide sets of heterogeneous whole genomes in order to benchmark cancer genome alignments. this is a special case for alignment, as cancer genomes can vary quite dramatically between patients and even within a single tumor. with such a range of variation in patients, it was important to ensure that the simulated genomes were representative of the heterogeneity, without introducing biases for specific mutations.

in order to ensure that figg was modeling heterogeneous genomes that fit a specific background  two different frequency backgrounds were generated . the "normal" frequency background was from data representative of the average human population: 1000genomes and hapmap. the second, "highly variant" frequency background was based on data from the dgva and cosmic databases of cancer and other disease variations. this greatly increased the frequency and size of the small structural variations .

using these two different backgrounds and grch <dig> as the parent genome, figg generated six whole genome sequences: three "normal", two "highly variant", and one additional genome from the "normal" background that included a common cancer structural variation. as expected, for both the "normal" and the "highly variant" sequences, the simulated genomes preserved the frequency distribution of variations observed in the background data, while differing in the raw counts per fragment.

these simulated whole genomes were then used as references to align a set of low-coverage paired-end sequencing reads from the 1000genomes project . the bwa alignment tool
 <cit>  was used to index the simulated genomes and align the reads against each reference, including the current reference genome grch <dig>  statistics regarding read mapping accuracy  for each genome were generated using samtools
 <cit> .


mapped
correctly paired
singletons
a comparison of the 1000genomes reads for erx <dig> mapped against each genome. grch <dig> is the current reference genome. s <dig>  s <dig> and s <dig> are genomes generated based on normal variation data. s4h and s5h were generated with high variation data and s6sv is based on normal variations but with the chromosome arm 19q deleted. the table columns are statistics provided by samtools flagstat: mapped provides the total percentage of reads that mapped to the genome on the left; correctly paired provides the percentage of reads that aligned to the genome in their proper pair; and singletons provides the percentage of reads that were orphaned in the alignment. as expected, genomes s1- <dig> show mapping statistics that are close to the reference genome, while the others show a significantly lower statistics due to the higher frequency and larger bp size of variations used to generate these genomes.

this comparison demonstrates that heterogeneous a whole genome sequences matching specific variation characteristics  can be generated by this tool. in the first three genomes the characteristics come from a "normal" population frequency and fairly closely match the mapping rates of the current public reference . the lower mapping rates in the high variation genomes are expected, as these will have a higher number of variations as well as longer insertions, deletions, and substitutions. this suggests that by using distributions for variations within distinct genomic populations, such as can be seen in different tumor types, highly specific simulated genomes can be generated. these specific simulated genomes could then be used as more accurate quality control sets for testing hypotheses or data. for instance, genome s6sv models a breakpoint that may be found in specific types of glioma
 <cit> . this simulation could therefore be used to more accurately align a clinically derived sequence, integrate with proteomics data to infer a potential effect or biomarker, or simply provide a test sequence for breakpoint analysis methods
 <cit> .

finally, it is important to note the benefits of using a highly distributed framework to generate these sequences. current sequencing projects are generating hundreds or thousands of sequences from patients. in order to provide artificial data models to assist computational researchers working on large-scale projects, the simulation tool must be able to rapidly generate data of similar complexity and size. distributed computing frameworks enable figg to generate this data quickly, allowing the researcher to simulate the scale of data they will actually be facing. using hadoop mapreduce enables figg to scale the mutation job nearly linearly to the number of cores available . however, as with other distributed environments optimization for large clusters must be done on an individual basis.

CONCLUSIONS
hts is now a primary tool for molecular biologists and biomedical investigations. identifying how an individual varies from others within a population or how populations vary from each other is central to understanding the molecular basis of a range of diseases from viral and parasitic, to autoimmune and cancer. as our understanding of these variations increases so too does the complexity of the analyses we need to undertake to find meaning in this data.

simulation data is a common measure of the usability and accuracy of any analysis tools, but in whole genome studies there continues to be a lack of standard whole genome sequence data sets. this is especially problematic with the production of hundreds or thousands sequences from different populations. comparing these to a single reference can lead to loss of important variation information found in even reasonably homogenous data. highly heterogeneous populations, such as those found in cancer, may not even be represented at all by the reference. generating thousands of whole genome models that vary predictably can provide highly specific test data for computational biologists investigating tumor diversity, software engineers who are tasked with supporting the large scale data that is being generated, and bioinformaticians who require reliable standards for developing new sequence analysis tools.

central to each of these research needs is the development and use of banks of whole genome simulation data which will allow for the development of quality control tools, standard experimental design procedures, and disease specific algorithm research. figg provides simulation data models based on observed population information, will enable disease sequence modeling, is designed for large-scale distributed computing, and can rapidly scale up to generate tens, hundreds, or thousands of genomes.

availability and requirements
project name: fragment-based insilico genome generator

home page:
http://insilicogenome.sourceforge.net

operating systems: platform independent

language: java

other requirements: java version  <dig>  or higher, a computational cluster running hadoop v <dig> . <dig> and hbase  <dig>  , pre-computed hbase tables for the frequency analysis, and fasta files for a reference genome.

open source license: apache  <dig> 

restrictions for use: none

all hadoop mapreduce jobs for this paper were run using amazon web services mapreduce clusters. please see the additional file
 <dig> for a walkthrough of the aws job creation.

abbreviations
cosmic: catalogue of somatic mutations in cancer; dgva: database of genomic variants archive; hts: high-throughput sequencing; snv: single nucleotide variation.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
sk and as conceived of, and planned project. sk analyzed variation data, implemented software and validated results. all authors read and approved the final manuscript.

supplementary material
additional file 1
amazon web services figg walkthrough.

click here for file

 acknowledgements
this work was supported by a grant from the fonds national de la recherche , luxembourg  <cit>  and amazon web services education & research.
