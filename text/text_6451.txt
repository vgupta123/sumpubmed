BACKGROUND
whole exome sequencing  is rapidly becoming the preferred method of analysis to study the genetic basis of disease in large cohorts of patient and control samples. wes studies examine the roles of both rare and common variants and, thus, have a distinct advantage over array-based technologies which generally focus on common variants. while common variants typically have modest effect sizes, rare variants, especially those in coding regions, can have larger effect sizes with greater potential to influence disease  <cit> . wes has been successfully utilized in numerous studies to identify functional mutations in mendelian and rare diseases as well as cancer, where small numbers of variants with large effects sizes are expected to be the major contributors to the disease  <cit> . in contrast to these disorders, where few samples may be sufficient to reveal causative mutations, the detection of associated variants in complex disorders requires larger cohorts to adequately detect associations in common variants with weak effects sizes and to identify sufficient numbers of rare variants to achieve adequate power to detect association using burden and collapsing methods  <cit> .

while wes sequencing studies have many advantages over array-based analyses, they are also susceptible to higher levels of genotyping errors  <cit> . these errors are generated throughout the sequencing process, especially at sites with low coverage or variants with low minor allele frequency . while population-based variant callers, such as the genome analysis toolkit   <cit> , have improved the accuracy of genotypes for low frequency variants, they perform poorly when identifying singletons and doubletons  <cit> . therefore, rare variants have a high heterozygote to homozygote error rate. alternatively, as the maf increases, homozygote to heterozygote errors increase in likelihood.

genotype errors affect both common variant  association tests as well as rare variants collapsing association methods  <cit> . non-differential errors  generally don’t affect type i errors in association analyses, but they do significantly decrease statistical power  <cit> . in fact, heterozygote to homozygote errors markedly decrease power, with the minimum sample size required to observe statistical significance increasing to infinity as the maf of the variant drops to zero  <cit> . thus, rare variant association tests, which collapse genotypes from multiple variants with very low mafs into single markers, are particularly sensitive to this type of genotyping error. therefore, applying stringent filtering methods to improve the accuracy of genotypes and variants is essential for achieving the variant calling accuracy in large wes datasets required to precisely detect signals in rare variant collapsing association tests  <cit> .

software suites, such as the gatk  <cit> , have been designed to manage large-scale sequencing projects. gatk’s best practices includes a variant filtering step following variant quality score recalibration . this “vqsr filter” uses annotation metrics, such as quality by depth, mapping quality, variant position within reads and strand bias, from “true” variants  to generate an adaptive error model. it then applies this model to the remaining variants to calculate a probability that each variant is real. using this recalibrated quality score, users can filter lower quality variants. gatk recommends choosing a threshold that maintains 99% sensitivity for the “true” variants. however, recent studies have shown that unvalidated variants remain in datasets after following gatk’s best practices including vqsr and filtration  <cit> . in addition, the vqsr filter does not explicitly filter genotypes, allowing low quality genotypes generated at variant sites that pass the vqsr filter to persist in the vqsr filtered dataset. these low quality genotypes are a major source of errors in sequencing studies, significantly lowering the power in downstream association analyses. lastly, gatk also notes that vqsr works best for wes with a minimum of  <dig> samples, indicating a need for appropriate thresholds that can function outside of vqsr. overall, gatk filtering has limitations; gatk documentation itself recommends the implementation of additional dataset specific filters after vqsr filtration.

along with sequenced variants, recent wes studies  <cit>  have employed imputation methods to calculate the genotypes of common variants to use as additional markers in association analyses. importantly, imputation expands the investigation beyond the exome and allows for the identification of quantitative trait loci within adjacent non-coding enhancer and other regulatory sequences which are known to harbor important variants influencing disease  <cit> . however, these imputation methods can generate inaccurate genotypes  <cit> . again, these genotype errors decrease the statistical power to detect associations with complex disorders  <cit> . to date, no standard filtering methods have been established for genotypes imputed from wes data.

here we describe effective data filtering methods that, when implemented between the gatk variant calling and vqsr filtering steps, improve the sequenced and imputed single nucleotide variant  quality in large-scale wes genetic studies. we focus on showing improvements compared to gatk’s best practices because a recent publication has shown that gatk is the best variant caller for general ngs analyses  <cit> . while filtering to improve the quality of insertion and deletion  variants is also important, here we focus only on snvs. we evaluate vqsr and prospective novel filters by calculating the non-reference concordance with an alternate dataset generated by genotyping  <dig> individuals using the illumina humanexome beadchip, which contains > <dig>  predominantly exonic markers. we also evaluated the ratio of transitions to transversions  in the identified snvs. while ti/tv ratios are only an approximate measure of quality, higher ti/tv ratios are associated with lower false positives, with high quality exome variant datasets expecting to have ti/tv ratios between  <dig>  and  <dig>   <cit> . we established filtering criteria by investigating quality metrics at both the genotype and variant levels. gatk variant calling generates genotype-level quality metrics including depth of data  and genotype quality . dp values represent the number of reads passing quality control used to calculate the genotype at a specific site in a specific sample, with higher values for dp generally leading to more accurate genotype calls. gq is a phred-scaled value representing the confidence that the called genotype is the true genotype. again, higher values reflect more accurate genotype calls.

in addition to improving the genotype qualities, we hypothesized that further variant filtration could improve the quality of the variants dataset. while vqsr uses various annotation values, including quality by depth, mapping quality, variant position within reads and strand bias, to recalibrate the quality score before filtering, it does not use hardy-weinberg equilibrium , average genotype quality or “call rate”  to filter out low quality variants. hwe, quality, and call rate, are common metrics used for filtering variants from genotyping arrays. as such, establishing thresholds for these variant metrics may have corresponding utility in sequencing studies.

due to the rapid development of sample preparation and sequencing technologies, large wes studies often generate data in sample batches using different versions of target capture and/or sequencing reagents. this creates data heterogeneity among the samples due to differences in sequencing coverage and can result in distinct variant qualities and call rates between batches. thus, we investigated the importance of separating wes samples into batches and determined that this is a critical step to perform prior to filtering in order to achieve the highest quality variant dataset.

these methods appreciably improve data quality, compared to data filtered on vqsr alone, by removing more discordant genotypes, leading to a higher non-reference genotype concordance, and improving the ti/tv ratio. application of these filters results in a significantly improved large-scale wes dataset. by removing non-differential errors, these filters theoretically increase the power to identify rare variants  <cit>  underlying the genetic basis of complex diseases.

RESULTS
exome sequencing, variant calling and standard gatk vqsr filtering
as part of a large case-control study, we sequenced the exomes of  <dig> samples from a norwegian population to an average depth of 100× in target regions, with an average of  <dig> % of the target base pairs having at least 30× coverage. using gatk best practices v <dig>  <cit>  we identified  <dig>  snvs  with  <dig> , <dig> genotypes  in the  <dig> samples. following vqsr filtering,  <dig>  snvs  and  <dig> , <dig> genotypes  were retained.

quality of the unfiltered and vqsr filtered datasets
to assess the accuracy of the genotype calls, we genotyped  <dig> of the  <dig> samples using illumina humanexome beadchips, which assay > <dig>  predominantly exonic markers. from these, only high quality humanexome array genotypes passing a stringent filter  were considered. this resulted in  <dig> , <dig> genotypes with an average snp call rate of  <dig> % per sample. of these genotypes,  <dig>  genotypes could be compared with a corresponding genotype from the unfiltered wes dataset .

we calculated the genotype concordance between the sequencing calls and the exome array, where concordance is defined as the percent of identical, or concordant, genotypes out of the total number of compared genotypes. to avoid artificially bolstering concordance by including homozygous reference matches, we calculated concordance separately for exome array homozygous reference genotypes  and exome array non-reference  genotypes  . before applying any filters to the wes dataset, the genotype concordance with exome array non-reference genotypes was  <dig> %. after applying the vqsr filter,  <dig> % of the remaining genotypes were concordant . since the vqsr filter identifies high quality variant loci, but does not target specific genotypes, low quality genotypes remain in the wes dataset. for example,  <dig> , <dig> low depth genotypes  and  <dig> , <dig> low quality genotypes  remain in the dataset after vqsr filtering . overall, the vqsr filter removed  <dig> % of the genotypes that were discordant with the non-reference exome array genotypes while removing  <dig> % of the non-reference concordant genotypes .

†
non-reference genotypes are shown above reference genotypes in brackets.

non-reference genotypes include genotypes that are heterozygous or homozygous alternate in the exome array. reference genotypes include genotypes that are homozygous reference in the exome array.

† includes heterozygote to homozygous alternate mismatches.

.

in addition to genotype concordance, we also calculated the sensitivity and specificity of the wes genotyping using the exome array genotypes as the “gold standard”. for this, we define true negatives  as identical homozygous reference genotype matches and true positives  as identical heterozygous or homozygous alternate genotype matches. false negatives  are instances where the wes data is missing at least one alternate allele, while false positives  are instances where the wes data has at least one extra alternate allele . from this calculation we observe a sensitivity and specificity of  <dig> % and  <dig> %, respectively, in the unfiltered wes dataset. these values improve to  <dig> % and  <dig> % after the vqsr filter is applied .

1
2
3
4
1tp = exact match of non-reference genotype; ref/alt with ref/alt or alt/alt with alt/alt.

2tn = exact match of reference genotype; ref/ref with ref/ref.

3fp = additional alternate allele in wes genotype; ref/ref with ref/alt or ref/ref with alt/alt or ref/alt with alt/alt.

4fn = missing alternate allele in wes genotype; ref/alt with ref/ref or alt/alt with ref/ref or alt/alt with ref/alt.

.

to further evaluate the variant quality of these datasets, we measured their ti/tv ratios. the unfiltered variant dataset has a ti/tv of  <dig> . after applying the vqsr filter, the ti/tv ratio improved to  <dig>  . while this is a significant improvement from the unfiltered dataset, the ti/tv ratio of the vqsr filtered variants is still below the expected ratio of  <dig>  for high quality datasets. based on these quality measurements, we posited that implementing additional filtering methods in conjunction with the standard vqsr filter would further improve the quality of the final variant dataset at both the genotype and variant levels.

separating samples into batches prior to filtering
during the course of our research, we incorporated technology improvements into our study design despite knowing that different clustering and targeting protocols would lead to batch effects caused by differences in factors such as target coverage . during our study, the illumina truseq pe cluster kit improved from version  <dig>  to version  <dig> , and the agilent sureselect target enrichment improved from the 50 mb kit  to v <dig> kit . as discussed later, we determined that separating samples into batches prior to filtering resulted in a higher quality variant dataset. we separated our samples into six different sample sets  before filtering each batch in parallel . for simplicity, we present data statistics for the batch containing the largest number of samples .

batch  <dig> contained  <dig>  unfiltered snvs  with  <dig> , <dig> genotypes  in the  <dig> samples. the vqsr filter removed  <dig> % of these variants, with  <dig>  snvs remaining . at these vqsr filtered sites,  <dig> % of the genotypes were retained . all  <dig> of the samples used for genotype concordance are present in batch  <dig>  therefore, the genotype concordance remains the same as the values presented for the entire dataset . in contrast, the ti/tv calculation is now based on a smaller number of snvs; thus, batch  <dig> has a different ti/tv ratio than the ratio presented for all  <dig> samples. in this batch, the unfiltered variant dataset has a ti/tv of  <dig>  , while the vqsr filtered dataset has a ti/tv ratio of  <dig>   .

filtering low quality genotypes improves concordance
to evaluate how dp and gq filters would affect concordance rates, we calculated genotype concordance at increasing dp and gq thresholds and plotted the percent of discordant genotypes removed versus the percent of concordant retained for non-reference array genotypes . we observed that as quality thresholds increased, genotype concordance, sensitivity, and specificity also increased before eventually reaching a plateau . at this plateau, increasing thresholds continued to remove variants without yielding concordance improvements. we chose a filtering threshold for each metric that was not based on this threshold, but that theoretically provided greater than 99% confidence for a genotype. for dp, we selected a minimum threshold of eight reads, corresponding to a  <dig> ×  <dig> chance  that a biallelic variant would appear to be monoallelic by random chance, assuming a two-tailed binomial model where each allele of a biallelic variant has a 50% chance of being in each read. for gq, we selected a minimum threshold of  <dig>  corresponding to a phred quality score with 99% accuracy. to see how different combinations of dp and gq thresholds affect the genotype concordance, see additional files  <dig> and  <dig> 

after applying these genotype filters to the unfiltered data, we compared our results to the quality of unfiltered and vqsr filtered genotypes . when combined, the dp and gq genotype filters improved the non-reference genotype concordance to  <dig> % after removing  <dig> % of the non-reference discordant genotypes. these filters also improve the sensitivity and specificity to  <dig> % and  <dig> %, respectively. when the vqsr filter is applied subsequent to the dp and gq genotype filters, further improvement is observed, with  <dig> % of the non-reference discordant genotypes removed, a concordance of  <dig> %, a sensitivity of  <dig> % and a specificity of  <dig> % .

applying these dp and gq genotype filters to the  <dig> samples in the batch  <dig> dataset removes  <dig> % of the non-reference genotypes . if we extrapolate the observed concordance improvement to all the variants in all  <dig> samples from batch  <dig>  we would expect to reduce the number of discordant non-reference genotypes in the entire filtered dataset by >60% .

filtering low quality variants improves the ti/tv ratio
to examine whether filters based on hwe, variant quality or call rate can meaningfully improve the variant data quality, we measured their effect on variant quality by examining changes in genotype concordance  and in the ti/tv ratio at different filtering thresholds . as a proxy for variant quality, we calculated the average gq value for each variant .

a
†
‡
†variants found in ncbi dbsnp build  <dig> 

‡variants found in hapmap phase  <dig> release  <dig> 

*combination of hwe, ave. gq and call rate filters.

ap-value based on a hypergeometric test of whether the removed variants were enriched for tv over ti vs. the unfiltered variant sets.

bp-value based on a hypergeometric test of whether the variants that differed between combined + vqsr variant sets and vqsr + combined variant sets were enriched for tv over ti.

first, we filtered out  <dig>  variants  that significantly deviated from hwe . since such a small number of variants are removed, we only observe a slight increase in the ti/tv ratio . this improvement is due to a significant enrichment in the filtering for the removal of tv variants over ti variants . in addition, we see a slight improvement in non-reference concordance . the hwe filter removes more fps  than fns . overall, we observed a slight improvement in ti/tv, concordance, sensitivity and specificity following hwe filtering that suggests that this generally standard quality filter may be applicable to sequencing projects that will be tested for association.

we next calculated ti/tv ratios at different filtering thresholds to determine whether average gq  or call rate  filters can improve variant quality. we contrasted the ti/tv improvement against the sensitivity for detecting “true” variants . in addition, we also separated known from novel variants. for average gq , improvement begins at a threshold of  <dig> due to the fact that we previously removed all genotypes with gq <  <dig>  following this, the ti/tv then quickly increased, most notably in the novel variants, before reaching a plateau. in addition, as we increased the average gq threshold, the number of true variants remaining dropped quickly. as with the vqsr filter, we chose a sensitivity threshold of 99%, which corresponded to variants with an average gq ≥ <dig>  this captured the majority of the ti/tv increase while sacrificing only a minimal percentage of the true variants in the dataset. in total, the average gq filter improved the overall ti/tv by  <dig>   while only removing  <dig> % of the original unfiltered variants . again, while this is only a slight improvement in ti/tv, the filter is significantly biased towards the removal of tv variants . there is also a slight concurrent improvement in sensitivity and specificity  that additionally suggests this filter is advantageous when applied to this dataset.

as the call rate threshold was raised , we observed a gradual increase in ti/tv. this is accompanied by a gradual drop in the number of true variants until very high call rate thresholds are reached, where the number of true variants dropped rapidly. to avoid this rapid drop while maximizing the gain of ti/tv, we chose to preserve a true variant threshold of 96%, which corresponded to variants with call rates ≥88%. again, this significantly improved the overall ti/tv , while only removing  <dig> % of the overall unfiltered variants . in addition, this filter improved concordance by  <dig> % while removing an additional  <dig> fps and  <dig> fns .

we observed that using a combination of hwe, average gq and call rate variant filters provided a significant increase in ti/tv  while removing  <dig> % of the unfiltered variants. importantly, when the vqsr filter is applied subsequent to these three variant filters, we saw the greatest improvement of ti/tv  with a concomitant loss of  <dig> % of the variants .

order of filtering steps is important
we next determined the optimal order of implementing our variant and vqsr filters to obtain the highest quality variant dataset. we compared the above order, which applied vqsr filtering subsequent to our variant filters, to an alternative filtering order, with vqsr filtering applied before our variant filters. in this alternative order, fewer variants were removed , but the resulting ti/tv was lower . to determine if this order consistently improved the ti/tv ratios of the filtered variants, we also compared the results from the different orders of filtering on each of the other five batches . in each case, applying the vqsr filter after performing the manual variant filters consistently resulted in a higher filtered ti/tv ratio. in addition, we tested whether the extra variants removed by this filtering order were enriched for tv variants . again, in each case the extra variants removed by performing vqsr filtering after the manual variant filters were significantly enriched for tv variants. therefore, applying vqsr filtering as the final step in our method provided the highest quality variant dataset.

batch effects cause data heterogeneity in large-scale exome sequencing projects
variant and genotype quality scores can differ depending on the chemistry and sequencing protocols used to generate the data and will frequently result in batch effects if these factors are not taken into account. to investigate the effect that splitting the data into batches had on the final variant dataset, we performed our quality control steps with  and without  partitioning the samples based on differences in their processing .

*used – hwe in vcftools to remove variants with bonferroni-corrected p-value <  <dig> .

†used – geno in vcftools to remove variants with call rates < 88%.

‡used an awk command to remove variants with average gq <  <dig> 

¥filtered vqsr processed variants at the 99% sensitivity tranche.

after filtering, the unbatched dataset contained  <dig>  variants  . since the target definitions changed between the 50 mb and v <dig> capture kits, some variants are “off target” in one kit and are “on target” in the other. this can lead to low quality variants being retained in the unbatched filtered dataset, even though they would be considered “off-target” in a subset of the batched data and appropriately removed. we identified  <dig>  such variants in the unbatched filtered dataset.

the batched dataset contained  <dig>  variants  . of these variants,  <dig>  were not found in the unbatched dataset. the vast majority  of these batched-specific variants were filtered out of the unbatched dataset during the call rate filtration step. these  <dig>  variants had low call rates in some batches, but a high call rate  in at least one batch. this call rate heterogeneity between batches was primarily due to the use of different target definitions in the two capture kits, but could also be caused by any factor that affects depth of coverage in batches.

we determined the quality of the variants unique to each dataset by measuring both their genotype concordance and their ti/tv ratio. the variants unique to the unbatched dataset were found to have a non-reference genotype concordance of  <dig> % , while the non-reference genotype concordance of the variants unique to the batched dataset was much higher at  <dig> % . in addition, variants unique to the batched dataset had a higher ti/tv ratio than the variants unique to the unbatched dataset . overall, we determined that by batching samples prior to performing filtering, we retained  <dig> % more high quality variants emphasizing the importance of accounting for target and chemistry variation during variant and genotype filtration.

imputation of common snps
imputation methods utilize sequenced variants from within the exome to calculate genotype likelihoods at positions outside of the exome. we obtained imputed genotypes at  <dig> , <dig> variant sites in all  <dig> samples using a combination of gatk and beagle . however, since these imputed sites have little to no sequencing coverage, it is difficult to assess the accuracy and quality of the resulting data. therefore, we again took advantage of the humanexome array by calculating imputed genotype concordance using  <dig>  high quality  array genotypes  from  <dig> of the samples.

much like sequencing data, each imputed genotype is given a corresponding gq value, allowing us to assess genotype quality at various gq thresholds . we observe that as the gq threshold increased, the non-reference concordance with the array genotypes increased with a concomitant drop in the number of genotypes remaining. to achieve a 99% confidence in the genotype calls, we again set the threshold at gq ≥  <dig> . this removed almost all of the discordant genotypes  and significantly improved the concordance . however, unlike with the genotypes obtained from sequencing, this removed a much larger proportion of the genotypes , suggesting that the unfiltered genotypes from imputation contain more low quality genotypes than the unfiltered genotypes from sequencing.

we next filtered the dataset to remove non-informative variants created by the gq filtering step. these included “monoallelic” variants, where all unfiltered genotypes are homozygous for the same allele, and “no genotype” where all genotypes at a variant site were removed by the gq filter. from the imputed dataset,  <dig> , <dig>  of the variants were “monoallelic” and  <dig> , <dig>  were “no genotype” variants after applying the gq filter. these “no genotype” variants were imputed with low likelihoods, suggesting they were of poor quality. this was confirmed by assessing the r <dig> distribution for these variants . variants with no genotypes passing the gq threshold generally had a lower r <dig> value than variants with genotypes passing the filter . after removing all “monoallelic” and “no genotype” variants, we retained  <dig> , <dig> imputed variants  with  <dig> , <dig>  high quality genotypes .

lastly, we compared the improvement in imputed data quality using the gq filter to using a simple r <dig> cutoff. many genome-wide association studies use a hard cutoff of r2 >  <dig>  to filter imputed data  <cit> . however, this r <dig> filter removed fewer discordant genotypes  and resulted in a lower concordance  than using the gq filter. therefore, the quality improvement observed using the gq filter is superior to using a r2 >  <dig>  cutoff.

discussion
we developed filters at both the genotype and variant levels . for genotypes, we selected thresholds for dp  and gq  to filter out genotypes with <99% likelihood. we demonstrated that these thresholds improve genotype quality by assessing the improvement in genotype concordance with high quality array genotypes. both thresholds individually improved genotype concordance, with greater improvement when combined . since these genotype quality thresholds were chosen to optimize genotype probability , these values can be applied universally to filter sequencing datasets. while some researchers may prefer higher specificities , the genotype concordance and sensitivity versus specificity curves  suggest that more stringent thresholds may provide only very minor quality improvements that are outweighed by a significant loss of genotypes. for example, selecting for genotype likelihoods greater than  <dig> %  would require thresholds of dp ≥  <dig> and gq ≥  <dig>  but would only increase overall concordance by  <dig> % while removing an additional  <dig> % of all genotypes. therefore, to achieve a 99% genotype likelihood, we recommend thresholds of dp ≥  <dig> and gq ≥  <dig> be chosen when this filtering method is applied to sequencing studies.

we found that the dp and gq filters made more of an impact on non-reference calls than reference. the vqsr filter was adept at removing fps, which were primarily non-reference calls at reference sites. however, it performed worse when asked to remove reference calls at non-reference sites, removing only  <dig> % of fn calls. by including the dp and gq filters, the fn calls were reduced by  <dig> %. we also observed that non-reference genotypes were preferentially affected in the imputed data. for non-reference genotypes, the concordance was initially poor  and was improved to  <dig> % with a gq filter. however, the same gq filter only increased the reference concordance from  <dig> % to  <dig> %. these increases are relevant for rare variant association tests as they rely on high accuracy at non-variant sites.

at the variant level, while we chose a universal threshold for hwe , we empirically determined thresholds for average gq  and call rate . of these three filters, the most crucial is the call rate filter, since it provided the largest quality improvement . while hwe and average gq have less significant ti/tv improvements, this is partially due to the smaller number of variants that are removed by these filters. while these two filters have a smaller effect on the ti/tv ratio, both filters remove a significantly larger proportion of tv variants than would be expected by chance  and also improve the concordance . this suggests that they are both beneficial to this filtering method. since these thresholds were empirically chosen to optimize ti/tv while minimizing the loss of “true” variants, researchers may prefer to similarly determine these thresholds for their own datasets, rather than relying on these specific thresholds. therefore, unlike the genotype filter thresholds, the variant filters should not be universally applied, but can be empirically determined using the methods that we have demonstrated.

we demonstrate the importance of grouping samples into batches according to technical methodologies prior to filtering for producing high quality variants without sacrificing sensitivity. differences in sequence depth coverage between batches can lead to significant call rate differences. since we recommended a call rate filter as part of our method, these differences can lead to the removal of variants with sufficient call rates in one batch even if other batch call rates fall below the filtering threshold. to illustrate the importance of separating samples into batches, we demonstrated that  <dig>  high quality variant  were lost from the “batched” variant set when all  <dig> samples were filtered together in an “unbatched” manner. of these variants,  <dig> % were removed by the call rate filter due to differences in call rates between batches caused by coverage heterogeneity. based on these results, separating batches prior to filtering, then recombining variants before performing downstream analyses is highly recommended.

we also demonstrated that the order of filtering has a significant effect on the quality of the final variant dataset. when vqsr is applied before our suggested filters, the ti/tv was lower than when the same thresholds are applied before running vqsr . however, coupled with this higher ti/tv  was the loss of an additional  <dig> % of the unfiltered variants. since many downstream analyses, especially burden and collapsing analyses, benefit most from a highly specific dataset with low levels of noise, we recommend running our suggested filters prior to performing vqsr filtering.

lastly, we showed that by filtering imputed genotypes we significantly improved the concordance of the data. in the same way as genotypes generated from sequencing data were filtered, we selected a threshold that provides a genotype likelihood greater than 99% by filtering for gq ≥  <dig>  this resulted in a compromise between an increased accuracy  and a minimized loss of genotypes  and can be applied universally to any imputed dataset.

the methods described provide the highest utility for rare variant association analyses. while genotyping errors reduce the statistical power for common variants, this decrease is more pronounced for variants with low maf. therefore, rare variant association tests, which collapse multiple variants with low mafs, are particularly sensitive to genotyping errors and should benefit the most from the described robust filtering methods. therefore, although large numbers of genotypes are removed during filtering to improve the quality of the dataset, the overall power to detect significance should increase by removing these errors from the downstream rare variant association analyses.

CONCLUSIONS
by utilizing the described processing and filtering method, we were able to improve: 1) the quality of the genotypes -  <dig> % non-reference concordance in the filtered dataset versus  <dig> % non-reference concordance in the unfiltered genotypes and  <dig> % non-reference concordanance after vqsr filtering alone; 2) the ti/tv ratio of the final variants -  <dig>  in the filtered dataset versus  <dig>  in the unfiltered dataset and  <dig>  in vqsr filtered dataset; and 3) the number of variants identified -  <dig>  “batched” variants versus  <dig>  “unbatched” variants. in addition, we improved the quality of genotypes from imputation -  <dig> % non-reference concordance in the filtered genotypes versus  <dig> % non-reference concordance in the unfiltered genotypes.

our results demonstrate effective methods for improving the quality of wes data using easily implemented and publically available tools. these methods are applicable to sequencing studies that identify germline variants, but are not suitable for somatic mutation detection. additionally, these filters can be applied to studies that have less than  <dig> samples which cannot optimally utilize gatk vqsr filtering. in these studies, the genotype and variant filters described may have increased utility since vqsr filtering may not sufficiently improve the variant quality of the dataset.

overall, the methods described represent significant improvements over the standard practices for sequencing data processing to decrease the number of errors carried forward into association, burden and collapsing analyses conducted in studies of complex diseases.

