BACKGROUND
the objective of class prediction  is to develop a rule based on a group of samples with known class membership , which can be used to assign the class membership to new samples. many different classification algorithms  exist, and they are based on the values of the variables  measured for each sample  <cit> .

very often the training and/or test data are class-imbalanced: the number of observations belonging to each class is not the same. the problem of learning from class-imbalanced data has been receiving a growing attention in many different fields  <cit> . the presence of class-imbalance has important consequences on the learning process, usually producing classifiers that have poor predictive accuracy for the minority class and that tend to classify most new samples in the majority class; in this setting the assessment of the performance of the classifiers is also critical  <cit> .

data are nowadays increasingly often high-dimensional: the number of variables is very large and greatly exceeds the number of samples. for example, high-throughput technologies are popular in the biomedical field, where it is possible to measure simultaneously the expression of all the known genes  but the number of subjects included in the study is rarely larger than few hundreds. many papers attempted to develop classification rules using high-dimensional gene expression data that were class-imbalanced .

despite the growing number of applications using high-dimensional class-imbalanced data, this problem has been seldom addressed from the methodological point of view  <cit> . it was previously shown for many classifiers that the class-imbalance problem is exacerbated when data are high-dimensional  <cit> : the high-dimensionality further increases the bias towards the classification into the majority class, even when there is no real difference between the classes. the high-dimensionality affects each type of classifier in a different way. a general remark is that large discrepancies between training data and true population values are more likely to occur in the minority class, which has a larger sampling variability: therefore, the classifiers are often trained on data that do not represent well the minority class. the high-dimensionality contributes to this problem as extreme values are not exceptional when thousands of variables are considered.

some of the solutions proposed in the literature to attenuate the class-imbalance problem are effective with high-dimensional data, while others are not. generally undersampling techniques, aimed at producing a class-balanced training set of smaller size, are helpful, while simple oversampling is not  <cit> . the reason is that in most cases simple oversampling does not change the classification rule. similar results were obtained also for low-dimensional data  <cit> .

the synthetic minority over-sampling technique  is an oversampling approach that creates synthetic minority class samples. it potentially performs better than simple oversampling and it is widely used. for example, smote was used for detecting network intrusions  <cit>  or sentence boundary in speech  <cit> , for predicting the distribution of species  <cit>  or for detecting breast cancer  <cit> . smote is used also in bioinformatics for mirna gene prediction  <cit> , for the identification of the binding specificity of the regulatory proteins  <cit>  and of photoreceptor-enriched genes based on expression data  <cit> , and for histopathology annotation  <cit> .

however, it was recently experimentally observed using low-dimensional data that simple undersampling tends to outperform smote in most situations  <cit> . this result was further confirmed using smote with svm as a base classifier  <cit> , extending the observation also to high-dimensional data: smote with svm seems beneficial but less effective than simple undersampling for low-dimensional data, while it performs very similarly to uncorrected svm and generally much worse than undersampling for high-dimensional data. to our knowledge this was the first attempt to investigate explicitly the effect of the high-dimensionality on smote, while the performance of smote on high-dimensional data was not thoroughly investigated for classifiers other than svm. others evaluated the performance of smote on large data sets, focusing on problems where the number of samples, rather than the number of variables was very large  <cit> . a number of works focused on improving the original smote algorithm  <cit>  but these modifications were mainly not considered in the high-dimensional context.

in this article we investigate the theoretical properties of smote and its performance on high-dimensional data. for the sake of simplicity we consider only two-class classification problems, and limit our attention to classification and regression trees , k-nn  <cit>  with k =  <dig>   <dig> and  <dig>  linear discriminant analysis methods   <cit> , random forests , support vector machine , prediction analysis for microarrays  and penalized logistic regression  with the linear  and quadratic penalty . we supplement the theoretical results with empirical results, based on simulation studies and analysis of gene expression microarray data sets.

the rest of the article is organized as follows. in the results section we present some theoretical results, a selected series of simulation results and the experimental results. in the discussion section we summarize and discuss the most important results of our study. in the methods section we briefly describe smote and simple undersampling, the classification algorithms, the variable selection method and the performance measures that we used; we also describe the procedure of data simulation, the breast cancer gene expression data sets and the classification problems addressed.

RESULTS
in this section we present some theoretical properties of smote  <cit> , the simulation results and the experimental data results.

smote is an oversampling technique that generates synthetic samples from the minority class. it is used to obtain a synthetically class-balanced or nearly class-balanced training set, which is then used to train the classifier. the smote samples are linear combinations of two similar samples from the minority class  and are defined as 

  s=x+u·, 

with 0 ≤ u ≤ 1; xr is randomly chosen among the  <dig> minority class nearest neighbors of x. we refer the reader to the methods section for a more detailed description of the method and of the notation used in the paper.

theoretical properties of smote for high-dimensional data
in this section we present some theoretical properties of smote for high-dimensional data, which are summarized in table  <dig> 

most of the proofs require the assumptions that xr and x are independent and have the same expected value ) and variance ). we conducted a limited set of simulations in which we showed that in practice these assumptions are valid for high-dimensional data, while they do not hold for low-dimensional data , where the samples are positively correlated. similar results were described by others  <cit> .

the proofs and details of the results presented in this section are given in additional file  <dig>  where most of the results are derived also without assuming the independence and equal distribution of the original and nearest neighbor samples.

smote does not change the expected value of the  minority class and it decreases its variability
smote samples have the same expected value as the original minority class samples =e), but smaller variance =23var).

practical consequences
the overall expected value of the smote-augmented minority class is equal to the expected value of the original minority class, while its variance is smaller. therefore, smote has little impact on the classifiers that base their classification rules on class-specific mean values and overall variances , while it has some  impact on the classifiers that use class-specific variances , because they use biased estimates.

smote impacts also variable selection. for example, the p-values obtained comparing two classes with a t-test after smote-augmenting the data are smaller than those obtained using the original data . this can misleadingly indicate that many variables are differentially expressed between the classes. smote does not substantially alter the ranking of the variables by their t statistics: the overlap between the variables selected using original or smote-augmented data is substantial when the number of selected variables is kept fixed.

smote introduces correlation between some samples, but not between variables
smote does not introduce correlation between different variables. the smote samples are strongly positively correlated with the samples from the minority class used to generate them  and with the smote samples obtained using the same original samples.

practical consequences
smote can be problematic for the classifiers that assume independence among samples, as for example penalized logistic regression or discriminant analysis methods. also, performing variable selection after using smote should be done with some care because most variable selection methods assume that the samples are independent.

smote modifies the euclidean distance between test samples and the  minority class
when data are high-dimensional and the similarity between samples is measured using the euclidean distance, the test samples are on average more similar to smote samples than to the original samples from the minority class.

practical consequences
this phenomenon is present also when there are some differences between classes but few variables truly discriminate the classes. this is often the case for high-dimensional data and it has important practical implications. for example, when the number of variables is large, smote is likely to bias the classification towards the minority class for k-nn classifiers that measure the similarity between samples using the euclidean distance. conversely, smote does not bias the classification towards the minority class if the number of variables is small, as the euclidean distance of new samples from both classes is similar for the null variables . for these reasons smote seems useful in reducing the class-imbalance problem for k-nn when the number of variables is small or if the number of variables is reduced using variable selection methods .

results on simulated data
simulations were used to systematically explore the behavior of smote with high-dimensional data and to show empirically the consequences of the theoretical results. under the null case the class membership was randomly assigned, while in the alternative case the class-membership depended on some of the variables. if not stated otherwise, the results refer to simulations where the variables were correlated , the samples  were normalized and smote was used before variable selection. in the alternative case we present the results where the difference between classes was moderate  = 1).

classification of low-dimensional data 
the  classifiers trained on low-dimensional class-imbalanced data assigned most of the samples to the majority class, both in null and in alternative case ; the classifiers with the smallest bias towards the majority class were dlda  and dqda, for which the bias decreased as the sample size increased. smote did not seem to impact the performance of these classifiers , while it reduced the bias towards the majority class for k-nn , plr-l <dig>  plr-l <dig> and pam, performing well also when the sample size was small  and increasing the overall predictive accuracy  in the alternative case. a similar but attenuated effect was observed for the other classifiers  where smote decreased the difference between class-specific pa, most notably for large sample sizes, but did not remove it. similar results were obtained using p = g =  <dig> variables .

classification of high-dimensional data 
adjusting the classification threshold substantially decreased the class-imbalance bias of 5-nn, rf and svm , and was helpful to some extent also for pam, provided that variable selection was performed. a slight improvement was observed also for plr-l <dig>  and plr-l <dig>  while this strategy was not effective for the other classifiers. the peculiar behavior of 5-nn with classification threshold is expected, as under the null hypothesis the class specific probabilities are piecewise monotone functions of class-imbalance with breakpoints at k1 = 1 /  <dig>  2 /  <dig>  3 /  <dig>  4 /  <dig> 

smote had only a small impact on the class-specific pa of all the classifiers other than k-nn and pam: smote either further increased the probability of classification in the majority class  or slightly decreased it . however, the overall effect of smote was almost negligible.

smote had the most dramatic effect on k-nn classifiers but the effectiveness of its use depended on the variable selection strategy. smote classified most of the new samples in the minority class for any level of class-imbalance when all the variables were used, while it reduced the bias observed in the uncorrected analyses when used with variable selection: the class-specific pa of the two classes were approximately equal for a wide range of class-imbalance levels, especially for 3-nn and 5-nn, both in the null and in the alternative case.

to a lesser extent, smote with variable selection was beneficial also in reducing the class-imbalance problem of pam, decreasing the number of samples classified in the majority class, both in the null and in the alternative case; this was not the case when pam was used without prior variable selection. a possible explanation of this behavior is given in the additional file  <dig> 

similar conclusions would be obtained using auc and g-mean to interpret the results . smote without variable selection reduced the g-mean for k-nn, dqda and svm, it increased it for rf, plr-l <dig>  plr-l <dig> and pam  and did not change it for dlda and cart. the auc values were very similar using smote or uncorrected analysis, but smote with variable selection increased auc and g-mean values for k-nn and pam.

performing variable selection before or after smote did not significantly impact the performance of the classification methods . in general, the results observed in the alternative case were similar to those observed in the null case, suggesting that our theoretical findings are relevant also in the situations where the class-membership depends on some of the variables. when the differences between the classes were larger, the class-imbalance problem was less severe, therefore using smote was less helpful .

similar conclusions were obtained when all the variables were differentially expressed  or were simulated from the exponential distribution . see also figure  <dig> for a visual summary of the results.

results from the experiments on gene expression data sets
we analyzed three high-dimensional gene expression data sets, performing two prediction tasks on each of them . these experiments were performed to validate the results from the simulation study and to show the practical application of our theoretical results. uncorrected analysis, analysis with the adjusted classification threshold , smote and simple undersampling  <cit>  results were displayed presenting average class-specific pa and g-mean .

number of samples, number of samples in the minority class , level of class-imbalance  and number of features for the analyzed gene expression data sets and different classification tasks.

the experimental results were very consistent with the simulation results. most uncorrected classifiers seemed to be sensitive to class-imbalance, even when the class-imbalance was moderate. with few exceptions, the majority class had a better class-specific pa ; the larger differences were seen when the class-imbalance was large  and for harder classification tasks . the class-specific pa of dlda and dqda were about the same for all the classification tasks; these classifiers, together with pam, had the largest auc and g-mean values and seemed the least sensitive to class-imbalance. smote, cut-off adjustment and undersampling had little or no effect on their classification results.

changing the cut-off point decreased the class-imbalance bias of rf, svm, pam, plr-l <dig> and plr-l <dig> and 5-nn  and outperformed undersampling, while it was inefficient with the other classifiers.

smote with variable selection had the most dramatic effect on k-nn classifiers, substantially reducing the discrepancy between the class-specific pa, generally increasing the g-mean and, to a lesser extent, the auc values ; in this case smote performed similarly, but not better, than undersampling. on the other hand, when variable selection was not performed smote worsened the performance of k-nn: most samples were classified in the minority class and the auc and g-mean values substantially decreased, while undersampling performed better than uncorrected analysis .

overall predictive accuracy , predictive accuracy for class  <dig> , predictive accuracy for class  <dig> , area under the roc curve  and g-mean for 1-nn, 3-nn and 5-nn achieved on the miller data set with different methods of training set manipulation , smote and undersampling - under). prediction of estrogen receptor status  and grade of the tumor . all variables were considered when training the classifiers.

smote reduced the discrepancy in class-specific pa for the other classifiers , but simple undersampling performed very similarly  or better .

results obtained modifying the class-imbalance of sotiriou’s data
to get a better insight into the class-imbalance problem, we obtained different levels of class-imbalance on sotiriou’s data set and compared the performance of smote with uncorrected analysis and undersampling. figure  <dig> displays the average class-specific pa for er classification  and grade ; the leftmost points of each graph show the results from simple undersampling and the total sample size increases with class-imbalance.

for the uncorrected classifiers the pa of the minority class markedly decreased as the class-imbalance increased, despite of the fact that the sample size of the training set was larger. this effect was more pronounced when the differences between classes were smaller  or for smaller sample sizes .

for most classifiers smote improved the pa of the minority class, compared to the uncorrected analyses. the classifiers that benefited the most from the use of smote were the k-nn classifiers, especially 5-nn ; smote was somehow beneficial also for pam, plr-l <dig> and plr-l <dig>  while the minority class pa improved only moderately for dlda, rf, svm and cart, and decreased for dqda. however, smote did not remove the class-imbalance problem and, even if it was beneficial compared to the uncorrected analysis, it generally performed worse than undersampling. the exceptions were pam and 5-nn for er classification , where the drop in the pa of the minority class was very moderate. overall, the classification results were in line with the simulation results and confirmed our theoretical findings.

discussion
the classifiers that we considered in this study were previously shown to be sensitive to class-imbalance: the predictive accuracy of the minority class tends to be poor and they tend to classify most test samples in the majority class, even when there are no differences between the classes. the high-dimensionality further increases the bias towards the classification in the majority class; undersampling techniques seem to be helpful in reducing the class-imbalance problem for high-dimensional data, while simple oversampling  <cit>  is not  <cit> .

in this article we focused on high-dimensional data and investigated the performance of smote, an oversampling approach that creates synthetic samples. we explored the properties of smote on high-dimensional data from a theoretical and empirical point of view, using simulation studies and breast cancer gene expression microarray data. the performance of the classifiers was evaluated with overall and class specific predictive accuracies, area under the roc curve  and g-mean.

most of the classifiers that we considered benefit from smote if data are low-dimensional: smote reduces the bias towards the classification in the majority class for k-nn, svm, pam, plr-l <dig>  plr-l <dig>  cart and, to some extent, for rf, while it hardly affects the discriminant analysis classifiers . on the other hand, for high-dimensional data smote is not beneficial in most circumstances: it performs similarly to uncorrected class-imbalanced classification and worse than cut-off adjustment or simple undersampling.

in practice, only k-nn classifiers seem to benefit substantially from the use of smote in the high-dimensional setting, provided that variable selection is performed before using smote; the benefit is larger if more neighbors are used. smote for k-nn without variable selection should not be used, because it surprisingly biases the classification towards the minority class: we showed that the reason lies in the way smote modifies the euclidean distance between the new samples and the minority class. our theoretical proofs made many assumptions; however, analyzing the simulated and real data, where the assumptions were violated, we observed that our results were valid in practice.

we showed that for high-dimensional data smote does not change the mean value of the smote-augmented minority class, while it reduces its variance; the practical consequence of these results is that smote hardly affects the classifiers that base their classification rules on class specific means and overall variances; such classifiers include the widely used dlda. additionally, smote harms the classifiers that use class-specific variances , as it produces biased estimates: our experimental data confirmed these finding, showing that smote further increased the bias towards the majority class. smote should therefore not be used with these types of classifiers.

for the other classifiers it is more difficult to isolate the reasons why smote might or might not work on high-dimensional data. smote has a very limited impact on svm and cart. plr-l <dig>  plr-l <dig> and rf seem to benefit from smote in some circumstances, however the improvements in the predictive accuracy of the minority class seem moderate when compared to the results obtained using the original data and can be probably attributed to the balancing of the training set. the apparent benefit of smote for pam is limited to situations where variable selection is performed before using pam, which is not a normally used procedure, and can be explained as the effect of removing the pam-embedded class-imbalance correction, which increases the probability of classifying a sample in the majority class.

using the gene expression data we compared smote with simple undersampling, the method that obtains a balanced training set by removing some of the samples from the majority class. our results show that for rf, svm, plr, cart and dqda simple undersampling seems to be more useful than smote in improving the predictive accuracy of the minority class without largely decreasing the predictive accuracy of the majority class. smote and simple undersampling perform similarly for pam  and dlda; similar results were obtained by others also for low-dimensional data  <cit> . sometimes smote performs better than simple undersampling for k-nn . our results are in agreement with the finding that smote had little or no effect on svm when data were high-dimensional  <cit> .

the results showing that simple undersampling ourperforms smote might seem surprising, as this method uses only a small subset of the data. in practice undersampling is effective in removing the gap between the class-specific predictive accuracies for high-dimensional data  <cit>  and it is often used as a reasonable baseline for algorithmic comparison  <cit> . one of its shortcomings is the large variability of its estimates, which can be reduced by bagging techniques that use multiple undersampled training sets. we previously observed that bagged undersampling techniques outperform simple undersampling for high-dimensional data, especially when the class-imbalance is extreme  <cit> . others showed that bagged undersampling techniques outperformed smote for svm with high-dimensional data  <cit> . therefore, we expect that the classification results presented in this paper could be further improved by the use of bagged undersampling methods.

we devoted a lot of attention to studying the performance of smote in the situation where there was no difference between the classes or where most of the variables did not differ between classes. we believe that in this context these situations are extremely relevant. it is well known that most of the problems arising from learning on class-imbalanced data arise in the region where the two class-specific densities overlap. when the difference between the class-specific densities is large enough, the class-imbalance does not cause biased classification for the classifiers that we considered, even in the high-dimensional setting  <cit> . the other reason is that when a very large number of variables is measured for each subject, in most situations the vast majority of variables do not differentiate the classes and the signal-to-noise ratio can be extreme. for example, sotiriou et al.  <cit>  identified  <dig> out of the  <dig>  measured genes as discriminating er+ from er- samples in their gene expression study; at the same time er status was the known clinico-pathological breast cancer phenotype for which the largest number of variables was identified . similar results can be found in most gene expression microarray studies, where rarely more than few hundreds of genes differentiate the classes of interest. furthermore, the results from the simulation studies where all the variables were differentially expressed were consistent with those obtained when only few variables differentiated the classes, indicating that our conclusions are not limited to sparse high-dimensional data.

variable selection is generally advisable for high-dimensional data, as it removes some of the noise from the data  <cit> . smote does not affect the ranking of variables if the variable selection method is based on class-specific means and variances. for example, when variable selection is based on a two-sample t-test and a fixed number of variables are selected, as in our simulations, the same results are obtained if variable selection is performed before or after using smote. however, the results obtained by performing variable selection on smote-augmented data must be interpreted with great care. for example, the p-values of a two-sample t-test are underestimated and should not be interpreted other than for ranking purposes: if the number of variables to select depends on a threshold on the p-values it will appear that many variables are significantly different between the classes. another reason of concern is that smote introduces some correlation between the samples and most variable selection methods  assume the independence among samples.

many variants of the original version of smote exist, however in this paper we only considered the original version of smote. the variants of smote are very similar in terms of the expected value and variance of the smote samples, as well as the expected value and variance of the euclidean distance between new samples and samples from the smote-augmented data set. under the null hypothesis all the theoretical results presented in this paper would apply also for borderline-smote  <cit>  and safe-level-smote  <cit> . further research would be needed to assess the performance of these algorithms for high-dimensional data when there is some difference between the classes.

we considered only a limited number of simple classification methods, which are known to perform well in the high-dimensional setting, where the use of simple classifiers is generally recommended  <cit> . our theoretical and empirical results suggest that many different types of classifiers do not benefit from smote if data are high-dimensional; the only exception that we identified are the k-nn classifiers. it is however possible that also in the high-dimensional setting smote might be more beneficial for some classifiers that were not included in our study.

CONCLUSIONS
smote is a very popular method for generating synthetic samples that can potentially diminish the class-imbalance problem. we applied smote to high-dimensional class-imbalanced data  and used also some theoretical results to explain the behavior of smote. the main findings of our analysis are: 

• in the low-dimensional setting smote is efficient in reducing the class-imbalance problem for most classifiers;

• smote has hardly any effect on most classifiers trained on high-dimensional data;

• when data are high-dimensional smote is beneficial for k-nn classifiers if variable selection is performed before smote;

• smote is not beneficial for discriminant analysis classifiers even in the low-dimensional setting;

• undersampling or, for some classifiers, cut-off adjustment are preferable to smote for high-dimensional class-prediction tasks.

even though smote performs well on low-dimensional data it is not effective in the high-dimensional setting for the classifiers considered in this paper, especially in the situations where signal-to-noise ratio in the data is small.

