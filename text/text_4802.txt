BACKGROUND
microarray technology allows scientists to monitor the expression of genes on a genomic scale. it increases the possibility of cancer classification and diagnosis at the gene expression level. several classification methods such as rbf neural nets, mlp neural nets, bayesian, decision tree and random forrest methods have been used in recent studies for the identification of differentially expressed genes in microarray data. however there is lack of comparison between these methods to find a better framework for classification, clustering and analysis of microarray gene expression.

another issue that might affect the outcome of the analysis is the huge number of genes included in the original data that some of them are irrelevant to analysis. thus, reducing the number of genes by selecting those that are important is critical to improve the accuracy and speed of prediction systems. in this study, we compared the efficiency of the classification methods; svm, rbf neural nets, mlp neural nets, bayesian, decision tree and random forrest methods. we used v-fold cross validation methods to calculate the accuracy of the classifiers. we also applied some common clustering methods such as k-means, dbc, and em clustering to our data and analysed the efficiency of these methods. further we compared the efficiency of the feature selection methods; support vector machine recursive feature elimination   <cit>  <cit> , chi squared  <cit> , and csf  <cit>  <cit> . in each case these methods were applied to eight different binary  microarray datasets. we evaluated the class prediction efficiency of each gene list in training and test cross-validation using our supervised classifiers. after features selection, their efficiencies are investigated by comparing error rate of classification algorithms applied to only these selected features versus all features.

supervised classification
supervised classification, also called prediction or discrimination, involves developing algorithms to priori-defined categories. algorithms are typically developed on a training dataset and then tested on an independent test data set to evaluate the accuracy of algorithms. support vector machines are a group of related supervised learning methods used for classification and regression. the simplest type of support vector machines is linear classification which tries to draw a straight line that separates data with two dimensions. many linear classifiers  are able to separate the data. however, only one achieves maximum separation. vapnik in  <dig> proposed a linear classifier as a original optimal hyperplane algorithm  <cit> . the replacement of dot product by a non-linear kernel function allows the algorithm to fit the maximum-margin hyperplane in the transformed feature space  <cit> . svm finds a linear separating hyperplane with the maximal margin in this higher dimensional space. k=ΦtΦ is called the kernel function  <cit> . there are four basic kernels: linear, polynomial, radial basic function , and sigmoid  <cit> .

in decision tree structures, leaves represent classifications and branches represent conjunctions of features that lead to those classifications. there are advantages with decision tree algorithms: they are easily converted to a set of production rules, they can classify both categorical and numerical data, and there is no need to have a priori assumptions about the nature of the data. however multiple output attributes are not allowed in decision tree and algorithms are unstable. slight variations in the training data can result it different attribute selections at each choice point within the tree. the effect can be significant since attribute choices affect all descendent subtrees  <cit> . id <dig>  is an algorithm used to generate a decision tree. developed by j. ross quinlan  <cit> , id <dig> is based on the concept learning system  algorithm  <cit> . j <dig> is an improved version of id <dig> algorithm. it contains several improvements, including: choosing an appropriate attribute selection measure, handling training data with missing attribute values, handling attributes with differing costs, and handling continuous attributes  <cit> .

artificial neural networks  is an interconnected group of nodes that uses a computational model for information processing. it changes its structure based on external or internal information that flows through the network. ann can be used to model a complex relationship between inputs and outputs and find patterns in data  <cit> . two common ann algorithms are multi-layer perceptron  and radial basis function  networks   <cit>  <cit> .

a bayesian network represents independencies over a set of variables in a given joint probability distribution . nodes correspond to variables of interest, and arcs between two nodes represent statistical dependence between variables. bayesian refers to bayes' theorem on conditional probability. bayes' theorem is a result in probability theory, which relates the conditional and marginal probability distributions of random variables. the probability of an event a conditional on another event b is in general different from the probability of b conditional on a. however, there is an explicit relationship between the two, and bayes' theorem is the statement of that relationship  <cit> . naive bayes is a rule generator based on bayes's rule of conditional probability. it uses all attributes and allows them to make contributions to the decision as if they were all equally important and independent of one another, with the probability denoted by the equation:

 p=p.p.....pp 

where p denotes the probability of event h, p denotes the probability of event h conditional on event e, en is the n th attribute of the instance, h is the outcome in question, and e is the combination of all the attribute values  <cit> .

random forest is another classifier that consists of many decision trees. it outputs the class that is the mode of the classes output by individual trees  <cit>  <cit> . bagging  can also be used as an ensemble method  <cit>  .

unsupervised clustering
cluster-analysis algorithms group objects on the basis of some sort of similarity metric that is computed for features. genes can be grouped into classes on the basis of the similarity in their expression profiles across tissues, cases or conditions. clustering methods divide the objects into a predetermined number of groups in a manner that maximizes a specific function. cluster analysis always produces clustering, but whether a pattern observed in the sample data remains an open question and should be answered by methods such as resampling-based methods. the k-means algorithm, farthest first traversal algorithm, density-based clustering, expectation maximization  clustering are four common methods used in this study  <cit> .

feature selection
feature selection methods can be divided into the wrapper model and the filter model  <cit> . the wrapper model uses the predictive accuracy of a mining algorithm to determine the goodness of a selected subset. wrapper methods generally result in better performance than filter methods because the latter suffers from the potential drawback that the feature selection principle and the classification step do not necessarily optimize the same objective function  <cit> . in gene selection, the filter model is often adopted due to its computational efficiency  <cit> . filter methods select predictive subset of the features using heuristics based on characteristics of the data. moreover, in wrapper method, the repeated application of cross validation on the same data set might result in finding a feature subset that performs well on the validation data alone. filter methods are much faster than wrapper methods and therefore are better suited to high dimensional data sets  <cit> .

svm-rfe: svm-rfe is a feature selection method to filter out the optimum feature set by using svm in a wrapper-style. it selects or omits dimensions of the data, depending on a performance measurement of the svm classifier. one of the advantages of svm-rfe is that it is much more robust to data overfitting than other methods  <cit> . this is an algorithm for selecting a subset of features for a particular learning task. the basic algorithm is the following: 1) initialize the data set to contain all features, 2) train an svm on the data set, 3) rank features according to ci =  <dig>  4) eliminate the lower-ranked 50% of the features, 5) return to step  <dig>  at each rfe step  <dig>  a number of genes are discarded from the active variables of an svm classification model. the features are eliminated according to a criterion related to their support for the discrimination function, and the svm is re-trained at each step.

correlation based : in cfs features can be classified into three disjoint categories, namely, strongly relevant, weakly relevant and irrelevant features  <cit>  <cit> . strong relevance of a feature indicates that the feature is always necessary for an optimal subset; it cannot be removed without affecting the original conditional class distribution. weak relevance suggests that the feature is not always necessary but may become necessary for an optimal subset at certain conditions. irrelevance indicates that the feature is not necessary at all. there are two types of measures for correlation between genes: linear and non-linear  <cit>  <cit> . linear correlation may not be able to capture correlations that are not linear. therefore non-linear correlation measures often adopted for measurement. it is based on the information-theoretical concept of entropy, a measure of the uncertainty of a random variable  <cit> .

chi squared: another commonly used feature selection method is chi-square statistic  method  <cit> . this method evaluates each gene individually by measuring the chi-square statistics with respect to the classes. the gene expression numbers are first discretized into several intervals using an entropy-based discretization method. then the chi-square value of each gene is computed by

 χ2=∑i=1m∑j=1k2ri.cjn 

where m denotes the number of intervals, k the counts of classes, n the total number of patterns, ri the number of patterns in the ith interval, cj the number of patterns in the jth class, and aij the number of patterns in the ith interval, jth class. the genes with larger chi-square statistic values are then selected as marker genes for classification.

RESULTS
datasets
we applied classification, clustering, and feature selection methods to eight datasets in this work . each dataset is publicly available and data were downloaded from microarray repositories from cageda website from university of pittsburgh  <cit> :

▪ lymphoma  <cit> , contains  <dig> samples of which came from normal vs. malignant plasma cells including  <dig> genes

▪ breast cancer  <cit> ,  <dig> samples of normal vs. tumor subtypes including  <dig> genes

▪ colon cancer  <cit> ,  <dig> samples of epithelial normal cells vs. tumor cells including  <dig> genes

▪ lung cancer  <cit> , contains  <dig> samples of which came from normal vs. malignant cells including  <dig> genes

▪ adenocarcinoma  <cit> , contains  <dig> samples of which came from survival in early-stage lung adenocarcinomas including  <dig> genes

▪ lymphoma  <cit> ,  <dig> samples of dlbcl <dig> vs. dlbcl <dig> cells including  <dig> genes

▪ melanoma  <cit> ,  <dig> samples of normal vs. malignant cells including  <dig> genes

▪ ovarian cancer  <cit> ,  <dig> samples of normal vs. malignant cells including  <dig> genes

pre-processing
we applied three steps pre-processing to the datasets. first we applied baseline shift for the datasets by shifting all measurements upwards by a number of means .

this process then followed by performing global mean adjustment. first, the global mean of all intensities of all datasets is calculated. then, the difference between each individual mean and the global mean is calculated. this difference value is then added to  each individual expression intensity value on each dataset. the result is that all datasets now have the same overall mean.

finally a log transformation applied to the datasets. log transformation has the advantage of producing a continuous spectrum of values.

classification
we used weka  <cit>  and svm classifier  <cit>  for applying classification, clustering and feature selection methods to our datasets. in house java program was used to convert dataset from delimited file format, which is the default import format for svm classifier, to arff  file, the import format for weka  <cit> . for the svm we applied the following procedures.

first we transformed data to the format of the svm software, arff for weka and labeled them for svm classifier. then we conducted simple scaling on the data. we applied linearly scaling each attribute to the range  or  <cit> .

we considered the rbf kernel and used cross-validation to find the best parameter c and γ. we used a “grid-search”  <cit>  on c and γ using cross-validation. basically pairs of  are tried and the one with the best cross-validation accuracy is picked. trying exponentially growing sequences of c and γ is a practical method to identify good parameters  <cit> , for example c = 2- <dig>  2- <dig>  … ,  <dig> and γ = 2- <dig>  2- <dig>  … ,  <dig> 

the classification methods were first applied to all datasets without performing any feature selection. results of 10-fold cross validation have been shown in figure  <dig> and table  <dig>  in most datasets svm and rbf neural nets performed better than other classification methods. in breast cancer data, svm classification and rbf neural nets had the best accuracy  <dig> %, and overall they performed very well on all datasets. the minimum accuracy for rbf we calculated was  <dig> % over melanoma dataset. in lung cancer dataset mlp neural nets did also perform well and it was equal to svm and rbf.

the lowest accuracies are detected from decision tree algorithms . as it is shown in figure  <dig>  in most cases they performed poorly comparing to other methods. bayesian methods had also high accuracy in most datasets. although it didn't performed as good as svm and rbf, but the lowest accuracy was  <dig> % on lymphoma datasets. however overall we have to mention that it seems that in some cases performance of the classification methods depends on the dataset and a specific method cannot be concluded as a best method. for example bayesian and j <dig> decision tree performed very well on colon and lung cancer, with 93% and 95% for bayesian respectively and 91% and  <dig> % for j <dig>  while rbf and mlp out performed on breast and lung cancer .

we applied two class clustering methods to the datasets that are illustrated in figure  <dig> and table  <dig>  as it is shown in figures  <dig> we have a consistence performance of farthest first in almost all datasets. em performed poorly in adenocarcinoma and lymphoma datasets  while it was performing well in breast melanoma .

the effect of feature selection
pairwise combinations of the feature selection and classification methods were examined for different samples as it is shown in table  <dig> and  <dig> and figure  <dig>  the procedure is illustrated as a pipeline in figure  <dig> 

first we tested svm-rfe, correlation based, and chi squared methods on several gene numbers . methods were mostly consistent when gene lists of the top genes  <dig>   <dig>  or  <dig> were compared. we selected  <dig> genes because it performed well, consumed less processing time, and required less memory configurations comparing to others.

almost in all cases, the accuracy performance classifiers were improved after applying feature selections methods to the datasets. in all cases svm-rfe performed very well when it applied with svm classification methods.

in lymphoma dataset svm-rfe performed 100% in combination of svm classification method. bayesian classification method performed well for svm-rfe and chi squared feature selection methods with 92% accuracy in both cases.

cfs and chi squared also improved the accuracy of the classification. in breast cancer dataset the least improvement is observed from applying chi squared feature selection methods with no improvement over svm, rbf and j <dig> classification methods with 97%, 84%, and 95% respectively.

in ovarian cancer dataset all feature selection methods performed very close to each other. however the svm-rfe had a slightly better performance comparing to other methods. we detected 100% accuracy with svm-rfe feature selection with both svm and rbf classification methods. we also observed high accuracies among mlp classification and all feature selection methods with 94%, 92%, and 92% for svm-rfe, cfs, and chi squared respectively.

in lung cancer datasets we can observe high accuracy in decision tree classification methods  with all feature selection methods.

overall we have to repeat again that although it is obvious that applying feature selection method improves the accuracy and also particularly it reduces the processing time and memory usage, but finding the best combination of feature selection and classification method might vary in each case.

CONCLUSIONS
the bioinformatics techniques studied in this paper are representative of general-purpose data-mining techniques. we presented an empirical study in which we compare some of the most commonly used classification, clustering, and feature selection methods. we apply these methods to eight publicly available datasets, and compare, how these methods perform in class prediction of test datasets. we report that the choice of feature selection method, the number of genes in the gene list, the number of cases  and the noise in the dataset substantially influence classification success. based on features chosen by these methods, error rates and accuracy of several classification algorithms were obtained. results reveal the importance of feature selection in accurately classifying new samples. the integrated feature selection and classification algorithm is capable of identifying significant genes.

