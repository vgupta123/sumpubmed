BACKGROUND
the taverna workbench  <cit>  is a tool which facilitates the design and execution of in silico experiments. the experiments are constructed as workflows which can be stored and executed when needed. the building blocks of a workflow are services, also known as processors. technically, workflow is a set of processors, together with connections between their inputs and outputs. the remote processors are implemented as web service   <cit>  operations. scattered physically throughout computational resources of numerous scientific facilities and combined together, the wss operations enable a highly complex analysis, surpassing limits of a common workstation.

taverna services come from a diverse set of life science domains. in the field of computational biology, the taverna workbench provides an access to services which are mainly related to the sequence annotation and analysis. here, we present remote processors that extend taverna’s functionality in the domain of systems biology, specifically, in the analysis of kinetic models of biological systems. our hardware base offers computational resources sufficient for computationally demanding experiments, such as multiple invocations of the model-checking procedure. essentially, the taverna workbench provides a convenient user interface for our ws operations. without programming their own ws client, users can analyze the behavior of cellular systems under various conditions.

features
for a given biochemical network model, the underlying mathematical model is determined by the chosen semantics. the most common representations are ordinary differential equations  for the deterministic framework and continuous-time markov chain  for the framework  <cit> . the latter representation may be equivalently expressed as a set of differential equations, know also as the chemical master equation. unlike the tav4sb project, almost all of the web-based applications reviewed in  <cit>  allow for the analysis of only deterministic representations of biological systems.

operations provided by our web server allow for:

 <dig>  numerical simulations for the deterministic formulation of a biochemical network model, using the sbml ode solver library   <cit> ,

 <dig>  probabilistic model checking of continuous stochastic logic   <cit>  formula over a ctmc, using prism  <cit> ,

 <dig>  visualization of data series, such as odes trajectories or values of parametrized csl properties, and probabilistic distribution sampling, using mathematica  <cit> , and

 <dig>  high-level analysis, such as multi-parameter sensitivity analysis   <cit>  of biological models, with error calculation via either numerical simulations or the probabilistic model checking technique.

the sbml ode solver library enables numerical analysis of models encoded directly in systems biology markup language   <cit> . the library employs libsbml  <cit>  to automatically derive odes, plus their jacobian and higher derivatives, as well as the cvodes package — the state of the art numerical integration library from sundials  <cit> .

prism is one of the leading tools implementing probabilistic model checking, a technique of formal verification of systems that exhibit a stochastic behavior. a system to be analyzed is modeled as a markov chain, and an examined property is expressed in a suitable probabilistic temporal logic. some recent works, see e.g.  <cit> , demonstrate applicability of prism to analysis of models of biological systems. case studies include models of cell cycle control, fibroblast growth factor signaling, and mapk cascade  <cit> . for biological applications a ctmc is typically chosen as an underlying mathematical model and its properties are specified in a continuous time logic, for instance in csl. this approach seems promising and, compared with numerical simulations, it can often yield a better understanding of the dynamics of analyzed systems.

prism handles models defined in the prism input language. currently, a prototype translator from sbml is not integrated into the application itself. therefore, we also provided a separate operation to automatically translate from sbml to the prism language, using the prototype translator.

finally, wolfram’s mathematica is a tool with one of the most advanced graphics engines among plotting software. tav4sb provides mathematica’s two- and three-dimensional list plots together with a versatile set of options for customizing their display. additionally, tav4sb allows to sample from the extensive collection of parametric probability distributions available in mathematica.

context
the aim of the tav4sb project is to support the orchestration of physically scattered tools for execution of repeatable scientific experiments to understand a place of tav4sb in a plethora of similar software, consider the following, mundane technical problem. you have a set of scripts, command line tools or any other form of legacy code, installed on one or more computational servers, not necessarily in the same local area network. for instance, you might have a mathematica script which can be only executed on a server which has mathematica installed on it; and simultaneously you might need to use prism, installed on a remote server with a large amount of required memory. you want to connect these tools in an in silico experiment, say described by a workflow. moreover, in case the experiment doesn’t go as planned, you want to be able to easily modify and re-run your workflow.

tav4sb project is a realization of a minimalist approach to a platform-independent solution, based on the workflow management system and a service-oriented architecture built around the web service standard and a straightforward queue of computational tasks.

tav4sb project consists of two parts. the client part of the project  is a library of sample workflows and helper scripts for analysis of kinetic models of biological systems, using earlier described features. the server part of the project  is a simple grid environment which wraps aforementioned computational tools. those tools are intended to be run in a multi-threaded manner, on one or more, possibly remote, computational servers.

as an utility for wrapping scientific software in web services, the tav4sb project enters premises of projects such as soaplab <dig>  <cit>  and opal <dig>  <cit> . the main difference is that the support for the physical scattering of computational tools is an integral part of the tav4sb server. moreover, tav4sb server easily allows for a direct connection with legacy code. if necessary, the java native interface   <cit>  can be used to connect with the platform-specific libraries written, for instance in c, c++, or fortran. however, in the current state of the project, all that comes at a cost of moderate programming skills required from a user of the tav4sb server, when compared to soaplab <dig> and opal <dig> strategy with the custom configuration file languages. please note however that these languages need to be learned and they pose an easier approach for the user only to a limited extend. also note that, as a minimalist solution with the stateless web service interface, the tav4sb server doesn’t comply with the standards of an open, stateful grid services architecture , which the most prominent representative is globus toolkit  <cit> , a full-fledged grid environment.

implementation
we have chosen the popular systems biology markup language   <cit> , an xml-based data format, to represent kinetic models of biological systems. due to the wide range of dedicated software and due to the support by models repositories like biomodels  <cit> , sbml can be used without a detailed knowledge of the language specification.

client communicates with the server side via ws operations, using simple object access protocol   <cit> . these operations represent the workflow’s remote processors. their signatures are defined in a web service definition language   <cit>  file. we employed a “wsdl first” approach: the wsdl file was manually written .

java web service classes were automatically generated from the wsdl file.

the wsdl file is hosted by the apache tomcat servlet container. it acts as a proxy between the client and the computational part of the server. a web service operation call is translated into a java message service   <cit>  messages. jms application programming interface  allows java applications to create, send, receive, and read messages. it is a part of the java platform, enterprise edition  standards. in our system, jms messages represent computational tasks, and their results. one operation call can be translated into multiple tasks, enabling seamless, tool-specific parallelization of a submitted job.

computational cluster management modules are written in java using the apache activemq implementation of the jms standard. these modules are deployed as the java archive  files. the jms messages are sent over tcp/ip, which basically makes modules independent of their physical location.

new tasks, created by the web server module, are added to the tasks queue. at this point tasks are assigned to any available worker of a compatible type. results are collected in a temporary queue, exclusive for a single ws operation call. long-running tasks use an asynchronous call registry. in such case, direct  response to the ws operation call is merely a message reporting the start of computations. the computed results are collected in a dedicated queue and, when completed, sent to a caller by email .

worker translates both a jms task message into running computational processes and results of these processes back into a jms result message. each worker supports a specific type of computation and can communicate with an actual computational tool differently. currently we implemented three types of workers: mathematica worker which communicates with mathematica via j/link library, prism and odesolver workers which communicate with, respectively, prism and soslib via a command-line interpreter .

RESULTS
we constructed a set of exemplary workflows. their main purpose is to demonstrate how tav4sb ws operations can be used by the taverna workbench client. there are two kinds of workflows: tav4sb ws operation wrappers and in silico experiments.

wrapper workflows illustrate a direct usage of tav4sb operations in taverna. their purpose is to be re-used as nested workflows — building blocks of experiments described below. additionally, we built a number of helper taverna processors, used for interacting with xml-formatted inputs and outputs of ws operations.

in all our in silico experiments we have used the enzymatic reaction model:

  e+s⇀↽k2k1es→k3e+p 

the species names s, e, es and p stand for substrate, enzyme, enzyme-substrate complex and product, respectively. length of an arrow indicates the order of the reaction rate. initial amounts of species and kinetic parameters values, taken from  <cit> , are

  s0=12e0=10es0=0p0= <dig> k1= <dig> k2= <dig> k3= <dig> . 

numerical odes simulations
the first workflow numerically simulates the odes of the model and plots resulting trajectories. odes are derived automatically from a sbml model file, based on rate laws of reactions. in the deterministic model of the enzymatic reaction, rates are described by the law of mass-action. as a result of running this simple experiment one gets time evolution of species concentrations in the form of both data points series and a plot.

probabilistic model checking
the second experiment uses the probabilistic model checking technique to calculate the probability of a property to be satisfied, over a stochastic model of the enzymatic reaction ). the stochastic version is also encoded in the sbml format. the property being checked is expressed as the following reward-based csl formula:

  r#r1=?◊p> <dig> ·limt→∞p. 

roughly speaking, this formula answers the following question: how many times, on average, the reaction r <dig> of association of the enzyme-substrate complex has to occur, before the amount of the product p reaches 50% of its maximum? it is motivated by the half maximal effective concentration . the formula is evaluated for different enzyme initial amounts to find the enzyme’s optimal efficiency. as this is not an instantaneous computation and plotting usually requires many repeats to fine-tune a plot parameters, the experiment is divided into two separate parts: a computational part and a plotting part. figure  <dig> depicts the computational part of the workflow and the resulting plot. the plot can be read as follows: if e is equal to  <dig> then, on average, before the product reaches half of its maximum, each enzyme has to convert slightly more than  <dig> substrates. when e is equal to  <dig> , each enzyme converts on average at most one substrate. the total, parallel efficiency of the enzymatic reaction model doesn’t improve significantly from that point on. not much more than  <dig> complex formation reactions r <dig> are needed to achieve half of the maximum product amount.

multi-parameter sensitivity analysis
sensitivity analysis investigates a relation between uncertain input or parameters of a model, and a property of an observable output  <cit> . sensitivity analysis has been used for various parametrization tasks of models of biological systems, including finding essential parameters for research prioritization  <cit> , identifying insignificant parameters for the model reduction  <cit>  or parameters clustering for the discovery of common functions  <cit> .

biochemical reaction networks yield models of a nonlinear nature for which global sensitivity analysis methods  are the most suitable  <cit> . gsa examines a range of input parameter values simultaneously as opposed to one-factor-at-a-time methods such as those calculating the derivatives of output with respect to parameters. multi-parameter sensitivity analysis   <cit>  is an implementation of the gsa concept. mpsa is an instance of a monte carlo filtering method, which maps samples from a parameter space into behavioral and non behavioral output regions  <cit> . for the examples of applications of the multi-parameter sensitivity analysis to signaling pathways see  <cit> . the mpsa method works as follows:

 <dig>  select parameters to assess.

 <dig>  set parameters range.

 <dig>  generate independent samples.

 <dig>  for each sample calculate the error .

 <dig>  classify samples as acceptable or unacceptable.

 <dig>  for each of the selected parameters compare the classified samples sets.

this procedure is depicted in figure  <dig> as a workflow.

calculating the error for each sample  involves a separate analysis of the model. this is a factor that determines the running time of the mpsa procedure. we ran two variants of mpsa, differing in the way in which the error is calculated. in one variant we used odes simulations and in the other one we exploited the probabilistic model checking technique. we focused on kinetic parameters of two forward reactions of enzymatic reaction models ), i.e. k <dig> and k <dig>  as an error function we took, respectively, the mean squared error of an ode trajectory of the product p and the absolute difference of the value of the formula , in both cases between results for a parameters sample and for the reference values of parameters ). in turn, we obtained empirical cumulative distribution functions  of acceptable and unacceptable samples, for each of the selected parameters. ecdfs were compared using the kolmogorov-smirnov test  and one minus the pearson product–moment correlation coefficient . as a final output of the mpsa method, we got two rankings for each of the sensitivity indices: ks-test and pmcc.

 ddt≈k3+k2+k3k <dig> , 

one can expect that, for values from equation , variation of parameter k <dig> will be more influential, with respect to the product rate, than variation of parameter k <dig> 

interestingly, the results of the other variant of the mpsa procedure are significantly different; one observes that now k <dig> dominates k <dig>  this may be ascribed to the particular choice of the formula  which calculates the average number of occurrences of the first reaction r <dig>  furthermore, an inspection of values of sensitivity indices given in figure  <dig> brings to light that the domination is not as definite as in the first variant of mpsa. results demonstrate that an application of the probabilistic model checking technique may allow for revealing more subtle dependencies in the model, depending on the properties of interest.

mpsa combined with pmc may be applied as a pre-processing step which finds parameters that are insignificant for an analysis oriented on a very specific property of a model. this would provide a novel notion of a probabilistic abstraction  <cit> , i.e. property-specific reduction of the probabilistic model. however, for a successful application, the pre-processing should have low running time, compared to an analysis that follows. in our experiment this is not the case, as we run the exact pmc procedure, which is essentially the same one that would be ran during the further analysis. however, we conjecture that for the mpsa procedure the level of accuracy offered by prism is much too high. we suppose that satisfactory results may be obtained using an approximate approach, such as monte carlo model-checking  <cit> . we plan to pursue this idea as a continuation of the work presented here.

performance test
to measure the network load and the overhead of the task management in tav4sb server we ran a performance test. the test was set up with the mapk cascade case study from the prism web page  <cit>  and with the asynchronous version of the prism ws operation. this version of the prism operation sends computation time statistics, together with results . to run the performance test, we deployed the tav4sb server on the conventional, computational cluster maintained by the center of excellence bioexploratorium at the university of warsaw. the cluster contains  <dig> machines with  <dig> dual-core cpus each, giving  <dig> cores in total. we used  <dig> machines to deploy workers and  <dig> machine for the management queue. the web server was deployed on a separate gate server.

the stochastic mapk model  defines the level of granularity of the represented system, denoted by n. n is the maximal number of molecules for each of the model species. size the an underlying ctmc grows exponentially with the granularity. additionally, verified properties  depend on a time point parameter t. the longest model checking task, for a fixed t value, took just over  <dig> minutes for n =  <dig>  approximately half a minute for n =  <dig> and just few seconds for n =  <dig>  we repeated the test multiple times, each time for n =  <dig>   <dig>   <dig> and t =  <dig>  …,  <dig>  i.e. total of  <dig> prism tasks;  <dig> fast,  <dig> medium and  <dig> long running computations.

table cells contain the average longest computation time in minutes, in different configurations of a number of machines and a number of threads for each worker.

CONCLUSIONS
web-based applications are still not as widely available for the systems biology domain as for other research areas  <cit> . one reason for this state of affairs is the fact that simulating cellular models is computationally expensive, when compared to the data processing tasks. in turn, there is a constant demand for a hardware dedicated to the analysis of kinetic models of biological systems  <cit> .

our services extend the functionality of the taverna workbench in the field of systems biology. together with the services we provide a hardware base for our minimalist grid environment. the grid itself can, and will be, easily extended, independently of a physical location of peripherals and independently of an operating system they are running. moreover, our grid facilitates integration of heterogeneous tools, such as mathematica, prism or soslib. the end-user goal of the tav4sb project is to abstract details of the technological infrastructure. finally, via sbml and the taverna workbench, we would like to promote standardization of models and experiments as well as accessibility of services and their usability for non-programmers. in order to further enhance the usability, we released the source code of the project so that users can extended the tav4sb functionality with their own workers modules. users with programming skills can contribute to the development of the technical aspects of the server part of the project. these aspects cover the plug-in architecture of workers, the library of legacy code connectors , descriptors for the automatic generation of the workers code for common types of wrapped applications , and, last but not least, the support for semantic web services and ontologies  <cit> .

from the point of view of in silico experiments, we propose a novel technique: application of the probabilistic model checking to the calculation of error in the multi-parameter sensitivity analysis procedure. it seem that this approach is particularly well suited for revealing intricate and subtle dependencies, that may not be discovered using, for instance, ode-based numerical simulations of a model. we suppose that this technique may have interesting applications, e.g. for probabilistic abstraction  <cit> .

availability and requirements
• project name: tav4sb

• project home page: http://bioputer.mimuw.edu.pl/tav4sb/

• operating system: platform independent 

• programming language: optionally, scufl/t2flow, beanshell, xslt  and java, mathematica, bash 

• other requirements: the taverna workbench client  <dig>  or higher, jsbml  <dig> -b <dig>  plus, optionally, any files hosting web server  and apache tomcat  <dig>  series, apache maven  <dig> or higher, plus, optionally, mathematica  <dig>  or higher, prism  <dig>  series and sbml ode solver  <dig>  

• license: gnu agpl

• any restrictions to use by non-academics: none

please note that, technically, scufl and t2flow are workflow description languages, but together with the graphical notation provided by the taverna workbench they can be seen as visual programming languages. these and other client dependencies on a programming language are optional because one can write their own ws client in virtually any language. also, be advised that the apache maven tool  automatically resolves all dependencies on java libraries, such as javamail or apache activemq .

the definition of operations provided by tav4sb ws plus workflows files, together with installation and execution instructions are available from the project’s home page. documentation of the tav4sb ws can be found in biocatalogue  <cit> , a curated catalogue of life sciences web services. wrappers and experiments workflows are also available from the myexperiment repository  <cit> , together with the workflow figures.

client workflows were tested on ubuntu linux , mac os x  and windows vista  operating systems. the production server is currently deployed on computational servers at the faculty of mathematics, informatics and mechanics of the university of warsaw . the performance test server was deployed on a cluster of ubuntu linux machines  and solaris gateway . a local developer’s environment, with both client and server, was deployed and tested on ubuntu linux  and mac os x .

competing interests
the authors declare that they have no competing interests.

authors’ contributions
mr performed experimental parts, implemented the system and prepared the final version of the manuscript. ml implemented the grid structure of the system and performed performance tests. pb did the initial implementation. ag and sl supervised this project and participated in drafting the manuscript. all authors have read and approved the final manuscript.

