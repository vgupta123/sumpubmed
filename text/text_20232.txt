BACKGROUND
biological pathways are represented as networks of interactions among biological entities such as cell, dna, rna and enzyme. the exposure of biological pathways are expected to deepen our understanding of how diseases come about and help expedite drug discovery for treating them. computer-based pathway study currently relies on two main approaches of entity/process representation: free-text descriptions and computer models. free-text based approaches used in genbank  <cit> , dip  <cit> , kegg  <cit> , swiss-prot  <cit> , and cope  <cit>  rely on free text annotations and narratives  <cit>  to target towards human comprehension. one major disadvantage with these approaches is their inherent lack of support for computer-based simulation of these processes. computer models  of biological processes, on the other hand, while enabling computer-based simulations of biological processes, are often constructed in isolated environments, limited to the study of known pathways, and lack the ability to facilitate the discovery of new pathways. we propose to use web service modeling strategy  <cit>  to bridge the gaps between the two representation approaches. using this strategy, biological processes are modeled as web service operations, which can be first described and published by one organization, and later discovered and invoked by independently developed applications from other organizations. a service operation may consume some input substance meeting a set of preconditions and then produce some output substance as a result of its invocation. some of these input and output substances may themselves carry processes that are known to us and thus can be also modeled and deployed as web services. domain ontologies containing definition of various entity types would be used by these web services for describing their operation inputs and outputs. this service oriented process modeling and deployment strategy not only allows for the identification of pathways linking processes of biological entities, as do existing natural language processing approaches , but would more importantly bring about unprecedented opportunities for analyzing such pathways right on the web through direct invocation of involved services. when enough details are captured in these process models, this in-place invocation capability presents an inexpensive and accessible alternative to existing in vitro and/or in vivo exploratory mechanisms.

the second key contribution of our work is the development of our service mining tool, named pathexplorer, used to discover potentially interesting biological pathways  linking service models of biological processes. unlike traditional top down service composition approaches that are driven by specific user goals, web service mining, which aims at the discovery of any interesting and useful compositions of web services, is carried out in a bottom up fashion with no such goals to guide the search process. as a result, it faces the challenge of combinatorial explosion as the number of service models increases. in search for efficient mining algorithms and framework, we drew inspirations from molecular recognitions and drug discovery methodologies and developed several key mining algorithms with performance that is linear to the number of service models that are involved  <cit> .

in  <cit> , we applied our web service mining framework  <cit>  to service models of biological processes that are deployed using web service modeling execution environment   <cit>  for the discovery of biological pathways. these service models are expressed using both web service markup language  and web services description language . we then explored the opportunity of evaluating such pathways on the web through direct invocation of involved services. in  <cit> , we extended our approach to also provide graph-based hints on discovered pathways to help user formulate hypotheses, which can then be either confirmed or rejected based on simulation results, leading to the identification of useful pathways. in this paper, we establish the analogies between molecules and web services, paving the way for future interdisciplinary exploration of these two seemingly unrelated subjects. we also describe in detail our graph expansion algorithms that are not covered in  <cit> . the algorithms are used to identify subgraphs linking interesting edges and user selected nodes within an existing pathway network. these subgraphs provide the basis for hypothesis formulation and simulation based evaluation.

methods
the bottom up web service mining inevitably exposes itself to the problem of combinatorial explosion, which, if left unaddressed, renders the mining process unscalable as the number of services involved increases. nature, however, has provided us with ample examples on how composition takes place in a bottom up fashion. in this section, we first establish analogies between molecular world and web services world. we then draw inspirations from molecular recognitions and drug discovery processes and present our web service recognitions mechanisms and mining framework.

analogies between molecules and web services
web services share similarities in many ways with atoms and molecules in the natural world. at the most basic level, web services are analogous to atoms, as illustrated in figures  <dig> and  <dig>  in the chemical world, oxygen and hydrogen atoms are two of the most basic building blocks in nature. under the right condition, the supply-demand relationship between the two types of atoms drives them to form bonds and, ultimately, a water molecule. similar supply-demand relationship is also what drives web services together. like a molecule that is composed of atoms or simpler molecules, a composite web service is composed of simpler component web services. operation invocations among services are realized through the exchange of extensible markup language  messages. similar to electrons in the natural world, these messages bind component web services into composite services. if we imagine each message as an electron, then this type of message exchange can help establish bonds between web services. here, we use bond as a notional concept to indicate the composability between two web services.

the analogy between the molecular and web service worlds continues at a more complex process level. in the chemical world, the dna inside our cells provides a complete genetic blueprint that carries the information required to manufacture the enzyme proteins, which in turn are responsible for orchestrating our body's chemistry. the progression from dna to mrna to protein involves a molecular assembly line  <cit>  that follows a remarkable process ). likewise, web service composition can also involve a complex process ) using process template as blueprint for process flow instances. the process template is analogous to the blueprint carried by the mrna and the process flow instance is analogous to the protein chain.

the similarities between web services and molecules offer some interesting insights. they suggest that like molecules that compose from bottom up as if they are living beings, web service can also be treated as living beings that recognize each other under the right conditions. the process analogy indicates that recognition-triggered service composition may extend to a flow network. such a flow network can be either designed from top down or emerged from bottom up. as a result, instead of having to search for interesting and useful web service compositions and composition networks exhaustively, the compositions and composition networks could form "naturally" from bottom up, similar to what is happening in the natural world.

web service/operation recognitions
similar to the molecular world, the natural formation of service compositions is based on automatic recognitions among web services and their corresponding operations. we have identified the following three service/operation recognition mechanisms that are applicable to web service models of biological processes:

promotion
when operation op <dig> of service sa produces an entity  that in turn provides service sb, we say that sa : op <dig> promotes sb, as shown in figure  <dig> 

inhibition
when operation op <dig> of service sa consumes an entity  that in turn provides service sb, we say that sa : op <dig> inhibits sb as shown in figure  <dig> 

indirect recognition
a target operation opt indirectly recognizes a source operation ops, if ops generates some or all input parameters of opt, as shown in figure  <dig>  indirect recognition is in contrast to the concept of direct recognition  <cit> , where an operation can be directly invoked by another. direct recognition is applicable to fields such as e-commerce but not pathway discovery and is thus not included here. these recognition mechanisms form the basis of the filtering algorithms  <cit>  in our mining framework.

pathexplorer architecture
as we look for appropriate web service mining framework, the identification of the similarities between web services and molecules leads us to the relevance of drug discovery methodologies used in the pharmaceutical industry. a typical drug discovery process  <cit>  involves seven steps:

 <dig>  disease selection,

 <dig>  target hypothesis,

 <dig>  lead compound identification and screening,

 <dig>  lead optimization,

 <dig>  pre-clinical trial,

 <dig>  clinical trial, and

 <dig>  pharmacogenomic optimization.

there are several interesting observations about the process described above. first, the drug discovery process has adopted the strategy of screening molecules  using "coarse-grained" filtering approach to quickly reduce the search space from the focused library of potential ligands to one that contains those most likely to bind to a protein target with high affinity. it then increases the computation complexity with better accuracy on a reduced search space for lead optimization . with a much smaller remaining space, the discovery process finally conducts more expensive clinical study for drug evaluation. this is a powerful strategy and can also apply well in the field of web service mining.

web service screening could take advantage of some "mining context" to scope down the searching space and identify potentially composable web services in an early stage. the identification of the composability can be achieved using a "coarse-grained" ontology-based filtering mechanism. automatic verification and objective analysis can be applied next in a reduced pool of candidate services. a more elaborate runtime simulation mechanism can then be applied towards composed web service leads in a much smaller search space to investigate the relationships among various composition leads involved in the composition network. finally, expensive subjective usefulness analysis involving human in the loop can be conducted in an even smaller search space to distinguish those that are truly useful.

based on these recognition mechanisms coupled with a publication/subscription-based algorithm  <cit> , linkages between web services and their operations in the focused library can be quickly established from bottom-up. these pathway segment leads are then semantically verified based on a subset of operation pre-and post-conditions involving binary variables  and enumerated properties . finally, verified pathway segment leads are linked together using our link algorithms for establishing more comprehensive pathway network.

discovered pathways from the screening phase are input to the evaluation phase, which consists of four sub-phases. objective evaluation identifies and highlights interesting segments of a pathway by checking whether such linkages are novel . an interactive session follows next with the user taking hints from these highlighted interesting segments within the pathway network and picking a handful of nodes representing services, operations and parameters to pursue further. pathexplorer then attempts to link these nodes into a connected graph using a subset of nodes and edges in the original graph. this subgraph provides the user the basis to formulate hypotheses. as an example, such a hypothesis may state that an increase in the dosage amount of aspirin will lead to the relief of pain, but may inadvertently increase the risk of ulcer in the stomach. these hypotheses can be tested out via simulation, which involves pathexplorer invoking the relevant service operations, changing the quantity/attribute value of various entities involved. simulation results showing the dynamic relationships between these biological entities are then presented to the user, whose subjective evaluation finally determines whether the pathway in pursuit is actually useful.

service-based modeling of biological processes
to model biological processes as web services, we first compiled a list of conceptual process models shown in figure  <dig> that are based on  <cit>  and  <cit> . in addition to describing process models, these sources also reveal some simple relevant pathways that can be manually put together. we use process models such as these as references when we develop real web services. we also use simple pathways manually constructed here as references when we check the correctness of pathways automatically discovered using our mining algorithms.

each of the conceptual process models is next captured in a java class and exposed through axis <dig>  <cit>  running inside a jetty web server  <cit>  as a wsdl  <cit>  service. although the internal details of biological processes can be modeled as wsdl web services, wsdl itself does not provide elaborate mechanism for expressing the pre- and post-conditions of service operations. wsdl also lacks the semantics needed to unambiguously describe data types used by operation input and output messages. we choose wsml  <cit>  among others   <cit> , wsdl with semantics   <cit> ) to fill this gap due to the availability of wsmx  <cit> , which supports the deployment of ontologies and web services described in wsml. we categorize biological entities within our mining context into several ontologies. these include fatty acid, protein, cell, and drug. they would all refer to a common ontology containing generic entity types such as substance, the root concept of all entity types. we use unknownsubstance as a placeholder for process inputs that are not fully described in the literature. we also create a miscellaneous ontology capturing definitions of entity types found in the literature that don't seem to belong to any domain. figure  <dig> shows several ontologies including those for cells, proteins, fatty acids and the miscellaneous entities rendered in web service modeling toolkit   <cit> .

using these ontologies, we then wrap the semantic interfaces of existing wsdl services as wsml services. wsml supports the descriptions of pre- and post-conditions in the capability section and the ontological type description in the interface section. figure  <dig> gives an example of each for the nf_kappab_rel service. to work with wsml, we have made slight adaptations to our screening algorithms so they can be applied directly to wsml services. first, we add a provider property in the non functional properties  section of each wsml service to indicate the corresponding ontological type of an entity that can provide the service. pathexplorer uses this information to establish the relationship between a service providing entity and the service it provides. second, we add a modelsource property in the nfp section to indicate the source information that the model is based on. third, we add a providerconsumable property in the nfp section to indicate to pathexplorer whether the service providing entity should be consumed along the invocation of its operation. for example, in order for mucus ) to cover the wall of stomach, the mucus itself will have to be consumed. finally, our validation algorithm has been customized to work with the service interrogation apis of the wsmx runtime library for determining the overlap between the postcondition of a source operation and the precondition of a target operation. unfortunately, wsml allows for the specification of pre- and post-conditions for only an entire service, but not its individual operations. thus we have to split services that each originally has multiple operations into several services  so that different conditions can be individually specified for these operations. pathexplorer uses the name of these services to keep track of their relationship and uses that information to merge these services towards the end of the screening phase. during simulation, pathexplorer uses lowering/lifting adapters  <cit>  to convert ontological entity instances used by wsml services to/from simply object access protocol  messages used by wsdl services.

pathway visualization and establishment of interesting subgraphs
to support pathway visualization, pathexplorer generates a graph markup language  file for each discovered pathway network and uses yed  <cit>  to render the corresponding graph. we have developed algorithms in pathexplorer to help user, during the evaluation phase, formulate hypotheses that would lead to the identification of useful pathways extended from interesting segments. the addition of the modelsource property  in the nfp section allows pathexplorer to identify novel  linkages between service models in a discovered pathway network by comparing the source indicator of linkages in the pathway graph representing the three types of service/operation recognitions as shown in figure  <dig>  pathexplorer then automatically highlights these edges in the graph, presenting them as visual aid to the user for focusing more on nodes that may lead to the identification of useful pathways. after the user selects nodes of interest, pathexplorer attempts to link them into a connected graph to the extent possible using steps illustrated in figure  <dig> 

connected graphs identified using the above process are then presented to the user as basis for hypothesis formulation. we list algorithm  <dig> used to achieve steps  <dig> and  <dig> in figure  <dig>  we first construct two global reference sets: eb for edges and nb for nodes . since we are trying to connect all interesting nodes, the algorithm stops when ni ≤  <dig> . the rest of the algorithm aims at coalescing each group of nodes in nb linked by interesting edges into one node. we first construct se  to initially contain all the interesting edge references. for each remaining edge e picked from se , we construct a group node reference set sn  and group edge reference set tete . a coalesce function  is then invoked to coalesce nodes that e connects. using coalesce, we first move e from the interesting edge reference set se to the corresponding group edge reference set te . then for each node n, which is in nb but not sn and which e connects to , we add it into the corresponding group node reference set sn . if n is an interesting node  and sn is already marked as interesting , then we know that n is not the first interesting node in sn, thus we can reduce the number ni of interesting node reference sets by  <dig> . if sn is not yet marked as interesting, we need to simply do so . we then recursively invoke algorithm  <dig> in figure  <dig> for all other edges in se that are connected to n . it is conceivable that se, te and sn may all change as a result of this coalescence process. going back to algorithm  <dig> in figure  <dig>  code in lines  <dig> through  <dig> aims at picking a node from each group as the proxy for the whole group during the incremental expansion phase . to achieve this, we pick out the first node ng found in sn  and convert it to a group node . lines  <dig> through  <dig> converts ng to a nucleus node and marks corresponding edges in the global edge set ea as already connected. lines  <dig> through  <dig> makes ng a surrogate node for all the other nodes in the same group. in line  <dig>  we also convert interesting nodes  that are not group nodes into nucleus nodes.

managed expansion described in step  <dig> is achieved via algorithm  <dig> as shown in figure  <dig>  the algorithm first checks whether there is only one interesting node and it should simply stop . if there are more than one interesting node, it constructs tn, used to keep track of all visited nodes, to initially contain all nucleus nodes . the rest of the algorithm then incrementally expands all the nuclei until they are all connected  or when all nodes in the graph have been visited . in addition, we use variable progress  to keep track of the progress of graph expansion and stops algorithm if no progress has been made during the last iteration . a distance variable d is used to manage the incremental expansion. as the expansion progresses, each of the encountered nodes is checked to see whether its distance attribute is already set. if this is not set , then the node must be a newly encountered node and the algorithm sets the distance , tags it as belonging to the same nucleus group  and indicates that the node has been visited by adding it into tn . if the node is a previously visited node  and it is associated to a nucleus group different from the current one , then a merge point  is potentially encountered. to be sure, the algorithm checks whether the edge extending to the node just encountered has already been visited . if not, it marks the corresponding edge in ea as connected and then invokes connectpathtonucleus, indicates that the edge has been visited by adding it into sp , and checks whether the stop criteria  have been met. algorithm  <dig> in figure  <dig> lists the algorithm used in connectp atht onucleus to mark all edges from the encountered node and leading to the corresponding nucleus node as connected. the traversal of a group node  would trigger additional expansion  that would mark all interesting edges in the corresponding group also as connected.

pathway simulation
discovered pathway networks are first presented to the user in yed graphs with interesting linkages highlighted. once interesting nodes are picked by the user, pathexplorer attempts to use the above process to link them into a connected subgraph, an example of which is shown in figure  <dig> and highlighted with thick edges. such graphs are then presented to the user as basis for hypothesis formulation. we keep track of the pre- and post-condition details of operation linking edges  in our algorithm as such information along with the ontological entity paths and wsml service paths are needed when we try to invoke these services during simulation. to ensure the correctness of our algorithms, we compared segments within the automatically discovered pathway network with those constructed manually in figure  <dig> and found them to be consistent in all cases.

when an operation is to be invoked, the algorithm checks two factors. first, it examines whether all the pre-conditions of the operation are met. an operation that does not have available input entities meeting its preconditions should simply not be invoked. second, it determines how many instances are available for providing the corresponding service. this factor is needed due to the fact that biological entities of the same type each has a discrete service process that deals with input and output of a finite proportion. the available instances of a particular service providing entity will drive the amount of various other entities they may consume and/or produce. for this reason, the algorithm treats each entity node in a pathway network such as one shown in figure  <dig> as a container of entity instances of the noted ontology type. in some cases, the service provider is also used as an input parameter. for example, the sensepain operation from the nociceptorservice in figure  <dig> has a precondition stating that the nociceptor itself should be bound in order to provide this service. in order to express this precondition, we decided to include the service providing entity also as an input parameter. in cases such as this, the number of service providing instances will be determined by checking further whether each of the service providing entity instances also meets the precondition of the corresponding operation.

in algorithm  <dig> , an initial number of instances for each entity type et are first generated based on function f . it is conceivable that an expert may want to create different number of instances at the beginning for different entity types. next, we conduct i iterations of operation invocations . we take a snapshot of the quantities at the end of each iteration and before the very first iteration . we determine the number of times the corresponding operation should be invoked based on the quantity of the corresponding service providing entity . to make sure that an operation from a service providing entity of a small quantity also gets the chance to be invoked, a random number generator is used . upon invocation of the operation, we remove corresponding entity instance based on the truth table depicted in the lower right corner of figure  <dig>  when we determine the provider should be removed , we remove the first instance found in the corresponding container. since the provider is not the input parameter, it is consequently not involved in the evaluation of the operation precondition. thus we can remove any one instance found in the container. lines  <dig> to  <dig> are for removing the input parameter instance when the corresponding condition is met. finally, we add the output parameter instance to the corresponding entity container .

RESULTS
while figures  <dig> to  <dig> clearly illustrate the relationships between aspirin and stomach_cell, the relationship between the dosage amount of aspirin and the sensation of pain is less obvious in these figures. except for figure  <dig>  which shows some accumulation of painsignal when the quantity of aspirin is  <dig>  the rest of plots show no pattern of such accumulation or the variation thereof. a closer look at the highlighted pathway in figure  <dig> reveals that this is actually consistent with the way the simulation is set up. since painsignal is created and then converted by the brain to reliefsignal, which disappears after it is sensed by nociceptor, this whole path at the bottom actually acts as a 'leaky bucket'. to examine exactly what is going on along that path, we decided to make two changes in the simulation setting. first, we reduce the maximum frequency of invoking the brain service to half that of nociceptor. this creates a potential imbalance between the production rate of painsignal and reliefsignal since the processpain operation from the brainservice will be consequently invoked less frequently than the sensepain operation from the nociceptorservice. second, we disable the senserelief operation of the nociceptorservice. this essentially stops the leaking of the reliefsignal that are generated as a result of the painsignal. when we apply only the first change to the simulation, the imbalance of the processing rates for painsignal and reliefsignal results in a net accumulation of painsignal when the quantity of aspirin is  <dig> ). when the quantity is increased to  <dig> ), we see there are some occasional and temporary accumulation of painsignal. finally, we apply the second change along with the first one. consequently, we notice that while the pattern of painsignal's accumulation hasn't changed much, there is a consistent accumulation of reliefsignal. since each painsignal is eventually converted to a reliefsignal by the brain according to the highlighted pathway in figure  <dig>  the rate of reliefsignal's accumulation actually provides a much better picture on how fast painsignal has been generated. we see that as the dosage amount of aspirin increases, less reliefsignal is generated, an indication that less painsignal has been generated. thus it is obvious that the increase of the dosage amount of aspirin has a positive effect on the suppression of painsignal's generation. this confirms the other half of user's original hypothesis.

simulation results such as these presented in figure  <dig> provide useful information to a pathway analyst. they can be used to determine whether further more expensive in vitro and/or in vivo experiments are needed. if enough details are captured in the process models that the simulation is based on, then the simulation itself would present an inexpensive and accessible alternative to existing in vitro and/or in vivo exploratory mechanisms. using the service-oriented simulation environment, the interrelationships among various entities involved in the pathway network can now be exposed in a more holistic fashion than traditional text-based pathway discovery mechanisms, which inherently lack the simulation capability.

CONCLUSIONS
we proposed to model biological processes as web service to bridge the gap between free-text description and traditional computer models of these processes. we presented our service mining tool named pathexplorer and demonstrated the feasibility of applying our service mining strategy to the discovery of pathways linking service models of biological processes. we described how pathexplorer identifies interesting segments in a pathway graph and automatically establishes a connected graph linking nodes that the user is interested in exploring. the graph, which is highlighted inside the discovered pathway network provides the user the basis for formulating hypothesis, which can then be tested out through simulation.

list of abbreviations used
graphml: ; owl-s: ; soap: ; wsdl: ; wsdl-s: ; wsml: ; wsmt: ; wsmx: ; xml: .

competing interests
the authors declare that they have no competing interests.

authors' contributions
george zheng and athman bouguettaya developed the web service modeling and mining framework. george zheng applied it to the discovery and analysis of biological pathways. athman bouguettaya supervised the project, reviewed and approved the final manuscript.

