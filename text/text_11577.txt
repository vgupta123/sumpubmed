BACKGROUND
the field of study of cortical processing of complex sounds has been highly productive in the recent past both in humans and monkeys. a model similar to the "what" and "where" segregation of the visual processing network has been suggested for auditory processes of sound identification and localization  <cit> . regions within the superior and middle temporal cortices and the inferior prefrontal gyrus have been identified as candidates for the "what" pathway of the auditory stream, whereas a "where" pathway would rely on the posterior temporal cortex and the inferior and superior parietal cortices  <cit> . within the auditory "what" pathway, functional magnetic resonance imaging  studies have identified the 'temporal voice areas' , i.e. bilateral auditory areas situated along the superior temporal sulcus  showing a greater response to human vocalisations than to other sound categories  <cit> . these regions were later found to be species-specific as they elicited stronger responses to human vocalisations compared to non-human vocalisations  <cit> . in a recent fmri study in macaques, petkov et al.  found a region of the secondary auditory cortex which showed a comparable preference for conspecific vocalisations over vocalisations from other species or non-vocal sounds. these findings suggest a long evolutionary history of voice-preferential processing  <cit> . yet, the time course of voice processing remains unclear.

in studies using intracranial electrophysiological recordings in human participants, early responses to sound stimulation were shown to reach the primary auditory cortex  as early as  <dig> ms after sound onset  <cit>  and differences between sound categories have been observed as soon as  <dig> ms after this early response to sound stimulation. using evoked related potentials  and an oddball paradigm, murray et al.  reported early erp differences between man-made  and living auditory objects  as early as  <dig> ms after stimulus onset  <cit> . although most of the living sounds in that study consisted of vocalisations, it remains unclear whether the effect was driven by voices or not. erp studies providing evidence directly relevant to the speed of voice/non-voice categorisation are scarce. two studies found a larger response to sung voices when compared to instrumental sounds at a latency of  <dig> ms after stimulus onset, with a fronto-central distribution, which was termed the "voice-specific response"   <cit> . to further assess the vsr, gunji et al.  used magnetoencephalography  and analysed two components of the evoked response: the n1m and the sustained field observed  <dig> ms after stimulus onset. they observed no difference in the magnitude of the n1m component between voices and instrumental sounds; however the source strength of the  <dig> ms sustained field was larger for vocal sound than for instrumental sounds  <cit> . both components had sources in heschl's gyrus in both hemispheres.

although previous studies did not address directly the voice/non-voice discrimination process, some of them suggest the existence of earlier correlates of voice processing. indeed, effects of voice familiarity  <cit> , voice gender adaptation  <cit> , human vs. computer voice  <cit> , voice priming  <cit> , speech vs. tones  <cit> , and speaker identity  <cit>  have been observed between  <dig> ms to  <dig> ms.

the relatively long latency of voice vs. non-voice erp differences  stands in strong contrast with these results and with the early living/non-living distinction reported by murray et al. .

in the present study, we investigated the speed of voice processing by measuring erps in response to sounds from three categories -- voices, bird songs and environmental sounds -- while participants were performing an incidental target  detection task. we hypothesised that since neural correlates of voice paralinguistic characteristics were observed in the range of  <dig> to  <dig> ms, investigating neural correlates of voice recognition by directly comparing neural responses to voices with those of sounds from other categories should lead to differences in the same latency range or earlier, in contrast with the previously reported  <dig> ms.

methods
participants
thirty-two french-speaking adults , participated in the study. they all reported normal audition and no neurological problems. they all gave informed written consent and the study was approved by the university of montreal ethics committee. participants were compensated  <dig> canadian dollars for their time. some of the subjects were included in the 'novice' group of a study focusing on differences between ornithologists and novices  <cit> , although they were never informed of the expertise nature of this study, and only informed to press a response button as fast as they could when they heard a  <dig> hz pure tone target.

stimuli and design
stimuli consisted of  <dig> sound samples,  <dig> in each of three categories: bird songs, human vocalisations  and environmental sounds . the bird songs were selected from the « chants d'oiseaux du québec et de l'amérique du nord »  audio cd. other sound stimuli came from commercially available sources and from recordings in the laboratory. sounds were edited using cool edit pro  to a sampling rate of  <dig> hz, a 16-bit resolution, and duration of  <dig> ms with a 10-ms linear attack and decay. they were all root mean square  normalised using matlab . a sample of the stimuli is available for consultation online .

analyses of sound power in the temporal, spectral and time-frequency domains were performed using one-way anovas at each time, frequency, or time-frequency bin  using matlab . in the time domain, power differences between the three sound categories were observed from  <dig> to  <dig> ms  and from  <dig> to  <dig> ms . post-hoc tests showed that differences in the temporal domain were driven by voices from  <dig> to  <dig> ms and by environmental sounds from  <dig> to  <dig> ms, with significantly less power than the other two categories.

in the frequency domain, significant effects were found around  <dig> hz  and  <dig> hz , reflecting the smaller power in low frequencies  and greater power at intermediate frequencies  for bird songs relative to the other two categories ; and more power at high frequencies  for environmental sounds compared to the other two categories although this last difference was not significant .

in the time-frequency domain, significant effects of sound category were observed across all frequencies for early latencies only , whilst differences in frequency bands around  <dig> hz and  <dig> hz were observed at all time-bins .

procedure: sound identification
 <dig> participants were included in a verification study in which they listened to each sound from the three stimulus categories and performed a three alternative forced choice categorisation task. all categories showed similar levels of recognition . statistical analyses testing the null h <dig> hypothesis with a bootstrap method  showed no significant differences between the three categories .

procedure: task
participants were seated in a sound-proof cabin and were presented with each sound from the three categories in a pseudo-random order with a 3000- <dig> ms random inter-stimulus-interval . each sound was played twice, in two different runs. stimuli were presented via beyerdynamic dt  <dig> headphones at a self-adjusted comfortable level of about  <dig> db sound level, as measured using a lutron sl- <dig> sound level meter. participants were instructed to detect a  <dig> hz sinusoidal pure sound target with a 10% probability of occurrence. they were instructed to press a button each time they heard the target stimulus, and also to minimise blinking, head motion and swallowing.

procedure: eeg recordings and analysis
electroencephalography  data were recorded continuously at a  <dig> hz sampling frequency using a brainamp amplifier  using a  <dig> - <dig> hz band-pass filter. the  <dig> ag/agcl electrodes were attached using a braincap 10- <dig> array, with an on-line reference at electrode fcz, a ground electrode on the midline posterior to oz, an ecg electrode attached above the left collar bone, and an eog electrode attached above the zygomatic bone, below the left eye. the electrode impedances were kept below  <dig> kΩ throughout the recording.

eeg recordings were analysed using eeglab v <dig>   <cit> , under matlab . trials with abnormal activities were excluded based on a detection of extreme values , abnormal trends , and abnormal distribution   <cit> . data were then re-referenced to the average of all electrodes and band-pass filtered in the range 1- <dig> hz. for each stimulus category, eeg epochs of  <dig> ms, starting  <dig> ms before stimulus onset, were averaged, and the mean pre-stimulus activity was subtracted from the activity at each time point.

statistical inferences on amplitude differences between sound categories were performed for each electrode and time point using bootstrap procedures implemented in matlab . the null hypothesis h <dig> that the three conditions were sampled from populations with similar means was evaluated by sampling with replacement, independently for each subject, among the three categories. the samples consisted of the full electrodes by time-points matrices. this was followed by averaging the erp across subjects for each resampled condition, and then computing the differences between the means of two fake conditions. this process was repeated  <dig> times, leading to a distribution of  <dig> bootstrapped estimates of the mean difference across subjects between two erp conditions. then the  <dig> % percent confidence interval was computed . finally, the difference between two sample means was considered significant if it was not contained in the  <dig> % null hypothesis confidence interval  <cit> . the statistical analyses were restrained to a  time-window as we were mainly interested in rapid brain discrimination processes.

in order to evaluate the relative contribution of speech and non-speech vocal sounds, post-hoc analyses were performed on a sample of  <dig> speech sounds  and  <dig> non-speech vocal sounds  selected from the voice category . electrophysiological responses to these two categories were then compared independently to the average of bird songs and environmental sounds using the bootstrap procedure described above.

RESULTS
behavioral results: target detection task during eeg recordings
mean reaction times  to correctly detected targets were  <dig> ms across subjects. in terms of accuracy,  <dig> % of the targets were followed by a button press  and only  <dig> % of the non-target events were followed by a button press ).

responses to targets were split according to whether targets followed voices, bird songs or environmental sounds. repeated measures anovas were implemented on rt for correct responses, proportion of hits and false alarms using spss . we did not observe differences in proportion of hits  =  <dig> , p =  <dig> ), nor in fa rate  =  <dig> , p =  <dig> ), but we observed a significant difference in rt  =  <dig> , p =  <dig> ). paired samples t-tests indicated significantly longer reaction times in response to the pure tone following presentation of bird songs than for voices  and environmental sounds , which did not differ from each other . mean values and their standard deviations for the behavioral results are reported in table  <dig> 

average reaction times , and proportion of hits and miss were split depending on whether the pure tone target was preceded by the presentation of voices, bird songs or environmental sounds. the table also presents the relevant standard deviations from the mean. proportion of false alarms following the presentation of the  <dig> sound categories are also presented in the table with standard deviations from the mean.

event related potentials
voices vs. bird songs and environmental sounds
erps in all participants showed the classical n1-p <dig> waveform components with central topographic distribution  <cit> . results from the comparison of erps to voice vs. the other categories are shown in figure  <dig>  the earliest amplitude differences were observed for the voice vs. bird song comparison, emerging around  <dig> ms after sound onset at electrodes o <dig>  po <dig> and po <dig> , and about  <dig> ms later at most of the occipital and frontal electrodes. these differences peaked at about  <dig> ms, and lasted until  <dig> ms after stimulus onset. the earliest amplitude differences between voices and environmental sounds were observed at  <dig> ms on fronto-temporal electrodes ft <dig> and ft <dig>  peaking at about  <dig> ms and lasting until  <dig> ms after stimulus onset . significant amplitude differences between voices and environmental sounds were also observed around  <dig> ms on several occipital electrodes . a conjunction of these two differences revealed a broadly distributed pattern of erps with a preferential response to voices . while bilateral fronto-temporal electrodes  showed a greater positivity in response to voices, a larger negativity was observed at occipital locations .

at fronto-temporal electrode fc <dig> , a significant amplitude difference showing a smaller negative potential for voice compared to both bird songs and environmental sounds was observed between  <dig> ms and  <dig> ms . following this smaller negativity, a larger positive erp amplitude elicited by voice sounds compared to bird and environmental sounds was observed at fronto-temporal electrode fc <dig> as early as  <dig> ms extending to  <dig> ms , and from  <dig> ms to  <dig> ms at fronto-temporal electrode fc <dig> . at occipital electrodes, larger erp negativities for voices compared to both bird songs and environmental sounds were observed from  <dig> ms to  <dig> ms with maximal difference at  <dig> ms at electrode po <dig> , and from  <dig> ms to  <dig> ms with maximal difference at  <dig> ms at electrode po <dig> .

speech contribution to the voice effect
in order to evaluate the contribution of speech information to the voice-preferential response observed in the time-window of the auditory p <dig>  voice stimuli were separated in speech vocal sounds  and non-speech vocal sounds . similar patterns of amplitude difference were observed for speech and non-speech voice stimuli when compared to other categories . both showed a significant difference around the p <dig> component starting at  <dig> ms  although effects were broader and longer lasting for speech than non-speech sounds. finally, as one can expect, speech vs. non-speech stimuli showed significant differences over many electrodes . importantly, effects appeared first between  <dig> and  <dig> ms and later on from  <dig> ms over fronto-temporal electrodes and from  <dig> ms over occipital electrodes . these differences between speech and non-speech stimuli were clearly different from those observed for voices  compared to environmental and bird sounds.

other categorical effects
in addition to the stronger voice responses reported above, another categorical effect was observed: bird songs elicited smaller n <dig>  and p <dig>  components than voices and environmental sounds at the vertex electrode cz .

discussion
scalp recordings were measured in  <dig> healthy adult participants to investigate the time-course of brain activity associated with the presentation of  <dig> categories of brief  sounds - voices, bird songs and environmental sounds - while participants performed a pure tone detection task. we observed significantly larger erp amplitudes for voices compared to other sound categories at fronto-temporal  and occipital  electrodes, emerging as early as  <dig> ms after stimulus onset and peaking around  <dig> ms , an electrophysiological response termed the "fronto-temporal positivity to voices" .

lack of voice sensitive response at electrode cz
results from electrode cz did not show a consistent preference for voice over other sound categories at any latency. the bootstrap results indicated that erps to voices at cz were never simultaneously larger than both bird songs and environmental sounds . on the contrary, bird songs elicited smaller amplitudes than both environmental sounds and voices on the n <dig>  and p <dig>  components. this effect is in line with  the difference in rts showing that targets following bird songs were processed slower than when following voices or environmental sounds;  subjects might have been less familiar with bird songs than with voices or environmental sounds, in line with recent findings showing an enhancement by familiarity of the n <dig> and the p <dig> components, which is predictive of the effects we observed, considering that subjects were more familiar to voices and environmental sounds  <cit> ;  acoustic analyses showing that bird songs were the most distinctive category .

in our study, the absence of the "vsr" reported by levy et al.  around  <dig> ms after sound onset, could be explained by differences in materials, or experimental design, or both. in order to recognise the target and perform the task as fast as they could, subjects had to maintain their attention on every stimulus that was presented to them. this is consistent with levy et al.,  who mentions that when participants attended to the stimulation sequence focusing on a feature other than timbre, the "vsr" was absent. in our experiment, because the target was a  <dig> hz pure tone, participants might have focused on pitch features, thus explaining the differences found for bird songs on the n <dig> and p <dig>  and the absence of a "vsr" in the latencies suggested by levy et al., . another potential aspect leading to the differences in the scalp localisation of the voice related effects we observed and the results reported by levy et al.,  is the choice of a common reference to the nose, whereas we opted for an off-line average reference  <cit> . finally, the difference in findings between the studies by levy et al.  and the present study may lie in their choice of a target tone  that belongs to the category of musical instruments, like all their stimuli except for the voices, whereas we were careful in the present study to choose a target  that clearly did not belong to any of the three compared stimulus categories.

early categorical differences
the earliest categorical difference was observed at the n <dig> latency , with smaller magnitude in response to bird songs at central and some occipital electrodes . these early categorical differences are comparable to latencies reported by murray et al., , who found a categorical difference between man-made and living auditory objects as early as  <dig> ms post sound onset.

we interpret the early birdsong erp difference as reflecting acoustical differences between sound categories: whereas acoustic energy was concentrated at low frequencies for both voices and environmental sounds , it peaked at much higher frequencies for birdsongs. this finding is consistent with the well-established sensitivity of the auditory n <dig> component to acoustical structure  <cit> . another early categorical difference was observed emerging  <dig> ms after sound onset on fronto-central electrode fc <dig>  with a smaller negative potential in response to voices . this difference on the n <dig> component at fronto-temporal electrode fc <dig> could also be related to acoustical differences: lower power was observed for voices at sound onset . this is consistent with findings that relate the acoustic energy at stimulus onset with erp amplitude on the n <dig> components  <cit> .

the ftpv: a rapid brain discrimination of sounds of voice emerging at  <dig> ms
the larger amplitude observed at fronto-temporal electrodes fc <dig> and fc <dig> in response to voice stimuli is consistent with our hypothesis of an early time-course for voice discrimination. as early as  <dig> ms post stimulus-onset, erps at electrodes fc <dig> and fc <dig> were consistently larger for voices than bird songs and environmental sounds. the same pattern was observed at similar latencies at occipital electrodes po <dig> and po <dig>  with reverse polarities. figure  <dig> shows an early erp to voice at electrode locations fc <dig>  fc <dig>  po <dig> and po <dig>  but this effect was also observed at the vast majority of occipital electrodes and at some frontal electrodes . at  <dig> ms, this electrophysiological response to voices reached nearly twice the amplitude of erps to other sounds, especially at fronto-temporal electrodes fc <dig> and fc <dig>  this effect does not relate to acoustical differences, which were observed at cz on the n <dig> and p <dig> components and at fc <dig> on the n <dig>  in addition, the erp voice response does not require subjects to make explicit discrimination among sound categories .

the latency of this electrophysiological marker of human voice processing is in keeping with previous studies addressing complementary questions. for example, beauchemin et al. , found eeg sensitivity to voice familiarity in the time-range of the auditory p <dig>  although they used different stimuli, different experimental design , and different eeg recording procedure  the latencies they report are consistent with the voice effects we observed in the latencies of the auditory p <dig>  therefore, neuronal activity in the time window of the auditory p <dig> seems to be sensitive to voice vs. non-voice differences, and also higher level cognitive processes such as voice familiarity, voice identity, and voice gender  <cit> .

voice-related brain mechanisms
this early ftpv probably corresponds to activity originating from the "what" part of the auditory stream  <cit> , and most likely in the temporal voice areas  <cit> . these brain regions have been reported to be very close  to core auditory regions  <cit>  and could potentially include areas that contain a large amount of voice-selective cells as it has been demonstrated for face selective neuronal patches in the macaque brain  <cit> , although this is very speculative and remains to be verified.

an auditory counterpart of the face-preferential n170?
the well established face-preferential n <dig> erp is characterised by larger amplitude in response to faces compared to other visual object categories from ~ <dig> ms to  <dig> ms and peaking at around  <dig> ms after stimulus onset  <cit> . the time course of the present ftpv shows some temporal coincidence with that of the n170: significant voice/non-voice amplitude differences emerged at  <dig> ms post onset and were well present at several electrodes at  <dig> ms. although onsets are delayed by about  <dig> ms, this time-course similarity between face-preferential and voice-preferential responses offers interesting avenues for future studies. because the same broad types of information -speech, identity, affect- are typically integrated across face and voice in social interactions, a parsimonious principle of organisation would be that unimodal preferential effects for faces and voices emerge at a comparable time-frame, well-suited for integrative mechanisms  <cit> . thus, we suggest that the ftpv could provide an auditory analogue of the well known n <dig>  <cit> .

the role of speech sounds
as illustrated on figure  <dig>  speech sounds contained in the voice category contributed strongly to the ftpv. however, it also appears that vocalisations  elicited a preferential response, although the pattern of activation was restricted to a few electrodes . both observations are consistent with previous fmri results that have shown i) a greater activity throughout the auditory cortex to speech sounds compared to their scrambled versions and ii) a greater activity to non-speech vocal sounds compared to their scrambled version restricted to the right anterior superior temporal gyrus/sulcus  <cit> . indeed, our results extend fmri ones, showing i) that the voice preferential response is not speech dependant; ii) that the preferential response for speech and non-speech stimuli has a similar time course; and iii) that the preferential response evoked for non-speech vocal stimuli compared to other sound categories is bilateral and more localised. further experiment are needed to test if the effects observed specifically for non-speech stimuli, here for electrodes po3/fc3-fc <dig>  po4/fc4-fc <dig>  oz-poz, correspond to activations of the anterior superior temporal gyri/sulci  <cit> .

limitations
although this study highlights for the first time an early electrophysiological response to voices, the degree of selectivity of the ftpv remains to be established. to demonstrate the robustness of the preferential electrophysiological responses in the face perception domain, several experiments were designed in order to account for the variety of visual objects  <cit>  and uncontrolled low-level differences  <cit> . future studies on voice categorisation should use a greater number of sound categories in order to better assess the robustness of this potentially selective response. because natural sound categories are necessarily characterised by acoustical differences that may contribute to erp differences, sound categories consisting of acoustical controls such as scrambled versions  <cit> , or sinusoidally amplitude-modulated noise  <cit>  could be used in order to rule out the contribution of factors such as amplitude modulation on the erp.

another way to better understand the early voice discrimination process would be to design an experiment with two stimulus categories  and at least two tasks  human vs. monkey discrimination and  expressions vs. no-expression discrimination) and a baseline condition, which would allow us to define whether the effect we report is specific, selective or preferential to voices, as described in  <cit> .

finally, an interesting possibility that remains to be tested is whether the ftpv is driven by increased attention to voices. as levy et al.  did in their study it would be interesting to manipulate attention in order to test its effect on the rapid brain discrimination of sounds of voice.

CONCLUSIONS
we searched for early erp markers of voice. our results provide the first evidence of an early electrophysiological response to sounds of human voices termed the "fronto-temporal positivity to voices" . this rapid brain response to voices appears in the latency range of the auditory p <dig>  which is comparable to the well-known face preferential n <dig> 

authors' contributions
ic, sfb, jpc, and pb designed the study. ic, sfb and jpc collected the data. ic conducted the analyses and wrote the manuscript. gar, crp, ml and iq helped analyse the data. crp, gar, ml and pb helped revise the manuscript. all authors read and approved the final manuscript.

