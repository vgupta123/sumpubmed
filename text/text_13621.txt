BACKGROUND
phylogeny inference allows, among others, detecting orthology/paralogy relationships among gene family members , estimating divergence times and evolutionary rates , reconstructing ancestral sequences , identifying molecular characters constrained by purifying selection or prone to positive selection , uncovering hidden biodiversity , and mapping the evolution of morphological, physiological, epidemiological, biogeographical and even behavioral characters  <cit> . molecular phylogeny inference is now a mature science, and an important part of the maturation process pertained to the realization  that the quest for the holy grail of absolute best tree should be abandoned for a much more meaningful goal: the inference of clades and trees robustness. still, that objective remained intractable in practice because of  the np-hard nature of optimality-criterion-based phylogeny inference  and  the computing-time requirements of using complex substitution models  in the framework of what had been identified as the probable most robust optimality criterion: maximum likelihood . today large phylogeny inference is incorporated, across biological disciplines, as an essential step in most comparative studies involving nucleotide or protein sequences. this has been made possible thanks to both theoretical and practical developments.

first, one key advance that made large phylogeny inference tractable is the implementation in this field of stochastic heuristics with inter-step optimization, i.e., a family of approaches that existed for decades in physics and computer science and explore multidimensional solution spaces in a much more efficient manner than the older intra-step optimization hill-climbing methods. indeed, in the latter, one prime parameter  is modified and all other parameters are optimized before the new solution is evaluated whereas, in stochastic heuristics, all free parameters are optimized while the search proceeds. inter-step optimization methods include mcmc approximations of the bayesian approach  <cit> , stochastic simulated annealing  <cit> , and genetic algorithms  <cit> . the efficiency of stochastic heuristics is quite counter-intuitive but can be explained by several factors:  poorer solutions are accepted with a non-null probability  such that valleys in likelihood space can eventually be crossed; , parameters are not over-optimized . we think that avoiding over-optimization at every topology evaluation generates a flatter likelihood space shape, such that valleys are more easily crossed and local optima more easily escaped. this suggestion however requires further investigation.

second, several stochastic methods have been incorporated into robust application softwares. the importance of that point should not be underestimated. for example, the recent success of bayesian methods is probably due as much to its incorporation into a robust and efficient software  as to the theoretical appeal of generating marginal posterior probabilities  <cit> . the software raxml  <cit> , enjoys well-deserved popularity because it is one of the fastest ml phylogeny inference programs available to date  thanks to the implementation of approximations to rate heterogeneity across sites and of smart computer science tricks speeding up likelihood computation: optimized parallel code and 'subtree equality vectors' . similarly, highly efficient parallel code has recently been implemented for the evaluation of phylogenies on graphics processing units , resulting in about 100-fold speed increase over an optimized cpu-based computation  <cit> . this efficient use of new hardware, existing stochastic heuristics , and smart code parallelization for efficient harnessing of the hundreds of gpu processing cores, allowed the authors to successfully use a 60-state codon model on a dataset of  <dig> complete mitochondrial genomes  <cit> .

the availability of multiple excellent softwares implementing different robust heuristics is clearly an asset for the end user: reliable results might be identified because they remain stable across softwares and methods. however, most users chose one single main software for their analyses, and this choice is sometimes dictated by availability of functionalities of importance but that do not pertain to the performances of the specific heuristic implemented . finally, given that the need to infer large trees is critical in multiple biological disciplines, the non-specialist can be baffled by the large number of available heuristics, parameters, and softwares, such that the most user-friendly tools are often preferred even if more robust or more efficient  softwares are available. there is therefore a challenge to supply softwares that are easy to use for the non-specialist, provide flexibility for the specialist, and allow fast and robust inference for both.

the metapopulation genetic algorithm  is an evolutionary computation heuristic in which several populations of trees exchange topological information which is used to guide the ga operators for much faster convergence. despite the fact that the metaga had been implemented in a simple and unoptimized software  together with simple nucleotide substitution models, an approximate rate heterogeneity method, and only a low number of functionalities, is has been shown as one of the most efficient heuristics under the ml criterion  <cit> . furthermore, it has been suggested that multiple metaga searches provide an estimate of the posterior probability distribution of possible trees  <cit>  although this proposition clearly warrants much further investigation. here, we present metapiga- <dig>  the first phase of a robust implementation of the metaga  together with complex substitution models, rate heterogeneity, and high parameterization for the phylogeneticist, as well as an ergonomic interface and easy-to-use functionalities for the non-specialist.

implementation and 
RESULTS
ml framework
trees are estimated in metapiga- <dig>  with the maximum likelihood criterion  using any of  <dig> nucleotide substitution models : jukes cantor , kimura's  <dig> parameters , hasegawa-kishino-yano  <dig> , tamura-nei  <dig> , and general time reversible . analyses can be performed with rate heterogeneity among sites using a proportion of invariant sites   <cit>  and/or a discrete gamma distribution of rates   <cit> . all parameters of the model  can be set by the user or estimated from a neighbor joining  tree  <cit> . the same parameters plus branch lengths and among-partition relative rates can experience intra-step optimization either periodically during the search and/or at the end of the search.

datasets can be partitioned into character sets  either using a graphical tool  or by writing the corresponding commands in a batch file. in metapiga- <dig> , we assume that all partitions evolve on the same topology , but all other parameters  are optimized separately for each partition. among-partition rate variation parameters are introduced in the likelihood equation as a factor that modifies branch length for the corresponding partition. branch lengths are optimized as usual, but the relative rates of partitions are optimized separately .

tools shared among heuristics
phylogeny estimation is an np-hard problem  <cit> , with unknown search space topography. metapiga- <dig>  implements four different heuristics for searching solution space . a set of tools is shared by all these heuristics: the starting tree generators, the operators, and some of the stopping rules.

a tree generator is used to produce the starting tree either as nj tree  <cit>  or as random tree  or as "loose neighbor joining"  tree, i.e., a pseudo-random topology . for generating a lnj tree, the user specifies a proportion value  and, at each step of the nj algorithm, the two nodes to cluster, instead of corresponding to the smallest distance value, are randomly chosen from a list containing the ntaxp <dig> smaller distances, where ntax is the number of taxa  in the dataset. branch lengths are computed as in the nj method  <cit> . in other words, the lnj tree is a nj tree with some topology randomization which amount is defined by the user. this approach is a particularly useful compromise between random starting topologies  that require long runs of the heuristic for optimization, and a good but fixed topology  that is prone to generate solutions around a local optimum. the distance matrix used for building nj or lnj starting trees can be computed using any of the  <dig> substitution models  and with or without pinv and/or γ-distr. arbitrary starting trees can also be imported by the user.

at the core of all stochastic heuristics are the operators, i.e., the topology and parameters' modifiers allowing the heuristic to explore solution space. in metapiga- <dig> , we implemented  <dig> operators for perturbing tree topology  and  <dig> operators for perturbing model parameters . these operators can be used in any combination, either at equal or user-defined frequencies. the user can choose for these frequencies to change dynamically during the search, i.e., metapiga periodically evaluates the relative gains in likelihood produced by each operator and adjusts their frequencies proportionally. minimum frequencies can be set such that operators that are inefficient early in the search remain available for increased use later in the search.

all stochastic heuristics require a stopping condition. in metapiga- <dig> , the user can choose any combination of the following criteria: number of steps, elapsed time, likelihood stability, and convergence of branch support distribution . when using the metaga heuristic, one can additionally use a stopping condition, within each replicate, based on convergence of the populations of solutions .

the heuristics
we implemented four heuristics in metapiga- <dig> : a simulated annealing algorithm , a classical genetic algorithm  and the metapopulation genetic algorithm based on the consensus pruning principle . as a reference, we also implemented a simple hill climbing  algorithm that generates a new solution tree at each step  and accepts it only if its likelihood is better than the current solution. hc algorithms are fast but tend to generate solutions trapped in local optima and are therefore highly dependent on the starting tree localization in tree space as well as on the  tree space topography.

the simulated annealing 
the sa algorithm uses statistical mechanics principles to solve combinatorial optimization problems  <cit> ; i.e., it mimics the process of minimal energy annealing in solids. sa starts with some initial state  and randomly perturbs that solution . if the new state is better , it is kept as the new current state; if the new state is worse , it is accepted as the current state with the boltzmann probability eΔe/t, where Δe is the negative difference in energy  between the two states, and t is the so-called 'temperature' of the system. if t is lowered slowly enough, the algorithm is guaranteed to find the optimal solution. the obvious asset of the algorithm is its ability to momentarily accept suboptimal solutions, allowing it to escape local optima whereas its obvious drawback is the difficulty to define the shape and speed of the "cooling schedule" . efficient schedules highly depend on the dataset. we implemented  <dig> highly-parametrized cooling schedules in metapiga- <dig> , including one specifically developed for phylogeny inference  <cit> . the user can control all cooling schedule parameters: the starting temperature computation method, the maximum acceptance probability, the temperature decrease frequency, and the possibility of 'reheating'.

the genetic algorithm 
the ga is an evolutionary computation approach that implements a set of operators mimicking processes of biological evolution such as mutation, recombination, selection, and reproduction . after an initial step of generating a population of trees, the individuals  within that population are  subjected to mutation  and/or recombination, and  allowed to reproduce with a probability that is a function of their relative fitness value . because selection preferentially retains changes that improve the likelihood, the mean score of the population improves across generations. however, because sub-optimal solutions can survive in the population , the ga allows, in principle, escaping local optima. in metapiga- <dig> , we implemented  <dig> alternative selection schemes  and one recombination scheme where each sub-optimal individual has a probability  to recombine with a better individual. recombination is performed by exchanging subtrees defined by one  of the identical taxa partitions in the two parental trees . a recombination can be viewed as a large number of simultaneous topological mutations. beside the selection scheme, the major parameter in the ga is the population size .

the metapopulation genetic algorithm 
this approach relies on the coexistence of p interacting populations  <cit>  of i individuals each : the populations are not fully independent as they cooperate in the search for optimal solutions. within each population, a classical ga is performed: trees are subjected to evaluation, selection , and mutation events. however, all topological operators are guided through inter-population comparisons defined and controlled by 'consensus pruning' : topological consensus among trees across populations defines the probability with which different portions of each tree are subjected to topological mutations. these comparisons allow the dynamic differentiation between internal branches that are likely correct  and those that are likely incorrect . although cp allows for the elaboration of many alternative inter-population communication procedures  <cit> , we implemented in metapiga- <dig>  the two that we identified  as the most useful: 'strict cp'  and 'stochastic cp' .

as constraining entirely an internal branch from being affected by topological mutations necessarily increases the likelihood to be trapped in a local optimum, a tolerance parameter t  is implemented, allowing any internal branch to be affected with a probability t even if the branch is shared by all trees. the user of metapiga- <dig>  has the choice between a 'blind' and a 'supervised' procedure for handling constrained partitions. in the former, a topological mutation that affects a constrained branch is simply aborted and the tree is left unchanged, whereas in the latter, topological operators exclusively target branches in a pool of acceptable  candidates.

the metaga allows for two, non-mutually exclusive, recombination flavors: 'intra-population recombination' where each sub-optimal individual at each generation has a probability  to recombine with a better individual from that population , and 'inter-population hybridization'  where, at each generation, there is a probability  that all sub-optimal individuals from one random population are recombined with one individual from another population; sub-optimal individuals from other populations experience the normal mutation procedure.

comparing, across generations, the frequencies of internal branches shared among the p*i trees  provides a means for assessing whether the populations converge towards a stable set of solutions, i.e., towards a consensus with stable branch frequencies. hence, a stopping rule, not available to other heuristics, can be used under cp: the user can choose to stop the search when a series of mean relative error  values remains, across generations, below a threshold defined by the user. to increase independence among samples, consensus trees are sampled every n >  <dig>  generations. for example, given two consensus tree, ti and tj, corresponding to the consensuses among the p*i trees at generations  <dig> and  <dig>  respectively, the mre is computed as follows:

mre=∑p=1npartition|Φtip−Φtjpmax|npartition, where npartition is the sum of taxa bi-partitions observed in ti and tj , and Φtip and Φtjp are the consensus values of bi-partition p in trees ti and tj, respectively. note that |Φtip−Φtjpmax|= <dig> if either both Φtip and Φtjp are nil, or if the corresponding internal branch does not exist in either ti or tj. internal branches that are absent from both ti and tj are not considered. if the mre is above the user-defined threshold , it is discarded and a new mre is computed for the comparison of generations  <dig> and  <dig>  on the other hand, if mre is below the threshold, a counter is incremented and a new mre is computed for the comparison of generations  <dig> with the next sample . the user defines for how many samples the mre must remain below the specified threshold before the search stops.

replicates
for any heuristic chosen by the user in metapiga- <dig> , the search can be repeated many times, generating a majority-rule consensus tree among the replicates. note that when the metaga is the selected heuristic, it has been suggested that the frequencies of clades in the among-replicates consensus might approximate the corresponding posterior probabilities  <cit> . the user can either fix the number of replicates, or specify a range of minimum and maximum number of replicates and let metapiga- <dig>  stop automatically, exploiting the mre metric in a similar way as the consensus across populations in a single metaga search . here, however, the mre is computed using consensuses across replicates, i.e., ti is the consensus among the final trees that have been obtained in replicates  <dig> to i. no additional replicate is produced when the mre among n replicates  remains below a given threshold. as an example, if n is set to  <dig>  and the first mre below the user-defined threshold  involves replicates 1- <dig> and 1- <dig>  the mre is computed  <dig> additional times, i.e., between the reference consensus t1- <dig> and tj, for j corresponding to replicates 1- <dig>  then 1- <dig>  then 1- <dig>  etc. the search stops if the inter-replicates mre remains below 5% for  <dig> consecutive replicates. on the other hand, the counter is reset to zero as soon as the mre exceeds 5%, and the new reference tree for computing mre is then set to t1-current replicate.

the inter-generations  mre stopping rule can be used in combination with the inter-replicate mre stopping rule, letting metapiga decide both when to stop each replicate and when to stop executing additional replicates .

language, formats, and interface
metapiga- <dig>  is written in java  <dig>  such that the single code runs on  <dig> and 64-bits platforms under macos x, linux, and windows. computing and storing the likelihood of large trees requires large amount of random-access memory . whereas 32-bits systems can allocate a maximum of ~2gb of memory to the java virtual machine , 64-bits systems are virtually limited only by the amount of memory installed on the computer . metapiga- <dig>  uses the java multi-threading technology to take advantage of multiprocessor and multicore computers, such that some tasks can be run in parallel. as replicates are independent, they are particularly prone to parallelization: any number of different cores/processors can be assigned to different replicates. in addition, the metaga heuristic itself is well suited to parallel implementation because processes such as mutation, selection, and likelihood computation are unrelated to cp and are therefore independent across populations. hence, different metaga populations can be distributed to different cores/processors. parallelization of metaga populations can be combined with parallelization of replicates .

metapiga- <dig>  uses standard formats: reading and writing datasets in nexus format  <cit>  and trees in newick format http://evolution.genetics.washington.edu/phylip/newicktree.html. all search settings can be saved in a metapiga block incorporated into the nexus file, allowing easy management and command line runs. a nexus file without a metapiga block will be correctly interpreted by metapiga- <dig>  and will run with default parameters.

metapiga- <dig>  can be run in command line but it also offers an extensive graphical user interface  for access to all search settings: defining and managing charsets; including/excluding taxa, characters, and charsets; managing dataset partitions; choosing and parametrizing heuristics ; defining substitution models and their parameters ; choosing starting tree options; controlling operators ; defining stop criteria and replicates. all settings are associated with an interactive "mouse-over" help system. metapiga- <dig>  also implements three statistical methods  for selecting the substitution model that best fits the data : the likelihood ratio test, the akaike information criterion, and the bayesian information criterion. the metapiga- <dig>  gui provides a detailed run window showing graphs specific to the chosen heuristic .

batch files are particularly useful for running sequentially a single data set under multiple different settings and/or several datasets with the same settings. metapiga- <dig>  supports the use of batch files which can be written manually or generated using tools available in the gui: datasets and their settings can be duplicated, settings can be copy-pasted from one dataset to another, and multiple combinations of datasets and settings can be saved in a batch file that can be run either in the gui  or using command line.

input and result trees are manipulated in newick format but visualized graphically in the gui and can be exported for other programs. metapiga- <dig>  also integrates a tree viewer  that allows viewing, rerooting, and printing trees as well as computing the likelihood of any tree  and optimizing its model parameters. three other tools are implemented in metapiga- <dig> : a tree generator , a consensus builder , and a memory setting tool defining the maximum amount of memory allocated to the program.

discussion and 
CONCLUSIONS
the metaga resolves the major problem inherent to classical ga approaches: should one use a soft or a stringent selection scheme? indeed, strong selection produces good solutions in a short computing time but tends to generate sub-optimal solutions around local optima, whereas mild selection schemes considerably improve the probability to escape local optima and find better solutions but greatly increase computing time. as the metaga involves several parallel searches, initial inter-population variation can be very high , and somewhat maintained during the search, even under extreme intra-population selection.

although the metaga has been shown to perform very well  <cit> , it had not been implemented together with complex substitution models, discrete gamma rate heterogeneity, and the possibility to partition data. here, we performed such an implementation together with a hill climbing, a classical ga, and a sa algorithm. this implementation into a single software will allow a rigorous identification of the optimal parameters' values under each of these heuristics as well as a meaningful comparison of performances among these algorithms. comparing the performances  between metapiga- <dig>  and other popular softwares such as, among others, mrbayes  <cit> , raxml  <cit> , garli  <cit> , and phyml  <cit>  is well beyond the scope of the present manuscript. however, our preliminary analyses  with large datasets indicate that metapiga- <dig>  and mrbayes- <dig> . <dig> generate very similar candidate trees and consensus cladograms  and require similar running times.

an in-depth assessment of the statistical significance of metaga branch support values is warranted. there might be some correspondence between metaga branch support values and posterior probabilities  <cit>  but theoretical and additional empirical analyses are required. for example, it would be important to asses how changing metaga  settings would affect sampling and estimates of probability distributions.

metapiga- <dig>  will constitute a platform on which we will incorporate additional functionalities , improve performances , identify optimal combinations of default parameters values, improve current heuristics, and possibly combine them for the development of higher-level metaheuristics. meanwhile, metapiga- <dig>  already gives access both to high customization for the phylogeneticist, as well as to an ergonomic interface and functionalities assisting the non-specialist for sound inference of large phylogenetic trees using nucleotide sequences.

availability and requirements
project name: metapiga- <dig> 

project home page: http://www.metapiga.org and http://www.lanevol.org

operating systems: platform independent

programming language: java

other requirements: java  <dig>  virtual machine

license: free for academics, license needed for non-academics.

authors' contributions
mcm conceived and designed the software, and wrote the manuscript. rh improved the design of many functionalities and wrote the code. both authors read and approved the final manuscript.

authors' information
mcm heads the laboratory of artificial & natural evolution  at the university of geneva , and works on various aspects of evo-devo, phylogenomics, phyloinformatics, experimental evolution, and conservation genetics. rh is a computer scientist in the department of biology of namur university  and specializes in the development of bioinformatics tools.

