BACKGROUND
with the rapid developments in genomics, high-throughput microarray pattern analysis shows a great potential in cancer diagnosis for its efficiency and cost-effectiveness  <cit> . however, such a promising technology remains an important research field rather than an applicable clinical-routine. aside intrinsic factors from microarray profiling technologies, a key issue preventing it from becoming a clinical paradigm is that the relatively low even poor sensitivities and specificities obtained from current pattern recognition methodologies are inadequate to provide a robust clinical support. moreover, some pattern classification methods may perform reasonably well in some data sets but fail badly in others. although there is an urgent need in clinical cancer research to develop high-performance pattern recognition methods in gene expression analysis, it is still a challenge in machine learning to attain high-accuracy classification for the special characteristics of gene expression profiles.

a gene expression profile can be represented by a p×n matrix after preprocessing, each column of which represents gene expression values of all biological samples at a gene; each row of which represents gene expression values of a single biological sample across a genome. the total number of genes is in the order of 103~ <dig>  and the total number of biological samples is on the magnitude of tens or hundreds. since the number of variables  is much greater than the number of samples , some traditional pattern recognition methods  may have instable solutions and lead to a low or poor classification performance. alternatively, although there are a large number of genes in a profile, only a small portion of them have meaningful contributions to data variations. in addition, the high-dimensional data are not noise-free because preprocessing algorithms may not remove systematic noise contained in raw data completely. obviously, the data redundancy and noise may inevitably affect the discriminative power of the classification algorithms applied to microarray data.

it is clear that feature selection play a critical role in gene expression analysis to decrease dimensionalities, remove noise, and extract meaningful features before performing classification. feature selection algorithms usually can be categorized into three types: statistical test-based , wrapper-based   <cit> , and transform-based feature selections. the transform-based feature selection may be mostly used data reduction techniques for their popularity and efficiency. they include principal component analysis   <cit> , independent component analysis   <cit> , nonnegative matrix factorization   <cit> , etc, and their different extensions  <cit> .

however, these transform-based feature selection algorithms are generally good at selecting global features instead of local features. the global and local features contribute to the global and local characteristics of data and interpret global and local behavior of data respectively. statistically, the global features consist of high-frequency signals and the local features consist of low-frequency signals. unlike the global features, the local features are difficult to extract for most feature-selection algorithms, because the low-frequency signals have a lower likelihood to get involved in inferring the ‘new’ low-dimensional data, which are generally the linear combinations of all input variables, than the high-frequency signals. finally, the low dimensional data obtained from the traditional feature selection methods may miss some local data characteristics described by the local features. for example, pca is by-nature a global feature selection algorithm: each principal component contains some levels of global characteristics of data and receives contributions from all input variables in the linear combinations. in addition, changes in one variable will inevitably affect all loading vectors globally. however, local features may be a key to attaining high-performance gene expression pattern classification for its subtle data behavior capturing. for example, some benign tumor samples may display very similar global characteristics with malignant tumor samples but with different local characteristics. to attain high-performance diagnosis, it is essential to capture local data characteristics to distinguish these samples with similar global characteristics.

the main reason for these algorithms’ global-feature selection mechanism is because they all are single-resolution feature selection methods, where all features are indistinguishably displayed in a single-resolution despite the nature of their frequencies. it inevitably causes global features more likely to be selected than local features and prevents effective local data-characteristics capturing. mathematically, all variables of the input data are involved in the linear combinations to compute principal components in pca, independent components in ica, and basis vectors in nmf respectively. such a global feature selection mechanism will prevent high-accuracy genomic pattern recognition in the following classification because only the features interpreting global characteristics are involved in training a learning machine . the redundant global features may inevitably decrease the generalization of the learning machine and increase the risk of misclassifications or over-fitting. finally, the learning machines integrated with the global feature-selection algorithms will display instabilities in classifications.

to avoid the global feature selection mechanism, it is desirable to distinguish  features according to their frequencies rather than treat them uniformly, which makes the high-frequency signals dominate the feature selection and the low-frequency signals lose opportunities. a discrete wavelet transform   <cit>  can hierarchically organize data in a multi-resolution way by low and high pass filters. the low -pass filters only pass low -frequency signals but attenuate signals with frequencies higher  than a cutoff frequency. finally, the dwt coefficients at the coarse levels capture global features of the input signals and the coefficients at the fine levels capture local features of the signals, i.e., the low-frequency and high-frequency signals are represented by coefficients in the coarse and fine resolutions respectively. obviously, the global feature selection mechanism can be relatively easy to overcome after such a ‘multi-resolution feature separation’, by selectively extracting local features and filtering redundant global features.

in this study, we propose a novel multi-resolution independent component analysis  to conduct effective feature selections for high dimensional heterogeneous gene expression data. then, a multi-resolution independent component analysis based support vector machines  are proposed to achieve a high-performance gene expression pattern prediction. we demonstrate its superiority and stability by comparing it with existing state-of-the-art peers on six heterogeneous microarray profiles, in addition to extending mica to linear discriminant analysis . we also develop a mica-based filter-wrapper biomarker discovery algorithm to further demonstrate the novel feature selection algorithm’s effectiveness in biomarker capturing. finally, we discuss potential extensions on the multi-resolution independent component analysis in microarray based molecular diagnosis and conclude this paper.

methods
multi-resolution independent component analysis is based on the discrete wavelet transform  and independent component analysis . a discrete wavelet transform decomposes input data in a multi-resolution form by using a wavelet and scaling function. the coefficients at the coarse and fine levels describe the global and local behavior of data respectively. mathematically, dwt is equivalent to multiplying the input data by a set of orthogonal matrices block-wisely. on the other hand, ica seeks to represent input data as the linear combination of a set of statistically independent components by minimizing their mutual information. theoretically, it is equivalent to inverting the central limit theorem  by searching maximally non-normal projections of the original data distribution. more information about the dwt and ica methods can be found in  <cit> .

multi-resolution independent component analysis
the goal of the multi-resolution independent component analysis is to seek the statistically independent genomic patterns from a meta-profile computed by suppressing the coarse level coefficients  and maintaining the fine level coefficients  in the dwt of an input profile. as an approximation of the high dimensional input profile, the derived meta-profile captures almost all local features and keeps the most important global features. unlike independent components in the classic ica that are mainly retrieved from the global features for their high frequencies, the independent components calculated by our proposed mica method are statistically independent signals, which contain contributions from almost all local features and the most important global features. as such, the latter is more representative in revealing the latent data structure than the former. moreover, the redundant global feature suppressing brings mica an automatic de-noising mechanism: since the coarse level coefficients  in dwt generally contain “contributions” from noise, suppressing coarse level coefficients not only filters unnecessary global features but also removes the noise. the mica algorithm can be described as following steps.

1). wavelet transforms
given a gene expression profile with p samples across n genes , mica conducts a l-level discrete wavelet transform for each sample to obtain a sequence of detail coefficient matrices  and an approximation coefficient matrix , i.e., , where .

2). feature selection
a level threshold  is selected to suppress redundant global features and maintain local features as follows. if , 1) conduct principal component analysis for dj to obtain its pc matrix:  and the corresponding score matrix  . 2) reconstruct the original dj by using the first loading vector u <dig> in the pc matrix as , where  is a vector containing all ‘1’s. if , reconstruct and update each detail coefficient matrix dj by using the loading vectors  with the 100% explained variance percentage and their corresponding vectors in the score matrix: . the explained variance percentage is the ratio between the accumulative variance from the selected data and the total data variance. for example, the explained variance percentage ρr from those first r loading vectors is defined as , where λi is the data variance from the ith loading vector. in the implementation, this step can be ‘lazily’ simplified as: keep all detail coefficient matrices  intact to save computing resources.

3). inverse discrete wavelet transforms
conduct the corresponding inverse discrete wavelet transform using the updated coefficient matrices  to get the meta-profile of x:, i.e., .

4). independent component analysis
conduct the classic independent component analysis for x* to obtain independent components and the mixing matrix: , where , and .

5). subspace decomposition
the meta-profile x* is the approximation of the original profile x by removing the redundant global features and retaining almost all local features by selecting features on behalf of their frequencies. it is easy to decompose each sample in the subspace spanned by all independent components . each independent component is a basis in the subspace., i.e., , where the mixing matrix is , and the independent component matrix is . in other words, each sample can be represented as , where the meta-sample ai is the ith row of the mixing matrix recording the coordinate values of the sample xi in the subspace. as a low dimensional vector, the meta-sample ai retains almost all local features and the most outstanding global features of the original high-dimensional sample xi. thus it can be called as a data-locality preserved prototype of xi.

multi-resolution independent component analysis based support vector machines 
the mica-based support vector machines apply the classic support vector machines   <cit>  to the meta-samples to gain classification information in a low-dimensional space. unlike the traditional svm that builds a maximum margin hyper-plane in the original data space ℝn where n~103- <dig>  mica-svm separates biological samples by constructing the maximum margin hyperplane in the spanned subspace  where k ≤ p ~  <dig>  using the meta-samples. if we assume the number of support vectors ns is much less than the training points l, the time complexity of the mica-svm is , which is much lower than that of the classic svm , provided the same number of training points and support vectors. we briefly describe the mica-svm classifier for binary classification. given a training dataset , and sample class type information , where , a meta-dataset  is computed by using the multi-resolution independent component analysis. then, a maximum margin hyper-plane:  is constructed to separate the '+1'  and '-1'  types of meta-samples. it is equivalent to solving the following quadratic programming problem,   

a way to solve  is through its lagrangian dual that is also a quadratic programming problem, where  are dual variables of the primal variables w and b.   

the normal of the maximum-margin hyperplane is calculated as . the decision rule  is used to determine the class type of a testing sample x′, where  are the corresponding meta-samples of samples  , computed from mica respectively. the function  is a svm kernel function that maps these meta-samples into a same-dimensional or high-dimensional feature space. in this work, we only focus on the linear kernel for its simplicity and efficiency in microarray pattern classifications. we will point out in the discussion section that most svm-based learning machines would encounter overfitting under the standard gaussian kernel .

RESULTS
we have performed extensive experiments using six publicly available gene expression microarray profiles consisting of five oligonucleotide profiles  <cit>  and one cdna profile  <cit> , in the experiment. table  <dig> includes their detailed information. these profiles are heterogeneous data generated from different experimental conditions, different profiling technologies, or even processed by different preprocessing algorithms. for example, the stroma, prostate, glioma, and hcc data only go through basic log <dig> transforms while the breast_ <dig> data is a dataset obtained by conducting two-sample t-tests from an original dataset going through delicate normalizations  <cit> .

cross validations
to address our algorithm’s superiority and reproducibility, we compare it with six comparison algorithms in terms of average classification rates, sensitivities, and specificities under the k-fold  and 100-trial of 50% holdout cross validations. the classification accuracy in the ith classification is the ratio of the correctly classified testing samples over total testing samples: , and the sensitivity and specificity are defined as the ratios:  respectively, where tp  is the number of positive  targets correctly classified, and fp  is the number of negative  targets incorrectly classified respectively. in the 100-trial of 50% holdout cross validation , all samples in the data set are pooled together and randomly divided into half to get training and testing data. such a partition is repeated  <dig> times to get  <dig> sets of training and testing datasets. in the k-fold cross validation, an input dataset is partitioned into k disjoint equal or approximately equal proportions. one proportion is used for testing and the other k- <dig> proportions are used for training alternatively in the total k rounds of classifications. compared with pre-specified training or testing data, the cross validations can decrease potential biases in algorithm performance evaluations.

six comparison algorithms
the existing six comparison algorithms can be categorized into two types. the first type consists of standard support vector machines   <cit>  and linear discriminant analysis   <cit> , both of which are state-of-the-art classification algorithms. especially, svm is widely employed in gene expression pattern recognition for its popularity. the second type consists of four methods embedding transform-based feature selections in svm and lda: they are support vector machines with principal component analysis/independent component analysis/ nonnegative matrix factorization, and linear discriminant analysis with principal component analysis. we refer them as pca-svm, ica-svm, nmf-svm, and pca-lda conveniently and their related implementation information can be found in additional file  <dig> 

we employ the wavelet ‘db8’ to conduct a 12-level discrete wavelet transform for each data set, and select a level threshold τ =  <dig> in mica for all profiles. although not an optimal level threshold for all data, it guarantees automatic de-noising and ‘fair’ algorithm comparisons. moreover, we have found that the meta-samples obtained from mica at τ =  <dig> can clearly distinguish two types of samples. although other level threshold selections may be possible, any too ‘coarse’  or too ‘fine’  level threshold selection may miss some important global or local features and affect following classifications.

multi-resolution independent component analysis based linear discriminant analysis
we also apply mica to linear discriminant analysis  to further explore its effectiveness. similar to the mica-svm algorithm, the mica-based linear discriminant analysis  applies the classic lda to the meta-samples obtained from mica to gain sample classifications . the mica-lda algorithm’s performance on the six profiles can be found in the additional file  <dig>  to keep consistency with the previous experiments, we still employ the ‘db8’ wavelet and set the level threshold τ =  <dig> in mica. interestingly, the mica-lda classifier is only secondary to the mica-svm classifier: it outperforms the other comparison algorithms on the five datasets except the prostate data in terms of the average performance under the  <dig> trials of hocv and 10-fold cv. this further indicates that mica’s effective feature selection and its contribution to subsequent classification methods. figure  <dig> compares the distribution of classification rates from the three lda-based algorithms: mica-lda, pca-lda, and lda on four data sets under the  <dig> trials of 50% hocv. interestingly, mica-lda obviously outperforms pca-lda and lda by its right-skewed classification rate distributions. although pca-lda also demonstrates classification advantages over lda, mica-lda has attained much more impressive improvements than pca-lda. on the other hand, this also indicates that the multi-resolution independent component analysis is more effective in the feature selection than principal component analysis, which contributes directly to improving lda classifier’s performance.

optimal level threshold selections
a remaining question is how to determine the optimal level threshold in mica so that the following svm classifier achieves best performance. we employ the condition number  of the independent component matrix z in mica to resolve it, where smax and smin are the maximum and minimum singular values of the matrix z calculated from mica. a smaller condition number indicates a more stable matrix that suggests a better status in global and local feature capturing. the level-threshold is counted ‘optimal’ if the condition number δ is the smallest. if the condition numbers from two level thresholds are same numerically, the lower level threshold  is counted as the optimal one. for example, the smallest δ value is achieved at τ =  <dig> and τ =  <dig> , <dig> , <dig> respectively on the hcc data. we choose τ =  <dig> as the optimal threshold which is corresponding to the best average the average classification rate:  <dig> %  with average sensitivity:  <dig> %  and specificity are  <dig> %  respectively.

although only wavelet ‘db8’ is employed in our experiments, there is no other specific requirement in mica-svm for a wavelet except it should be orthogonal. to compare effects of different wavelet selections on the algorithm performance, we select four family wavelets: ‘db8’, ‘sym8’, ‘coif4’, and ‘bior <dig> ’, in the classifications on the six profiles at the level threshold τ =  <dig>  it seems that there is no obvious classification advantage from one wavelet over the other under the 10-fold cv, because the robust prior knowledge and less number of trials may have larger impact factors on the algorithm performance than a wavelet selection. however, we have found that the wavelet ‘db8’ show some advantages over the others under the  <dig> trials of 50% hocv. in addition, it is interesting to see that the wavelets ‘coif4’ and ‘sym8’ have almost same-level performance, but the wavelet ‘bior <dig> ’ has a relatively low performance for the six profiles.

we further demonstrate the superiority of mica-svm by comparing it with three state-of-the-art partial least square  based regression methods, which can be found in the additional file  <dig>  moreover, we present a novel algorithm stability analysis for the seven classifications and show the advantages of the mica-svm and mica-lda algorithms over the others .

mica-based biomarker discovery
in addition to classifying large scale heterogeneous tumor profiles with exceptional performance, multi-resolution independent component analysis can be also applied to capture biomarkers for microarray profiles. we present a mica-based filter-wrapper biomarker capturing algorithm and apply it to the stroma data. the details of this algorithm can be found in the additional file  <dig>  table  <dig> lists the details on all the three biomarkers captured, where the svm-rate for each biomarker is the classification ratio achieved by a svm classifier with the ‘rbf’ kernel on the biomarker under leave-one-out cross validations. the order of the three biomarkers in table  <dig> is listed according to its order identified in the biomarker discovery process. the svm accuracy under the three biomarkers is  <dig> % and the corresponding sensitivity and specificity are  <dig> % and 100% respectively. the first biomarker is gene usp <dig>  which is a broadly expressed gene reported as one gene associated with breast cancer and glioblastomas  <cit> . the second biomarker is fosl <dig>  which is one of four members in the fos gene family. it is responsible for encoding leucine zipper proteins, which is able to dimerize with proteins of the jun family, and form the transcription factor complex ap- <dig>  as a regulator in cell proliferation, differentiation, and transformation, recent studies  <cit>  have showed that it is one of important genes associated with breast cancer, by being involved in the regulation of breast cancer invasion and metastasis. the third biomarker is gene rpl <dig>  which encodes a ribosomal protein that catalyzes protein synthesis. it was reported to associate with biosynthesis and energy utilization that is a cellular function associated with pathogenesis of breast cancer  <cit> . in addition, it also links to the breast cancer by lowering mdm <dig>  which is a major regulator of p <dig> levels, preventing p <dig> ubiquitination and increasing its transcriptional activity  <cit> . figure  <dig> visualizes the  <dig> samples  and  <dig> non-inflammatory breast cancers ) of the stroma data using the three biomarkers. it is interesting to see that two types of cancers are separated into two spatially disjoint sets clearly, though one ‘ibc’ sample is wired in the ‘non-ibc’ samples.

discussion
it is worthy to note that independent component analysis is a necessary step to achieve a good classification performance. a similar multi-resolution principal component analysis based svm algorithm is not able to reach comparable performance as our algorithm because of the loss of statistical independence in the feature selection. also, mica-svm encounters overfitting as svm, pca-svm, ica-svm classifiers under the standard gaussian kernel , where each learning machine can only recognize the majority type samples of the training data in classification despite the testing sample type. moreover, we have tried kernel ica  <cit>  based support vector machines  in our experiments in addition to the previous nine comparison algorithms. however, the kica-svm classifier generally has a lower performance level than the standard svm classifier. furthermore, the kica-svm not only shows a strong instability in classification but also inevitably encounters overfitting under the standard gaussian kernel like the other learning machines. it seems to suggest that kernel based data reduction may not be a desirable approach in effective feature selection for high dimensional heterogeneous gene profiles. similar results can be also found in kernel pca  <cit>  based support vector machine  classifications: a kpca-svm classifier is essentially the pca-svm classifier when its two kernels are selected as ‘linear’, otherwise, it encounters overfitting under the standard gaussian kernel. in our ongoing project, in addition to further polishing our algorithm by comparing them with other state-of-the-art methods , we are interested in theoretically validating the mica-svm‘s advantages over the classic svm classifier from the viewpoint of vapnik–chervonenkis  dimension theory  <cit> .

CONCLUSIONS
in this study, we present a novel multi-resolution feature selection algorithm: multi-resolution independent component analysis for effective feature selection for high-dimensional heterogeneous gene expression profiles, propose a high-performance mica-svm classification algorithm, and demonstrate its superiority and stability by comparing it with the nine state-of-the-art algorithms. our algorithm not only consistently demonstrates the high-accuracy or clinical-level cancer diagnosis by treating an input profile a whole biomarker but also shows effectiveness in meaningful biomarker discovery. it suggests a great potential to facilitate high-throughput microarray technology into a clinical routine, especially, current classification methods have relative low even poor performance on the gene expression data. in addition, the multi-resolution data analysis based redundant global feature suppressing and effective local feature extraction will have a positive impact on large scale ‘omics’ data mining. in our future work, we plan to further explore mica-svm’s potential in other platform gene expression data, snp, and protein expression data classification.

competing interests
the authors declare that they have no competing interests.

authors' contributions
hey collects and processes the data, designs algorithms, implements the methods, and drafts paper. lxl participates in discussion and provides help to polish the paper. hey and lxl jointly finalize the paper.

supplementary material
additional file 1
implementations of the four comparison algorithms pca-lda, pca-svm, ica-svm, and nmf-svm algorithm implementations

click here for file

 additional file 2
mica-lda performance mica-lda performance under  <dig> trials of 50% hocv and 10-fold cv

click here for file

 additional file 3
comparing mica-svm with pls-based regression methods comparisons mica-svm with three partial least square  based regression methods

click here for file

 additional file 4
algorithmic stability analysis

click here for file

 additional file 5
mica-based biomarker discovery algorithms

click here for file

 acknowledgements
the authors want to thank three anonymous reviewers for their valuable comments in improving this manuscript.

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2011: selected articles from the ninth asia pacific bioinformatics conference . the full contents of the supplement are available online at http://www.biomedcentral.com/1471-2105/12?issue=s <dig> 
