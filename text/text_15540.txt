BACKGROUND
representing high dimensional data in a low dimensional space is an important task because it becomes much easier to study the information structure when the dimension is greatly reduced. the main idea of mds techniques is to configure the coordinates of the data in the significant space such that the pairwise relationship of relocated data in a low dimensional space is similar to that in the high dimensional space of the original data. with the dimensional reduction, one can cluster the data relationships by their distribution in the low dimensional space and explore significant patterns. when the data configuration is euclidean, mds is similar to principle component analysis , which can remove inherent noise with its compact representation of data  <cit> . when the data configuration is nonlinear, mds can be further improved to capture the imbedded manifold in data  <cit> .

mds techniques have been applied to many fields, e.g., pattern recognition, stock market analysis, and molecular conformational analysis. however, the computational complexity of most metric mdss is over o, though some non-metric methods can reduce the complexity to o <cit> . genomics research represents a challenging application of mds. data from microarray experiments are typically noisy with a large number of genes, but few replicates and frequent data updates. due to the high computational complexity, it is very difficult to apply mds to whole genome data, such as ~ <dig> genes in yeast, not to mention ~ <dig>  genes in human.

taguchi and oono  <cit>  developed a novel algorithm for non-metric mds analysis and applied it to analyze patterns of gene expression. however, the result of a non-metric mds method depends heavily on the initial configuration and non-metric mds only preserves the order of similarities instead of the original scale of similarities. therefore, it remains an important issue to reduce the computational complexity for a metric mds. in this paper, we develop a fast metric mds method for large data sets that is suitable for data analysis and updates. indeed, the computational time for  <dig> target data points is within  <dig> seconds in a pc with cpu  <dig>  ghz and 2g memory.

we review typical mds techniques in the following and propose a new mds method to solve the problem of large data sets in section  <dig>  we split the data into overlapping subsets, apply our mds technique to each subset, and then recombine the configurations of the subsets into the same space. we call this method the split-and-combine mds  method. the complexity of sc-mds is o, where p is the dimension of the feature space, which is far smaller than the number of data points n. in section  <dig>  we evaluate the performance of sc-mds using simulation, apply sc-mds to the go database, and lastly, improve the k-means clustering of gene expression profiles by applying sc-mds to yeast cell cycle microarray data  <cit> .

there are many different categories of mds techniques. for example, a distinction can be made between metric and non-metric mdss, between weighted and unweighted mdss, between single matrix and multiple matrices, and between deterministic and probabilistic matrices  <cit> . in this section, we introduce three typical mds methods that are relevant to the present work.

classical mds 
torgerson  <cit>  proposed the first mds method. the distance is the euclidian distance, and the similarity matrix is complete  and symmetric. the main idea was that given the euclidean distances or the inner products among points, it is possible to construct a matrix x of cartesian coordinates of these points in the euclidean space. torgerson derived matrix x from the distance  matrix d and showed what to do when the distance matrix includes noisy data. the key is to apply the double centering operator and singular value decomposition .

double centering is the process of subtracting the row and column means of a matrix from its elements and adding the grand mean. for example, suppose that d is a product matrix, that is, d = xt x, where x is a p × n matrix with each column of x being a vector in a p -dimension space. we define i as a n ×  <dig> vector in which every element is one and b=t as the similarity matrix with the means of the column vectors being zero. then we have

  b=t=xtx−1nxtx1˙1˙t−1n1˙1˙txtx+1n21˙1˙txtx1˙1˙t=d−1nd1˙1˙t−1n1˙1˙td+1n21˙1˙td1˙1˙t=d−d¯r−d¯c+d¯g, 

where d¯r denotes the row means, d¯c denotes the column means and d¯g denotes the grand mean. if h=1−1n1˙1˙t,  can be simplified as

  b = hdh. 

after performing double centering on d, one can apply svd to b. since b is symmetric, the decomposition is of the form

  b = uvut. 

hence, b=x−1nx11t=uv <dig>  where the columns of b are the coordinates of data, with the mean of the data being moved to the original point.

when d is a distance matrix, di,j=t, where xi is the configuration of the i-th data point. the double centering of d <dig> is equal to -2b, provided that ∑i=1nxi= <dig>  hence, the cmds method performs double centering on d <dig>  multiplies by − <dig>  and then performs svd, which gives the configurations of the data. thus, the key scheme of pca is embedded in this cmds method. when we want to find out the r dimensional configurations of the data in the space generated by the r principal components, we only use the first leading r spectrums and r columns of u to generate x, that is,

  x=vrur, 

where vr is the r × r sub-matrix of v and ur is a matrix of size n × r.

there are many drawbacks of this method  <cit> . for example, missing data is not allowed and the computational complexity is o. hence, this method is not suitable for massive data sets.

chalmer's linear iteration algorithm
one of the force-based models of mds is the spring model  <cit> . it considers each point of the data as a vertex in a low dimensional space, with springs connecting each vertex and the distance  between vertices proportional to their high dimensional distance. if di, j denotes the high dimensional distance between vertices i and j, and δi, j denotes the low dimensional distance, then the stress between vertices i and j is proportional to |di,j - δi, j|. the spring model computes  forces at each vertex per iteration, and the computational complexity of this model is o per iteration.

chalmers  <cit>  proposed a linear iteration time layout algorithm. instead of computing all the forces at each point, he computed only constant forces in the neighborhood of each point and randomly chose another constant point that is not in the neighborhood to compute the large distance effect. only constant points are computed at each point, so that the computational complexity is reduced to o per iteration. this spring model does not find the steady state solution in general. one can only process a fixed number of iterations, say  <dig> or  <dig>  as opposed to finding the converged solution. note that the constant forces are selected in both the neighborhood and afar. failure to incorporate one of these two forces will diminish the performance of this method  <cit> .

anchor point method
as in chalmers' linear layout algorithm, in the anchor point mds method  <cit>  only a portion of data is used to reconstruct the layout for intermediate steps. the data are grouped into clusters, so that the distances between points in different clusters are less meaningful than the distances between points in the same cluster.

in this method, some points in the same cluster are chosen as anchors and others are considered as floaters. distance information of anchors is used to construct the coarse structure of layout, and the floaters are used to update the fine structure. when a small number of k anchor points are chosen, a modified mds procedure only computes the n × k matrix. buja et al.  <cit>  showed that the number of anchors could not be smaller than the dimension p of the given data. moreover, the anchors should be chosen carefully because random choices of anchors do not work  <cit> . this is challenging when the grouping structure is unknown.

from these two methods, we can see that the intermediate steps for calculating mds do not need to employ all entries of the dissimilarity matrix. we can use this property to reduce the computational complexity of mds. another important issue is choosing the number of dimensions for layout. in a small data set, one can use the elbow test or similar methods to detect the changing shape for the decay of the spectrum of svd to determine the layout dimension. in a large data set, one feasible approach is to use stochastic methods by cross-validation to measure the layout dimension  <cit> .

methods
we first describe sc-mds using a simple case, and for convenience we use the classical mds  as the default mds method to show how we can improve it. assume that xi ∈ rp are the coordinates of data points for i =  <dig> ...,n and p <<n. we define di,j = ||xi - xj|| <dig> as the euclidean distance between xi and xj. n is large such that applying the cmds technique is impractical. we split the points into two overlapping sets s <dig> and s <dig>  and the intersection of these two sets contains more than p points. the main idea is to apply mds to each individual set to get the configuration, and to use the information of the overlapping points to combine the two sets into one. there are two problems that need to be solved:  how to combine two sets into one and  what is the sufficient condition for this solution to be equivalent to that obtained by working directly with the full set? the solutions to these two problems are proposed in the next subsections.

combination method
when we split the whole set of data points into two overlapping subsets of equal sizes, the combined size of the two distance matrices for the two subsets is less than that for the whole set. assume that the configurations of these two subsets obtained from mds are xi,1and xi, <dig> and the dimensions of the two configurations are the same. we can fix the coordinates of the data points in the first set and use the overlapping data points to find an affine mapping u + b such that for each intersection point xk, <dig> ∈ s <dig>  xk, <dig> = uxj, <dig> + b, where xj, <dig> ∈ s <dig> for some j. note that the matrix u of the affine mapping is a unitary matrix, which is a volume-preserving operator. the affine mapping can be found as follows.

assume x <dig> and x <dig> are matrices in which the columns are the two coordinates of the overlapping points obtained by applying mds to two data sets, and x¯ <dig> and x¯ <dig> are the means of columns of x <dig> and x <dig>  respectively. in order to use the same orthogonal basis to represent these vertices, we apply qr factorization to x1−x¯11˙t and x2−x¯21˙t, so that x1−x¯11˙t=q1r <dig> and x2−x¯21˙t=q2r <dig>  since these two coordinates represent the same points, the triangular matrices r <dig> and r <dig> should be identical when there is no rounding error from computing the qr factorization in x <dig> and x <dig>  the positive and negative signs of columns of qi could be arbitrarily assigned in the computation of qr factorization. hence, the signs of columns of qi should be adjusted according to the corresponding diagonal elements of ri so that the signs of diagonal elements of r <dig> and r <dig> become the same.

after the signs of columns of qi are modified, we conclude

  q1t=q2t. 

furthermore, we can obtain

  x1=q1q2tx2−q1q2t+x¯11˙t. 

that is, the unitary operator is u=q1q2t and the shifting operator is b=−q1q2tx¯2+x¯ <dig> 

in practice, one problem with the x <dig> and x <dig> obtained by cmds from a distance matrix is that some of the eigenvalues of the double centered distance matrix can be negative. when negative eigenvalues occur, the dimension of the configuration is determined by the number of positive eigenvalues. this could cause those two triangular matrices r <dig> and r <dig> to be unequal and sometimes the dimensions of the configurations of the two subsets are different. in the case of equal dimensions, we can still use equation  to combine the two data sets into one, but the equality in equation  becomes an approximation and the combination will induce computing errors. in the case that the dimension of x <dig> is not equal to that of x <dig>  for example, dim = q <dig> < dim = q <dig>  we project xi, <dig> into the space generated by the leading q <dig> basis of q <dig> we then use the new projected configuration of xi, <dig> and the configuration xi, <dig> to perform the combination processing. the projection of xi, <dig> from q <dig> dimension to q <dig> dimension induces computational errors too. to avoid this error, the sample number of the overlapping region is important. this sample number must be large enough so that the derived dimension of data is greater or equal to the real data.

sufficient condition for successive combinations
in the case of a large number of data points, the data points are split into several overlapping groups such that the number of overlapping points is greater than the dimension of real data. the recombination approach is similar to the case of two overlapping subsets. for example, we split data points into k overlapping chained subsets {s <dig> ...,sk}, i.e. si ∩ si+ <dig> ≠ ∅; we apply the mds techniques to each si; we use the configurations of s <dig> as the central reference and combine the subsets around s1; we repeat this procedure until all the subsets are combined.

the minimal number of points of each overlapping region and the grouping method used will strongly affect the performance of the low dimensional layout of mds. firstly, if the number of the overlapping points is smaller than the real data dimension, the rank of the affine mapping will be less than the dimension of data and the affine mapping cannot transform the coordinates to the corresponding coordinates. we demonstrate this point by a simulation case in the next section. secondly, points of each group should be chosen both in the neighborhood and beyond. this has been mentioned in  <cit> , where the information of both the neighborhood and afar is used for the spring model. if one puts only the neighboring points into the same group, the rotation effect will hamper the performance of the low dimensional layout . these two conditions are sufficient to guarantee good performance in a low dimension layout. the layout of the correct approximation solution of our method is outlined .

computational complexity reduction
we now show how sc-mds reduces the computational complexity of cmds from o to o when p <<n. assume that there are n points in a data set, ni is the number of points in each intersection region, and ng is the number of points in each group. when we split n points into k overlapping groups, we have kng - ni = n, and we have k=.

for each group, we apply cmds to compute the coordinates of the group data, which costs o computation time. at each overlapping region, we apply qr factorization to compute the affine transform, which costs o computation time. since the lower bound of ni is p+ <dig>  we can assume that ng = αp for some constant α. then the total computation time is about

  n−ppo+n−αppo~o. 

thus, when p <<n, the computation time of the sc-mds becomes o. moreover, if p<n, the computational complexity is smaller than o, which is the computational complexity for the fast mds method proposed by morrison et al.  <cit> . when p is very large , sc-mds has no advantage in computational speed, but it makes the computation feasible even when the computer memory does not afford the mds computation. furthermore, if we do not use cmds as our default mds method but use the linear time algorithm  <cit>  instead, then we can further reduce the complexity of the sc-mds method to o. this improvement makes application of the mds to large data sets feasible. hence, data analysis using sc-mds can guarantee better accuracy than existing non-metric mds methods.

RESULTS
simulation experiments
we simulate a spiral in two dimensions:

 r = 2θ, 2π ≤ θ ≤ 5π. 

first, let us construct the reference coordinates x. discretize θ into n- <dig> intervals and let θi = 2π + idθ, dθ=3πn− <dig>  at each θi, k points are constructed with noise. the following steps generate the data:

construct two raw vectors q <dig> and q <dig> 

 q <dig> i=2θ⌊i/k⌋cos⁡θ⌊i/k⌋+ni,q <dig> i=2θ⌊i/k⌋sin⁡θ⌊i/k⌋+ni, 

where ni is a random variable with normal distribution ni ~ n, for i = 1⋯kn. then we add some p- <dig> dimensional randomness into the coordinate matrix as follows. let z = ⟨αjui,j⟩, ui,j ~ n, for i = 1⋯kn and j = 1⋯p -  <dig>  where αj is the parameter to control the standard deviation of random variables. the final reference coordinates matrix x is

 x = t = , 

where xi ∈ rp. so the distance matrix is d = ⟨di, j⟩ with di,j = ||xi - xj||. in this simulation, p =  <dig> is used.

fig. 1a is the 2d projection obtained by applying cmds to  <dig> simulation points. then, in the first application of sc-mds, we set the number of points in each group to ng =  <dig> and the number of points in each intersection region to ni =  <dig> such that the number of the overlapping points is greater than the real dimension  <dig>  all points in each group are chosen randomly. in order to measure the difference between classical mds and sc-mds, we use the stress  to compute the error between the distance matrixes. the formula of stress is

 stress=∑i,j2∑i,jdi,j <dig>  

where di,j refers to the distance matrix of the original data and d˜i,j refers to that for the sc-mds reconstruction. in this spiral example, the stress of computing errors for sc-mds is only  <dig>  × 10- <dig>  and the stress for cmds is only  <dig>  × 10- <dig>  these errors are here considered as rounding errors. thus, sc-mds can reconstruct the configuration as does cmds; the result of our method  is similar to the 2d projection in fig. 1a. because this spiral example has only two essential dimensions, the 3rd to 17th dimensions are considered as random perturbations. if we reduce the number ni from  <dig> to  <dig> but keep ng =  <dig>  we can see that the 2d sc-mds representation keeps the shape of spiral but becomes more and more blurred as ni decreases. at the same time, the stress increases when ni decreases, as will be shown in fig  <dig>  in fig. 1c, we set ng =  <dig> and ni =  <dig> and the points in each group are also chosen randomly. the performance is now slightly worse than cmd due to dimension loss; the stress here is  <dig> . in fig. 1d, we set ng =  <dig> and ni =  <dig>  and group points neighboring to each other into the same group. it shows that the performance is totally bad – the spiral structure is lost and the stress increases to  <dig> .

in the same simulation example, we observed the relationship between error and the number of groups. when ni is larger than the dimension of the data, like ni =  <dig>  we can reconstruct the configuration of the data well. the number of groups of our grouping method does not affect the stress and the average stress is ~10- <dig>  hence, sc-mds gives accurate results if we carefully control the number of overlapping points and choose random points in each group.

next, we compare the speeds of non-metric mds, cmds and sc-mds. we create random vectors in a 20-dimension space, with the sample size ranging from  <dig> to  <dig> 

in figure  <dig> the black line  and the blue line  are produced by matlab default scripts, mdscale.m and cmdscale.m. the red line refers to our sc-mds method. we can see that the simulation result matches our theoretical analysis.

because of the hardware limitation to process cmds, we use only a set of  <dig> sample points as the maximal sample size to demonstrate that sc-mds performs as well as cmds when the number of data points  is not very large. we give below an example of large n to show that sc-mds works well in a large data set that cannot be handled by cmds.

gene correlation map
gene correlation maps are used to represent the correlations of genes such that genes with similar biological functions or in the same biological pathway tend to be located in the same neighborhood. it provides a prior probability in many applications of genome research by bayesian methods. since affy u133a genechip is widely used in many studies, we used genes listed in this chip and go descriptions on the gene ontology website to create the gene correlation map. in this chip, there are  <dig>  genes and  <dig> go terms are used in the list of these genes. hence, we consider each gene a binary vector with length of  <dig>  if the i-th term of the vector is one, then this gene has the i-th go description. there are  <dig> genes without any go description so that these genes are not used to compute the correlation with the genes with a go description. hence, the term-document matrix is reduced to the size of  <dig> ×  <dig> 

to apply the cmds to a distance matrix of  <dig> ×  <dig> is impossible for current hardware. so we use sc-mds to randomly separate the  <dig>  genes into  <dig> overlapping subsets, where ni =  <dig> and ng =  <dig>  although the essential dimension should be smaller then  <dig>  we still use ni =  <dig> to ensure accurate reconstruction. the qr operation and svd operation are available for this size. in each subset, we compute the  <dig> ×  <dig> distance matrix and then compute the 3d mds layout. figure  <dig> is the 3d layout of sc-mds results on these  <dig>  genes. in this figure, two genes located closely have similar go descriptions. we use the euclidian distance in this 3d layout to measure the relationship between genes. for example, gene probe ids 220259_at, 220815_at, 221980_at, 201015_s_at, 209880_s_at, 219765_at, 203018_s_at, 205523_at, 209879_at, 218796_at refer to the same go description and they have the same coordinates in figure  <dig>  this gene correlation map is useful for many gene selection problems. although we could not validate this result by comparing it with cmds, we can repeat the sc-mds procedure to see if we get consistent results among repeats. we find that the values of stress between different repeats are all small. this suggests that our method is stable for this large data set.

another way to check our method is to sample a subset of genes to create the correlation map, for example,  <dig> sampled genes. then we can compare the performance between cmds and sc-mds. we randomly sampled  <dig> genes from the  <dig>  genes, and then compute the distance matrix corresponding to these genes. the cmds layout showed that there are  <dig> dimensions in this gene set. then we applied sc-mds to this distance matrix with three different pairs ,  and . we repeated sc-mds  <dig> times for each pair. figure 5a is the 2d cmds layout. the stress between the original data and cmds is  <dig>  × 10- <dig>  figure 5b is the 2d sc-mds layout with , and its average stress compared with the original data is  <dig> . figure 5c is the 2d sc-mds layout with , its average stress is  <dig> . figure 5d is 2d sc-mds layout with , and its average stress is  <dig> . we can see that all the 2d structures are preserved, though less well in figure 5d where there is a substantial dimension loss.

gene expression clustering
the goal of gene expression clustering is to subdivide a set of gene expressing profiles into clusters such that genes in the same cluster share the same or similar patterns of expression profiles. in the situation with high dimensional data, researchers tend to obtain a manifold, which is defined by the regression of the data. because gene expression data is typically noisy, by clustering the projection of data in this manifold, which is in a lower dimensional space in comparison to the original data, a better result can be obtained. in this section, we show that the sc-mds method can successfully transform a high dimensional gene expression data set to a much lower dimension and preserve the intrinsic information of the original data. this transformation makes the clustering algorithm faster to get a converged solution. moreover, the representation of these gene expression data in this lower dimensional space reveals a better clustering result in biological validation.

we use the α38-synchronized yeast cell cycle dataset  <cit> . there were  <dig> genes in the data set, and each gene has a  <dig> point-time-course expression profile. however, this expression profile contained missing values, and we input these missing values by the knn imputation method  <cit> . to avoid the synchronized block effect, we remove the first two points of the time course data, so that the expression profiles for each gene are left with  <dig> time-course points. we then  compute the pair-wise dissimilarity of genes from each subset that are randomly chosen by the sufficient condition of sc-mds;  apply sc-mds to the subsets to reconstruct the new coordinate in the feature space; and  cluster genes in this feature space. because cmds cannot handle large samples and thus cannot be used to reduce the dimension when the sample size is large, we do not consider the clustering results in the reduced dimension space derived from cmds. instead, we compare the clustering results in the reduced dimension space derived from sc-mds with the results obtained in the original  space.

using the standard euclidean distance to measure the pair-wise dissimilarity of genes, we process the sc-mds method such that the data points are split into  <dig> groups, ni =  <dig> and ng =  <dig> 

note that instead of computing the pair-wise dissimilarity for every pair, we need to compute only the pair-wise dissimilarity in each group. this reduces the computational complexity of all processing to o. we choose ni to be greater than the length of original time course data points to satisfy the sufficient condition for an accurate layout.

using a pc with cpu  <dig>  ghz, 2g memory and matlab r <dig> as the testing software, we complete the analysis with the cpu time of ~ <dig> seconds. from the degradation rate of the distribution of standard deviations of output coordinates  the derived from the average of  <dig> repeats with different randomly grouping the elements of each subgroup, there are turning points at the 4th, 7th and 11th coordinates; after the 11th coordinate, the degradation is very small. by examining the second order difference of figure  <dig>  a local extremum occurs at 4th, 7th and 11th coordinates. because the standard deviation decreases smoothly after the 7th coordinate, and because the variation of the concavity in the 7th coordinate is larger than those at the 4th and 11th coordinates, we assume that the variables after dimension  <dig> are noisy and can be removed for dimension reduction. hence, we lay out these  <dig> genes on a 7-dimensional space. in this space, two genes located nearby will have similar expression profiles in this dataset. then we perform the k-means clustering method to compare the clustering result between the original expression data and our low dimensional layout.

note that k-means usually get a local optimal solution. when we want to obtain a reliable solution, we need to repeat k-means several times and choose the best solution. or we can define a function to measure the quality of k-means solutions, and then apply the simulated annealing to get the optimal solution. thus, obtaining a reliable k-means solution will take time. fortunately, applying k-means in the reduced dimension space are more stable than applying k-means in the high dimension space. that is, the iteration of k-means in the reduced dimension space converges faster than in the high dimension space. to demonstrate how dimension affects the stability of k-means, we repeat k-means on the yeast cell cycle data in the sc-mds space with different restricted dimensions, until k-means obtain  <dig> converged solutions. figure  <dig> shows that the cpu times are inversely proportional to the data dimension. the time k-means requires in a seven dimensional space is ~1/ <dig> of a  <dig> dimensional space. hence dimension reduction accelerates the k-means solution. however, although applying k-means in a low dimension space is faster than in a high dimension space, loss of too many dimensions will distort the distance relationship between data points, thus distorting the clustering result. hence, the dimension of reduced space should be determined carefully.

before applying k-means clustering, we determine how many clusters we should use as the parameter in the k-means process. we search the number of clusters in the k-means process from  <dig> to  <dig>  since the clustering result of k-means depends on the initial guess of the centroids of the sets, we repeat  <dig> times the k-means process and use the bayesian information criterion  score  <cit>  to pick up the best clustering index from the mean of these  <dig> clustering results. in each k-means process, if it does not reach convergence in  <dig> iterations, we reset the initial values of the cluster centers and re-run the k-means process until the process converges. we set the upper bound for re-running time to be  <dig>  if the re-running time reaches this upper bound, we choose the final cluster index by the best cluster sets from the previous iteration results.

the bic score is computed by the following formula:

 bic=∑i=1nlog⁡pr⁡)−p2log⁡, 

where n is the number of data points, k is the number of clusters, cj is the j-th model and p is the length of data points. we assume that each cluster from the k-means procedure is a multivariate normal distribution. the first term of the formula is the log-likelihood and the second term is penalty. when the data fit the model well, the term of log-likelihood becomes larger. we assume that the standard deviations of the multivariate normal distributions are the same, so that there are p parameters, k means, and one standard deviation in the p dimensional space, in these k clusters . if a model becomes complicated, i.e., the number of the model parameters becomes large, the bic score will decay. in fig.  <dig>  the dash line is reduced by taking the average of  <dig> repeats and the solid line is reduced by taking the maximal value of  <dig> repeats. we can see that the maximal value of solid line occurs in  <dig> clusters and the maximal of the dash line occurs in  <dig> clusters. hence, we partition  <dig> genes into  <dig> clusters in our example.

we cluster the original data set and the reduced 7-dimensional space data set from sc-mds separately, and each data set gives rise to  <dig> distinct gene clusters. then we input gene names from these clusters to the mips functional catalogue database. the outputs indicate that  <dig> clusters of the original clustering data set are significant to the cell cycle . in contrast,  <dig> clusters of the 7-dimensional data set are significant to the cell cycle function. twenty pairs of almost matched clusters, which consist of highly similar genes, are found by comparing the clusters from the two data sets. nine clusters of the 7-dimensional data set are not annotated to any function, while  <dig> clusters of the original clustering data set are not annotated. the details of these paired clusters are shown in additional file  <dig> and file  <dig>  there are many unpaired clusters, which contain lower proportions  of similar genes. in figs.  <dig> to  <dig>  we show these lower matched pairs that are related to cell cycle. in figs.  <dig> and  <dig> the biological validation shows that these clusters obtained from sc-mds are better than the corresponding clusters from the original 23-dimensional space data. in fig.  <dig>   <dig> % genes are indicated to have cell cycle function in the cluster obtained in the reduced space, and its p-value is  <dig>  × 10- <dig>  in fig.  <dig>  40% genes are indicated to have cell cycle function in the cluster obtained in the original space, and its p-value is  <dig>  × 10- <dig>  there are, however, another  <dig> genes that are included in the corresponding cluster in the original space. nine of them have significant functions in dna processing. conversely, there are another  <dig> genes that are included in the corresponding cluster in the reduced space. six of them have significant function in mitotic cell cycle and cell cycle control. in fig.  <dig>   <dig> % of the genes in the 32th cluster are annotated to have cell cycle functions in the cluster of k-means in the reduced space, while the genes in the corresponding cluster in the original space, the 29th cluster,  <dig> % are annotated to have cell cycle function. compare the difference between fig.  <dig> and  <dig>  five genes included in the cluster of the original space have no significant in the cell cycle function. conversely,  <dig> genes included in the cluster of the reduced space include  <dig> genes  that are significant in meiosis. in fig.  <dig>   <dig> % of the genes in the  <dig> cluster of the reduced space are annotated to have cell cycle, its p-value =  <dig>  × 1014; 48% of the genes in the  <dig> cluster of the original space are annotated to cell cycle function, its p-value =  <dig>  × 10- <dig>  we can see that in this pair, the cluster in the reduced space is worse than that in the original space. the other comparisons between the clusters from the reduced 7-dimensional space data and from the original space data are shown in additional file  <dig> and file  <dig>  combine the above three cases and twenty pairs that are almost matched, we can see that applying k-means to the sc-mds reduced space performs at least as well as in the original data set. sc-mds did not distort the pairwise relationship between data points, and it speeds up the analysis with accuracy preserved.

CONCLUSIONS
our new method reduces the computational complexity from o to o when the dimension of the feature space is far less than the number of genes n, and it successfully reconstructs the low dimensional representation as does the classical mds. its performance depends on the grouping method and the minimal number of the intersection points between groups. feasible methods for grouping methods are suggested; each group must contain both neighboring and far apart data points. our method can represent a high dimensional large data set in a low dimensional space not only efficiently but also effectively. this split-and-combine method makes the parallel computation of mds feasible. if samples increase to the level that one computer could not handle, we can split data to several subgroups, assign them to different computers to compute the mds, and then collect the results and combine them into one. in the cell cycle example, we showed that the clustering results of dimension reduction are more stable than the results in the original space. hence, sc-mds has overcome the curse of dimensionality in mds.

availability and requirements
• project name: scmds

• project home page: 

• operating system: os portable 

• programming language: matlab  <dig>  or higher

• licence: gnu gpl

• any restrictions to use by non-academics: license needed. commercial users should contact jengnan@gmail.com

authors' contributions
wh li was the project leader in this study and he investigated the biological issues. hhs lu and j tzeng designed and gave the mathematical proof of the fast algorithm of mds. j tzeng carried out the programming, data collection and analysis. all authors read and approved the final manuscript.

supplementary material
additional file 1
function annotation of original space clustering. this file provided the function annotation of the best k-means clustering result in data presented by original space. each column refers to a cluster. if the cluster has no efficient annotated function, then the corresponding column is empty.

click here for file

 additional file 2
function annotation of reduced space clustering. this file provided the function annotation of the best k-means clustering result in data presented by reduced 7-dimensional space. each column refers to a cluster. if the cluster has no efficient annotated function, then the corresponding column is empty.

click here for file

 acknowledgements
we thank the three reviewers for their valuable comments. we also thank national center for high-performance computing, national applied research laboratories for use of the computer facilities. this study was supported by academia sinica and national science council, taiwan.
