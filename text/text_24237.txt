BACKGROUND
markov models for the protein sequence evolution have been widely used for the past  <dig> years. these evolutionary matrices highlight the most common mutational changes between amino acids and codons. protein sequence evolution has been investigated at both amino acid and codon levels. the evolutionary matrices on the basis of amino acids are widely used for sequence alignments and phylogenetic tree construction. as more than one codon encode to the same amino acid, it is easy to estimate alignments in amino acids as compared to codons.

codon-level models demonstrate the mutational changes among the codons. this gives us more information by highlighting the tendency of mutations between codons encoding the same amino acid  as well as the mutational effects between codons that code for different amino acids . as codons are the smallest genetic information unit in protein-encoding regions, it is obvious to model mutations by a codon-based communications channel model highlighting all codon-to-codon changes in nature.

substitution matrices define the rate at which one amino acid in the protein sequence is changed into another amino acid. dayhoff et al.  <cit>  estimated the first such model in  <dig>  resulting in the widely used point accepted mutations  matrix. it is computed by counting the mutations in the closely related proteins. henikoff and henikoff proposed the block substitution matrix  for divergent protein sequences, which uses log-likelihood ratios to construct scoring matrices from the transition matrices between amino acids  <cit> . later on, whelan and goldman  proposed a novel approach to estimate amino acid replacement matrices from a large database of aligned protein sequences in  <dig>  <cit> . it combines the estimation of transition and scoring matrices by a maximum-likelihood approach that accounts for the phylogenies of sequences within each training alignment.

as the codon  is the basic genetic information that directly encodes the amino acid as the building block of proteins, we have used the first empirical codon substitution matrix  in our analysis. this was proposed by schneider et al., where sequences of five vertebrates were aligned and the number of codon substitutions were counted among them  <cit> . according to conversations with the authors, it is estimated that these mutations on average happened in roughly  <dig> million years.

yockey was one of the first to model and describe a central dogma using information theoretic tools  <cit> . he viewed the flow of information from dna or rna to proteins as a communication system and employed entropy, rate, and capacity calculations with a transition matrix he developed by considering base changes of equal probability. a detailed analysis of the application of information theory to molecular biology can be found in his book  <cit> . relatively recently, l. gong, n. bouaynaya, and d. schonfeld have proposed a communication model for protein evolution  <cit> . they used the amino acid based pam matrix and a matrix they produced, similar to yockey’s, as a communication channel and performed capacity calculations over it.

we computed an exponential factor for the ecm matrix that would still allow for preserving the genetic information given the redundancy that is present in the codon-to-amino acid mapping. this gives an insight on how such a mutation matrix relates to the preservation of a species in an information-theoretic sense.

for the underlying capacity computation, we used the arimoto-blahut algorithm  <cit>  to determine the input distribution that maximizes the mutual information.

methods
in order to compute the mutation probability in the ecm matrix,  <dig> alignments of sequences from five vertebrate genomes yielded  <dig>  million aligned codons from which the number of substitutions between codons were counted. this matrix has 64× <dig> entries stating the mutation probability of each codon to every other codon. basically, the substitution from sense codons to stop codons is not included in the ecm matrix, which makes the matrix block diagonal with a 61× <dig> matrix for coding codons and a 3× <dig> entries for substitutions between stop codons. therefore, we will consider only substitutions between coding codons and regard the ecm matrix as 61× <dig>  from the communication perspective, this mutation matrix describes channel transition probabilities p.

there is also another matrix in  <cit> , which gives the actual count of substitutions observed. from this substitution count matrix c, we obtained the biological probability distribution of the codons as 

  px=∑jcij∑i∑jcij. 

thereafter, we combined the codons which encode for the same amino acid and computed the probability distribution of amino acids, denoted p
a
. using this distribution, the to be preserved information content of the  <dig> codons representing the  <dig> amino acids can be computed as 

  r20=-∑i=120palog2). 

according to shannon’s channel coding theorem, a communication through a noisy channel of capacity c at an information rate of r is possible with an arbitrarily small probability of error, if r<c <cit> . hence, the channel capacity has to, at least, exceed the value of r <dig> 

in communication systems, the channel capacity is determined by maximizing the mutual information between input  and output  over the input probability distribution p
x
. 

  c=suppxi. 

i is the mutual information which measures the mutual dependence between input and output distributions, and is defined as 

  i=h-h, 

where h is the entropy of the codon distribution at the output of the ecm “channel”, and h is the conditional entropy, called prevarication or irrelevance.

however, in the system we are considering, the input distribution  is not something to adjust. it is defined by nature. therefore, we determine the channel capacity corresponding to the mutation “channel” matrix for a biological codon frequency obtained by eq. . h is computed as 

  h=-∑i=161pyilog <dig>  

where pyi is the output probability distribution of the i
t
h
 codon. the conditional entropy h between input and output distribution of codons is computed as 

  h=-∑i=161p∑j=161plog2p. 

p is the conditional probability between codons, which is given by the empirical codon mutation  matrix.

we now compute, what exponent of the ecm matrix would be needed to make the capacity just match the required rate obtained by eq. . hereto, we use the singular value decomposition  yielding 

  pf=ufv∗, 

where u,v are unitary matrices, Σ is a diagonal matrix with nonnegative real numbers in the diagonal, and f is an exponent to be fine-tuned. the value of the exponent is changed in steps from zero to one. a value of  <dig> means the original ecm matrix is used.

moreover, we would like to find the optimum codon distribution by solving eq.  and compare it with the biological distribution. for solving the optimization problem, the arimoto-blahut algorithm was employed  <cit> . the arimoto-blahut algorithm is an iterative numerical algorithm that monotonically converges to the capacity value. to compute the capacity, it is starting from any arbitrary input probability distribution p
x
  and performs the following two steps until the algorithm converges. 

 <dig>  compute a quantity related to the mutual information per input symbol 

  c:=exp∑kplogp∑jpp. 

this results from a lagrange multiplier step in  <cit> .

 <dig>  update the input probability distribution according to 

  p=pc∑jpc. 

the termination criteria is based on the lower and upper bounds of the channel capacity, 

  log∑jpc≤c≤logmaxxjc. 

the iterations are terminated when the upper and lower bounds are equal up to a certain accuracy.

once the optimized codon distribution is obtained using the arimoto-blahut algorithm, to note the similarity with the biological distribution, we applied the so called kullback-leibler divergence   <cit> . d
k
l
 is a quantitative measure of how similar a probability distribution p is to a model distribution q, and is defined as 

  dkl=∑ipilog2piqi. 

d
k
l
 is non-negative and gives a zero result when the distributions are perfectly matched. technically speaking, d
k
l
 measures the average number of extra bits required  for using a code based on q instead of p.

RESULTS
the to be preserved information content of the amino acids, using the amino acid distribution and computed according to eq.  is  <dig>  bit, which is less than the maximum value of log2= <dig>  bit. likewise, the required rate obtained by using the amino acid probability distribution provided by king & jukes in  <cit> , derived from  <dig> residues of  <dig> vertebrate polypeptides is  <dig>  bit. thus, it is reasonable to look for a capacity that is at least greater than  <dig> . hence, using the biological codon distribution in the five vertebrates obtained by using eq. , we stepwise reduced the exponent of the ecm matrix until it satisfies the rate requirement. furthermore, we used the arimoto-blahut algorithm to find the optimal input probability distribution of the  <dig> codons to maximize the mutual information and compare it with the biological distribution of codons. the optimal capacity-achieving codon distribution and the observed biological codon distribution are both shown in figure  <dig>  the corresponding values are also tabulated in table  <dig> and table  <dig> 

the codon relative frequency of the five vertebrates genomes  from the data presented by schneider a., cannarozzi g., and gonnet g.  <cit> .

the codon relative frequency that maximizes the mutual information between input and output and yielding a capacity close to what is required for preserving the information content of amino acids. an exponential factor of  <dig>  is applied to the ecm matrix.

the capacity obtained by optimizing the codon distribution, the mutual information based on the observed biological codon distribution, and the required rate are shown together in figure  <dig>  when the exponent of the ecm matrix is reduced, the output codon distribution changes and the prevarication h will be smaller. as a result, the capacity increases. the maximal exponent which satisfies the rate requirement of  <dig>  bit for an error-free “transmission” using the biological codon frequency is found to be ≈  <dig> . at the same exponent, the optimized “channel” capacity is  <dig>  bit. it can also be seen that the capacity curve is very close to the one found by using the biological codon distribution. this indicates that the biological probability distribution is almost optimally “chosen” to achieving the capacity of the “channel”.

it is not surprising that the exponent is not one, since the matrix was obtained comparing five different vertebrate dnas, the times corresponding to time spans between  <dig> m –  <dig> m years. however, the exponent is not extremely small, which indicates that the matrix is at least roughly in agreement with information-theoretic calculations. one may also see this as an argument to recompute the matrix using the obtained coefficient.

to see how well the biological and the optimized codon distributions agree, we applied the kull-back–leibler divergence  and obtained a value of  <dig>  bit, which is not a very small difference  but still, similarities are obvious. both of the probability distributions satisfy the rate requirement of  <dig>  bit. in addition, the distribution among synonymous codons is very similar. to mention one example, codons encoding alanine  in decreasing order of abundance, is gcc, gct, gca, and gcg, for both the biological and the capacity-achieving distributions.

CONCLUSIONS
from the so-called empirical codon substitution matrix , a mutation probability matrix, we derived the capacity when regarding the matrix as a communication channel. we found that an exponent of  <dig>  would lead to a capacity of  <dig>  bit that is at least required to preserve the genetic information represented by the  <dig> amino acids encoded by  <dig> codons. additionally, for the desired channel capacity, we have presented the optimal codon distribution found by searching the distribution that maximizes the mutual information starting from a random initialization. a comparison of the biological distribution with the optimized codon distribution shows that the two distributions are not too similar. however, the biological distribution is not too far from the capacity-achieving distribution in terms of “channel” capacity, which indicates that the biological distribution is well “chosen”. additionally, the optimal codon distribution has preserved the relative abundance of synonymous codons. we concluded that the ecm as a channel is not too far from what would be expected following information theoretic arguments although it was derived from  <dig> different species.

competing interests
we have no competing interests.

authors’ contributions
am: original computations and first version of the paper for first review. dn: correcting computations and major updates of the paper according to the wishes of the reviewers. wh: supervisor, original idea, checking results, and proof-reading. all authors read and approved the final manuscript.

