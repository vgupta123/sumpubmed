BACKGROUND
the revolutionary development of massively parallel dna sequencing has enabled identification of biomedically relevant genomic variants via whole genome  <cit>  and exome resequencing  <cit> . information relevant for personalized medicine such as assessment of longitudinal disease risks, and personalized treatment  <cit>  are now within reach.

in a few very recent personal genomic studies, results have directly led to targeted treatment and dramatic improvement in the patient's quality of life  <cit> . these examples are paving the way to soon turn genomic sequencing into a routine diagnostic procedure and enable personalized medicine.

currently, analysis of sequencing data on a genomic scale requires bioinformatic expertise and access to extensive computational resources, presenting a significant barrier. most cutting-edge genome analysis applications  <cit>  are still limited to a command line interface and require at least moderate informatics expertise to operate. in addition, large scale genomic data analysis requires routine access to a high performance compute cluster. such requirements are entirely unsuitable for the operational models of smaller research/diagnostic laboratories due to the excessive investment requirements in computing infrastructure and personnel.

the deployment of genomic analysis software as a service  within a cloud computing framework offers a unique solution for these problems. the concept behind cloud computing is to outsource computation to third-party servers or clusters at a remote location. this allows small laboratories to take advantage of external computational resources without having to maintain an in-house compute cluster. this software as a service model removes the upfront investment requirement and any delays associated with building local computing infrastructure. earlier solutions such as cloudburst  <cit>  and crossbow  <cit>  have attempted to tackle the very specific problem of mapping short read data and assembling large genomes using the scalability offered by the map-reduce framework deployed on top of a compute cluster. while this is useful the users would still need to have considerable bioinformatics skill and acquaintance with cluster infrastructure to undertake such an analysis. other solutions such as cloudman  <cit>  from the galaxy project provide a user interface and remove the need for user to have informatics experience but are not specifically designed for personal genome analysis.

to this end we integrated our variant analysis pipeline - atlas <dig> suite - onto a "local cloud" using the genboree workbench http://www.genboree.org and onto a "commercial cloud" via the amazon web services http://aws.amazon.com. we performed a case study using the atlas <dig> genboree pipeline as a proof of concept to demonstrate the potential of personal genome analysis on the cloud. we also processed two whole exome capture samples using our atlas <dig> amazon pipeline to outline the cost of running analysis on amazon. our cloud analysis pipeline on genboree has a web browser-based drag and drop interface, allowing users to interact with the software through their browser at any location, and making it practical for the software to be used by non-bioinformaticians. our cloud pipeline is actively maintained by our team, which also removes the need for users to update the software.

methods
deploying the atlas <dig> personal genomic analysis pipeline via the genboree workbench 
the atlas <dig> suite is a variant detection software package optimized for variant discovery in exome capture data on all the three next generation sequencing platforms  <cit>  . the suite consists of atlas-snp <dig> for calling single nucleotide polymorphisms  and atlas-indel <dig> for calling short insertions and deletions  http://www.hgsc.bcm.tmc.edu/cascade-tech-software-ti.hgsc. these tools have been available for command line usage, and applied to a number of large scale projects including the international  <dig> genomes project  <cit> , the cancer genome atlas project , and follow-up resequencing in the context of disease genome wide association studies.

genboree workbench is a platform for deploying genomic tools as a service and is deployed at baylor college of medicine http://www.genboree.org. the genboree workbench graphical user interfaces  extensively relies on ext-js, a javascript library. tools within the workbench make api  calls to the rest  api which is hosted on a thin server. this is done asynchronously using asynchronous javascript and xml . since genboree system uses rest style of architecture to communicate between the server and the client, it allowed us to easily integrate atlas <dig> within a couple of weeks. genboree is backed by a small cluster of nodes which are managed by the torque resource manager  and maui  to schedule jobs. atlas <dig> genboree can be accessed as a genboree workbench toolset. users from external groups with access to a web browser can 1) upload data onto the cloud, 2) run atlas <dig> for variant analysis, and 3) visualize the variant calling results using different genome browsers such as the genboree browser or university of california, santa cruz genome browser <cit>   . atlas <dig> genboree has a web-interface with hierarchical click-through steps. the self-explanatory nature of the web-interface eases the usage overhead. the workflow illustrated in figure 1b shows the specific steps in running the atlas <dig> suite on the genboree system.

genboree data selector
the genboree workbench organizes data in a hierarchal tree. before using the atlas <dig> suite users must define a group and create a database. within the database are the "files" and "tracks" subdirectories. files contain input files uploaded by the user and output files generated by atlas <dig>  tracks contain processed output files which can be used for visualization on the genboree browser. this hierarchical representation is shown in a screenshot of the genboree workbench in figure 2a.

uploading data onto atlas <dig> genboree
atlas <dig> genboree accepts binary sequence alignment/mapping format  files as input. files are uploaded onto genboree by dragging the destination database from the data selector to the output targets box and selecting "transfer files" under the data tab in the menu. a prompt window allows users to select an input bam file from their local computer and upload it to the cloud servers. a  <dig> gb bam file took approximately one hour to upload on a  <dig> mb/sec bandwidth connection.

variant calling
the atlas <dig> suite may be run by simply assigning the desired input and output and selecting the appropriate tool . atlas <dig> genboree allows users to specify parameter cutoffs in the job parameter-setting window . here one can choose from the three different sequencing platforms and tune the parameters.

the tool produces two output files, an lff file and a variant call format   <cit>  file which are stored under the files section inside of the database specified in the output target box. the lff format is adapted from the ldas upload format used to store variants and annotations http://www.genboree.org/java-bin/showhelp.jsp?topic=lfffileformat. both the files can be downloaded by selecting the specific file and clicking on the download file option.

genboree system allows integration with third party tools
cloud deployment may produce "silos" of integration where extension of analysis pipelines and addition of analysis steps beyond those offered as a service may be hard to accomplish. to overcome this problem, genboree system provides application programming interfaces for programmatic access to all the data and tools. also data is accessible in formats that can be readily fed into a variety of ancillary tools. the interfaces and data format compatibilities enable mixing-and-matching of tools required in specific steps such as visualization in various genome browsers including ucsc genome browser, invocation of pipelines such as galaxy  <cit> , and integration with custom or third-party variant analysis and annotation tools such as annovar <cit> . as described next, we successfully tested all three types of integration.

visualizing variants with genome browsers
genboree browser
the variant calls can be readily viewed in the genboree genome browser. after going into the browser, variants can be visualized by selecting the appropriate database. genboree browser supports looking at variants from multiple samples simultaneously.

ucsc genome browser
the variants called by atlas <dig> genboree can be directly exported to ucsc genome browser  <cit>  for further viewing, annotation and analysis. the variants can be exported by converting our variants file into a bigbed format file  via the cloud file conversion functionality.

integration with galaxy
as our initial trial, we were able to upload our raw vcf file downloaded from genboree without post-processing onto galaxy and convert the vcf file into a multiple alignment format  custom track using the vcf to maf custom track function with graph/display data.

post-processing with third party variant annotation tools
the vcf file downloaded from genboree was annotated and filtered using annovar. annovar categorizes variants into intronic, exonic, splicing, non-coding rna, 5' untranslated region, 3' untranslated region, upstream, downstream and intergenic. the exonic variants are further categorized into synonymous, nonsynonymous, stop gain , stop lost , and frameshift or non-frameshift changes caused by insertions, deletions or block substitutions. annovar can also be used to filter out variants found in dbsnp.

enabling the atlas <dig> personal genomic analysis pipeline via amazon web services 
the amazon web services  provides virtualized computational infrastructure on demand. aws can be tailored to provide scalable and flexible solutions for application hosting, web applications and high performance computing. we used the amazon elastic compute cloud  and amazon simple storage service  to enable atlas <dig> on amazon. the ec <dig> allows users to lease a wide variety of ec <dig> instances which differ based on the amount of compute nodes and memory . amazon s <dig> is a persistent data storage solution offered by aws, which is meant to be highly scalable and have low latency. both the ec <dig> and s <dig> services can be managed from the aws management console.

our atlas <dig> cloud pipeline on aws was designed ground up to be specifically used for personal genome analysis. the web user interface, developed using the spring framework written in java, provides access to atlas <dig> suite on the machine image; this user friendly interface can be used to submit jobs and monitor worker nodes . the application runs on apache tomcat  and can be accessed through port  <dig>  the backend code, on the atlas <dig> machine image, is optimized to efficiently analyze data and ease the process of adding newer tools to the pipeline in the context of genome analysis. the backend code was written in python . fabric , amazon ec <dig> api tools  and s3cmd  are integral part of the backend code. fabric was used for executing commands on the worker nodes, amazon ec <dig> api tools were used to start, terminate and monitor the status of worker instances and s3cmd was used to interact with the s <dig>  figure  <dig> provides an overview of the atlas <dig> amazon pipeline.

in order to access the atlas <dig> cloud pipeline the user has to first register for an aws account by going to http://aws.amazon.com/. once registered the user needs to sign up for ec <dig> and s <dig> services. the user then starts an instance using the public atlas <dig> machine image  which can be found inside the community amazon machine image  tab. before starting the instance the user needs to change his security groups setting so as to enable http access on port  <dig> to be able to access the webpage which can be done using the public dns of the instance which can be found in aws management console. since the master instance is only acting as a portal to access and monitor the jobs running on aws this master instance can be a "t <dig> micro" instance. the advantage of a "t <dig> micro" instance is they are cheap and at the time of writing this article every new registered aws user would get  <dig> hours of free runtime every month for a year; afterwards it is $ <dig> /hour.

once able to access the webpage the user must create an account before they can access the pipeline. by way of creating an account, this instance can support multiple users and users do not have to type in aws credentials each time they submit a job. the aws credentials are needed to start additional instances and to access user data on s <dig>  to submit a job users must provide the name of the folder on s <dig> containing the input files and reference fasta needed for analysis, maximum number of parallel ec <dig> instances to run, upload a file with the list of input files to be processed, reference file name, sequencing platform and analysis to be performed. figure 4a shows a screenshot of the job submissions page. currently atlas <dig> amazon expects the user to upload the data onto s3; this can be done by going to the s <dig> tab on aws management console. alternatively, users may take advantage of the aws import/export option wherein the user can ship a portable storage device to amazon and it will securely process and transfer the data onto s <dig>  this option is extremely useful in uploading large amounts of data to s <dig> due to network bandwidth bottleneck.

the monitoring page shows information regarding each worker node, i.e. each row in the monitoring page represents a worker instance. for each worker instance the following information is shown; instance id, public dns, node status, job status, start time, end time, input bucket and output bucket. the instance id and the public dns can be used to access the worker node. node status can have two possible states either the worker node is running or has been terminated. job status can have five possible states, started refers to worker node has been instantiated, setup refers to head node is prepping the worker instance, downloading reference suggests that the worker instance is downloading the reference fasta file, variant calling suggests worker node is running the analysis and termination refers to there are no more jobs in the queue and worker node is being terminated. figure 4b shows a screenshot of the monitoring page.

RESULTS
applying the altas <dig> genboree to a case of personal genome study
we tested atlas <dig> genboree by performing an analysis on a recently published personal whole genome sequencing data set  <cit> . we examined the resource usage metrics and reproducibility in variant analysis, and examined the challenges related to integrating multiple tools required for variant detection, visualization, and analysis.

description of the personal genome data set
bainbridge et al. <cit>  employed the solid  <dig> next-generation sequencing platform, and sequenced the complete genomes of a 14-year-old fraternal twin pair, one female  and one male  diagnosed with dopa -responsive dystonia . drd is a genetically heterogeneous and clinically complex movement disorder with parkinsonian features that is usually treated with l-dopa. after identifying six heterozygous autosomal mutations in three genes, a new clinical intervention was prescribed that dramatically improved the quality of life of both twins.

variant analysis
we analyzed chromosomes  <dig> and  <dig> since all six mutations were on these two chromosomes. uploading the bam files, ~  <dig> gb in size, took ~ <dig> minutes using the genboree workbench interface. we ran atlas <dig> using snp default settings for the solid platform. it took an average of ~ <dig> hours to run chromosome  <dig> and ~  <dig> minutes to run chromosome  <dig> . the average memory usage on the cloud node was ~ <dig> mb. detailed numbers of time taken to upload, run atlas-snp <dig> and memory usage are summarized in table  <dig>  the vcf file generated on the cloud were then downloaded for further analysis.

combining results from the chromosome  <dig> and  <dig>  atlas <dig> called  <dig>  and  <dig>  high confidence single nucleotide variants  in patientx and patienty, respectively. annotating the vcf file using annovar we found  <dig> % and  <dig> % of snv called in patientx and patienty respectively overlapped with dbsnp , which is very similar to what had been found by bainbridge et al.   <cit> . the annotations generated using annovar, were used to filter variants such that we could get to novel nonsynonymous snvs which are more likely to be causal .

raw variants were then filtered with dbsnp  and annotated with genetic information.

our atlas <dig> pipeline successfully called all the six variants relevant in three genes in patienty whereas only five of six variants were called in patientx. information regarding the three genes, variants and whether it was called is summarized in table  <dig>  the one undetected mutation by our pipeline was in spr gene at position  <dig>  causing a change from arginine to glycine. the reason atlas-snp <dig> was not able to call this snv was due to a default heuristic filter which requires at least two high quality reads with variants. after examining the raw bam file, we found that only one such variant read was found at this locus. in cases such as this, in order to lower the detection threshold, users can go back to the settings window and lower our heuristic cutoffs to achieve much higher sensitivity.

the table shows how many of those mutations we reproduced.

applying atlas <dig> amazon to whole exome capture data
we ran atlas-indel <dig> on two bam files, one illumina and one solid, using atlas <dig> amazon, and outlined the cost of running atlas <dig> amazon to generate indel calls on whole exome capture data. the solid and illumina bam files were obtained from the  <dig> genomes phase  <dig> project they were 35gb and 14gb in size respectively and took  <dig> and  <dig> hours to upload to s <dig> using the aws management console. the illumina bam  contained ~ <dig> million reads with average read lengths of 75bp and the solid bam  contained ~ <dig> million reads with average read lengths of 50bp the average depth coverage across the capture region in both the bam files was ~30x. the processing was done on an "m <dig> large" ec <dig> instance which comes with  <dig> elastic compute units ,  <dig>  gb of memory and 850gb of local instance storage. it took us ~ <dig> hours and ~ <dig> hours at a cost of $ <dig>  and $ <dig>  to process solid and illumina bam files respectively. a detailed breakdown of the cost incurred on storage, compute and i/o on the attached ebs volume is summarized in table  <dig>  based on our experiences with amazon we projected the cost of running atlas <dig> amazon on  <dig>   <dig>   <dig>   <dig> and  <dig> bam files the data is shown in table  <dig>  while trying to make the projections we tried to be as realistic as possible, but had to make a few assumptions. we assumed average size of bam files to be 20gb and storage cost was computed for a  <dig> month period. compute cost was based on "m <dig> large" ec <dig> instance which has a running cost of $ <dig> /hour and  <dig> million i/o requests per bam. graph depicting the cost projection can be seen in figure  <dig> 

CONCLUSIONS
if personal genomic studies are to become a routine part of personalized diagnostics and medical management that is accessible to small research and clinical laboratories, advanced bioinformatic analysis must be made accessible both in terms of computational resources and usability. we have demonstrated the suitability of deploying existing analysis tools onto a cloud resource to address these issues, and demonstrated its utility by duplicating a real-world case study of clinical significance. we also outlined the cost of running atlas <dig> amazon pipeline on solid and illumina whole exome capture samples and made cost projections of running the analysis on much larger sample sizes.

these analyses show that atlas <dig> on genboree and amazon provide both possible and practical solution for personal genome variant analysis on the cloud by outsourcing the computation resources and expertise needed to perform such analysis. by removing these barriers, atlas <dig> on the cloud enables non-bioinformaticians at small research labs to perform this analysis without the need to invest in expensive compute clusters. it is our hope that various pipelines will have output that is cross-compatible with each other so as to enable and facilitate the creation of customized personal genomic analysis.

while the present atlas <dig> amazon architecture and aws cost structure are certainly a viable solution for small scale personal genome analysis the storage cost in the long run can make it prohibitive for large scale analysis. with the growing number of competitors in the cloud computing space we believe the cost of storage and compute is eventually going to come down and by harnessing the power of distributed computing algorithms like map-reduce framework will make it attractive for large scale analysis. there are other serious consideration such as data security which is of utmost importance especially in a clinical setting, the burden of which lies in the hands of both developers and end users and until such issues are resolved they pose a serious hindrance for clinical use. other minor issues include the network-bandwidth bottleneck, but this is a onetime problem since once the data is uploaded onto the cloud it can used for multiple analyses. once these challenges have been addressed we believe that genome analysis on the cloud will become a valuable resource, enabling both large and small scale clinical analysis by a variety of diverse research groups.

abbreviations
aws: amazon web services; ec2: amazon elastic compute cloud; s3: amazon simple storage service; ebs: amazon elastic block storage; ecu: elastic compute unit; ami: amazon machine image; snv: single nucleotide variants; indel: insertions and deletions; bam: binary sequence alignment/map format; vcf: variant call format; yri: yoruba in ibadan nigeria; gbr: british from england and scotland.

availability and requirements
the atlas2-cloud machine image is made public and can be instantiated from the aws management console by searching for the following amazon machine image id ami-ec469c <dig>  since machine image ids are not permanent and susceptible to change when we update the machine image the better way to find the atlas <dig> image would be to search for "atlas2" in the community ami tab. the atlas2-amazon backend source code is released under the bsd license and is available for download at http://sourceforge.net/projects/atlas2cloud/ . more detailed instructions and tutorial on how to access the pipeline can be found at our sourceforge page.

competing interests
am is a founder and owns shares in ip genesis, inc., a company that owns commercial licensing rights to genboree. use, dc, jy, arj, sp, mnb, aj, pp, cc and fy declare they have no conflicts of interest.

authors' contributions
fy and am conceived and directed the project. dc, jy and use developed atlas <dig> suite. arj, sp, cc and am developed and maintain genboree workbench, and integrated the atlas <dig> suite with it. use, aj and pp enabled atlas <dig> on amazon. mnb provided us with clinical data. use and fy designed and carried out the analysis. use, dc, am and fy prepared the manuscript.

