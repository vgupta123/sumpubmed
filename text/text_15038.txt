BACKGROUND
rna-seq has provided a powerful tool for analysis of transcriptomes. for non-model organisms with limited genomic information, transcriptome sequencing provides a cost-saving tool by only sequencing functional and protein coding rnas, thus providing direct information about the genes  <cit> . there are many benefits of sequencing a genome, but for relatively large genomes such as human and mouse, protein coding regions account for under 5%, thus most of the sequencing effort would go to sequencing either regulatory regions or repetitive elements  <cit> . smaller genomes could be sequenced and assembled to complement the transcriptomes, though this is not a tractable approach if a genome is quite large. even still, de novo genome assembly can produce errors by itself  <cit> .

despite its advantage, transcriptome assembly does present additional challenges when compared to genome assembly. unlike genomes where most sequences should be approximately equally represented, coverage of any given sequence in a transcriptome can vary over several orders of magnitude due to expression differences  <cit> . because coverage can vary, there is also a question of sequencing depth. theoretically, there is a sequencing depth beyond which addition of more reads does not provide new information, known as the saturation depth. several studies have used approaches which map reads onto reference genomes and these have suggested saturation depths at 95% gene coverage ranging from  <dig>  million reads to  <dig> million for mrna level coverage, and up to  <dig> million for splice variants  <cit> . however, these studies all made use of short reads around 36bp and were not assembling the transcriptomes de novo.

several recent studies have already made use of next-generation sequencing reads for de novo transcriptome assembly  <cit> . the number of reads used for assembly in these studies varies widely, ranging from  <dig>  million reads up to  <dig> million reads  <cit> . the assembly strategies are equally varied, but share the initial step of removing low-quality reads and adapters whereupon all remaining reads are assembled. the assembly quality estimates vary as well with the most common measure of quality based on blast hits to public databases like uniprot, though it was noted that under-representation of many taxa in public databases limits this approach  <cit> .

while many parameters must be optimized for the specific assembly, it is both inconvenient and costly to acquire more reads by resequencing. presently, there is no clear consensus of what sequencing depth is optimal or what factors would contribute to the adequate depth. the problems of omitted genes or variants are obvious with too few reads. on the other hand, it was suggested that greater depth may create errors in differential expression analyses, cost more, and take longer to assemble  <cit> . thus, here we use the same assembly strategy across a diverse set of organisms to isolate the effects of read count on assembly quality to attain a general estimate of optimal read count. we compare trends from de novo assemblies across six phyla. these animals include the mouse , the humboldt squid dosidicus gigas, the scaleworm harmothoe imbricata, the decapod sergestes similis, the copepod pleuromamma robusta, the ctenophore hormiphora californensis, and the siphonophore chuniphyes multidentata. to our knowledge, this is the first study to suggest an optimal number of reads for de novo assembly for the purposes of mrna level analysis. these results are applicable to studies of organisms with limited genomic resources.

RESULTS
de novo assembly of transcriptomes
assembly of mouse heart transcriptome
raw mouse-transcriptome reads from the encode project were downloaded from ncbi short-read archive. sample srr <dig>  consisted of  <dig> , <dig> x76bp reads as paired-ends. filtration  removed  <dig> % of the reads, almost 95% of which were due to low quality scores. in order to examine the role of number of reads on the assembly, we computationally sub-sampled randomized sets from the original library. it is suggested that sequencing of very small numbers of reads can be most subject to biases and that cdna normalization can improve the uniformity of the library at low numbers of reads  <cit> . such an approach might be quite costly, and the computational sub-sampling approach has the advantage of drawing from the largest pool of reads and avoid biases which could occur at low numbers of reads. subsets of the filtered library were generated containing  <dig> , <dig> , <dig> , <dig> , and  <dig> million reads. reads from each set were included in the next largest set, thus all of the reads in the  <dig> million set are included in the  <dig> million read set, and so forth. these sets were assembled with velvet/oases  <cit>  and trinity  <cit>  .

schulz et al. reported reliable parameters for oases which produced high-quality assemblies of mouse and human cell cultures, using  <dig> million and  <dig> million reads, respectively  <cit> . this included use of a broad k-mer range with a low starting k-mer of  <dig> or  <dig> up to a k-mer of  <dig> or  <dig>  accordingly we used k-mers from  <dig> to  <dig>  also, a minimum k-mer coverage is required by oases to retain any given node during the assembly process; by default this is  <dig> in oases, that is, any node must have at least three-fold coverage for that node to be used. some differences were observed in the output when this parameter was changed, and so the same data were assembled with coverage cutoff of  <dig>  and a stricter cutoff of  <dig> .

the number of transcripts  increases steadily for all assemblies . c <dig> also had substantially fewer transcripts and accordingly much higher mean and median lengths . the pattern of increase for median and n <dig>  tracked the mean for the c <dig> assembly, but not the c <dig> assembly which did not have a clear qualitative pattern. the mean, median and n <dig> were all lower for the trinity assembly than the c <dig> despite having far fewer contigs.

oases generates transcript “loci”, which is oases terminology for the de-brujin graph clusters meant to represent genes and their splice variants or highly-similar paralogs. both curves approach to a plateau for locus counts . the greatest increase in loci was between using  <dig> million to  <dig> million reads for both c <dig> and c <dig>  similarly, the c <dig> assembly shows a decrease in the number of transcripts per read , while the c <dig> assembly shows an almost constant number of transcripts per read. the number of transcripts increases while the number of loci tend to level off and this means the number of transcripts per locus always increases with more reads . that is, on average, more variants will be generated with more reads even though some of these are likely due to noise. while the trinity assembly more closely matches the trends for transcripts per read of the c <dig>  the “components”  remain close to a unit ratio, suggesting that most components have only one associated sequence.

assembly of invertebrate transcriptomes
transcriptomes across a broad range of taxa were assembled as with the mouse and statistics of the largest assemblies are presented in table  <dig>  the stated gc content of the mouse genome is 42% while a subset of conserved genes showed a much higher value of  <dig> %  <cit> . interestingly, for all assemblies except for mouse, the average gc content of the assembled contigs was lower than that of the raw reads , suggesting either that certain genes contribute much more to the overall gc content of the library or that biases can be introduced from the assembly.

summary statistics of the largest transcriptome assembly for each organism.

for three of six samples , only select tissues were used for rna extraction while the rest were whole body  number of transcripts;  mean length;  median length;  n <dig> of the assembly;  number of loci;  loci per million reads;  transcripts per million reads;  transcripts per locus. purple - c. multidentata; blue - h. californensis; teal - p. robusta; green - d. gigas; yellow - h. imbricata; red - s. similis. for comparison, c <dig> mouse data are shown in gray.

four of the animals showed modest gains in mean, median and n <dig> with more reads , while p.robusta and h.californensis nearly doubled from the fewest to the most reads . most of the transcript-length increase occurred before  <dig> million reads, suggesting that adding more reads did not produce longer sequences beyond that threshold, or that they became longer at the same rate that new, short transcripts were generated. as with the mouse samples, transcripts were added continually with more reads . compared to the mouse, on average these six animals all had more transcripts per locus . it is unclear why this would be the case, though the c <dig> assembly had the fewest number of transcripts overall for all numbers of reads. the most pronounced gains in loci happened within the first  <dig> million reads, particularly for p.robusta and h.californensis . gains in loci tended to level out between  <dig> and  <dig> million reads, suggesting most genes  were assembled by  <dig> million reads.

a very high number of transcripts for c.multidentata  led to the lowest mean, median, and n <dig>  the number of removed, low-quality reads is comparable in this sample to others, so low quality is unlikely to be the cause. as two sets of reads were combined into a whole animal, this may have created artifacts. however, another c.multidentata siphosome sample produced assemblies with large numbers of relatively short sequences . one possible explanation is that siphonophores have continuously developing differentiated zooids  <cit> . these zooids have specialized functions which are in some ways analogous to organs, and a whole organism can contain multiple developmental stages and express a large part of the genome, possibly confounding the assembly process. assemblies of a number other siphonophores  similarly had many short transcripts. we speculate that alternate assembly strategies or very careful dissections might be required for animals in this lineage.

discovery of conserved genes
conserved mouse genes
one approach used to assess genome completeness is to search only for conserved eukaryotic orthologous genes . the current ncbi kog database has  <dig> gene clusters across  <dig> eukaryotes with over  <dig> proteins  <cit> . the kog reference genes did not include mouse sequences, and this provided an opportunity to test predictions about de novo transcriptome quality while still having a reference in the end to confirm the reliability of the sequences. for each kog, the transcripts were aligned against the reference kogs with tblastn, and the best coding sequence was kept. the putative proteins were classified by length relative to the range of sizes of the reference kogs. the size range allowed some flexibility, as  <dig> mouse proteins were larger than the longest reference protein for that kog, and  <dig> were shorter than the shortest reference protein. finally the proteins were aligned with blastp against reviewed mouse proteins in uniprot to determine accuracy. one protein was unreviewed . for this test, trinity and oases are comparable at assembling full-length proteins, though trinity appears to be slightly better at reconstructing canonical proteins .

however, gene duplications present difficulties for such assessments unless one had a priori knowledge of how many copies should be present in the genome. for this study, we also used the subset of eukaryotic kogs containing  <dig> genes from the cegma pipeline which were identified as single-copy orthologs in most genomes  <cit> . almost one third of these kogs are involved in processes like transcription and translation and were expected to be expressed in many tissues. trinity and oases with a lower coverage cutoff of  <dig> found similar numbers of kogs at much lower numbers of reads  than compared to the c <dig> assembly. also more kogs were found within expected length much faster with c <dig> than with the higher cutoff of  <dig>  and the trinity assembly outperformed both of these. these results suggest that it is better to have a lower cutoff and assemble more sequences. likewise, the trinity assembly had more transcripts than c <dig> and were shorter than those in c <dig>  yet more kogs were found with fewer reads and more coding transcripts were correctly assembled at greater numbers of reads. however, for the oases assemblies this had remarkably little effect on the number of correct canonical proteins that were found . although there is some overestimation, no protein designated as too short was ever correct. regarding the fate of the other full-length proteins, for c <dig> at  <dig> million reads,  <dig> kogs were found within the expected range, though only  <dig> were correct. eight of the  <dig> kogs had only  <dig> mismatch in the amino-acid sequence compared to the reference protein which could be due to errors, splice variants, tissue-specific modifications or alleles. the remaining kogs had at least two amino-acid changes but were within the size range. thus for the mouse, the size range was a reliable predictor of true full-length proteins.

conserved invertebrate genes
we then examined our invertebrate transcriptomes for completion using the same set of kogs. there was a clear, qualitative difference between whole-body organisms  and dissected tissues . c <dig> mouse data are included for reference. for whole-body transcriptomes, over 90% of the kogs were detectable at  <dig> million reads, yet the number of within-length kogs went down with higher numbers of reads past  <dig> million. this could be caused if proteins declared to be within-range were longer than the true protein due to mis-assembly causing addition of pieces, or if the true protein became mis-assembled with addition of noisy reads. in nearly all of our assemblies, it was the latter: mis-assembly of the putative protein which generated stop codons. c.multidentata  was again exceptional, as the number of within-length kogs increased more slowly with addition of more reads than the other two whole-body animals  and only decreased after  <dig> million reads rather than  <dig> million.

for dissected-tissue transcriptomes , the rate of discovery of kogs was much slower with between 63% and 81% of kogs detectable at  <dig> million reads . this was not surprising since those genes may not be highly-expressed in all tissues and it is likely tissue-specific genes account for the bulk of the assembly at low numbers of reads. isolated tissues may express fewer universal kogs that we selected in our test, and we expected that other abundant transcripts should mis-assemble at high numbers of reads in that tissue. however, the dissected-tissue transcriptomes had longer transcripts and fewer loci, suggesting this was not the case. since whole-animal transcriptomes include all tissues, a greater proportion of the genome is expressed so coverage of any given transcript or splice-variant is proportionally much lower. the length saturation patterns appear to be different between whole-animal and tissue transcriptomes. however, using conserved genes as a metric, there appears to be limited benefit of sequencing beyond  <dig> million reads.

mis-assembly at high numbers of reads
kogs with single-exon coding sequences in the mouse were examined for mis-assembly. to increase the number of genes examined, another set of kogs from only metazoans  was used. the kog database at ncbi contained  <dig> clusters common to cdh. again, only genes that were annotated as single copy in all three animals were used, leaving a final set of  <dig> kogs specific to metazoans. these combined sets of  <dig> had  <dig> genes in mouse which were presumed single-copy and annotated in ncbi to have a single-exon coding sequence . at  <dig> million reads,  <dig> genes in c <dig> had alternate erroneous coding sequences: nat <dig>  chmp1b1/did <dig>  ftsj . the sequence of chmp1b <dig> was never assembled correctly for any number of reads and the best version was missing  <dig> amino acids at the n-terminus including the start codon. only nat <dig> had extraneous coding sequence in c <dig>  suggesting that such errors can be controlled by limiting read count as well as increasing k-mer coverage thresholds.

while some mis-assemblies can occur with more reads, overall this is not a problem, as shown by the curves in figures  <dig> and  <dig>  however, select cases of mis-assembly of the mouse genes are shown in figure  <dig>  alars  presents an example of the optimal scenario, whereby the protein is not found at all with few reads, but then pieces come together with the addition of more reads until the final protein is correctly assembled. the majority of proteins follow this trend. 2-ogdh shows an unusual oscillation between the reference protein and alternate forms. ef <dig> is assembled correctly with few reads, then errors accumulate as more reads are added. from this, it cannot be assumed that the largest set of reads will produce the best contigs. schulz et al. indicated that between  <dig> and 20% of oases transcripts had some degree of misassembly  <cit> . this value was found to correlate with the smallest k-mer used in assembly and the authors suggest using larger k-mers if problems arise due to chimeric transcripts. thus if using more reads, it may be advisable to use larger k-mers or a higher static coverage cutoff.

CONCLUSIONS
in this study, number of whole animals and tissues from non-model organisms and one mouse organ were assembled and the completeness was assessed using a set of conserved genes. additionally, a comparison was made between two high-performing assemblers with respect to the mouse data. oases required much greater memory usage while trinity had much longer run times . both trinity and oases perform comparably at assembling conserved genes across a large set, indicating that the saturation depth is not greatly affected by assembler choice.

overall, these results suggest that for whole-body transcriptomes and individual organs or cells,  <dig> and  <dig> million reads are sufficient for mrna level coverage, respectively. for the read length used in this study, that would produce 2- <dig> gigabases of sequence. it should be noted that the mouse data consisted of shorter reads than used for the invertebrates, but this did not appear to have substantial effect as this difference was only between 75bp reads and 100bp reads. assembly errors are evident in whole-body transcriptomes after  <dig> million reads, and the average length appeared to level off at the same depth. presumably this depth would apply for studies of differential expression as well, as the highly expressed transcripts should be present and distinguishable at that sequencing depth. in our experience, we find it is optimal to acquire between  <dig> and  <dig> million reads, and then sub-sample up around  <dig> or  <dig> million. this approach reliably assembles nearly all proteins of interest. there are still observable differences between assemblies, although some of these differences may ultimately be due to variations in rna quality or properties of the animal.

