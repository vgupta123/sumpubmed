BACKGROUND
chromosomal aberrations often occur in solid tumors: tumor suppressor genes may be inactivated by physical deletion, and oncogenes activated via duplication in the genome. gene dosage effect has become particularly important in the understanding of human solid tumor genesis and progression, and has also been associated with other diseases such as mental retardation  <cit> . chromosomal aberrations can be studied using many different techniques, such as comparative genomic hybridization , fluorescence in situ hybridization , and representational difference analysis . although chromosome cgh has become a standard method for cytogenetic studies, technical limitations restrict its usefulness as a comprehensive screening tool  <cit> . recently, the resolution of comparative genomic hybridizations has been greatly improved using microarray technology  <cit> .

the purpose of array-based comparative genomic hybridization  is to detect and map chromosomal aberrations, on a genomic scale, in a single experiment. since chromosomal copy numbers can not be measured directly, two samples of genomic dna  are differentially labelled with fluorescent dyes and competitively hybridized to known mapped sequences  that are immobilized on a slide. subsequently, the ratio of the intensities of the two fluorochromes is computed and a cgh profile is constituted for each chromosome when the log <dig> of fluorescence ratios are ranked and plotted according to the physical position of their corresponding bacs on the genome  <cit> . different methods and packages have been proposed for the visualization of array cgh data  <cit> .

each profile can be viewed as a succession of "segments" that represent homogeneous regions in the genome whose bacs share the same relative copy number on average. array cgh data are normalized with a median set to log <dig> =  <dig> for regions of no change, segments with positive means represent duplicated regions in the test sample genome, and segments with negative means represent deleted regions. even if the underlying biological process is discrete , the signal under study is viewed as being continuous, because the quantification is based on fluorescence measurements, and because the possible values for chromosomal copy numbers in the test sample may vary considerably, especially in the case of clinical tumor samples that present mixtures of tissues of different natures.

two main statistical approches have been considered for the analysis of array cgh data. the first has focused many attentions, and is based on segmentation methods where the purpose is to locate segments of biological interest  <cit> . a second approach is based on hidden markov models , where the purpose is to cluster individual data points into a finite number of hidden groups. our approach can be put into the first category. segmentation methods seem to be a natural framework to handle the spatial coherence of the data on the genome that is specific to array cgh. in this context the signal provided by array cgh data is supposed to be a realization of a gaussian process whose parameters are affected by an unknown number of abrupt changes at unknown locations on the genome. two models can be considered, according to the characteristics of the signal that is affected by the changes: it can be either the mean of the signal  <cit>  or the mean and the variance  <cit> . since the choice of modeling is crucial in any interpretation of a segmented cgh profile, we provide guidelines for this choice in the discussion. two major issues arise in break-points detection studies: the localization of the segments on the genome, and the estimation of the number of segments. the first point has lead to the definition of many algorithms and packages: segmentation algorithms  <cit>  and smoothing algorithms  <cit>  where the break-points are defined with a posterior empirical criterion. these methods are defined by a criterion to optimize and an algorithm of optimization. different criteria have been proposed: the likelihood criterion  <cit> , the least-squares criterion  <cit> , partial sums  <cit> , and algorithms of optimization are based on genetic algorithms  <cit> , dynamic programing  <cit> , binary segmentation  and adaptive weigths smoothing . since many criteria and algorithms have been proposed, one important question is the resulting statistical properties of the break-point estimators they provide. note that smoothing techniques do not provide estimators of the break-point coordinates, since the primary goal of the underlying model is to smooth the data, and break-points are not parameters of the model . here we consider the likelihood criterion and we use dynamic programming that provides a global optimum solution, contrary to genetic algorithms  <cit> , in a reasonable computational time.

as for the estimation of the number of segments, the existing articles have not defined any statistical criterion adapted to the case of process segmentation. this problem is theoretically complex, and has lead to ad hoc procedures  <cit> . since the purpose of array cgh experiments is to discover biological events, the estimation of the number of segments remains central. this problem can be handled in the more general context of model selection. in the discussion we explain why classical criteria based on penalized likelihoods are not valid for break-points detection. criteria such as the akaike information criterion  and the bayes information criterion  lead to an overestimation of the number of segments. for this reason, an arbitrary penalty constant can be chosen in order to select a lower number of segments in the profile  <cit> . we propose a new procedure to estimate the number of segments, choosing the penalty constant adaptively to the data. we explain the construction of such penalty, and its performances are compared to other criteria in the results section, based on simulation studies and on publicly available data sets. put together, we propose a methodology that considers a simple modeling, a fast and effective algorithm of optimization and that takes advantages of the statistical properties of the maximum likelihood. our procedure has been implemented on matlab software and is freely available .

RESULTS
comparison of model selection criteria
to show the importance of the choice of the model selection criterion on simple data, we use the results of a single experiment performed on fibroblast cell lines , with one known chromosomal aberration. figure  <dig> shows the resulting segmentations when using the bayesian information criterion, and our criterion. bic leads to an oversegmented profile that is not interpretable in terms of relative copy numbers. our procedure estimates the correct number of segments . this example shows the practical consequences of the use of theoretically unappropriated criteria. this point constitutes the main purpose of the discussion .

numerical simulations are performed to study the sensitivity of different criteria to varying amounts of noise. the simulation design is described in the methods section. we compare four different criteria: the bayesian information criterion, two previously described criteria  <cit> , and the criterion we propose, in their ability to estimate the correct number of segments. two configurations were tested, for a true number of segments k* =  <dig>  in the first situation, the segments are regularly spaced with a jump of the mean of  <dig> , whereas in the second case, the segments are not regularly spaced and the differences of means vary between d =  <dig> and d =  <dig>  . the first result is that bic overestimates the number of segments, whatever the noise and the configuration . on the contrary, previously described criteria  <cit>  tend to underestimate the number of segments when the noise increases, whatever the configuration. these results suggest that those two criteria "prefer" to detect no break-point as the noise increases, leading to possible false negative results.

the behavior of the criterion we propose is different. it seems to be more robust to the noise, as it will give a number of segments that is close to the true number. in particular, the irregular configuration presents a segment of small size  that could be interesting to detect in the case of array cgh profile . since the previously described criteria  <cit>  tend to underestimate the number of segments, this particular region would not be detected. on the contrary, the adaptive criterion will be able to detect it, even if the noise is important, since it selects a constant number of segments close to the true number whatever the noise. these simulation examples perfectly illustrate the capacity of an adaptive criterion to find a reasonable number of segments even in configurations where the profile is not very separated.

we also compare the performance of our criterion and of the arbitrary criterion  <cit>  on breast cancer cell lines. figure  <dig> shows the resulting segmentations on chromosomes  <dig> and  <dig> of the bt <dig> cell line . as previously mentioned, the arbitrary criterion  <cit>  selects a lower number of segments compared to the adaptive criterion, and we note that interesting regions are not detected . since the aim of array cgh experiments is to discover unknown chromosomal aberrations, the use of an adaptive criterion seems more appropriate in this context since it allows the identification of regions that seem biologically relevent.

the second simulation-based result concerns the ability of dynamic programming to locate the break-points at the correct coordinate, given different amounts of noise . in the regular configuration , simulation results show that dynamic programming perfectly localizes the break-points when the variability of the noise σ <dig> is low regarding the jump d of the mean. if d/σ =  <dig> the estimated probability to localize the break-points at the correct coordinate is  <dig>  and this probability deacreases with the noise . the effect of additional noise is to widden the zone of estimation, but the estimated break-points remain close to the true break-points. if the true break-point is located at t*, the estimated break-point stays in the interval t* ±  <dig>  in the irregular configuration, additional noise has similar effects on the break-point's positioning, but the probability to correctly estimate a break-point depends on the jump of the mean between two segments. in the irregular case, figure  <dig>  at position t =  <dig> the difference of mean is d =  <dig>  and the probability to locate the break-point at the true coordinate is higher than  <dig>  for any additional noise. on the contrary, at position t =  <dig> where the different of mean equals d =  <dig>  the probability to correctly locate the break-point decreases dramatically with the noise . this means that dynamic programming is sensitive to small segments that present little differences in the mean regarding the noise. nevertheless, the example on the real data set presented in figure  <dig> shows that using an adaptive criterion with dynamic programming allows for the identification of small regions of putative biological interest as mentioned above. put together, these simulation results show that the adaptive method selects the good number of segments even in the presence of important noise, and that when this number is selected, dynamic programming is able to correctly localize the break-point. in addition to its ability to locate precisely the break-points, it is important to notice that dynamic programming provides a global optimum of the likelihood that is required for any model selection procedure to select the number of segments, compared to genetic algorithms  <cit> .

segmentation models in the gaussian framework
the cgh profile is supposed to be a gaussian signal. in a segmentation framework, two types of changes can be considered: changes in the mean and the variance of the signal, or changes in the mean only. let us define model  where each segment has a specific mean and variance  <cit> , and model , where the variance is common between segments  <cit> .

since both models can be used, it is important to explore their behavior in order to know which model is the best adapted to the special case of array cgh data. we use clinical data obtained from primary dissected tumors of colorectal cancers . figure  <dig> presents the results of segmentations for three experiments obtained with the two models  and  when our criterion is used to estimate the number of segments. the main result of this comparison is that the number of segments is higher using model  compared to model . this behavior of model  could be interpreted as a trend to divide large segments into smaller parts, in order to maintain the variance homogeneous between segments. this leads to a more segmented profile, maybe more precise, but that may be more difficult to interpret in terms of relative copy numbers. nevertheless, as model  allows the exploration of segments with one observation, it will be more efficient for the identification of outliers, as shown in figure  <dig> .

discussion
the definition of an appropriate penalized criterion has been an issue for previous works using segmentation methods for array cgh data analysis  <cit> . in this section, we explain the specificity of model selection in the case of process segmentation, in order to give further justification to the inefficiency of classical criteria to select the number of segments, as shown in the results section.

estimating the number of segments via penalized likelihood
when the number of segments is known, the maximization of the log-likelihood  gives the best segmentation with k segments . in real situations this number is unknown, and one has to choose among many possible segmentations. the maximum of the log-likelihood  can be viewed as a quality measurement of the fit to the data of the model with k segments, and will be maximal when each data point is in its own segment. therefore selecting the number of segments only based on the likelihood criterion would lead to overfitting. furthermore, the number of parameters to estimate is proportional to the number of segments, and a too large number of segments would lead to a large estimation error. a penalized version of the likelihood is used as a trade-off between a good adjustement and a reasonable number of parameters to estimate. it is noted



where pen is a penalty function that increases with the number of segments, and β is a constant of penalization. the estimated number of segments is such as :



it is crucial to notice that the criterion which is penalized should provide the best partition of k-dimensional, ie for a fixed k the criterion has to be globally maximized to ensure convergence of the break-point estimators to the true break-points  <cit> . this optimum is provided by dynamic programming, but not by other algorithms  <cit> .

choice of the penalty function and constant
classical penalized likelihoods use the number of independent continuous parameters to be estimated as a penalty function. even though those criteria are widely used in the context of model selection, theoretical considerations suggest that they are not appropriate in the context of an exhaustive search for abrupt changes.

let us focus on the penalty function in a first step. table  <dig> provides a summary of different penalties. for classical information criteria, such as the akaike information criterion and the bayes information criterion, the penalty function equals to 2k  for a heteroscedastic model with k segments. penalized criteria have already been used in the context of array cgh data analysis to estimate the number of segments  <cit> . in addition to the 2k parameters, they implicitly consider that the break-points are also continuous parameters, leading to a new penalty function pen = 3k -  <dig>  which considers k -  <dig> break-points. nevertheless, the characteristic of break-point detection models lies in the mixture of continuous parameters and discrete parameters that can not be counted as continuous parameters, since the number of possible configurations for k segments is finite and equals    <cit> .

this leads to the definition of a new penalty function adapted to the special context of the exhaustive search of abrupt changes. this function  is proportional to the number of continuous parameters, but is also proportional to a new term in  that takes the complexity of the visited configurations into account. it is written pen = 2k, where c <dig> and c <dig> are constant coefficients that have to be calibrated using numerical simulations. since aic and bic and the criterion proposed in  <cit>  do not consider the complexity of the visited models, they select a too high number of segments. the second term of the penalty is the penalty constant β. this term is constant in the case of aic and bic , and contributes to the oversegmentation as mentioned above. this can lead to an empirical choice for the constant, in order to obtain expected results based on a priori knowledge. for this reason, an arbitrary penalty constant can be chosen for the procedure to select a reasonable number of segments . instead of an arbitrary choice for this constant, β can be adaptively chosen to the data  <cit> . furthermore, when the number of segments is small with respect to the number of data points , the log-term can be considered as a constant  <cit> . the author rather suggests to use the penalty function pen = 2k and to define an automatic procedure to choose the constant of penalization β adaptively. we explain the estimation procedure for the penalty constant in the methods section.

the power of adaptive methods for model selection lies in the definition of a penalty that is not universal . this means that the dimension of the model is estimated adaptively to the data. the efficiency of such method has been shown on simulated data as well as on experimental results , and adaptive model selection criteria seem to be very appropriate for array cgh data analysis.

choice of modelling for array cgh data
since the choice of modeling affects the resulting segmentation, it is crucial to provide guidelines for their use. this can be done with the interpretation of the statistical models in terms of their biological meaning. the difference between model  and  concerns the modeling of the variance: model  assumes that the variability of the signal is organized along the chromosome, whereas model  specifies that the variance is constant. since it has been shown that the vast majority of clones all had the same response to copy number changes in the aneuploid cell lines  <cit> , the use of model  would be justified regarding this experimental argument.

outliers seem to be a major concern in microarray cgh data analysis. for instance, if only one bac is altered whereas its neighbors are not, the conclusion could be either that it is biologically relevant, or that the signal is due to technical artefacts. replications are crucial in this situation, as well as secondary validations. an other possibility could be that the bac is misannotated: if the ratio is plotted at the wrong coordinate on the genome, it will appear as an outlier, when it is not. the importance of outlier identification is another argument in favor of model , that can detect changes for one data point, whereas with model  outliers would belong to segments with higher variance.

it has to be noted that classical models used in segmentation methods assume the independence of the data. this may be a reasonnable assumption for bac arrays whose genome representation is approximately  <dig> bac every  <dig>  mb  <cit> . nevertheless, a new generation of arrays now provides a tiling resolution of the genome  <cit> . the overlapping of successive bacs could lead to statistical correlations that will require developments of new segmentation models for correlated processes.

CONCLUSIONS
microarray cgh currently constitutes the most powerful method to detect gain or loss of genetic material on a genomic scale. to date, applications have been mainly restricted to cancer research, but the emerging potentialities of this technique have also been applied to the study of congenital and acquired diseases. as expression profile experiments require careful statistical analysis before any biological expertise, cgh microarray experiments will require specific statistical tools to handle experimental variability, and to consider the specificity of the the studied biological phenomena. we introduced a statistical method for the analysis of cgh microarray data that models the abrupt changes in the relative copy number ratio between a test dna and a reference dna. we discuss the effects of different modelings that can be used in segmentation methods, and suggest the use of a model that considers the homogeneity of the signal variability based on experimental arguments and regarding the specificity of array cgh data.

the main theoretical issue of array cgh data analysis lies in the estimation of the number of segments that requires the definition of appropriate penalty function and constant. we define a new procedure that estimates the number of segments adaptively to the data. this method selects the number of segments with high accuracy compared to previously mapped aberrations, and seems to be more efficient compared to others proposed to date. the use of dynamic programming remains central to localizing the break-points, and the simulation results show that when the good number of segments are selected, the algorithm localizes the break-points very close to the truth. assessing the number of segments in a model is theoretically complex, and requires the definition of a precise model of inference. to that extent, microarray cgh analysis not only requires computational approaches, but also a careful statistical methodology.

