BACKGROUND
mathematical modelling is the new key tool in systems biology  <cit> : there now exist multiple differential equation models for a wide range of biological phenomena, sometimes over multiple time and spatial scales, from the molecular to the systems level. depending on the system under study, models  <cit>  can be in the form of discrete-time or continuous-time ordinary differential equations , functional  differential equations , stochastic differential equations , partial differential equations , or even hybrid models  to model genetic networks. central to all of these models are the inherent non-linearities that real life systems exhibit, which makes their analysis more complicated.

models are usually developed based on physical principles which constrain their structure; typically these models contain parameters that are identified  using appropriate sets of observed experimental data and computational techniques  <cit> . no two experiments will ever yield exactly the same data even when carried out under a strict protocol, but one hopes that the model can explain the data within experimental error – hence it is claimed that the model is valid. in fact, to validate a model would require an infinite number of experiments and data  <cit> . models that have many free parameters are usually prone for invalidation, as it is possible to fit a wide range of system responses if there are enough free parameters in the model. the claim that a model is valid is refuted when someone undertakes a different experiment which yields a new data set that cannot be explained by the model. given this new data set, the model can be invalidated, by exhibiting the discrepancies between observed experimental data and the model behaviour. model invalidation forms an integral part of the model development cycle, in which experiments are specifically designed in order to provide data sets that can be used to invalidate a subset of the models  <cit> . also, model invalidation may help in identifying where parameters and/or system structure should be refined to reveal where the model is just incorrect.

model validation has been studied from a control engineering perspective in  <cit> , where the problem was posed in the robust control context: given experimental input/output data and a model p, does there exist an external input  and set of system perturbations  such that the observed data are produced exactly? here, the uncertainty in the model Δ is bounded and of a particular structure. moreover, in  <cit>  a methodology for validating continuous time models using finite experimental data is proposed.

several other approaches have been developed using a statistical framework based on graphical residual analysis. very frequently the r <dig> statistic is used, which is a measure of the difference in response between model behaviour and data. cross-validation can be used to test the capability of a model, constructed using a training set, to represent an unseen subset of the data, usually called the 'validation set'  <cit> . in this paper, we approach model invalidation from a different perspective. we first seek to answer the question 'how bad is the best model for this data set?', ie, to evaluate how good the model structure is to represent the experimental observations. to answer this question, recall that in parameter identification one seeks to find the point in the allowable parameter set that minimizes an objective function which encodes the requirement that the error between model evolution and data is small.  while the objective is to identify a parameter choice in which the error is small, one should also pay attention to the dual question of what is the minimum of the possible error that could be obtained with the particular model structure. if this minimum error is unacceptably high, such a model may not be adequate to represent the experimental observations and a different one should be sought. in this paper, we develop an approach that can calculate a lower bound on this minimum error, which is important information to the user as for the validity of the model. for this purpose we use ideas based on the sum of squares decomposition of multivariate polynomials  <cit> .

we then provide a methodology for discrete-time and continuous-time model invalidation using ideas for real algebraic geometry and semidefinite programming. the aim is to construct functions/certificates that provide proof of the fact that the model can never represent an experimental data set. we stress that simulation cannot be used for this purpose, unless the data is certain, the model size is small and its structure, initial conditions and parameters are fixed. the reason for this is that as models become more complex  exhaustive simulation for model invalidation becomes computationally prohibitive – as well as being inconclusive. our methodology uses semidefinite

programming techniques which have been shown to have in the worst case a polynomial time complexity. for this purpose ideas presented in  <cit>  are extended and a different class of functions is introduced. related work appears in  <cit> .

ideas and notions related to model invalidation are reachability and systems verification; several different approaches have been developed in this area. for example, if one considers the experimental data set as the target set in a reachability analysis, then its backwards reachable set  can be used for model invalidation: if the backwards reachable set does not intersect the initial data set then the model can be invalidated. mitchel et al.,  <cit>  propose a computationally tractable method for calculating the backward reachable set by solving the hamilton-jacobi-isaacs partial differential equation.

the structure of this paper is as follows. in the next section we present some background material on the sum of squares decomposition of multivariate polynomials and how it can be obtained. following this, we describe a method for obtaining a bound on the minimum achievable error between data and model structure as well as the methodology for invalidating continuous-time and then discrete-time models. finally, we illustrate our results through a series of examples.

methods
in this section we formulate the way system models will be represented and introduce the sum of squares techniques that will be used in the sequel. we first introduce the following notation:

ℝn n-dimensional real vector field

n number of sampled data points

 sets describing the state space, initial conditions, state values at tℓ and parameter values

x or xk predicted value of value of x ∈ ℝn at time tk from model

 or  experimental data at time tk

  or ith element of the data  taken at time tk

the final three terms have alternate notations as shown above and are used interchangeably where appropriate.

the models we consider are autonomous, in the form of ordinary differential equations  whose n-dimensional vector fields satisfy appropriate smoothness conditions in order to ensure that given an initial condition there exists a locally unique solution.

for x ∈ ℝn, let

   

be a candidate system model, where p ∈  ⊂ ℝm is a vector of parameters, such as kinetic constants etc. an alternative representation which is sometimes used is a discrete-time ode of the form:

   

for the rest of the paper, we assume that either a model of the form  or of the form  is being considered, in which f is a polynomial function of its arguments. experimental data  for i =  <dig> ..., n are provided, where  are the data points. the sets  encode the uncertainty in the data because of experimental error, etc. we will assume in the sequel that these sets are semi-algebraic, ie, they can be described by a finite set of polynomial inequalities. for example, if  for i =  <dig> ..., n where  refers to the ith element of the experimental data taken at time t <dig> then we obtain the n-dimensional hypercube:

   

note that when it is clear that we are talking about the whole vector we drop the superscript notation. in this paper we make repeated use of the notion of sum of squares  polynomials. a polynomial p, with real coefficients, where y ∈ ℝn, admits an sos decomposition if one can find other polynomials q <dig> ..., qm such that

   

where the subscripts denote the index of the m polynomials. if p is sos, it can be easily seen that p ≥  <dig> for all y, which means that p is non-negative. polynomial non-negativity is a very important property  which is however very difficult to test . the existence of a sos decomposition is a powerful relaxation for non-negativity – in fact, it can be verified in polynomial time. the reason is that p being sos is equivalent to the existence of a positive semidefinite matrix q  and a chosen vector of monomials z such that

   

a proof of this can be found in  <cit> . this essentially means that the sos decomposition of p can be computed using semidefinite programming. software capable of formulating and solving the problem has been developed – sostools  <cit> . all examples in this paper have been solved with sostools and sedumi  <cit> , a semidefinite programming solver. semidefinite programming has been used in the past for classification of complete parameter regions, see  <cit>  for examples relevant to systems biology.

we now turn to the problems we wish to address. first, if a data set is provided, we develop a method for establishing how well the model structure, irrespective of the particular parameter values that one could choose, can represent the data. this can provide an indication for the quality of the model under consideration. consequently, we consider the problem of model invalidation for continuous and discrete-time systems.

how bad is the best model?
we begin this investigation by highlighting the link between system identification and model invalidation. the question we set out to answer is "given experimental data, what is the least error one can expect between the data and predictions from a model with the best parameter choice within the allowable parameter range?" most system identification questions try to find the best parameters in order to minimize an objective function of the error between model predictions and data, while the question we are asking here is dual to that: 'how bad is the best model, for all allowable parameters?' if the error is large, then this could indicate that the model structure may be inappropriate and one may want to invalidate the model and repeat the system identification cycle.

for the continuous time system , the scope of system identification is to find

   

where p* denotes the 'best' parameters and ||·|| denotes a norm on ℝn  and n the number of data points. the term  is many times obtained by simulation, but an appropriate approximation can also be used . note that if the model is already in discrete-time, ie, its dynamics are described by , and if data are taken at the same discrete update points then the integral term does not need to be approximated. either way, if this approximation results in an expression affine in the parameters p, and  then p* can be obtained via convex optimization . in this case, a simple gradient descent algorithm can be used to find the  minimum.

however, when the parameters do not enter in such a manner or when the experimental measurements are uncertain, then finding p* becomes more complicated and locally  optimal parameter estimates may be obtained using optimization methods.

instead, in this paper we develop a methodology for obtaining a lower bound on this error, ie, for finding γ such that

  

which can be used as an indication to whether the model structure that we are trying to fit is suitable or not. the reader is referred to the discussion section for a description of the meaning of the value of γ. for this purpose we can use a method to approximate the integral term by setting f, p) = f, which amounts to euler discretization of the ode , and then the left hand side of the equation above becomes

  

where f and g are polynomial functions of the parameter. note that other discretization methods can also be used. obtaining γ can be done efficiently by solving the following optimization problem:

  

the optimal value, p*, where p* ∈ ℝm, can be obtained by the following simple procedure: substitute γ into the expression f - γg and compute its sos decomposition using sostools. this gives a vector of monomials z and a positive semidefinite matrix q as per . applying some linear algebra we obtain a set of m simultaneous polynomial equations with m unknown variables in the form of , setting the right hand side of the system of equations to zero, i.e. q <dig> = q <dig> = ... = qm =  <dig> and solving produces the optimal parameter set p*.

we note that as the number of data points increases, so will the size of the error bound γ and a normalising factor should be introduced to facilitate comparison. the simplest method for achieving such a normalisation is to divide γ by the number of points, n. this will avoid favouring models fitted on smaller data sets to those fitted on larger ones.

when obtaining a bound on the error from the model it is unavoidable that the dataset will contain some uncertainty due to experimental error. ignoring error in the data can be thought of as searching for a nominal model, while an appropriately defined parametric uncertainty accounts for the experimental error, a technique that underlies modern robust control theory  <cit> . if an estimate of the error in the data is known a priori then it is possible to include this in the identification formulation. the problem now reduces to finding a model that minimizes γ while ensuring that it accounts for the uncertain data. a model satisfying these conditions is said to be valid for that dataset and should have γ =  <dig> 

model invalidation
we now turn to the problem of model invalidation given experimental data. given the data points  for i =  <dig> ..., n where  and a model of the form  or  with parameters that can either be fixed or p ∈ , our task is to show that the model can not represent the data. note that in order to invalidate a model, one data point is enough apart from the initial time t <dig> – assume this has been chosen to be at tℓ where ℓ ∈ { <dig> ..., n}. the aim is to show that this experimental observation could not have arisen from the set of models that are being considered. in cases where it is not clear which point ℓ ∈ { <dig> ..., n} to choose, it is suggested that the point with the largest residual  is chosen. as already mentioned, the sets  and  are assumed to be described by polynomial inequalities, eg,  where gi are polynomial functions.

continuous-time case
for invalidating nonlinear continuous-time models with parametric uncertainty given experimental data, we can use a method similar in concept to that of constructing a lyapunov function to establish equilibrium stability: lyapunov functions ensure the stability property by making sure that trajectories do not escape their sub-level sets. in  <cit>  the related concept of barrier certificates is introduced. these are functions of state, parameter and time, whose existence proves that the candidate model is invalid given a parameter set and experimental data, by ensuring that the model behaviour does not intersect the set of experimental data. in this paper we describe some practical aspects of using barrier functions and develop a parametrization that is more efficient for practical applications. these barrier certificates can be constructed efficiently using semidefinite programming and sostools.

consider a system of the form  and assume that . given this information, if it can be shown that for all possible system parameters p ∈  the model cannot produce a trajectory x such that x ∈ , x ∈  and x ∈  for all t ∈ , then the model and parameter set are invalidated by .

theorem  <dig>  given the candidate model  and the sets , suppose there exists a real valued function b that is differentiable with respect to x and t such that

  

then the model is invalidated by . we refer to the function b as a barrier certificate.

the above theorem requires the construction of a function that satisfies certain non-negativity conditions, which is not easy. if we relax these constraints to sums of squares, then this computational relaxation can be used to construct barrier certificates. at the same time, this imposes that b will be polynomial in the states, parameters and time variables. this may make sense for states and parameters, but a more natural choice for the dependence of b on time is of the form e-λt for some λ. this is because the barrier function b resembles in shape system trajectories and therefore this choice for time-dependence is more appropriate. the value of λ can be chosen, eg, based on the characteristic time constant of the system. denoting μ = e-λt for fixed λ, we seek a b as a polynomial in  with real coefficients c <dig> ..., cm:

   

where bj's are monomials in x, p, μ. the sos program is then a search for the coefficients cj's such that the conditions stated in theorem  <dig> are met. note that the time re-parametrization will change the derivative condition in the theorem above, as we will see in the sequel, to:

  

where  and .

for concreteness, define the sets  and  as

   

   

   

   

where the g's are polynomials written in the format described by .

the first condition in theorem  <dig> states that b - b must be positive. as a sos decomposition provides a polynomial that is nonnegative and not positive, a small positive scalar " must be subtracted before the constraint is asked to be a sos.

in order to include the constraints in the formulation, we need to adjoin them to the two conditions given in theorem  <dig>  this can be done using lagrange-type multipliers, for the first condition we denote the multipliers with an m: mp, i for all gpi, m <dig>  i for all g <dig>  i and mℓ, i for all gℓi, while the multipliers for the second condition are denoted with by an n. all of the multipliers have to be nonnegative , as they are used to adjoin inequality constraints to the optimization problem. the sos optimization problem can now be stated as:

   

   

if a solution to the optimization problem described by – can be found that satisfies all the constrains – then a barrier certificate b has been constructed which proves that the model is invalid as per theorem  <dig>  a solution to the optimization problem will provide the coefficients  which make expressions – and mp, i, m <dig>  i, mℓi, np, i, nx, i for all i and nt sums of squares and invalidates the model and the parameter set by . in the final section results are shown for barrier certificates constructed using the ordinary and new time parametrization method.

discrete-time case
given the discrete-time model of the form  one can develop a relationship between xℓ and x1:

  

which we write in short

  

since f was assumed to be polynomial, fℓ is also polynomial and the problem of model invalidation can be formulated as the emptiness of the following set:

  

the sets ,  and  are assumed to be described by polynomial inequalities as described in the previous section, in which case invalidation is equivalent to

  

where h = fℓ - xℓ. testing emptiness of the latter can be done in many ways, the simplest of which is to construct multipliers σi that are sum of squares and λ polynomial so that

   

to see why this is so, suppose there is a point in the set we want to show is empty, ie, a point  that satisfies the inequalities gi ≥  <dig> and h =  <dig>  then the above condition says that something negative is a sos, a contradiction. in this case the multipliers form a certificate that the set is empty, ie, that the model is invalidated by the data. these certificates can be constructed via convex optimization and sum of squares programming, as explained in the previous section. the above formulation is a special case of positivstellensatz, a central theorem in real algebraic geometry – for more details see  <cit> .

RESULTS
we now present examples that illustrate how the methods described in the previous section can be used. we first consider a simple biochemical reaction network, that of an enzymatic reaction with product degradation. the second example demonstrates the methodology previously described for invalidating a discrete time model.

continuous-time: biochemical reaction network
a simple biochemical reaction network with a product degradation term is given by:

   

the enzyme, e, binds reversibly with the substrate, s, to form a complex es which then forms a product p and releases the enzyme e. the parameters on the arrows denote the rate constants and are used to quantitatively describe the speed of a reaction in a given direction. denoting the concentration of the reactants by lowercase letters where  denotes concentration and e = , p = , s =  and c = , the law of mass action results in the system of equations:

   

   

   

where e can be calculated from the conservation relation e = e -c. the above system is initialized from s = s <dig>  c =  <dig> and p =  <dig>  the reaction rates and e are assigned values as described later in this section.

suppose a model for the above system has been proposed that takes the form

   

   

this model structure is very commonly used in practice to describe the chemical reaction network described by . in fact, the appropriate parameter choice should be v = k2e <dig>   and λ = k <dig> if a singular perturbation of the c dynamics in  –  were allowed.

the original model  is simulated to generate experimental data, for the purposes of this example. the parameters are fixed to k <dig> =  <dig> , k- <dig> =  <dig> s- <dig>  k <dig> = 56s- <dig>  k <dig> = 2s- <dig> and the initial condition to e <dig> =  <dig>  nm. we first follow the approach described previously to find a lower bound on the error between the model predictions and experimental data, and then use the continuous-time approach that we described to invalidate this model.

the error between the model predictions and the experimental data is given by:

   

here, the following  discretized version of the model described by – is used

  

where Δt denotes the discretization time step, which is equal to the sampling rate of the experimental data. it should be noted that the euler discretization has been chosen for simplicity. there are numerous alternative methods of discretizing the system .

a lower bound on the minimum value that the expression  can achieve can be found using the sum of squares decomposition, as described in the methods section. reformulating  to remove the denominator we require that

   

where n is the number of samples,  and

  

the optimization objective is now to maximize γ, which represents the lowest bound achievable for the difference between model predictions and observed data. once such a value for γ has been obtained and is exact, we can substitute this back into the original equation and solve for p* as described in the methods section.

we used the method described above to find that the lower bound on the error between data and the predictions of the model given by – is γ =  <dig> . this was done using nine data points and the optimal parameter values that give this error are k =  <dig> , v =  <dig>  and λ =  <dig> . the behavior of the system for this set of parameters is shown in figure  <dig>  plotted against the experimental data. there, it is evident that the model has captured adequately the dynamics of the system – the fast transient responses have been reflected in the model data.

we now focus on the validity of the model given the parameters obtained. for this, a second data set is simulated for the purpose of invalidation. the new experimental data is obtained by simulating model  in the same manner as before with k <dig> =  <dig> - <dig>  k- <dig> =  <dig> s- <dig> and e <dig> =  <dig>  nm with the remaining parameters unchanged. we also consider an uncertain model of the system  and we assume that we do not have an exact value for v  but rather we are confident that v lies in the interval v ∈  <cit> . uncertain initial conditions in the substrate concentration  are introduced such that s = s <dig> ∈  and finally, the time point chosen to invalidate the model at is tℓ =  <dig>  where the product data due to experimental error lies in the range p = pℓ ∈ . we choose to attempt the invalidation at this point because the error between the nominal model prediction and the data point is very large as can be seen in figure  <dig>  for high order nonlinear models invalidation through visual inspection of all possible trajectories is not possible. once it has been shown that the model and data do not agree at a single time point the model is said to be invalid. describing the uncertainty in the form of constraints using the notation of – we have:

  

in this instance we look for a barrier certificate of the form b ie, a polynomial function of state, parameter and time. this is a more demanding problem than the nominal system invalidation problem as we are dealing with uncertain data, as well as model uncertainty in the form of unknown initial conditions and parameters. we aim to invalidate the model based on experimental observations from the product of the reaction alone.

the algorithm is able to construct a barrier certificate that is bounded by monomials of maximum degree  <dig> in state and first degree in time and parameter . the existence of a barrier certificate tells us that given the observed experimental data at t =  <dig> and t = tℓ the proposed model is not able to replicate the observed data given these initial conditions and parameter sets, hence it is invalidated. for comparison we now seek to find a barrier certificate of the form b where μ = e-λt. setting λ =  <dig>  the algorithm is able to find a lower order certificate, in this case the barrier certificate is bounded by monomials of the state vector of degree three. we have shown for this example that the time re-parametrization can provide a more efficient means for constructing barrier certificates.

discrete-time: population growth
we now give an example of discrete model invalidation using the formulation shown previously. we use a delayed logistic equation to model single species growth with after effect to generate experimental data:

   

where r ∈ . the model that is being considered ignores the delay and takes the following form:

   

the only free parameter in this model is r and we shall constrain it such that r ∈ .

our aim is to show that given a set of experimental data we can provide an invalidation certificate for our model.

a plot of the behaviour of the two models given the parameter and initial condition ranges is shown in figure . model invalidation amounts to ensuring that at some point the set of possible model behaviours and the experimental data set do not intersect given the uncertainty. examining figure  it is clear that at k =  <dig> and k =  <dig> the intersection of the two sets is empty, however this is also true for k =  <dig>  normally when a model has multiple parameters or is nonlinear of high dimension it may not be possible analytically or through simulation to see where this occurs. this reason alone highlights the importance of our method. for this example we have r ∈  and x <dig> ∈   and at k = <dig>   where x is the experimental data at that point in time and x <dig> =  <dig> ,  =  <dig> . we therefore define the following sets:

   

in this case, the invalidation problem becomes

   

where

  

the test described by  becomes

   

where si are sums of squares and t is a polynomial. the degree of the unknown polynomial t and the sos variables si has to be chosen carefully in order to make sure that the computational effort is acceptable. the above sos program was implemented using sostools and the model was successfully invalidated based on the experimental data at k =  <dig>  the degree of the multipliers was deg{s1} =  <dig>  deg{s2} =  <dig>  deg{s3} =  <dig> and deg{t1} = 1:

  

as we can see from the above, the multipliers certifying the emptiness of set  are large, indicating the sensitivity of the certificate to changes in the model structure and parameters. this should be expected, as the fact that the model can be invalidated using data at k =  <dig> is not immediately obvious by looking at figure . the computational effort needed to invalidate the model increases as data for larger k are used in the invalidation, since the polynomial fk in  becomes more complicated. however, the coefficients of the multipliers become smaller, indicating that the certificate robustly verifies that the model is invalid.

discussion
in this paper we have presented a method for calculating a bound on the error between predictions from an ode model of a particular structure and experimental data. we introduced an error metric γ which is used to quantify a lower bound on this difference. note that γ is actually an error metric in the same way the sum of squares error  is commonly used for fitting models to data: for that reason it should only be used to compare the goodness of fit in different scenarios, eg, comparing competing model structures or the same model structure for different data sets. ideally γ should be as low as possible.

an alternative error metric that may be worth investigating in the future is the maximum difference between the model prediction and all data points. with this method, normalization of γ is not required. this error metric gives us additional information on where the model predictions are weakest, which could be used to automatically select the instance at which to begin the invalidation process.

as mentioned already, our methodology for identification and invalidation uses the fact that we can quantify the level of uncertainty in the experimental data and using this information, develop a parameterized, uncertain model that can be used to describe the data set. in fact, the researcher that conducted the experiments should be able to provide upper and lower bounds on measurements, which can then be used for building the uncertain sets . at the same time, the set  should ensure that the uncertain model can describe every point in the data set that was used to fit it. this set could also be used to encompass all the uncertainty present in the system, including environmental fluctuations, experimental error and measurements that prove difficult to obtain accurate measures for. see  <cit>  for more details on how the set  can be constructed.

traditionally, models are fitted on one data set and a second, unseen data set can be used to either validate it , or invalidate it. assume that we have fitted a model of the form  = f, from some experimental data set x ∈  and obtained the parameter set p ∈  using the error bounding method described in the methods section. given a second data set, we would like to know how well our model describes these new data. this can be formulated as an optimization problem that seeks to find a parameter value , so that the new model  = f can represent the new data set. the size of ||δp|| = ||q - p||, reflects the magnitude of the change required in the parameter values in order for the model to replicate the new data. if ||δp|| is large then this indicates that the original model does not accurately describe the system under study. if ||δp|| is not large and , then we can refine the model parameters appropriately by grouping the two data sets. otherwise, we can formally invalidate the old model and seek a different structure in the dynamics of the system under consideration, using eg, the approach in  <cit> . to this end, ||δp|| provides a measure of the sensitivity of the model parameter set to new experimental data. this cyclic approach will then lead to a better understanding of the system under study as it can also be used to identify what in the model structure fails to replicate the data.

CONCLUSIONS
we have demonstrated how questions related to system identification and model invalidation for biological systems can be answered using optimization, real algebraic geometry and dynamical systems concepts. we have emphasized that formal model invalidation in conjunction with system identification can be used to develop reliable models of biological systems.

finding a lower bound on the error during an identification approach is important information that can not only be used to provide feedback as to the suitability of the model structure chosen, but also can help in identifying the suitable parameters, as demonstrated in the results section. more complicated model structures can also be used, even if these are not polynomial, through appropriate recasting  <cit> .

one of the biggest problems encountered when trying to construct a barrier certificate  and appropriate multipliers  is determining the minimum order of the polynomials that would allow invalidation. for complicated system descriptions, constructing such polynomials can be computationally demanding, but the problem still remains polynomial-time verifiable. this is in contrast to simulation-based approaches that require an exponential number of runs and anyway cannot always answer the invalidation question conclusively. the examples given in this paper aim to introduce the algorithms that can be used for model invalidation and were hence kept simple for clarity-the algorithmic formulations are suitable for more complicated examples.

a future direction will be to investigate what information barrier certificates and multipliers  can provide about the invalidated model – ie, what system structure could be changed in order for such a certificate not to exist.

authors' contributions
ja developed the theory, implemented the examples and wrote the paper. ap conceived the general idea of using sos decompositions for discrete model invalidation and error estimation, and suggested the reparametrization in the continous-time case.

supplementary material
additional file 1
barrier certificate. this pdf file details the barrier certificate obtained by solving the optimization problem – that invalidates the biochemical reaction model.

click here for file

 acknowledgements
ja's work was funded through the life sciences interface doctoral training centre  at the university of oxford. ap's research was supported by a grant from the epsrc . the authors wish to thank elias august for his valuable comments and helpful discussions at various stages of this work.
