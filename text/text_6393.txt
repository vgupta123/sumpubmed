BACKGROUND
we have previously developed a peptide database  as a repository to identify peptide based targeting ligands, peptidomimetic drugs, biological interactors, and imaging agents. some contents of pepbank stem from public data sources; however, the major part was extracted by text mining from medline abstracts. most of the entries are not manually curated, hence, potentially useful data are not automatically extracted from associated texts. consequently, it is often hard to find relevant peptides without manually examining a large number of associated abstracts. this exemplifies a constraint of many modern biological databases and represents a bottleneck in more sophisticated analysis.

biological end-users are often interested in identifying "hits" that relate to a specific disease , a disease process  or a specific application category . using pepbank as a model, we defined categories that are highly relevant to a large number of biological end-users of peptide information . we selected frequently used, broad categories based on  the analysis of the user queries recorded in the database log files, and the corresponding article abstracts, and  feedback from end users. our users predominantly retrieved abstracts falling into categories such as those related to cancer  <cit> , cardiovascular disease  <cit> , diabetes  <cit> , angiogenesis  <cit> , apoptosis  <cit> , molecular imaging related  <cit> , and abstracts that have binding data available  <cit> . while there were many other frequently used categories, we selected a total of seven for the current study. we continue to monitor the types of queries users submit to pepbank and will add new categories as required. this includes adding more specific categories, e.g., related to subfields within oncology research. for each category, any given pepbank entry can be classified as belonging to either of two classes: related or unrelated to the category.

binding data available
our aim was to provide a more natural and interactive way of searching, browsing, and contextualizing entries in pepbank by adding an interactive retrieval heat map that allows to interactively drill down to the relevant entries. the user can add relevance constraints with respect to these common categories and immediately observe their effect on the result set. we sought to address the problem of determining the relevance to each category by integrating collection of expert knowledge and automated classification into the retrieval workflow.

no single annotator can be an expert in all disciplines and approaches published. moreover, disagreement among experts and nomenclature specific to different scientific fields make it non-trivial to create annotation guidelines. community voting is one paradigm well suited to overcome this problem. the central idea of our approach is to leverage annotations contributed by the users and utilize it as feedback to improve automatic classification. in our approach, annotation  is extremely simple for the user and amounts to voting  on whether the current classification is correct when examining an individual entry.

pepbank currently contains nearly  <dig>  peptide sequence entries and, like most other biological databases, is constantly growing. most of its contents are automatically extracted from the literature and lack suitable annotation. even though mesh terms  are available for many abstracts from which the sequences are extracted, they are often not yet available for recent entries. also, they often do not reliably capture the kind of information our users are interested in . even when using community voting, manually labelling all the database contents is human labor-intensive and naturally would lag behind as new entries are acquired. consequently, the amount of contributed data is usually small compared to the amounts of automatically gathered data. machine learning represents an alternative, automated strategy, as it makes it possible to train classification algorithms on abstracts that have previously been labelled by community voting. this approach maximizes the use of community-contributed information by labelling actual entries and at the same time helping to build better models for automatic classification of unlabelled entries.

we envisioned a system that is interactive and incorporates user input. hence, it is desirable for classification to benefit from newly labelled entries as fast as possible while making reasonable use of the available computing resources. to this end, we implemented a system that immediately responds to new labels and builds improved models for automatic classification on the fly while the system is in use.

the system uses a novel combination of heat maps and text classification methods in a web  <dig>  setting. each of these aspects have been well studied:

heat maps are a well-known tool in the life science community, e.g., for the visualization of microarray data  <cit>  and search results  <cit> . they deliver a concise and quick overview of tabular numerical data. extended by a set of controls to add constraints and impose a search order on the visualized data, heat maps can be used to intuitively navigate to the most relevant entries in large result sets.

web  <dig>  approaches are based on participation, self-improvement  and trust in users  <cit> . typically they deliver a rich user experience through the use of technologies such as ajax. approaches to glean knowledge in a form that has relatively little predefined structure from community participation have been successful in applications such as wikipedia  <cit> . the quality of community-contributed data has been the subject of ongoing debate, but is generally considered to be adequate . the united states patent and trademark office is currently participating in testing a web-based system for open peer-aided patent review  <cit> . social web  <dig>  environments are also emerging in a scientific context. in the case of the public sequence database genbank, it is increasingly recognized that the growing volume of information paired with the inability to correct annotation errors by users other than the contributing authors and internal curators may lead to deterioration of database quality  <cit> . it has been argued that uncorrected errors in public databases can percolate as they may cause dependent studies to be based on false assumptions  <cit> . community participation, such as web  <dig>  approaches, may help correct some of these errors. entrez gene includes the gene references into function , which are valuable annotation data contributed in part by the users  <cit> . article stubs in wikipedia have been automatically created using data from authoritative sources to assist in gleaning additional information from the life science community about biological entities such as genes  <cit>  or rna  <cit> . using semantic technologies, the wikiproteins system  <cit>  strives to combine factual knowledge with community annotation in a wiki-like system to gather knowledge that is accessible to methods of data mining. the idbd database collects biomarker information pertaining to infectious diseases by community collaboration of registered users  <cit> . the oreganno system  <cit>  collects information on gene regulatory elements and polymorphisms in a collaborative way and cross-references against public repositories. its annotation queue contains papers entered by experts or identified by text-mining methods. other approaches extend established information retrieval systems by facilities that add a community context: the cbioc project, for example, enables community annotation of molecular interaction data  <cit>  by providing a browser plugin that opens a window when the user visits pubmed. automated text extraction is used by cbioc for bootstrapping the database with initial data, while allowing community users to refine annotation of contents by contributing factual knowledge or voting on classification accuracy. a recent plugin for the cytoscape software enables user annotation of molecular interaction data from the mimi database directly within the application  <cit> . examples of other community-centered approaches include platforms related to publication of original research , management of references  and dissemination of research efforts using media like video and podcasts .

in contrast to approaches that allow collection of knowledge in unstructured form  or structured representations , our system gathers binary votes on the relevance of entries with respect to certain categories of interest. rating of relevance or quality of entries  has been adopted by many websites .

text classification is a well-studied task in the field of biomedical literature. a large number of different learning techniques such as naïve bayes  <cit> , rule learners  <cit> , bayesian networks  <cit> , and support vector machines  <cit>  has been used to classify biomedical texts, for example medline abstracts and medical reports. the state-of-the-art of text classification in the biomedical domain has been evaluated via common challenge evaluations such as the kdd cup  <cit>  and the text retrieval conferences . in  <dig> the kdd cup included a task centered around finding articles that contained gene expression products warranting annotation for inclusion into the flybase database  <cit> . in  <dig> the genomics track of the trec competition  <cit>  included text categorization tasks that dealt with automatic assignment of gene ontology terms to full text biomedical documents  <cit> . a subtask in the biocreative ii competition in  <dig> dealt with detection of articles relevant to annotation and extraction of interaction information  <cit> . to aid human curators, the utility of text classification for pre-annotation filtering or ranking of list of search results according to their relevance to a database annotation task has been investigated on data focusing on immune epitopes  <cit> , protein-protein interactions  <cit> , genetic variants of human proteins  <cit> , and allergen cross-reactivity  <cit> . in text mining applications, text classification is often only the first step in finding sets of documents that are pertinent to a certain topic. it is often followed by identification of relevant entities in the text  and automatically finding semantic relationships between these entities . an overview of text mining approaches and applications is given in  <cit>  and  <cit> . in general, performance of text classification is domain-specific and depends on factors such as the training data, pre-processing, and the selection of a supervised learning algorithm. even choosing a sensible measure of performance depends on the requirements at hand, e.g., whether false positive or rather false negative rates are to be minimized. hence, it is important to thoroughly test different established approaches for each new application. a recent survey of opinions about the future prospects and challenges in biomedical text mining among leading experts in the field  <cit>  identified the need for intuitive and easy-to-use interfaces for the biological end user and an increasing interest in community-based annotation among the major themes.

the goal of this study was to develop methods to speed up navigation in large automatically extracted biomedical datasets by offering users a way to prioritize search results according to information that previously had to be manually found in the associated abstracts. rather than ensuring correctness of each contributed or predicted annotation, we aimed to make the confidence of the prediction transparent to the user, so that they are in the position to decide which entries in the result set they should manually inspect first.

RESULTS
information retrieval from textual databases using thesaurus terms or full-text search has a number of limitations: often thesaurus terms have not yet been assigned to recent entries. another issue arises for highly interdisciplinary subjects that span broad fields  and cannot be adequately described by one single term. full-text search in abstracts complements searching with thesaurus terms in these cases. however, full-text search always requires the user to know in advance what search terms are relevant. hence, entries containing relevant terms that the user is not aware of can be missed. for example, it is not easy to come up with a comprehensive list of all search terms that signal the availability of binding data in an abstract. hence, although useful, thesaurus and full-text search in abstracts alone do not fully cater to the needs of efficiently finding relevant entries in search results that have not been annotated by experts, which calls for a novel approach.

navigating through search results using heat maps
our approach to improve navigation in search results is to present the user a heat map. in the heat map each row represents a target entry  and each column represents a category of interest. each cell then represents a prediction value  for a peptide entry to relate to a category, converted into a color between red  and green . the interface and an example user workflow are shown in figure  <dig> 

controls on the web page allow exploration of the result set by sorting on individual categories, e.g., bringing all entries to the top that are related to cancer. sorting on multiple categories  can be accomplished by first sorting on the category for the secondary order  before sorting on the primary category . to allow for an even more finely grained control of the search results, we added sliders to the interface that can be used to choose a minimum threshold for each category, e.g., to only display entries that contain binding data. this allows adding constraints after the query has been made. the interface is also highly responsive: immediate feedback enables the user to directly understand the effect that adding a constraint has on the result set, which is not possible at query time.

exploiting community knowledge by voting
the values shown in the heat map are generated by either of two mechanisms: first, we allow external users of the database  to cast votes on each entry. as disagreement among users is possible, the value for each heat map cell is calculated as the ratio of "related" votes to the total number of votes for the entry . the values shown in the heat map are these ratios . second, we use the class probability estimates of text classification if votes do not exist, as is currently true for the majority of the entries.

the rationale behind letting users vote without restrictive access control is to lower the hurdle of contributing knowledge as much as possible. although it can be argued that this might lead to a diminished quality of labelling, we believe that the benefits outweigh potential drawbacks. experts from different domains can contribute knowledge in a very direct fashion. even if there is disagreement among voters, the data are not completely invalidated since disagreement is made visible by a change in color. the majority vote determines whether an entry is on the green or red side of the spectrum. second, even weakly labelled data speeds up the retrieval process: community gathered votes are strictly separated from data in pepbank. having the right label means that a relevant entry might be found more quickly by using the heat map. at the same time, no data that are labelled incorrectly in the heat map are lost as they are still accessible via conventional search options like full-text search in abstracts, advanced search on single database fields, and blast  <cit> , or smith-waterman  <cit>  sequence similarity searches. the user can also make a trade-off between precision and recall by choosing a cut-off for further inspection of results. the interactive slider mechanism enables the user to instantly observe the effect of the chosen cut-off value on the result set.

each vote is recorded separately with a time-stamp and client information. this allows for removal of individual votes in the event of vandalism, which is an extreme case of incorrect labelling. however, most machine learning schemes  do not assume that the class labels  are always correct. rather, they are based on the assumption that classes are noisy or assigned according to a conditional probability distribution .

application of established learning techniques
to evaluate the performance of different classifiers on our domain of application we created training sets for each of the seven categories . while good performance is critical for the choice of a classification algorithm, it was not the only criterion for our application. we sought an algorithm that also yields class probability estimates that could be used as confidence guides for the user and is fast enough to be used in the database-driven setting to continually rebuild classification models upon addition of user-contributed votes. we benchmarked a number of established techniques on the seven training sets using  <dig> iterations of 10-fold cross validation to find algorithms that perform well on all categories present in the current system. the input texts were transformed into a bag-of-words representation and stop words were removed. after lovins stemming  <cit> , features were transformed into tfidf values before being presented to the learners. we used the f-measure for related entries  as the performance criterion, with corrected resampled paired t-test  <cit>  to assess statistical significance . we found  that no learning scheme performs consistently better than bagging of the j <dig> variant of c <dig>  decision tree learners , which achieved a performance of 91-98%  on the benchmarking data. figure 2a shows the roc analysis  <cit>  for the angiogenesis category. bagged j <dig> is reasonably fast: updating the heat map for the entire pepbank through the daemon  takes at most five minutes per category on our production system . it also offers the advantage that bagged prediction values have a sufficiently broad distribution suitable for graphically presenting the results in a heat map , rather than a simpler yes/no prediction. figure 2b shows the utility of these prediction values for the angiogenesis category by combining a precision-recall plot with the actual color values used in the heat map for displaying search results. note that the most notable change in the trade-off between precision and recall occurs in the yellow region  where it would be naturally expected. thus, the class probability estimates produced by this classification setup serve as useful guidance for the end user to assess prediction confidence.

f-measure  and standard deviation in parentheses. • statistically significant degradation. no statistically significant improvement was observed.

f-measure  and standard deviation. no statistically significant degradation or improvement was observed.

some classifiers such as support vector machines  had f-measures comparable to those of the bagged j <dig> and were faster, however, the classifier output could not be consistently translated into meaningful class probability estimates spreading over the whole range between zero and one. one way to achieve this would be to convert the distance of each classified instance from the optimally separating hyperplane into a prediction value  <cit> . other classifiers, such as k-nearest neighbor, performed consistently worse than bagged j <dig>  perhaps reflecting the relative sparseness of the training sets.

the utility of user votes as training data for automatic classification
allowing users to vote without access restrictions eases collection of labelled training data at the cost of erroneous and inconsistent labels. some inconsistency in user-contributed information is due to diverging expert opinions that take the form of probability distributions rather than a single standard of truth. the pepbank system shows such disagreement by converting the ratio of user votes  into a probability estimate. such "debated" entries are presented to the user as cells with yellow hue in the heat map.

however, some inconsistency is due to incorrect or negligent voting behavior. to estimate the impact of votes that are incorrect with respect to a gold standard, we created an additional set of  <dig> labelled abstracts for the "binding" category. each entry in the data set was independently labelled by two curators, who adhered to an annotation manual that was established before the start of annotation. inter-rater agreement was calculated to be 98% . labels from both curators were then consolidated into a gold standard and disagreement on the four abstracts with different labels resolved using the annotation manual. the final data set contained  <dig> abstracts that were labelled as containing binding data. . note that a classifier that performs random guessing  is expected to achieve an f-measure of  <dig> % on this data set .

next, we simulated the addition of user-contributed votes with different error rates to a baseline set used to provide the classifier with initial training data. to obtain robust estimates of performance and variance, for each error rate, the simulation was carried out using  <dig> different splits of the gold standard into simulated user votes and test data. specifically, for each of the  <dig> runs, three data sets were automatically created in the following way: first,  <dig> baseline votes were sampled from the set of  <dig> labelled abstracts used in the previous investigation that compared performance of different algorithms . resampling of the base votes was carried out for each run to ensure that subsequent performance changes upon addition of votes were not dependent on a particular choice of base votes. second, a set of  <dig> abstracts was sampled from the gold standard to simulate user-contributed votes with controlled error rates. this was done to ensure that the simulation results were not dependent on a particular sample of simulated user votes. third, the remainder of the gold standard was used as a test set for this run. sampling from the gold standard was done on a per class level, so that the simulated votes had the same number of positive and negative examples  as the test set.

all sampling was done without replacement. for the simulation of user votes, we made the assumption that the gold standard indeed represented the standard of truth. to simulate erroneous votes, labels from the gold standard were flipped with a probability representing the simulated error rate before addition to the training set. for example, an error rate of  <dig>  means that a label from the gold standard was converted from "contains binding data" to "contains no binding data" and vice versa with a probability of  <dig>  before addition to the training set.

from the data sets  created for each run of the simulation individual training sets were automatically created in turn by successively adding labelled abstracts from the set of simulated user votes to the baseline set. for each training set the classifier was retrained as in the online version of pepbank and evaluation was carried out on the test set for this run. thus, each training set consisted of the base votes  and a variable additional number of simulated user-contributed votes .

the box plots in figure  <dig> show the development of classifier performance as simulated user votes are added . while inconsistent votes have a negative effect on the classification performance in general, voting helps to significantly increase classification performance even if one assumes voting error rates as high as 20%.

adding text classification capability to an existing database: a database-driven approach
one key requirement for our classification system was to adapt to new user-contributed data dynamically and fast. also, integration of text classification into the database was desirable since it simplifies the system and renders the user interface completely independent from the machine learning part. inductive databases, systems that natively support data mining operations, would be an ideal solution for this application. theoretical and practical attempts have been made to define formal requirements for inductive databases and to extend database systems by machine learning operations  <cit> . oracle  <dig> g offers database-integrated text classification using support vector machines and decision trees  <cit> . we created a system that is similar in spirit in that it is controlled by the database but leaves classification to a background process  running alongside of the database server.

the classification system is controlled by the database through a native extension. update events can thus be issued from within the database and triggers can be built that automatically notify the daemon that changes have occurred. the system architecture is outlined in figure  <dig> 

based on the benchmarking results, we selected bagged j <dig> decision trees to classify unlabelled entries in pepbank. the daemon trains the classifier on the initial expert-curated training sets  and on those database entries that have received votes from the users. the trained classifier is then used to classify all entries that have not yet received votes. as more votes are being cast, the model for classification is expected to improve.

peptide entries  are classified on the basis of their associated abstracts using the same pre-processing steps of stop-word removal, lovins stemming, and tf-idf transformation that were used during benchmarking . for example, the text fragment "tumor-induced angiogenesis can be targeted by rgd  peptides, "  would yield the tokens {angiogenes, arg-gly-asp, pept, rgd, targes, tumor} after stop-word removal and lovins stemming. tf-idf values are then calculated on the basis of the token frequencies. the bag-of-words representation of an abstract is a sparse vector containing the tf-idf values of the observed tokens. note that the bag-of-words representation of an abstract does not change when new votes are cast. voting only affects the class labels of abstracts presented to the classifier during training.

each user vote and associated meta-information are recorded separately in a database table. this enables deletion of individual votes from a certain origin  or adding a manually uploaded batch of training data independently from other votes.

the class probability estimates in the heat map are updated through the following mechanism: first, whenever a new vote is cast, triggers in the database aggregate individual votes into a single entry for each abstract and each category, containing the vote counts for both classes. this enables the classification daemon to read the counts for each entry with a single inexpensive query. second, the triggers also call a database extension that notifies the classification daemon that a change in the training data for a specific category has occurred. finally, the daemon schedules the category for reclassification by putting a time-stamped entry into a priority queue. if another request for reclassification with an earlier time-stamp exists that is not yet being processed, the new request is ignored and the time-stamp of the earlier entry is updated to ensure non-redundant classification.

whenever there are reclassification requests in the queue, the daemon asynchronously retrieves the next entry from the priority queue and reads the training data for the corresponding category: the set of labelled abstracts and the vote counts for each class. if there is disagreement among the user votes for an entry, the majority vote is used to determine the right class assignment for the training data. if the same amount of votes has been cast for both classes, the entry is excluded from the training data. after the classifier has been trained, the daemon updates the class probabilities for entries that have not received any votes yet.

the setup described above makes efficient use of resources since no redundant classification is done. at the same time the heat map benefits from new votes as fast as possible. the scheduler ensures that the waiting time from casting a vote to the heat map update is bounded by o where tc is the maximum time for reclassification of a single category, and is not dependent on the number of concurrent votes being cast. the notification of the daemon and the use of a priority queue scale well when casting thousands of votes during batch uploading.

future directions
the current system has a fixed set of categories for classification, which were determined by the needs of our users. however, since the system is scalable, it could be worthwhile to allow dynamic creation of categories by the users themselves in the future.

even with community voting the generation of training data still requires human labor making it relatively expensive to obtain compared to the millions of unlabelled abstracts in medline. the availability of large amounts of unlabelled data is a common theme in bioinformatics. recently, semi-supervised or transductive learning algorithms have received attention in machine learning  <cit> . in semi-supervised learning, the learner benefits from unlabelled examples by capturing information about the domain-specific distribution of examples. this is motivated by the so-called cluster assumption  <cit> , which states that nearby instances in the example space are likely to have the same label and that classification boundaries are unlikely to run through regions densely populated by examples. it has been shown that semi-supervised learning can lead to performance improvements especially when dealing with small amounts of labelled data  <cit> . the current application with its large amounts of unlabelled training data seems to be a good target for transductive machine learning methods and we are considering benchmarking different transductive learners on the abstracts in pepbank. finally, we believe that the problem would be suitable for methods from online learning  <cit> .

currently, only abstract texts but not titles or mesh terms are used for classification. the reason for leaving the latter out is that many newly submitted entries, which contain information relevant to pepbank, have not yet been assigned mesh terms yet. using title words, mesh terms, journal titles, and author names for classification of medline abstracts can lead to performance gains  <cit> . we are planning to address this in a future release.

stemming is an important part of pre-processing text for classification. whether stemming is beneficial in terms of classification performance highly depends on the domain of application and the stemming algorithm used. han et al.  <cit>  and wang et al.  <cit>  report that the use of standard stemming algorithms might not be suitable for texts in the biomedical domain with terms vastly more complex than in everyday english. for our future work we are looking into more rigorous ways to evaluate different stemming algorithms for our application and into testing other stemming methods such as language independent frequent substrings, which have been shown to be well-suited for a variety of biomedical text classification tasks  <cit> .

CONCLUSIONS
machine learning can be used to tackle the problem that, even when using community voting, data available in biomedical databases often outnumber the amount of manually contributed meta information. furthermore, machine learning can be used in an interactive setting to take advantage of community-contributed information as fast as possible. our approach to implement a database-driven machine learning system is independent of the presentation layer and scales well with frequent changes in the user-contributed data that are used to build automatic models for classification. in our application, bagging allows ensembles of j48/c <dig>  decision tree learners to deliver meaningful predicted class probabilities that are helpful to visualize the trade-off between precision and recall in heat maps for navigation. the setup we describe can be readily applied to other databases that store textual data to enhance navigation without requiring changes to the respective underlying database models.

