BACKGROUND
recent technological advances have led to an explosive growth in high-throughput genomic and proteomic data such as dna microarrays. the rapid growth in available data has led in turn to a need for novel quantitive methods for analysis. as a consequence of this need, the reconstruction of gene network architectures from dna microarray expression data has become a major goal in the field of systems biology. an increased understanding of the network architectures and their respective dynamics will enable novel approaches to disease treatments by allowing us, for example, to identify drug targets in silico which manipulate the functional outputs of these networks. this process is expected to lead to novel classes of drug based on a network approach to cellular dynamics.

frequently, the gene expression data itself is derived from perturbation experiments such as stress conditions, temperature shifts, and chemical treatments; for example, the widely used yeast cell-cycle datasets of cho  <cit>  and spellman  <cit> . although these global perturbations are carried out in order to reveal causality between genes, it is not always clear how experiments should be designed so as to reveal as much causality as possible, while both minimising costly experimentation and remaining computationally tractable.

a range of computational and mathematical techniques have been adopted in the effort to find a successful gene network reconstruction technique. reconstruction methods often have to negotiate a tradeoff between intensive  computations, and having to perform a large number of costly experiments. certain progress can be achieved by making simplifications, such as imposing a limit on the number of inputs to each gene, or making steady state assumptions about the system  <cit> . some techniques described in the literature offer efficient algorithms, but require a large number of experiments, perhaps as many as there are genes  <cit> . on the other hand, theoretical work on boolean models has shown  <cit>  that perhaps as few as o) experiments  might be required for n genes, but that to infer these relationships requires the use of computationally costly enumeration methods.

in this paper, we propose to explore the issue of how perturbation microarray experiments might be designed, and to suggest how such experiments might be optimised so as to maximize inference capability. logical gene network models have previously been used to investigate gene network robustness  <cit> , perturbation dynamics  <cit>  and evolutionary potential  <cit> , and form the basis of the inference method used in this study. this inference method  <cit>  is similar to others in which networks with a minimal number of connections are reconstructed through enumeration  <cit> . given the significant speed advantage of integer computation over floating point computation, and that most genes are expected to have few inputs , the method is considered to be adequate for this investigation. in this study, exhaustive evaluation was performed up to a maximum of  <dig> inputs of both positive and negative sign . enumeration is computationally feasible on an ordinary desktop computer for medium-sized networks , and still tractable for large networks , though this would require some parallelisation. the global perturbations themselves are simulated by changing the state of each gene at random. a perturbation intensity measure q, defines the probability that each gene will change state .

RESULTS
a limited number of perturbations significantly improve accuracy
a discrete dynamical model was used to generate time series data from random networks . to measure the effect of adding perturbations on inference ability, inference sensitivity  was measured against p, the number of additional perturbations. figure  <dig> shows the results for predicted solutions with one and two inputs, as well as overall sensitivity. the top graph in figure  <dig> shows that overall sensitivity is clearly enhanced by including more perturbation experiments, with lower order solutions  reaching higher levels of sensitivity. the bottom graph shows the corresponding inverse relationship for the standard deviation of the sensitivity .

it should be noted that the algorithm tends to underestimate the number of inputs a gene may have. this is to be expected in genes for which dynamics cannot be informative: for example, consider a gene i which has one or more negative inputs, as well as having default value off. since the discrete dynamics for this gene will be the same as if it had no inputs at all , the presence of the inputs is impossible to infer. this underestimation effect is clear in table  <dig>  which compares the distribution of inferred solution set sizes  with the actual solution sizes , and shows that the method is only able to produce roughly half the number of one and two input solution sets that actually exist.

the increase in sensitivity with p can be explained at least partially, in the following way. since the time series are discrete, many of the genes may have identical behaviour over time despite having different inputs  = sj for two different genes i and j). if we define a "concatenated" time series vector  for gene i, and then map each gene i onto si, we obtain a many-to-one mapping. as we increase the number of perturbations, we might expect the number of distinct time series also to increase. we define a simple measure to quantify this mapping, m = n'/n where n' is the number of distinct vectors si, and n is the number of genes. the maximum value of m =  <dig> indicates that the mapping of genes to time series is one-to-one, whereas lower values indicate degenerate mappings. the manner in which m increases with the number of perturbations is shown in figure  <dig>  and shows how the increase in m reflects the corresponding increase in sensitivity .

network size and optimal perturbation intensity
the experiments described above were repeated to consider variations in two other parameters: the network size n, and the perturbation intensity parameter q .

to consider the first case, the minimum number of perturbations p* required to reach a given high accuracy criterion was measured for different values of the network size n. the high accuracy criterion was defined as average sensitivity =  <dig>  for one-input solution sets . to find p*, we first find the number of perturbations p+, such that average sensitivity p+ ≥  <dig> , and average sensitivity  <  <dig> . if average sensitivity p+ >  <dig> , we use simple linear interpolation to find the  value of p* between p+ and  for which average sensitivity =  <dig> .

the resulting values for p* are shown in figure  <dig>  since the relationship is expected to be logarithmic  <cit> , the plot shows log against p* . a least squares best fit gives p* ≃  <dig>  log +  <dig> , which, for n =  <dig>  gives p* ≃  <dig> . in order to obtain a measure of variance for p*, we would need to calculate p*-equivalent values for many individual networks separately, then consolidate these values to obtain the relevant statistics. however, because it was only feasible to consider medium-sized networks , and for any such network we often find only a small number of one-input solution sets, such statistics were found to be unreliable.

the second case  suggests an optimal range for q. figure 4a shows the inference sensitivity over a range of values for q, and figure 4b shows the corresponding standard deviation. again, inference sensitivity for one-input solutions is higher than for two-input solutions, which in turn is higher than overall sensitivity. for one-input solutions, the results show a clear peak for sensitivity close to the range  <dig>  <q <  <dig> . together with a corresponding minimisation of the standard deviation in this interval , these results suggest that perturbation intensity should be close to this range to optimise inference accuracy.

CONCLUSIONS
a recent analysis of the yeast genetic network has shown that 93% of genes are regulated by between  <dig> and  <dig> genes  <cit> . this suggests that enumerative network reconstruction methods can be useful within computationally feasible limits. experiments involving large-scale perturbations  are a standard way of obtaining time-series of gene expression data  <cit> . a key result of  <cit>  is that indegree appears to follow an exponential distribution, whereas outdegree follows a scale-free distribution, which has enabled the generation of realistic artificial gene networks used here. a logical model  <cit>  was used to simulate the perturbed expression data. subsequently, experimental parameters were considered in relation to inference accuracy, namely: a) number of perturbations required, p, and b) perturbation intensity, q.

the inference method itself is most useful for low order inputs, with inference accuracy maximized for predicted single input genes. more accurate methods have been proposed, though these generally require a much larger number of experiments  <cit> . methods such as the one proposed here, which infer relationships from expression data may well be more successful when used in conjunction with other methods such as promoter analysis  <cit> , or when used to drive experimental procedure  <cit> . here, the results show that only a relatively small number of perturbations are necessary in order to achieve a substantial inference accuracy, even for large n. these relatively modest experimental requirements would presumably imply lower experimental costs. the results also suggest that the perturbations should be calibrated , so as to alter the expression levels of approximately half the genes in each experiment. generating perturbations which alter the expression level of half the genes at random may be difficult to achieve in practice, though experiments can be designed to come as close to this goal as possible. even in the absence of optimal perturbations, we hope the simulation approach described here will still serve as a useful tool for planning experiments.

