BACKGROUND
motif discovery, as a main means to locate conserved fragments in biological sequences, is a fundamental problem in computational biology. the conserved fragments usually have special biological significance. for example, transcription factor binding sites in dna sequences  <cit>  play a key role in gene expression regulation and they usually range from  <dig> to  <dig> base pairs; short protein sequence signatures  <cit> , which usually range from  <dig> to  <dig> residues, can be used in identifying potential interaction sites of proteins.

the planted  motif search   <cit>  is a famous formulation for motif discovery: given a data set d = {s <dig>  s <dig>  …, st} with t n-length sequences over an alphabet Σ, q satisfying 0 < q ≤ t, and l and d satisfying 0 ≤ d < l < n, the goal is to find one or more l-length strings m such that m occurs in at least q sequences in d with up to d mismatches. the string m is called a  motif, and each occurrence of m is called a motif instance. finding all  motifs present in the input sequences is np-complete  <cit> .

in the pms problem, the value of q implies the corresponding sequence model of motif discovery . the usual sequence models include oops, zoops and tcm  <cit> , representing that each input sequence contains one occurrence, zero or one occurrence and zero or more occurrences, respectively. when q = t and 0 < q < t, the pms problem corresponds to the oops and zoops or tcm sequence models, respectively.

there have been numerous motif discovery algorithms  <cit> . they are either approximate or exact, based on whether the algorithm can always find all motifs or the optimum motif. the approximate algorithms usually adopt probability analysis and statistical methods. for instance, the two classical algorithms meme  <cit>  and gibbs sampling  <cit>  identify motifs by using expectation maximization and gibbs sampling techniques, respectively. in general, these approximate algorithms can solve the problem in a short time but cannot guarantee global optimum.

in this paper, we mainly focus on exact motif discovery algorithms, which can find all  motifs by traversing the whole search space. the main indicator to assess exact algorithms is time performance, and researchers usually compare exact algorithms on the challenging pms problem instances, for which the expected number of random  motifs present in the sequences is more than  <dig>  <cit> . for the exact algorithms proposed in earlier years, such as winnower  <cit> , dpcfg  <cit>  and recmotif  <cit> , their search space is composed of t possible alignments of motif instances. in recent years, the exact algorithms verify all the l-length patterns in the o search space, and then output the patterns with motif property; we call them the pattern-driven pms algorithms .

the pattern-driven pms algorithms have better time performance than other exact algorithms so far in identifying both short motifs and long motifs with weak signal. their basic idea is to generate candidate motifs by using several reference sequences in the input, and then verify each candidate motif one by one. specifically, they generate candidate motifs by using all possible h-tuple t =  composed of h l-length strings coming from h distinct reference sequences. in existing pattern-driven pms algorithms, h is  <dig> for pmsp  <cit>  and pmsprune  <cit> ; h is  <dig> for stemfinder  <cit> , pairmotif  <cit> , qpms <dig>  <cit>  and travstrr  <cit> ; h is  <dig> for itriplet  <cit>  and pms <dig>  <cit> ; for pms <dig>  <cit>  and qpms <dig>  <cit> , h is greater than or equal to  <dig> and self-adaptive in dealing with different pms problem instances. moreover, these algorithms use k = t – q + h reference sequences to generate candidate motifs, ensuring that there exists at least one h-tuple t so that each l-length string in t is a motif instance.

although pattern-driven pms algorithms outperform other exact algorithms, most of them use the first k sequences in the input as reference sequences, without considering the effect of different reference sequences on time performance. in this study we found that given a data set, different reference sequences may lead to quite different number of candidate motifs, especially for large alphabets. so, in dealing with different inputs with the same scale, the pattern-driven pms algorithms may exhibit sharp fluctuations in running time. for instance, we randomly generate multiple groups of data sets with |∑| =  <dig>  t =  <dig>  q =  <dig> and n =  <dig> following the method described in the results and discussion section. when solving the  problem instance, qpms <dig> sometimes consumes  <dig>  minutes, but sometimes over 48 hours. some other pattern-driven pms algorithms like travstrr and pms <dig>  suffer from the same problem .

to solve this problem, we propose a method named refselect to quickly select reference sequences that generate a small number of candidate motifs. refselect can bring a practical time improvement of the state-of-the-art pattern-driven pms algorithms, without doing any modifications to them.

methods
problem description and notations
reference sequence selection problem
given a data set d = {s <dig>  s <dig>  …, st} over an alphabet Σ that contains t sequences of length n, the  problem instance  and the number of reference sequences k  required by the pattern-driven pms algorithms, the task is to select k reference sequences from d to form the reference sequence set d', such that when using d' the pattern-driven pms algorithms can efficiently solve the  problem instance without sharp fluctuations in running time.

in the reference sequence selection problem the value of provided k should be greater than  <dig>  if k is  <dig>  it means that the candidate motifs will be generated from multiple single l-mers; in this case, no matter how we select a reference sequence, the number of generated candidate motifs is fixed. in fact, k is greater than  <dig> for all the efficient and recently proposed pattern-driven pms algorithms.

we evaluate a reference sequence selection algorithm from two perspectives. one is the time performance. the time cost of the reference sequence selection algorithm should be as small as possible because it is a preprocessing for pattern-driven pms algorithms. it will be meaningless if it costs too much time to select reference sequences. the other is validity, namely whether the reference sequence selection algorithm brings a good speedup for pattern-driven pms algorithms. the speedup is the ratio of t <dig> to trs + t <dig>  where t <dig> is the running time of the pattern-driven pms algorithms on the input sequences of original order, trs is the running time of the reference sequence selection algorithm, and t <dig> is the running time of the pattern-driven pms algorithms on the input sequences of new order generated by the reference sequence selection algorithm.

table  <dig> summarizes the notations used in this paper. notice that, a sequence specially refers to an n-length string in a data set, and an l-mer refers to a short string of length l .table  <dig> notations used in this paper


|x|

d is the set of input sequences. d' is the set of reference sequences. d = {s
 <dig>  s
 <dig>  …, s
t} and d' = {s
r <dig>  s
r <dig>  …, s

t

k

n

x ∈l
s



d

m
d = {y: |y| = |x| = |x'|, d
h ≤ d, d

n

n
i and s

i and s


overview of refselect
we introduce why and how to select reference sequences for the pattern-driven pms algorithms. let us consider the following two observations, which indicate how the hamming distance between pairs of l-mers affects the number of candidate motifs. examples and detailed discussion for the two observations are given in the results and discussion section.

observation  <dig>  for two l-mers x and x', the smaller their hamming distance dh, the larger the number of their common candidate motifs |md|.

observation  <dig>  for a tuple t of h l-mers, when it contains pairs of l-mers with a relatively small hamming distance, it generates a relatively large number of candidate motifs.

based on the two observations, different reference sequences may lead to different number of candidate motifs. the pattern-driven pms algorithms utilize all tuples of h l-mers in k  reference sequences to generate candidate motifs. once there are relatively more pairs of l-mers with small hamming distance in these h-tuples, more candidate motifs will be generated.

since the time performance of pattern-driven pms algorithms mainly depends on the number of generated candidate motifs, we should select the reference sequence set generating a small number of candidate motifs. for example, as shown in fig.  <dig>  assume the input sequence set d is {s <dig>  s <dig>  s <dig>  s4} where each sequence has two l-mers and we select k =  <dig> reference sequences from d. in the figure, the thicker the dotted line, the more candidate motifs are generated by the associated two l-mers. obviously, {s <dig>  s <dig>  s4} is the optimal reference sequence set.fig.  <dig> an example of selecting reference sequences. in this example, we select three reference sequences from four input sequences. each sequence contains two l-mers. a thick line indicates that more candidate motifs are generated by the associated two l-mers



naturally, we select reference sequences by evaluating the number of generated candidate motifs, ensuring the selected reference sequences generate a small number of candidate motifs. when we evaluate the number of candidate motifs generated from a tuple t of h l-mers, it is difficult to directly compute the number of common candidate motifs shared by all the h l-mers in t, denoted by n <dig>  an alternative way is to compute the sum of the number of common candidate motifs shared by each pair of l-mers in t, denoted by n2 = Σ|md| for 1 ≤ i < j ≤ h. as shown in fig.  <dig>  n <dig> and n <dig> have the consistent tendency with the variation of the hamming distance between the pairs of l-mers in t.

furthermore, we use  and  to evaluate nr, the number of candidate motifs generated from the reference sequence set d'. it is defined as the sum of the number of common candidate motifs shared by each pair of l-mers in d' .  <dig> nr=∑1≤i<j≤|d'|nr   <dig> nrsisj=∑x∈lsi,x'∈lsjmdx,x' 

our method of selecting reference sequences includes two steps. the first step is to compute the number of candidate motifs generated from every two sequences in d, in order to quickly evaluate the number of candidate motifs generated from the specified k reference sequences in the next step. the second step is to select k sequences from d to form the reference sequence set d' such that the number of candidate motifs generated from d' is as small as possible. in the following, we describe the two steps in detail.

step 1: computing the number of candidate motifs
we compute the number of candidate motifs generated from every two sequences si and sj in d according to . for an l-mer x in si and an l-mer x' in sj, the number of their common candidate motifs |md| depends on their hamming distance dh  <cit> . the details about how to compute |md| are described in  <cit> . in implementing refselect, we store the values of |md| under different dh in a table in advance. once we know dh, we can immediately get |md| by looking up the table in o time.

thus, the core operation of  is to compute the hamming distance between every two l-mers x ∈lsi and x' ∈lsj. for any two sequences of length n, we have o pairs of l-mers. a simple method is to traverse all these pairs of l-mers; for each pair of l-mers x and x', the hamming distance can be computed in o time by comparing the characters x and x' for 1 ≤ i ≤ l. the time complexity of this method is o.

we introduce a more efficient method to compute the hamming distance between every pair of l-mers in si and sj. we fill an n × n matrix m, where the element in row a  and column b  is denoted by m. let lmin = min, str1 = si, str2 = sj[b – lmin 
+ 1…b]; then, m is the number of such position i that str1 = str2[b – lmin 
+ i] for 1 ≤ i ≤ lmin, namely m = lmin − dh. for example, in fig.  <dig>  a < b, str1 = si, str2 = sj, and then m = a − dh, which denotes the number of positions where the two characters are identical in the alignment of str <dig> and str <dig> fig.  <dig> illustration of fast computation of hamming distance. this figure shows an example for explaining how to compute the hamming distance between every pair of l-mers in two input sequences



in filling the matrix m, we initialize m with  <dig> for the case of min =  <dig>  and obtain m based on m:  <dig> ma+ <dig> b+1=mab+ <dig> ifsia+1=sjb+1mab,otherwise, where both a and b range from  <dig> to n.

with the matrix m, we use  to compute the hamming distance between a pair of l-mers str1' and str2', where str1' = si and str2' = sj are the l-mers at the position a  of si and the position b  of sj, respectively.  <dig> dhstr1',str2'=l−mab−ma−l,b−l 

our method is mainly to fill the matrix m. that is, we need to compute n <dig> elements one by one for any two n-length sequences si and sj. in computing each element m by  in o time, we simultaneously compute the hamming distance between the l-mer at position a  of si and that at position b  of sj by  in o time. therefore, the time complexity of computing the hamming distance for all pairs of l-mers in two sequences is reduced to o.

step 2: selecting reference sequences
after getting the number of candidate motifs generated from every two sequences in d, we can evaluate the number of candidate motifs generated from a set of reference sequences according to . in this section, we introduce how to select a set of reference sequences that generates a small number of candidate motifs.

first, let us consider the exhaustive search strategy. for the data set d consisting of t sequences, after evaluating the number of candidate motifs for every possible k sequences in d, the exhaustive search regards the k sequences corresponding to the minimum number of candidate motifs as the reference sequences. its time complexity is as follows. otk×k <dig> 

obviously, the running time of the exhaustive search grows dramatically with the increase of the number of input sequences t and the number of selected reference sequences k. a simple experiment can show that the exhaustive search is too time-consuming for large input: we set |Σ| =  <dig>  n =  <dig>  q = t =  <dig> and k = t × 5 % =  <dig>  and the running time of the exhaustive search exceeds one day on personal computers.

in order to quickly select reference sequences, we convert the problem to graph clustering. the t sequences in d are taken as t nodes in a graph. the similarity between two nodes si and sj is related to the number of candidate motifs generated from si and sj ). we hope that the nodes corresponding to the reference sequences with small number of candidate motifs form a dense subgraph, and they belong to the same cluster after graph clustering. we use the mcl algorithm  <cit>  to complete clustering, and set the inflation parameter as  <dig>  following  <cit> . we describe the details involved in clustering as follows.

similarity measure
we design the similarity of two nodes  si and sj based on nr. simultaneously, we consider the following two factors.

first, we further increase the effect of the pairs of l-mers in si and sj with small hamming distance on the total number of candidate motifs generated from si and sj. by doing this, it is helpful for the clustering process to distinguish different reference sequence sets that lead to different number of candidate motifs. specifically, we use  instead of  to evaluate the number of candidate motifs generated from two sequences si and sj.  <dig> nr'sisj=∑x∈lsi,x'∈lsjmdx,x'dhx,x'+ <dig> 

second, we aim to put a set of sequences d' to the same cluster such that every two sequences si and sj in d' generate a small number of candidate motifs. so, we should ensure that si and sj have a larger similarity when they generate a smaller number of candidate motifs. finally, we compute the similarity of si and sj as follows:  <dig> simsisj=1nr'sisj×max1≤i<j≤tnr'sisj. 

cluster refinement
the clustering process may produce more than one cluster, and there may not be exact k nodes in each cluster. we refine each obtained cluster c in order to get a set of k reference sequences. then, we sort the sets of reference sequences and output the set with the highest score.

for the cluster c with only one node, we take it as an invalid cluster, since the node in c has a low similarity with other nodes. for the cluster c with two or more nodes, it corresponds to three cases:  there are exact k nodes in c;  there are more than k nodes in c;  there are less than k nodes in c. for case , we can get the reference sequence set d' directly by using the k sequences in c. next, we introduce how to refine c under cases  and .

for case , we use greedy strategy to select k sequences from c  to form d'. first, we initialize d' with {sa, sb} such that sim = max{sim} for all si, sj ∈ c and si ≠ sj. then, we repeatedly choose a node sr satisfying  from c − d' and add it to d' until |d'| = k.  <dig> sr=argmaxsi∈c−d'∑sj∈d'simsi,sj 

for case , we use the similar method to choose k − |c| nodes from d − c, and add them to c to form d'. first, d' is initialized with c. then, we repeatedly choose a node sr satisfying  from d − d' and add it to d' until |d'| = k.  <dig> sr=argmaxsi∈d−d'∑sj∈d'simsi,sj 

figures  <dig> and  <dig> show examples under case  with k =  <dig> and case  with k =  <dig>  respectively. differences between the two cases are: in case , we get d' by selecting reference sequences  from the sub-graph corresponding to the cluster c; while in case , we get d' by selecting reference sequences  from the whole graph and adding them to c.fig.  <dig> an example of cluster refinement for |c| > k. this figure shows an example of cluster refinement in the case that there are more than k nodes in the cluster c


fig.  <dig> an example of cluster refinement for |c| < k. this figure shows an example of cluster refinement in the case that there are less than k nodes in the cluster c




we describe how to refine a cluster c in algorithm  <dig>  because the process that we select reference sequences by using greedy strategy is similar to the prim algorithm for computing minimum spanning tree, the time complexity of algorithm  <dig> is o and o2lg) under case  and case , respectively.

after cluster refinement, if we obtain more than one reference sequence set d', we score each d' by , and then output the d' with the highest score.  <dig> scored'=∑si∈d',sj∈d',i≠jsimsisj 

whole algorithm
this section gives the whole algorithm of refselect.

in line  <dig> of the pseudocode, we initialize d' with an empty set. lines  <dig> to  <dig>  corresponding to the first step of refselect, compute the similarity of any two nodes . the core operation of this step is to compute the hamming distance from all l-mers in si to all l-mers in sj in o time, for any two sequences si and sj in d. therefore, the time complexity of this step is: ot2×n <dig>  

lines  <dig> to  <dig>  corresponding to the second step of refselect, cluster the t sequences in d by using the mcl algorithm and refine each obtained cluster. the time complexity of clustering is o. the time complexity of refining clusters is negligible with respect to the time complexity of the first step. so, the time complexity of refselect is: ot2×n2+t <dig>  

in executing refselect, we need o space to store the input sequence set d, o space to store the matrix m for computing hamming distance, and o space to store the similarity matrix of t input sequences. so, the space complexity of refselect is o.

parallel implementation
to efficiently deal with large data sets, we can further accelerate refselect by using parallel processing. refselect consists of two steps, and we mainly use parallel processing for the first step. the reasons are  the first step is the bottleneck of the whole refselect algorithm in running time as shown in table  <dig>  and  the first step is easy to parallelize, because it repeatedly calculates the number of candidate motifs generated from two sequences si and sj in d and each calculation is independent of the others.

we implement the parallel version of refselect by using openmp  <cit> , which provides a flexible programming model for shared memory architectures and allows to add parallelism into serial codes easily by using one or several openmp directives. the pseudocode is shown in algorithm  <dig>  where we add an openmp “for” directive before the inner iterations of the first step to split parallel iteration spaces across threads.

the reason why we add the directive before the inner iterations  rather than the outer iterations  is for the consideration of load balancing among threads. note that, the number of inner iterations is not fixed for each outer iteration. if we add the directive before the outer iterations, the smaller the value of i, the more computational work will be needed by the thread processing the ith outer iteration.

RESULTS
analysis of effect of hamming distance on candidate motif number
first, we consider the case of two l-mers x and x', and analyze the effect of their hamming distance dh on the number of their common candidate motifs |md|. tables  <dig> and  <dig> give the values of |md| with dh varying from  <dig> to 2d under the dna and protein data, respectively. in both tables, the values are obtained by using two challenging pms problem instances. we can find that |md| increases with the decrease of dh for both the dna and protein data.table  <dig> the effect of hamming distance on the candidate motif number and occurrence probability for a pair of l-mers under the dna data


i =
d
|m
p
i
p
i
p
i
p
i
 <dig>  × 10
7
 <dig>  × 10
−6
 <dig>  × 10
6
 <dig>  × 10
−6
 <dig>  × 10
7
 <dig>  × 10
−8
 <dig>  × 10
5
 <dig>  × 10
−7
 <dig>  × 10
6
 <dig>  × 10
−8
 <dig>  × 10
7
 <dig>  × 10
−9
 <dig>  × 10
4
 <dig>  × 10
−7
 <dig>  × 10
5
 <dig>  × 10
−8
 <dig>  × 10
6
 <dig>  × 10
−9
 <dig>  × 10
7
 <dig>  × 10
−10
 <dig>  × 10
4
 <dig>  × 10
−8
 <dig>  × 10
5
 <dig>  × 10
−10
 <dig>  × 10
7
 <dig>  × 10
−11
 <dig>  × 10
8
 <dig>  × 10
−12

i
= d
|m
p
i
p
i
p
i
p
i
 <dig>  × 10
12
 <dig>  × 10
−6
 <dig>  × 10
13
 <dig>  × 10
−7
 <dig>  × 10
11
 <dig>  × 10
−7
 <dig>  × 10
13
 <dig>  × 10
−9
 <dig>  × 10
12
 <dig>  × 10
−8
 <dig>  × 10
14
 <dig>  × 10
−10
 <dig>  × 10
10
 <dig>  × 10
−7
 <dig>  × 10
12
 <dig>  × 10
−9
 <dig>  × 10
14
 <dig>  × 10
−12
 <dig>  × 10
8
 <dig>  × 10
−7
 <dig>  × 10
10
 <dig>  × 10
−9
 <dig>  × 10
12
 <dig>  × 10
−11
 <dig>  × 10
14
 <dig>  × 10
−13
 <dig>  × 10
9
 <dig>  × 10
−8
 <dig>  × 10
11
 <dig>  × 10
−10
 <dig>  × 10
13
 <dig>  × 10
−12
 <dig>  × 10
15
 <dig>  × 10
−15
 <dig>  × 10
9
 <dig>  × 10
−9
 <dig>  × 10
11
 <dig>  × 10
−12
 <dig>  × 10
13
 <dig>  × 10
−14
 <dig>  × 10
15
 <dig>  × 10
−17
 <dig>  × 10
9
 <dig>  × 10
−11
 <dig>  × 10
11
 <dig>  × 10
−14
 <dig>  × 10
13
 <dig>  × 10
−16
 <dig>  × 10
15
 <dig>  × 10
−18
 <dig>  × 10
10
 <dig>  × 10
−13
 <dig>  × 10
12
 <dig>  × 10
−15
 <dig>  × 10
14
 <dig>  × 10
−18
 <dig>  × 10
15
 <dig>  × 10
−20
 <dig>  × 10
10
 <dig>  × 10
−15
 <dig>  × 10
12
 <dig>  × 10
−18
 <dig>  × 10
14
 <dig>  × 10
−20
 <dig>  × 10
16
 <dig>  × 10
−23
 <dig>  × 10
10
 <dig>  × 10
−17
 <dig>  × 10
12
 <dig>  × 10
−20
 <dig>  × 10
14
 <dig>  × 10
−23
 <dig>  × 10
16
 <dig>  × 10
−25


second, we consider the case of h  l-mers containing pairs of l-mers with different hamming distance, and analyze the effect of hamming distance on the number of common candidate motifs shared by the h l-mers. in our example, we set h as  <dig> and the three l-mers x <dig>  x <dig> and x <dig> can form three pairs of l-mers; then, we fix dh = 2d −  <dig> and vary dh' =  + dh)/ <dig> from 2d −  <dig> to 2d −  <dig>  figure  <dig> and  give the tendency of the number of common candidate motifs |md| in contact with the decrease of dh' on  problem instance for the dna data and  problem instance for the protein data, respectively. the y-axis is in log-scale. we can see that no matter for the dna or protein data, |md| increases with the decrease of dh'. in other words, when h  l-mers contain some pairs of l-mers with a relatively small hamming distance, they generate a relatively large number of candidate motifs. also, the tendency of |md'| = |md| + |md| + |md| is given in fig.  <dig>  both |md'| and |md| increase with the decrease of dh', namely they have the consistent tendency.fig.  <dig> the effect of hamming distance on the number of candidate motifs for three l-mers. this figure shows the effect of hamming distance on the number of candidate motifs for three l-mers x
 <dig>  x
 <dig> and x
 <dig>  the used  is set as  and  under the dna data and protein data, respectively. we fix d
h = 2d −  <dig> and vary d
h' =  + d
h)/ <dig> from 2d −  <dig> to 2d −  <dig>  |m
d| is the number of common candidate motifs shared by the three l-mers. |m
d'| = |m
d| + |m
d| + |m
d| is the sum of the number of common candidate motifs shared by each pair of l-mers. a it is for the dna data. b it is for the protein data



in addition, tables  <dig> and  <dig> also give the probability that the hamming distance between two random l-mers x and x' is i , denoted by pi and calculated by . as seen from the tables, pi decreases with the decrease of the hamming distance i.  <dig> pi=i|Σ|l 

for two n-length sequences si and sj, let ei denote the expected number of the pair of l-mers x ∈lsi and x' ∈lsj with dh = i. it is calculated by . the bold values in tables  <dig> and  <dig> indicate that ei is less than  <dig> in the case of n =  <dig>   <dig> ei=n−l+12×pi 

although they rarely occur in two sequences, some pairs of l-mers with ei <  <dig> are usually contained in the whole data set. the reasons are: first, the whole data set can form multiple pairs of sequences, which increases the probability of the occurrence of the pairs of l-mers with hamming distance i; second, the conservation of motifs makes some highly similar motif instances form some pairs of l-mers with ei <  <dig> 

from the tables, when ei is less than  <dig>  the value of |md| is relatively large, especially for the protein data. thus, the more pairs of l-mers with ei <  <dig> in the reference sequence set, the more candidate motifs generated by the algorithms.

results on practical time improvement of pms algorithms
in this section we check the validity of refselect as follows. first, we use refselect to select k reference sequences from the given t input sequences, and adjust the order of the t input sequences by preposing the k sequences; refselect is implemented in c++ and its running time is denoted by trs. second, we test pattern-driven pms algorithms on the input sequences of original order and that of new order, obtaining the running time t <dig> and t <dig>  respectively. finally, we compare t <dig> with trs + t <dig> 

three pattern-driven pms algorithms qpms <dig>  <cit> , travstrr  <cit>  and pms <dig>  <cit>  are chosen to participate in the test. they are all newly proposed algorithms and outperform the previous exact algorithms on challenging instances. notice that qpms <dig>  <cit>  is also a newly proposed pms algorithm with good time performance; we do not choose it as a tested algorithm and related discussion is given in the applicability of refselect section. all the tested algorithms are executed on a  <dig>  ghz single core and a  <dig> gbyte memory, except for pms <dig>  which is executed on a 16-core platform in solving the  and  instances of the protein data.

in the experiments, we generate data sets following  <cit> . first, we randomly generate t sequences of length n and a motif m of length l, and randomly choose q  out of the t sequences; then, for each of the q sequences, we generate a random motif instance m' that differs from m in at most d positions, and implant m' into a random position of the sequence. for each specific test instance, we generate five data sets to get an average result.

first, we fix t =  <dig>  n =  <dig> and q =  <dig>  and give in tables  <dig> and  <dig> the results on challenging instances of the protein and dna data, respectively. for qpms <dig> and travstrr, k is set as  <dig>  while for pms <dig> k is set dynamically under different  instances according to  <cit> . from this experiment, we find that:table  <dig> running time and speedup for the protein data


t
1
t
rs + t
2
speedup
t
1
t
rs + t
2
speedup
t
1
t
rs + t
2
speedup
s: seconds; −o:over 48 hours; t
 <dig> and t
2: running time of a pms algorithm on the input sequences of original order and new order; t
rs: running time of refselect; min, ave and max: the minimum, average and maximum running time on five data sets; speedup: average t
1/average t
rs + t
 <dig> 


t
1
t
rs + t
2
speedup
t
1
t
rs + t
2
speedup
t
1
t
rs + t
2
speedup
s: seconds; −o:over 48 hours; t
 <dig> and t
2: running time of a pms algorithm on the input sequences of original order and new order; t
rs: running time of refselect; min, average and max: the minimum, average and maximum running time on five data sets; speedup: average t
1/average t
rs + t
 <dig> 

the running time of refselect on each of these data sets is less than one second, which is nearly negligible and not listed in tables  <dig> and  <dig> 

refselect can make the tested algorithms solve the pms problem steadily in an efficient way. for example, for the  problem instance in table  <dig>  the minimum and maximum running time of qpms <dig> are reduced to  <dig> s and  <dig> s from  <dig> s and  <dig> s after using the reference sequences selected by refselect.

the speedup on the protein data is significantly larger than that on the dna data. we give the explanation by using tables  <dig> and  <dig>  the fact that pattern-driven pms algorithms sometimes show poor performance is mainly caused by the pairs of l-mers with ei <  <dig> in the reference sequences; these l-mers can generate more candidate motifs. the larger the difference between the number of candidate motifs for ei <  <dig> and that for ei ≥  <dig>  the larger speedup can be achieved. as can be seen from tables  <dig> and  <dig>  the difference on the protein data  is significantly larger than that on the dna data .

larger speedup is achieved on larger  instances for the protein data. this can also be explained by ei. that is, as shown in tables  <dig> and  <dig>  the difference between the number of candidate motifs for ei <  <dig> and that for ei ≥  <dig> increases with the increase of l and d.



second, we discuss the case of q < t by fixing |Σ| =  <dig> , t =  <dig> and n =  <dig>  in the above three algorithms, we only choose qpms <dig> as the tested algorithm because pms <dig> cannot solve the pms problem with q < t and travstrr usually quits unexpectedly in our test environment. we select t – q +  <dig> reference sequences for qpms <dig>  and test it by using the same  instances with  <cit> . on the one hand, we set q as  <dig>  and test qpms <dig> on different  instances; as shown in fig.  <dig>  refselect makes qpms <dig> perform better and the speedup increases with the increase of l and d. on the other hand, we fix  =  and test qpms <dig> by varying q from  <dig> to 19; as shown in fig.  <dig>  refselect can effectively accelerate qpms <dig> under different q.fig.  <dig> the speedup brought by refselect for qpms <dig> in the case of q < t. this figure shows the speedup brought by refselect for qpms <dig> in the case of q < t. a we fix q =  <dig> and vary  from  to . b we fix  =  and vary q from  <dig> to 19



finally, we test the effect of the sequence length n on the speedup brought by refselect for existing algorithms. in the experiment, we fix |Σ| =  <dig>  and q = t =  <dig>  and vary n from  <dig> to 500; the tested algorithm is qpms <dig> and pms problem instances are  and . the results are shown in fig.  <dig>  overall, the speedup increases with the decrease of n. this is because, according to  and table  <dig>  the smaller the value of n, the larger the difference between the number of candidate motifs for ei <  <dig> and that for ei ≥  <dig> fig.  <dig> the effect of sequence length on the speedup for the dna data. this figure shows the effect of sequence length on the speedup for the dna data. we fix |Σ| =  <dig> and q = t =  <dig>  and vary the sequence length n from  <dig> to  <dig>  the tested algorithm is qpms <dig> and pms problem instances are  and 



the fact that refselect works better for short input sequences makes sense to motif discovery in next-generation or high-throughput sequencing data sets, such as chip-chip  <cit>  and chip-seq  <cit>  data sets. these data sets have a better resolution for containing motifs than the traditional promoter sequences. for example, the length of each sequence in chip-seq data sets is usually  <dig> base pairs, while the length of a promoter sequence is about  <dig> base pairs. therefore, refselect can bring a better practical time improvement of the pattern-driven pms algorithms on the chip-seq data sets than that on the traditional promoter sequences.

assessment of refselect on large data sets
all experiments involved in the previous section focus on the data sets of small scale, namely the number of input sequences t is small. in recent years, with the rapid development of high-throughput technologies, which allows genome-wide identification of motifs, the data sets such as chip-seq  <cit>  contain hundreds or more sequences. thus, it is necessary to further assess the time performance and validity of refselect on large data sets.

first, we make the following settings in the experiment: set the maximum value of t as  <dig>  as the chip-tailored version of meme can effectively identify motifs by using  <dig> sequences randomly selected from the whole chip-seq data sets  <cit> ; set k as 5 % × t, as most of the sequences in chip-seq data sets contain motif instances; set the sequence length n as  <dig>  as the resolution that chip-seq sequences contain motifs is higher than that for traditional promoter sequences  <cit> .

since there is not an exact algorithm that can efficiently deal with large data sets, we assess the validity of refselect as follows. let noriginal denote the number of candidate motifs generated from the first k sequences in the original input sequences, and nimproved denote the number of candidate motifs generated from the k reference sequences selected by refselect. then, we compute noriginal/nimproved. a larger noriginal/nimproved indicates more candidate motifs can be reduced.

on the above basis, we get the running time of refselect and noriginal/nimproved on both the dna and protein data sets, by varying t from  <dig> to  <dig>  from the results shown in table  <dig>  we can find that:  refselect can quickly select reference sequences from these data sets, and its running time is independent of the alphabet size;  the running time of refselect increases with the number of input sequences t and exceeds one minute when tackling the task of t = 600;  refselect can still reduce the generated candidate motifs, especially for the protein data . besides the running time of the whole refselect algorithm, we also list the running time of the first step of refselect, which shows that the first step is the bottleneck of the whole refselect algorithm.table  <dig> assessment of refselect on large data sets


t
k

time
time
a
n
original
/n
improved
time
time
a
n
original
/n
improved
s: seconds; time and time
a: the running time of refselect and that of the first step of refselect; n
original and n
improved: the number of candidate motifs generated from the first k original input sequences and that for the k reference sequences selected by refselect.



second, we test the parallel version of refselect. it is pretty common that a chip-seq data sets contains more than one thousand sequences, and parallel processing is a good choice in this case. in the experiment, we use the protein data sets with k = 5 % × t and n =  <dig>  we give the running time in table  <dig> by varying t from  <dig> to  <dig> and the number of threads from  <dig> to  <dig>  we can find that the acceleration of refselect through parallel processing is obvious, and the speedup is almost linearly proportional to the number of threads.table  <dig> running time of refselect using parallel processing


t


applicability of refselect
for the proper use of refselect, we summarize the applicability of refselect as follows.refselect can accelerate such pattern-driven pms algorithms that use random or the first k ≥  <dig> sequences in the input as reference sequences to generate candidate motifs. for the efficient and recently proposed pms algorithms, refselect is applicable for qpms <dig>  travstrr and pms <dig>  but not for qpms <dig>  which does not use fixed k reference sequences to obtain h-tuples.

refselect can deal with large data sets containing hundreds or even more sequences.

the speedup brought by refselect for pms algorithms is affected by the alphabet size. the larger the alphabet size, the larger the speedup.

the speedup brought by refselect for pms algorithms is also affected by the sequence length n, which increases with the decrease of n.

refselect works better on the challenging instances with large l and d. for the challenging instances with small l and d, however, it is not necessary to use refselect, for they can be quickly solved by existing pms algorithms.



moreover, it is necessary to declare the following two points. first, the instability of the time performance is not reported in the previous literatures  <cit> . this is because we find that in their experimental data, the implanted motif instances differ from the motif in exact d positions. in this case, the probability of containing pairs of l-mers with ei <  <dig> in the reference sequences is small, and accordingly the number of generated candidate motifs is also small. but it should be pointed out that, in reality motif instances differ from the motif in at most d positions, which leads to the execution time instability for some of the existing algorithms.

second, although refselect is not applicable for qpms <dig>  which can solve challenging instances with larger l and d than previous algorithms, our research is still valuable. the reason is that qpms <dig> cannot be used as a substitute for other pms algorithms; we found in the experiments that qpms <dig> sometimes exits unexpectedly with an out of memory error. particularly, this phenomenon becomes frequent in dealing with challenging pms instances of large  such as  =  and .

CONCLUSIONS
we build the reference sequence selection problem and propose a method named refselect to select reference sequences for the pattern-driven pms algorithms, in order to solve the problem that many pattern-driven pms algorithms present execution time instability. refselect requires a small amount of storage space and is capable of selecting reference sequences efficiently and effectively. also, the parallel version of refselect is provided for handling large data sets. for the state-of-the-art algorithms qpms <dig>  travstrr and pms <dig>  refselect enables them steadily solve pms problems in an efficient way without doing any modification to these algorithms.

our work in this paper only focuses on selecting reference sequences for the pattern-driven pms algorithms. it is recommended that further research be undertaken in selecting reference sequences for the iterative optimization algorithms of finding motifs in large data sets. these algorithms, such as meme-chip  <cit> , usually randomly select hundreds of sequences from a large input to make motif discovery, with a low chance of discovering infrequent motifs  <cit> . thus, elaborate selection of sequences may help them obtain more motif information.

