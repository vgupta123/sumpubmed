BACKGROUND
it is clear that one of the remaining challenges hindering the progress of protein fold recognition and comparative modelling is the selection of the highest quality 3d model of a protein structure from a number of alternatives  <cit> . the identification of appropriate templates used for building models has been significantly improved both through profile-profile alignments and meta-servers, to the extent that traditional threading methods are becoming less popular for fold recognition. increasingly, for the majority of sequences with unknown structures, the problem is no longer one of template identification; rather it is the selection of the sequence to structure alignment that produces the most accurate model.

a number of methods have been developed over recent years in order to estimate the quality of models and improve selection. a popular technique has been to use methods such as procheck  <cit>  and whatcheck <cit>  in order to evaluate stereochemistry quality following comparative modelling. these methods were developed in order to check the extent to which a model deviates from real x-ray structures based on a number of observed measures. however, such evaluations are often insufficient to differentiate between stereochemically correct models. traditionally, a variety of energy-based programs have been developed more specifically for the discrimination of native-like models from decoy structures. these programs were based either on empirically derived physical energy functions or statistical potentials derived from the analysis of known structures <cit> . for some time, methods such as prosaii  <cit>  and verify3d  <cit>  have been in popular use for rating model quality. more recently, methods such as proq  <cit> , frst  <cit>  and modcheck  <cit>  have proved to be more effective at enhancing model selection.

during the 4th critical assessment of fully automated structure prediction , such methods were collectively termed as model quality assessment programs  and a number of them were evaluated in a blind assessment  <cit> . for the purposes of cafasp <dig>  an mqap was defined as a program which took as its input a single model and which outputted a single score representing the quality of that model. developers were encouraged to submit mqaps as executables, which were subsequently used to evaluate models by the assessors.

more recently, quality assessment  was incorporated as a new "manual" prediction category in the 7th critical assessment of techniques for protein structure prediction   <cit> . the qa category was divided into two sub categories qmode  <dig> referring to the prediction of the overall model quality and qmode  <dig>  in which the quality of individual residues in the model was predicted. in the qmode  <dig> category, the format of the new experiment allowed users to run their methods in-house and then submit a list of server models with their associated predicted model quality scores. while this new format had certain advantages, it also allowed more flexibility in the type of methods which could be used for quality assessment. for example, this format allowed methods to be used which could not be evaluated as "true" mqaps in the original sense, such as meta-servers approaches which may have used the clustering of multiple models or incorporated additional information about the confidence of models from the fold recognition servers.

in this paper, several of the top performing mqaps are benchmarked in order to gauge their value in the enhancement of protein fold recognition. a number of top performing "true" mqap methods are compared against some of the best clustering and meta-server approaches. in addition, two novel methods, which can be described as true mqaps according to the original definition, are also benchmarked. firstly, the modssea method which is based on the secondary structure element alignment  score previously benchmarked  <cit>  and incorporated into versions of mgenthreader  <cit>  and nfold  <cit> . secondly, modfold which combines the output scores from the proq methods <cit> , the modcheck method  <cit>  and the modssea method using an artificial neural network.

RESULTS
measurement of the correlation of predicted and observed model quality
the official casp <dig> assessment of mqap methods in the qmode <dig> category involved measuring the performance of methods based on the correlation coefficients between predicted and observed model quality scores. in this section, the analysis is repeated both on a global and target-by-target basis. in figure  <dig>  each point on the plot represents a model submitted by a server to the casp <dig> experiment. the models from all targets have been pooled together and so the "global correlation" is shown. the modfold output score is clearly shown to correlate well with observed mean model quality score.

in table  <dig>  the global measures of spearman's rank correlation coefficients  between predicted and observed model quality scores are shown for a number of the top performing mqap methods. the spearman's rank correlation is used in this analysis, as the data are not always found to be linear and normally distributed. the results shown here confirm the results in the official casp <dig> assessment and show the lee method and the modfold method outperforming the other methods tested at casp <dig> in terms of the global measure of correlation. interestingly, the 3d-jury method, which was not entered in the official assessment, is shown to outperform the lee method based on all observed model quality scoring methods. the modfold consensus approach appears to be working in this benchmark, as it is shown to outperform the individual constituent methods . the modssea method, which was not individually benchmarked in the official assessment, also appears to be competitive with the established individual "true" mqaps, which are capable of producing a single score based on a single model.

all models are pooled together and the ρ is measured between predicted and observed model quality scores. the combined observed model quality score was also calculated for each individual model e.g. mean score for each model / <dig> *the mqap scores for these methods were downloaded from casp <dig> website; all other mqap methods were run in house during the casp <dig> experiment. †mqap methods which rely on the comparison of multiple models or include additional information from multiple servers; all other methods are capable of producing a single score based on a single model.

the results in table  <dig> again show the spearman's rank correlation coefficients for each method, but in this instance the rho values are calculated for each target separately and then the mean overall rho value is taken. it is clear that the ordering of methods has changed and this was also shown to occur in the official assessment. the 3d-jury method and the lee method are still ranked as the top performing methods but there is a re-ordering of the other methods. contrary to the results shown in table  <dig>  it would appear that there is no value from using the consensus approach of the modfold method. how can these contradictory results be explained?

target-by-target measure – ρ is measured using the models for each target separately and the overall mean score is calculated. the combined observed model quality score was also calculated for each individual model e.g. mean score for each model / <dig>  *the mqap scores for these methods were downloaded from casp <dig> website; all other mqap methods were run in house during the casp <dig> experiment. †mqap methods which rely on the comparison of multiple models or include additional information from multiple servers; all other methods are capable of producing a single score based on a single model.

the results in figure  <dig> appear to show a roughly linear relationship between the predicted and observed model quality scores with few outliers based on the global measure where the models are pooled together for all targets. however, when the results are examined for individual targets  the relationship is often non-linear, the data are not always normally distributed and there are often a proportionately greater number of outliers which can influence the rho values. in developing mqaps for the improvement of fold recognition the primary goal is to select the highest quality model as possible given a number of alternative models. does the measurement of correlation coefficient on a target-by-target basis always help us to distinguish the best method for selecting the top model?

in figure  <dig> , the scores from modssea and modfold are compared against modcheck and proq for four example casp <dig> template based modelling targets. in these examples the rho values are higher for the modcheck and proq methods, however it can be seen that the observed quality scores for the top ranked models  are shown to be higher for the modfold and modssea methods. of course, there are also several cases where the the rho values for modcheck and proq are lower yet the m scores are higher than either modfold or modssea. indeed by testing on a target-by-target basis, it was found that, on average, for each individual casp <dig> target, the mqap with the highest correlation coefficient between observed and predicted model quality was most often not the method with highest observed quality of the top ranked model.

from the scatter plots in figure  <dig> it is apparent that the correlation between observed and predicted model quality may not necessarily be the best measure of performance if we are interested in methods which can identify the highest quality models. in real situations, developers and users of fold recognition servers would arguably be most concerned with the selection of the best model from a number of alternatives for a given target. the comparison of correlations coefficients should not necessarily replace the individual examination of the data. however, the individual examination of data for each method and for each individual target may not always be practical. it is therefore suggested that a more appropriate measure of the usefulness would be to simply measure the observed model quality of the top ranked models for each target  when benchmarking mqaps for fold recognition.

measurement of the observed model quality of the top ranked models 
results in bold indicate the cumulative observed model quality scores of the top ranked models for each target  obtained by using each mqap method to rank the top models from all fold recognition servers. the maximum achievable mqap score – obtained by consistently selecting the best model for each target – is also highlighted. all other results are based on the cumulative scores of the ts <dig> or al <dig> models from each fold recognition server taking part in the automated category at casp <dig>  each column indicates the method for measuring the observed model quality. scores are sorted by the combined observed model quality. *the mqap scores for these methods were downloaded from casp <dig> website; all other mqap methods were run in house during the casp <dig> experiment. †mqap methods which rely on the comparison of multiple models or additional information from multiple servers; all other methods are capable of producing a single score based on a single model.

the methods which rely on the comparison of multiple models and/or additional information from multiple servers  are shown to greatly outperform the individual true mqaps, however the consensus approach taken by modfold is shown to be competitive.

the cumulative model quality scores of the ts <dig> or al <dig> models from each fold recognition server are also shown in table  <dig>  the 3d-jury, pcons, lee and modfold methods achieve a higher cumulative score than all fold recognition servers except the zhang-server. it must be noted that the cumulative scores which can be achieved by ranking models using any of the existing mqap methods are still far lower than the maximum achievable mqap score obtained if the best model were to be consistently selected for each target. table  <dig> shows the cumulative observed model quality scores if mqap methods are used to rank all models from all servers. for all of the methods, except the 3d-jury method, there is a reduction in the cumulative observed model quality. the lee method outperforms the pcons method but the relative performance of all other methods is unchanged. however, are the differences in m scores from the different mqap methods significant?

the cumulative observed model quality scores of the top ranked models for each target  obtained by using each mqap method to rank all models from all fold recognition servers.*the mqap scores for these methods were downloaded from casp <dig> website; all other mqap methods were run in house during the casp <dig> experiment. †mqap methods which rely on the comparison of multiple models or additional information from multiple servers; all other methods are capable of producing a single score based on a single model.

often the differences observed between methods in terms of cumulative observed model quality scores , may not be significant. the results in tables  <dig>   <dig>   <dig> are provided to demonstrate that the rankings between methods shown in table  <dig> and  <dig> are only relevant if a significant difference is observed according to the wilcoxon signed rank sum tests. the p-values for wilcoxon signed ranks sum tests comparing the mqap methods are shown in tables  <dig>   <dig>   <dig>  the null hypothesis is that the observed model quality scores of the top ranked models  from method x are less than or equal to those of method y. the alternative hypothesis is that the m scores for method x are greater than those of method y.

the different mqap methods are compared in terms of the observed model quality of the top ranked models for each target. h <dig> = mx ≤ my, h <dig> = mx > my, where h <dig> is the null hypothesis; h <dig> is the alternative hypothesis; mx is the observed model quality of models selected by method x and my is the observed model quality of models selected by method y according to the tm-score. * the mqap scores for these methods were downloaded from casp <dig> website; all other mqap methods were run in house during the casp <dig> experiment. † mqap methods which rely on the comparison of multiple models or additional information from multiple servers; all other methods are capable of producing a single score based on a single model.

the different mqap methods are compared in terms of the observed model quality of the top ranked models for each target. h <dig> = mx ≤ my, h <dig> = mx > my, where h <dig> is the null hypothesis; h <dig> is the alternative hypothesis; mx is the observed model quality of models selected by method x and my is the observed model quality of models selected by method y according to the maxsub score. * the mqap scores for these methods were downloaded from casp <dig> website; all other mqap methods were run in house during the casp <dig> experiment. † mqap methods which rely on the comparison of multiple models or additional information from multiple servers; all other methods are capable of producing a single score based on a single model.

the different mqap methods are compared in terms of the observed model quality of the top ranked models for each target. h <dig> = mx ≤ my, h <dig> = mx > my, where h <dig> is the null hypothesis; h <dig> is the alternative hypothesis; mx is the observed model quality of models selected by method x and my is the observed model quality of models selected by method y according to the gdt score. * the mqap scores for these methods were downloaded from casp <dig> website; all other mqap methods were run in house during the casp <dig> experiment. † mqap methods which rely on the comparison of multiple models or additional information from multiple servers; all other methods are capable of producing a single score based on a single model.

the top models selected using the 3d-jury method are shown to be of significantly higher quality  than those selected using any other method according to the tm-score, maxsub score and gdt score. the top models selected using the modfold method are of significantly higher quality than those of proq-mx, proq-lg and modcheck according to the tm-score , maxsub score  and gdt score  . according to the maxsub score the top models selected by both lee and pcons are significantly higher quality  than those selected by modfold .

however, there is no significant increase in the quality of the top models selected by pcons over those selected by modfold according to the tm-score . in addition there is no significant increase in the quality of models selected by the lee method over the modfold method according to gdt score . variation in the predicted secondary structures or other input parameters would explain the observed differences between the in house version of proq-lg and the proq scores downloaded from the casp <dig> website, however the overall difference between scores is not shown to be significant .

the modssea method was developed independently for the casp <dig> experiment, prior to the publication of the comparable method developed by eramian et al.  <cit> . although the two methods are similar in that they both compare the dssp assigned secondary structure of the model against the psipred predicted secondary structure of the target, they differ in their scoring. the two methods were found to show differences in cumulative observed model quality scores , however none of these were found to be significant according to the wilcoxon signed rank sum test with each measure of observed model quality: using the tm-score the p-value was  <dig> , using the maxsub score the p-value was  <dig>  and using the gdt score the p-value was  <dig> .

measurement of the confidence in the true mqap output scores
one of the advantages of the so called "true" mqaps  over clustering methods  and those which use also use information from multiple fold recognition servers , is that they provide a single consistent and absolute score for each individual model. this means that the models from different protein targets can be directly compared with one another on the same predicted model quality scale. conversely, with clustering methods the scores for a given model are potentially variable as they are dependent on the relationship between many models of the same target protein. similarly, the information which can be obtained from multiple fold recognition servers may vary from target to target. therefore, the predicted model quality scores between different targets may not be directly comparable as they do not directly relate to model quality.

the consistency of the output scores from the true mqaps is useful in the context of the structural annotation of proteomes, where it is important to be able estimate the coverage of modelled proteins at a particular level of confidence. in order to be able to measure the confidence of a prediction we must be able to directly compare model quality scores from different protein targets. in figure  <dig>  the confidence in output scores from the  <dig> true mqaps are compared by ranking all models according to predicted model quality and then plotting the number of true positives versus false positives, according to observed model quality, as the output scores decrease. a tm-score of  <dig>  is used as a stringent cut-off to define false positives. models above this cut-off are likely to share the same fold as the native structure  <cit> . a higher true positive rate is shown for the modfold method than for the other mqap methods tested at low rates of false positives. this indicates that we can have a higher confidence in the modfold output score over the other true mqap methods, implying that modfold method should be a more useful method in the context of proteome annotation using fold recognition. in other words, a higher coverage of high quality models can be selected with a lower number of errors.

benchmarking on standard decoy sets
it could be argued that data sets such as the casp <dig> server models provide a more appropriate and larger test set for the benchmarking mqap methods, particularly in the practical context of fold recognition. methods such as modfold, are often developed and tested for the selection of the best real fold recognition model rather than for the detection of the native fold amongst a set of artificial decoys.

however, in order to enable direct comparisons with additional published methods, benchmarking was carried out the using three commonly used standard decoy sets from the decoys 'r' us  <cit>  database  and the results are shown in table  <dig>  the modfold method appears to be competitive with other mqaps using the standard decoy sets according to standard measures of performance such as the rank and z-score of the native structure . however, due to the smaller number of targets in these sets it is not often possible to calculate significant differences between the methods. it is also observed that the relative performance of methods appears to be dependent on which dataset is used, although it is not possible to draw sound conclusions from this data.

rank  <dig> – the number of native structures correctly ranked first by each method out of the total proteins in decoy set; z-score – the average z-scores calculated as the distance in standard deviations from the mqap score of the native structure to the mean score of the decoy set.

measurement of the added value of re-ranking few models from individual servers
it is clear from the cumulative observed model quality scores  in tables  <dig> and  <dig> and wilcoxon signed rank sum tests  that if we have many models from multiple servers then the best mqap methods to use are those which carry out comparisons between multiple models for the same target . however, what if only few models are available from an individual server? can developers and users of individual fold recognition servers gain any added value from re-ranking their models using an mqap method?

the mean difference in cumulative observed model quality scores if each mqap method is used to re-rank the models from each individual fold recognition server. the results achieved from a random re-ranking of models from each server  are also shown for comparison. * the official predicted mqap scores for these methods were downloaded from casp <dig> website; all other mqap methods were run in house during the casp <dig> experiment. † mqap methods which rely on the comparison of multiple models or additional information from multiple servers; all other methods are capable of producing a single score for a single model.

in the case of the caspita-fox server, the cumulative quality score of the top selected models can be improved from  <dig>  to  <dig> , using modfold, which would improve the overall ranking of the method by  <dig> places in table  <dig>  the zhang-server score can also be marginally improved upon from  <dig>  to  <dig>  if modfold is used to re-rank models. several individual servers can also be improved using the 3d-jury method; however, for the majority of servers, there is less benefit to be gained from re-ranking very few models using the clustering approach.

on average the cumulative observed model quality score of an individual server is improved by  <dig>  if the modfold method is used to re-rank the  <dig> submitted models . table  <dig> also shows that on average the quality score of the top selected model is improved for individual servers using the proq, proq-lg and modcheck methods, confirming our previous results  <cit> . the proq-mx, modssea and 3d-jury methods on average show an overall decrease in the quality of the top selected models from each server, if these methods are used as post filters to re-rank models.

the proportion of the fold recognition servers  which have been improved according to observed model quality scores through the re-ranking of models using each mqap method. the results achieved from a random re-ranking of models from each server  are also shown for comparison. * the official predicted mqap scores for these methods were downloaded from casp <dig> website; all other mqap methods were run in house during the casp <dig> experiment. † mqap methods which rely on the comparison of multiple models or additional information from multiple servers; all other methods are capable of producing a single score for a single model.

what if we were also to use the information from the original server ranking in addition to the mqap scores? can further improvements to model ranking be made by using this information as an additional weighting to the mqap score? the results in table  <dig> and table  <dig> show the additional improvement to model rankings made by combining the information from the original server ranking with that of the mqap score. in this benchmark, models initially ranked by a server as the top model achieve a higher additional score than models initially ranked last. a useful additional score was found to be / <dig>  where r is the initial server ranking of the model between  <dig> and  <dig> .

similar to table  <dig>  however the original server ranking is also considered and added to the score as an extra weighting / <dig>  where r is the original server ranking between  <dig> and 5). the results achieved from a random re-ranking of models from each server  are also shown for comparison. * the official predicted mqap scores for these methods were downloaded from casp <dig> website; all other mqap methods were run in house during the casp <dig> experiment. † mqap methods which rely on the comparison of multiple models or additional information from multiple servers; all other methods are capable of producing a single score for a single model.

similar to table  <dig>  however the original server ranking is also considered and added to the score as an extra weighting / <dig>  where r is the original server ranking between  <dig> and 5). the results achieved from a random re-ranking of models from each server  are also shown for comparison. * the official predicted mqap scores for these methods were downloaded from casp <dig> website; all other mqap methods were run in house during the casp <dig> experiment. † mqap methods which rely on the comparison of multiple models or additional information from multiple servers; all other methods are capable of producing a single score for a single model.

this is a stringent benchmark as there are few models to choose from each individual server. this means that there is less information to be gained from a comparison of the structural features shared between models. therefore, the clustering approach  does not perform well at this task. the modssea method also performs badly at this task as it is also dependent on differentiating models based on structural features. if there is conservation of secondary structure among the top few models from the same server, then the modssea method will perform badly. indeed, many servers already include secondary structure scores and so the top models provided by the same server are often likely to share similar secondary structures. the value of randomly selecting the top models  has also been included in tables  <dig> to  <dig>  a random selection of the top model on average shows a marked decrease in model quality as the probability of a correctly selecting the top model for a given target is  <dig> .

CONCLUSIONS
the consensus mqap method  is shown to be competitive with methods which use clustering of multiple models or information from multiple servers  according to the cumulative observed model quality scores of the top ranked models . furthermore, according to this benchmark the modfold method significantly outperforms some of the best "true" mqap methods tested here , all of which produce single consistent scores based on a single model.

benchmarking based on correlation coefficients is not always helpful in measuring the usefulness of mqap methods. there is not always a linear relationship between the mqap score and the observed model quality score and scores for an individual target may not be normally distributed. even with the non-parametric test, outliers can affect the results and so the correlation coefficient should not replace the individual examination of the data. it is therefore proposed that simply measuring the observed model quality scores of the top ranked model  on a target by target basis, or the cumulative scores  over all targets, may be more useful for benchmarking mqaps in the context of protein fold recognition, followed by measures of the statistical significance. in practical terms, predictors require the best model to be selected for a given target and so m is an appropriate measure of the performance of an mqap method in this context.

if there are many models available from multiple fold recognition servers then clustering models using the 3d-jury approach is demonstrably the most effective tested method for ranking models. however, the method can perform poorly when there are very few models available and often no value is added by re-ranking of models from an individual sever. furthermore, methods such as 3d-jury, lee and pcons may not produce consistent scores and therefore scores of models from different targets cannot be directly compared against one another. clustering methods, such as 3d-jury, are also computationally intensive and the cpu time required for calculating a score increases quadratically with number of available models.

the so called "true" mqap methods tested here  are less computationally intensive as they consider only the individual model when producing a score. therefore, the computational time for these methods scales linearly with the number of available models. they are also demonstrated here to add value to predictions when used as a post filter to re-rank even very few models from individual fold recognition servers.

in the context of a casp assessment it is clear that the mqap methods that make use of clustering of multiple models are currently superior to true mqap methods that score individual models. server developers wishing to perform well in casp will therefore be more likely to use and develop the former methods as they will have access to many models produced by many different servers. however, in a practical context, experimentalists may have collected only very few models from the limited number of publicly accessible servers which remain available outside the context of casp. therefore, experimentalists would be advised to consider using the true mqap methods in order to rank their models prior to investing valuable time in the laboratory. however, it is clear that there is room for the further improvement of both the true mqap methods and the methods which make use of clustering and multiple servers, in the selection of the highest quality models. this is evidenced by the maximum possible score that could be achieved by consistently selecting the highest quality model.

