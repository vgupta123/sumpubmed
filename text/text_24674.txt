BACKGROUND
in biological or clinical research the creation of knowledge, here defined as "the realisation and understanding of patterns and their implications existing in information" relies on data mining. this in turn requires the collection and integration of a diverse set of up-to-date data and the associated context i.e. information. these sets include unstructured information from the literature, specifically extracted information from the multitude of available databases, experimental data from "-omics" platforms as well as phenotype information and clinical data. although a large amount of information is stored in numerous different databases  even more is still embedded in unstructured free text. over the last  <dig> years a large number of methods and software tools have been developed to integrate aspects of biological knowledge such as signalling pathways or functional annotation with experimental data. however, it has proven extremely difficult to couple true semantic integration  across all information types relevant in a life science project with a flexible and extendible data model, robustness against structural changes in services and data, transparent usage, and low set-up and maintenance requirements . in principle this difficulty arises from the high complexity of life science data, which is partly an artefact of the fragmented landscape of data sources but also stems from reasons integral to the life sciences. the ever extending "parts-list of life" itself already offers an astounding number of object classes, from the molecular to the organism, even if common naming/identifier and definitions could be agreed upon. in addition experimental data can only be interpreted in the context of the exact identity of the experimental sample, the samples environment, the samples processing and the processing and quality of the generated data. even more than the occasional extension of the "parts-list" from our growing knowledge, technical development continually generates new data types, processing methods and experimental conditions. while life science projects in general will  share some concepts, almost each one will require some individual adjustment to integrate and view the relevant information. therefore an optimal data integration approach will ensure that the data model can be based on existing concepts  yet remains flexible and extendible by the advanced user. in this respect today's most successful  data integration approaches such as srs  <cit>  or entrez  <cit>  show only weak, cross-reference based data integration without semantic mapping to a common concept . they depend on pairwise mappings between individual database entries provided by the data source e.g. from a protein sequence entry to the corresponding transcript, the mappings lack semantic meaning i.e. the notion that a protein is expressed from a gene can not be stated or queried. additional processing and data mapping is required to answer even simple questions such as "which molecular mechanisms are known to be involved in the pathology of chronic obstructive pulmonary disease ?". currently custom-developed data warehouses such as atlas  <cit> , biozon  <cit>  or biogateway  <cit> , are the most common technical concept to achieve full semantic integration . while these are ideally suited to answer complex queries their inflexible and pre-determined data model and the necessary, often difficult, data synchronisation result in high set-up and maintenance costs. further, adaptation of such data warehouses structure to an ever changing environment or requirements are difficult at best  <cit> . fortunately, as more data sources start to adopt semantic web representations such as owl  <cit>  and rdf  <cit>  maintenance for semantic mappings becomes less of an issue as concomitantly to adopting a common language to transport semantics many data sources also standardise the semantics they provide such as using common entity references and ontologies.

an optimisation, at least regarding data synchronisation, has been to present a semantically fully integrated view of the data while the underlying data is assembled on-the-fly from distributed sources using a coherent data model and semantic mappings  <cit>  . details of this approach vary widely. the ad-hoc data assembly process can be provided by home made scripts or, more recently, using workflow engines such as taverna  <cit> . the data model can be programmed with a specific language as in kleisli  <cit>  or may make use of standard ontologies as with tambis  <cit> . semantic mapping to the common concept can be produced by a view providing environment, such as biomediator  <cit>  and the bio2rdf project  <cit> , or can come from individual integrated data sources. in the latter case the data sources either provide such mappings voluntarily, working for the common good of the "semantic web"  <cit>  or are forced to do so by a closed application environment such as cabig  <cit> , gen2phen/page-om  <cit>  or gmod  <cit> . while conceptually elegant, these approaches have some disadvantages: the start-up costs are quite high , the performance is determined by the slowest, least stable of the integrated resources, complex queries result in large joins which are hard to optimise, and data models are often hard to extend. ad-hoc desktop data integration and visualisation tools such as cytoscape  <cit> , osprey  <cit>  or ondex  <cit>  on the other hand combine excellent flexibility with good performance due to local data storage, however they do not allow large scale knowledge bases to be collaboratively generated, managed and shared.

another issue, which is only partially addressed by current data integration solutions, is the need to organise not only public information but project-specific knowledge and data, keep it private or partially private for some time, store and connect experimental results and corresponding metainformation about materials and methods and, if eventually verified, merge it into the pool of common knowledge. this may for example take the form of an existing signal transduction pathway which is privately extended with new members or connections. the extension is then published and discussed within a specific project until it is accepted as common knowledge. while data resources such as geo provide the option to keep submitted data private for some time, they generally do not allow existing knowledge to be extended as described above or allow existing data to be annotated with private or public comments.

our challenge was to develop a knowledge management environment that achieves several goals: focus on the management of project-specific knowledge; ease data model generation and extension; provide completely flexible data integration and reporting methods combined with intuitive visual navigation and query generation; and address the issues of set-up and maintenance cost.

to do so we chose to apply different aspects of the approaches described above. in the next sections we describe the creation of a knowledge base for chronic diseases based on the bioxm software platform that efficiently models complex research environments with a flexible management, query and reporting interface which automatically adapts to the conceptualisation of the modelled information.

implementation
the bioxm rationale
bioxm has been developed around the concept of object-oriented semantic integration. in this concept semantically identical objects, which represent information about the same real world object, and the meanings of associations between these objects are identified and mapped based on data and descriptive metainformation  <cit> . in the life sciences this mostly concerns the mapping of biological entities and descriptive data from literature and databases to common instances of objects like genes, phenotypes or patients., associations between the entities are mapped as relations  and object - relation information is contextually structured . based on objects as nodes  and relations as edges a "semantic network" which provides semantic information about the connection between participating object instances can be generated.

to enable the extraction of knowledge from integrated information the definition of a protein complex through a series of associations  should be supplemented by evidence why, when and where protein a participates in complex b and how we know about it. in bioxm any associated information not represented directly as object or relation can be stored as "annotation" or as a special, performance optimised type "experiment" for high-throughput data. such evidence may constrain the association to certain conditions e.g. "proteina interacts with proteinb during cell cycle phase m". sub-sections of the semantic network which are true only in specific conditions are modelled as "context". these elementary concepts, "elements", "relations", "annotations" and "contexts" can be defined freely or use existing structured, controlled vocabularies . due to this flexibility when configuring the data model, it becomes possible to formalise huge networks of knowledge  even when concepts as diverse as molecular processes, disease phenotypes or clinical information about patients are concerned. these complex semantic networks of relationships allow one to detect connections, extract patterns and answer complex questions like: "which drugs are known to interact with the over-expressed genes of patient a and have been shown to influence the patients cancer type in a clinical trial?" or "retrieve all inflammation related human genes which interact  with the metabolic enzymes in skeletal muscle which are affected by copd".

strong usability is a pre-requisite for user acceptance of any knowledge management system and is approached in several ways in bioxm. the data model itself can be generated graphically and, based on project requirements and processes, may start as a simple sketch converted into a concept map. parts of the concept map may develop into complex ontologies as discussion and collaboration with the user community extend quick ad-hoc sketched concepts iteratively into precisely defined and structured vocabularies. while not providing an environment for formal ontology development and evaluation such as protege  <cit> , bioxm allows ontologies to be used for knowledge representation and supports importing, integrating, editing and exporting ontologies. to optimise performance, storage and maintenance resources, data can be managed alternatively within an underlying relational database or be seamlessly integrated from external data sources . this combines the advantages of a data-warehouse-based deep integration with the low maintenance costs of a federated environment. external information can be incorporated by using the embedded biors™integration and retrieval system  <cit>  or by directly accessing external objects from e.g. a relational database, a web service or a software application. integration of the external resource is achieved by xml based registration of the application interface into bioxm. the registration must contain information about the actual programmatic access to the application and the attributes made available. registered objects become directly available within the graphical data model viewer where they can be mapped in the same way as internal objects . external information can serve either as "virtual" semantic objects, or as "read-only" annotation of semantic objects. read-only annotations add information to any object, relation or other annotation; the information remains within the external data source and is read by the bioxm system on-the-fly but not modified. "virtual semantic objects" can be organized in the project tree, can become part of a network, and can be annotated by the user like any other object. however, in contrast to bioxm internal objects they can not be changed e.g. renamed and the available search functions and performance depend on the implemented interface and features of the external source. any change in content in the original data source will become visible in bioxm immediately while full control of the business logic of publishing the objects and associated information remains with the external source. to access the data bioxm provides visual browsing of the knowledge network  as well as "natural language like" data mapping and query wizards  which automatically adapt to the changes of a given data model. the system thus enables scientists to create knowledge networks with flexible workflows for handling data and information types, including annotation and ontologies. in this way research projects can be modelled and extended dynamically to ultimately gain an adequate understanding of whole biological systems. as we are aware of the many issues associated with metainformation based identification of semantically identical objects we do not aim to provide an overall algorithmic solution to these but rather a framework which supports different mapping methods  and solves the organisation and use of the resulting semantic network.

software implementation
details about the technical implementation of the bioxm software have been published elsewhere  <cit>  and are only briefly summarised here.

bioxm is implemented as a platform-independent java client-server application. the client java application downloads as a webstart package and can be started from any browser . the server application has a modular architecture . the resource and user management module allows control and restriction of user rights to view or modify certain data or parts of the data model. a complete audit trail for all entities and their relations is logged and supports secure data management. the application uses a relational database management system as backend . a hibernate layer is employed for object-relation modelling. therefore the database scheme is generic, containing the information about the object oriented data model as content. the command line and the soap web service application-programming interfaces  allow to integrate bioxm into larger bioinformatic infrastructures. based on these apis external applications such as blast  <cit>  or network search algorithms are integrated. a plugin for the r statistics language http://www.r-project.org/ allows to access bioxm data directly from within the r interface and also to integrate any r method transparently as native bioxm views or analyses.

RESULTS
the eu fp <dig> biobridge systems medicine project http://www.biobridge.eu focused on the integration of genomics and chronic disease phenotype data with modelling and simulation tools for clinicians to support understanding, diagnosis and therapy of chronic diseases. we have configured and extended the generic bioxm knowledge management environment to create the knowledge base for this translational system biology approach, focusing on chronic obstructive pulmonary disease  as an initial use case.

data model configuration
in general within bioxm a particular scientific area of interest is semantically modelled as a network of related elements . while there is some agreement throughout the life sciences regarding a number of semantic objects such as gene or phenotype the different communities such as clinical research, virology, plant research or synthetic biology differ on the concepts and definitions they use. not only will a plant related knowledge base require the object plant instead of patient, a vector in virology might describe an infectious agent while in synthetic biology it will more likely define a dna expression shuttle. different ontology development initiatives  try to develop a consensus on these issues but currently no overarching "life science" data model can be defined. therefore one of the main features of bioxm is the ability to dynamically create a data model specific to the project for which it is used and, while a project develops, easily adapt the data model based on the consensus between the project stakeholders.

patient record
protein entry
literature abstract
metabolomics data
gene ontology to classify biological function
the fundamental semantic objects used in bioxm which allow a descriptive model of the world to be formulated and data resources to be related to that model. the semantic objects allow an extendable model to be defined from a set of well-defined building blocks .

within biobridge we had initial discussions between the project partners  about the kind of knowledge that needed to be represented  as well as how this representation should be conceptualised. this is a typical first step for a knowledge management project which usually provides only an initial, rough idea of the final goal that will need many iterations to reach. setting-up and changing the data model is done directly within the data model graph  using a context menu to create and edit the fundamental semantic objects . from the "new" function the basic semantic concepts  become available to generate for example an element gene. by selecting existing object definitions they can be edited, connected by new or existing relations, assigned an annotation or assigned into a context. we created elements such as "gene", "protein", "patient", "compound" and a total of  <dig> element types sufficed to describe the clinical and biological knowledge relevant to chronic disease . information about the interconnection of entities is captured in relations such as "is medicated by" for the compound a patient receives as medication or "regulates" for a protein that regulates expression of a gene. the initial data model was configured within one week and subsequently many iterations extended and adapted the model while the knowledge base was already being populated and in productive use. in total within the current biobridge copd knowledge base we used  <dig> relation types to capture the details of semantic connections between element types, ontology terms, experiments or sub-networks. to describe assemblies of entities or relations with some common feature e.g. a signalling pathway we defined sub-networks as contexts.  <dig> types of contexts where used to capture for example sbml-based simulation models  <cit> , kegg pathways  <cit>  or inflammatory processes involved in copd derived by literature mining. all "semantic concepts"  can be associated by annotations with information such as age, weight and gender for a patient, function for a gene or experimental evidence for a protein-protein interaction. annotations are based on freely definable forms and support hierarchical organisation of information . multiple semantic objects can share annotation to imply relationships. within biobridge we defined  <dig> annotation forms with  <dig> attributes to provide for example, electronic case report forms for anthropometric, diagnostic, physiologic and questionnaire data. experimental data is seen conceptually as special, performance optimised annotation. we defined seven experiment formats covering data types such as transcription, metabolite or enzyme kinetics. bioxm supports the conceptualisation of entire areas of interest by using ontologies, which can be used to infer facts and construct abstract queries.  <dig> different pre-existing ontologies such as go or the nci-thesaurus were integrated into the biobridge data model. the graphical data model builder and the context menu used for the configuration of the biobridge specific data model is shown in figure  <dig>  within biobridge we focus on the configuration of a data model suited to knowledge, clinical and experimental date around copd, other instances of bioxm have been configured to cover different diseases or indeed fields of life science research such as enzyme biotechnology or synthetic biology, underscoring the general applicability of the semantic data model configuration process.

molecular complexes
comparative
toxicogenomics
database
mips
public data integrated into bioxm in the current version of the biobridge copd knowledge base.

populating the data model
the model described above enabled us to semantically integrate existing public databases and information derived from the literature with clinical and experimental data created during the biobridge project.

to populate the knowledge base with data from public databases with large sizes and regular updates we mainly use virtual objects with a manual mapping of the object concept into the data model. for sources without appropriate interfaces or weak performance and for project internal data and knowledge we manually generated mappings in import-templates. a graphical wizard for import-template generation provides a selection of possible import options and mappings. only applicable objects of the data model are presented for example after defining an object as gene only relations enabled for gene are available for the next mapping step. the available selection automatically adapts to any change in the data model configuration. from this selection the import operations are assembled by drag-and-drop to provide the mapping for a given data source forming an import script which can be saved and re-used . for sources which are imported and provide regular updates a scheduling system is used to define automatic execution of the corresponding access and import methods.

mapping a resource to the data model requires expertise about the semantic concept of the resource and the configured bioxm data model. to integrate the individual entities of a data source semantically the mapping method for the entities need to be defined. if available, bioxm makes use of namespace based standard identifiers, existing cross-references and ontologies for the population of the data model. in most cases the semantics of a given data source are not  described in machine readable form and the initial mapping template needs to be generated manually. the bioxm core framework is extended with pre-defined semantic mappings currently existing for about  <dig> public data resources and formats . in addition text-mining and sequence similarity  based mappings are enabled, however users need to be aware of the pitfalls of these methods as no automatic conflict resolution is attempted.

for the copd knowledge base we use entities, references and id mappings provided by entrezgene  <cit> , genbank  <cit> , refseq  <cit> , hgnc  <cit> , ensembl  <cit> , uniprot  <cit>  and embl  <cit>  to populate the system with instances of genes and proteins from human, mouse and rat. starting with entrezgene we create gene instances and map entities from the other sources iteratively by reference. for each database the quality of the references to external sources needs to be judged individually and correspondingly be constrained against ambiguous connections. uniprot protein entries for example provide references to dna databases, with some references pointing to mrna which allows the corresponding gene to be identified uniquely, and others pointing to contigs and whole chromosomes with multiple gene references. use of references in this case therefore is constrained to the target entry type "mrna". a new instance is generated for each database entry from the corresponding organisms which can not be mapped to an existing instance by id reference; no name based mapping or name conflict resolution is attempted at this stage. as the knowledge base develops iterative rounds of extension occur with additional data sources. based on non-ambiguous identifiers we map additional information from the sources described below with mappings being extended, removed and remapped during each updating round.

generating an import template using the import wizard requires no software development knowledge and for many sources only takes minutes . however, integration can also take up to a week of software development if extensive parsing and transformation of a complex data source such as ensembl is required.

naming conflicts and lack of descriptive, structured metainformation are the main reasons for the lack of semantic integration in the life sciences, issues that are as much technological as sociological. the use of a structured knowledge management tool within biobridge ensured all newly produced data makes use of unique identifiers and provides extensive, structured metainformation. this semantic integration and standardisation fostered data exchange as well as social interactions within the project, which are a pre-requisite for translational systems biology projects and their highly diverse multi-subject expert teams. in addition the semantic integration greatly simplifies the future sharing of the produced data as it is immediately available in semantic form.

for the import of data several formats are supported from simple manually mapped delimiter formats such as tab-delimited to xml formats with potentially fully automatic semantic mapping like pedro  <cit> , sbml or owl. if machine readable metainformation is provided, such as miriam references in sbml, they are used to automatically map the imported entities to existing instances of semantic objects. in the current version, the knowledge base integrates more than  <dig> different public databases  representing a total of  <dig>  <dig> relevant genes ,  <dig>  <dig> pathways,  <dig>  <dig> compounds with related gene/disease information,  <dig>  <dig>  <dig> protein interactions and the entire gene expression omnibus and pubchem databases resulting in a total of  <dig>  <dig>  <dig> connections within the knowledge network. in addition two biobridge specific datasets,  <dig> inflammation and tissue specific pathways and  <dig> copd and exercise specific metabolite and enzyme concentrations and activities were manually curated from the literature within the project. the pathway curation followed a standard text-mining supported process as described for example in  <cit>  while the enzyme concentration and activity curation was fully manual due to the small set of available relevant publications. to our knowledge biobridge thus provides the first semantically integrated knowledge base of public copd-specific information. in addition the resource will be continuously extended as more copd specific data becomes publicly available e.g. the experimental data generated within biobridge  will become publicly available as soon as the consortium has generated an initial analysis of the data. currently the copd knowledge base contains almost  <dig> million experimental result data of which almost  <dig> million come from public data. in other projects we are currently using bioxm with several hundred million data points on networks with tens of million edges and nodes, showing that the approach scales for at least two more orders of magnitude .

browse, query and retrieve
here we provide an overview of the available functionality, for a detailed step-by-step tutorial of the knowledge base please see the additional file 3: step_by_step_tutorial_bioxm.pdf. when accessing the biobridge portal http://www.biobridge.eu/bio/ you will have to register to access the knowledge network . then access the knowledge base by following the bioxm links. the bioxm user interface  provides a visually driven query system with which information can be browsed from a network graph , based on pre-defined queries  or by interactive query generation .

users visually browse and query the network simply by right-clicking on any focus of interest  so that associated entities can be added to the existing network visualisation. the corresponding context menu is dynamic, offering all those entities for selection which, based on the data model, are directly associated with the initial focus . a researcher could, for example, expand from a gene to include its relationships with diseases. from the disease association it may be of interest to identify patients represented in the gene expression database who share that particular diagnosis. entities distanced by more than one step in the data model can be associated with each other by complex queries which transverse several nodes within the graph and aggregate information to decide whether a connection is valid. these complex queries are transparent to the user who executes them as part of the graphical navigation when asking for "associated objects" . graph based navigation will become difficult in terms of visualisation layout and performance beyond several thousand objects.

the intuitive graphic query system therefore is supplemented by a more complex wizard that allows dynamic networks to be created by in depth, structured searches, which combine semantic terms that are dynamically pre-defined by the data model . the query construction is natural language like and thus allows to generate complex searches without knowledge of special query languages such as sql or sparql but some knowledge about the data model must be acquired to work efficiently with the wizard. a search for all patients diagnosed with copd severity grade above  <dig> but no cancer which have low body mass index for example would read: "object to find is a patient which simultaneously is annotated by patient diagnostic data which has gold attribute greater than  <dig> and is annotated by patient anthropometrics which has bmi-bt attribute less than  <dig> and never is diagnosed with a nci thesaurus entry which is inferred by ontology entry which has name like '*cancer*'". a query can be saved as a "smart folder" or query template for re-use and thus allows experienced users to share their complex queries with less frequent users. for saved queries "query variables" can be defined so generic smart folder queries can be adapted to specific question. in the example above the actual parameters for copd severity grade, bmi and diagnosed disease might be set as variables for other users to change. normal folders  allow users to organise data manually in their private space by drag-and-drop e.g. to create a permanent list of "favourite genes" or a specific pathway. in contrast the content of "smart folders" is dynamic, as it is actually a query result, which immediately updates whenever changes in the content of the knowledge base occur. defining a query takes between seconds for simple "search all compounds used as medication" type questions to tens of minutes for complex questions which traverse the full connectivity of the semantic network. in the same way performance of query execution directly depends on the complexity of the query. queries traversing many connections in the semantic network with entwined constrains may take several minutes to execute while simple queries even with millions of results return within seconds.

all individual entries, contents of folders and query results are organized in configurable reports presenting the initial element and any desired associated information. to reflect differences in interest and focus for different users multiple reports can be defined for any object for example providing a quick overview of patient laboratory data for the clinician while another patient report provides the expression data for the data analyst. reports can be exported both in tabular and xml format. reports can transparently integrate external applications  in order to derive visualisation and/or further analysis of the data . the report is based on individual "view items" defined with the same type of wizard as for query generation and import template generation.

therefore configuring reports does require no software development skill but is based on an understanding of the data model configuration. as with the query and import template wizards, view items are drawn from the data model using functions such as "related object", "assigned annotation" or "query result" which can be further restricted to specific types such as "relation of type protein expression". configuring a new report on average takes only minutes but, as reports can contain query results, can also take tens of minutes if a new complex query needs to be defined. reports defined for an object like gene can be re-used as "nested reports" wherever a gene type object is included as view item in another report allowing complex reports to be assembled from simple units.

report display performance directly depends on the configuration and takes between seconds and several minutes. simple, fast reports depend on directly related information e.g. a gene report which brings together sequence variant, gene-disease and gene-compound information. complex, slow reports integrate queries to traverse the semantic network and pull together distant information e.g. the medication for the patients for which a given gene was upregulated.

view items can also be used within "information layers" which visualise the information directly on top of a network graph by changing size and colour of the displayed objects. to define the information layer ranges of expected numerical or nominal values in the view item are assigned to colour and size ranges for the graphical object display. in a simple case this is used to display expression data on top of a gene network but based on using query results as view items it can also be used to display the number of publications associated with a gene - phenotype association. within the graph information layers are executed for every suitable object displayed and depending on the complexity of the defined view item the generation of the overlay can take between seconds and several minutes.

application case
the biobridge knowledge base implementation enables the integrative analysis of clinical data e.g. questionnaires, anthropometric and physiologic data with gene expression and metabolomics data and literature derived molecular knowledge. the knowledge base is currently used by data analysis and modelling groups within biobridge to extend literature-derived, copd-specific molecular networks with probabilistic networks derived from expression data . output of the probabilistic networks together with expression and metabolomics data is then used to tune mathematical models of the central metabolism  for copd specific simulations.

as an example use case we briefly describe the initial search for connections between molecular sub-networks affected by exercise, which is generated as a starting point of the biobridge investigation . to this end we searched the expression data in the copd knowledge base to retrieve studies involving patients diagnosed with diseases affecting muscle tissue or involving "exercise" as treatment. based on these experiments we used the r interface to conduct a principal component analysis to extract the kegg pathways most strongly affected by expression changes . interestingly the affected pathways are mainly associated with tissue remodelling and signal transduction pathways. using the enzyme and compound concentration and kinetic measurements extracted from key manuscripts on muscle dysfunction in copd and training effects we find that a number of compounds and proteins involved in key pathways with altered expression derived from the principal component analysis show significant changes in concentration/activity in the published literature. therefore independent support from existing knowledge is gained from the integrated copd knowledge resource for the statistical analysis results of expression data. for these compounds and proteins we use the network search algorithm to search the entire copd knowledge network but restrict the allowed connections to human genetic interaction, protein-protein interaction, gene-compound or gene-disease interaction . the resulting network connects inflammatory and metabolic processes affected in copd patients . visualising the number of disease specific pathways associated with each of the nodes as information layer in this network immediately provides one aspect of the potential weight of the individual nodes for future investigation  with tnf-receptor associated factor  <dig> and  <dig> showing the highest relevance in this respect for all newly connected nodes. the definition of the network search can be changed to include multiple additional information types e.g. drug-relations. weighing of evidence is achieved by specifying penalty weights in the network search parameter set for each type of relation searched. additionally query results provide further evidence directly visualised as information layer on top of the network graph to change size and colour of the displayed objects. within the query all attributes available in the knowledge base can be used as described in the previous section, from the number of independent literature occurrences supporting a gene-disease relation to the category of the experimental evidence supporting a protein-protein interaction to derive an informed weighted list of further investigation targets. different information layers can be defined to overlay different types of information such as quantitative experimental data from gene expression or qualitative text-mining results. the decision about what kind of evidence should be weighted in which way is depending on the individual type of data considered, we do currently not provide automatic scoring or ranking algorithms. however, based on the r plug-in or the api, r-scripts or external algorithms can be integrated to calculate corresponding scores. due to the fact that information layer can integrate complex queries for many objects this is a performance critical function and may take several minutes to execute.

CONCLUSIONS
while the promises of the semantic web continue to creep slowly into existence  <cit>  individual projects need immediate, adaptable solutions which allow project specific knowledge conceptualisations to be set-up with low start-up cost and the flexibility to extend, standardise and exchange their data and knowledge. using a generic knowledge management framework we were able to configure and populate a productively used, project specific systems biology knowledge base within  <dig> month with similar, software development based integration projects being reported to take between 2- <dig> years  <cit> . the copd knowledge base, set-up as the central knowledge management resource of the biobridge project, provides a free, comprehensive, easy to use resource for all copd related clinical research and will be continuously extended aiming to generate the definitive resource on clinical research in copd. more broadly our configuration based approach to semantic integration is generally applicable to close the knowledge management gap between public and project specific data affecting a large number of current systems biology and high-throughput data dependent clinical research projects. to bridge the gap between the current user interface, tuned to suit experienced, frequent users, and everyday clinical application the biobridge project developed a simplified web portal interface for a number of use-cases. based on the feedback from clinical users we recently developed a foswiki  <cit>  plug in for bioxm which unifies the simple set-up of a wiki with the knowledge management functions. from this we will develop a browser based portal as the primary access to the copd knowledge base. future directions include: support of additional structured languages for import and export such as biopax  <cit> and cellml  <cit> ; development of a workflow framework for data analysis and integration of algorithmic methods for semantic mapping.

availability and requirements
the biobridge copd knowledge base as a data resource is freely available to academic users . upon request to dm the bioxm software application itself can be made available for academic phd research projects within the biomax bioxm phd collaboration programme.

competing interests
dm, wk, mw and sl are employed at biomax informatics ag and will therefore be affected by any effect of this publication on the commercial version of the bioxm software. jr, ff, nt, mc, imm sk and jvf are employed at academic institutes and will therefore be affected by any effect of this publication on their publication records.

authors' contributions
dm developed and populated the biobridge specific data model, provided pre-structured queries and reports and drafted the manuscript. wk devised the bioxm software architecture. mw devised and implemented the bioxm r interface and integration. sgk co-developed the experimental part of the biobridge data model and provided data analysis. jr conceived the integrative biobridge approach and provided input to the biobridge specific data model. ff conceived the data analysis workflow and co-drafted the manuscript. nt implemented the data analysis workflow and co-drafted the manuscript. mc co-developed the biobridge data model, conceived the literature curation process and co-drafted the manuscript. imm implemented the literature curation for copd specific enzyme and compound data. mh implemented the biobridge portal integration with bioxm. jfv conceived the biobridge portal architecture. sl devised the bioxm concept and managed its implementation. all authors read and approved the final manuscript.

supplementary material
additional file 1
xml export of the full copd knowledge base data model containing all semantic objects, annotation and experiment definitions.

click here for file

 additional file 2
table of existing mapping scripts and wrappers.

click here for file

 additional file 3
step-by-step tutorial to access and query the copd specific bioxm instance set-up as part of the biobridge project.

click here for file

 additional file 4
a set of affymetrix probes used within the step-by-step tutorial.

click here for file

 additional file 5
r script for pathway level principal component analysis of expression data integrated into bioxm and used as part of the analysis described in the step-by-step tutorial.

click here for file

 additional file 6
r script for significance testing of differentially regulated pathways identified by principal component analysis integrated into bioxm and used as part of the analysis described in the step-by-step tutorial.

click here for file

 additional file 7
analysis of the sub-network connecting inflammation to central metabolism which is derived from the overall copd knowledge network based on shortest path network search.

click here for file

 acknowledgements
we thank all the members of the biobridge team for valuable discussions on the data model and their evaluation of bioxm. this work was supported by the european commission  biobridge lshg-ct-2006- <dig>  we thank the unknown reviewers for their very valuable suggestions and criticism which helped to clarify and structure the manuscript.
