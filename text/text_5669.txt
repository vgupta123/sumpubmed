BACKGROUND
the amount of scientific biomedical literature readable by computer programs is overwhelming. for example, pubmed  <cit>  contains about  <dig>  million article abstracts. therefore automatic literature-mining methods can be exploited in order to retrieve relevant information . for example, several algorithms have been developed for extracting information about protein-protein interactions from the biomedical literature  <cit> .

the problem
in order to find the relations between biological or chemical entities, first the names of the entities have to be recognized in a reliable way. there has been a significant amount of effort to do that automatically  <cit> . a large standardized domain corpus helps to consolidate the research efforts. the genia corpus  <cit>  has been commonly used in biomedical named entity recognition. the state-of-the-art systems have recently been compared, for example, in kim et al.  <cit>  using the genia corpus.

the task of named entity recognition can be divided in two subtasks, the identification of entities, that is, determining the boundaries of the named entities, and their classification into proper classes. the problem of finding the class the entity belongs to can be treated as a word sense disambiguation  task, which, on its own, is an essential part of natural language processing .

the entities in biomedical text are highly ambiguous. for example, it is common that a gene has the same name as the protein it codes for. in the following three sentences from the genia corpus, the occurrences of bzlf <dig> are a protein, a gene, and an rna, respectively:  expression of either bzlf <dig> or brlf <dig> triggers expression of...  ... dna in lymphoblastoid cell lines induced by transfection with bzlf <dig>  and  ... lysis of certain hla b8+ lcl targets was associated with the abundance of bzlf <dig> transcripts. similar ambiguity is illustrated in the two sentences given by hatzivassiloglou et al.  <cit> : by uv cross-linking and immunoprecipitation, we show that sbp <dig> specifically binds selenoprotein mrnas both in vitro and in vivo. the sbp <dig> clone used in this study generates a  <dig> nt transcript . the occurrence of sbp <dig> is a protein in the first sentence, whereas the occurrence of sbp <dig> in the second sentence is a gene. in the same study, a domain corpus was annotated by three biology experts. the three experts unanimously agreed only in 78% of the cases, each name being classified as either a gene, protein or mrna. this low rate of inter-annotator agreement suggests that the task is relatively difficult even for human experts, reflecting the inherent complexity of the domain. however, the study does not analyse more closely the reasons that lead to annotation disagreements.

in this paper, we consider the disambiguation of the sense "gene" or "protein" when the name is not disambiguated explicitly by the author with the word "gene" or "protein" . this task is important, because the release of the human genome and large scale functional genomics studies and methods have made it important to be able to find information from literature specifically for proteins and the corresponding genes. however, database searches provide a lot of hits among which the correct and important articles have to be sorted manually. therefore, for example, in data mining related to proteomics the scientists could save much time if they could direct their search only to proteins.

much of the ambiguity in biomedical text is caused by inconsistent or non-existent naming conventions. for example, there exist drosophila gene names such as ring and arc that can be confused with their ordinary meanings. manual analysis of a small set of abstracts returned by pubmed for the query ring and drosophila shows that the word ring appears in its gene/protein sense in about 30% of the cases, both capitalized and non-capitalized. similarly, the word arc is ambiguous and appears in about 75% of the cases in its gene/protein sense, again both capitalized and non-capitalized. in both cases, only abstracts regarding drosophila were considered, thus the two example words retain their ambiguity even in the sublanguage of articles concerning drosophila. in contrast, some other gene/protein names, such as, tax do not retain their ambiguity in the sublanguage: all occurrences of tax in the genia corpus refer to its gene/protein sense. another major source of ambiguity in scientific biomedical text are abbreviations, which are widely used and therefore are very important to be identified correctly in natural language processing applications  <cit> .

there have already been applications of word sense disambiguation methods in the field of scientific biological text processing. hatzivassiloglou et al.  <cit>  disambiguated names of genes, proteins and rnas using a naive bayes classifier. previously we have developed a method named here weighted additive classifier  and applied it to the problem of gene/protein name disambiguation  <cit> . liu et al.  <cit>  disambiguated abbreviations from medline abstracts using a naive bayes classifier. yu et al.  <cit>  achieved better results for the same task using support vector machines  with the one sense per discourse hypothesis. furthermore, a system developed by podowski et al.  <cit>  assigns gene names to their locuslink ids in previously unseen abstracts.

lee and ng  <cit>  performed a comparison of several supervised learning algorithms for wsd tasks and in their study, svms were confirmed to have the best performance. svms have also been applied in biomedical wsd  as well as in biomedical named entity recognition  <cit> . furthermore, in the coling- <dig> jnlpba shared task of bio-entity recognition  <cit> , five studies  <cit>  used svms either alone or combined with other algorithms. in this paper, we apply svms and as baselines, we consider the naive bayes and wac classifiers. the studies mentioned above use narrow context windows and focus mainly on studying different features such as orthographical, morphological, lexical, contextual, part-of-speech, head-noun, and name-alias features. we, in contrast, focus on context representations that use distance of the words from the ambiguous name in addition to the context words themselves. we evaluate the methods on the genia data set , using the area under roc curve  as a performance measure.

support vector machines
svms can be used to classify multidimensional data into two classes. svms were introduced by boser et al.  <cit> . thorough presentations of svms are given by burges  <cit>  and vapnik  <cit> , for example, and in the methods section we give a concise introduction to svms. in a binary classification task, the training set consists of data points which are labeled as positive or negative. in our case, the training data points are the contexts of the ambiguous names. the positive and negative labels denote genes and proteins, respectively. in order to improve linear separability, the data points are mapped from the input space to a new feature space before they are used for training or for classification. the mapping is done implicitly by a so-called kernel function, which computes the similarity of two data points in the feature space. the choice of an appropriate kernel function is a nontrivial problem, but there are certain standard kernel functions which are frequently used. kernels and the svm itself also have certain parameters which have to be adjusted in order to make the svm classifier work in the best possible way.

contribution of this work
in short, we considered application of svms to the gene versus protein name disambiguation problem in abstracts of biomedical articles. while other studies focus mainly on studying different features, our work primarily considers context representations. we resolve the ambiguous names using their context which spans up to the whole abstract, in contrast to other previous applications of svms which typically use narrow context windows. to improve the performance of conventional svms and accommodate the wide context span, we adopted a weighting scheme introduced by ginter et al.  <cit>  that exploits the information about the distances of the words from the name to be disambiguated, and adjusted the scheme for the svm classifier. we carefully searched for the best parameter values of svms and kernel functions using grid optimization as suggested by hsu et al.  <cit> , and we also performed a similar search for the parameters of the proposed weighting scheme. finally, we measured the performance of both conventional and weighted svms together with two baseline methods, and showed that the performance improvement was statistically significant.

RESULTS
we experimented with the protein versus gene name disambiguation problem using conventional svms with linear, gaussian, as well as second and third degree polynomial kernels. also, we tested svms using different kernels augmented with the proposed weighting scheme. as additional baseline classifiers, we used the weighted additive classifier, which also uses contextual weighting, and the naive bayes classifier. these methods and their parameters to which we refer in this section are described in detail in the methods section.

in the following, we first discuss the weighting scheme and the reasons why its use is beneficial. then, we present how the data was generated and preprocessed. finally, we present the performance measure used in the experiments, describe the experimental setting and the results of the parameter estimation and the final validation.

contextual weighting
the training data points are vectors of word frequencies in the context in which the names to be disambiguated were found. the basic svms with any kernel use only the word frequencies and do not take into consideration the distances of the words with respect to the position of the name to be disambiguated. however, the distance information seems intuitively to be important, and therefore we apply a weighting scheme that incorporates this information into the context representation used by svms. the weighting scheme models the distances of the words from the ambiguous name, while the information whether the words are before or after the ambiguous name is not considered. the weight of a context word at the distance d is given by d-λ + β, where the parameters λ and β are used to control the effect of the distances of the words from the name to be disambiguated. a more detailed explanation of the weighting scheme is presented in the methods section.

we now discuss some possible reasons for the weighting scheme achieving a statistically significant gain in classification performance. yarowsky  <cit>  argues that the effect of context words is strongest for immediately adjacent words, and weakens with distance. this phenomenon is called the one-sense-per-collocation principle. yarowsky also considers the one-sense-per-discourse principle, that is, all instances of an ambiguous word tend to have the same sense within one discourse unit, the article abstract in our case. in that case also distant words can help in disambiguation. one-sense-per-discourse is, however, presumed to be a weaker hypotheses, which should be overridden when the local evidence is strong. in order to study the tenability of these hypotheses in our data, we estimated the following conditional probabilities of the name to be disambiguated to have another instance of an ambiguous name in its context. for each distance from the name to be disambiguated, we estimated the conditional probability that there is a word in the context at that distance and the word is another ambiguous name with the same sense, as well as the conditional probability that the word is a name with the opposite sense. these probabilities are illustrated in figure  <dig>  where the solid line denotes the probability of an occurrence of a name with the same sense and the dashed line denotes the probability of the opposite sense. at close distances , the probability of the same sense turned out to be high and decreasing with distance, whereas the probability of the other sense behaved in the opposite way. in the proposed method, the words in the area of influence of the one-sense-per-collocation principle, that is, the words at close distances, have more weight than the long distance words and these weights are controlled by the parameter λ. on the other hand, at long distances, the probabilities of the same and the opposite senses settled down to  <dig>  and  <dig> , respectively, indicating that mostly the one-sense-per-discourse principle holds. therefore, when the close context is unable to make a strong decision, the information of the long distance words may be useful. this effect is controlled by the β parameter, which balances the influence of the one-sense-per-discourse principle compared to the one-sense-per-collocation principle. note that one-sense-per-discourse does not have to hold strictly, because the information can be useful if there are on average more instances of the names with the same sense than with the opposite sense in the far context. both near and far context words are important when deciding the sense of a name. for example, verbs like "activate" or "phosphorylate" are often found around protein names, whereas verbs like "express" or "transcribe" may be found around gene names. similarly, head nouns, such as expression, are also highly indicative of the sense. these words may be located near to the name to be disambiguated, being strong indicators of its sense. as shown above, ambiguous names in the abstract are more likely to be of the same sense and therefore the words around the other ambiguous names are partly indicative about the sense of the name to be disambiguated. since other ambiguous names can occur at any position of the abstract, it is beneficial to use long context. descriptions of experimental conditions can indicate one sense common to all of the names in the abstract. for example, "yeast two-hybrid" indicates protein-protein interaction finding, while "microarray" relates to gene experiments. further, the occurrence of a distant coreference, for example, between the full form of an ambiguous name and its abbreviation, in the context of the ambiguous name may provide distant words indicative of the correct sense.

the phenomena discussed above are highly data dependent. however, the proposed weighting scheme models them if the weighting scheme is equipped with the optimal values of the parameters λ and β found in the training phase.

data and its preprocessing
the data set considered in this paper is constructed as follows. we obtained the evaluation data set for the coling- <dig> jnlpba shared task of bio-entity recognition  <cit> , which is derived from the genia corpus  <cit> , a standard corpus for biomedical named entity recognition, by conflating the original  <dig> classes into five classes  of which we only use two classes, namely, dna and protein. the data set consists of  <dig> hand-annotated abstracts and contains  <dig> protein examples and  <dig> dna examples. average number of words in the abstracts is  <dig> 

naturally, not all names are truly ambiguous in all domains and corpora. in these cases a classifier could gain by simply memorizing the names. we made an experiment in which we used only the ambiguous gene and protein names as data points and the performance obtained was over 95%. using context words as extra features improved the performance only by  <dig> % percentage points, with optimal context span  <dig> found experimentally. the difference was not statistically significant. to assess the performance in a more general setting and to avoid the overfitting effect of memorizing, we do not include the instance of the term to be disambiguated into its context, as the purpose of this paper is to study context-based name disambiguation. note also that memorizing the names does not help the classifier to disambiguate names that do not exist in training data, for example, names introduced only recently.

the text was further preprocessed by removing stop words and stemming the words with the porter stemming algorithm  <cit>  .

measure of performance
the number of protein examples  in our corpus is about three times greater than the number of gene examples . thus, we could achieve a classification accuracy of about 75% by always predicting the protein class. to cope with the imbalance in the data, we measure the performance of each classifier as the area under roc curve . roc curve is a relation between the true-positive rate  and the false-positive rate  at various classification thresholds:



where tp, fn, fp, and tn are true positives, false negatives, false positives, and true negatives, respectively. unlike other popular measures such as accuracy and precision-recall analysis, the auc measure is invariant to the prior class probabilities. auc corresponds to the probability that given a randomly chosen positive example and a randomly chosen negative example, the classifier will correctly say which is which. for a thorough discussion of roc curves and the auc measure, see, for example, fawcett  <cit> , maloof  <cit> , and bradley  <cit> .

we cross-validate all auc measurements using the  <dig> ×  <dig> cross-validation scheme, which is an ordinary 2-fold cross-validation performed five times. to obtain a 2-fold cross-validated performance estimate, we randomly divide a set of abstracts into two equally-sized sets and average the two performance measurements obtained by training the classifier on one set and testing the classifier on the other set. to obtain a  <dig> ×  <dig> cross-validated performance estimate, a 2-fold cross-validation is performed five times and the estimates are then averaged. to avoid indirect overlap between test and training sets, we form the sets so that examples originating in the same abstract always remain in the same set.

to test for statistical significance, we use the robust  <dig> × 2-cv test  <cit> . the test avoids the problem of dependence between folds in n-fold cross-validation schemes and results in a more realistic estimate than, for example, the t-test.

experimental setup
we randomly divided the preprocessed set of  <dig> abstracts into two equal-sized sets;  <dig> abstracts for parameter estimation and  <dig> abstracts for final validation of the methods. the  <dig> ×  <dig> cross-validated auc was used as the measure of performance in both parameter estimation and final validation. in all the experiments with svms, we normalized the word frequency vectors to unit length, because the sizes of the contexts varied considerably. we carried out the svm experiments using the libsvm  <dig>  software  <cit>  and the naive bayes experiments using the bow toolkit  <cit> .

in the experiments, optimal parameter values for svms with different kernels and for the proposed weighting scheme must be searched . every svm itself has always a penalty parameter c, linear kernel has no other parameters than c, and both gaussian and polynomial kernels have an additional parameter λ. adopting a contextual representation yields a context span parameter s. the svm equipped with the weighting scheme has two additional parameters λ and β by which we may control the effect of the distances of the words from the name to be disambiguated when weighting the context words. the performance of a classifier may be strongly influenced by the choice of the values for its parameters. for example, from figure  <dig>  it can be observed that a wrong choice of the kernel parameters as well as the svm penalty parameter c can lead to a severe loss in performance. particularly when comparing the methods, the correct parameter setting for each of the compared methods is crucial, as only then a reliable estimate of the performance is obtained for each of the methods. the correct parameter values cannot be known in advance and the use of the default values may result in sub-optimal classification performance. therefore, the parameter values are most commonly estimated from the data. hsu et al.  <cit>  recommend a grid-search on c and γ parameters using cross-validation and exponentially growing sequences of c and γ. since an exhaustive search for the parameters can not be done in the continuous space of svm and kernel parameters, we performed a coarse preliminary search in order to find an auspicious region, and subsequently conducted a finer grid search. as can be observed from figure  <dig>  the choice of values of the weighting parameters λ and β is as important for the classification performance as the choice of the other parameters. moreover, as noticed by ginter et al.  <cit> , the optimal values depend on the task and are not known beforehand. therefore, we use a grid-search also for finding the optimal values of the parameters λ and β.

in short, the parameter estimation for svm classifiers was performed as follows. first we estimated the context span s for the conventional svm and the values of λ and β for the weighted svm, using a grid search with the linear kernel function. we used the whole abstract to form the examples for the weighted svm. with the s, λ and β parameters fixed, we evaluated different types of svm kernels, estimating the kernel parameters with a grid search. the svm penalty parameter c is optimized separately at each point of the context span, weighting and kernel parameter grids.

in a similar manner, the optimal combination of λ, β and s for the wac classifier was found by performing a 3-dimensional grid search. for the naive bayes classifier, only the optimal value for s must be searched. for final validation, we chose the best performing kernel function for conventional and weighted svms. using the parameter values found in the parameter estimation phase, we then measured the performance of each of the compared classifiers on the validation set, again using  <dig> ×  <dig> cross-validation.

a detailed explanation of the grid search for the parameter estimation described above is presented in the following sections.

parameter estimation for conventional svm
first, we searched for the optimal context span s that we will use in our experiments with the conventional svm. we experimented with different context spans using the conventional svm with the linear kernel, namely spans of  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> words to both directions from the term to be disambiguated. the parameter c for the svm with the linear kernel was searched with values 2- <dig>  2- <dig> ...,  <dig> for each of the different context spans. in these experiments, the context span of  <dig> words to both directions from the term to be disambiguated resulted in the highest performance for the conventional svm. we used this context span when experimenting with gaussian and polynomial  kernels, because simultaneous searching for optimal context span and kernel parameters c and γ for the gaussian kernel and polynomial kernels with degrees d =  <dig> and d =  <dig> would have been computationally impractical. the values of the c and γ parameters of the gaussian kernel were 2- <dig>  2- <dig> ...,  <dig> and 2- <dig>  2- <dig> ...,  <dig>  respectively, and the values for the c and γ parameters of the polynomial kernels were 2- <dig>  2- <dig> ..., 2- <dig> and 2- <dig>  2- <dig> ...,  <dig>  respectively. the results obtained with different kernel functions using the optimal context span s =  <dig> found with the linear kernel are shown in table  <dig> 

parameter estimation for weighted svm
with the weighted svm, we always used the whole abstract as a context. for the weighted svm, we estimated the best combination of the weighting parameters λ and β with the linear kernel. the parameter c was also separately searched with values 2- <dig>  2- <dig> ...,  <dig> for each of the different weightings. the comparison of the performance with different weightings using the linear kernel is illustrated in figure  <dig>  at this point, we found that the values λ =  <dig>  and β =  <dig>  performed best for the linear kernel . we used these parameters when experimenting with gaussian and polynomial  kernels. the values of the c and γ parameters of the gaussian kernel were 2- <dig>  2- <dig> ..., 23and 2- <dig>  2- <dig> ...,  <dig>  respectively, and the values for the c and γ parameters of the polynomial kernels were 2- <dig>  2- <dig> ..., 2- <dig> and 2- <dig>  2- <dig> ...,  <dig>  respectively. the performance of the weighted svm with different c and parameters of the gaussian kernel is illustrated in figure  <dig>  the figure illustrates the importance of correct parameter selection. the weighted svm performance with different combinations of γ and the penalty parameter c follows the behavior described by keerthi and lin  <cit> : areas of underfitting can be seen at the left, where the value of the c parameter is low, and at bottom left where the values of both c and γ are low. on the other hand, svm with gaussian kernel overfits heavily if the value of γ is too large, as can be seen at the top of the figure. overfitting happens also with noisy data at the right part of the figure, where the value of c is too large. the results obtained with different kernel functions using the best weighting parameters λ =  <dig>  and β =  <dig>  found with the linear kernel are shown in table  <dig> 

parameter estimation for baseline methods
the only parameter of the naive bayes classifier is the context span s. we performed a search for s ∊  <cit>  with step  <dig>  the performance reached maximum for s =  <dig>  the wac incorporates an identical weighting scheme as the weighted svm. we found the optimal parameters by performing a 3-dimensional grid search for λ ∊  <cit>  with step  <dig> , β ∥  with step  <dig>  and context span in the interval  <cit>  with step  <dig>  the maximum performance was obtained for λ =  <dig>  β =  <dig>  and context span s =  <dig> .

final validation
the gaussian kernel was found to be the best with both the conventional and weighted svms when tested in the parameter estimation. the best parameters for the gaussian kernel were c =  <dig>  and λ =  <dig> with the conventional svm, and c =  <dig> and λ =  <dig> with the weighted svm. the auc results of the final validation are presented in table  <dig> and the roc curves are given in figure  <dig>  to test the statistical significance of auc differences between the weighted svm, the conventional svm, wac and naive bayes, we performed the robust  <dig> × 2-cv test on the validation data. each of the pairwise differences were strongly significant with p-values below  <dig> , except for the difference between the conventional svm and the naive bayes classifier . the conventional svms performed poorly compared to the baselines, especially to the wac classifier that takes advantage of the contextual weighting. incorporation of the weighting scheme into svms, however, improved their performance by five percentage points.

CONCLUSIONS
in this paper, we show that svms can be successfully applied to gene versus protein name disambiguation. we demonstrate how their performance can be further improved by incorporating a weighting scheme based on the intuition that the words near the name to be disambiguated are more important than the other words. the weighting scheme results in a notable performance gain of five percentage points. we also study carefully the effects of different kernel functions and parameters and show that the proposed weighting scheme influences the performance even more than the selection of the kernel part of svms. the weighted methods statistically significantly outperformed their unweighted counterparts, the difference being particularly notable for svms.

in ginter et al.  <cit> , we have shown that the optimal values for λ and β are non-zero and differ substantially depending on the classification task at hand. this suggests that the extent to which the long distance words contribute to the classification is task-dependent and could reflect differing properties of the tasks. while finding correct values of these parameters is clearly important as shown by the experiments, an exact interpretation of the values remains speculative. however, we discuss several reasons why the use of the proposed weighting scheme is beneficial. further study could bring a better insight into the underlying phenomena.

the performance of the weighted svm might be further improved, for example, by using collocations in order to capture the local syntax around the term to be disambiguated. however, the proposed weighting scheme uses the local information, and therefore it already captures the information represented by collocations to some extent. in addition, several special text kernels have successfully been applied to text classification as reported by lodhi et al.  <cit>  and by cancedda et al.  <cit> . these kernels and different weighting methods based on the distances and also, for example, on the biological relevance of the words in the context, are still to be studied.

