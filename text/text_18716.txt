BACKGROUND
the introduction of next generation sequencing technologies has led to important breakthroughs throughout the life sciences, with applications in de novo genome, exome or amplicon sequencing, gene expression analysis, identification of transcription factor binding sites, and so on. also in clinical and environmental microbial community analysis, 16s rdna sequencing and metagenomics have been instrumental. for the assessment of microbial community structures based upon 16s rdna amplicon sequencing, the  <dig> pyrosequencing platform has already been in use for many years, mainly due to its longer read length  <cit>  nowadays allowing up to  <dig> bp reads.

having access to highly reliable sequencing data is a necessary requirement for biodiversity assessments using 16s rrna amplicons  <cit>  as this approach is highly sensitive to sequencing errors. indeed, the natural variation in the 16s marker genes for different bacterial species significantly complicates the problem of distinguishing between erroneous sequences on the one side, and sequencing reads representing rare taxa on the other side. this may lead to an overestimation of the number of operational taxonomic units  and sample biodiversity as a whole  <cit> . therefore, error-correction prior to starting biological interpretation of the data is a matter of utmost importance.

several efforts have been made to study the sources of those errors and how to eliminate or correct them  <cit> . there are three major causes of errors in sequencing data at different stages in the sequencing process: i) errors originating from usage of the pcr polymerase enzymes , ii) pcr artifacts, known as chimeras  <cit> , and iii) errors originating from the sequencing platform  <cit> . concerning the latter type of errors, different indicators have already been identified for the gs <dig>  <cit>  and gs-flx titanium  <cit>  platforms:  position within the read i.e. the quality of the read is dropping with increasing position in the read,  the presence of homopolymers gives difficulties to identify the correct length of the homopolymer or cause an insertion or substitution near it ,  abnormality in the read length  that may be caused by extreme quality filtering, accumulation of errors or stochastic polymerization ending,  position of the bead on the plate ,  distribution of the errors, as errors tend to accumulate in small subset of sequencing reads, meaning that a majority of reads will be error-free or only contain a single position error while a minority of reads is problematic,  nucleotide type, as mismatch transitions are not of equal rates.

different methods have been developed to enhance the quality of pyrosequencing reads, starting with the most basic approaches e.g. by removing those reads where no perfect match with the pcr primer could be identified  <cit> . another approach for correcting sequencing errors was introduced by huse and co-workers  <cit> , including: i) removal of reads with one or more ambiguous bases, ii) reads with a length outside the main distribution or iii) reads containing mismatches in their primer sequence. additionally, a major improvement was achieved via the use of the read quality scores  as proposed in kunin et al.  <cit>  by trimming reads from the most upstream position where the quality dropped below the assigned cut-off quality score. similarly, schloss et al.  <cit>  implemented a method where trimming of sequencing reads was based on a drop in the average quality of the read in total or within a specific sliding window.

in addition to methodologies trimming or removing sequencing reads, more sophisticated approaches were made by clustering of the  <dig> standard flowgrams, to better handle homopolymer related errors, either by applying an expectation maximization  algorithm as in pyronoise  <cit>  or greedy scheme as implemented in denoiser  <cit> . three sequence based clustering algorithm with a much faster computation time were developed quite recently, namely the single linkage pre-clustering method   <cit> , ampliconnoise   <cit>  and acacia  <cit> . slp clusters low redundant reads containing errors with the more redundant error-free ones. this is clustering is based on the pairwise distance scores between both reads, thereby tolerating some errors . ampliconnoise consists of two steps: i) the pyronoise algorithm, which applies a clustering approach directly on the flowgrams rather than relying on the quality scores assigned by the  <dig> pyrosequencing platform, and ii) seqnoise, which applies a sequence clustering step.

acacia  <cit>  is denoising algorithm that aligns each read to dynamically updated cluster consensus sequences, thereby avoiding the time-consuming all-against-all alignments, and is mainly focusing on correcting sequencing errors in homopolymer regions. slp was re-implemented in mothur  <cit>  as the pre.cluster command. in contrast to slp, pre.cluster is applied on the aligned sequences thereby avoiding the computationally intensive all-against-all alignments. pre-cluster was found to outperform slp in terms of speed and performance as it avoids overclustering of sequences  since pre-cluster only groups reads when they are within a maximum distance to the cluster center. these implementations make use of the sequence frequencies to remove errors based on the relative abundance of an error versus a correct nucleotide. since slp and pre-cluster both work directly on the sequencing data , those algorithms do not distinguish between pcr point errors and errors produced by the sequencing technology.

when dealing with error-correcting algorithms, researchers need to find a balance between the quantity , and the quality of their pyrosequencing reads . retaining more sequencing data by increasing the average read length, while keeping the error rate within an acceptable range, will have important consequences for taxonomic precision. another factor that needs to be taken into consideration is the computational cost, which is getting more important with the dramatic increase of the size of sequencing data, resulting in a need for fast and accurate methods to analyse them. here, we introduce a novel way of artificial intelligence-based prediction of erroneous positions in sequencing reads  and subsequent clustering which will correct error-containing reads in the sequencing data by grouping them with error-free reads , thereby fulfilling the need for quality and speed. this methodology was benchmarked against other state-of-the-art algorithms and multi-step methodologies.

methods
mock communities
in this work, previously published mock datasets were used, as well as a new in-house made mock community. first we used the mock datasets presented in schloss et al.  <cit>  as available online  consisting of  <dig> samples , targeting three hypervariable regions in the 16s rrna gene i.e. v <dig> to v <dig>  v <dig> to v <dig> and v <dig> to v <dig> , and are made available on the node website. one of these samples was randomly selected for training the classifier  and the other  <dig> samples were used for testing . although the composition of mock <dig> samples was known , we did not have access to the exact concentrations for each species.

the in-house built mock community , consists of  <dig> bacterial species, namely acidovorax defluvii, pseudomonas xanthomarina, pseudomonas aeruginosa, paracoccus denitrificans, rhodospirillum rubrum, microbacterium phyllosphaerae, arthrobacter oryzae, delftia tsuruhatensis, nitrosomonas europaea, cupriavidus metallidurans, clostridium botulinum, staphylococcus aureus, bacillus cereus, arthrospira platensis, enterococcus faecium, yersinia enterocolitica, and desulfovibrio oxamicus ordered by their concentrations in descending order.

the dna was extracted from the individual cultures, mixed, and pcr amplified . next, the dna mixture was sequenced in triplicate using the  <dig> gs-flx titanium sequencing platform, covering the region v1-v <dig>  allowed us to test the capability of each tool and pipeline to recover the exact initial microbial composition. to have absolute confidence in the reference genomes used for calculating the error rates, the exact 16s rrna gene sequence for the  <dig> species was obtained using sanger sequencing. for all paralogous 16s rrna genes within the mock community, no differences between paralogs could be observed for  <dig> species. for the  <dig> species with sequence variations between the paralogs , only one difference  or three differences  could be observed. however, this variation could not contribute more than  <dig> % to the total error rate, and will certainly not lead to an inflation of otus in the downstream analysis as the percentage difference is much lower than the 3% difference cut-off used. as we have to take into account the technical  and pcr bias that might result in an aberration from the presumed concentrations, the raw sequencing reads were mapped back using nast  <cit>  to the  <dig> reference genomes, leading to a compositional range between  <dig> % and  <dig> % . the sequencing data are submitted to the ncbi sequence read archive . applying basic trimming  resulted in a mock community data set consisting of  <dig>  reads with an average length of  <dig> and a raw error rate of  <dig> .

error calculation and chimera detection
to obtain erroneous positions  in the trainingdb dataset, each read was blasted  <cit>  against a database containing all the reference sequences of the corresponding mock community. after finding the potential reference sequence, an accurate alignment was produced using clustalw  <cit>  adjusting the parameters as recommended in gilles et al.  <cit> , to get the highest accuracy for the identification of insertions, deletions and mismatches. this computationally costly approach was needed to get the highest sensitivity in identifying those errors that are used as training data to build the classifier, and to obtain the positional information of the sequencing errors needed to train the classifier.

in the comparative study between different tools, the global error rate was used to evaluate the accuracy of each approach, reflecting the number of erroneous nucleotides over the total number of nucleotides. for this benchmarking step the mothur command seq.error as implemented in schloss et al.  <cit>  was used to calculate the over-all error rate. importantly, this algorithm also implements a highly efficient approach to detect chimeric sequences in mock communities. in a first step, both the reference sequences and sequencing reads were aligned using the nast algorithm to the silva reference alignment. in the second step, chimera were detected via calculating the number of mismatches between each  <dig> pyrosequencing read and all possible two-parent chimera that could be generated by the reference sequences. in those cases where a sequence is at least three bases more similar to a multi-reference chimera than to a single reference sequence it is considered a chimera, and thus excluded from error calculations. the percentage of chimera was found to be  <dig> % and  <dig> % for mock <dig> and mock <dig> reads. additionally to the chimera detection step, the error rate is calculated via counting the distance between the  <dig> reads and closest reference sequence. it was applied on both mock <dig> and mock <dig> for the non-denoised data and after each different denoising tool.

denoising algorithms
our newly introduced denoising algorithm was benchmarked with four other commonly used denoising algorithms: single linkage preclustering   <cit> , ampliconnoise  <cit> , acacia  <cit>  and denoiser  <cit> . for slp we used the implementation pre.cluster as available in mothur  <cit>  . we will refer to this algorithm as pre-cluster in the text below. similarly, ampliconnoise  was run using the mothur commands shhh.flow and shhh.seqs. for denoiser we used mac-qiime implementation via the denoise_wrapper.py  script. for acacia we used the original implementation  as available online. all algorithms were run using their default parameters.

preprocessing sequencing data
as shown in the introduction, trimming of pyrosequencing reads is a common preprocessing step in 16s rrna amplicon sequencing  <cit> . in order to allow a fair comparison between different denoising algorithms, the same input data should be used for all tools, thereby preventing the confounding effect of using different pre-processing pipelines. therefore, the same basic preprocessing approach was applied as proposed in schloss et al.  <cit> , i.e. culling reads with one or more ambiguities, removing too short reads , and filtering out reads with homo-polymers longer than  <dig> bp. this approach – what we will refer to as “basic trimming” – is applied on all datasets discussed in this work.

following the basic trimming step, optionally a more stringent trimming approach – further referred to in the text as “strict trimming” – can be applied. for preprocessing the data used as input for pre-cluster, acacia, denoiser and our newly developed approach , a sliding window approach was used to trim reads until the position where the average quality of this window drops below a cut-off phred score . as mentioned in the introduction, the aim of this work is to develop a denoising algorithm resulting in an acceptable error rate, while preserving longer read lengths. these longer read lengths can be guaranteed by using a sliding window of  <dig> nt and a cut-off on the average phred score of  <dig>  for ampliconnoise, a similar effect could be achieved by trimming the sequencing data according to the guidelines stated in the original paper describing the tool,  <cit> , by retaining only those reads with minimum flow length of  <dig> and maximum flow length of  <dig> 

for both approaches  reads were aligned to a  <dig> -column wide silva-based reference alignment  <cit> , using a nast-based aligner  <cit> , as available in mothur  <cit>  and filtered , and subsequently subjected to error calculation  for comparative analysis. for assessing the impact of the error correcting algorithms on the otu clustering, reads were clustered using the clustering algorithm as integrated in uparse  using the uparse command with the following options: sortbysize, cluster_otus, and usearch_global  <cit> . next, reads were classified using the rdp classifier  <cit>  with 80% cutoff by applying mothur classify.seqs command. first, we applied this clustering approach  on the selected v1-v <dig> region of the correct reference sequences to assure that a correct taxonomic classification could be obtained theoretically  . the same algorithmic approach was used for taxonomic classification of the mock <dig> reads after applying the pre-processing as mentioned above.

RESULTS
node  algorithm
our algorithm consists of two steps. first, a pre-trained classifier is used to identify those positions in the reads that are conceivable to be erroneous nucleotides based on a list of features potentially acting as a predictor for sequencing errors. in a second step the slp algorithm  <cit>  as implemented in mothur  <cit>  is adapted in such a way that those nucleotides being marked as potentially erroneous are not penalized in the mismatch counting used to cluster similar reads. both steps are explained more in detail below.

for training the node classifier, an exhaustive list of features potentially able to predict sequencing errors was derived based on conclusions presented in schloss et al.  <cit> , gilles et al.  <cit>  and huse et al.  <cit> : i) the position in the read, ii) phred score, iii) the presence and exact location within a homopolymer, iv), the possibility whether this position is sensitive to carry forward events, and v) the flowgram signal intensity. additionally we also examined the predictive effect of characteristics derived from neighbouring nucleotides : i) the phred score, ii) the presence and exact location within a homopolymer, iii) flowgram signal intensity and iv) the highest flowgram signal intensity score that did not result in a base call, measured over all signal intensity levels in the flowgram between the position studied and the neighboring nucleotide. combining those features lead to a list of  <dig> attributes, five related to the position itself and eight linked to the neighbouring positions. those  <dig> attributes were extracted from the standard flowgram format  files. to properly describe the homopolymer status , following annotation was used: “n” for non-homopolymer, “a” and “z” for the first and last position in the homopolymer respectively, and for the second, third, fourth, etc. position  we use “b”, “c”, “d”, etc. since the exact microbial composition is known for the trainingdb, we were able to know for each position whether it was an error  or not based upon the original genome sequence of the organism at hand. principal component analysis showed that all proposed features were needed to explain 95% of the variation in the training data .

for training the model, several classifiers were considered , decision tree, naive bayes, nearest neighborhood, logistic regression) as implemented in weka  <cit> . in order to select the best performing classifier the training dataset was split into two subsets, subset a for training, and subset b for initial testing. for constructing these subsets, the training dataset was dereplicated and distributed over subsets a and b with a ratio 1: <dig> in a stratified way. afterwards, normalization of the ratio between different error types  was applied to equally train and assess each type of error . within subset a the native ratio between erroneous versus non-erroneous instances was 1: <dig>  which would bias the classifier towards the more abundant one . therefore subset a was reduced to have a count of  <dig> for each error type as well as  <dig> non-erroneous instances via random selection.

as evaluation criteria for assessing the best performing classifier , we used the sensitivity ), and specificity ). the best performing classifier was found to be an svm with a pearson vii universal kernel   <cit>  and sequential minimal optimization algorithm  <cit> , which achieved a sensitivity of  <dig>  and specificity of  <dig> . the relative influence of each feature was assessed using the feature weights of each of them. however, as the puk kernel is a non-linear kernel, it does not directly allow to calculate feature weights. therefore, the feature weights were illustrated by training a linear svm for classification, showing again that all features were essential for the optimal performance of the classifier . the selected classifier  optimally integrating these  <dig> features was used as error predicting tool in the node algorithm. for any position predicted by this classifier in node to be erroneous, the nucleotide will be marked as such.

in the second step, a modified version of the slp algorithm  <cit>  as implemented via pre.cluster in mothur  <cit>  was developed. the pre.cluster implementation merges the less redundant reads with no more than 2% mismatches with the more redundant reads . in node, the pre-cluster like algorithm is proceeded by a machine learning approach identifying potentially erroneous nucleotides, and those positions are masked. accordingly, we adapted the pre-cluster algorithm implemented in node in such a way that is able to ignore those masked positions in the difference calculations of pre-cluster, which means that those positions will not lead to an increase in differences. after the clustering, the remaining masked positions  are converted back to their original nucleotide upon rechecking the original version of the read . a schematic representation of this approach is given in additional file  <dig> 

the source code and binaries for node are freely available at http://science.sckcen.be/en/institutes/ehs/mcb/mic/bioinformatics/node.

benchmarking of node
the mock <dig> dataset was used to benchmark node to other state-of-the-art denoising tools: pre-cluster  <cit> , acacia  <cit> , denoiser  <cit>  and ampliconnoise . two evaluation factors were used at this stage: the quality of those sequences in terms of error rates retained after error correction and the computational cost.

to assess the capability of each algorithm we tested them on datasets after basic and strict trimming respectively. the error rate of the mock <dig> data set after basic trimming and without performing any error correction step was found to be  <dig> , with an average read length of  <dig> . node resulted in the lowest error rate  while ampliconnoise, slp, acacia and denoiser had an error rate of  <dig> ,  <dig> ,  <dig>  and  <dig>  respectively . although different denoising algorithms processed an equal amount of data , the required computational time varied dramatically. node was found to yield an almost 40-fold speed improvement over ampliconnoise . on the other hand, node requires more computational time compared with acacia and pre-cluster. however, this relatively small increase in running time over the full preprocessing pipeline is largely compensated with a significant decrease of the error rate. one sample was used for additional analysis, delivering a detailed overview of the computational costs of the different preprocessing steps . an overview is given in additional file  <dig>  these computational costs can be further reduced via utilizing multiple cores simultaneously, which is an option available for node, ampliconnoise, denoiser and pre-cluster. for acacia this parallelization can be done manually by the end user via staring up different runs in parallel. inherent to the mode-of-action of node is the linear increase of the computational time with the number of unique reads. it should be noted that this does not mean that node will increase linearly with the input data, as the number of unique reads will reach to an asymptotic value upon increasing the coverage of the sample.table  <dig> 
benchmarking of different denoising algorithms using the mock <dig> dataset



basic trimming
average length
error
cpu cost
number of seq

strict trimming
average length
error
cpu cost
number of seq
the comparison covers the final error rate as well as the computational cost  for the analysis pipelines including all tested denoising algorithms . also the number of reads and average read length returned by the different algorithms is displayed.



the node classifier consist of three processes: 1) extracting and preparing the data, 2) running the classifier, and 3) the masking phase. the first and last process  required a maximum of  <dig> mbs ram memory using mock <dig> samples, and this during 80% of the execution time. during the residual 20%, the classifier component using the weka software as implemented in java, required a maximum of  <dig>  mbs of ram memory. for the other denoising tools, the maximum ram requirement was found to be  <dig>  mbs,  <dig> mbs,  <dig> mbs and  <dig>  mbs for ampliconnoise, denoiser, pre-cluster and acacia, respectively.

similarly, after applying the strict trimming step, the error rate without applying any denoising algorithm was found to be  <dig>  with an average read length of  <dig>  applying node on these sequencing data resulted in an error rate of  <dig> , which is significantly lower than the error rates obtained with denoiser , acacia , pre-cluster  and ampliconnoise  . a graphical representation of the effect of the different denoising tools on the sequencing data with respect to the position of the error in the read is given in figure  <dig>  as expected, these plots show that the total error rate  is mainly increasing towards the end of the read. in the second plot, the performance of each denoising tool on different types of errors  is illustrated. also for the strictly trimmed sequencing data, node outperformed the second best benchmarked tool  in computing time, as node processed the same dataset in  <dig>  hours versus  <dig> hours for ampliconnoise. an overview of the computational cost of each step in the preprocessing pipeline – including the different denoising algorithms – is given in table  <dig> figure  <dig> 
effect of denoising algorithms with respect to position in read. a) plot showing the error rate versus the position in the read after being treated with different denoising algorithms, including: acacia , denoiser , slp , ampliconnoise  and node , with the raw error rate in black. b) plots showing the insertion , deletion  and substitution  error rates produced in the raw reads , as well as after being treated by different approaches, versus the position in the read.
tabular overview of the computational cost of the different denoising algorithms



denoising approach
sff extraction
trim reads
denoising algorithm 
aligning
filter alignment
total time
to have an idea about the computational cost for each step, the complete pipeline was subdivided in different steps to illustrate its running time, as described above. from the table, it can be observed that the computational burden added to the complete preprocessing pipeline  was relatively small, and it was largely compensated with a significant improvement in the error rate, that exceeded the second best performing  algorithm ampliconnoise. for the denoising algorithms, the average amount of memory required was added.



impact of error correction methods on otu clustering
the final step in amplicon sequencing-based community profiling is the clustering of reads into operational taxonomic units , which are believed to reflect a well-delineated taxonomic group. however, different proposed amplicon sequencing processing pipelines tested on artificial communities all lead to an inflation of the number of otus reported, often multiple times higher than the number of bacterial species present in the tested mock community  <cit> .

a second mock community  was used to assess the influence of different denoising and preprocessing methods on the otu distribution. as the exact concentration of each species is known in this uneven mock community, the accuracy of the otu clustering process can be followed up . this mock <dig> community consists of  <dig> species spread over a wide taxonomic range and sequenced in triplicate as described in the methods section. the difference in error rates between different denoising algorithms for mock <dig> showed the same trend as for mock  <dig>  i.e. an error rate of  <dig>  for ampliconnoise,  <dig>  for denoiser,  <dig>  for acacia,  <dig>  for pre-cluster while the lowest error rate of  <dig>  was achieved by node.

we applied the otu clustering algorithm using uparse  <cit>  as described in the methods section. as a validation step we first applied this algorithm on the  <dig> reference sequences only, resulting in  <dig> distinct otus each representing a species in the mock <dig> dataset. in the ideal case, such  <dig> distinct otus would also be returned upon analyzing the 16s amplicon pyrosequencing data. to check to which extent this could achieve using any of the denoising tools, uparse was applied on each of the denoised sequencing data sets obtained after applying ampliconnoise, pre-cluster, acacia, denoiser and node, and the number of otus are reported as average number over the three replicates. node had the smallest number of otus , while ampliconnoise, denoiser, pre-cluster and acacia had  <dig>   <dig>   <dig> and  <dig> otus on average respectively. when omitting the error correcting algorithms  the number of otus even further inflated to  <dig> otus, pointing out the importance of integrating a denoising algorithm in a 16s rrna metagenomics pipeline.

all the otus obtained were evaluated qualitatively  and quantitatively . for the qualitative analysis, we counted the number of “correct otus” , “noisy otus” , “missed otus” , “over-splitted otus” , “contaminant otus”  and “other otus” .

all of the different preprocessing pipelines determined the correct relative percentage for each mock species. however, from a qualitative point of view some of the species suffered from otu oversplitting , and is observed with all error-correcting algorithms  however at different levels. the number of over-splitted otus was  <dig>   <dig>   <dig>   <dig> and  <dig> for node, ampliconnoise, denoiser, pre-cluster and acacia, respectively. additionally, the number of unclassified otus  could also be used as a quality criterion, since all organisms present in mock <dig> are well-known species present in all standard reference databases . as such this number of unclassified otus should be as low as possible. for this mock community  <dig>   <dig>   <dig>   <dig> and  <dig> unclassified otus were detected for node, ampliconnoise, denoiser, pre-cluster, and acacia respectively. for both aberrant types of otus node showed the lowest number compared to other tested denoising algorithms, showing the beneficial influence of an accurate error correction tool. additionally, over-clustering was assessed by checking the number of missed otus. upon checking the closely related species  within mock <dig>  we could see that on average both denoiser and ampliconnoise suffer from a missing otu due to over-clustering, while node, acacia an pre-cluster do not have this problem.table  <dig> 
otus produced after treating the data with different noise removal approaches


qualitative
quantitative

approaches
total otus
correclt otus
over-splitted otus
missed otus
noisy
contaminants
others
approaches
rare-otus
redundant otus

node
node

ampliconnoise
ampliconnoise

denoiser
denosier

pre-cluster
pre-cluster

acacia
acacia

non denoised
non denoised
the left side of the table displays the qualitative otu assessment and the right side displays the quantitative analysis. for the qualitative analysis, we counted the number of “correct otus” , “noisy otus” , “missed otus” , “over-splitted otus” , “contaminant otus”  and “other otus” . in the quantitative analysis, the number of otus with a redundancy below  <dig> %  and the ones with a redundancy above  <dig> %  were counted.



in the quantitative analysis, we count the number of otus with a redundancy below  <dig> %  and the ones with a redundancy above  <dig> % . if we used the number of rare otus as an indicator for a better error correction step , the noise removal step was more accurate with node resulting in only  <dig> rare otus, while ampliconnoise, denoiser, pre-cluster and acacia had  <dig>   <dig>   <dig> and  <dig> rare otus respectively. indeed, the number of rare otus is – with exception of denoiser – proportional to the error rate produced. although, as denoiser is returning the highest error rate yet resulting in a low number of rare otus, an extra analysis was performed plotting the expected percentage of a species versus the observed percentage after otu clustering . from these plots it can be derived that the percentages of the different species in the mock communities obtained using different denoising strategies correlate better with the actual percentages with decreasing error rate. this is largely reflected by the r-square goodness of fit value derived for each denoising algorithm, resulting in the highest value for node , while observing the lowest r-square value for denoiser  showing the highest error rate. the same conclusion could be drawn when using the method as described in bragg et al.  <cit>  for assessing the correspondence between the theoretical and observed proportions of the species in the mock community. also here, the average relative deviation of the observed concentration of a species after analyzing the data versus the theoretical concentration was the lowest for node, while denoiser was one of the tools with the highest deviation .

discussion
new sequencing technologies have revolutionized the way microbial communities are characterized, but still suffer from amplification and sequencing artifacts. in this work we proposed a new denoising methodology node which is able to significantly reduce the sequencing error rates at a low computational cost . in general our method consists of a two-step approach. first an artificial intelligence based classifier is trained to identify those positions in  <dig> sequencing reads having a high sequencing error probability. these positions are identified via a set of features that are able to predict less reliable sequencing regions. by marking those positions, a quality-driven clustering of reads is made possible via a modified version of the pre-cluster command in mothur in the second step.

when comparing our algorithm with other denoising algorithms, we could show a significant improvement at the level of error rate reduction . as such, node manages to bring the error rate of  <dig> pyrosequencing reads to an acceptable level in mock <dig>  while retaining a large proportion of the sequencing data. the latter is reflected in a long read length  which will result in a more precise taxonomic classification  <cit> . moreover, when comparing the required computational resources of node with the second best denoising algorithm , more than an order of magnitude reduction in computational cost could be obtained. the computational burden added by node in the complete preprocessing pipeline as implemented in mothur is very limited, and largely compensated by the improvement in the error rate .

additionally, we could show that denoising using node has a beneficial effect on the number of otus returned after clustering, reaching almost a one-to-one relationship between the number of otus and the number of species that are present in our artificial community. moreover this process could be completed at a reasonably low computational cost. such a close correlation between the number of otus and the number of present bacteria could not be achieved on the studied mock community using any of the other error correcting algorithms. moreover, also applying the uparse pipeline without integrating the error correction step leads to a larger deviation from the one to one relationship between otus and species present in the tested communities. however, it should be noted that obtaining such a one-to-one relationship with node was obtained using mock communities, and caution should be taken when extrapolating those results to real biological samples.

in this work we focused on applying error correction on 16s rrna amplicon sequencing data. however, in principle this method is applicable to all amplicon sequencing data obtained via the roche  <dig> pyrosequencing technology. as a proof of concept, we successfully tested our algorithm on the sequencing data presented in gilles et al.  <cit>  containing control dna fragment type i sequences as provided with  <dig> sequencing kits . similarly, the node implementation is trained and benchmarked using different  <dig> gs-flx titanium sequencing data set. however a highly similar approach  could also work with sequencing data obtained via the recent gs-flx+ technology, producing reads longer than  <dig> bp. moreover, the theoretical framework presented in this paper can also be applied to other sequencing platforms like illumina hiseq or miseq, an implementation which is currently under development.

CONCLUSIONS
we have developed a new denoising algorithm node that produces lower error rates compared with other existing denoising algorithms. moreover, using the mock <dig> community we could show that error correcting algorithms are a necessary and powerful step to come to biologically relevant numbers of otus, which were hard to obtain without any denoising step. node is able to perform this error correcting step in a computational realistic time frame, without being a bottleneck in the preprocessing pipeline.

additional files
additional file 1: 
mock <dig> composition. table giving the targeted concentrations for the mock <dig> community, and the exact concentrations together with the number of paralogous genes in the 16s genes of the reference genomes).

additional file 2: 
taxonomic classification of mock <dig>  excel sheet of the detail taxonomic classification of each of the three mock <dig> replicates after being treated with the different denoising approaches, together with the taxonomic classification of the reference sequences for the same region.

additional file 3: 
principal component analysis performed on the training data. information on the feature selection procedure, including data on the principal component analysis of the training data, and the relative weight of each feature in the classifier.

additional file 4: 
node algorithm workflow. schematic overview showing the different steps of the node algorithm.

additional file 5: 
schematic overview of the computational cost. schematic overview of the computational cost of the different denoising algorithms.

additional file 6: 
illustration of the rates of different error types. illustration of the percentage of different error types  after being treated by different denoising algorithms using the mock <dig> dataset.

additional file 7: 
denoising algorithms effect on otu level. three figures. a) the logarithmic percentage of each otu against the expected percentage, b) the relative deviation from the expected value for each otu. c) information on extra analyses performed to assess the effect of denoising algorithms on the otu-clustering.



competing interests

the authors declare that they have no competing interests.

authors’ contributions

mm, jr and pm conceived and designed the experiment. mm, nl and pm performed the computational work and analysed the data. mm and pm drafted the manuscript. all authors read and approved the final manuscript.

