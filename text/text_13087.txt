BACKGROUND
contact prediction methods identify residue pairs that are in spatial proximity in the native structure of a protein. contacts can be used as constraints to guide ab initio methods  and to reconstruct the 3d structure of a protein . in the 11th critical assessment of structure prediction , a bi-annual set of blind studies to assess the current state of the art in protein structure prediction, predicted contacts were the decisive factor to model the structure of target t0806-d <dig> with unprecedented accuracy for ab initio methods  <cit> . furthermore, predicted contacts can be used to compute long-range contact order   <cit>  to predict the folding rates of globular, single-domain proteins  <cit> . this is possible because long-range contact order and its variations correlate with protein folding rates .

despite recent successes, contact prediction remains a difficult problem. the difficulty is primarily due to the size of the solution space which renders an exhaustive search unfeasible. to render the search tractable, information are given as priors to constrain the search space. currently, many different sources of information are used in contact prediction. each source comes with its specific strengths and weaknesses. it is therefore promising to combine as many sources as possible so as to combine their strengths and to alleviate their weaknesses. methods from machine learning are well-suited for this task; they can be used to automatically determine how information sources should be combined and which combination is most appropriate under which conditions.

ensembling is one common approach in machine learning to combine multiple sources of information. it uses diverse models, each capturing different aspects of the data. ensembling is an established technique to boost the performance of predictors . existing meta methods for contact prediction follow this general idea. they typically outperform methods based on only a single source of information .

clearly, merging multiple information sources is a promising way towards improving contact prediction accuracy. however, leveraging multiple sources of information via machine learning introduces new challenges. inevitably, the combination of information increases the dimensionality of the feature space that is used as input to the machine learning algorithm. this is problematic, because the high dimensionality of the feature space increases learning complexity, data size, and training time. high-dimensional spaces also promote over-fitting because a learner might pick up irrelevant patterns in the data that explains the training data but does not generalize to unseen data. therefore, we cannot simply rely on the concatenation of features that work well by themselves and need to find a more powerful representation of our information to construct strong prediction methods.

in this paper, we develop a novel meta prediction method called epsilon-cp  based on deep neural networks that combines sequence-based, evolutionary, and physicochemical information. in the case of contact prediction, traditional features used in sequence-based methods suffer from high dimensionality. our study suggests that many of these features are not effective in the context of meta contact prediction. meta contact predictors include features based on other predictors . we develop a new representation with drastically reduced dimensionality that translates into a deep neural network predictor with improved performance.

we show that this approach reaches  <dig> % accuracy for the top l/ <dig> long-range contacts on  <dig> casp <dig> free modeling target domains, 11% better than the casp <dig> winning version of metapsicov, where l is the length of the protein. the increase in mean precision on  <dig> l is 17%. we further show through a feature importance analysis that dropping the amino acid composition from the feature set results in a dimensionality reduction of up to 75%. the approach presented here might be seen as a roadmap to further boost the performance of contact prediction methods.

related work
the focus of this paper is the combination of information sources to improve contact prediction. therefore, we will review related work with respect to the leveraged information sources. in addition, we will discuss how current meta approaches combine multiple information sources for contact prediction.

evolutionary information
the first source of information stems from evolutionary methods. statistical correlations in the mutations of residue pairs are indicative of contacts. since the mutation of a residue can lead to a destabilization of the structure, the other residue mutates as well to maintain stability. evolutionary methods look for co-evolving patterns in multiple sequence alignments . different methods have been developed for the statistical analysis of msas and to reduce the effects of phylogenetic bias and transitive couplings that can lead contact prediction astray  <cit> .

dunn et al.  <cit>  introduced the average product correction  to mitigate the effect of phylogenetic bias in the computation of mutual information. psicov  <cit>  builds on apc to also remove the effect of indirect coupling. other approaches work with different assumptions and use pseudo likelihood maximization  <cit> . the downside of evolutionary methods is that they are critically dependent on the quality of the msa. the number of homologous sequences in the msa needs to be in the order of 5l sequences  <cit> , where l is the length of the protein.

evolutionary methods are highly specialized. each method adds only a single dimension  to the meta learning approach. they have been shown to work well on their own, but combining multiple different methods can further improve the results  <cit> .

sequence-based information
the second source of information is extracted from the sequence of amino acids. sequence-based contact predictors identify sequence patterns indicative of a contact by applying machine learning on sequence-derived features. svmcon  <cit> , a sequence-based approach, ranked among the top predictors in casp <dig>  the approaches vary in their use of machine learning algorithms and the overall composition of the feature set, as well as the training procedures . commonly used features are for example the amino acid composition, secondary structure predictions, and solvent accessibility.

sequence-based methods are robust when only few sequences are available. most successful entries in recent casp experiments had a significant sequence-based component  <cit> . however, this class of methods does not benefit to the same degree from sequence homologs as evolutionary methods and therefore does not excel even if a large number of sequences is available.

commonly, sequence-based learners use very high dimensional feature sets with many weak features. an associated problem is the curse of dimensionality. the training data that is necessary for proper generalization increases exponentially  <cit> . further, some features may overlap with other more high level features that are used in meta approaches. for example, amino acid compositions or evolutionary profiles identify evolutionary patterns, which is something evolutionary methods do as well.

physicochemical information
the third source of information is extracted from candidate structures . decoys are the result of sampling the energy function in ab initio structure prediction and contain the physicochemical knowledge that is encoded in this function. since native contacts should be favored by the energy function, they appear more frequently in decoys than non-contacts. a successful approach in casp <dig> used simple occurrence statistics  <cit>  to identify contacts.  <cit>  use a similar approach and add and energy-dependent weighting of the decoys. epc-map  <cit>  uses an intermediate graph structure based on the identified contacts and its neighbors in the decoys to extract additional features.

structure-based approaches work well in ab initio contact prediction because it has lower requirements on the availability of homologous sequences compared to sequence-based approaches  <cit> . however, ab initio structure prediction methods are challenged by large proteins and proteins with complex topology. according to  <cit> , folding simulations become the limiting factor for proteins exceeding 120− <dig> residues in length. this is again due to the large conformational space that has to be sampled. further, rosetta  <cit>  is biased towards low contact order proteins  <cit> .

in the context of meta approaches, physicochemical information might play an important role because it is extracted from structure prediction decoys and not from sequence information. thus, physicochemical information is orthogonal to sequence-based and evolutionary information.

meta approaches
meta approaches combine multiple sources of information. we will briefly review different meta methods, focusing on what types of information they combine and how they combine the information. the presented methods are categorized based on the combination process into averaging and stacking. in averaging, the final prediction is a  average of the contact predictions from multiple methods. in stacking, the combination process is treated as a learning problem. the predictions of individual models are used as input features to a machine learning algorithm, usually in combination with other features. stacking allows to capture more complex relationships in the data than weighted averaging but is also prone to overfitting.

averaging

epc-map  <cit>  combines physicochemical and evolutionary information. the final output is a linear combination of the result of the svm ensemble, gremlin and the frequency f
ij of contact c
ij occurring in the decoys. membrain  <cit>  combines evolutionary and sequence-based information. the sequence-based approach uses ensembling of models trained on different subsets of the training data. the features are constructed from a window centered at the residue. the final feature vector is created by a) concatenation or b) parallel combination. additionally, in the latter case, the feature dimensionality is reduced by applying gpca  <cit> . the final output is a linear combination of the sequence-based and the evolutionary prediction.

stacking

bcl::contact  <cit>  leverages sequence-based and physicochemical information. the physicochemical information comes from  <dig> different servers. each server provides ten different models. based on these models two features are generated  and combined with common sequence-based features, all in all  <dig> features. the classifier is a single layer neural network with  <dig> neurons.


phycmap  <cit>  combines sequence-based information with two co-evolutionary methods . the sequence-based features include a context-specific interaction potential and the homologous pairwise contact score. the features  are fused and used to train a random forest classifier. additionally, physical constraints are used to filter false positive predictions.


pconsc  <cit>  combines sequence-based information with two different evolutionary methods. the evolutionary information is obtained using different multiple sequence alignments which adds further variety. the final classifier is a random forest trained on the fused feature set.


metapsicov  <cit>  combines sequence-based information with the output of three evolutionary methods. the high level evolutionary input features are merged with the crude sequence-based features for a total of  <dig> features. the classifier is a single layer neural network with  <dig> neurons.


epsilon-cp  uses stacking to combine physicochemical, evolutionary and sequence-based information, therefore extending over current methods. we critically analyze feature importance to reduce the dimensionality of the feature set. as we will show later, this allows us to use a more complex neural network architecture which improves performance .
the number of features specified for metapsicov refer to stage1/ stage <dig>  the number of features specified for membrain refer to the serial/parallel combination. the first part of the table contains methods that linearly combine the features/high-level predictions, the second part non-linearly. the numbers in parentheses for evolutionary information denote the number of different methods utilized




methods: contact predictor epsilon-cp
contact definition and evaluation
two residues are considered to be in contact if the distance between their respective c β atoms  is smaller than 8Å in the native structure of the protein. contacts are classified according to the sequence separation of the contacting residues. long-range contacts are separated by at least  <dig> residues. long-range contacts capture information about the spatial relationship of parts of the protein that are far apart in the sequence. as a result, they are the most valuable type of contact for structure prediction.

the standard evaluation criteria for contacts is the precision of the top ranked contacts. predicted contacts are sorted in descending order by confidence and the top predictions up to a given threshold are taken into consideration. we will use l/ <dig> l/ <dig> l/ <dig> l and  <dig> l for the threshold, where l is the length of the protein in residues. we use the precision on these lists as the performance criterion. the precision is defined as the ratio of true positives  to the number of true and false positives  combined ). true positives are predicted contacts that are indeed in contact in the native structure. false positives are predicted contacts that are not in contact in the native structure.

data and training setup
the final neural network is trained on  <dig> proteins. the hyperparameters are determined with a 5-fold cross-validation on  <dig> proteins, with the remaining  <dig> proteins as a holdout set. we used random splits for the folds. in total, there are over  <dig> million training examples. note that we use 5-fold cross-validation along the different proteins, not contact training examples. this ensures that training samples from validation proteins are unseen by the learner during parameter tuning.

the final training set has been build gradually over intermediate experiments, which are described below. we started from an original training set with  <dig> proteins from epc-map train  <cit>  and metapsicov training sets  <cit> . to filter similar proteins, we made pairwise comparisons with hhsearch  <cit>  and removed redundant sequences with an e-value below 10− <dig>  the e-value is close to zero for a highly specific match . in the beginning, we struggled with slow training and could not fit all the data into memory. to alleviate this issue, we subsampled the data. the subsampled training set included  <dig> proteins from epc-map train and  <dig> proteins from metapsicov train that exceeded  <dig> amino acids . this set is used in “feature importance analysis reveals that aa composition is obsolete in meta contact predictors” section. because of this analysis, we are able to extend the original training set to also include proteins from metapsicov test. these proteins represent difficult prediction cases because they have smaller multiple sequence alignments. to include as many as possible, we relaxed the initial, stringent filtering with hhsearch. we kept  <dig> proteins that exceed an e-value of 10− <dig> but have a sequence identity below 50% for matches that did not span more than half of the sequence. the mean sequence identity for our whole set is below 77%  and the hhsearch e-value is <10− <dig> for roughly 99% of the proteins. this diversity allows for random cross-validation splits without the fear of overfitting.

to further prevent overfitting, we use early stopping on the validation sets. the number of residues range from  <dig> to  <dig>  the median size of the number of alignments is  <dig> 

we use the following data sets to benchmark our algorithm. the test data sets include proteins from all four cath  <cit>  classes . it covers  <dig> of the top  <dig> cath superfamilies,  <dig> in total and roughly 50% of all architectures as well as  <dig> different folds. the mean sequence identity between the proteins from the training set and proteins from the test sets is roughly  <dig> % .

casp11
we take the  <dig> hard fm targets from casp <dig> for which pdb structures are available. the targets are evaluated on a domain basis with the official casp <dig> domain assignments . the sequence lengths are between  <dig> and  <dig> 

noumenon
the noumenon  <cit>  benchmark data set was designed to avoid observation selection bias in contact prediction. we removed  <dig> proteins matching a protein in the training set with a hhsearch e-value <10− <dig> to avoid overfitting, leaving  <dig> proteins.

pooled data set
the last benchmark set pools the proteins from epc-map_test  <cit> , d <dig>  <cit> , svmcon  <cit>  and psicov  <cit> . we removed all proteins with a hhsearch e-value <10− <dig> matching a protein from the training set, leaving a total of  <dig> proteins. the size of the proteins varies from  <dig> to  <dig> amino acids.

data generation
we generate multiple sequence alignments with hhblits  <cit>   with an e-value of 10− <dig> and the uniprot <dig>  <cit>  database from march  <dig> 

we use the contact prediction scores of the following five evolutionary methods: psicov  <cit> , gremlin  <cit> , mfdca  <cit> , ccmpred  <cit>  and gaussdca  <cit> .

we obtain secondary structure predictions from psipred  <cit>  and solvent accessibility from solvpred.

features
our meta prediction method epsilon-cp combines sequence-based information, evolutionary information, and physicochemical information. the  <dig> features are primarily based on the standard sequence-based features that have been used in previous studies  <cit> . additionally they include the prediction of five evolutionary methods and epc-map.

the features can be divided into local and global features. local features are computed on a window of the sequence. we use two windows of size  <dig> centered at i,j and a window of size  <dig> located at the midway / <dig>  where i,j denote the sequence position. the column features consist of the secondary structure predictions , solvent accessibility  and the column entropy of the msa. all of the column features, in addition to the amino acid composition, are replicated on a global sequence level as well.

the global features consist further of the number of effective sequences in the alignment, the sequence length, number of sequence in the alignments, the sequence positions i,j, as well as the sequence separation.

the co-evolutionary features are computed for the residue pair i,j. we use the mutual information and the apc corrected mutual information  <cit>  as well as the predictions of psicov, gaussdca, mfdca, ccmpred and gremlin and a mean contact potential  <cit> .

further, the prediction of epc-map for i,j is included. by construction, epc-map does not have predictions for all possible residue-residue pairs because contacts that do not appear in any decoy are not scored. the absence of an epc-map prediction is encoded as zero.

architecture and training
our machine learning system is a fully connected 4-hidden layer neural network with 400-200-200- <dig> neurons. we use the maxout  <cit>  activation function to model non-linearity and softmax activation in the last layer. maxout units are generalizations of rectified linear units and have been specifically developed to work in tandem with dropout  <cit> . dropout randomly “drops” units and their connections during training. it forces the network to learn a more robust representation and approximately corresponds to training and averaging many smaller networks. this approximation is only accurate for linear layers. since maxout units learn an activation function by combining linear pieces, it is linear almost everywhere except at the intersections of the linear pieces. dropout is therefore more accurate in maxout networks compared to networks that use non-linear functions  <cit> . we apply dropout with p= <dig>  after each hidden layer to avoid overfitting. we use stochastic gradient descent with a learning rate of  <dig> , a decay of 1e
− <dig> and mini batches of size  <dig>  the gradient descent is accelerated with  <cit>  momentum of  <dig> . the weights are initialized using the initialization scheme proposed by  <cit>  which scales the weights by the respective layer dimensions. we set epc-map predictions on epc-map_train proteins to zero to avoid overfitting. this is necessary because epc-map has been trained on epc-map_train. each input feature is standardized by subtracting its mean and dividing by standard deviation.

we build the neural network with keras  <cit>  and trained it between  <dig> and  <dig> epochs. due to dropout, the network can be trained for a long time, both training and validation error still decrease after  <dig> epochs of training. training of the neural network was performed on a cpu, not taking advantage of possible speed-ups achievable on a gpu. training for a single epoch took roughly 45min on a machine with two cpus and  <dig> threads. each cpu has six  <dig> ghz cores . the network outputs a probability for each class .

we obtained the best results with a first hidden layer that had significantly more units than input features, instead of commonly used bottleneck layers. we hypothesize that this architecture might allow the network to generate a new meta representation that captures of the information more effectively.

RESULTS
in the first experiment, we evaluate the performance of epsilon-cp on the three benchmark sets described in methods. we compare our method to the casp <dig> version of metapsicov  <cit> , which outperformed all other methods in casp <dig>  at the time of writing, the contact prediction assessment results of casp <dig> became available. in casp <dig>  epsilon-cp ranked behind the newer version of metapsicov .

however, we were not able to perform our own experiments using top casp <dig> algorithms because standalone versions of the top servers  were not available at the time of writing. our intention in this paper is to control for pipeline effects, such as generated msas  <cit> . thus, we used the same msas as input to all algorithms to measure their performance under the same conditions. raptorx  <cit>  does provide a web service, but using their pipeline would no longer allow us to control for pipeline effects. as standalone versions of the best casp <dig> participating methods are not yet available, we benchmark our algorithm against the best algorithm from casp <dig>  which is the casp <dig> version of metapsicov.

metapsicov operates in two stages. stage  <dig> is the output of a neural network classifier. stage  <dig> filters predictions of stage  <dig>  we will compare our method to metapsicov stage  <dig> and will from now on refer to metapsicov stage  <dig> as metapsicov. our evaluation focuses on long-range contacts since they are the most helpful in structure prediction. we used the multiple sequence alignments from the original metapsicov paper  <cit>  to ensure that our results are comparable with this study.

in the second experiment, we analyze the importance of features. we evaluate the reduced feature set in the context of meta approaches. to show that the reduction in dimensionality by excluding features is not detrimental to the performance, we compare the performance of the neural network with both the refined feature set and the complete feature set.

in the third experiment we show data that combining multiple sources of information improves the prediction accuracy. we evaluate the performance of the individual methods as well as their combinations. if the assumption holds, performance should improve with additional information sources.

finally, we briefly discuss limitations of our approach.

performance on test data sets
we will first evaluate contact prediction performance on  <dig> hard free modeling  targets from the casp <dig> experiment. contact prediction is most useful for free modeling targets because they cannot be modeled by only using templates due to the lack of similar structures. in the spirit of the casp evaluation, we analyzed the results on the basis of the  <dig> target domains.
fig.  <dig> results for long-range contacts. epsilon-cp outperforms the other predictors on all three benchmark sets


precision is computed on the  <dig> domains of these targets for the top predictions relative to the sequence length l





on the noumenon  <cit>  dataset the difference in performance is more pronounced. here, epsilon-cp outperforms metapsicov stage  <dig> on average by 65%. the trend continues that the relative mean precision increases over longer cut-offs from 37% on l/ <dig> to 77% on  <dig> l. epsilon-cp has an accuracy of  <dig> % on  <dig> l compared to  <dig> % for metapsicov stage  <dig>  notably, metapsicov stage  <dig> outperforms stage  <dig> on this dataset. a possible explanation are the low confidence predictions of stage  <dig> – on this data set even for short-range contacts. since stage  <dig> is essentially a filtering step, the performance may further deteriorate because of false negatives.

we also see a strong decline in prediction accuracy for the co-evolutionary methods . here, epsilon-cp outperforms the co-evolutionary methods 5-fold with an accuracy of  <dig> % on l/ <dig> compared to  <dig> % for gaussdca. in general, there is a big improvement over the co-evolutionary methods.
precision of the top predictions relative to the sequence length l



precision of the top predictions relative to the sequence length l





on the easier pooled data set the improvements are less pronounced but epsilon-cp still outperforms the second best method by 10% on l/ <dig> improving the accuracy from  <dig> % to  <dig> % . the proteins are easier compared to casp <dig> and noumenon since most proteins have a lot of known homologs.

summarizing, our classifier improves contact prediction over metapsicov. further, we could employ a similar strategy to metapsicov stage  <dig> to boost performance.

feature importance analysis reveals that aa composition is obsolete in meta contact predictors
in the previous section, we have shown that our meta prediction method generally improves contact prediction results over metapsicov. combining multiple sources of information can help to mitigate drawbacks exhibited by individual methods. similar to ensembling in machine learning, to maximize the impact the sources should be as diverse as possible. however, this approach of combining information as features in a machine learning system also has downsides. the concatenation of features increases the dimensionality of the learning problem which might lead to a harder learning problem and to diminishing returns in prediction precision.

the increase in dimensionality introduces mainly two problems. first, due to the curse of dimensionality the training data that is necessary to generalize correctly increases exponentially  <cit> . this results in a more complex optimization problem as well as increased data size and slower training. second, most of the commonly used feature sets have been devised to be used on their own and not in the context of meta approaches. therefore, the features from different information sources might contain information of the same subspace and thus their combination might not contribute to learning.

to investigate this issue in the context of contact prediction, we re-evaluated the features. in our initial experiments, the training of the neural network suffered especially from the large data set and the high dimensional feature set which lead to slow training. thus, we conducted a feature analysis to potentially reduce the dimensionality of our learning problem.

using neural networks for feature selection was not straightforward because there is no simple way of computing feature importance from neural networks and feature selection experiments were unfeasible due to long training times and the amount of features.

thus, we employed xgboost  <cit>  for evaluating feature importance, which is a decision tree-based algorithm. during construction, tree-based algorithms perform a feature importance ranking. the feature importance can be used as a starting point to evaluate the feature set. although interesting, the feature importance sometimes lack meaningfulness. correlation can inflate or deflate the importance of a feature. xgboost splits the data set recursively. in each split, the feature that best separates the two classes is chosen. features used in earlier splits are deemed more important. the specific feature importance measure we use is called mean decrease of impurity  <cit> . thus, the feature importance values need to be critically analyzed.

an excerpt of the results is shown in the feature importance plot . we include the results of two co-evolutionary methods to show the difference in importance depending on the method. for the purpose of visibility, the rest are left out and some features with fairly similar importance have been aggregated and averaged .
fig.  <dig> simplified and aggregated depiction of the feature importance as emitted by xgboost. the amino acid composition is attributed with the least importance, although it makes up roughly 75% of the features. the different co-evolutionary information entries correspond to different co-evolutionary methods. the feature importance depicted here is the average over a 5-fold cross-validation




strikingly, the feature importance of the amino acid composition is the lowest of all features. interestingly, this feature has  <dig> dimensions and makes up 75% of the features. note that the real feature importance according to xgboost might be masked by the aforementioned covariance issues. nevertheless, we used this feature importance information to refine our representation and re-train the neural network model.

to test the utility of the amino acid composition in our neural network model, we re-trained the system with and without the amino acid composition. the result is depicted in fig.  <dig>  removing the amino acid composition does not harm performance. performance actually increases slightly by 1−2%, likely due to the easier optimization problem . the results are based on our original training set, a smaller set where the feature set does not yet contain epc-map. the smaller training set is a strict subset of the final training set, combining  <dig> proteins from epc-map_train with  <dig> randomly chosen proteins from the metapsicov training set that exceed  <dig> amino acids. due to the aforementioned scaling issues, we cannot replicate the experiment on the whole data set in a reasonable time with the neural network. with xgboost however, we were able to verify that the performance is not harmed. xgboost allowed us to test both configurations  on the whole data set. the difference in performance is less pronounced because xgboost already largely ignores unimportant variables, but here the accuracy improved as well or at least remained the same . for instance on l/ <dig> the accuracy increased from  <dig> % to  <dig> %, on medium-range contacts the performance increased on average by  <dig> %.
fig.  <dig> comparison of three neural networks with identical architecture on epc-map_test . the baseline network  uses the full feature set and is trained on  <dig> proteins. the training proteins are a mix of epc-map_train and metapsicov proteins. the square marker denotes the neural network that is trained without the amino acid composition but on the same data set. the second network  shows the performance of the neural network after increasing the training set size from  <dig> to  <dig> proteins, which was possible because dropping the amino acid composition reduced the dimensionality of the learning problem. note here that most of the new proteins are much more complex and may not be helpful for predicting proteins in epc-map_test




training on more data generally improves the performance of neural networks  <cit> . the significant reduction in dimensionality made it possible to increase the training set size from the smaller, original training set to the final training set described in data and training setup. the performance improvements are depicted in fig.  <dig> . here, we compare the performance without the amino acid composition on the original and the final training set. on epc-map_test the mean precision increased by 6% on long-range contacts.

our assumption is that the introduction of evolutionary information renders the amino acid composition redundant.

combination of different sources of information
in this section, we aim to quantify the benefit on contact prediction accuracy of combining multiple information sources. figure  <dig> compares the performance of the individual types of information on long-range contacts on the epc-map_test data set. we use the following abbreviations: s for sequence-based information, e for evolutionary information and p for physicochemical information. s uses the feature set introduced in features  without the input features obtained from the co-evolutionary methods and epc-map. we pick gaussdca  <cit>  as the representative for evolutionary information because it had the highest feature importance out of all the evolutionary methods in our experiment. for physicochemical information we use epc-map as the representative algorithm. in this experiment, we removed gremlin from the epc-map algorithm. strictly speaking, the physicochemical predictor in epc-map also contains some sequence-based features and could also be seen as a mix of sequence-based and physicochemical information. the epc-map_test set contains many small proteins with rather small multiple sequence alignments  <cit> . epc-map performs well on this set with a mean accuracy of 50%  <cit>  because the small protein size  enables generation of decoys with good quality which impacts physicochemical contact prediction. nevertheless, combining multiple sources of information clearly improves the results. the performance increases by almost 80% from  <dig> % on l/ <dig> for s to  <dig> % for s,e,p. the increase over p is 22%.
fig.  <dig> performance comparison of different information types on long-range contacts on the epc-map_test data set. s, e, p and the respective combinations. s uses the feature set described in features minus epc-map and the co-evolutionary methods. e  is the best evolutionary method in our experiments. for p the representative is epc-map




limitations
the main limitation of our approach is that contacts are predicted in isolation. since there are reoccurring contact patterns, it makes sense to try to incorporate predictions of surrounding contacts. this could be done in a similar fashion to metapsicov stage  <dig>  where an excerpt of the contact map is used as an input for a second model. ideally, this could be done with end-to-end learning and include a feedback loop.

a second limitation concerns the feature set. most of the feature set is computed on a fixed window of the sequence potentially ignoring useful information. more powerful methods that work directly on sequences  instead may be able to lift this limitation.

CONCLUSIONS
we presented epsilon-cp, a contact predictor that combines evolutionary information from multiple sequence alignments with physicochemical information from structure prediction methods and with sequence-based information. these three sources of information are combined using a deep neural network model. we show that combining multiple sources of information improves prediction accuracy when compared to the casp <dig> winning version of metapsicov. we use stacking and train a deep neural network to derive on this relationship from data, effectively learning when a specific source of information is most likely to be effective.

the key to performance improvements achieved by our method is the reduced and refined feature set. due to a careful feature analysis, we found that the amino acid composition, a commonly used feature, can be removed without harming the performance. our hypothesis is that the introduction of evolutionary methods make the amino acid composition redundant. our results show that common features must be re-evaluated in the context of meta approaches so as to avoid redundant features that do not contribute to learning. we removed features related to the amino acid composition, reducing the size of the feature set by 75%. this allowed us to train more complex networks and to increase the size of the training set considerably. using this strategy, epsilon-cp achieves  <dig> % mean precision for the top l/ <dig> predicted long-range contacts on  <dig> casp <dig> fm hard targets, 11% higher than the second-best method. for the top  <dig> l long-range contacts the improvement is 17%.

our study suggests that further improvements in contact prediction will arise from adequately balancing feature-set size and feature expressivity on the one hand, and the size of the training data and the complexity of machine learning algorithms on the other hand. we demonstrated that a reduced feature set enables an increased amount of training data, which leads to improved contact prediction. we hypothesize that further improvements will result from creating even more powerful and compact feature sets that in turn enable the expansion of the training set and the use of more sophisticated learning methods.

additional file

additional file  <dig> supplementary. further comparisons on casp <dig> to top  <dig> predictors on medium- and long-range contacts. comparison of prediction accuracy for different number of effective sequences as computed in the metapsicov paper  <cit> . head-to-head comparison of metapsicov and epsilon-cp on all  <dig> fm targets for long-range contacts. 




abbreviations
aaamino acid composition

apcaverage product correction

caspcritical assessment of structure prediction

fmfree modeling

fpfalse positives

msamultiple sequence alignments

sgdstochastic gradient descent

tptrue positives

electronic supplementary material

the online version of this article  contains supplementary material, which is available to authorized users.

we thank tomasz kosciolek for providing the alignments they used in casp <dig> 

funding
this work was supported by the alexander-von-humboldt foundation through an alexander-von-humboldt professorship sponsored by the german federal ministry for education and research .

availability of data and materials
the datasets generated and/or analysed during the current study are available in the rbo-epsilon repository, https://github.com/lhatsk/rbo-epsilon

authors’ contributions
conceived and designed the experiments: ks ms ob. performed the experiments: ks. analyzed the data: ks. wrote the paper: ks ms ob. all authors read and approved the final manuscript.

competing interests
the authors declare that they have no competing interests.

consent for publication
not applicable.

ethics approval and consent to participate
not applicable.

publisher’s note
springer nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
