BACKGROUND
mutations give diversity to dna sequences, which led to the evolution of a variety of different species and a multitude of species from the same ancestor. even though they have similar dna sequences from a common ancestor, due to evolution, these species also have their own unique dna sequences which may be understood as the signatures for those particular species and can be used as a way to separate the species <cit> . for example, dna signatures have already been used to identify  <dig> types of human pathogenic yeast <cit> .

signatures are defined as dna patterns that are significantly different from other sequences and appear only once in the sequence database. thus, the purpose of signature discovery is to find all of the signatures in a database <cit> . much research has already been conducted in signature discovery. amin et al. integrated multiple bioinformatics tools, including cg <cit>  and islandpath <cit> , to determine horizontally transferred, pathotype-specific signature genes as targets for specific, high-throughput molecular diagnostic tools and reverse vaccinology screens <cit> . primerhunter can be used to select highly sensitive and specific primers for virus subtyping identification <cit> . to guarantee high sensitivity and specificity, primerhunter selects primers such that they efficiently amplify one of the target sequences representing different isolates of the subtype of interest, and none of the non-target sequences representing isolates of closely related virus subtypes. accurate estimates of the melting temperature of mismatches, based on a nearest-neighbor model and calculated via a fractional programming algorithm, are used in primerhunter to ensure the desired amplification properties. tofi is a tool for identifying oligonucleotide fingerprints for microarray-based pathogen diagnostic assays, which combines genome comparison tools, probe design software, and sequence alignment programs <cit> . tofi is typically used to design fingerprints for a single genome. an enhanced multiple-genome pipeline presented by satya et al. allows for efficient design of microarray probes common to groups of target genomes <cit> . insignia is web-based tool for identifying genomic signatures that are perfectly conserved by all target genomes and absent from all background genomes based on databases of bacterial and viral genomic sequences, which comprise over  <dig> distinct organisms <cit> . tofi designs signatures for microarray-based assays, and insignia finds unique sequence segments that can be used to design both pcr and microarray signatures. insignia and tofi have the ability to identify genomic signatures that are common to multiple target genomes. insignia and tofi perform similar computations, but insignia can be run online and requires less computational resources. tofi and insignia both build consensus regions among multiple genomes through pairwise alignments between the target genomes. insignia reports only the unique segments in the target genomes and provides an option for users to run primer <dig> <cit> , a pcr signature design software, on these unique segments. to quickly identify signatures in target and background genomes, insignia has to maintain a specialized database containing pre-computed matches between every pair of genomes. however, the concomitant advantage in speed comes with the limitation that users are restricted to the target and background genomes that are part of the insignia database, with no option to use other sequences as target or background genomes. topsi is a tool that extends the tofi framework to design signatures for pcr-based pathogen diagnostic assays <cit> . like insignia, topsi identifies unique segments through pairwise alignments between the input genomes. however, topsi goes beyond identification of unique segments, and incorporates modules to design pcr signatures from the unique segments and perform extensive specificity analysis on the designed signatures. topsi can provide a list of pcr signatures common to all input targets without manual manipulation. cassis is capable of computing comprehensive sets of sequence- and group-specific signatures that guarantee a predefined hamming distance, the number of mismatches with non-target sequences, from collections of deeply hierarchically clustered sequences <cit> . cassis tries to determine perfect group-covering signatures for every target group. for groups lacking a perfect common signature, cassis finds signatures with maximal group coverage within a user-defined specificity. zheng’s algorithm uses the hamming distance between sequences as a measuring stick for signature discovery <cit> . suppose l and d are two whole numbers. an l-pattern represents the dna sequence with a length of l. if two l-patterns are -similar, this means that the hamming distance between the two l-patterns does not exceed d. moreover, if -similar l-patterns could not be found, the pattern is defined as a signature under the discovery condition  in the database. zheng’s algorithm can find all of the signatures in a database as defined above. the imus algorithm improved upon zheng’s algorithm to give better discovery efficiency, but requires a larger memory <cit> . based on mathematical analysis, if a discovery condition is set as , when discovering signatures in a uniformly distributed database with a size of  <dig>  imus requires only  <dig> % of the string comparisons made by zheng’s algorithm but creates  <dig> times more entries in the index. cmd is designed to discover all implicit signatures from dna databases, where implicit signatures are signatures that satisfy discovery conditions looser than a given discovery condition <cit> .

however, none of the above algorithms distribute the computation of the databases onto multiple computers in a cluster. to use the algorithms in such a way, additional scripts must be applied to control the distribution and collection of the databases and results. unfortunately, many of these approaches do not provide a formal definition for their distribution strategies. some of the approaches, for example insignia and cassis, provide strategies to distribute the computation of the databases onto multiple computers in a cluster, but the steps of distribution and collection are not automatic. manual manipulation is necessary to use these algorithms to distribute the computation of the databases onto multiple computers in a cluster. the match pipeline in insignia applies strategies to reduce redundancy in sub-datasets, but relies mainly on preprocessing. ptpan <cit> , jellyfish <cit>  and dsk <cit>  apply different strategies to avoid the necessity of loading the whole database into memory for searches. each of the three approaches uses secondary storage. for example, jellyfish and dsk use hash tables to compute the k-mers for a given k. both algorithms achieve space efficiency by keeping most of the hash tables on disk. when counting k-mers over multiple hash tables, jellyfish would need to store the intermediate k-mer counts on disk, which requires significantly more space, and the merge phase is not parallelized. this makes the algorithm time intensive for large databases. imus and zheng’s algorithm both have two disadvantages. first, these algorithms require that the entire database to be processed  be loaded into memory, meaning that when the amount of data exceeds the memory capacity, these algorithms are unable to complete processing and cannot be used. second, they are both sequential algorithms, so the time necessary for larger databases is extensive. due to these two disadvantages, neither imus nor zheng’s algorithm is suitable for applications that require processing large databases. this is a particular problem with the development of next generation sequencing , as the rate of creation of sequence data is increasing daily, leading also to larger databases. this renders both imus and zheng’s algorithm, which are unable to process large amounts of data and require longer processing times, unsuitable for ngs data analysis.

divide-and-conquer is a computational strategy for solving both extensive and complicated problems and processing large amounts of data. the basic thought behind this is as follows: suppose the amount of data that needs to be processed for a problem is represented by |d|. if |d| is smaller, it can be easily solved and can be solved directly. otherwise, the problem may be divided into multiple smaller scale subproblems with close similarities to the original problem. these subproblems may be solved recursively, and the results combined to find a solution to the original problem. therefore, with the divide-and-conquer strategy, each recursion may include three main steps:  solve: if the problem is smaller in scale and easy to solve, it looks for a solution directly;  divide-and-recur: divide the original problem into multiple smaller scale subproblems closely similar to the original problem, then recursively try to find the solution to each subproblem;  combine: take the solutions from the subproblems and combine to find the solution to the original problem <cit> . in addition, as technology has matured, the price of multi-core cpus has continued to fall, so the possibility to use parallel processing technology on a computer cluster to enhance processing efficiency has greatly improved. in fact, parallel processing technology is already used in many bioinformatics research fields, such as sequence alignment and analysis, protein structure prediction, and motif finding. if we can use the divide-and-conquer strategy and parallel processing technology in signature discovery, this will improve the efficiency of discovery in large databases, which will be immensely helpful.

in this research, we propose a signature discovery algorithm called distributed divide-and-conquer-based signature discovery  algorithm. the ddcsd algorithm is designed specifically for discovering signatures on a computer cluster. the ddcsd algorithm automatizes the steps of distributing the database and collecting the unique signatures. the signatures are discovered from the database and provided to users without manual manipulation. the ddcsd algorithm uses the divide-and-conquer strategy to overcome the problem of processing large databases and compares multiple patterns in parallel to accelerate signature discovery. therefore, the algorithm not only shortens the amount of time needed for discovery, it also is able to process the large databases that could not be processed in the past using imus and zheng’s algorithm. in addition, by setting the threshold value of the direct discovery, ddcsd can limit the memory requirement in discovery to the memory size of the computers in the cluster. more specifically, the ddcsd algorithm can process any amount of data and is not limited by the amount of memory available. the ddcsd algorithm is implemented using a basic divide-and-conquer strategy as the basic structure. first, it decides whether to do direct discovery based on the size of the database. if the database is too large to load in its entirety, it will split the database into two equal parts and recursively processes the parts. as the recursive processing is in progress, the amount of data in a single part will gradually decrease until it can load the single part all into the memory of one computer in the cluster at one time. at the end, it will combine the results that were found separately in the two different parts and find the signatures in the original database. the ddcsd algorithm gives the formal definition in recursion for the dataset distribution strategy, that is not provided by the previous approaches. the ddcsd algorithm includes main and discovery routines. the main routine organizes discovery in a planned way. the discovery routine is used to find the unique patterns from a specified dataset in another dataset. the computation of discovery and collection in ddcsd is distributed onto discovery nodes for parallelization. based on the experiments made on the human whole-genome est database that has approximately  <dig> g bases, the ddcsd algorithm proposed here can successfully process that database. whereas previous algorithms could not process databases so large, the ddcsd algorithm took  <dig>  hours to find all of the signatures under the discovery condition  on the cluster of ten discovery computers with  <dig> gb memory. the main contribution of this research is utilizing the divide-and-conquer strategy in signature discovery to process discovery in large databases, something previous algorithms were unable to do, and providing a parallel signature discovery algorithm on a cluster, that can process databases of any size regardless of the amount of memory available. this algorithm can be applied to ngs data analysis and other analysis of large databases.

methods
suppose that l and d represent the length and the number of allowed mismatches of signatures, respectively, and Λ is a dataset made up of l-patterns. we define signatures in Λ under a discovery condition  as patterns that exist in Λ and where there are no other -similar patterns inside of Λ. the purpose of this research is to utilize a divide-and-conquer strategy to provide a parallel algorithm that can rapidly discover the signatures in datasets with massive amounts of data on a computer cluster.

for any subset Θ of Λ, if no -similar pattern can be found in Θ, this pattern is considered unique in Θ. according to this definition, we can deduce that if one pattern p is a signature in Λ, then p must be unique in Θ. therefore, if we divide Λ into two partitions of equal size , then p will be a signature for either Λi or Λj and will be unique to the other partition. thus, when the signatures of Λi and Λj are combined, they will include all of the signatures for Λ, making them valid candidates to discover signatures in Λ. most importantly, no matter how many levels of recursive processing are applied, this characteristic still stands, meaning that we can use the divide-and-conquer strategy on a computer cluster to deal with the original problem posed to signature discovery algorithms where they could not process large databases. using the above as the foundation, we designed a distributed divide-and-conquer-based signature discovery  algorithm that can rapidly discover the signatures that satisfy the discovery condition  in a large dataset on a computer cluster. the ddcsd algorithm includes main and discovery routines. the discovery routine accepts the candidate and source datasets that are made up of l-patterns and will find the patterns that are unique in the source in the candidate. it must be made clear that when the candidate and source are set as the same dataset, the patterns found by the discovery routine are the signatures for the dataset. each of the computers in the cluster is called a node. the node that handles the main routine is called a main node, and those that handle the discovery routine are called discovery nodes.

the main routine of the ddcsd algorithm is shown in figure <dig>  the symbols used in ddcsd are presented in table <dig>  the ddcsd algorithm first examines the size of the dataset Λ; if the number of patterns is less than or equal to the preset threshold value, n, then it will automatically send Λ to a discovery node and use the discovery routine to find the signatures in Λ. the discovery result is sent back to the main node. the threshold value is decided based on the memory space of discovery nodes and is set so that the patterns in Λ  can be loaded into the memory. on the other hand, if the number of patterns is more than the preset threshold, then Λ will be divided into two equal partitions Λi and Λj, with each being recursively processed individually. after recursive processing, the algorithm combines the results to find the signatures of Λ. suppose that Ωi and Ωj represent all of the signatures in Λi and Λj, respectively. because all signatures of Λ must be present in either Ωi or Ωj and will be unique to the other partition, additional comparisons must be made between Ωi and Λj as well as between Ωj and Λi to find the patterns unique in the other partition and delete the non-unique ones. therefore, ddcsd uses the discovery routine to find the unique patterns in Λj that are in Ωi and the unique patterns in Λi that are in Ωj. these two discovery results are then combined, with the final result being the signatures of Λ.figure  <dig> 
the main routine of the ddcsd algorithm. the algorithm discovers signatures from Λ under the discovery condition , where l and d are the length and the number of allowed mismatches of the signatures, and, Λ is a dataset made up of l-patterns. |Λ| represents the number of patterns in Λ. n is the preset threshold value for direct discovery.


the symbols and their definitions in the ddcsd algorithm



l

d

n
Λk
Ωk
the following example illustrates the processes of ddcsd. it is assumed that the number of patterns in a given dataset Λ is |Λ| = 4n, where n is the threshold value of direct discovery. as |Λ| is more than n, according to the processing rule of ddcsd, Λ is divided into two partitions, Λ <dig> and Λ <dig>  of equal size with |Λ|/2 = 2n. as |Λ1| is still greater than n, Λ <dig> is further divided into two partitions, Λ <dig> and Λ <dig>  of size n. now, since |Λ3| is not greater than n, ddcsd stops dividing Λ <dig> into partitions, and executes discoveryroutine  on a discovery node, which yields the signature set, Ω <dig>  from Λ <dig>  similarly, as |Λ4| is not more than n, ddcsd directly discovers the signature set Ω <dig> from Λ <dig> on a discovery node. then, ddcsd executes discoveryroutine  and discoveryroutine  on discovery nodes, respectively. the union of the obtained Ω <dig> and Ω <dig> is the signature set, Ω <dig>  of Λ <dig>  similarly, ddcsd processes Λ <dig> by dividing it into two partitions, Λ <dig> and Λ <dig>  each with a size of n. from Λ <dig> and Λ <dig>  the signature sets Ω <dig> and Ω <dig> are discovered. then, Ω <dig> and Ω <dig> are combined to obtain the signature set of Λ <dig>  namely Ω <dig>  finally, ddcsd combines Ω <dig> and Ω <dig> to get the signature set of Λ, namely Ω. table <dig> shows the sequence of signature sets identified by ddcsd. the required processes for discovering the signature sets are also presented in the table. the discovery processes are encoded for clear illustration. table <dig> lists the processing time of the discovery processes shown in table <dig>  assume that a, b, c, d, e, f, g, h, i and j are discovery processes. the cluster contains two discovery nodes, namely dn <dig> and dn <dig>  first, ddcsd assigns dn <dig> and dn <dig> to discovery processes a and b, respectively. after the processing of discovery process a is completed, dn <dig> is immediately assigned to the next discovery process, discovery process c. dn <dig> and dn <dig> are assigned to process the discovery processes, until all of the discovery processes are completed. in this case, dn <dig> is assigned to discovery processes a, c, e, f, g and j, and dn <dig> is assigned to discovery processes b, d, h and i. the discovery time consumed by dn <dig> and dn <dig> is  <dig> seconds each, so the overall discovery time is  <dig> seconds. it is noteworthy that some of the discovery processes are sequentially interdependent. the sequential interdependence might affect the overall processing time for discovering signatures on the computer cluster. that is, when the preceding discovery process needs relatively more processing time, the successive dependent discovery process might have to wait. for example, discoveryroutine  can only be executed after discoveryroutine  is completed, that is, discovery process h can only be executed after discovery process f is done. assume the processing time of discovery process f is  <dig> seconds. in this case, although discovery process g is completed, discovery process h cannot be immediately processed because discovery process f is still executing. therefore, discovery process h has to wait. this reduces the discovery efficiency of ddcsd.table  <dig> 
an example of ddcsd


a
b
c
d
e
f
g
h
i
j
the order field presents the order of the signature sets identified by ddcsd. the process field presents the required process for discovering each signature set. the process id field lists the represented code of each discovery process. dr() is the abbreviation of discoveryroutine().
processing time for the discovery processes shown in table
2


a
b
c
d
e
f
g
h
i
j
the process id field lists the discovery processes shown in table <dig>  the time field presents the processing time of each discovery process. the time unit is seconds.



suppose that p and q are two l-patterns. if p is divided into equal and non-overlapping ⌈l/γ⌉ number of γ-patterns, these γ-patterns are called γ-segments of p. pγ,i represents the i-th γ-segment in p. p is called -matched to a γ-pattern Γ if pγ,i is -similar to Γ. we arrive at the observation that if p and q are -similar, for a given γ, there will be at least one i such that p is -matched to qγ,i. using the observation as the foundation, we designed the discovery routine of ddcsd.

the discovery routine of the ddcsd algorithm runs on discovery nodes. the discovery routine allows multiple processors to compare similarity of different patterns at the same time to allow for faster discovery speed. the discovery routine is shown in figure <dig>  suppose that the candidate and source datasets received by the discovery routine are Ξ and Θ, respectively. first of all, the discovery routine will set a suitable γ according to the memory that is available in the discovery node, where γ is a whole number between ⌈l/⌉ and ⌈l/2⌉. the larger the number, the less strings are compared during a discovery, but more memory is needed. conversely, the smaller the number, the more strings are compared during a discovery, but the memory requirement will be smaller. suppose that Υ is a γ-pattern. a -group is a group of l-patterns. all of the l-patterns are -matched to Υ. according to the γ-segments included in the l-patterns in Ξ and Θ, the discovery routine assigns the l-patterns to -groups. more specifically, if p is in Ξ, then p will be put into -groups, where 1 ≤ i ≤ ⌈l/γ⌉. for exampmle, assume that l, d and γ are  <dig>   <dig> and  <dig>  respectively. if p = ’acgt’, then p will be put into - and -groups. assume that Δ is one of the -groups. ΔΞ represents the set of ps in Δ. according to the size of the memory, the discovery routine can pull the patterns that have yet to be processed in Θ. if there are too many patterns in Θ which cannot be loaded into the memory all at once, it can split them into multiple parts and load and process the parts one at a time. suppose that Φ represents the group of patterns that are loaded at this time. if q is in Φ and q is -matched to a γ-pattern Γ, q will be put into -groups, where 1 ≤ i ≤ ⌈l/γ⌉. for example, assume that l, d and γ are  <dig>   <dig> and  <dig>  respectively. if q = ’tgca’, then q will be put into - and -groups, where Γ1 ∈ {’tg’,’gg’,’cg’,’ag’,’tt’,’tc’,’ta’} and Γ2 ∈ {’ca’,’aa’,’ga’,’ta’,’cc’,’cg’,’ct’}. assume that Δ is one of the -groups. ΔΦ represents the set of qs in Δ. pairing this definition with the previous observation, we find that the patterns that are -similar to the patterns in ΔΞ must be present in ΔΦ. therefore, for the patterns in ΔΞ, when examining whether they are unique, this principle can be applied to limit the discovery to similar patterns to those patterns to decrease the number of patterns compared. each time a processor in the discovery node completes the task that it is given, the discovery routine takes a -group for that processor to process, which allows for parallel processing. suppose the -group taken was Δ. for an l-pattern p in ΔΞ, when searching for -similar patterns to p in Φ, the discovery routine only compares p and the patterns in ΔΦ to find whether there are -similar patterns to p. if no -similar pattern to p is found in ΔΦ, then it means that p is unique in Φ. conversely, it is not unique and is deleted. the discovery routine repeats the above process until all -groups are processed.figure  <dig> 
the discovery routine of the ddcsd algorithm. the algorithm runs on a discovery node. the algorithm discovers the unique patterns from Θ that are in Ξ under the discovery condition , where l and d are the length and the number of allowed mismatches of signatures, and, Ξ and Θ are datasets made up of l-patterns.



RESULTS
mathematical analysis
the time complexity of the discovery routine used in the ddcsd algorithm is analyzed and the results are integrated, yielding the time complexity of the ddcsd algorithm. the memory consumption is also analyzed.

suppose that l and d represent the length and the number of allowed mismatches of signatures, respectively. γ is a whole number between ⌈l/⌉ and ⌈l/2⌉. Ξ and Θ that are made up of l-patterns are the candidate and source datasets received by the discovery routine. |Ξ| and |Θ| denote the number of patterns in Ξ and Θ. according to the γ-segments included in the l-patterns in Ξ and Θ, the l-patterns are assigned to -groups, where Υ is a γ-pattern and 1 ≤ i ≤ ⌈l/γ⌉. assume Ψ is the set of all possible -groups. Δ ∈ Ψ. ΔΞ = Δ ∩ Ξ and ΔΘ = Δ ∩ Θ. since the patterns that are -similar to the patterns in ΔΞ must be in ΔΘ, each pattern in ΔΞ requires |ΔΘ| string comparisons to check whether it is unique, where |ΔΞ| and |ΔΘ| are the number of patterns in ΔΞ and ΔΘ, respectively. each of the string comparisons includes α = l-γ character comparisons. the total number of character comparisons in the discovery routine, denoted as tdr, is:  

suppose that Ξ and Θ are in uniform distribution. in this case, Ψ should contain 4γβ -groups, where β = ⌈l/γ⌉. |ΔΞ| ≈ β|Ξ|/ = |Ξ|/4γ and |ΔΘ| ≈ κβ|Θ|/ = κ|Θ|/4γ, where. in the uniformly distributed case, the total amount of character comparisons in the discovery routine, denoted as, is:  

suppose that the input dataset Λ has a uniform distribution, and contains |Λ| = 2nn patterns, where n is the threshold value for direct discovery and n is a whole number. in each recursion, the division can be done by performing a sequential scan on Λ when dividing Λ into two partitions. the computational cost of division is η1|Λ|, where η <dig> is a constant. the amount of patterns sent to and received from discovery nodes in data transmission for processing each partition are all |Λ|/ <dig>  the total computational cost for data division and transmission in each recursion is η1|Λ| + 2 = η0|Λ|, where η <dig>  η <dig> and η <dig> are constants and η0 = η1 + η2 + η <dig>  the computational cost of using ddcsd to discover signatures from Λ, denoted as, is:  

where and η = nη <dig> 

the computational cost for data division and transmission, η|Λ|, is not too large in comparison with the computational cost for discovery, ζ|Λ| <dig>  the time complexity of using ddcsd to discover signatures from Λ is o.

suppose that the input dataset Λ has a uniform distribution. according to the γ-segments included in the l-patterns in Λ, the l-patterns are assigned to 4γβ -groups. each of the -groups should contain approximately |Λ|/4γ + κ|Λ|/4γ patterns. in ddcsd, the memory is mainly used to store the patterns in the -groups. the total memory consumption in ddcsd, denoted as, is:  

where.

the discovery node handles the discovery routine in ddcsd. if there are too many patterns in the source and candidate datasets, so that they cannot be loaded into memory all at once, the discovery routine will split them into multiple parts and load and process the parts one at a time. in addition, the threshold value for direct discovery, n, is decided based on the memory space of discovery nodes so that the patterns in the datasets can be loaded into the memory. thus, the number of patterns in each of the parts is on the order of n. according to the γ-segments included in the l-patterns in the parts, the l-patterns are assigned to -groups. in discovery nodes, the memory is mainly used to store the patterns in the -groups. based on the above discussion about the total memory consumption in ddcsd, the memory consumption of each discovery node is τ|n|.

the space complexity of using ddcsd to discover signatures in Λ is o. the space complexity of a discovery node is o.

performance evaluation
the experimental platform that we used was a cluster of eleven computers, including one main node and ten discovery nodes. the main node was equipped with an intel core i <dig> cpu  <dig> at  <dig>  ghz,  <dig> gb of memory and  <dig>  tb of disk space. each of the discovery nodes was equipped with an intel core i <dig> cpu  <dig> k at  <dig>  ghz,  <dig> gb of memory and  <dig> tb disk space. the operating system was centos release  <dig> , and the algorithm tested was coded in java and compiled in jdk  <dig> . in this experiment, we used the human whole-genome est database with  <dig> g bases to test the performance of the ddcsd algorithm. in order to avoid impacting the testing, we deleted all remarks and sequences shorter than  <dig> bases in the database and replaced all universal characters, for example ‘don’t care’, in the sequences with an ‘a’.

when testing the ddcsd algorithm, each recursion only loads the beginning and ending position of the data partition and not the actual data. only when the discovery needs to happen does it load the data completely into the memory in order to avoid taking up large amounts of memory. in the tests, each l-pattern is divided into  <dig> segments, with the γ value set to l/ <dig> 

in terms of testing the discovery performance of the ddcsd algorithm, we used the human whole-genome est database as the experimental dataset, ten discovery nodes, and a dataset threshold for direct discovery at  <dig> mb. the results are shown in table <dig>  our data shows that the ddcsd algorithm can discover all signatures under the discovery condition  from the human whole-genome est database in  <dig> seconds, about  <dig>  hours, and discover all signatures under the discovery condition  from the database in  <dig> seconds, about  <dig>  hours. when the length of patterns were the same, if the mismatch tolerance d is larger, a larger number of strings are compared. thus, time required is significantly greater than that of when the d value is lower. comparison is difficult when dealing with implementations that were optimized for different tasks. the requirements of discovery algorithms with regard to hardware components, for example the demand on memory size, are different. although in cases where there is a sufficient amount of memory many existing discovery algorithms, such as tallymer <cit> , are able to process the dataset in the experiment. however, the memory requirement of those algorithms is often too large so that they cannot be executed on general-purpose computers with normal memory size. ddcsd uses a divide-and-conquer strategy to recursively divide large datasets into smaller datasets until the split datasets can be processed using the current memory size of discovery nodes. then, the discovered signature sets from each of the split datasets are integrated to obtain the signature set of the original dataset. therefore, when the memory size available for the discovery nodes is limited, even to the  <dig> gb or  <dig> gb common on regular personal computers or smaller, ddcsd can still process large datasets. the processing ability of ddcsd is not limited by memory size. in addition, by setting the threshold value for direct discovery, ddcsd can limit the memory requirement of discovery nodes during discovery, which ensures that ddcsd can run on a cluster of discovery nodes of different memory sizes.table  <dig> 
the discovery time for the ddcsd algorithm to discover signatures from the human whole-genome est database under various discovery conditions




the experiment uses ten discovery nodes. the time unit is seconds.



in order to test the impact of the number of discovery nodes on the discovery performance of ddcsd, under the discovery condition , we utilized two to ten discovery nodes to perform signature discovery on the human whole-genome est database. the dataset threshold for direct discovery is set to  <dig> m bases. we define acceleration as the ratio of the discovery time when two discovery nodes are used to the discovery time when various number of discovery nodes are used. the acceleration indicates the improvement in discovery performance. the results are shown in table <dig> and figure <dig>  table <dig> presents the discovery time for the ddcsd algorithm to discover signatures when various number of discovery nodes were used, and figure <dig> presents the acceleration due to the various number of discovery nodes. as we can see, when the number of discovery nodes increases, the discovery time decreases and acceleration increases. for example, when using four discovery nodes, the discovery performance is  <dig>  times what it was with two discovery nodes. when using ten discovery nodes, the discovery performance is  <dig>  times that of when there was two discovery nodes. the improved discovery performance is linearly related to the number of discovery nodes.table  <dig> 
the discovery time when various number of discovery nodes were used


the time unit is seconds.
the acceleration when using various number of discovery nodes. the values within the inside of the bars are the number of discovery nodes.



finally, we tested the effect of thresholds for direct discovery on the discovery performance of ddcsd. this test was done on the human whole-genome est database using ten discovery nodes with the discovery condition set as . the threshold value n was chosen between  <dig> and  <dig> mb. the results are shown in figure <dig>  from the results, we can see that with the same amount of data, as n decreases, discovery time increases, and, discovery time decreases with the increase of n. for example, if n is set to  <dig> mb, then discovery time is approximately  <dig> % more than that of when n is set to  <dig> mb. the smaller the n value, the more recursions are necessary in discovery, and the need for additional computation and overheads in data transmission also increases. however, the memory requirement for discovery processes decreases with the decrease of n, as is intuitively obvious. the results also indicate that the size of the database that the ddcsd algorithm can process is not limited by the amount of memory available. as long as a suitable n value is set based on the size of the memory in discovery nodes, even when facing large amounts of data, it can still be successfully processed.figure  <dig> 
the discovery time when using various thresholds for direct discovery. the values within the inside of the bars are the threshold values.



CONCLUSIONS
in this research, we proposed a distributed divide-and-conquer-based signature discovery  algorithm. the ddcsd algorithm uses a divide-and-conquer strategy to overcome the problem of processing larger databases, thus solving the disadvantage of previous algorithms that could not process large databases. also, a parallel computation mechanism on a computer cluster was used to accelerate the signature discovery. therefore, this algorithm is not limited by the amount of memory available, and can rapidly find signatures in large databases, making it applicable to analysis of ngs and other large amounts of data.

competing interests

the authors declare that they have no competing interests.

authors’ contributions

hpl carried out the unique signature studies, participated in the design of the study, programmed the algorithms, evaluated the experimental results and drafted the manuscript. tfs participated in its design and coordination, performed the mathematical analysis, drafted the manuscript, convinced of the study and helped to gather data. both authors read and approved the final manuscript.

