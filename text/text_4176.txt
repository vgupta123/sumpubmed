BACKGROUND
microarrays are a powerful tool for biologists as they enable the simultaneous measurement of the expression levels of thousands of genes per tissue sample  <cit> . one of the interesting applications of gene expression profiling is the identification of compact gene signatures for diagnostic or prognostic purposes, such as cancer classification. one of the first studies in this regard was the work of van 't veer et al.  <cit> , in which a prognostic 70-gene signature is identified, that can be used to assess whether a breast tumor is likely to metastasize or not. signatures like the 70-gene signature of van 't veer are, in essence, comprised of two parts: a limited set of features and a classifier that maps a vector of feature values to a class label. limiting the number of features has several advantages. for one, using too many features with flexible classifiers quickly leads to overfitted decision rules. the inclusion of irrelevant features can also substantially degrade the performance of some classifiers. furthermore, understandability, efficiency, and cost also benefit from more compact rules.

microarray breast cancer event prediction, however, has proven to be difficult, as few classification rules are able to obtain a balanced accuracy rate of over 70%, when properly validated  <cit> . these performance indicators are also often associated with wide confidence intervals  <cit> . furthermore, ein-dor et al.  <cit>  showed that signature composition strongly depends on the subset of patient samples used for feature selection. in recent years many different signatures have been proposed, mostly derived using different patient populations and/or array technologies. although the overall performance of these signatures is comparable, there is often a high level of inconsistency between class assignments obtained using different signatures, as was recently reported in  <cit> . this poses significant challenges for the use of gene expression classifiers in clinical routine. although biological variability is conjectured to play a major role in the observed discrepancies, in this paper we show that even in a very controlled setting, using identical arrays, patient samples, signature composition, and classifiers, still large discrepancies in performance and individual class assignments can be observed under two types of variability.

one of the challenging aspects of microarray data is that they are subject to various sources of technical variation, arising from the many experimental laboratory steps needed to get from a tissue sample to an array scan, such as array batch variability, dye incorporation, uneven hybridizations, probe-failure caused by dust or scratches, or washing conditions  <cit> . some noise factors bias large groups of measurements in a systematic way. fortunately, most of this bias can be removed by proper preprocessing. many preprocessing methods have been proposed to address these systematic biases. the effectiveness of such procedures and the plausibility of their assumptions, however, depends on factors such as study design, the array technology being used, and the biological phenomenon under study  <cit> . furthermore, even after correction for systematic effects by the preprocessing method, there remains a residual variance that is both array and feature specific and that can be substantial  <cit> . detailed error models have been proposed that attempt to quantify such uncertainty around the expression data point estimates, e.g. the rosetta error model  <cit> . such uncertainty information has been incorporated in differential gene expression analysis methods  <cit> , as well as in clustering analysis  <cit> , and principal component analysis  <cit> , often leading to more consistent results.

the impact of noise on the outcome of the statistical analysis of microarray data has been a subject of debate. tu et al.  <cit>  performed a detailed sensitivity analysis to separate noise caused by sample preparation from noise related to the hybridization process. the latter was identified to be the more dominant of the two. in addition, a strong dependence of hybridization noise on the expression level was reported. based on data from the maqc study  <cit> , however, klebanov et al.  <cit>  claim that for affymetrix arrays the magnitude of technical variation has been gravely exaggerated in the literature and that the effects on the results of statistical inference from affymetrix genechip microarray data are negligibly small. however, contradictory findings have been reported in  <cit> , based on the very same data. in addition, the maqc study itself has been criticized for presenting their case in a best case scenario, using too few and overly clean reference samples  <cit> . with regard to the impact of the choice of preprocessing method, it has been observed in differential expression studies that preprocessing can strongly influence whether a gene is detected to be differentially expressed or not  <cit> . similar observations have been made for the influence of preprocessing on classification  <cit> , albeit in a different and much smaller setting than the work presented here.

although microarray data is known to be subject to the sources of variation described above, in microarray breast cancer classification studies the influence of the choice of preprocessing scheme and of the uncertainty around expression data point estimates are almost always ignored. in this paper, we study the effect of these two types of variability of expression data on breast cancer classification in detail. we define preprocessing variability as the variation in the value of a feature as induced by switching to an alternative preprocessing scheme. perturbation variability is defined as the variation in the value of a feature as caused by adding noise based on the uncertainty information associated with the expression data point estimates. furthermore, feature variability is understood to be the variation in the value of a feature as caused by either preprocessing or perturbation variability.

we have performed a comprehensive sensitivity analysis of microarray breast cancer classification under feature variability. we used a large breast cancer compendium consisting of eight different datasets, involving  <dig> hybridizations, and containing data from both one and two-color array technology. we studied the impact of preprocessing and perturbation variability on feature selection, classification performance, and classification concordance for six different preprocessing methods. in addition, we performed a comprehensive stability analysis for a diverse set of classifiers, by explicitly testing these classifiers for their noise tolerance. stability was quantified by the variation in class assignment of perturbed expression profiles, where the amount of perturbation is based on uncertainty information directly related to the selected preprocessing strategy. our results indicate that even when using identical arrays and sample populations, preprocessing and perturbation variability have a strong impact on the classification of individual breast cancer samples, as well as on the composition of breast cancer signatures, especially when the number of features is low.

methods
data
the datasets we consider in this paper share a common theme, i.e., they have been used to predict whether a breast tumor will metastasize within five years  or not , based on gene expression data inferred from removed tumor tissue. we performed our sensitivity analysis using a compendium of eight publicly available datasets. in total, the compendium contains microarray data from  <dig> hybridizations and for  <dig> samples class label information was available . some of the eight datasets initially had an overlap, either in patient samples or in hybridizations. the compendium of  <dig> arrays, however, contains no overlap, as all duplicate cases were removed. data from the studies of van 't veer and van de vijver were obtained using two-color custom ink-jet oligonucleotide arrays produced by agilent. processed data for these datasets can be downloaded from http://www.rii.com/publications/2002/default.html. like the original authors, we combined the two datasets. we refer to this combined dataset as the rosetta dataset. the rosetta dataset consists of  <dig> lymph-node negative samples of the van de vijver dataset and of the  <dig> training and  <dig> validation samples of the van 't veer dataset. data for all other datasets was obtained using affymetrix genechips and cel files were downloaded from geo  <cit>  and arrayexpress  <cit> . a more comprehensive overview of the selected hybridizations for the affymetrix datasets, including class label information, can be found in additional file  <dig>  additional file  <dig> gives an overview of the  <dig> cases of the van de vijver dataset that were added to the van 't veer dataset.

the column total contains the total number of hybridizations available, while the column labeled shows the number of samples that have a properly defined class label. the next two columns indicate the decomposition of this number into good and poor prognosis cases. the columns repository and accession list in what repository and under which accession number each dataset can be found.

preprocessing
for the van 't veer and van de vijver datasets, we used the publicly available expression estimates and corresponding error information based on the rosetta error model  <cit> . in principle, the rosetta error model is applicable to both one and two-color arrays. however, for this model no freely available implementation exists and hence for the affymetrix datasets this model was not applied. for the datasets using affymetrix genechips we generated expression data from the available cel files based on five different, frequently used preprocessing strategies: mas  <dig> , mgmos, its multi-chip version mmgmos, rma, and dchip. for preprocessing, all available hybridizations were used. this is especially relevant for the multi-chip models dchip, rma, and mmgmos, which benefit from having more arrays assuming all hybridizations are of similar quality. the dchip expression estimates are constructed using only the information of the pm-probes, which is the default choice for dchip. affymetrix datasets were log-transformed and all probesets were median centered after preprocessing, for each dataset separately. the validity and benefits of this step are further discussed in  <cit>  and  <cit> . preprocessing for the affymetrix datasets was performed in r  <cit>  using bioconductor  <cit>  packages affy  <cit>  and puma  <cit> . table  <dig> provides a summary of the six preprocessing methods used.

the column package indicates which r package was used to obtain the expression values, while the column function provides the name of the function used from the package. column log <dig> indicates if the expression estimates as returned by the function are already on log <dig> scale or not. the column σ indicates if the function directly computes uncertainty information or not.

perturbation
after preprocessing, we get an expression estimate xij for each array i and each feature  j. in fact, xij is usually stochastic, following some distribution dij with mean μij = xij and standard deviation σij reflecting the measurement uncertainty associated with the point estimate xij. in this paper, we utilized the uncertainty information as captured by the distributions dij to generate perturbed expression profiles as alternatives for expression point estimates xij, in a similar fashion as presented in  <cit> . for each sample i, for each gene j in a given signature, we simply draw a new data point  by sampling from the corresponding distribution dij. complete perturbed training and validation sets can be constructed by repeating this process for all samples and genes.

the rosetta model, mgmos, and mmgmos are specifically designed to provide a σij that reflects the uncertainty of the complete preprocessing cascade. in these three models dij is a gaussian distribution. for mgmos and mmgmos, the corresponding σij values were obtained using the r package puma <cit> . for the van 't veer and van de vijver datasets, we used the published expression values. for the van de vijver data, the standard deviations σij, as estimated by the rosetta error model, were reported directly. for the van 't veer data σij was not provided directly, but σij could be recovered from the published p-value information . mas  <dig> , dchip and rma are not specifically designed to provide detailed error estimates, although some of the uncertainty associated with the point estimates can be derived from the summarization step in the preprocessing cascade. for rma and dchip, the uncertainty corresponding to the summarization step can again be modeled by a gaussian distribution. the estimated σij values for these two models were obtained using the r package affy <cit> . for mas  <dig>  it turns out that the estimates follow a distribution closely related to a t-distribution. although error information for mas  <dig>  is not available from affy directly, it can be computed from the information affy provides .

stability measure: minority assignment percentage
classification instability occurs when for a given classifier and a given sample, perturbed expression profiles are not all assigned to the same class. in order to quantify the instability over a large number of perturbed datasets, we propose the following simple stability measure, which we refer to as the minority assignment percentage  score. for a given sample and feature, we denote the percentage of perturbed datasets that lead to a classification into class  <dig> by p <dig>  and the percentage leading to a class label  <dig> by p <dig>  then the minority assignment percentage is equal to min{p <dig>  p1}. in the ideal case, a map-score is equal to zero, indicating that all perturbed datasets lead to the same classification for this specific sample. in the worst case, it equals 50%, indicating that classification is purely random. note that this observation is independent of the choice of dataset, perturbation mechanism, classifier or number of features. in the remainder of the paper we will consider a classification to be unstable if the map-score exceeds a conservative threshold of 35%, meaning an almost random classification.

sensitivity analysis protocol
all classification results are obtained in a systematic fashion, closely related to the protocol proposed in  <cit> . figure  <dig> provides a schematic overview of our workflow. assume we have obtained expression values xij and the corresponding σij values, for a given measure of expression, for some set of samples and a set of genes, using the methods described in the previous sections. in addition, assume we have selected an appropriate classifier, which we need to train. in the first step, we create a stratified split of the available data, in which 80% is used as a training set, while the remaining 20% serves as a validation set. in step  <dig>  we create p =  <dig> perturbed versions of the validation set. in step  <dig>  we rank the features based on their signal-to-noise ratio  on the  training set. in the next step, we use the top- <dig> ranked features to construct a sequence of  <dig> classifiers, where the nth classifier is constructed on the training data, using only the top-n ranked genes. at step  <dig>  we invoke each classifier to obtain class assignments for both the unperturbed validation set, and for all perturbed versions. in step  <dig>  we obtain a performance estimate for the unperturbed validation data by computing the balanced accuracy rate, that is, the average of the sensitivity and specificity. in step  <dig>  we use the class assignments of the  <dig> perturbed validation sets, as obtained in step  <dig>  to compute the associated map-scores and collect them in a map-matrix, where the entry at row i and column n represents the map-score of validation sample i, for a classifier trained on the top-n ranked features. to ensure that results are not split-specific, steps  <dig> to  <dig> are repeated r =  <dig> times . at step  <dig>  we compute a performance curve, referred to as the p-curve, which for each signature size n ∈ { <dig> ...,100}, displays the average balanced accuracy over the r splits. furthermore, as mentioned, for a given sample we consider a class assignment to be unstable if the corresponding map-score is larger than some threshold t =  <dig>  for a given threshold, in step  <dig> we compute the stability curve, referred to as the s-curve, which for each signature size tells us the average percentage of cases, over r splits, that had a map-score larger than the selected threshold t. note that ideally the s-curve should be zero for all entries. the whole procedure described above is repeated for each preprocessing method . in order to compare results for different classifiers and preprocessing methods, for a given dataset and for each repeat of the inner loop we always used the same set of stratified splits. finally, in step  <dig> we generate a discordance curve, referred to as the d-curve, for all distinct preprocessing method pairs. for a preprocessing method pair  and given classifier, the corresponding d-curve tells us for each signature size the average percentage of cases, over r splits, of inconsistent class assignments on the  validation sets. similarly to the s-curve, ideally a d-curve is zero for all entries. note that the map-scores used for the s-curves can also be viewed as a measure of concordance, under perturbation variability.

snr-based feature rankings
as stated in previous section, in the third step of our protocol we rank the available features based on their signal-to-noise ratios. for a given feature, let μ <dig> and μ <dig> denote the mean intensity value for class  <dig> and class  <dig>  respectively, and let σ <dig> and σ <dig> be the corresponding standard deviations. then the snr is equal to   

let snrj, m denote the snr value corresponding to gene j, based on data corresponding to preprocessing method m. in the construction of a signature we typically select the top-n features from such a ranking.

let fn, m denote the top-n genes, obtained using data from preprocessing method m, for a particular split. different preprocessing methods may lead to different lists of top-n genes. for two different methods m and m', a trivial measure to compare the lists fn, m and fn, m' would be to look at their intersection. from a classification standpoint, however, we would at least hope to obtain two lists that are of comparable strength. let the total strength of a feature set f with respect to method m be defined as   

to compare two gene lists of cardinality n, we introduce the concept of relative strength, given by   

the relative strength compares the total strength with respect to m for a selection based on m' to the selection based on preprocessing method m itself. as the latter gives the maximal total strength for a set of size n with respect to method m, the resulting relative strength will always be at most  <dig> 

furthermore, since snr values are non-negative, the relative strength is also non-negative. note that a high relative strength implies that we expect a similar performance when using fn, m' as when using fn, m. it does not imply that this performance is high per se.

classifiers
in order to investigate whether the impact of variability is classifier specific, we employed a broad range of classifiers, being the nearest centroid  classifier, k-nearest neighbors  with k ∈ { <dig>  3}, a support vector machine  with a linear kernel  and radial basis function kernel , and the random forest  classifier. for descriptions of the individual methods, see  <cit> . the nc and k-nn used a cosine based distance function . all svm results were obtained using the r package e <dig> and for each feature set a grid search was performed to find the best hyperparameter values. classification results for rf were obtained using the r package randomforest. further details are presented in additional file  <dig> 

computing environment
although our proposed protocol is conceptually quite simple, it is computationally demanding to obtain results, since for each dataset, preprocessing method, split and signature size, a classifier needs to be trained and validated. in addition, performing perturbation experiments for certain classifiers such as nearest neighbors can be time consuming as well. one benefit of our protocol is that it lends itself well to parallelization. in order to perform our computations we used a grid with over  <dig> cores, divided over  <dig> dell poweredge blade servers, each with  <dig> intel xeon l <dig> quadcore cpu's, with 16gib fdb dual rank memory. all computations were performed using r  <cit>  and bioconductor  <cit> .

RESULTS
the aim of our work is to get a comprehensive overview of the impact of feature variability on microarray breast cancer classification. we will operate under the null hypothesis that preprocessing and perturbation variability have no effect on feature selection and classification. under this null hypothesis we expect that for different preprocessing methods or for perturbed versions of a dataset we 1) typically select the same features, 2) obtain identical class assignments and as a consequence 3) obtain overlapping p-curves and 4) obtain d-curves that are flat and close to zero. in addition, we expect to 5) obtain s-curves that are flat and close to zero as well. we first report our results of studying the impact of perturbation and preprocessing variability on feature selection, before moving on to their influence on classification.

impact of feature variability on feature selection
in this paper we focus on compact gene signatures. unfortunately, feature selection on high-dimensional datasets, like the ones associated with microarray-based expression profiling, is typically unstable as different subsets of samples frequently lead to the identification of different feature sets  <cit> . from a classification perspective, such a difference does not necessarily signal a problem, as long as the performances of the sets are similar, although from a biological perspective it makes reasoning about the data much more challenging.

it has been observed that the impact of preprocessing strategies on differential expression detection is high  <cit> . note that feature selection strategies in microarray literature are often based on univariate ranking strategies, e.g. based on snr-statistics or t-tests  <cit> . one would expect that genes that are strongly differentially expressed are also highly ranked by univariate selection procedures and hence that feature selection is also influenced by feature variability. in this section we show several examples of the influence of perturbation and preprocessing variability on signature composition, i.e. feature selection. for the rosetta data it was not possible to assess the influence of preprocessing variability, as for this dataset only processed data is publicly available.

van 't veer breast cancer signature composition is sensitive to perturbation variability
as a first example, consider the feature selection step used to identify the 70-gene breast cancer signature by van 't veer et al.  <cit> . this signature is comprised of the top- <dig> genes with an absolute pearson correlation coefficient with the class label  larger than  <dig>  as obtained from the  <dig> training samples of van 't veer. note that the computation of correlation coefficients can be very sensitive to the presence of outliers. to test the sensitivity of this feature selection step, we created  <dig> perturbed instances of the training set, using the rosetta uncertainty estimates  and recomputed the pearson correlation coefficients.

high impact of perturbation variability on feature rankings for affymetrix datasets
in the previous example, the composition of the signature was given. in practice, however, the identification of a suitable set of marker genes is part of the discovery process. our protocol, similarly to the protocol suggested in  <cit> , employs a signal-to-noise ratio based ranking on each training split, in order to identify useful features for signature construction. this implies that the composition of our signatures is fully determined by the outcome of the ranking step and independent of the classifier used.

we examine the overlap between snr-based rankings obtained using an unperturbed and a perturbed version of a dataset. let fn, m, k denote the top-n ranked genes, using data from preprocessing method m, for split k and let  be the ranking obtained using a perturbed version. although a complete overlap between these lists is preferable, we would at least hope to find a substantial part of the top half of one list in the other list. how large these parts are, is shown in figure  <dig> 

for most preprocessing methods the impact of perturbation noise appears to be large. although the overlap increases when signature size increases, the overlap between a ranking based on unperturbed data and one based on perturbed expression data is generally less than 50%. in the desmedt dataset there were two genes that almost always appeared at top of the snr rankings in each split, which is the reason of the shape irregularity seen in the  overlap curves for the study by desmedt. for rma, the overlap between rankings based on unperturbed and perturbed versions is much larger, with overlaps between  <dig> and 90%. in comparison to the other preprocessing methods rma appears to give lower estimates on the measurement errors, although on the basis of our data one cannot tell if rma underestimates the errors or if the other methods typically overestimate the errors.

note that a lack in overlap does not necessarily signal a problem if the selected feature sets are of equal strength. although the overlap for most preprocessing methods is quite low, the related relative strengths  are still high, with values of over 80% for most preprocessing schemes and values of over 95% for rma, indicating that the performance for signatures based on the different rankings is expected to be comparable. a similar observation was made in  <cit> , which for instance shows that on the van 't veer data the performance of the second best  <dig> genes was very comparable to the performance achieved by selecting the top- <dig> genes. note that the latter two lists by construction have an overlap of zero. for some datasets many equally performing signatures exist, as was also noted in  <cit> .

affymetrix breast cancer signature composition is sensitive to preprocessing variability
here we inspect the overlap between top-ranked feature lists, as obtained using different preprocessing methods i.e. we consider preprocessing variability. consider two top-ranked feature lists, based on two different preprocessing schemes m and m', say of size  <dig>  i.e. f <dig>  m and f <dig>  m'. similarly to the example in the previous section, we would hope to find a substantial part of the top half of one list in the other list. figure 4a shows the overlap of the top- <dig> of one list in the top- <dig> of the other.

different preprocessing strategies give rise to the selection of different features as well, as for all preprocessing pairs again none have a complete overlap. within the same preprocessing family, i.e. mgmos and mmgmos, the overlap is high, although for the dataset of loi there is already quite a discrepancy. for the remaining pairs we see that the overlap between top-ranked feature lists can be quite low. the overlap between different preprocessing families for the various datasets lies between  <dig> and 80%. the highest overlap between methods from different families was found between rankings based on dchip and rma, with a median overlap of 70% over six datasets. the overlap between rma and mas is lower, with a median of only 56% over all six datasets. from the last block, we can see that even though dchip and mmgmos are both multi-chip preprocessing strategies, they usually tend to pick different feature sets, with a median of 44% over six studies. excluding the  pair, the median overlap over all data sets and splits is 52%. note that this lack in overlap is completely due to the preprocessing method chosen, as the feature selection criterion, the array platform, and the set of samples  are all identical. comparing the overlap from figure 4a  to that in figure  <dig>  we see that the scores have a similar range, i.e. around 50%.

relative strength of affymetrix-based breast cancer signatures is more robust against preprocessing variability
in the previous section, we saw that the use of a different preprocessing strategy typically leads to the identification of a different feature set and that the overlap between top-ranked feature sets for different preprocessing pairs can be quite poor. figure 4b shows the distribution of the relative strengths for top-ranked feature lists from the example in figure 4a. the order of the boxplots in panel b is the same as in panel a. comparing the two panels, we see that a lower overlap is typically associated with a lower relative strength as well. however, although the overlap between top-ranked features sets can be quite poor, the relative strengths are reasonably high. the highest scores are again obtained between preprocessing pairs from the same family. since the mgmos models have a large overlap in top-ranked lists, their relative strengths are high as well, with values of over 90%. even for the loi dataset, the median relative strength over  <dig> splits is still above 89%, while the actual overlap is quite poor with a median of 60%. furthermore, distributions of relative strengths for the minn dataset, for pairs of preprocessing strategies from different families , are mostly wider and have a lower tail than the other distributions. this is probably caused by the small number of samples in the minn dataset. comparing the relative strengths from figure 4b  to those in figure  <dig>  we see that the scores are similar, with a mean relative strength of  <dig> % taken over all entries in figure 4b to a mean relative strength of  <dig> % taken over all entries corresponding to figure  <dig> at n =  <dig> 

preprocessing-neutral top gene lists
the lack of overlap between top-ranked lists corresponding to different preprocessing methods, as observed in figure 4a, presents an additional complication in comparing performances between signatures based on such lists as we then cannot know whether a difference in performance is due to a difference in selected features, or due to a difference in feature values as obtained from the preprocessing method. in order to compare the performances of signatures constructed on data from different methods, ideally we would like to use the same set of features. here, we show that we can obtain a ranking with a high relative strength over all preprocessing methods by combining the ranking information associated with the different preprocessing methods. in the previous sections, each top-ranked feature set was based on data from a single preprocessing method. for a given method m ∈ m we will refer to this ranking as a single-rank feature list. the strength of a feature i for method m, denoted by sm, was measured by snri, m. here we base the strength of feature i on the average of the individual strengths, as obtained by the different preprocessing methods in m, i.e., we use a strength   

in the remainder we will refer to the ranking based on this combined strength s as a multi-rank list. for each split k and for each dataset, we computed the top- <dig> ranked feature list based on this multi-rank strategy and determined its relative strength in the top-rank list f <dig>  m, k for each preprocessing method m. figure 4c gives for each dataset the distribution of these relative strengths. relative strengths of the multi-ranked lists are high, with a median score of over 90% for all datasets. in order to decouple the effect of feature selection from the impact of perturbation and preprocessing variability on classification performance, we will therefore mainly use multi-rank gene lists, although all experiments on the affymetrix datasets were also performed using the single-rank lists.

impact of feature variability on classification
we start our investigation of the effects of feature variability on classification by taking an in-depth look into the van 't veer  <cit>  and van de vijver  <cit>  expression data, which is based on the rosetta error model. starting from a single split of the data and using only features as considered in the original publications, we progress towards a more sophisticated setting, ending up in using the full sensitivity analysis protocol and applying it on all affymetrix datasets using multiple preprocessing strategies and multiple classifiers.

van 't veer signature is sensitive to perturbation variability
we investigated the classification stability of the original 70-gene signature of van 't veer et al.  <cit> . the classifier used for the construction of their signature is a nearest centroid classifier. classification for this classifier can be linked to a discriminant score , by which we assign a sample to the good prognosis class if the discriminant score is positive, and to the poor prognosis class otherwise. we use the original van 't veer training set of  <dig> samples to estimate the class centroids. as a validation set we took the  <dig> remaining samples in the rosetta dataset. next, using the uncertainty information estimated by the rosetta error model , we created  <dig> perturbed versions of the validation set and classified these with the classifier built on the original training data.

a map-matrix example for the rosetta dataset
now we extend the example of the previous section by considering a sequence of  <dig> signatures constructed using the top- <dig> ranked features from the van 't veer data and zoom in on the impact of perturbation variability on classifications of individual samples from the rosetta data by taking an in-depth look at a map-matrix, such as the ones obtained from our sensitivity analysis protocol. classifications are again performed using the nearest centroid classifier. the nth signature is constructed using only the top-n features. note that this setting is similar to our protocol, in which at step  <dig> we take the  <dig> training cases of van 't veer data as a training set, the  <dig> remaining samples as a validation set, at step  <dig> take the top- <dig> features as described above and at step  <dig> train a sequence of  <dig> nc classifiers, thus yielding  <dig> signatures. following the protocol, at step  <dig> we obtain a map-matrix, which in this case is a  <dig> by  <dig> matrix, where the entry at row k and column n contains the map-score of sample k using a signature involving the top-n features.

performance and stability curves for the rosetta dataset
in the previous section, results were obtained using only a single split of the data in a training and validation set. here we apply the full sensitivity analysis protocol to the rosetta dataset consisting of  <dig> samples. figure  <dig> shows the resulting performance  and stability  curves for five classifiers, based on  <dig> splits of the data. the nc classifier performs best and clearly increases its performance when using more features with a highest performance of around 65%. this is comparable to the estimates reported in  <cit> . furthermore, on this dataset the nc classifier also had the best s-curve. s-curves generally improve when using more features, however, none are flat and close to zero, indicating that perturbation variability can consistently disrupt these classifications. for the nc classifier the impact of perturbation variability on this dataset quickly diminishes, with an average number of unstable assignments leveling off around only  <dig> % at a signature size of  <dig>  for other classifiers we see that the impact of perturbation variability is higher than for the nc classifier and especially the 1-nearest neighbor seemed very sensitive at small signature sizes, only leveling off around 10% at a size of  <dig> features. although stability is a desirable characteristic, we should not directly link ascending p-curves to descending s-curves and simply attribute the higher performance of the nc classifier to perceived noise tolerance. although the s-curves typically decrease when the signature size increases, the p-curve does not generally show such a monotonic behavior. for instance, the nearest neighbor classifier shows a decreasing p-curve for larger signature sizes and is indeed known to be intolerant to the inclusion of irrelevant features.

impact of feature variability for affymetrix datasets
in order to investigate the impact of both preprocessing variability and perturbation variability on affymetrix genechip data, we ran our complete protocol, for each of the six affymetrix datasets, five different preprocessing methods , and six classifiers . each dataset was analyzed with  <dig> different splits into a training and validation set. each validation set was perturbed  <dig> times in order to infer the s-curves. furthermore, the experiments were performed using both the single-rank and multi-rank sets.

from figure 4a we saw that different preprocessing methods tend to pick different features and that the overlap between rankings can be low. hence, if we use single-rank sets, we will observe a combined effect where differences between curves corresponding to different preprocessing methods can be due to a difference in signature composition, as well as due to feature variability. the advantage of the multi-rank approach is that for a given dataset-classifier pair, observed differences in performance, discordance , and stability curves are not due to a difference in signature composition, but solely due to feature variability. using a signature based on a multi-rank set effectively decouples the impact of feature selection from the effect of feature variability on classification performance. given the high relative strengths of the multi-rank sets, as observed in figure 4c, we therefore show in the main text only the figures corresponding to these multi-rank sets. figures  <dig>   <dig> and  <dig> show the resulting p, d and s-curves, respectively, for all  <dig> classifier-dataset combinations. the corresponding p, d and s-curves for the single-rank sets are shown in additional files  <dig>   <dig> and  <dig>  respectively.

lack of overlap in performance curves on affymetrix datasets
different preprocessing methods produce discordant class assignments
in the previous section we observed that in several studies there was a lack in overlap between performance curves for different preprocessing methods, clearly indicating a discordance in outcome prediction. even in the case of overlapping performance curves, however, one cannot ascertain that the individual class assignments are concordant. figure  <dig> shows for several classifiers  the discordance curves corresponding to the p-curves of figure  <dig>  for all preprocessing pairs clear discrepancies can be seen, which is in direct disagreement with our null hypothesis. the highest d-curves for the selected classifiers are observed for the 3nn and svmlin classifier, with an overall median discordance  of  <dig> % and  <dig> %, respectively. the nc and rf classifiers show lower numbers of discordant class assignments with an overall median discordance of  <dig> % and  <dig> %, respectively. for the latter two classifiers the discordance also clearly decreases with larger signature sizes, leveling off at a signature size of  <dig> with an overall median discordance of  <dig> % and  <dig> %, respectively. the discordance is often larger in the poor prognosis group than in the good prognosis group . note that in most breast cancer datasets, the former group is also much smaller than the latter. when using balanced performance indicators, a discordance in the poor prognosis group is then more heavily penalized than a discordance in the good prognosis group. for instance, in figure  <dig> a clear difference in performance curves can be seen for the preprocessing pair , when applying the rf classifier on the pawitan dataset. from figure  <dig>  however, the lack in concordance for the preprocessing pair  does not seem much larger than on other datasets. from additional file  <dig>  we can see that for this preprocessing pair and dataset the number of discordant cases for the rf classifier in the poor prognosis group is indeed higher, with an overall median of  <dig> % compared to an overall median of  <dig> % on the remaining datasets.

high impact of perturbation variability for small signature sizes on affymetrix datasets
discussion
finding high-quality stable biomarkers in breast cancer applications using microarray expression profiling has proven to be quite challenging with reported balanced accuracy rates for most breast cancer signatures somewhere between  <dig> and 70%. signature composition strongly depends on the subset of patient samples used for feature selection  <cit> . furthermore, a high level of inconsistency between individual class assignments between different signatures has recently been reported  <cit> . differences in array platforms as well as biological variability have been conjectured to play a major role in these discrepancies.

we designed an experimental protocol to evaluate the impact of two other types of variability, namely preprocessing and perturbation variability, on signature composition and classification. for this purpose several state of the art and frequently applied preprocessing methods were selected. complementary to ein-dor et al.  <cit> , we showed that signature composition is strongly influenced by perturbation variability and preprocessing variability, even if the array platform and the stratification of patient samples are identical. in addition, using our multi-rank feature sets we showed that there is often a high level of discordance between individual class assignments for signatures constructed on data coming from different preprocessing schemes, even if the actual signature composition is identical. for the single-rank feature sets, the observed discrepancies were even larger. no preprocessing scheme, however, yielded data that was clearly superior for classification purposes. when comparing preprocessing variability to perturbation variability, we found their impact on feature selection to be equally strong. on classification, however, the impact of preprocessing variability often remained strong with increasing signature size, whereas the impact of perturbation variability quickly diminished.

preprocessing noise is mainly caused by different underlying assumptions that are made on the data and on the available sources of information that are used. some methods deliberately ignore some sources of information or exclude certain steps. rma, for instance, does not use mismatch probe information to infer expression levels, while standard applications of dchip do not perform a background correction step. note that the latter can have great implications on the final expression data for both one  <cit>  and two-color array data  <cit> .

our stability analysis performs explicit noise tolerance tests for a diverse set of classification routines, by using the class assignments of perturbed expression profiles. the results indicate that all classifiers considered were sensitive to perturbation variability, although the impact was much stronger at small signature sizes and quickly diminished for larger signature sizes. furthermore, in most cases we found the level of noise tolerance for the nc, svmrbf, and rf classifiers to be very comparable.

we chose to use realistic estimates of gene-wise measurement error in the stability analyses. methods like the rosetta error model, but also the mgmos and mmgmos models, are specifically designed to obtain such uncertainty information associated with the fitted expression data. methods like dchip, rma, and mas  <dig>  are not designed with this goal in mind. however, some uncertainty information can be derived from the summarization step, as performed in the preprocessing cascade. although the uncertainty estimates for dchip, rma and mas are based on the same type of information, we found that perturbations corresponding to rma seemed much less severe than those based on other methods; cf  <cit> . for the affymetrix preprocessing methods a potential problem with basing uncertainty estimates solely on the summarization step is that most probesets consist of a small number of probes, with a median size of  <dig> for the genechips used here, which can make the standard error estimates less reliable. although the stability curves for mas and dchip were closer to those of the mgmos and mmgmos models, from our experiments one cannot tell if rma underestimates the errors or if the other methods overestimate the errors.

our results also show that a high stability and a good performance do not always go hand in hand . although stability is a desirable property, it is sometimes conflicting with achieving a high performance, which presents us with a dilemma, similar to the bias-variance dilemma  <cit> . to this end, consider figure  <dig>  from a classification standpoint, the second scenario is obviously the preferred scenario, while scenario three is equal to tossing a coin. note that scenario one can always be achieved by using a rule that assigns all samples to the same class. such a rule is extremely stable, yet when using balanced accuracy rates, will also have a poor performance. this scenario was sometimes observed for the svm classifiers. for both linear and non-linear svms, parameter estimation was hard. this might be an explanation for the observed poor performance, although our performance estimates on single datasets for svm were often comparable to those reported earlier  <cit> . finally, scenario four would be a strong indicator that the perturbed expression profiles are not very realistic, given the fact that performance and stability are both measured on the same validation data. this scenario was, however, not observed in our experiments. we did encounter this scenario in attempts to base perturbed expression profiles on jitter i.e. artificial noise estimates. the main problem in using jitter is that such estimates are either much too low or much too high and therefore this type of perturbation was not further pursued here.

note that our goal was not to compare classifiers or even to find optimal biomarkers per se and it is likely that the performance of some classifiers can be further improved e.g. by changing the feature selection step in our protocol, which in our case was based on univariate signal-to-noise-ratio statistics. alternative univariate ranking strategies such as those based on the t-test, mann-whitney u-test, and mahalanobis distance were reported to perform similarly  <cit>  and were therefore not pursued here. note that the former methods all construct rankings based on binary class-label information. survival information on which the class labels are based could be incorporated in the ranking step as well. for instance, in  <cit>  a 76-breast cancer gene signature was derived using a ranking step based on information from univariate cox proportional-hazards regression models using the length of distant metastasis free survival.

for some classifiers it might be advantageous to resort to multivariate wrapper-based feature selection methods. perhaps the simplest computationally efficient multivariate wrapper is the top-scoring-pair  classifier  <cit> , which performs its classifications on the basis of the expression values of just two genes. on several classical tumor data sets e.g. leukemia  <cit> , colon  <cit> , lymphoma  <cit> , and prostate  <cit> , the tsp was able to obtain balanced accuracy rates well over 90%; see  <cit> . in  <cit>  the tsp is claimed to be invariant to pre-processing changes, as it is invariant to any monotonic transformation of the expression data. although our forms of feature variability are very realistic, they can certainly not be considered as monotonic transformations of the raw expression data. initial experiments with the tsp on the affymetrix breast cancer datasets revealed that the classifier was extremely sensitive to feature variability, with corresponding balanced accuracy rates often close to 50% . on the rosetta data a similar observation on the performance of the tsp was reported in  <cit> . as the tsp uses only two genes, these results are in agreement with our observation that breast cancer signatures comprised of few genes seem very susceptible to feature variability. in addition, in  <cit>  several alternative multivariate approaches were benchmarked, with the main conclusion that multivariate variate selection approaches often do not lead to consistently better results than univariate approaches. moreover, compared to multivariate approaches, univariate ranking procedures have the benefit of a considerable computational speed up, which in our case was very important considering the large number of experiments performed.

our sensitivity analysis was performed on a sizable collection of patient sample hybridizations and in a breast cancer classification context, which is different from the small scale spike-in and dilution studies on which most previous microarray sensitivity analyses were performed  <cit> . one advantage of the latter two types of studies is that the ground truth is known, which for most breast cancer studies is less obvious. in our framework, however, under the null hypothesis we also know exactly what should be expected, i.e. for different preprocessing methods or for perturbed versions of a dataset we should have selected the same features, had overlapping p-curves and obtained d-curves and s-curves that were zero for all signature sizes, as stated in our null hypothesis. based on the outcome of our experiments, however, we conclude that this is not the case, and hence we conclude that in microarray breast cancer studies feature variability can have a strong impact on both feature selection and classification. we conjecture feature variability to be less of an issue in microarray studies for which a high performance can be obtained such as for the classical tumor datasets mentioned above. note that these studies all deal with tissue-type recognition problems, which are considerably easier classification problems than event prediction studies, such as the breast cancer studies treated here; see also  <cit> .

finally, the focus of this paper has been of a descriptive nature, analyzing the impact of feature variability. obviously, one would next like to enhance the performance and stability of classifiers by exploiting the feature variability information. for instance, in the context of point injection techniques, one can use the perturbed expression profiles as additional candidates to be injected, instead of the rather artificial candidates obtained by linear interpolation  <cit> . another avenue that one may take is to directly increase classification concordance by explicitly enforcing it, for instance in a wrapper framework.

CONCLUSIONS
we performed an extensive sensitivity analysis of microarray breast cancer classification under feature variability. our results indicate that signature composition is strongly influenced by preprocessing variability and perturbation variability, even if the array platform and the stratification of patient samples are identical. in addition, we show that there is often a high level of discordance between individual class assignments for signatures constructed on data coming from different preprocessing schemes, even if the actual signature composition is identical.

we presented evidence of discrepancies induced by technical variation that cannot be considered negligible, as previously claimed by some researchers  <cit> . we therefore strongly recommend that feature variability is taken into account during the construction of a signature, especially when using microarray technology for the classification of individual patients. in addition, measures should be taken to minimize the technical variation of microarray procedures when used for such high impact applications as cancer diagnostics.

authors' contributions
hmjs drafted the manuscript, designed the experiments, created the software implementations and finalized the manuscript. pdm co-designed the experiments, helped develop the software implementations, and drafted the manuscript. rvdh provided expert advice on microarray technology and helped draft the manuscript. mjtr critically reviewed the manuscript. wfjv instigated and guided the research project, co-designed the experiments and helped draft the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
overview of  <dig> affymetrix hybridizations. the column datasetname indicates to what study each hybridization corresponds. for each study the repository and corresponding accession number can be found in table  <dig> in the main text. the column filename indicates the exact file name for each hybridization as used in the corresponding repository. for the datasets of desmedt, minn, loi and chin, the class label was based on the time of distant metastasis free survival  and corresponding event indicator e.dmfs. for the datasets of miller and pawitan, the class label was based on the time of breast cancer specific overall survival  and corresponding event indicator e.sos.

click here for file

 additional file 2
overview of additional  <dig> hybridizations from the van de vijver. the column sampleid indicates the identifiers as used by van de vijver et al.  <cit> . the selected hybridizations represent all lymph-node negative cases that were not yet contained in the original publication by van 't veer et al.  <cit> .

click here for file

 additional file 3
supplementary information on perturbation schemes and classifiers. the file contains additional information on how to construct perturbed expression profiles for mas <dig> , dchip and the rosetta data. in addition, information on parameter settings for the svm and rf classifiers is provided.

click here for file

 additional file 4
performance curves for the affymetrix datasets using the single-rank feature sets. rows represent different preprocessing pairs, while columns represent curves for different datasets. within each cell, performance curves corresponding to different classifiers are shown in separate colors. the color scheme is shown at the bottom of the figure. within a cell the x-axis provides the signature size, while the y-axis gives the average balanced accuracy over  <dig> splits. for each dataset and split, the top- <dig> feature set was computed using the single-rank strategy and this ranking was subsequently used for all classifiers in order to construct signatures.

click here for file

 additional file 5
discordance curves for the affymetrix datasets using the single-rank feature sets. rows represent different preprocessing pairs, while columns represent curves for different datasets. within each cell, discordance curves corresponding to different classifiers are shown in separate colors. the color scheme is shown at the bottom of the figure. within a cell the x-axis provides the signature size, while the y-axis gives the average percentage of cases, over  <dig> splits, of inconsistent class assignments on the unperturbed validation sets. for each dataset and split, the top- <dig> feature set was computed using the single-rank strategy and this ranking was subsequently used for all classifiers in order to construct signatures.

click here for file

 additional file 6
stability curves for the affymetrix datasets using the single-rank feature sets. rows represent curves obtained using different classifiers, while columns represent curves for different datasets. within each cell, stability curves associated with different preprocessing methods are shown in separate colors. the color scheme is shown at the bottom of the figure. within a cell the x-axis provides the signature size, while the y-axis gives the average percentage of cases over  <dig> splits with a map-score larger than  <dig>  for each dataset and split, the top- <dig> feature set was computed using the single-rank strategy and this ranking was subsequently used for all classifiers in order to construct signatures.

click here for file

 additional file 7
discordance curves for the poor prognosis cases in the affymetrix datasets using the multi-rank feature sets. rows represent different preprocessing pairs, while columns represent curves for different datasets. within each cell, discordance curves corresponding to different classifiers are shown in separate colors. the color scheme is shown at the bottom of the figure. within a cell the x-axis provides the signature size, while the y-axis gives the average percentage of cases, over  <dig> splits, of inconsistent class assignments on the unperturbed validation sets. for each dataset and split, the top- <dig> feature set was computed using the multi-rank strategy and this ranking was subsequently used for all classifiers in order to construct signatures.

click here for file

 additional file 8
discordance curves for the good prognosis cases in the affymetrix datasets using the multi-rank feature sets. rows represent different preprocessing pairs, while columns represent curves for different datasets. within each cell, discordance curves corresponding to different classifiers are shown in separate colors. the color scheme is shown at the bottom of the figure. within a cell the x-axis provides the signature size, while the y-axis gives the average percentage of cases, over  <dig> splits, of inconsistent class assignments on the unperturbed validation sets. for each dataset and split, the top- <dig> feature set was computed using the multi-rank strategy and this ranking was subsequently used for all classifiers in order to construct signatures.

click here for file

 acknowledgements
we would like to thank ronald van driel, serge vrijaldenhoven, donie collins and jurgen rusch for extensive computer support and for enabling excellent grid computing facilities. we thank evgeny verbitskiy for statistical advice and nevenka dimitrova and emile aarts for feedback on earlier drafts of this manuscript and for their overall involvement in this work. furthermore, we thank martin van vliet and fabien reyal for providing detailed information on the affymetrix genechip data as used in the compendium. finally, we thank richard pearson for assistance on the puma package.
