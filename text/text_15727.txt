BACKGROUND
the papara tool
 <cit>  implements a new method for aligning a—typically—large number of short sequence reads against a reference multiple sequence alignment  and a corresponding phylogenetic tree. hmmalign
 <cit>  can also be used to accomplish this task. with certain limitations, programs for de novo msa such as muscle
 <cit>  and mafft
 <cit>  can also be deployed for this purpose. however, hmmalign, muscle, and mafft align short sequence reads against a single, monolithic profile that is derived from the reference msa without taking into account the corresponding phylogeny. papara takes the phylogeny into account by calculating multiple profiles that are obtained from the phylogeny induced by the reference msa. the short reads are aligned against each of these phylogeny-aware profiles and the best alignment for each short read is kept. since a large number of pairwise alignments are computed  is aligned against every edge of the reference tree ), this operation dominates the runtimes of papara. note that all pairwise alignment operations can be carried out independently. hence, the algorithm exhibits a large degree of parallelism.

a characteristic property of papara and hmmalign is that they use dynamic programming algorithms for the alignment step. dynamic programming alignment algorithms generally exhibit a time complexity of o for aligning two sequences of length m and n against each other. this can become a limiting factor when either two long sequences or a large number of sequences are aligned. therefore, optimization and acceleration efforts typically focus on optimizing these dynamic programming kernels. because of its generality and importance, optimization efforts have so far mainly been undertaken for the smith-waterman algorithm . because of the analogies between the swa and papara kernels, we briefly survey swa optimization efforts.

there exists extensive literature on vectorizing swa with simd instructions on general purpose cpus:
 <cit>  for the intel i <dig> 
 <cit>  for the sun ultra sparc, and
 <cit>  for intel x <dig> cpus. the above implementations deploy fundamentally different techniques for vectorizing the algorithm:
 <cit>  deploy simd instructions to speed up the pairwise alignment of two sequences at a fine-grain level. they exploit the data parallelism in the calculations of a single dynamic programming matrix . however, the inherent wavefront parallelism limits the parallel efficiency of these approaches, which motivated the introduction of increasingly sophisticated methods for alleviating these limitations. other implementations use a fundamentally different approach. they simultaneously compute multiple pairwise sequence alignments intead of vectorizing individual pairwise alignment computations. in
 <cit> , a 64-bit special purpose register is divided into four parts for simultaneously aligning a single sequence against four other sequences. this represents a straightforward application of data parallelism, generally referred to as inter-task parallelism. in other words, the basic alignment algorithm is executed sequentially but is applied simultaneously to multiple data. the authors obtained a 6-fold speedup over the sequential implementation. recently, rognes introduced swipe
 <cit> , a highly optimized inter-task vectorization approach for modern x <dig> architectures that uses sse instructions and achieves a speedup of up to  <dig> over the fastest intra-task sse vectorization
 <cit> .

furthermore, several approaches have already been assessed for accelerating the swa on gpus. initial efforts used opengl
 <cit> . later implementations, after the introduction of cuda, led to the development of sw-cuda
 <cit> , cudasw++
 <cit> , and cudasw++ <dig> 
 <cit> . according to
 <cit> , the most efficient cpu and gpu implementations yield comparable performance on current state-of-the-art hardware .

finally, we recently introduced a fpga implementation of an earlier version of the papara alignment algorithm
 <cit> . this hardware architecture deploys intra-task parallelism and exploits the data dependencies in this early alignment kernel version. although the techniques employed on the fpga can not be directly applied to the current version of the algorithm, we obtained a speedup of two orders of magnitude.

performance comparisons between gpu and cpu kernels are generally difficult and debatable. in order to obtain an as fair as possible performance assessment, we deploy what we term the “competing programmer approach”. as in previous work on using accelerators  for computing the phylogenetic parsimony kernel
 <cit> , sb explicitly worked on obtaining the best possible x <dig> performance and na competed with sb to obtain the best possible gpu performance. by investing a comparable amount of optimization effort and man-hours into the x <dig> and gpu implementations, we hope to obtain a more realistic performance evaluation for these architectures. provided the fast x <dig> and gpu implementations, we can also address the problem of how to optimally use all available computational resources on a representative modern desktop to accelerate papara. despite the analogies to swa, the papara dynamic programming algorithm exhibits specific challenges that are associated with the fact that our alignment model also incorporates the evolutionary signal of the phylogenetic tree. despite the fact that the simt version of the algorithm was tested on nvidia hardware, we chose to use the industry standard opencl instead of cuda. while cuda is better adapted to the specifics of nvidia hardware and can potentially offer better performance, opencl allows for using the same code on different gpus . moreover, opencl can also be used on general-purpose multi-core systems, which is particularly convenient for testing and debugging.

methods
the papara algorithm
in a phylogenetic tree, known sequences of living species  are located at the tips. the inner nodes of the tree represent hypothetical common ancestors of these species. since the actual sequences of the ancestors are unknown, different methods for accommodating the uncertainty of ancestral states have been introduced in the context of scoring criteria for phylogenetic trees. in papara, ancestral states are obtained via maximum parsimony , a widely used optimality criterion for phylogenetic inference. based on a fixed, given reference msa , the key idea is to find the phylogenetic tree which explains the data  by the least number of mutations. the ancestral state vectors are calculated using sankoff’s algorithm
 <cit> . every edge  b of the reference tree  can be represented by a parsimony state vector
ab=ab <dig> .…,abn, where each
abi represents the parsimony state of the ra at site i . each entry
abi is encoded as a bit vector. for dna data, each bit corresponds to one of the four dna characters: a , c , g  and t . this representation differs from the simpler case of pairwise sequence alignment  where each element of both input sequences represents exactly one character and not potential, alternative character states. in papara, an individual
abi entry can either be an a, c, g, or t, or any combination thereof . this representation is used to encode the uncertainty of an ancestral sequence state. thus, using these bit vectors is analogous to ambiguous character representations . the bit vectors can also be used for other input data types. the current cpu versions of papara also support protein data for instance. as mentioned before, the qs are aligned against all ancestral states derived from the edges of the rt. at each edge an additional node  is inserted, for which an ancestral state vector is computed. when the ancestral state vector has been calculated, the virtual root is removed again and inserted into another edge. in conjunction with this ancestral state vector, papara uses an additional signal which provides information about the gap  distribution in the rt. for this purpose, we use a supplementary flag  at each site i. this flag is used to appropriately adapt  the scoring function of the dynamic programming algorithm to the indel pattern as encoded in the rt and ra. the cgap signal is calculated along with each ancestral state vector at each edge, based on the rules described in
 <cit> . the scoring function of the dynamic programming algorithm is provided in equation  <dig>  note that the default gap and mismatch penalties are different from the default values reported in
 <cit> . equation  <dig> recursively defines the score of the dynamic programming matrix cell si,j in column i and row j for aligning site
abi of the ancestral state vector against site bj in the qs. 

 cgi=3if cgap is set for sitei0otherwise=ifcgi=0otherwisemi,j=0ifaiandbjmatch3otherwiseii,j=si,j−1+ <dig> 

 di,j=minsi− <dig> j+gpoeidi− <dig> j+gpei 

  si,j=minsi− <dig> j−1+mi,j+cgidi,jii,j 

the papara algorithm has two phases: 

• scoring phase, during which scores are calculated for each qs/ancestral state pair using equation  <dig>  the algorithm keeps track of the currently best  alignment score and the corresponding qs/ancestral pair for each qs.

• alignment phase, during which the actual alignments are created  for the best-scoring qs/ancestral state pairs.

evidently, the scoring phase accounts for the largest part of overall runtime. the scoring phase carries out r ∗ q pairwise alignments for q query sequences and r ancestral state vectors, while in the alignment phase only the best q alignments are created . therefore, our optimization efforts focused on the scoring phase. note that there are no data dependencies between the r ∗ q pairwise alignments. thus, they can easily be calculated in parallel on a multi-core platform.

opencl programming model
opencl  is an open standard for parallel programming of heterogeneous systems. an opencl application typically runs on a host cpu and one or more gpus. the opencl architecture is similar to nvidia’s cuda , which represents an extension of c/c++ for writing scalable codes on simt architectures .

a cuda device consists of several streaming multiprocessors  which correspond to the opencl compute unit terminology. the opencl work-items and work-groups correspond to the cuda thread and thread block concepts. in analogy to cuda applications, opencl applications consist of a host program which is executed on the cpu and one or more kernel functions that are offloaded to the gpu. the kernel code executes work-items/threads sequentially and work-groups/thread blocks in parallel. since threads are organized into thread blocks, thread blocks are in turn organized in grids of thread blocks. an entire kernel is executed by such a grid of thread blocks.

opencl applications can access various types of memory: global, local, shared, constant, and texture. the global memory resides in the device memory  and is accessed via 32-, 64-, or 128-byte transactions. global memory is allocated on a per-application basis and can be accessed by all work-items and work-groups. to maximize global memory throughput, it is essential to maximize memory coalescence and minimize address scatter. cuda local memory accesses occur only for some automatic variables that the compiler places in local memory, e.g., large structures or variables that do not fit into the kernel’s register space. since cuda local memory resides in device memory, local memory accesses exhibit identical high latency and low bandwidth as global memory accesses. shared memory resides on the chip and is therefore substantially faster than local and/or global memory. in opencl, local memory is located in shared memory and can be accessed on a per-thread basis, while shared memory can be accessed on a per-block basis. therefore, as long as there are not bank conflicts, using shared memory allows for attaining high memory bandwidth. finally, constant and texture memory reside in device memory and are cached.

inter-reference memory organization
an inter-reference memory organization model, similar to the one described in
 <cit> , is deployed for both the simd as well as the simt implementations. the inter-reference organization facilitates the vectorization of the code on the simd  architecture, while on the simt architecture it allows for coalesced global memory accesses and thereby high device throughput. in figure
 <dig> we illustrate this generic inter-reference memory organization approach. a certain number w   of reference sequences  is organized in one large array, the ‘inter-reference vector’, that consists of consecutive groups of elements. each group contains all r elements for one specific position/index of the rs. the elements of a group are placed contiguously and all groups are placed sequentially in memory. in other words, the group containing the i + 1th elements follows the group containing the ith elements. the group size  varies for the simd and simt architectures. a similar organization is used to store the dynamic programming matrix d : each entry di,j consists of a group of w  elements, while the groups in di,j and di +  <dig> j occupy consecutive memory locations. the group width  of the simd implementation depends on the selected integer data type and the x <dig> target architecture . for simt architectures, group sizes are multiples of  <dig> unsigned integers to ensure that the global memory is accessed in chunks of  <dig> bytes. moreover, all memory transactions are aligned automatically . the actual number of reference elements that a group contains is either  <dig>  or larger, depending on the compression of the reference entries .

simd vectorization
as mentioned above, the r ∗ q independent alignment operations of the papara scoring phase can easily be distributed to multiple cores using threads. this corresponds to a mimd  parallelization scheme, which scales linearly with the number of cores. further performance improvements can be obtained by exploiting the capability of modern cpus to simultaneously work on multiple data elements by means of dedicated simd  instructions. based on the input data organization described in the preceding section, individual instructions of the sequential implementation  can be transformed into corresponding simd instructions. in contrast to the sequential implementation that aligns a single qs against a single ancestral reference at a time, the simd implementation simultaneously aligns a single qs against w  ancestral reference sequences on each x <dig> core.

the number of instructions that are necessary to calculate individual entries in the dynamic programming matrix is similar for the sequential and simd implementations, but the memory throughput is increased by w  since each simd instruction works on w  times more data compared to its sequential counterpart. it is therefore crucial to reduce the amount of memory used by the alignment algorithm such that frequently accessed data is kept in cache. during the dynamic programming matrix computations , we do not keep the entire dynamic programming matrix in memory, but only a single line. this is possible because, according to equation  <dig>  the calculation of a matrix entry di,j in row j depends only on values of d that have previously been calculated either in the same row or in the preceding row j− <dig> 

to make the simd implementation more generic, we hide the specifics of the actual simd instruction set  in a thin abstraction layer called a generalized vector unit, which is implemented using c++ templates. the generalized vector unit can be instantiated with different data types  and vector unit widths . the top-level algorithm uses abstract vector data types  and vector operations . here, the group width w  corresponds to the width of the vector unit  on our primary target platform, an intel x <dig> with sse instructions. this ensures that the element groups in the rs and the dynamic programming matrix d can be directly loaded into cpu vector registers. we implemented the generalized vector unit for intel x <dig> cpus using sse version  <dig>  intrinsics. note that sse version  <dig>  is only required for using 32-bit integers, while for 16-bit integers sse version  <dig> or higher is sufficient. we have verified that the concept of the generalized vector unit also works correctly for other vector instruction sets .

simt inter-task parallelization
on the simt platform, each alignment kernel invocation calculates one dynamic programming matrix. to efficiently execute the kernel on a gpu, every individual dynamic programming matrix calculation is assigned to a distinct thread . an alternative parallelization scheme using intra-task parallelization would assign each task  to a thread block, and all threads within that block would then cooperate to accomplish the task. liu et al. <cit>  investigated both the inter-task as well as the intra-task parallelization approaches for porting the swa to simt platforms. they found that inter-task outperforms intra-task parallelization. however, the intra-task approach requires significantly less device memory per thread. the intra-task approach is also appealing in cases where a large number of independent pairwise alignments need to be performed. however, in papara, each qs is aligned against a large number of rs. this alignment workload can be grouped into many 1-to-w alignments, which naturally fits the inter-task paralellization scheme. previous experiments with the swa by other authors showed that, if the problem at hand is such that the inter-task approach can be deployed, it consistently outperforms the intra-task approach on simd
 <cit> and simt
 <cit>  architectures. the inter-task approach is more communication-efficient since frequent synchronization between threads operating on a single, shared dynamic programming matrix is not required. threads only need to be synchronized once upon kernel termination. therefore, we did not further investigate the intra-task approach for papara. figure
 <dig> depicts the inter-task parallelization approach we use here.

block-based matrix calculation
a straightforward approach for calculating the papara dynamic programming matrix is to compute one row after the other in a diagonal direction as shown in figure
 <dig>  the memory requirements of this approach depend on the size of the inter-reference vector, since only the last matrix row needs to be stored in memory for calculating the next row. however, this approach requires off-chip global memory and does not allow for using on-chip shared memory. the main reason for this is that the size of the inter-reference vector in real-world scenarios will typically exceed the amount of shared memory available on a representative simt platform. furthermore, the entire row needs to be calculated before proceeding to the next row. to alleviate these shortcomings, and to be able to use shared memory, we devised a block-based approach. the matrix cells are calculated in square or rectangular blocks of adjustable size. the block size is a function of the length of the query sequences, the amount of available shared memory, and the number of rs. figure
 <dig> depicts this blocked matrix calculation model.

due to the diagonal direction of the papara matrix calculations and the limited amount of on-chip memory, it is not possible to use shared memory efficiently along the entire inter-reference vector. this means that the very first and very last parts of it need to be calculated using global memory for storing the intermediate values of the row fraction they are operating on. the part of the inter-reference vector residing in the rectangular blocks can, however, be calculated using shared memory. using rectangular blocks at either end of the rs requires the evaluation of additional conditional  statements in the kernel code. note that evaluating conditionals can substantially deteriorate gpu performance. thus, we use global memory at either end as a trade-off to circumvent this problem and to avoid evaluating conditionals that would slow down processing of the main part of the inter-reference vector. our approach only requires a fixed amount of global memory, irrespective of the length of the rs , since the full length rs is processed by iterating over blocks of fixed size. in the example provided in figure
 <dig>  blocks  <dig> and  <dig> use global memory while blocks  <dig> and  <dig> operate on shared memory.

loop unrolling
an advantage of the blocked approach over the straightforward approach is that the amount of global memory required for the computation of a single dynamic programming matrix does not depend on the length of the rs. nevertheless, code complexity increases since three nested for-loops are required to compute the entire matrix: one loop iterates over the rectangular blocks, the second loop iterates over the query sequence, and the innermost loop iterates over the reference fraction in the current block. we observed that the anticipated performance gain by using shared memory was reduced by the increased code complexity, which hindered the gpu threads from efficiently executing the blocked version of the opencl kernel. to solve this problem, we unrolled the innermost for-loop that iterates over the fraction of the reference corresponding to the rectangular block.

loop unrolling significantly improved kernel performance but comes at a cost: the number of rs that the kernel can align is hard-coded. however, this limitation is not problematic, since in typical real-world scenarios users will align thousands of reference and query sequences. in this case, the amount of dynamic programming matrices to be computed is organized in groups that contain a fixed number of rs. thus, the opencl kernel can be launched several times on those groups. we opted for using a fixed number of rs based upon an in-depth investigation of the impact of the shared memory size setting on opencl kernel performance. in our implementation, we use  <dig> kb of shared memory because a significant slow-down was observed when using larger amounts of shared memory. the number of rs per kernel launch is fixed  to  <dig>  which allows for unrolling the innermost loop  <dig> times. thus, we were able to completely remove this loop because the selected shared memory size only allows for processing  <dig> columns of the dynamic programming matrix in each block.

data compression
the global memory access pattern is also performance-critical on simt platforms. therefore, we compressed the part of the inter-reference vector that is processed by the blocks in shared memory to reduce the frequency of global memory accesses. every element  in the inter-reference vector requires  <dig> bits  which allows the use of one unsigned 32-bit integer value to store  <dig> elements. this boils down to only two global memory accesses for every  <dig> elements . since square blocks are not unrolled, the inter-reference vector entries in such blocks are also not compressed for the sake of code simplicity, and to allow for the efficient execution of threads. figure
 <dig> outlines how the inter-reference vector is organized on the simt platform. the uncompressed initial and final parts of the vector require one integer per element while the compressed part requires one integer for every  <dig> elements. note that all three parts of the vector  are padded to a multiple of  <dig> unsigned integers.

opencl application
the complete opencl application consists of tasks performed by the cpu  and operations that are offloaded to the gpu . the host program handles the inter-reference memory organization and compresses the input rs. once the references have been rearranged  and compressed, they are stored in pinned  contiguous memory space. the query sequences are also stored in contiguous blocks, each the length of the longest query sequence. allocating pinned memory for the query and inter-reference vectors allows for fast gpu data transfers via pci express. after the data  have been transferred to the global gpu memory, the host program launches a one-dimensional kernel of size q∗r_padded, where q is the number of query sequences and r_padded the smallest multiple of  <dig> that is greater than the number r of rs. those q∗r_padded threads are organized in q local groups of size r_padded. this local, group-based thread organization simplifies the indexing of the query sequences in the kernel function. note that the number q of query sequences that is aligned per kernel call usually only represents a fraction of the total amount of query sequences in the input dataset, due to memory limitations.

finally, the kernel function implements the actual papara alignment kernel. the local group index in the kernel references the query sequence while the thread index references the rs. thus all threads in a local group align the same query sequence to all rs in device memory. the query and reference sequences are retrieved from global memory and the intermediate values  are either stored in global memory  or in shared memory . as described before, all global memory accesses are correctly aligned to minimize the performance impact of high-latency global memory accesses.

RESULTS
gpu performance
to assess performance of the opencl simt implementation, we used a heterogeneous system equipped with an intel i <dig>  <dig> cpu running at  <dig>  ghz  and a nvidia geforce  <dig> gpu with  <dig> cuda cores and  <dig> gb ddr <dig> device memory . we measured total execution times as well as gcups  for kernel executions on real-world datasets.

initially, we focus on the performance of the opencl kernel for different input dataset sizes. to this end, we investigate the behavior of single kernel launches with different rs lengths between  <dig> and  <dig>  nucleotides. every kernel launch aligns  <dig> query sequences with an average length of  <dig> nucleotides to  <dig> rs. as shown in table
 <dig>  the performance of the opencl implementation improves with the rs length until the peak performance of  <dig> gcups is reached. for very short rs , kernel performance drops below 50% of peak performance. as already stated, the block-based implementation uses global memory for the first and last square blocks of the matrix and shared memory for the intermediate rectangular blocks. when the rs is short, global memory will predominantly be used for dynamic programming matrix calculations, since the square blocks almost cover the entire matrix. in other words, because of the short reference length, the potential performance gains by using shared memory are negligible, since the majority of operations is conducted on global memory. for longer sequences however, the majority of operations is carried out on shared memory and therefore improved gpu performance is attained.

execution times  to align  <dig> qs to  <dig> rs.

simd performance behaves differently with increasing rs length. the cumulative performance on  <dig> cores is highest for the short references with  <dig> base pairs  and decreases linearly to reference lengths of  <dig>  base pairs . we observed a further substantial performance deterioration on very long reference sequences . this is due to the increased number of cache misses when longer references are analyzed. our simd implementation keeps one line of the dynamic programming matrix in memory. each matrix entry corresponds to a vector of 8x16-bit values . a reference length of  <dig>  requires a matrix line size of roughly  <dig> kb, which fits into the l <dig> cache  of the intel i <dig>  <dig> cpu. for a reference length of  <dig> , the matrix row occupies  <dig> kb and does not fit into the l <dig> cache. therefore, most matrix cell calculations need to access the slower l <dig> cache . the slowdown is even larger for the longest sequences under examination, where the data do not fit into l <dig> cache either. currently, we do not expect these slowdowns to be problematic for real-world scenarios because current sequencing devices rarely produce reads exceeding a length of  <dig> base pairs
 <cit> . however, when future sequencing technologies  are capable of generating a large number of reads longer than  <dig> base pairs, then a blocking technique similar to the one devised for the simt implementation could also be applied to the simd implementation.

we also investigated the performance of the opencl kernel for different numbers of query sequences. we chose a fixed rs length of  <dig> since this represents an average case scenario, and this is close to our gpu’s peak performance levels. keeping the reference length constant, we launched the kernel multiple times using different query sequence numbers. the results of this performance assessment are provided in table
 <dig>  while kernel performance is practically independent of the number of query sequences, there is a slight performance improvement up to  <dig> qs for the simd implementation. this is caused by the initial overhead required for transforming the rs into their inter-reference representation, which has to be done once for each block of w  rs but is independent of the qs number. for more than  <dig> qs this initial overhead becomes negligible. as expected, the performance of the sequential implementation is completely independent of the qs number. we did not conduct any tests to examine the performance of the kernel for different average query sequence lengths, as the papara algorithm has been designed for aligning short read qs  to a ra.

execution times  to align the qs to  <dig> rs with length  <dig> 

hybrid cpu-gpu approach
to achieve the maximum possible performance for a typical cpu-gpu system, we also designed a hybrid papara version that simultaneously uses all available cores as well as a gpu. in addition to multi-threading , the cpu part of the hybrid system uses the simd implementation described earlier with a vector/group-width w  =  <dig> and 16-bit scores. initially, the algorithm generates and groups the rs into ‘blocks’ of w  sequences. these blocks are stored in a work queue, and are sequentially retrieved by multiple worker threads simultaneously. each worker thread aligns all qs against the w  reference sequences in the block. once all queued blocks have been calculated, the master thread resumes control and, for each qs, only re-computes the alignment for the respective best-scoring qs/rs pair to perform the backtracking step.

the gpu part of the hybrid system extends this mechanism by an additional gpu thread. in analogy to the cpu threads, the gpu thread consumes blocks from the same work queue. a key difference is that, while each cpu thread only removes one block from the queue at a time , the gpu works on a higher number of rs at a time . the gpu thread can remove up to  <dig> blocks from the work queue at a time. the gpu thread continues to obtain and work on multiple blocks until the block queue is empty. thereby, the cpu cores and the gpu compete for rs. for a sufficiently large number of rs this yields good load balance between the cpu cores and the gpu. this approach can also be extended for using more than one gpu.

the gpu dram size limits the number and the length of qs that can be transferred to the gpu at each invocation of the alignment kernel. while the number of qs per invocation has to be small enough to fit into the available dram when the qs are long, for short qs the number has to be high enough to achieve good performance . the hybrid cpu-gpu algorithm dynamically optimizes the number of qs based on the available amount of dram and the actual qs lengths. thereby, we ensure that each kernel invocation operates on the largest possible qs number. this dynamic load balancing allows for stable performance over different dataset shapes with different sequence length distributions .

currently, the opencl implementation faces a technical difficulty on nvidia gpus. once a kernel is invoked, an internal thread of the gpu driver executes a busy-wait, apparently  waiting for the gpu to finish. if a clfinish call is deployed on the cpu side to wait for completion of the gpu kernel, this will, in addition to the internal driver thread, cause the calling thread to execute a busy-wait, effectively wasting the computational power of two cpu cores. while cuda offers a method for selecting between a busy-wait  and a lazy-wait, this option does not yet exist in opencl. from the values shown in table
 <dig>  we can roughly estimate that the speedup of the gpu over two cpu cores is approximately three-fold  in the best case. this effectively means that wasting two cpu cores for interacting with the gpu has a negative impact on overall system performance. an analysis of the nvidia opencl library showed that the internal busy-wait implementation executes sched_yield function calls which should yield cpu cycle time to other threads. in practice, this call has no positive effect and overall system performance decreases in proportion to the number of threads that are executing a busy-wait. this busy-wait issue is a well-known linux problem . to temporarily circumvent this issue, we use a customized shared library to replace  all sched_yield calls by usleep calls. this actually yields cpu cycles to other threads at the cost of increased kernel call latency, but only of the order of a few milliseconds. this latency increase is not critical here because each gpu kernel invocation requires a few seconds to complete when the number of qs is sufficiently large . using this work-around, we can leverage the entire computational power of the gpu and all cpu cores.

system performance
we assessed overall performance of the hybrid cpu-gpu algorithm using two representative real-world datasets. the experiments were performed on the same system as described above , using all  <dig> cpu cores. the first test dataset  has already been used to evaluate the original implementation of papara in
 <cit> . this dataset contains  <dig> rs of length  <dig> and  <dig>  qs with a mean length of  <dig> base pairs. we did not use other datasets from the original study because the hybrid cpu-gpu algorithm targets larger datasets. the second dataset  is from a recent study comparing papara to a newly developed algorithm
 <cit> . the unoptimized, proof-of-concept implementation of papara performs better than competing alignment approaches on the latter real-world dataset at the cost of substantially higher runtimes. this dataset consists of  <dig>  rs of length  <dig> and  <dig>  qs of lengths that vary between  <dig> and  <dig> 

table
 <dig> shows the performance of the hybrid cpu-gpu algorithm on the two datasets. column tscoring provides the runtime for the scoring phase. column tall shows the overall runtime for the whole algorithm, including the pre-processing of input files and the generation of the actual alignments. these pre- and post-processing steps are unoptimized sequential tasks that are performed on the cpu. the overall cpu performance is shown in column gcupscpu, which provides the accumulated performance on  <dig> cpu cores. overall gpu performance is provided in column gcupsgpu. on both datasets, the relative contribution of the cpu cores and of the gpu are very similar; the cpu and gpu contribute 40% and 60% of the overall gcups to the accumulated cpu-gpu system performance, which is shown in the last column . all gcups values refer to sustained gcups since they include the overhead induced by load imbalance between the cpu and the gpu. load imbalance is observed when either one of the cpu threads or the gpu finish last and require the other computational resources to wait. the sustained real-world performance of the hybrid system corresponds, almost exactly, to the performance we obtained for the synthetic benchmarks.

runtimes  for the scoring phase and the whole algorithm.

discussion
intuitively, the papara algorithms exhibits a similar complexity to the swa, since they both are dynamic programming kernels. however, specific features for incorporating phylogenetic signal in the papara algorithm make it more challenging to implement than the swa. in papara, the reference input sequences are parsimony state vectors. hence,  <dig> bits are required for representing parsimony vector entries for dna data. in swa, reference input sequences are simply plain characters and not parsimony states. thus, swa requires only  <dig> bits for each nucleotide, which means that papara exhibits higher memory requirements than swa. furthermore, papara calibrates gap penalties by using an additional cgap flag. this model increases the gap calculation cost and does not allow for optimizing gap-open penalty calculations as used in swipe
 <cit> . the fact that the swa algorithm can therefore be mapped in a more compact way to x <dig> and simt platforms explains the higher gcups performance obtained
 <cit> .

the papara kernel was implemented in opencl. several informed design decisions and optimization techniques were applied to achieve the best possible performance. the inter-reference memory organization allowed for efficient use of global memory, while the block-based approach was devised to exploit on-chip shared memory. the block-based implementation gave rise to applying loop unrolling and data compression, which further improved performance. apart from the thoroughly optimized opencl implementation, we also developed a sse <dig>  vectorized version of the kernel to investigate how state-of-the-art simd platforms perform for papara. via the competing programmer approach, we hope to provide an as fair as possible performance comparison between two fundamentally different hardware architectures. on an intel i <dig>  <dig> cpu , we obtained a x <dig> simd peak performance of  <dig> gcups and an average performance of  <dig>  gcups. a mid-range gaming gpu like the gtx  <dig> delivered peak and average performance of  <dig>  and  <dig>  gcups.

we developed and optimized the opencl kernel mainly for the nvidia fermi gpu architecture. nonetheless, we also executed some exploratory tests on an amd/ati gpu . as expected, the opencl kernel could be executed on the ati system without any substantial modifications, but we observed poor performance. we measured performance of  <dig>  gcups  on a subset of the real-world dataset used to evaluate the performance of the hybrid cpu-gpu system. one would expect a  <dig> to  <dig> times better gcups performance for a fully ati-tuned kernel. note however that, in contrast to the nvidia fermi architecture, the amd/ati architecture heavily relies on instruction level parallelism . thus, it may become necessary to re-write the alignment kernel such that the opencl compiler can group similar instructions more efficiently in an simd-like manner to attain peak performance. thus, while opencl code is portable, achieving satisfying performance still requires an architecture-aware tuning.

finally, we combined and coupled the simd and simt implementations to create a hybrid cpu-gpu system that can now exploit all available computational resources of a representative modern desktop system . the total system peak performance amounts to  <dig>  gcups. by efficiently exploiting all computational resources, we are able to fundamentally improve the applicability of the highly accurate papara algorithm to large real-world datasets. for dataset 16s.b.all, the major concern regarding papara expressed in
 <cit>  is the relatively long program runtimes of the original  proof-of-concept implementation. in terms of alignment quality, papara outperforms all alternative approaches that have been assessed on large real-world datasets according to this independent study. to test the current limits of cpu performance using papara, we conducted additional tests on an overclocked system . this type of cpu has an unlocked clock-frequency generator and can therefore be overclocked without additional hardware modifications, except for providing appropriate cooling. on the overclocked cpu, we measured a peak performance of  <dig>  gups on  <dig> cores . this corresponds to an improvement of 20% over the i <dig>  <dig> cpu. we did not experience any stability issues for the specific overclocking configuration during these tests. based on the performance data published at
http://www.spec.org/cpu2006/results/cpu <dig> html, we conclude that this cpu, when pushed to its technical limits, currently offers the highest per-core performance of any available cpu on the market. finally, our results corroborate the observations made in
 <cit>  that the computational capabilities of modern cpus and gpus are in the same order of magnitude provided that highly optimized alignment kernels are developed on both platforms.

CONCLUSIONS
in this paper we described the adaptation and acceleration of a novel phylogeny-aware short-read alignment kernel named papara to modern x <dig> and simt architectures. for the simt architecture we used opencl while for the simd platform we deployed multi-threading and sse <dig>  vector intrinsics. we observed that state-of-the-art cpus and gpus deliver comparable performance for sequence alignment algorithms if properly optimized. we also demonstrated that overall system performance can be substantially improved when all computational resources are used .

the simd and simt implementations were developed using the “competing programmer approach”. thus, the programming time and tuning effort spent on both implementations was comparable. the performance of the resulting codes is analogous to the performance obtained for previous simd and simt accelerations of the related, yet not identical swa. we conclude that, for representative dynamic programming kernels, deploying simd vector intrinsics is as challenging as porting the algorithm to an simt platform. in both cases, a thorough understanding of the underlying hardware architecture is required . an understanding of cpu architectures can help to reduce cache misses and/or pipeline stalls. understanding how threads are launched on a gpu can help to reduce/eliminate memory access conflicts among parallel threads and therefore increase the number of executed instructions per cycle.

regarding future work, we intend to also explore cuda as an alternative to opencl as well as to devise an analogous performance comparison. furthermore, we plan to investigate how a opencl code, as optimized for a gpu, performs on multi-core cpus. we also intend to analyze the programming effort that is required to transform a gpu-optimized opencl implementation into a cpu-optimized one. finally, we plan to implement a block-based cpu version of the kernel for further reducing cache misses and for improving cpu performance on very large reference sequences.

abbreviations
simd: single instruction multiple data; mimd: multiple instruction multiple data; simt: single instruction multiple threads; papara: parsimony-based phylogeny-aware short read alignment; gcups: giga cell updates per second; msa: multiple sequence alignment; qs: query sequence, rs, reference sequence; rt: reference tree; swa: smith-waterman algorithm; fpga: field-programmable gate array; opencl: open computing language; cuda: compute unified device architecture.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
na, as and sb designed the study. na implemented the opencl code for the simt platform. s.b. implemented the vectorized c code for the simd platform. na, sb and as wrote and edited the manuscript. all authors read and approved the final manuscript.

