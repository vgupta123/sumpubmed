BACKGROUND
studies exploring genetic associations in human diseases have flourished in the past few years due to the progress of molecular biology techniques. presently, these genomic studies focus mainly on single nucleotide polymorphisms  as evidenced by the recent advent of genotyping chips which can analyze up to  <dig>  snps simultaneously in a single individual. in these studies, the standard comparisons between patients and controls are performed at the level of the snps and at the level of their combinations which are called haplotypes. haplotypes are of great interest for genetic association studies since they correspond to chromosomal fragments transmitted from one generation to the next. the importance of haplotypes is emphasized by the hapmap project which identifies the most prevalent and relevant haplotypes in the human population  <cit> .

normal genotyping  of an autosomal snp yields the two alleles present on the maternal and paternal chromosomes. as a consequence, snp haplotypes cannot be determined directly because it is not known which alleles lie on the maternal chromosome and which lie on the paternal chromosome. the experimental determination of haplotypes is very expensive and time-consuming  <cit> . as an alternative, computational methods can resolve the haplotypes in a population when the genotypic information is available for enough individuals in that population . these methods are possible because experience shows that there are a relatively small number of haplotypes present in a given population and they are maintained according to rather simple rules in the course of evolution.

in the past decade, several algorithms have been developed for inferring the haplotypes from a population of genotypes. these computational methods are either combinatorial  or statistical .

an initial combinatorial method was introduced by clark  <cit> . this algorithm first constructs a list of all haplotypes found from unambiguous individuals, i.e. individuals with at most one heterozygous site. then, for all the ambiguous individuals , it picks up a compatible haplotype from that list and adds the complementary haplotype to continue the process. this method is a variation of the parsimonious approach which asserts that the smaller the haplotypes set is to solve all the individuals, the better the solution will be. this method has two caveats: the presence of unambiguous individuals is mandatory and the final result depends on the order of treatment of the individuals. several other authors have looked further for a parsimonious approach to extract the smallest haplotypes set explaining the genotypes in a population. for example, wang & al used a "branch & bound" approach  <cit>  and gusfield a linear programming formulation  <cit>  to find the most parsimonious haplotype sets count among all the possible sets of haplotypes. to take into account the haplotypes with a common evolutionary history, gusfield proposed a refinement of the parsimonious principle  <cit>  with a focus on the number of mutation events needed to generate the haplotype set rather than on the number of different haplotypes needed to resolve all genotypes in the sample. the resulting algorithm seeks a set of haplotypes that fits a perfect phylogeny. the software hap <dig> has been developed based on this principle  <cit> .

whilst the combinatorial methods have proved valuable, generally they cannot handle a large number of snps , and many cases of missing data may prevent the resolution of haplotypes. further, there is the theoretical problem that the true set of haplotypes carried in the population may not be the most parsimonious.

statistical methods consider the haplotype inference problem through the distribution of the haplotype frequencies in the population rather than through the direct assignment of haplotype pairs for each individual. this statistical framework can handle a higher level of complexity in the data such as a larger number of snps, missing data, or multi-allelic sites. one of the best-known approaches is the em algorithm  <cit>  which estimates haplotype frequencies by maximizing the likelihood of the sample genotype under the assumption of hardy-weinberg equilibrium. the most frequent haplotypes pairs can then be assigned for each genotype in the sample. this method works well but has limitations linked to storage requirements because the number of possible haplotypes grows exponentially with the number of loci treated. a computational strategy has been proposed to alleviate this limitation by partitioning the dataset into smaller subsets for which the em algorithm is applied and then joining the blocks of results obtained on each subset : it is called pl-em  <cit> . as the em algorithm often fails to capture the haplotype diversity of a sample population, alternative approaches based on bayesian statistics have been developed  <cit> . they rely on a gibbs sampler which computes the posterior distribution of the haplotype frequencies given the genotype of the sample and assumed prior information about the haplotype distribution  <cit> . these bayesian algorithms differ in the prior they use. stephens & donnelly use an approximate coalescent prior that will give a better weight to the haplotypes that are most similar to  or that are a mosaic of  the previously sampled haplotypes. niu & al  <cit>  use a dirichlet prior that chooses randomly among all possible haplotypes if the genotype cannot be made with previously sampled haplotypes. the accuracy of the statistical approaches was studied by several authors and although there is some dispute  <cit> , it seems that the phase algorithm provides a slightly better haplotype inference than the other methods  <cit> . however, phase still has longer run-times. recently, new programs such as gerbil  <cit> , fastphase  <cit> , haplorec  <cit>  and 2snp  <cit>  allow to infer haplotypes under various models of cluster of similarity in order to handle large snp datasets. whilst they are faster, they seem to be less accurate than phase.

in practice, statistical methods now allow inference of haplotypes despite missing data and provide a probability for each haplotype resolution.

in this work, we present a new haplotyping algorithm which runs faster than fastphase in common snp datasets  while providing similar or better accuracy than phase.

algorithm
rationale of the algorithm
the major hurdle for the haplotype inference problem is the very large number of haplotype pairs to be explored consistent with the genotypes in the population. our rationale has been to try and limit the set of possible solutions to be explored, and then adapt the most efficient haplotyping procedures to this restricted set.

the algorithm that we have developed is based on  <dig> improvements :  <dig>  the use of a iterative multiallelic em  to obtain very fast em estimations.  <dig>  the use of a bootstrap approach to generate sufficient diversity while defining a limited set of possible haplotype pairs for each genotype.  <dig>  the adaptation of the best haplotyping procedures to this restricted haplotype space. in particular, we have tried hereafter to estimate frequencies on this restricted haplotype space by a pseudo gibbs sampler based on a recombination and/or a coalescent model similar to the approach proposed by stephens & donnelly  <cit> .  <dig>  the use of a specific partition-ligation strategy to adapt our method for larger snp datasets.

the two first improvements  are combined to generate a set of candidate haplotypes of reasonable size very rapidly, and then the third improvement is used to produce an optimal solution from the previously defined set of possible solutions. in case of larger datasets, we have adapted a new partition-ligation strategy in which the segments which have been haplotyped according to our algorithm  are in turn treated as simple loci with the same multiallelic iem, bootstrap and pseudo gibbs sampler approach .

first step: bootstrapping the iterative multiallelic em algorithm
multiallelic iterative expectation-maximization algorithm 
the storage requirement and the computational effort needed by the classical em algorithm to reconstruct the haplotype grow exponentially with the number of heterozygous and missing sites included in the genotype. in the case of multi-allelic polymorphisms this complexity easily becomes intractable for standard computers. an idea to break this growing complexity is to construct the haplotypes space gradually rather than only once. a simple iterative process constructs the haplotypes starting from  <dig> loci, then adding a 3rd locus, then a 4th locus etc... the iteration at the lth snp is performed by applying the em algorithm on the haplotypes obtained at the l- <dig> locus combined with the alleles of the lth locus. more precisely, the algorithm runs as follows :

define the treatment order of the loci . for each genotype, take the alleles observed at the first locus as corresponding haplotype pairs. loop until completion :

 <dig>  set as current locus the following one according to a defined treatment order.

 <dig>  extend haplotype pairs for each genotype by combining them with the alleles observed at the current locus.

 <dig>  estimate by em the probability of each haplotype pair.

 <dig>  remove all haplotype pairs whose probability is under a settable threshold .

practically, we have observed that there are rarely more than  <dig> or  <dig> haplotype resolutions with a significant probability  obtained for a given subject at each iteration. this justifies keeping only a limited number of haplotypes in memory at each new locus inclusion. it is noteworthy that this approach is limited by the fact that the more snps there are, the more genotypes will correspond to orphan haplotypes . indeed, at a given snp inclusion, the resulting configurations for a genotype may correspond only to orphan haplotypes. in this case, the em algorithm chooses randomly -and likely erroneously- a "most probable" configuration among them. this limitation is inherent to the em algorithm itself. we will see that for the iem, it needs to be addressed only for larger snp datasets: we describe our solution in the third step below.

iem presents a clear advantage over the classical em approach with respect to minimizing orphan haplotypes since only the most frequent sub-haplotypes obtained with the already treated snps are used for defining the possible haplotypes on the next round .

bootstrap procedure
the main idea of this approach is to apply the iem algorithm repeatedly on bootstrap samples of the original population to define the most probable corresponding haplotype pairs with more flexibility than with a single run of the iem algorithm. this bootstrap approach introduces more possibilities in the haplotype configurations consistent with the genotypes in the population in order to increase the chance of capturing the true ones. another advantage of the bootstrap procedure is that we use loci ordered randomly at each sampling and this allows us to escape the bias of iem linked to the treatment order of the loci. indeed, when running iem in a given order one will always find the same solution, but changing the order can lead to different solutions thus to more diversity. of course, this generation of diversity is mainly targeting rare haplotypes since frequent haplotypes are always retrieved whatever the initial order of the loci.

here is a description of the bootstrap procedure:

start with a single run of the iem on the initial population in order to store the obtained haplotypes pairs for each of the genotypes. and then repeat the following step n times :

 <dig>  generate a bootstrap sample by sampling with replacement from the original sample.

 <dig>  use iem to reconstruct the haplotypes for the generated bootstrap sample with a random input order of loci.

 <dig>  store the haplotype pairs obtained for each genotype included in the bootstrap sample, and their associated probability.

finally, compute a posterior probability for all the haplotype pairs found. this is the sum of the probabilities stored during the bootstrap procedure divided by the number of time the genotype was sampled. at the end, for each genotype, the haplotype pairs with very low average probability  are removed.

the underlying idea of this bootstrap approach is to create enough diversity in haplotypes configurations by  randomizing the treatment order of loci for iem and  perturbing the genotype composition of the population. on the one hand, taking multiple bootstrap samples of the population introduces more perturbations in the resulting haplotypes configurations for genotypes corresponding to rare haplotypes than for those corresponding to frequent haplotypes, and it is the rare genotypes for which the haplotyping algorithms normally diverge the most. on the other hand, applying the iem on each bootstrap sample with random ordered loci allows building up haplotype configurations following different scenarios of dealing with frequent haplotypes.

second step: application of an accurate haplotype inference method on the restricted haplotype space
the bootstrapped iem provides each genotype of the population with a limited set of candidate haplotype pairs. the initial problem thus becomes much less complex since the set of solutions to explore is smaller. on this pretreated problem, it is possible to apply sophisticated haplotype inference methods. in the present work, we have chosen the same approaches as the phase <dig>  and phase <dig>  programs  <cit>  based on a pseudo-gibbs sampler combined with various models of haplotype distribution. the two models considered rely on different models of the population evolution :  in the coalescence model, the future sampled haplotypes tend to be similar to the ones previously found  <cit>  and  in the recombination model, the future sampled haplotypes tend to be a mosaic of the ones previously found  <cit> . the implementation of the pseudo gibbs sampler is as follows :

make a randomly ordered list of the genotypes of the population and randomly assign to each genotype one candidate haplotype pair from those selected by the bootstrapped iem. then, iterate a large number of times the two following steps:

 <dig>  update model parameters. for the coalescence model, order the list of the genotypes randomly. for the recombination model, invert the order of two randomly chosen genotypes and estimate recombination rates in view of the current haplotype assignments.

 <dig>  for each genotype in the list :

a. calculate a probability according to the model for each of the haplotype pairs retained by the bootstrapped iem under the assumption that all the others genotypes are correctly reconstructed .

b. assign to the genotype a haplotype pair from those retained by the bootstrapped iem by a random draw according to probabilities computed in .

the algorithm iterates steps  <dig>  and  <dig>  a large number of times to get sufficiently close to the final solution . for each additional iteration, current haplotype frequencies and haplotype pair probabilities are stored in order to provide reliable statistical results at the end of the iterations.

in the following, similarly as phase, we have compiled our program in two versions, ishape <dig> for the use of the pseudo gibbs sampler with a coalescent model , ishape <dig> for the use of the pseudo gibbs sampler with a recombination model . like some other haplotyping software, ishape will produce a list of haplotype pairs with a probability for each genotype and a list of the haplotypes found with their frequencies.

in practice, we find that datasets above 30– <dig> snps generate an explosion of the candidate haplotypes generated after the iem bootstrap procedure . as a consequence, we investigated a partition-ligation  approach. the strategy was as follows:

third step: partition-ligation strategy
larger snp datasets are divided into segments of limited size to avoid an explosion of candidate resolutions .

in each segment, candidate haplotype configurations are then generated with the bootstrap-iem approach and, among them, the gibbs sampler estimates the most probable ones according to the chosen model. the iterative aspect of the bootstrap-iem approach adapts nicely to a progressive strategy  <cit>  of ligating the haplotype resolutions previously found on the individual blocks. indeed, if the resolutions on each segment are considered as a multi-allelic marker, it is possible to apply exactly the same bootstrap-multiallelic iem approach to delimit a set of candidate haplotypes on the whole segment dataset and so to precisely estimate haplotypes with gibbs sampler on this limited haplotype space. to summarize, this partition-ligation strategy is done in only two steps:  obtain reliable solutions for each segment and  ligate them with the same approach applied to the pre-haplotyped segments.

we have investigated the optimal division of the snp dataset into segments: based on a given size , or divisions into  <dig>   <dig>   <dig>  or  <dig> segments. we also tested a strategy defining segments according to the proportion of orphan haplotypes generated at each iteration of the iem approach . when too many orphan haplotypes were generated, we backtracked and started to define a new segment starting from the snp at stake. in other words, the segments are defined as the largest snp subset in which the iem generates a minimum number of orphan haplotypes. in practice, the size of the segments varies from  <dig> to  <dig> snps, depending on the level of linkage disequilibrium between the snps in the studied region. this latter pl strategy was used in the present study.

RESULTS
impact of the number of bootstrap resamplings
first, we evaluated the impact of the number of bootstrap resamplings on the size and the relevance of the space of the candidate haplotypes produced. to characterize the relationships between these points, we applied our algorithm several times with a growing number of bootstrap resamplings  on two datasets, gh <dig> and apoe  with different levels of missing data . for each, we replicated the test  <dig> times and measured the icr and the number of candidate haplotype configurations. figure  <dig> summarizes the results obtained for the apoe gene. we observed a limit in the size of the haplotype space and ability of the bootstrap to capture the true haplotypes configurations was reached after  <dig> samplings whatever the percentage of missing data . on our test platform ,  <dig> bootstrap resamplings took  <dig>  and  <dig>  seconds for 0% and 10% missing data .

the results were identical with the gh <dig> gene . in the remainder of this work, we have thus parametered our software to perform  <dig> bootstrap resamplings by default.

relevance and size of the haplotype space generated by bootstrap resampling
to test the ability of our bootstrap approach to define a relevant haplotype space, we compared the capture rate of true haplotype configurations at different missing data levels with those obtained by the other algorithms. as shown in table 1a, the capture rate was better for the multiallelic iem-bootstrap than for phase <dig> , phase <dig>  on both the apoe and the gh <dig> datasets, and all these approaches greatly outperformed pl-em and fastphase. this fact suggests that applying the phase algorithms on the restricted number of possibilities produced by the iem-bootstrap should lead to the same accuracy, but much more quickly.

a) comparison of the algorithms'performance on the apoe and gh <dig> datasets regarding the average icr  and the average number of detected haplotypes . for each level of missing data ,  <dig> experiments were performed. best performances are highlighted in bold.

b) estimation of the reduction of the space of possible haplotypes for the gh <dig> and apoe genes operated thanks to the bootstrap-iem. the values are the average of  <dig> experiments. in the lower line, the values in brackets give the correspondingaverage icr.

another interesting point was to compare the size of the possible haplotypes space and the candidate haplotypes space in order to have an idea of the savings of our approach in terms of computational cost. to illustrate how much the haplotype space is reduced, we compared the average number per genotype of candidate haplotypes generated by the iem-bootstrap algorithm versus the average number per genotype of possible haplotypes, for the two real datasets gh <dig> and apoe with  <dig>  2%, 5% and 10% levels of missing data . in the context of complete data, we can see that with the bootstrap-iem, the number of resolutions for a gene of  <dig> snps  is roughly divided by  <dig> compared to the possible configurations, and divided by  <dig> for  <dig> snps , while capturing respectively 100% and 99% of the true configurations . the reduction is even more important when there are missing data: the haplotype space is divided by up to  <dig> and up to  <dig> for respectively apoe and gh <dig> with 10% missing data while keeping a capture rate above 97% . this suggests that our approach is more effective in the context of missing data.

overall, in less than a second , the iem bootstrap approach reduces the space of haplotypes to explore by a factor between  <dig> and  <dig> compared to the regular haplotyping algorithms which must explore all possibilities, thus saving substantial computer time-costs.

comparison with the existing haplotyping programs
we compared ishape to other reference software: phase <dig>   <cit> , phase <dig>   <cit> , fastphase  <cit>  and pl-em  <cit> . all of these programs were set up with default parameters. we made the choice to use phase both with its original coalescence model  and with its recent recombination model  to see how the performance of these models is influenced by the input haplotype space. to obtain reliable comparisons of the time consumption with our gibbs sampler-based algorithms, we set the number of burn-in and main iterations to  <dig> to match the default for phase. we compared our software on the experimentally determined haplotypes of gh <dig> and apoe datasets with various levels of missing data . for each dataset, we estimated the accuracy of the algorithms with the ier coefficient and measured the time consumption .

we did not include hap, hap <dig>  and gerbil in the comparisons because they were shown to be less accurate than phase  <cit> . among the software developed to treat large numbers of snps, we did not test haplorec because it currently does not handle missing data. however, we included 2snp  <cit>  because it was described very recently and tested only on very large datasets .

the means obtained on  <dig> experiments for each level of missing data are presented in table  <dig> for the real gh <dig> and apoe datasets. the results demonstrate that our algorithm significantly outperforms pl-em and fastphase for accuracy in these real datasets, and is even slightly better than phase . in terms of time, ishape <dig> run  <dig> times faster than phase <dig>  for gh <dig> and  <dig> times faster for apoe.

different missing data levels are tested, each with  <dig> experiments. the mean accuracy  and runtime of the haplotyping algorithms are compared on a. the gh <dig> dataset, b. the apoe dataset. the 95% confidence intervals are also given. best performances are highlighted in bold. for 2-snp, the software does not provide haplotype frequency estimation: thus, the if is not available .

we then tested the various programs on much larger real datasets derived from the hapmap project , here making a large jump from  <dig> snps  and  <dig> snps  to  <dig> snps. the results are given as an average of  <dig> experiments for each size of snps tested :  <dig>   <dig>   <dig>   <dig> .. <dig> snps.  <dig> types of snps subsets were analyzed: adjacent snps and snps spaced by  <dig> kb in average.

software
different size of snp datasets are tested under two assumptions for the choice of the snps retained: adjacent snps and snps spaced by  <dig> kb in average. all the snps have a maf above 1%. for each given size  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  and  <dig> snps, one hundred different snps datasets were tested. the table provides a summary showing the average performances obtained over entire hapmap segments tested. the 3rd column presents the average rank i.e. the mean of the ranks given to each software regarding the ser they obtained for each experiment. the last column gives the average time obtained for the experiments. 95% confidence intervals are provided for the ser and for the ranking . best performances are highlighted in bold.

for adjacent snps, we saw that ishape <dig> outperformed all the other software tested, even phase <dig> , at the level of the mean ser and median ser per experiment. this excellent performance was outlined by the better average ranking of the software : overall ishape <dig> had a better average rank than all the other software. for time consumption, ishape <dig> runs from about  <dig> times faster than phase <dig>  for  <dig> snps and up to  <dig> times faster for  <dig> snps .

for the 5kb-spaced snps, phase <dig>  yielded slightly better results than ishape <dig> in terms of average ser but in terms of median ser and ranking, ishape <dig> was slightly better . this suggests that for a few datasets, ishape <dig> makes larger errors than phase <dig> . ishape <dig> was faster than both phase <dig>  and phase <dig> , and could compute haplotypes up to  <dig> times faster than phase <dig>  on  <dig> snps with a similar accuracy .

pl-em and ishape <dig> are much faster than ishape <dig> but much less reliable. in this line, as for the phase programs, the use of a recombination model appears to improve the quality of the haplotyping significantly.

since we have tested our program on middle-size groups , we tried to evaluate the impact of smaller and larger numbers of subjects. we used the experimental data proposed by rieder et al.  <cit>  regarding  <dig> subjects and  <dig> snps, and by daly et al.  <cit>  regarding  <dig> subjects and  <dig> snps. table  <dig> summarizes all the results. for  <dig> subjects and  <dig> snps, 2snp yields the best results, for  <dig> subjects and  <dig> snps, ishape <dig> yields the best results. in this latter case, fastphase also yields good results but runs  <dig> times faster than ishape <dig> 

accuracy and time comparison of the algorithms on the four real datasets ace  <cit> , apoe  <cit> , gh <dig>  <cit>  genes and data from chr. 5q <dig>  <cit> . the 95% confidence interval corresponds to  <dig> runs of each program for ace, apoe and ace, and to  <dig> runs for chr.5q <dig> data. best performances are highlighted in bold.

discussion and 
CONCLUSIONS
in this work, we have presented new software  for the computation of haplotypes. this software relies on the combination of the following improvements:  <dig>  use of a iterative multiallelic em algorithm;  <dig>  use of a bootstrap procedure;  <dig>  adaptation of the pseudo gibbs sampler to a limited set of candidate haplotypes; and  <dig>  use of a specific partition-ligation strategy. when reviewing the literature, we found that an iterative haplotyping approach has been previously described for biallelic polymorphisms  <cit> , otherwise all these improvements are totally new.

we performed comparison tests of ishape with other reference programs such as phase, fastphase, and pl-em. we first performed a test on  <dig> haplotype real datasets with or without missing data  and found that ishape <dig> and phase <dig>  yielded similar results on these "small" datasets .

we then performed the comparisons on snp datasets of various size  derived from the hapmap project. the snps were either adjacent or spaced by  <dig> kb and all snps had a minor allele frequency  > 1%. to ensure a fair comparison, the parameters were set identical to the phase <dig>  default parameters . we have limited our tests on the hapmap data with up to  <dig> snps for runtime reasons and also because biologists usually work at the gene level, and few genes will contain more than  <dig> snps.

ishape <dig> produced results with a similar accuracy as phase <dig>  but much more rapidly. it was also more reliable than fastphase and just as rapid. pl-em was not competitive with any of the other three programs in terms of accuracy.

it is interesting to note that when working on adjacent snps, ishape <dig> outperformed phase and fastphase in speed and accuracy. when working on snps spaced by  <dig> kb, our data  suggest that if one considered more than 100– <dig> snps, fastphase would run faster than ishape <dig> 

ishape <dig> behaves better in genomic regions exhibiting a certain level of linkage disequilibrium probably because the reduction of the haplotype space is more important and relevant in that case, and high ld helps the convergence of the posterior gibbs sampler. in the case of a low level of ld, the diversity generated may become too important to ensure a proper convergence of the gibbs sampler, and the risk of missing a true configuration increases : ishape <dig> performed better in terms of median ser and average ranking, but worse in terms of mean ser for the 5kb-spaced snps .

finally, we have tested two additional real datasets with small and large numbers of subjects . table  <dig> suggests that ishape <dig> is more robust when the number of subjects is larger. this is easily understandable since ishape <dig> relies on a bootstrap approach with multiple samplings in the population. if the population is too small, the samplings will not bring enough diversity. table  <dig> shows again that with over  <dig> snps fastphase becomes very competitive in terms of speed and accuracy: this warrants further studies.

the results provided by 2snp show that it is not suited for the common use of haplotyping software by biologists . however one can remark that when dealing with independent snps, its reliability increases  and it will be of interest to compare it with fastphase on very large numbers of snps.

the model of ishape which combines a bootstrap approach and a gibbs sampler approach opens new possibilities for fast and accurate haplotyping. future improvements of the software will target a better treatment of snps with low ld  and the rapid treatment of even larger numbers of snps. for that, we plan to refine the threshold values used, test other partition-ligation strategies, and investigate alternatives to the gibbs sampler since this is the rate limiting step.

in terms of applications, ishape <dig> appears to be a robust haplotyping program suitable for disease association studies which typically address less than  <dig> snps at a time. it may prove advantageous to use ishape <dig> to compute ld in genetic regions, since it is faster and more reliable on neighboring snps. ishape <dig> may also be useful for computing haplotypes serially on genes spanning whole genomes since this application will expand with the advent of large scale genotyping chips  <cit> .

in conclusion, the results presented here show that the ishape heuristic approach is very competitive in terms of accuracy and speed and deserves to be evaluated extensively for its future wide use.

